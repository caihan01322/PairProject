-- phpMyAdmin SQL Dump
-- version 4.1.14
-- http://www.phpmyadmin.net
--
-- ‰∏ªÊ©ü: 127.0.0.1
-- Áî¢ÁîüÊôÇÈñìÔºö 2021 Âπ?03 ??22 ??17:38
-- ‰º∫ÊúçÂô®ÁâàÊú¨: 5.6.17
-- PHP ÁâàÊú¨Ôºö 5.5.12

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;

--
-- Ë≥áÊñôÂ∫´Ôºö `paper`
--

-- --------------------------------------------------------

--
-- Ë≥áÊñôË°®ÁµêÊßã `complexpapers`
--

CREATE TABLE IF NOT EXISTS `complexpapers` (
  `name` varchar(100) NOT NULL,
  `year` varchar(50) NOT NULL,
  `keyword` varchar(100) NOT NULL,
  `link` varchar(200) NOT NULL,
  `digest` varchar(20000) NOT NULL,
  PRIMARY KEY (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

--
-- Ë≥áÊñôË°®ÁöÑÂåØÂá∫Ë≥áÊñô `complexpapers`
--

INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"(MP)2T: Multiple People Multiple Parts Tracker"', '"ECCV 2012"', '["multi-target", "tracking", "pedestrians", "humans", "body part tracking", "network flow optimizati', '"https://doi.org/10.1007/978-3-642-33783-3_8"', '"We present a method for multi-target tracking that exploits the persistence in detection of object parts. While the implicit representation and detection of body parts have recently been leveraged for improved human detection, ours is the first method that attempts to temporally constrain the location of human body parts with the express purpose of improving pedestrian tracking. We pose the problem of simultaneous tracking of multiple targets and their parts in a network flow optimization framework and show that parts of this network need to be optimized separately and iteratively, due to inter-dependencies of node and edge costs. Given potential detections of humans and their parts separately, an initial set of pedestrian tracklets is first obtained, followed by explicit tracking of human parts as constrained by initial human tracking. A merging step is then performed whereby we attempt to include part-only detections for which the entire human is not observable. This step employs a selective appearance model, which allows us to skip occluded parts in description of positive training samples. The result is high confidence, robust trajectories of pedestrians as well as their parts, which essentially constrain each other\\u2019s locations and associations, thus improving human tracking and parts detection. We test our algorithm on multiple real datasets and show that the proposed algorithm is an improvement over the state-of-the-art."'),
('"2.1 Depth Estimation of Frames in Image Sequences Using Motion Occlusions"', '"ECCV 2012"', '["Relative Depth", "Depth Estimation", "Motion Parallax", "Depth Order", "Occlude Pixel"]', '"https://doi.org/10.1007/978-3-642-33885-4_52"', '"This paper proposes a system to depth order regions of a frame belonging to a monocular image sequence. For a given frame, regions are ordered according to their relative depth using the previous and following frames. The algorithm estimates occluded and disoccluded pixels belonging to the central frame. Afterwards, a Binary Partition Tree (BPT) is constructed to obtain a hierarchical, region based representation of the image. The final depth partition is obtained by means of energy minimization on the BPT. To achieve a global depth ordering from local occlusion cues, a depth order graph is constructed and used to eliminate contradictory local cues. Results of the system are evaluated and compared with state of the art figure/ground labeling systems on several datasets, showing promising results."'),
('"2.5D Dual Contouring: A Robust Approach to Creating Building Models from Aerial LiDAR Point Clouds"', '"ECCV 2010"', '["Point Cloud", "Principal Direction", "Leaf Cell", "Input Point", "Boundary Polygon"]', '"https://doi.org/10.1007/978-3-642-15558-1_9"', '"We present a robust approach to creating 2.5D building models from aerial LiDAR point clouds. The method is guaranteed to produce crack-free models composed of complex roofs and vertical walls connecting them. By extending classic dual contouring into a 2.5D method, we achieve a simultaneous optimization over the three dimensional surfaces and the two dimensional boundaries of roof layers. Thus, our method can generate building models with arbitrarily shaped roofs while keeping the verticality of connecting walls. An adaptive grid is introduced to simplify model geometry in an accurate manner. Sharp features are detected and preserved by a novel and efficient algorithm."'),
('"2D Action Recognition Serves 3D Human Pose Estimation"', '"ECCV 2010"', '["Action Class", "Action Recognition", "Human Action Recognition", "Visual Hull", "Action Recognitio', '"https://doi.org/10.1007/978-3-642-15558-1_31"', '"3D human pose estimation in multi-view settings benefits from embeddings of human actions in low-dimensional manifolds, but the complexity of the embeddings increases with the number of actions. Creating separate, action-specific manifolds seems to be a more practical solution. Using multiple manifolds for pose estimation, however, requires a joint optimization over the set of manifolds and the human pose embedded in the manifolds. In order to solve this problem, we propose a particle-based optimization algorithm that can efficiently estimate human pose even in challenging in-house scenarios. In addition, the algorithm can directly integrate the results of a 2D action recognition system as prior distribution for optimization. In our experiments, we demonstrate that the optimization handles an 84D search space and provides already competitive results on HumanEva with as few as 25 particles."'),
('"2D and 3D Multimodal Hybrid Face Recognition"', '"ECCV 2006"', '["Face Recognition", "Face Match", "Skin Detection", "Face Recognition Algorithm", "Face Recognition', '"https://doi.org/10.1007/11744078_27"', '"We present a 2D and 3D multimodal hybrid face recognition algorithm and demonstrate its performance on the FRGC v1.0 data. We use hybrid (feature-based and holistic) matching for the 3D faces and a holistic matching approach on the 2D faces. Feature-based matching is performed by offline segmenting each 3D face in the gallery into three regions, namely the eyes-forehead, the nose and the cheeks. The cheeks are discarded to avoid facial expressions and hair. During recognition, each feature in the gallery is automatically matched, using a modified ICP algorithm, with a complete probe face. The holistic 3D and 2D face matching is performed using PCA. Individual matching scores are fused after normalization and the results are compared to the BEE baseline performances in order to provide some answers to the first three conjectures of the FRGC. Our multimodal hybrid algorithm substantially outperformed others by achieving 100% verification rate at 0.0006 FAR."'),
('"2D Image Analysis by Generalized Hilbert Transforms in Conformal Space"', '"ECCV 2008"', '["Conformal Space", "Poisson Kernel", "Conformal Curvature", "Monogenic Signal", "Radon Space"]', '"https://doi.org/10.1007/978-3-540-88688-4_47"', '"This work presents a novel rotational invariant quadrature filter approach - called the conformal monogenic signal - for analyzing i(ntrinsic)1D and i2D local features of any curved 2D signal such as lines, edges, corners and junctions without the use of steering. The conformal monogenic signal contains the monogenic signal as a special case for i1D signals and combines monogenic scale space, phase, direction/orientation, energy and curvature in one unified algebraic framework. The conformal monogenic signal will be theoretically illustrated and motivated in detail by the relation of the 3D Radon transform and the generalized Hilbert transform on the sphere. The main idea is to lift up 2D signals to the higher dimensional conformal space where the signal features can be analyzed with more degrees of freedom. Results of this work are the low computational time complexity, the easy implementation into existing Computer Vision applications and the numerical robustness of determining curvature without the need of any derivatives."'),
('"3-D Histogram-Based Segmentation and Leaf Detection for Rosette Plants"', '"ECCV 2014"', '["3-D Histogram thresholding", "Euclidean distance map", "Graph analysis", "Leaf counting", "Leaf se', '"https://doi.org/10.1007/978-3-319-16220-1_5"', '"Recognition and segmentation of plant organs like leaves is one of the major challenges in digital plant phenotyping. Here we present a 3-D histogram-based segmentation and recognition approach for top view images of rosette plants such as Arabidopsis thaliana and tobacco. Furthermore a euclidean-distance-map-based method for the detection of leaves and the corresponding plant leaf segmentation method were developed. An approach for the detection of optimal leaf split points for the separation of overlapping leaf segments was created. We tested and tuned our algorithms for the Leaf Segmentation Challenge (LSC). The results demonstrate that our method is robust and handles demanding imaging situations and different species with high accuracy."'),
('"3-D Motion and Structure from 2-D Motion Causally Integrated over Time: Implementation"', '"ECCV 2000"', '["Pattern Anal", "Rigid Motion", "Motion Error", "Minimal Realization", "Correspondence Problem"]', '"https://doi.org/10.1007/3-540-45053-X_47"', '"The causal estimation of three-dimensional motion from a sequence of two-dimensional images can be posed as a nonlinear filtering problem. We describe the implementation of an algorithm whose uniform observability, minimal realization and stability have been proven analytically in [5]. We discuss a scheme for handling occlusions, drift in the scale factor and tuning of the filter. We also present an extension to partially calibrated camera models and prove its observability. We report the performance of our implementation on a few long sequences of real images. More importantly, however, we have made our real-time implementation - which runs on a personal computer - available to the public for first-hand testing."'),
('"3-D Ultrasound Probe Calibration for Computer-Guided Diagnosis and Therapy"', '"CVAMIA 2006"', '["Root Mean Square Error", "Feature Extraction", "Root Mean Square", "Distance Error", "Angular Erro', '"https://doi.org/10.1007/11889762_22"', '"With the emergence of swept-volume ultrasound (US) probes, precise and almost real-time US volume imaging has become available. This offers many new opportunities for computer guided diagnosis and therapy, 3-D images containing significantly more information than 2-D slices. However, computer guidance often requires knowledge about the exact position of US voxels relative to a tracking reference, which can only be achieved through probe calibration. In this paper we present a 3-D US probe calibration system based on a membrane phantom. The calibration matrix is retrieved by detection of a membrane plane in a dozen of US acquisitions of the phantom. Plane detection is robustly performed with the 2-D Hough transformation. The feature extraction process is fully automated, calibration requires about 20 minutes and the calibration system can be used in a clinical context. The precision of the system was evaluated to a root mean square (RMS) distance error of 1.15mm and to an RMS angular error of 0.61\\u00b0. The point reconstruction accuracy was evaluated to 0.9mm and the angular reconstruction accuracy to 1.79\\u00b0."'),
('"30Hz Object Detection with DPM V5"', '"ECCV 2014"', '["Fast Object Detection", "Real-time Object Detection", "Fast Deformable Parts Model"]', '"https://doi.org/10.1007/978-3-319-10590-1_5"', '"We describe an implementation of the Deformable Parts Model [1] that operates in a user-defined time-frame. Our implementation uses a variety of mechanism to trade-off speed against accuracy. Our implementation can detect all 20 PASCAL 2007 objects simultaneously at 30Hz with an mAP of 0.26. At 15Hz, its mAP is 0.30; and at 100Hz, its mAP is 0.16. By comparison the reference implementation of [1] runs at 0.07Hz and mAP of 0.33 and a fast GPU implementation runs at 1Hz. Our technique is over an order of magnitude faster than the previous fastest DPM implementation. Our implementation exploits a series of important speedup mechanisms. We use the cascade framework of [3] and the vector quantization technique of [2]. To speed up feature computation, we compute HOG features at few scales, and apply many interpolated templates. A hierarchical vector quantization method is used to compress HOG features for fast template evaluation. An object proposal step uses hash-table methods to identify locations where evaluating templates would be most useful; these locations are inserted into a priority queue, and processed in a detection phase. Both proposal and detection phases have an any-time property. Our method applies to legacy templates, and no retraining is required."'),
('"3D Deformable Face Tracking with a Commodity Depth Camera"', '"ECCV 2010"', '["Texture Image", "Depth Image", "Iterative Close Point", "Head Model", "Deformable Model"]', '"https://doi.org/10.1007/978-3-642-15558-1_17"', '"Recently, there has been an increasing number of depth cameras available at commodity prices. These cameras can usually capture both color and depth images in real-time, with limited resolution and accuracy. In this paper, we study the problem of 3D deformable face tracking with such commodity depth cameras. A regularized maximum likelihood deformable model fitting (DMF) algorithm is developed, with special emphasis on handling the noisy input depth data. In particular, we present a maximum likelihood solution that can accommodate sensor noise represented by an arbitrary covariance matrix, which allows more elaborate modeling of the sensor\\u2019s accuracy. Furthermore, an \\u21131 regularization scheme is proposed based on the semantics of the deformable face model, which is shown to be very effective in improving the tracking results. To track facial movement in subsequent frames, feature points in the texture images are matched across frames and integrated into the DMF framework seamlessly. The effectiveness of the proposed method is demonstrated with multiple sequences with ground truth information."'),
('"3D Digitization of a Hand-Held Object with a Wearable Vision Sensor"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_13"', '"It is a common human behavior to hold a small object of interest and to manipulate it for observation. A computer system, symbiotic with a human, should recognize the object and the intention of the human while the object is manipulated. To realize autonomous recognition of a hand-held object as well as to study human behavior of manipulation and observation, we have developed a wearable vision sensor, which has similar sight to a human who wears the sensor. In this paper, we show the following results obtained with the wearable vision sensor. First, we analyzed human manipulation for observation and related it with acquirable visual information. Second, we proposed dynamic space carving for 3D shape extraction of a static object occluded by a dynamic object moving around the static object. Finally, we showed that texture and silhouette information can be integrated in vacant space and the integration improves the efficiency of dynamic space carving."'),
('"3D Face Model Fitting for Recognition"', '"ECCV 2008"', '["Face Recognition", "Mean Average Precision", "Face Model", "Iterative Close Point Algorithm", "Fac', '"https://doi.org/10.1007/978-3-540-88693-8_48"', '"This paper presents an automatic efficient method to fit a statistical deformation model of the human face to 3D scan data. In a global to local fitting scheme, the shape parameters of this model are optimized such that the produced instance of the model accurately fits the 3D scan data of the input face. To increase the expressiveness of the model and to produce a tighter fit of the model, our method fits a set of predefined face components and blends these components afterwards. Quantitative evaluation shows an improvement of the fitting results when multiple components are used instead of one. Compared to existing methods, our fully automatic method achieves a higher accuracy of the fitting results. The accurately generated face instances are manifold meshes without noise and holes, and can be effectively used for 3D face recognition: We achieve 97.5% correct identification for 876 queries in the UND face set with 3D faces. Our results show that contour curve based face matching outperforms landmark based face matching."'),
('"3D Face Recognition by Local Shape Difference Boosting"', '"ECCV 2008"', '["Face Recognition", "Depth Image", "Iterative Close Point", "Nose Bridge", "Gallery Face"]', '"https://doi.org/10.1007/978-3-540-88682-2_46"', '"A new approach, called Collective Shape Difference Classifier (CSDC), is proposed to improve the accuracy and computational efficiency of 3D face recognition. The CSDC learns the most discriminative local areas from the Pure Shape Difference Map (PSDM) and trains them as weak classifiers for assembling a collective strong classifier using the real-boosting approach. The PSDM is established between two 3D face models aligned by a posture normalization procedure based on facial features. The model alignment is self-dependent, which avoids registering the probe face against every different gallery face during the recognition, so that a high computational speed is obtained. The experiments, carried out on the FRGC v2 and BU-3DFE databases, yield rank-1 recognition rates better than 98%. Each recognition against a gallery with 1000 faces only needs about 3.05 seconds. These two experimental results together with the high performance recognition on partial faces demonstrate that our algorithm is not only effective but also efficient."'),
('"3D Facial Landmark Localization Using Combinatorial Search and Shape Regression"', '"ECCV 2012"', '["Shape Model", "Deformable Model", "Gabor Wavelet", "Spin Image", "Statistical Shape Model"]', '"https://doi.org/10.1007/978-3-642-33863-2_4"', '"This paper presents a method for the automatic detection of facial landmarks. The algorithm receives a set of 3D candidate points for each landmark (e.g. from a feature detector) and performs combinatorial search constrained by a deformable shape model. A key assumption of our approach is that for some landmarks there might not be an accurate candidate in the input set. This is tackled by detecting partial subsets of landmarks and inferring those that are missing so that the probability of the deformable model is maximized. The ability of the model to work with incomplete information makes it possible to limit the number of candidates that need to be retained, substantially reducing the number of possible combinations to be tested with respect to the alternative of trying to always detect the complete set of landmarks. We demonstrate the accuracy of the proposed method in a set of 144 facial scans acquired by means of a hand-held laser scanner in the context of clinical craniofacial dysmorphology research. Using spin images to describe the geometry and targeting 11 facial landmarks, we obtain an average error below 3 mm, which compares favorably with other state of the art approaches based on geometric descriptors."'),
('"3D Finger Biometrics"', '"BioAW 2004"', '["Range Image", "Fusion Rule", "False Acceptance Rate", "False Rejection Rate", "Hand Shape"]', '"https://doi.org/10.1007/978-3-540-25976-3_22"', '"This paper investigates the use of the back surface of the hand, specifically the fingers, as a biometric feature. We focus our efforts on the index, middle and ring fingers. We use segmented 3D range images of the back of the hand. At each pixel lying on one of the these fingers, the minimum and maximum curvature values are calculated and then used to compute the shape index, resulting in a shape index image of each of the three fingers. The shape index images are then compared to determine the similarity between two images. We use data sets obtained over time to analyze the stability of this feature, and examine the performance of each finger as a separate biometric feature along with the performance obtained by combining them. Our approach yields good results indicating this approach should be further researched."'),
('"3D Gesture Touchless Control Based on Real-Time Stereo Matching"', '"ECCV 2012"', '["Gesture Recognition", "Stereo Match", "Stereo Camera", "Brush Stroke", "FPGA Board"]', '"https://doi.org/10.1007/978-3-642-33885-4_65"', '"In this demonstration a real-time three-dimension gesture control system is reported. It consists with an FPGA board for the depth extraction from stereo camera and a PC with gesture recognition software followed by user interface examples. Application of this technology includes TV remote control, vendor machine interface, or other outdoor touchless control, etc. With optimization on both chip-area and operating clock rate, the stereo matching in FPGA yields a frame rate up to 60 FPS at a resolution of Full HD 1080i. Thus, the system makes the real-time gesture control practical."'),
('"3D Glasses as Mobility Aid for Visually Impaired People"', '"ECCV 2014"', '["Wearable", "Visually impaired", "3D", "Stereo vision", "Obstacle detection"]', '"https://doi.org/10.1007/978-3-319-16199-0_38"', '"This paper proposes an effective and wearable mobility aid aimed at improving the quality of life of people suffering for visual disabilities by enabling autonomous and safe navigation in unknown environments. Our system relies on dense and accurate depth maps, provided in real-time by a compact stereo vision system mapped into an FPGA, in order to detect obstacles in front of the user and to provide accordingly vibration feedbacks as well as audio information by means of a bone-conductive speakers. Compared to most approaches with similar purposes, even in the current prototype arrangement deployed for testing, our system is extremely compact, lightweight and energy efficient thus enabling hours of safe and autonomous navigation with standard batteries avoiding the need to carry cumbersome devices. Moreover, by conceiving the 3D sensing device as a replacement of standard glasses typically worn by visually impaired people and by using intuitive feedbacks provided by means of lightweight actuators, our system provides an ergonomic and comfortable user interface with a fast learning curve for its effective deployment. This fact has been extensively verified on the field by means of an experimental evaluation, in indoor as well as in outdoor environments, with different users simulating visual impairment including a blind person."'),
('"3D Hand Pose Detection in Egocentric RGB-D Images"', '"ECCV 2014"', '["Egocentric vision", "Hand pose", "Multi-class classifier", "RGB-D sensor"]', '"https://doi.org/10.1007/978-3-319-16178-5_25"', '"We focus on the task of hand pose estimation from egocentric viewpoints. For this problem specification, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is exacerbated when considering a wearable sensor and a first-person camera viewpoint: the occlusions inherent to the particular camera view and the limitations in terms of field of view make the problem even more difficult. We propose to use task and viewpoint specific synthetic training exemplars in a discriminative detection framework. We also exploit the depth features for a sparser and faster detection. We evaluate our approach on a real-world annotated dataset and propose a novel annotation technique for accurate 3D hand labelling even in case of partial occlusions."'),
('"3D Human Body Tracking Using Deterministic Temporal Motion Models"', '"ECCV 2004"', '["Joint Angle", "Motion Vector", "Motion Model", "Motion Capture", "Principal Component Analysis Com', '"https://doi.org/10.1007/978-3-540-24672-5_8"', '"There has been much effort invested in increasing the robustness of human body tracking by incorporating motion models. Most approaches are probabilistic in nature and seek to avoid becoming trapped into local minima by considering multiple hypotheses, which typically requires exponentially large amounts of computation as the number of degrees of freedom increases."'),
('"3D Interest Point Detection via Discriminative Learning"', '"ECCV 2014"', '["3D computer vision"]', '"https://doi.org/10.1007/978-3-319-10590-1_11"', '"The task of detecting the interest points in 3D meshes has typically been handled by geometric methods. These methods, while designed according to human preference, can be ill-equipped for handling the variety and subjectivity in human responses. Different tasks have different requirements for interest point detection; some tasks may necessitate high precision while other tasks may require high recall. Sometimes points with high curvature may be desirable, while in other cases high curvature may be an indication of noise. Geometric methods lack the required flexibility to adapt to such changes. As a consequence, interest point detection seems to be well suited for machine learning methods that can be trained to match the criteria applied on the annotated training data. In this paper, we formulate interest point detection as a supervised binary classification problem using a random forest as our classifier. We validate the accuracy of our method and compare our results to those of five state of the art methods on a new, standard benchmark."'),
('"3D Layout Propagation to Improve Object Recognition in Egocentric Videos"', '"ECCV 2014"', '["Scene understanding", "Egocentric vision", "Object detection"]', '"https://doi.org/10.1007/978-3-319-16199-0_58"', '"Intelligent systems need complex and detailed models of their environment to achieve more sophisticated tasks, such as assistance to the user. Vision sensors provide rich information and are broadly used to obtain these models, for example, indoor scene modeling from monocular images has been widely studied. A common initial step in those settings is the estimation of the \\\\(3\\\\)D layout of the scene. While most of the previous approaches obtain the scene layout from a single image, this work presents a novel approach to estimate the initial layout and addresses the problem of how to propagate it on a video. We propose to use a particle filter framework for this propagation process and describe how to generate and sample new layout hypotheses for the scene on each of the following frames. We present different ways to evaluate and rank these hypotheses. The experimental validation is run on two recent and publicly available datasets and shows promising results on the estimation of a basic \\\\(3\\\\)D layout. Our experiments demonstrate how this layout information can be used to improve detection tasks useful for a human user, in particular sign detection, by easily rejecting false positives."'),
('"3D Modelling of Static Environments Using Multiple Spherical Stereo"', '"ECCV 2010"', '["Environment modelling", "Spherical stereo", "PDE-based disparity estimation", "Multiple stereo rec', '"https://doi.org/10.1007/978-3-642-35740-4_14"', '"We propose a 3D modelling method from multiple pairs of spherical stereo images. A static environment is captured as a vertical stereo pair with a rotating line scan camera at multiple locations and depth fields are extracted for each pair using spherical stereo geometry. We propose a new PDE-based stereo matching method which handles occlusion and over-segmentation problem in highly textured regions. In order to avoid cumbersome camera calibration steps, we extract a 3D rigid transform using feature matching between views and fuse all models into one complete mesh. A reliable surface selection algorithm for overlapped surfaces is proposed for merging multiple meshes in order to keep surface details while removing outliers. The performances of the proposed algorithms are evaluated against ground-truth from LIDAR scans."'),
('"3D Modelling Using Geometric Constraints: A Parallelepiped Based Approach"', '"ECCV 2002"', '["Geometric Constraint", "Camera Calibration", "Intrinsic Parameter", "Calibration Problem", "Dualit', '"https://doi.org/10.1007/3-540-47979-1_15"', '"In this paper, efficient and generic tools for calibration and 3D reconstruction are presented. These tools exploit geometric constraints frequently present in man-made environments and allow camera calibration as well as scene structure to be estimated with a small amount of user interactions and little a priori knowledge. The proposed approach is based on primitives that naturally characterize rigidity constraints: parallelepipeds. It has been shown previously that the intrinsic metric characteristics of a parallelepiped are dual to the intrinsic characteristics of a perspective camera. Here, we generalize this idea by taking into account additional redundancies between multiple images of multiple parallelepipeds. We propose a method for the estimation of camera and scene parameters that bears strong similarities with some self-calibration approaches. Taking into account prior knowledge on scene primitives or cameras, leads to simpler equations than for standard self-calibration, and is expected to improve results, as well as to allow structure and motion recovery in situations that are otherwise under-constrained. These principles are illustrated by experimental calibration results and several reconstructions from uncalibrated images."'),
('"3D Multimodal Simulation of Image Acquisition by X-Ray and MRI for Validation of Seedling Measureme', '"ECCV 2014"', '["Seedling monitoring", "X-Ray", "MRI", "3D simulation"]', '"https://doi.org/10.1007/978-3-319-16220-1_10"', '"In this report, we present a 3D simulator for the numerical validation of segmentation algorithms for seedling in soil from X-ray or MRI. A 3D simulator of root in elongation is coupled to a simulator of the image acquisition to generate images of simulated seedling associated with a known synthetic ground truth. We detail how acquisition parameters of the seedling and parameters of the imaging systems are estimated and combined to produce realistic images. The resulting simulator is available on line to open the possibility of segmentation challenges with in silico validation based on unlimited number of seedling."'),
('"3D Non-rigid Surface Matching and Registration Based on Holomorphic Differentials"', '"ECCV 2008"', '["Conformal Mapping", "Connected Domain", "Iterative Close Point", "Ricci Flow", "Surface Match"]', '"https://doi.org/10.1007/978-3-540-88690-7_1"', '"3D surface matching is fundamental for shape registration, deformable 3D non-rigid tracking, recognition and classification. In this paper we describe a novel approach for generating an efficient and optimal combined matching from multiple boundary-constrained conformal parameterizations for multiply connected domains (i.e., genus zero open surface with multiple boundaries), which always come from imperfect 3D data acquisition (holes, partial occlusions, change of pose and non-rigid deformation between scans). This optimality criterion is also used to assess how consistent each boundary is, and thus decide to enforce or relax boundary constraints across the two surfaces to be matched. The linear boundary-constrained conformal parameterization is based on the holomorphic differential forms, which map a surface with n boundaries conformally to a planar rectangle with (n - 2) horizontal slits, other two boundaries as constraints. The mapping is a diffeomorphism and intrinsic to the geometry, handles an open surface with arbitrary number of boundaries, and can be implemented as a linear system. Experimental results are given for real facial surface matching, deformable cloth non-rigid tracking, which demonstrate the efficiency of our method, especially for 3D non-rigid surfaces with significantly inconsistent boundaries."'),
('"3D Object Classification Using Scale Invariant Heat Kernels with Collaborative Classification"', '"ECCV 2012"', '["Heat kernels", "shape retrieval", "collaborative classification", "3D shape descriptors"]', '"https://doi.org/10.1007/978-3-642-33863-2_3"', '"One of the major goals of computer vision is the development of flexible and efficient methods for shape representation. This paper proposes an approach for shape matching and retrieval based on scale-invariant heat kernel (HK). The approach uses a novel descriptor based on the histograms of the scale-invariant HK for a number of critical points on the shape at different time scales. We propose an improved method to introduce scale-invariance of HK to avoid noise-sensitive operations in the original method. A collaborative classification (CC) scheme is then employed for object classification. For comparison we compare our approach to well-known approaches on a standard benchmark dataset: the SHREC 2011. The results have indeed confirmed the high performance of the proposed approach on the shape retrieval problem."'),
('"3D Object Detection with Multiple Kinects"', '"ECCV 2012"', '["Point Cloud", "Object Detection", "Depth Information", "Depth Image", "Camera View"]', '"https://doi.org/10.1007/978-3-642-33868-7_10"', '"Categorizing and localizing multiple objects in 3D space is a challenging but essential task for many robotics and assisted living applications. While RGB cameras as well as depth information have been widely explored in computer vision there is surprisingly little recent work combining multiple cameras and depth information. Given the recent emergence of consumer depth cameras such as Kinect we explore how multiple cameras and active depth sensors can be used to tackle the challenge of 3D object detection. More specifically we generate point clouds from the depth information of multiple registered cameras and use the VFH descriptor [20] to describe them. For color images we employ the DPM [3] and combine both approaches with a simple voting approach across multiple cameras."'),
('"3D Plant Modeling: Localization, Mapping and Segmentation for Plant Phenotyping Using a Single Hand', '"ECCV 2014"', '["Plant phenotyping", "SLAM", "Structure from motion", "Segmentation"]', '"https://doi.org/10.1007/978-3-319-16220-1_18"', '"Functional-structural modeling and high-throughput phenomics demand tools for 3D measurements of plants. In this work, structure from motion is employed to estimate the position of a hand-held camera, moving around plants, and to recover a sparse 3D point cloud sampling the plants\\u2019 surfaces. Multiple-view stereo is employed to extend the sparse model to a dense 3D point cloud. The model is automatically segmented by spectral clustering, properly separating the plant\\u2019s leaves whose surfaces are estimated by fitting trimmed B-splines to their 3D points. These models are accurate snapshots for the aerial part of the plants at the image acquisition moment and allow the measurement of different features of the specimen phenotype. Such state-of-the-art computer vision techniques are able to produce accurate 3D models for plants using data from a single free moving camera, properly handling occlusions and diversity in size and structure for specimens presenting sparse canopies. A data set formed by the input images and the resulting camera poses and 3D points clouds is available, including data for sunflower and soybean specimens."'),
('"3D Point Correspondence by Minimum Description Length in Feature Space"', '"ECCV 2010"', '["Feature Space", "Reconstruction Error", "Minimum Description Length", "Kernel Parameter", "Kernel ', '"https://doi.org/10.1007/978-3-642-15558-1_45"', '"Finding point correspondences plays an important role in automatically building statistical shape models from a training set of 3D surfaces. For the point correspondence problem, Davies et al. [1] proposed a minimum-description-length-based objective function to balance the training errors and generalization ability. A recent evaluation study [2] that compares several well-known 3D point correspondence methods for modeling purposes shows that the MDL-based approach [1] is the best method."'),
('"3D Reconstruction from Tangent-of-Sight Measurements of a Moving Object Seen from a Moving Camera"', '"ECCV 2000"', '[]', '"https://doi.org/10.1007/3-540-45054-8_40"', '"Consider the situation of a monocular image sequence with known ego-motion observing a 3D point moving simultaneously but along a path of up to second order, i.e. it can trace a line in 3D or a conic shaped path. We wish to reconstruct the 3D path from the projection of the tangent to the path at each time instance. This problem is analogue to the \\u201ctrajectory triangulation\\u201d of lines and conic sections recently introduced in [1,3], but instead of observing a point projection we observe a tangent projection and thus obtain a far simpler solution to the problem."'),
('"3D Reconstruction of a Moving Point from a Series of 2D Projections"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_12"', '"This paper presents a linear solution for reconstructing the 3D trajectory of a moving point from its correspondence in a collection of 2D perspective images, given the 3D spatial pose and time of capture of the cameras that produced each image. Triangulation-based solutions do not apply, as multiple views of the point may not exist at each instant in time. A geometric analysis of the problem is presented and a criterion, called reconstructibility, is defined to precisely characterize the cases when reconstruction is possible, and how accurate it can be. We apply the linear reconstruction algorithm to reconstruct the time evolving 3D structure of several real-world scenes, given a collection of non-coincidental 2D images."'),
('"3D Reconstruction of Dynamic Scenes with Multiple Handheld Cameras"', '"ECCV 2012"', '["multi-view stereo", "depth recovery", "dynamic scene", "spatio-temporal optimization"]', '"https://doi.org/10.1007/978-3-642-33709-3_43"', '"Accurate dense 3D reconstruction of dynamic scenes from natural images is still very challenging. Most previous methods rely on a large number of fixed cameras to obtain good results. Some of these methods further require separation of static and dynamic points, which are usually restricted to scenes with known background. We propose a novel dense depth estimation method which can automatically recover accurate and consistent depth maps from the synchronized video sequences taken by a few handheld cameras. Unlike fixed camera arrays, our data capturing setup is much more flexible and easier to use. Our algorithm simultaneously solves bilayer segmentation and depth estimation in a unified energy minimization framework, which combines different spatio-temporal constraints for effective depth optimization and segmentation of static and dynamic points. A variety of examples demonstrate the effectiveness of the proposed framework."'),
('"3D Reconstruction of Dynamic Textures in Crowd Sourced Data"', '"ECCV 2014"', '["Dynamic Scene", "Dynamic Texture", "Structure From Motion", "Visual Hull", "Dynamic Content"]', '"https://doi.org/10.1007/978-3-319-10590-1_10"', '"We propose a framework to automatically build 3D models for scenes containing structures not amenable for photo-consistency based reconstruction due to having dynamic appearance. We analyze the dynamic appearance elements of a given scene by leveraging the imagery contained in Internet image photo-collections and online video sharing websites. Our approach combines large scale crowd sourced SfM techniques with image content segmentation and shape from silhouette techniques to build an iterative framework for 3D shape estimation. The developed system not only enables more complete and robust 3D modeling, but it also enables more realistic visualizations through the identification of dynamic scene elements amenable to dynamic texture mapping. Experiments on crowd sourced image and video datasets illustrate the effectiveness of our automated data-driven approach."'),
('"3D Reconstruction of Non-Rigid Surfaces in Real-Time Using Wedge Elements"', '"ECCV 2012"', '["Strong Deformation", "Bundle Adjustment", "Scene Point", "Visible Side", "Isometric Deformation"]', '"https://doi.org/10.1007/978-3-642-33863-2_12"', '"We present a new FEM (Finite Element Method) model for the 3D reconstruction of a deforming scene using as sole input a calibrated video sequence. Our approach extends the recently proposed 2D thin-plate FEM+EKF (Extended Kalman Filter) combination. Thin-plate FEM is an approximation that models a deforming 3D thin solid as a surface, and then discretizes the surface as a mesh of planar triangles. In contrast, we propose a full-fledged 3D FEM formulation where the deforming 3D solid is discretized as a mesh of 3D wedge elements. The new 3D FEM formulation provides better conditioning for the rank analysis stage necessary to remove the rigid boundary points from the formulation. We show how the proposed formulation accurately estimates deformable scenes from real imagery even for strong deformations. Crucially we also show, for the first time to the best of our knowledge, NRSfM (Non-Rigid Structure from Motion) at 30Hz real-time over real imagery. Real-time can be achieved for our 3D FEM formulation combined with an EKF resulting in accurate estimates even for small size maps."'),
('"3D Rotation Invariant Decomposition of Motion Signals"', '"ECCV 2012"', '["3D", "motion trajectory", "rotation invariant", "shift-invariant", "matching pursuit", "Procrustes', '"https://doi.org/10.1007/978-3-642-33885-4_18"', '"A new model for describing a three-dimensional (3D) trajectory is introduced in this article. The studied object is viewed as a linear combination of rotatable 3D patterns. The resulting model is now 3D rotation invariant (3DRI). Moreover, the temporal patterns are considered as shift-invariant. A novel 3DRI decomposition problem consists of estimating the active patterns, their coefficients, their rotations and their shift parameters. Sparsity allows to select few patterns among multiple ones. Based on the sparse approximation principle, a non-convex optimization called 3DRI matching pursuit (3DRI-MP) is proposed to solve this problem. This algorithm is applied to real and simulated data, and compared in order to evaluate its performances."'),
('"3D Statistical Shape Models Using Direct Optimisation of Description Length"', '"ECCV 2002"', '["Shape Model", "Minimum Description Length", "Active Shape Model", "Statistical Shape Model", "Shap', '"https://doi.org/10.1007/3-540-47977-5_1"', '"We describe an automatic method for building optimal 3D statistical shape models from sets of training shapes. Although shape models show considerable promise as a basis for segmenting and interpreting images, a major drawback of the approach is the need to establish a dense correspondence across a training set of example shapes. It is important to establish the correct correspondence, otherwise poor models can result. In 2D, this can be achieved using manual \\u2018landmarks\\u2019, but in 3D this becomes impractical. We show it is possible to establish correspondences automatically, by casting the correspondence problem as one of finding the \\u2018optimal\\u2019 parameterisation of each shape in the training set. We describe an explicit representation of surface parameterisation, that ensures the resulting correspondences are legal, and show how this representation can be manipulated to minimise the description length of the training set using the model. This results in compact models with good generalisation properties. Results are reported for two sets of biomedical shapes, showing significant improvement in model properties compared to those obtained using a uniform surface parameterisation."'),
('"3D Surface Reconstruction Using Graph Cuts with Surface Constraints"', '"ECCV 2006"', '["Sink Node", "Surface Point", "Search Region", "True Surface", "Normalize Cross Correlation"]', '"https://doi.org/10.1007/11744047_17"', '"We describe a graph cut algorithm to recover the 3D object surface using both silhouette and foreground color information. The graph cut algorithm is used for optimization on a color consistency field. Constraints are added to improve its performance. These constraints are a set of predetermined locations that the true surface of the object is likely to pass through. They are used to preserve protrusions and to pursue concavities respectively in the first and the second phase of the algorithm. We also introduce a method for dealing with silhouette uncertainties arising from background subtraction on real data. We test the approach on synthetic data with different numbers of views (8, 16, 32, 64) and on a real image set containing 30 views of a toy squirrel."'),
('"3D2PM \\u2013 3D Deformable Part Models"', '"ECCV 2012"', '["Object Detection", "Average Precision", "Object Class", "Deformable Part Model", "Pairwise Term"]', '"https://doi.org/10.1007/978-3-642-33783-3_26"', '"As objects are inherently 3-dimensional, they have been modeled in 3D in the early days of computer vision. Due to the ambiguities arising from mapping 2D features to 3D models, 2D feature-based models are the predominant paradigm in object recognition today. While such models have shown competitive bounding box (BB) detection performance, they are clearly limited in their capability of fine-grained reasoning in 3D or continuous viewpoint estimation as required for advanced tasks such as 3D scene understanding. This work extends the deformable part model [1] to a 3D object model. It consists of multiple parts modeled in 3D and a continuous appearance model. As a result, the model generalizes beyond BB oriented object detection and can be jointly optimized in a discriminative fashion for object detection and viewpoint estimation. Our 3D Deformable Part Model (3D2PM) leverages on CAD data of the object class, as a 3D geometry proxy."'),
('"5D Motion Subspaces for Planar Motions"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_11"', '"In practice, rigid objects often move on a plane. The object then rotates around a fixed axis and translates in a plane orthogonal to this axis. For a concrete example, think of a car moving on a street. Given multiple static affine cameras which observe such a rigidly moving object and track feature points located on this object, what can be said about the resulting feature point trajectories in the camera views? Are there any useful algebraic constraints hidden in the data? Is a 3D reconstruction of the scene possible even if there are no feature point correspondences between the different cameras? And if so, how many points are sufficient? Does a closed-form solution to this shape from motion reconstruction problem exist?"'),
('"A 2D Fourier Approach to Deformable Model Segmentation of 3D Medical Images"', '"MMBIA 2004"', '["Distal Femur", "Active Contour", "Shape Model", "Manual Segmentation", "Iterative Close Point"]', '"https://doi.org/10.1007/978-3-540-27816-0_16"', '"Anatomical shapes present a unique problem in terms of accurate representation and medical image segmentation. Three-dimensional statistical shape models have been extensively researched as a means of autonomously segmenting and representing models. We present a segmentation method driven by a statistical shape model based on a priori shape information from manually segmented training image sets. Our model is comprised of a stack of two-dimensional Fourier descriptors computed from the perimeters of the segmented training image sets after a transformation into a canonical coordinate frame. We apply our shape model to the segmentation of CT and MRI images of the distal femur via an original iterative method based on active contours. The results from the application of our novel method demonstrate its ability to accurately capture anatomical shape variations and guide segmentation. Our quantitative results are unique in that most similar previous work presents only qualitative results."'),
('"A 2D Human Body Model Dressed in Eigen Clothing"', '"ECCV 2010"', '["Body Shape", "Beta Distribution", "Camera View", "Body Contour", "Active Shape Model"]', '"https://doi.org/10.1007/978-3-642-15549-9_21"', '"Detection, tracking, segmentation and pose estimation of people in monocular images are widely studied. Two-dimensional models of the human body are extensively used, however, they are typically fairly crude, representing the body either as a rough outline or in terms of articulated geometric primitives. We describe a new 2D model of the human body contour that combines an underlying naked body with a low-dimensional clothing model. The naked body is represented as a Contour Person that can take on a wide variety of poses and body shapes. Clothing is represented as a deformation from the underlying body contour. This deformation is learned from training examples using principal component analysis to produce eigen clothing. We find that the statistics of clothing deformations are skewed and we model the a priori probability of these deformations using a Beta distribution. The resulting generative model captures realistic human forms in monocular images and is used to infer 2D body shape and pose under clothing. We also use the coefficients of the eigen clothing to recognize different categories of clothing on dressed people. The method is evaluated quantitatively on synthetic and real images and achieves better accuracy than previous methods for estimating body shape under clothing."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A Background Maintenance Model in the Spatial-Range Domain"', '"SMVP 2004"', '[]', '"https://doi.org/10.1007/978-3-540-30212-4_13"', '"In this article a background maintenance model defined by a finite set of codebook vectors in the spatial-range domain is proposed. The model represents its current state by a foreground and a background set of codebook vectors. Algorithms that dynamically update these sets by adding and removing codebook vectors are described. This approach is fundamentally different from algorithms that maintain a background representation at the pixel level and continously update their parameters. The performance of the model is demonstrated and compared to other background maintenance models using a suitable benchmark of video sequences."'),
('"A Batch Algorithm for Implicit Non-rigid Shape and Motion Recovery"', '"WDV 2006"', '["Image Point", "Camera Motion", "Measurement Matrix", "Generalization Error", "Basis Shape"]', '"https://doi.org/10.1007/978-3-540-70932-9_20"', '"The recovery of 3D shape and camera motion for non-rigid scenes from single-camera video footage is a very important problem in computer vision. The low-rank shape model consists in regarding the deformations as linear combinations of basis shapes. Most algorithms for reconstructing the parameters of this model along with camera motion are based on three main steps. Given point tracks and the rank, or equivalently the number of basis shapes, they factorize a measurement matrix containing all point tracks, from which the camera motion and basis shapes are extracted and refined in a bundle adjustment manner. There are several issues that have not been addressed yet, among which, choosing the rank automatically and dealing with erroneous point tracks and missing data."'),
('"A Bayesian Approach to Alignment-Based Image Hallucination"', '"ECCV 2012"', '["Input Image", "Bayesian Approach", "Super Resolution", "Bicubic Interpolation", "SSIM Index"]', '"https://doi.org/10.1007/978-3-642-33786-4_18"', '"In most image hallucination work, a strong assumption is held that images can be aligned to a template on which the prior of high-res images is formulated and learned. Realizing that one template can hardly generalize to all images of an object such as faces due to pose and viewpoint variation as well as occlusion, we propose an example-based prior distribution via dense image correspondences. We introduce a Bayesian formulation based on an image prior that can implement different effective behaviors based on the value of a single parameter. Using faces as examples, we show that our system outperforms the prior state of art."'),
('"A Bayesian Estimation of Building Shape Using MCMC"', '"ECCV 2002"', '["Image Processing", "Artificial Intelligence", "Pattern Recognition", "Computer Vision", "Image Seq', '"https://doi.org/10.1007/3-540-47967-8_57"', '"This paper investigates the use of an implicit prior in Bayesian model-based 3D reconstruction of architecture from image sequences. In our previous work architecture is represented as a combination of basic primitives such as windows and doors etc, each with their own prior. The contribution of this work is to provide a global prior for the spatial organization of the basic primitives. However, it is difficult to explicitly formulate the prior on spatial organization. Instead we define an implicit representation that favours global regularities prevalent in architecture (e.g. windows lie in rows etc.). Specifying exact parameter values for this prior is problematic at best, however it is demonstrated that for a broad range of values the prior provides reasonable results. The validity of the prior is tested visually by generating synthetic buildings as draws from the prior simulated using MCMC. The result is a fully Bayesian method for structure from motion in the domain of architecture."'),
('"A Bayesian Framework for Multi-cue 3D Object Tracking"', '"ECCV 2004"', '["Importance Sampling", "Object Tracking", "Bayesian Framework", "Active Track", "Deformable Object"', '"https://doi.org/10.1007/978-3-540-24673-2_20"', '"This paper presents a Bayesian framework for multi-cue 3D object tracking of deformable objects. The proposed spatio-temporal object representation involves a set of distinct linear subspace models or Dynamic Point Distribution Models (DPDMs), which can deal with both continuous and discontinuous appearance changes; the representation is learned fully automatically from training data. The representation is enriched with texture information by means of intensity histograms, which are compared using the Bhattacharyya coefficient. Direct 3D measurement is furthermore provided by a stereo system."'),
('"A Benchmark Dataset to Study the Representation of Food Images"', '"ECCV 2014"', '["Food dataset", "Food recognition", "Near duplicate image retrieval", "Textons", "PRICoLBP", "SIFT"', '"https://doi.org/10.1007/978-3-319-16199-0_41"', '"It is well-known that people love food. However, an insane diet can cause problems in the general health of the people. Since health is strictly linked to the diet, advanced computer vision tools to recognize food images (e.g. acquired with mobile/wearable cameras), as well as their properties (e.g., calories), can help the diet monitoring by providing useful information to the experts (e.g., nutritionists) to assess the food intake of patients (e.g., to combat obesity). The food recognition is a challenging task since the food is intrinsically deformable and presents high variability in appearance. Image representation plays a fundamental role. To properly study the peculiarities of the image representation in the food application context, a benchmark dataset is needed. These facts motivate the work presented in this paper. In this work we introduce the UNICT-FD889 dataset. It is the first food image dataset composed by over \\\\(800\\\\) distinct plates of food which can be used as benchmark to design and compare representation models of food images. We exploit the UNICT-FD889 dataset for Near Duplicate Image Retrieval (NDIR) purposes by comparing three standard state-of-the-art image descriptors: Bag of Textons, PRICoLBP and SIFT. Results confirm that both textures and colors are fundamental properties in food representation. Moreover the experiments point out that the Bag of Textons representation obtained considering the color domain is more accurate than the other two approaches for NDIR."'),
('"A Benchmarking Campaign for the Multimodal Detection of Violent Scenes in Movies"', '"ECCV 2012"', '["Physical Violence", "Violent Event", "Psychological Violence", "Multimedia Event Detection", "Sixt', '"https://doi.org/10.1007/978-3-642-33885-4_42"', '"We present an international benchmark on the detection of violent scenes in movies, implemented as a part of the multimedia benchmarking initiative MediaEval 2011. The task consists in detecting portions of movies where physical violence is present from the automatic analysis of the video, sound and subtitle tracks. A dataset of 15 Hollywood movies was carefully annotated and divided into a development set and a test set containing 3 movies. Annotation strategies and resolution of borderline cases are discussed at length in the paper. Results from 29 runs submitted by the 6 participating sites are analyzed. The first year\\u2019s results are promising, but considering the use case, there is still a large room for improvement. The detailed analysis of the 2011 benchmark brings valuable insight for the implementation of future evaluation on violent scenes detection in movies."'),
('"A Bio-Inspired Robot with Visual Perception of Affordances"', '"ECCV 2014"', '["Affordance perception", "Robotic vision", "Cooperative neural agents", "Deep learning"]', '"https://doi.org/10.1007/978-3-319-16181-5_31"', '"We present a visual robot whose associated neural controller develops a realistic perception of affordances. The controller uses known insect brain principles; particularly the time stabilized sparse code communication between the Antennal Lobe and the Mushroom Body. The robot perceives the world through a webcam and canny border openCV routines. Self-controlled neural agents process this massive raw data and produce a time stabilized sparse version, where implicit time-space information is encoded. Preprocessed information is relayed to a population of neural agents specialized in cognitive activities and trained under self-critical isolated conditions. Isolation induces an emergent behavior which makes possible the invariant visual recognition of objects. This later capacity is assembled into cognitive strings which incorporate time-elapse learning resources activation. By using this assembled capacity during an extended learning period the robot finally achieves perception of affordances. The system has been tested in real time with real world elements."'),
('"A Bioinformatics Approach to 3D Shape Matching"', '"ECCV 2014"', '["Non-rigid shape matching", "Biological sequence alignment", "Spectral mesh processing", "Local geo', '"https://doi.org/10.1007/978-3-319-16220-1_22"', '"In this paper we exploit the effectiveness of bioinformatics tools to deal with 3D shape matching. The key idea is to transform the shape into a biological sequence and take advantage of bioinformatics tools for sequence alignment to improve shape matching. In order to extract a reliable ordering of mesh vertices we employ the spectral-based sequencing method derived from the well known Fiedler Vector. Local geometric features are then collected and quantized into a finite set of discrete values in analogy with nucleotide or aminoacid sequence. Two standard biological sequence matching strategies are employed aiming at evaluating both local and global alignment methods. Preliminary experiments are performed on standard non-rigid shape datasets by showing promising results in comparison with other methods."'),
('"A Biologically Motivated and Computationally Tractable Model of Low and Mid-Level Vision Tasks"', '"ECCV 2004"', '["Coarse Scale", "Simple Cell", "Illusory Contour", "Saliency Detection", "Feedback Term"]', '"https://doi.org/10.1007/978-3-540-24671-8_40"', '"This paper presents a biologically motivated model for low and mid-level vision tasks and its interpretation in computer vision terms. Initially we briefly present the biologically plausible model of image segmentation developed by Stephen Grossberg and his collaborators during the last two decades, that has served as the backbone of many researchers\\u2019 work. Subsequently we describe a novel version of this model with a simpler architecture but superior performance to the original system using nonlinear recurrent neural dynamics. This model integrates multi-scale contour, surface and saliency information in an efficient way, and results in smooth surfaces and thin edge maps, without posterior edge thinning or some sophisticated thresholding process. When applied to both synthetic and true images it gives satisfactory results, favorably comparable to those of classical computer vision algorithms. Analogies between the functions performed by this system and commonly used techniques for low- and mid-level computer vision tasks are presented. Further, by interpreting the network as minimizing a cost functional, links with the variational approach to computer vision are established."'),
('"A Boosted Particle Filter: Multitarget Detection and Tracking"', '"ECCV 2004"', '["Particle Filter", "Color Histogram", "Observation Model", "Proposal Distribution", "Mixture Repres', '"https://doi.org/10.1007/978-3-540-24670-1_3"', '"The problem of tracking a varying number of non-rigid objects has two major difficulties. First, the observation models and target distributions can be highly non-linear and non-Gaussian. Second, the presence of a large, varying number of objects creates complex interactions with overlap and ambiguities. To surmount these difficulties, we introduce a vision system that is capable of learning, detecting and tracking the objects of interest. The system is demonstrated in the context of tracking hockey players using video sequences. Our approach combines the strengths of two successful algorithms: mixture particle filters and Adaboost. The mixture particle filter [17] is ideally suited to multi-target tracking as it assigns a mixture component to each player. The crucial design issues in mixture particle filters are the choice of the proposal distribution and the treatment of objects leaving and entering the scene. Here, we construct the proposal distribution using a mixture model that incorporates information from the dynamic models of each player and the detection hypotheses generated by Adaboost. The learned Adaboost proposal distribution allows us to quickly detect players entering the scene, while the filtering process enables us to keep track of the individual players. The result of interleaving Adaboost with mixture particle filters is a simple, yet powerful and fully automatic multiple object tracking system."'),
('"A Boundary-Fragment-Model for Object Detection"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744047_44"', '"The objective of this work is the detection of object classes, such as airplanes or horses. Instead of using a model based on salient image fragments, we show that object class detection is also possible using only the object\\u2019s boundary. To this end, we develop a novel learning technique to extract class-discriminative boundary fragments. In addition to their shape, these \\u201ccodebook\\u201d entries also determine the object\\u2019s centroid (in the manner of Leibe et al. [19]). Boosting is used to select discriminative combinations of boundary fragments (weak detectors) to form a strong \\u201cBoundary-Fragment-Model\\u201d (BFM) detector. The generative aspect of the model is used to determine an approximate segmentation."'),
('"A Close-Form Iterative Algorithm for Depth Inferring from a Single Image"', '"ECCV 2010"', '["Image Segmentation", "Markov Random Field Model", "Scene Reconstruction", "Shape From Shade", "Sce', '"https://doi.org/10.1007/978-3-642-15555-0_53"', '"Inferring depth from a single image is a difficult task in computer vision, which needs to utilize adequate monocular cues contained in the image. Inspired by Saxena et al\\u2019s work, this paper presents a close-form iterative algorithm to process multi-scale image segmentation and depth inferring alternately, which can significantly improve segmentation and depth estimate results. First, an EM-based algorithm is applied to obtain an initial multi-scale image segmentation result. Then, the multi-scale Markov random field (MRF) model, trained by supervised learning, is used to infer both depths and the relations between depths at different image regions. Next, a graph-based region merging algorithm is applied to merge the segmentations at the larger scales by incorporating the inferred depths. At the last, the refined multi-scale image segmentations are used as input of MRF model and the depth are re-inferred. The above processes are iteratively continued until the expected results are achieved. Since there are no changes on the segmentations at the finest scale in the iterative process, it still can capture the detailed 3D structure. Meanwhile, the refined segmentations at the other scales will help obtain more global structure information in the image. The contrastive experimental results verify the validity of our method that it can infer quantitatively better depth estimations for 62.7% of 134 images downloaded from the Saxena\\u2019s database. Our method can also improve the image segmentation results in the sense of scene interpretation. Moreover, the paper extends the method to estimate the depth of the scene with fore-objects."'),
('"A Closed-Form Solution to Non-rigid Shape and Motion Recovery"', '"ECCV 2004"', '["Reconstruction Error", "Shape Base", "Dynamic Scene", "Basis Constraint", "Motion Recovery"]', '"https://doi.org/10.1007/978-3-540-24673-2_46"', '"Recovery of three dimensional (3D) shape and motion of non-static scenes from a monocular video sequence is important for applications like robot navigation and human computer interaction. If every point in the scene randomly moves, it is impossible to recover the non-rigid shapes. In practice, many non-rigid objects, e.g. the human face under various expressions, deform with certain structures. Their shapes can be regarded as a weighted combination of certain shape bases. Shape and motion recovery under such situations has attracted much interest. Previous work on this problem [6,4,13] utilized only orthonormality constraints on the camera rotations (rotation constraints). This paper proves that using only the rotation constraints results in ambiguous and invalid solutions. The ambiguity arises from the fact that the shape bases are not unique because their linear transformation is a new set of eligible bases. To eliminate the ambiguity, we propose a set of novel constraints, basis constraints, which uniquely determine the shape bases. We prove that, under the weak-perspective projection model, enforcing both the basis and the rotation constraints leads to a closed-form solution to the problem of non-rigid shape and motion recovery. The accuracy and robustness of our closed-form solution is evaluated quantitatively on synthetic data and qualitatively on real video sequences."'),
('"A Closer Look at Context: From Coxels to the Contextual Emergence of Object Saliency"', '"ECCV 2014"', '["Salient Object", "Salient Region", "Saliency Detection", "Visual Context", "Foreground Region"]', '"https://doi.org/10.1007/978-3-319-10602-1_46"', '"Visual context is used in different forms for saliency computation. While its use in saliency models for fixations prediction is often reasoned, this is less so the case for approaches that aim to compute saliency at the object level. We argue that the types of context employed by these methods lack clear justification and may in fact interfere with the purpose of capturing the saliency of whole visual objects. In this paper we discuss the constraints that different types of context impose and suggest a new interpretation of visual context that allows the emergence of saliency for more complex, abstract, or multiple visual objects. Despite shying away from an explicit attempt to capture \\u201cobjectness\\u201d (e.g., via segmentation), our results are qualitatively superior and quantitatively better than the state-of-the-art."'),
('"A Coarse-to-Fine Taxonomy of Constellations for Fast Multi-class Object Detection"', '"ECCV 2010"', '["Detection Time", "Object Class", "Query Image", "Class Number", "Constellation Model"]', '"https://doi.org/10.1007/978-3-642-15555-0_50"', '"In order for recognition systems to scale to a larger number of object categories building visual class taxonomies is important to achieve running times logarithmic in the number of classes [1,2]. In this paper we propose a novel approach for speeding up recognition times of multi-class part-based object representations. The main idea is to construct a taxonomy of constellation models cascaded from coarse-to-fine resolution and use it in recognition with an efficient search strategy. The taxonomy is built automatically in a way to minimize the number of expected computations during recognition by optimizing the cost-to-power ratio [3]. The structure and the depth of the taxonomy is not pre-determined but is inferred from the data. The approach is utilized on the hierarchy-of-parts model [4] achieving efficiency in both, the representation of the structure of objects as well as in the number of modeled object classes. We achieve speed-up even for a small number of object classes on the ETHZ and TUD dataset. On a larger scale, our approach achieves detection time that is logarithmic in the number of classes."'),
('"A Column-Pivoting Based Strategy for Monomial Ordering in Numerical Gr\\u00f6bner Basis Calculations', '"ECCV 2008"', '["Polynomial Equation", "Point Correspondence", "Linear Basis", "Basis Selection", "Basis Size"]', '"https://doi.org/10.1007/978-3-540-88693-8_10"', '"This paper presents a new fast approach to improving stability in polynomial equation solving. Gr\\u00f6bner basis techniques for equation solving have been applied successfully to several geometric computer vision problems. However, in many cases these methods are plagued by numerical problems. An interesting approach to stabilising the computations is to study basis selection for the quotient space \\u2102[x]/I. In this paper, the exact matrix computations involved in the solution procedure are clarified and using this knowledge we propose a new fast basis selection scheme based on QR-factorization with column pivoting. We also propose an adaptive scheme for truncation of the Gr\\u00f6bner basis to further improve stability. The new basis selection strategy is studied on some of the latest reported uses of Gr\\u00f6bner basis methods in computer vision and we demonstrate a fourfold increase in speed and nearly as good over-all precision as the previous SVD-based method. Moreover, we get typically get similar or better reduction of the largest errors."'),
('"A Combined PDE and Texture Synthesis Approach to Inpainting"', '"ECCV 2004"', '["Landau Equation", "Texture Synthesis", "Geometry Part", "Image Decomposition", "Geometry Image"]', '"https://doi.org/10.1007/978-3-540-24671-8_17"', '"While there is a vast amount of literature considering PDE based inpainting and inpainting by texture synthesis, only a few publications are concerned with combination of both approaches. We present a novel algorithm which combines both approaches and treats each distinct region of the image separately. Thus we are naturally lead to include a segmentation pass as a new feature. This way the correct choice of texture samples for the texture synthesis is ensured. We propose a novel concept of \\u201clocal texture synthesis\\u201d which gives satisfactory results even for large domains in a complex environment."'),
('"A Comparative Analysis of RANSAC Techniques Leading to Adaptive Real-Time Random Sample Consensus"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_37"', '"The Random Sample Consensus (RANSAC) algorithm is a popular tool for robust estimation problems in computer vision, primarily due to its ability to tolerate a tremendous fraction of outliers. There have been a number of recent efforts that aim to increase the efficiency of the standard RANSAC algorithm. Relatively fewer efforts, however, have been directed towards formulating RANSAC in a manner that is suitable for real-time implementation. The contributions of this work are two-fold: First, we provide a comparative analysis of the state-of-the-art RANSAC algorithms and categorize the various approaches. Second, we develop a powerful new framework for real-time robust estimation. The technique we develop is capable of efficiently adapting to the constraints presented by a fixed time budget, while at the same time providing accurate estimation over a wide range of inlier ratios. The method shows significant improvements in accuracy and speed over existing techniques."'),
('"A Comparative Study of Energy Minimization Methods for Markov Random Fields"', '"ECCV 2006"', '["Energy Function", "Stereo Match", "IEEE Trans Pattern Anal", "Markov Random", "Energy Minimization', '"https://doi.org/10.1007/11744047_2"', '"One of the most exciting advances in early vision has been the development of efficient energy minimization algorithms. Many early vision tasks require labeling each pixel with some quantity such as depth or texture. While many such problems can be elegantly expressed in the language of Markov Random Fields (MRF\\u2019s), the resulting energy minimization problems were widely viewed as intractable. Recently, algorithms such as graph cuts and loopy belief propagation (LBP) have proven to be very powerful: for example, such methods form the basis for almost all the top-performing stereo methods. Unfortunately, most papers define their own energy function, which is minimized with a specific algorithm of their choice. As a result, the tradeoffs among different energy minimization algorithms are not well understood. In this paper we describe a set of energy minimization benchmarks, which we use to compare the solution quality and running time of several common energy minimization algorithms. We investigate three promising recent methods\\u2014graph cuts, LBP, and tree-reweighted message passing\\u2014as well as the well-known older iterated conditional modes (ICM) algorithm. Our benchmark problems are drawn from published energy functions used for stereo, image stitching and interactive segmentation. We also provide a general-purpose software interface that allows vision researchers to easily switch between optimization methods with minimal overhead. We expect that the availability of our benchmarks and interface will make it significantly easier for vision researchers to adopt the best method for their specific problems. Benchmarks, code, results and images are available at http://vision.middlebury.edu/MRF."'),
('"A Comparison of Search Strategies for Geometric Branch and Bound Algorithms"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47977-5_55"', '"Over the last decade, a number of methods for geometric matching based on a branch-and-bound approach have been proposed. Such algorithms work by recursively subdividing transformation space and bounding the quality of match over each subdivision. No direct comparison of the major implementation strategies has been made so far, so it has been unclear what the relative performance of the different approaches is. This paper examines experimentally the relative performance of different implementation choices in the implementation of branch-and-bound algorithms for geometric matching: alternatives for the computation of upper bounds across a collection of features, and alternatives the order in which search nodes are expanded. Two major approaches to computing the bounds have been proposed: the matchlist based approach, and approaches based on point location data structures. A second issue that is addressed in the paper is the question of search strategy; branch-and-bound algorithms traditionally use a \\u201cbest-first\\u201d search strategy, but a \\u201cdepth-first\\u201d strategy is a plausible alternative. These alternative implementations are compared on an easily reproducible and commonly used class of test problems, a statistical model of feature distributions and matching within the COIL-20 image database. The experimental results show that matchlist based approaches outperform point location based approaches on common tasks. The paper also shows that a depth-first approach to matching results in a 50-200 fold reduction in memory usage with only a small increase in running time. Since matchlist-based approaches are significantly easier to implement and can easily cope with a much wider variety of feature types and error bounds that point location based approaches, they should probably the primary implementation strategy for branch-and-bound based methods for geometric matching."'),
('"A Comparison of the Statistical Properties of IQA Databases Relative to a Set of Newly Captured Hig', '"ECCV 2012"', '["Image Database", "Natural Image", "Versus Plane", "Image Quality Assessment", "Subjective Image Qu', '"https://doi.org/10.1007/978-3-642-33765-9_57"', '"A broad range of image processing applications require image databases during development and testing. Whilst some image databases have been assembled with specific applications in mind, others are intended for more general use, with image content that is purposefully not application-specific. General-purpose image databases are in frequent use in the development of new compression algorithms, including in the evaluation of the efficacy of lossy compression techniques via statistical and human (perceptual) image quality assessment methods. The question of how the images featuring in standard image databases are selected is important, but is rarely quantitatively justified. In this article, we describe the compilation of a new image database of high-definition color images. We present statistical analyzes both of the images that feature in the most widely used extant databases, and the new database that we have compiled, in order to evaluate how broad a range of the statistics measured each database spans."'),
('"A Complete Confidence Framework for Optical Flow"', '"ECCV 2012"', '["Optical flow", "confidence measures", "sparsification plots", "error prediction plots"]', '"https://doi.org/10.1007/978-3-642-33868-7_13"', '"Assessing the performance of optical flow in the absence of ground truth is of prime importance for a correct interpretation and application. Thus, in recent years, the interest in developing confidence measures has increased. However, by its complexity, assessing the capability of such measures for detecting areas of poor performance of optical flow is still unsolved."'),
('"A Constrained Semi-supervised Learning Approach to Data Association"', '"ECCV 2004"', '["Posterior Distribution", "Data Association", "Markov Chain Monte Carlo Algorithm", "Statistical Ma', '"https://doi.org/10.1007/978-3-540-24672-5_1"', '"Data association (obtaining correspondences) is a ubiquitous problem in computer vision. It appears when matching image features across multiple images, matching image features to object recognition models and matching image features to semantic concepts. In this paper, we show how a wide class of data association tasks arising in computer vision can be interpreted as a constrained semi-supervised learning problem. This interpretation opens up room for the development of new, more efficient data association methods. In particular, it leads to the formulation of a new principled probabilistic model for constrained semi-supervised learning that accounts for uncertainty in the parameters and missing data. By adopting an ingenious data augmentation strategy, it becomes possible to develop an efficient MCMC algorithm where the high-dimensional variables in the model can be sampled efficiently and directly from their posterior distributions. We demonstrate the new model and algorithm on synthetic data and the complex problem of matching image features to words in the image captions."'),
('"A Continuous Max-Flow Approach to Potts Model"', '"ECCV 2010"', '["Potts Model", "Dual Model", "Label Function", "Variational Perspective", "Simplex Constraint"]', '"https://doi.org/10.1007/978-3-642-15567-3_28"', '"We address the continuous problem of assigning multiple (unordered) labels with the minimum perimeter. The corresponding discrete Potts model is typically addressed with a-expansion which can generate metrication artifacts. Existing convex continuous formulations of the Potts model use TV-based functionals directly encoding perimeter costs. Such formulations are analogous to \\u2019min-cut\\u2019 problems on graphs. We propose a novel convex formulation with a continous \\u2019max-flow\\u2019 functional. This approach is dual to the standard TV-based formulations of the Potts model. Our continous max-flow approach has significant numerical advantages; it avoids extra computational load in enforcing the simplex constraints and naturally allows parallel computations over different labels. Numerical experiments show competitive performance in terms of quality and significantly reduced number of iterations compared to the previous state of the art convex methods for the continuous Potts model."'),
('"A Contour Completion Model for Augmenting Surface Reconstructions"', '"ECCV 2014"', '["3D Reconstruction", "Scene Completion", "Surface Reconstruction", "Contour Completion"]', '"https://doi.org/10.1007/978-3-319-10578-9_32"', '"The availability of commodity depth sensors such as Kinect has enabled development of methods which can densely reconstruct arbitrary scenes. While the results of these methods are accurate and visually appealing, they are quite often incomplete. This is either due to the fact that only part of the space was visible during the data capture process or due to the surfaces being occluded by other objects in the scene. In this paper, we address the problem of completing and refining such reconstructions. We propose a method for scene completion that can infer the layout of the complete room and the full extent of partially occluded objects. We propose a new probabilistic model, Contour Completion Random Fields, that allows us to complete the boundaries of occluded surfaces. We evaluate our method on synthetic and real world reconstructions of 3D scenes and show that it quantitatively and qualitatively outperforms standard methods. We created a large dataset of partial and complete reconstructions which we will make available to the community as a benchmark for the scene completion task. Finally, we demonstrate the practical utility of our algorithm via an augmented-reality application where objects interact with the completed reconstructions inferred by our method."'),
('"A Contrast Enhancement Framework with JPEG Artifacts Suppression"', '"ECCV 2014"', '["Contrast Enhancement", "Dehazing", "JPEG Artifacts Removal", "Deblocking"]', '"https://doi.org/10.1007/978-3-319-10605-2_12"', '"Contrast enhancement is used for many algorithms in computer vision. It is applied either explicitly, such as histogram equalization and tone-curve manipulation, or implicitly via methods that deal with degradation from physical phenomena such as haze, fog or underwater imaging. While contrast enhancement boosts the image appearance, it can unintentionally boost unsightly image artifacts, especially artifacts from JPEG compression. Most JPEG implementations optimize the compression in a scene-dependent manner such that low-contrast images exhibit few perceivable artifacts even for relatively high-compression factors. After contrast enhancement, however, these artifacts become significantly visible. Although there are numerous approaches targeting JPEG artifact reduction, these are generic in nature and are applied either as pre- or post-processing steps. When applied as pre-processing, existing methods tend to over smooth the image. When applied as post-processing, these are often ineffective at removing the boosted artifacts. To resolve this problem, we propose a framework that suppresses compression artifacts as an integral part of the contrast enhancement procedure. We show that this approach can produce compelling results superior to those obtained by existing JPEG artifacts removal methods for several types of contrast enhancement problems."'),
('"A Convergent Incoherent Dictionary Learning Algorithm for Sparse Coding"', '"ECCV 2014"', '["mutual coherence", "dictionary learning", "sparse coding"]', '"https://doi.org/10.1007/978-3-319-10599-4_20"', '"Recently, sparse coding has been widely used in many applications ranging from image recovery to pattern recognition. The low mutual coherence of a dictionary is an important property that ensures the optimality of the sparse code generated from this dictionary. Indeed, most existing dictionary learning methods for sparse coding either implicitly or explicitly tried to learn an incoherent dictionary, which requires solving a very challenging non-convex optimization problem. In this paper, we proposed a hybrid alternating proximal algorithm for incoherent dictionary learning, and established its global convergence property. Such a convergent incoherent dictionary learning method is not only of theoretical interest, but also might benefit many sparse coding based applications."'),
('"A Convex Discrete-Continuous Approach for Markov Random Fields"', '"ECCV 2012"', '["Unary Potential", "Markov Random", "Label Problem", "Match Cost", "Pairwise Potential"]', '"https://doi.org/10.1007/978-3-642-33783-3_28"', '"We propose an extension of the well-known LP relaxation for Markov random fields to explicitly allow continuous label spaces. Unlike conventional continuous formulations of labelling problems which assume that the unary and pairwise potentials are convex, our formulation allows them to be general piecewise convex functions with continuous domains. Furthermore, we present the extension of the widely used efficient scheme for handling L 1 smoothness priors over discrete ordered label sets to continuous label spaces. We provide a theoretical analysis of the proposed model, and empirically demonstrate that labelling problems with huge or continuous label spaces can benefit from our discrete-continuous representation."'),
('"A Convex Formulation of Continuous Multi-label Problems"', '"ECCV 2008"', '["Variational Problem", "Continuous Formulation", "Discrete Approach", "Metrication Error", "Proxima', '"https://doi.org/10.1007/978-3-540-88690-7_59"', '"We propose a spatially continuous formulation of Ishikawa\\u2019s discrete multi-label problem. We show that the resulting non-convex variational problem can be reformulated as a convex variational problem via embedding in a higher dimensional space. This variational problem can be interpreted as a minimal surface problem in an anisotropic Riemannian space. In several stereo experiments we show that the proposed continuous formulation is superior to its discrete counterpart in terms of computing time, memory efficiency and metrication errors."'),
('"A Convolutional Treelets Binary Feature Approach to Fast Keypoint Recognition"', '"ECCV 2012"', '["Root Mean Square Error", "Recognition Rate", "Hash Table", "Image Patch", "Model Image"]', '"https://doi.org/10.1007/978-3-642-33715-4_27"', '"Fast keypoint recognition is essential to many vision tasks. In contrast to the classification-based approaches [1,2], we directly formulate the keypoint recognition as an image patch retrieval problem, which enjoys the merit of finding the matched keypoint and its pose simultaneously. A novel convolutional treelets approach is proposed to effectively extract the binary features from the patches. A corresponding sub-signature-based locality sensitive hashing scheme is employed for the fast approximate nearest neighbor search in patch retrieval. Experiments on both synthetic data and real-world images have shown that our method performs better than state-of-the-art descriptor-based and classification-based approaches."'),
('"A Correlation-Based Approach to Robust Point Set Registration"', '"ECCV 2004"', '["Cost Function", "Registration Method", "Iterative Close Point", "Registration Algorithm", "Iterati', '"https://doi.org/10.1007/978-3-540-24672-5_44"', '"Correlation is a very effective way to align intensity images. We extend the correlation technique to point set registration using a method we call kernel correlation. Kernel correlation is an affinity measure, and it is also a function of the point set entropy. We define the point set registration problem as finding the maximum kernel correlation configuration of the the two point sets to be registered. The new registration method has intuitive interpretations, simple to implement algorithm and easy to prove convergence property. Our method shows favorable performance when compared with the iterative closest point (ICP) and EM-ICP methods."'),
('"A Crop/Weed Field Image Dataset for the Evaluation of Computer Vision Based Precision Agriculture T', '"ECCV 2014"', '["Computer vision", "Phenotyping", "Dataset", "Precision agriculture", "Classification", "Bonirob fi', '"https://doi.org/10.1007/978-3-319-16220-1_8"', '"In this paper we propose a benchmark dataset for crop/weed discrimination, single plant phenotyping and other open computer vision tasks in precision agriculture. The dataset comprises 60 images with annotations and is available online (http://github.com/cwfid). All images were acquired with the autonomous field robot Bonirob in an organic carrot farm while the carrot plants were in early true leaf growth stage. Intra- and inter-row weeds were present, weed and crop were approximately of the same size and grew close together. For every dataset image we supply a ground truth vegetation segmentation mask and manual annotation of the plant type (crop vs. weed). We provide initial results for the phenotyping problem of crop/weed classification and propose evaluation methods to allow comparison of different approaches. By opening this dataset to the community we want to stimulate research in this area where the current lack of public datasets is one of the barriers for progress."'),
('"A Data-Driven Approach for Event Prediction"', '"ECCV 2010"', '["Video Clip", "Anomaly Detection", "Query Image", "Event Prediction", "Unusual Event"]', '"https://doi.org/10.1007/978-3-642-15552-9_51"', '"When given a single static picture, humans can not only interpret the instantaneous content captured by the image, but also they are able to infer the chain of dynamic events that are likely to happen in the near future. Similarly, when a human observes a short video, it is easy to decide if the event taking place in the video is normal or unexpected, even if the video depicts a an unfamiliar place for the viewer. This is in contrast with work in surveillance and outlier event detection, where the models rely on thousands of hours of video recorded at a single place in order to identify what constitutes an unusual event. In this work we present a simple method to identify videos with unusual events in a large collection of short video clips. The algorithm is inspired by recent approaches in computer vision that rely on large databases. In this work we show how, relying on large collections of videos, we can retrieve other videos similar to the query to build a simple model of the distribution of expected motions for the query. Consequently, the model can evaluate how unusual is the video as well as make event predictions. We show how a very simple retrieval model is able to provide reliable results."'),
('"A Dictionary Learning Approach for Classification: Separating the Particularity and the Commonality', '"ECCV 2012"', '["Dictionary Learning", "Classification", "Commonality", "Particularity"]', '"https://doi.org/10.1007/978-3-642-33718-5_14"', '"Empirically, we find that, despite the class-specific features owned by the objects appearing in the images, the objects from different categories usually share some common patterns, which do not contribute to the discrimination of them. Concentrating on this observation and under the general dictionary learning (DL) framework, we propose a novel method to explicitly learn a common pattern pool (the commonality) and class-specific dictionaries (the particularity) for classification. We call our method DL-COPAR, which can learn the most compact and most discriminative class-specific dictionaries used for classification. The proposed DL-COPAR is extensively evaluated both on synthetic data and on benchmark image databases in comparison with existing DL-based classification methods. The experimental results demonstrate that DL-COPAR achieves very promising performances in various applications, such as face recognition, handwritten digit recognition, scene classification and object recognition."'),
('"A Discrete Chain Graph Model for 3d+t Cell Tracking with High Misdetection Robustness"', '"ECCV 2012"', '["chain graph", "graphical model", "cell tracking"]', '"https://doi.org/10.1007/978-3-642-33712-3_11"', '"Tracking by assignment is well suited for tracking a varying number of divisible cells, but suffers from false positive detections. We reformulate tracking by assignment as a chain graph\\u2013a mixed directed-undirected probabilistic graphical model\\u2013and obtain a tracking simultaneously over all time steps from the maximum a-posteriori configuration. The model is evaluated on two challenging four-dimensional data sets from developmental biology. Compared to previous work, we obtain improved tracks due to an increased robustness against false positive detections and the incorporation of temporal domain knowledge."'),
('"A Discriminative Data-Dependent Mixture-Model Approach for Multiple Instance Learning in Image Clas', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33765-9_47"', '"Multiple Instance Learning (MIL) has been widely used in various applications including image classification. However, existing MIL methods do not explicitly address the multi-target problem where the distributions of positive instances are likely to be multi-modal. This strongly limits the performance of multiple instance learning in many real world applications. To address this problem, this paper proposes a novel discriminative data-dependent mixture-model method for multiple instance learning (MM-MIL) approach in image classification. The new method explicitly handles the multi-target problem by introducing a data-dependent mixture model, which allows positive instances to come from different clusters in a flexible manner. Furthermore, the kernelized representation of the proposed model allows effective and efficient learning in high dimensional feature space. An extensive set of experimental results demonstrate that the proposed new MM-MIL approach substantially outperforms several state-of-art MIL algorithms on benchmark datasets."'),
('"A Discriminative Latent Model of Object Classes and Attributes"', '"ECCV 2010"', '["Training Data", "Object Class", "Object Category", "Visual Attribute", "Class Accuracy"]', '"https://doi.org/10.1007/978-3-642-15555-0_12"', '"We present a discriminatively trained model for joint modelling of object class labels (e.g. \\u201cperson\\u201d, \\u201cdog\\u201d, \\u201cchair\\u201d, etc.) and their visual attributes (e.g. \\u201chas head\\u201d, \\u201cfurry\\u201d, \\u201cmetal\\u201d, etc.). We treat attributes of an object as latent variables in our model and capture the correlations among attributes using an undirected graphical model built from training data. The advantage of our model is that it allows us to infer object class labels using the information of both the test image itself and its (latent) attributes. Our model unifies object class prediction and attribute prediction in a principled framework. It is also flexible enough to deal with different performance measurements. Our experimental results provide quantitative evidence that attributes can improve object naming."'),
('"A Discriminative Model with Multiple Temporal Scales for Action Prediction"', '"ECCV 2014"', '["Action Prediction", "Structured SVM", "Sequential Data"]', '"https://doi.org/10.1007/978-3-319-10602-1_39"', '"The speed with which intelligent systems can react to an action depends on how soon it can be recognized. The ability to recognize ongoing actions is critical in many applications, for example, spotting criminal activity. It is challenging, since decisions have to be made based on partial videos of temporally incomplete action executions. In this paper, we propose a novel discriminative multi-scale model for predicting the action class from a partially observed video. The proposed model captures temporal dynamics of human actions by explicitly considering all the history of observed features as well as features in smaller temporal segments. We develop a new learning formulation, which elegantly captures the temporal evolution over time, and enforces the label consistency between segments and corresponding partial videos. Experimental results on two public datasets show that the proposed approach outperforms state-of-the-art action prediction methods."'),
('"A Dual Theory of Inverse and Forward Light Transport"', '"ECCV 2010"', '["Dual Theory", "Global Illumination", "Light Transport", "High Dynamic Range Image", "Indirect Illu', '"https://doi.org/10.1007/978-3-642-15552-9_22"', '"Inverse light transport seeks to undo global illumination effects, such as interreflections, that pervade images of most scenes. This paper presents the theoretical and computational foundations for inverse light transport as a dual of forward rendering. Mathematically, this duality is established through the existence of underlying Neumann series expansions. Physically, we show that each term of our inverse series cancels an interreflection bounce, just as the forward series adds them. While the convergence properties of the forward series are well-known, we show that the oscillatory convergence of the inverse series leads to more interesting conditions on material reflectance. Conceptually, the inverse problem requires the inversion of a large transport matrix, which is impractical for realistic resolutions. A natural consequence of our theoretical framework is a suite of fast computational algorithms for light transport inversion \\u2013 analogous to finite element radiosity, Monte Carlo and wavelet-based methods in forward rendering \\u2013 that rely at most on matrix-vector multiplications. We demonstrate two practical applications, namely, separation of individual bounces of the light transport and fast projector radiometric compensation to display images free of global illumination artifacts in real-world environments."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A Dynamic Conditional Random Field Model for Joint Labeling of Object and Scene Classes"', '"ECCV 2008"', '["Input Image", "Object Detection", "Object Class", "Conditional Random Field", "Dynamic Scene"]', '"https://doi.org/10.1007/978-3-540-88693-8_54"', '"Object detection and pixel-wise scene labeling have both been active research areas in recent years and impressive results have been reported for both tasks separately. The integration of these different types of approaches should boost performance for both tasks as object detection can profit from powerful scene labeling and also pixel-wise scene labeling can profit from powerful object detection. Consequently, first approaches have been proposed that aim to integrate both object detection and scene labeling in one framework. This paper proposes a novel approach based on conditional random field (CRF) models that extends existing work by 1) formulating the integration as a joint labeling problem of object and scene classes and 2) by systematically integrating dynamic information for the object detection task as well as for the scene labeling task. As a result, the approach is applicable to highly dynamic scenes including both fast camera and object movements. Experiments show the applicability of the novel approach to challenging real-world video sequences and systematically analyze the contribution of different system components to the overall performance."'),
('"A Dynamic Programming Approach to Reconstructing Building Interiors"', '"ECCV 2010"', '["Surface Orientation", "Wall Segment", "Dynamic Program Approach", "Orientation Estimate", "Dominan', '"https://doi.org/10.1007/978-3-642-15555-0_29"', '"A number of recent papers have investigated reconstruction under Manhattan world assumption, in which surfaces in the world are assumed to be aligned with one of three dominant directions [1,2,3,4]. In this paper we present a dynamic programming solution to the reconstruction problem for \\u201cindoor\\u201d Manhattan worlds (a sub\\u2013class of Manhattan worlds). Our algorithm deterministically finds the global optimum and exhibits computational complexity linear in both model complexity and image size. This is an important improvement over previous methods that were either approximate [3] or exponential in model complexity [4]. We present results for a new dataset containing several hundred manually annotated images, which are released in conjunction with this paper."'),
('"A Fast Algorithm for Creating a Compact and Discriminative Visual Codebook"', '"ECCV 2008"', '["Discriminative Power", "Visual Word", "Training Image", "Fast Algorithm", "Object Class"]', '"https://doi.org/10.1007/978-3-540-88693-8_53"', '"In patch-based object recognition, using a compact visual codebook can boost computational efficiency and reduce memory cost. Nevertheless, compared with a large-sized codebook, it also risks the loss of discriminative power. Moreover, creating a compact visual codebook can be very time-consuming, especially when the number of initial visual words is large. In this paper, to minimize its loss of discriminative power, we propose an approach to build a compact visual codebook by maximally preserving the separability of the object classes. Furthermore, a fast algorithm is designed to accomplish this task effortlessly, which can hierarchically merge 10,000 visual words down to 2 in ninety seconds. Experimental study shows that the compact visual codebook created in this way can achieve excellent classification performance even after a considerable reduction in size."'),
('"A Fast and Flexible Computer Vision System for Implanted Visual Prostheses"', '"ECCV 2014"', '["Visual prosthesis", "Bionic eye", "Cortical implant", "Simulated prosthetic vision", "Wearable com', '"https://doi.org/10.1007/978-3-319-16199-0_48"', '"Implanted visual prostheses generate visual percepts by electrically stimulating the human visual pathway using an array of electrodes. The resulting bionic vision consists of a spatial-temporal pattern of bright dots called phosphenes. This patient-specific phosphene pattern has low resolution, limited dynamic range and is spatially irregular. This paper presents a computer vision system designed to deal with these limitations, especially spatial irregularity. The system uses a new mapping called the Camera Map to decouple the flexible spatial layout of image processing from the inflexible layout of phosphenes experienced by a patient. Detailed simulations of a cortical prosthesis currently in preclinical testing were performed to create phosphene patterns for testing. The system was tested on a wearable prototype of the cortical prosthesis. Despite having limited computational resources, the system operated in real time, taking only a few milliseconds to perform image processing and visualisations of simulated prosthetic vision."'),
('"A Fast and Simple Algorithm for Producing Candidate Regions"', '"ECCV 2014"', '["Hierarchical grouping", "segments selection", "candidate regions"]', '"https://doi.org/10.1007/978-3-319-10578-9_35"', '"This paper addresses the task of producing candidate regions for detecting objects (e.g., car, cat) and background regions (e.g., sky, water). We describe a simple and rapid algorithm which generates a set of candidate regions \\\\(\\\\mathcal{C_R}\\\\) by combining up to three \\u201dselected-segments\\u201d. These are obtained by a hierarchical merging algorithm which seeks to identify segments corresponding to roughly homogeneous regions, followed by a selection stage which removes most of the segments, yielding a small subset of selected-segments \\\\(\\\\mathcal{S}\\\\). The hierarchical merging makes a novel use of the PageRank algorithm. The selection stage also uses a new criterion based on entropy gain with non-parametric estimation of the segments\\u2019 entropy. We evaluate on a new labeling of the Pascal VOC 2010 set where all pixels are labeled with one of 57 class labels. We show that most of the 57 objects and background regions can be largely covered by three of the selected-segments. We present a detailed per-object comparison on the task of proposing candidate regions with several state-of-the-art methods. Our performance is comparable to the best performing method in terms of coverage but is simpler and faster, and needs to output half the number of candidate regions, which is critical for a subsequent stage (e.g, classification)."'),
('"A Fast Approximation of the Bilateral Filter Using a Signal Processing Approach"', '"ECCV 2006"', '["Bilateral Filter", "Fast Approximation", "Homogeneous Intensity", "Nonlinear Operation", "Adaptive', '"https://doi.org/10.1007/11744085_44"', '"The bilateral filter is a nonlinear filter that smoothes a signal while preserving strong edges. It has demonstrated great effectiveness for a variety of problems in computer vision and computer graphics, and a fast version has been proposed. Unfortunately, little is known about the accuracy of such acceleration. In this paper, we propose a new signal-processing analysis of the bilateral filter, which complements the recent studies that analyzed it as a PDE or as a robust statistics estimator. Importantly, this signal-processing perspective allows us to develop a novel bilateral filtering acceleration using a downsampling in space and intensity. This affords a principled expression of the accuracy in terms of bandwidth and sampling. The key to our analysis is to express the filter in a higher-dimensional space where the signal intensity is added to the original domain dimensions. The bilateral filter can then be expressed as simple linear convolutions in this augmented space followed by two simple nonlinearities. This allows us to derive simple criteria for downsampling the key operations and to achieve important acceleration of the bilateral filter. We show that, for the same running time, our method is significantly more accurate than previous acceleration techniques."'),
('"A Fast Dual Method for HIK SVM Learning"', '"ECCV 2010"', '["Feature Vector", "Feature Space", "Training Time", "Coordinate Descent", "Stochastic Gradient Desc', '"https://doi.org/10.1007/978-3-642-15552-9_40"', '"Histograms are used in almost every aspect of computer vision, from visual descriptors to image representations. Histogram Intersection Kernel (HIK) and SVM classifiers are shown to be very effective in dealing with histograms. This paper presents three contributions concerning HIK SVM classification. First, instead of limited to integer histograms, we present a proof that HIK is a positive definite kernel for non-negative real-valued feature vectors. This proof reveals some interesting properties of the kernel. Second, we propose ICD, a deterministic and highly scalable dual space HIK SVM solver. ICD is faster than and has similar accuracies with general purpose SVM solvers and two recently proposed stochastic fast HIK SVM training methods. Third, we empirically show that ICD is not sensitive to the C parameter in SVM. ICD achieves high accuracies using its default parameters in many datasets. This is a very attractive property because many vision problems are too large to choose SVM parameters using cross-validation."'),
('"A Fast Illumination and Deformation Insensitive Image Comparison Algorithm Using Wavelet-Based Geod', '"ECCV 2012"', '["Face Recognition", "Gradient Direction", "Image Gradient", "Wavelet Domain", "Geodesic Curve"]', '"https://doi.org/10.1007/978-3-642-33765-9_6"', '"We present a fast image comparison algorithm for handling variations in illumination and moderate amounts of deformation using an efficient geodesic framework. As the geodesic is the shortest path between two images on a manifold, it is a natural choice to use the length of the geodesic to determine the image similarity. Distances on the manifold are defined by a metric that is insensitive to changes in scene lighting. This metric is described in the wavelet domain where it is able to handle moderate amounts of deformation, and can be calculated extremely fast (less than 3ms per image comparison). We demonstrate the similarity between our method and the illumination insensitivity achieved by the Gradient Direction. Strong results are presented on the AR Face Database."'),
('"A Fast Line Segment Based Dense Stereo Algorithm Using Tree Dynamic Programming"', '"ECCV 2006"', '["Line Segment", "Dynamic Programming", "Stereo Match", "Global Method", "Neighboring System"]', '"https://doi.org/10.1007/11744078_16"', '"Many traditional stereo correspondence methods emphasized on utilizing epipolar constraint and ignored the information embedded in inter-epipolar lines. Actually some researchers have already proposed several grid-based algorithms for fully utilizing information embodied in both intra- and inter-epipolar lines. Though their performances are greatly improved, they are very time-consuming. The new graph-cut and believe-propagation methods have made the grid-based algorithms more efficient, but time-consuming still remains a hard problem for many applications. Recently, a tree dynamic programming algorithm is proposed. Though the computation speed is much higher than that of grid-based methods, the performance is degraded apparently. We think that the problem stems from the pixel-based tree construction. Many edges in the original grid are forced to be cut out, and much information embedded in these edges is thus lost. In this paper, a novel line segment based stereo correspondence algorithm using tree dynamic programming (LSTDP) is presented. Each epipolar line of the reference image is segmented into segments first, and a tree is then constructed with these line segments as its vertexes. The tree dynamic programming is adopted to compute the correspondence of each line segment. By using line segments as the vertexes instead of pixels, the connection between neighboring pixels within the same region can be reserved as completely as possible. Experimental results show that our algorithm can obtain comparable performance with state-of-the-art algorithms but is much more time-efficient."'),
('"A Fast Method for Tracking People with Multiple Cameras"', '"ECCV 2010"', '["Ground Plane", "Object Position", "Integral Image", "Multiple Camera", "Epipolar Geometry"]', '"https://doi.org/10.1007/978-3-642-35749-7_10"', '"We propose a multi-camera method to track several persons using constraints from the epipolar and projective geometries. The method is very accurate, fast, and simple. We first compute accumulator images for each time frame that shows the probability of object positions on the ground. We developed a voting based method that allows employment of the integral images to make the accumulator computation very fast. Next, we perform two-pass 3D tracking on the volume generated by stacking these accumulator images. Our main contributions are the fast computation of the accumulator images and application of fast 3D tracking methods like the Kalman Smoother instead of the computationally expensive methods like the Viterbi algorithm."'),
('"A Fast Radial Symmetry Transform for Detecting Points of Interest"', '"ECCV 2002"', '["Face Image", "Detect Point", "Symmetry Transform", "Radial Symmetry", "Gradient Orientation"]', '"https://doi.org/10.1007/3-540-47969-4_24"', '"A new feature detection technique is presented that utilises local radial symmetry to identify regions of interest within a scene. This transform is significantly faster than existing techniques using radial symmetry and offers the possibility of real-time implementation on a standard processor. The new transform is shown to perform well on a wide variety of images and its performance is tested against leading techniques from the literature. Both as a facial feature detector and as a generic region of interest detector the new transform is seen to offer equal or superior performance to contemporary techniques whilst requiring drastically less computational effort."'),
('"A Feature-Based Approach for Determining Dense Long Range Correspondences"', '"ECCV 2004"', '["Image Pair", "Thin Plate Spline", "Original Frame", "Motion Segmentation", "Motion Layer"]', '"https://doi.org/10.1007/978-3-540-24672-5_14"', '"Planar motion models can provide gross motion estimation and good segmentation for image pairs with large inter-frame disparity. However, as the disparity becomes larger, the resulting dense correspondences will become increasingly inaccurate for everything but purely planar objects. Flexible motion models, on the other hand, tend to overfit and thus make partitioning difficult. For this reason, to achieve dense optical flow for image sequences with large inter-frame disparity, we propose a two stage process in which a planar model is used to get an approximation for the segmentation and the gross motion, and then a spline is used to refine the fit. We present experimental results for dense optical flow estimation on image pairs with large inter-frame disparity that are beyond the scope of existing approaches."'),
('"A Fisheye Camera System for Polarisation Detection on UAVs"', '"ECCV 2012"', '["Camera Calibration", "Polariser Orientation", "Polarisation Detection", "Omnidirectional Camera", ', '"https://doi.org/10.1007/978-3-642-33868-7_43"', '"We present a light-weight polarisation sensor that consists of four synchronised cameras equipped with differently oriented polarisers and fisheye lenses allowing us to image the whole sky hemisphere. Due to its low weight and compact size it is well-suited as a biomimetic sensor on-board a UAV. We describe efficient methods for reconstruction of the full-sky polarisation pattern and estimation of sun position. In contrast to state-of-the art polarisation systems for UAVs that estimate sun azimuth only, our approach can determine sun elevation as well, even in the presence of clouds and for significant pitch and roll angles of the UAV. The calibration and registration of the four fisheye cameras is achieved by extending an existing omni-directional calibration toolbox to multi-camera calibration. We present examples of full-sky reconstruction of the polarisation pattern as well as an analysis of the error in the sun position estimate. In addition, we performed a preliminary test on-board a quadcopter."'),
('"A Fluid Motion Estimator for Schlieren Image Velocimetry"', '"ECCV 2006"', '["Particle Image Velocimetry", "Flow Visualization", "Regularization Term", "Data Term", "Schlieren ', '"https://doi.org/10.1007/11744023_16"', '"In this paper, we address the problem of estimating the motion of fluid flows that are visualized through a Schlieren system. Such a system is well known in fluid mechanics as it enables the visualization of unseeded flows. As the resulting images exhibit very low photometric contrasts, classical motion estimation methods based on the brightness consistency assumption (correlation-based approaches, optical flow methods) are completely inefficient. This work aims at proposing a sound energy based estimator dedicated to these particular images. The energy function to be minimized is composed of (a) a novel data term describing the fact that the observed luminance is linked to the gradient of the fluid density and (b) a specific div curl regularization term. The relevance of our estimator is demonstrated on real-world sequences."'),
('"A Fourier Theory for Cast Shadows"', '"ECCV 2004"', '["Spherical Harmonic", "Fourier Theory", "Eigenvalue Spectrum", "Cast Shadow", "Illumination Directi', '"https://doi.org/10.1007/978-3-540-24670-1_12"', '"Cast shadows can be significant in many computer vision applications such as lighting-insensitive recognition and surface reconstruction. However, most algorithms neglect them, primarily because they involve non-local interactions in non-convex regions, making formal analysis difficult. While general cast shadowing situations can be arbitrarily complex, many real instances map closely to canonical configurations like a wall, a V-groove type structure, or a pitted surface. In particular, we experiment on 3D textures like moss, gravel and a kitchen sponge, whose surfaces include canonical cast shadowing situations like V-grooves. This paper shows theoretically that many shadowing configurations can be mathematically analyzed using convolutions and Fourier basis functions. Our analysis exposes the mathematical convolution structure of cast shadows, and shows strong connections to recently developed signal-processing frameworks for reflection and illumination. An analytic convolution formula is derived for a 2D V-groove, which is shown to correspond closely to many common shadowing situations, especially in 3D textures. Numerical simulation is used to extend these results to general 3D textures. These results also provide evidence that a common set of illumination basis functions may be appropriate for representing lighting variability due to cast shadows in many 3D textures. We derive a new analytic basis suited for 3D textures to represent illumination on the hemisphere, with some advantages over commonly used Zernike polynomials and spherical harmonics. New experiments on analyzing the variability in appearance of real 3D textures with illumination motivate and validate our theoretical analysis. Empirical results show that illumination eigenfunctions often correspond closely to Fourier bases, while the eigenvalues drop off significantly slower than those for irradiance on a Lambertian curved surface. These new empirical results are explained in this paper, based on our theory."'),
('"A Framework for Foreground Detection in Complex Environments"', '"SMVP 2004"', '["Posterior Distribution", "Prior Distribution", "Change Detection", "Background Model", "Foreground', '"https://doi.org/10.1007/978-3-540-30212-4_12"', '"In this paper, a framework is proposed for the foreground detection in various complex environments. This method integrates the detection and tracking procedures into a unified probability framework by considering the spatial, spectral and temporal information of pixels to model different complex backgrounds. Firstly, a Bayesian framework, which combines the prior distribution of the pixel\\u2019s features and the likelihood probability with a homogeneous region-based background model, is introduced to classify pixels into foreground and background. Secondly, an updating scheme, which includes an on-line learning process of the prior probability and background model updating, is employed to guarantee the accuracy of accumulated statistical knowledge of pixels over time when environmental conditions are changed. By minimizing the difference between the priori and the posterior distribution of pixels within a short-term temporal frame buffer, a recursive on-line prior probability learning scheme enables the system to rapidly converge to the new equilibrium condition in response to the gradual environmental changes. This framework is demonstrated in a variety of environments including swimming pools, shopping malls, office and campuses. Compared with existing methods, this proposed methodology is more robust and efficient."'),
('"A Framework for High-Level Feedback to Adaptive, Per-Pixel, Mixture-of-Gaussian Background Models"', '"ECCV 2002"', '["Illumination Change", "Foreground Object", "Stereo Camera", "Foreground Pixel", "Observation Histo', '"https://doi.org/10.1007/3-540-47977-5_36"', '"Time-Adaptive, Per-Pixel Mixtures Of Gaussians (TAPP-MOGs) have recently become a popular choice for robust modeling and removal of complex and changing backgrounds at the pixel level. However, TAPPMOG-based methods cannot easily be made to model dynamic backgrounds with highly complex appearance, or to adapt promptly to sudden \\u201cuninteresting\\u201d scene changes such as the repositioning of a static object or the turning on of a light, without further undermining their ability to segment foreground objects, such as people, where they occlude the background for too long. To alleviate tradeoffs such as these, and, more broadly, to allow TAPPMOG segmentation results to be tailored to the specific needs of an application, we introduce a general framework for guiding pixel-level TAPPMOG evolution with feedback from \\u201chigh-level\\u201d modules. Each such module can use pixel-wise maps of positive and negative feedback to attempt to impress upon the TAPPMOG some definition of foreground that is best expressed through \\u201chigher-level\\u201d primitives such as image region properties or semantics of objects and events. By pooling the foreground error corrections of many high-level modules into a shared, pixel-level TAPPMOG model in this way, we improve the quality of the foreground segmentation and the performance of all modules that make use of it. We show an example of using this framework with a TAPPMOG method and high-level modules that all rely on dense depth data from a stereo camera."'),
('"A Framework for Pencil-of-Points Structure-from-Motion"', '"ECCV 2004"', '["Interest Point", "Camera Motion", "Fundamental Matrix", "Supporting Point", "Local Geometry"]', '"https://doi.org/10.1007/978-3-540-24671-8_3"', '"Our goal is to match contour lines between images and to recover structure and motion from those. The main difficulty is that pairs of lines from two images do not induce direct geometric constraint on camera motion. Previous work uses geometric attributes | orientation, length, etc. | for single or groups of lines. Our approach is based on using Pencil-of-Points (points on line) or pops for short. There are many advantages to using pops for structure-from-motion. The most important one is that, contrarily to pairs of lines, pairs of pops may constrain camera motion. We give a complete theoretical and practical framework for automatic structure-from-motion using pops | detection, matching, robust motion estimation, triangulation and bundle adjustment. For wide baseline matching, it has been shown that cross-correlation scores computed on neighbouring patches to the lines gives reliable results, given 2D homographic transformations to compensate for the pose of the patches. When cameras are known, this transformation has a 1-dimensional ambiguity. We show that when cameras are unknown, using pops lead to a 3-dimensional ambiguity, from which it is still possible to reliably compute cross-correlation. We propose linear and non-linear algorithms for estimating the fundamental matrix and for the multiple-view triangulation of pops. Experimental results are provided for simulated and real data."'),
('"A Framework for Unsupervised Segmentation of Multi-modal Medical Images"', '"CVAMIA 2006"', '[]', '"https://doi.org/10.1007/11889762_11"', '"We propose new techniques for unsupervised segmentation of multi-modal grayscale images such that each region-of-interest relates to a single dominant mode of the empirical marginal probability distribution of gray levels. We follow most conventional approaches such that initial images and desired maps of regions are described by a joint Markov\\u2013Gibbs random field (MGRF) model of independent image signals and interdependent region labels. But our focus is on more accurate model identification. To better specify region borders, each empirical distribution of image signals is precisely approximated by a linear combination of Gaussians (LCG) with positive and negative components. Initial segmentation based on the LCG-models is then iteratively refined by using the MGRF with analytically estimated potentials. The convergence of the overall segmentation algorithm at each stage is discussed. Experiments with medical images show that the proposed segmentation is more accurate than other known alternatives."'),
('"A General Framework for Motion Segmentation: Independent, Articulated, Rigid, Non-rigid, Degenerate', '"ECCV 2006"', '["Segmentation Result", "Spectral Cluster", "Rigid Motion", "Trajectory Data", "Linear Manifold"]', '"https://doi.org/10.1007/11744085_8"', '"We cast the problem of motion segmentation of feature trajectories as linear manifold finding problems and propose a general framework for motion segmentation under affine projections which utilizes two properties of trajectory data: geometric constraint and locality. The geometric constraint states that the trajectories of the same motion lie in a low dimensional linear manifold and different motions result in different linear manifolds; locality, by which we mean in a transformed space a data and its neighbors tend to lie in the same linear manifold, provides a cue for efficient estimation of these manifolds. Our algorithm estimates a number of linear manifolds, whose dimensions are unknown beforehand, and segment the trajectories accordingly. It first transforms and normalizes the trajectories; secondly, for each trajectory it estimates a local linear manifold through local sampling; then it derives the affinity matrix based on principal subspace angles between these estimated linear manifolds; at last, spectral clustering is applied to the matrix and gives the segmentation result. Our algorithm is general without restriction on the number of linear manifolds and without prior knowledge of the dimensions of the linear manifolds. We demonstrate in our experiments that it can segment a wide range of motions including independent, articulated, rigid, non-rigid, degenerate, non-degenerate or any combination of them. In some highly challenging cases where other state-of-the-art motion segmentation algorithms may fail, our algorithm gives expected results."'),
('"A General Method for Appearance-Based People Search Based on Textual Queries"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_45"', '"Person re-identification consists of recognising a person appearing in different video sequences, using an image as a query. We propose a general approach to extend appearance-based re-identification systems, enabling also textual queries describing clothing appearance (e.g., \\u201cperson wearing a white shirt and checked blue shorts\\u201d). This functionality can be useful, e.g., in forensic video analysis, when textual descriptions of individuals of interest given by witnesses are available, instead of images. Our approach is based on turning any given appearance descriptor into a dissimilarity-based one. This allows us to build detectors of the clothing characteristics of interest using supervised classifiers trained in a dissimilarity space, independently on the original descriptor. Our approach is evaluated using the descriptors of three different re-identification methods, on a benchmark data set."'),
('"A General Method for Unsupervised Segmentation of Images Using a Multiscale Approach"', '"ECCV 2000"', '["Image Segmentation", "Cluster Centre", "Coarse Scale", "Shift Vector", "Texture Segmentation"]', '"https://doi.org/10.1007/3-540-45053-X_5"', '"We propose a general unsupervised multiscale approach towards image segmentation. The novelty of our method is based on the following points: firstly, it is general in the sense of being independent of the feature extraction process; secondly, it is unsupervised in that the number of classes is not assumed to be known a priori; thirdly, it is flexible as the decomposition sensitivity can be robustly adjusted to produce segmentations into varying number of classes and fourthly, it is robust through the use of the mean shift clustering and Bayesian multiscale processing. Clusters in the joint spatio-feature domain are assumed to be properties of underlying classes, the recovery of which is achieved by the use of the mean shift procedure, a robust non-parametric decomposition method. The subsequent classification procedure consists of Bayesian multiscale processing which models the inherent uncertainty in the joint specification of class and position via a Multiscale Random Field model which forms a Markov Chain in scale. At every scale, the segmentation map and model parameters are determined by sampling from their conditional posterior distributions using Markov Chain Monte Carlo simulations with stochastic relaxation. The method is then applied to perform both colour and texture segmentation. Experimental results show the proposed method performs well even for complicated images."'),
('"A Generative Method for Textured Motion: Analysis and Synthesis"', '"ECCV 2002"', '["Motion Dynamic", "Markov Chain Model", "Atomic Decomposition", "Texture Synthesis", "Texture Model', '"https://doi.org/10.1007/3-540-47969-4_39"', '"Natural scenes contain rich stochastic motion patterns which are characterized by the movement of a large number of small elements, such as falling snow, raining, flying birds, firework and waterfall. In this paper, we call these motion patterns textured motion and present a generative method that combines statistical models and algorithms from both texture and motion analysis. The generative method includes the following three aspects. 1). Photometrically, an image is represented as a superposition of linear bases in atomic decomposition using an over-complete dictionary, such as Gabor or Laplacian. Such base representation is known to be generic for natural images, and it is low dimensional as the number of bases is often 100 times smaller than the number of pixels. 2). Geometrically, each moving element (called moveton), such as the individual snowflake and bird, is represented by a deformable template which is a group of several spatially adjacent bases. Such templates are learned through clustering. 3). Dynamically, the movetons are tracked through the image sequence by a stochastic algorithm maximizing a posterior probability. A classic second order Markov chain model is adopted for the motion dynamics. The sources and sinks of the movetons are modeled by birth and death maps. We adopt an EM-like stochastic gradient algorithm for inference of the hidden variables: bases, movetons, birth/death maps, parameters of the dynamics. The learned models are also verified through synthesizing random textured motion sequences which bear similar visual appearance with the observed sequences."'),
('"A Generative Model for Online Depth Fusion"', '"ECCV 2012"', '["Markov Random Field", "Visible Surface", "Normalize Cross Correlation", "Fusion Frame", "Static Sc', '"https://doi.org/10.1007/978-3-642-33715-4_11"', '"We present a probabilistic, online, depth map fusion framework, whose generative model for the sensor measurement process accurately incorporates both long-range visibility constraints and a spatially varying, probabilistic outlier model. In addition, we propose an inference algorithm that updates the state variables of this model in linear time each frame. Our detailed evaluation compares our approach against several others, demonstrating and explaining the improvements that this model offers, as well as highlighting a problem with all current methods: systemic bias."'),
('"A Generative Model for Simultaneous Estimation of Human Body Shape and Pixel-Level Segmentation"', '"ECCV 2012"', '["Body Part", "Body Model", "Evaluation Class", "Viewpoint Model", "Individual Body Part"]', '"https://doi.org/10.1007/978-3-642-33715-4_51"', '"This paper addresses pixel-level segmentation of a human body from a single image. The problem is formulated as a multi-region segmentation where the human body is constrained to be a collection of geometrically linked regions and the background is split into a small number of distinct zones. We solve this problem in a Bayesian framework for jointly estimating articulated body pose and the pixel-level segmentation of each body part. Using an image likelihood function that simultaneously generates and evaluates the image segmentation corresponding to a given pose, we robustly explore the posterior body shape distribution using a data-driven, coarse-to-fine Metropolis Hastings sampling scheme that includes a strongly data-driven proposal term."'),
('"A Generative Model for the Joint Registration of Multiple Point Sets"', '"ECCV 2014"', '["point set registration", "joint registration", "expectation maximization", "Gaussian mixture model', '"https://doi.org/10.1007/978-3-319-10584-0_8"', '"This paper describes a probabilistic generative model and its associated algorithm to jointly register multiple point sets. The vast majority of state-of-the-art registration techniques select one of the sets as the \\u201cmodel\\u201d and perform pairwise alignments between the other sets and this set. The main drawback of this mode of operation is that there is no guarantee that the model-set is free of noise and outliers, which contaminates the estimation of the registration parameters. Unlike previous work, the proposed method treats all the point sets on an equal footing: they are realizations of a Gaussian mixture (GMM) and the registration is cast into a clustering problem. We formally derive an EM algorithm that estimates both the GMM parameters and the rotations and translations that map each individual set onto the \\u201ccentral\\u201d model. The mixture means play the role of the registered set of points while the variances provide rich information about the quality of the registration. We thoroughly validate the proposed method with challenging datasets, we compare it with several state-of-the-art methods, and we show its potential for fusing real depth data."'),
('"A Generative Shape Regularization Model for Robust Face Alignment"', '"ECCV 2008"', '["Local Binary Pattern", "Deformable Model", "Inference Algorithm", "Kernel Principal Component Anal', '"https://doi.org/10.1007/978-3-540-88682-2_32"', '"In this paper, we present a robust face alignment system that is capable of dealing with exaggerating expressions, large occlusions, and a wide variety of image noises. The robustness comes from our shape regularization model, which incorporates constrained nonlinear shape prior, geometric transformation, and likelihood of multiple candidate landmarks in a three-layered generative model. The inference algorithm iteratively examines the best candidate positions and updates face shape and pose. This model can effectively recover sufficient shape details from very noisy observations. We demonstrate the performance of this approach on two public domain databases and a large collection of real-world face photographs."'),
('"A Generic Concept for Camera Calibration"', '"ECCV 2004"', '["Calibration Point", "Camera Calibration", "Optical Center", "Camera Model", "Bundle Adjustment"]', '"https://doi.org/10.1007/978-3-540-24671-8_1"', '"We present a theory and algorithms for a generic calibration concept that is based on the following recently introduced general imaging model. An image is considered as a collection of pixels, and each pixel measures the light travelling along a (half-) ray in 3-space associated with that pixel. Calibration is the determination, in some common coordinate system, of the coordinates of all pixels\\u2019 rays. This model encompasses most projection models used in computer vision or photogrammetry, including perspective and affine models, optical distortion models, stereo systems, or catadioptric systems \\u2013 central (single viewpoint) as well as non-central ones. We propose a concept for calibrating this general imaging model, based on several views of objects with known structure, but which are acquired from unknown viewpoints. It allows in principle to calibrate cameras of any of the types contained in the general imaging model using one and the same algorithm. We first develop the theory and an algorithm for the most general case: a non-central camera that observes 3D calibration objects. This is then specialized to the case of central cameras and to the use of planar calibration objects. The validity of the concept is shown by experiments with synthetic and real data."'),
('"A Generic Model to Compose Vision Modules for Holistic Scene Understanding"', '"ECCV 2010"', '["Vision Task", "Sparse Code", "Event Categorization", "Depth Estimation", "Salient Region"]', '"https://doi.org/10.1007/978-3-642-35749-7_6"', '"The problem of holistic scene understanding involves many vision tasks such as depth estimation, scene categorization, event categorization, etc. Each of these tasks explores some aspects of the scene but, these tasks are related in that, they represent attributes of the same scene. An intuition is that one task can provide meaningful attributes to aid the learning process of another task. In this work, we propose a generic model (together with learning and inference techniques) for connecting different vision tasks in the form of a 2-layer cascade. Our model considers the first layer as a hidden layer, where the latent variables are inferred by feedback from the second layer. The feedback mechanism allows the first layer classifiers to focus on more important image modes, and draws their output towards \\u201cattributes\\u201d rather than the original \\u201clabels\\u201d. Our model also automatically discovers sparse connections between the learned attributes on the first layer and the target task on the second layer. Note that in our model, the same vision tasks can act as attribute learners as well as target tasks, while being set up on different layers. In extensive experiments, we show that the same proposed model improves the performance in all the tasks we consider: single image depth estimation, scene categorization, saliency detection and event categorization."'),
('"A Generic Neighbourhood Filtering Framework for Matrix Fields"', '"ECCV 2008"', '["Image Denoising", "Complexity Order", "Smoothness Term", "Matrix Field", "Restoration Quality"]', '"https://doi.org/10.1007/978-3-540-88690-7_39"', '"The Nonlocal Data and Smoothness (NDS) filtering framework for greyvalue images has been recently proposed by Mr\\u00e1zek et al. This model for image denoising unifies M-smoothing and bilateral filtering, and several well-known nonlinear filters from the literature become particular cases. In this article we extend this model to so-called matrix fields. These data appear, for example, in diffusion tensor magnetic resonance imaging (DT-MRI). Our matrix-valued NDS framework includes earlier filters developped for DT-MRI data, for instance, the affine-invariant and the log-Euclidean regularisation of matrix fields. Experiments performed with synthetic matrix fields and real DT-MRI data showed excellent performance with respect to restoration quality as well as speed of convergence."'),
('"A Global Hypotheses Verification Method for 3D Object Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33712-3_37"', '"We propose a novel approach for verifying model hypotheses in cluttered and heavily occluded 3D scenes. Instead of verifying one hypothesis at a time, as done by most state-of-the-art 3D object recognition methods, we determine object and pose instances according to a global optimization stage based on a cost function which encompasses geometrical cues. Peculiar to our approach is the inherent ability to detect significantly occluded objects without increasing the amount of false positives, so that the operating point of the object recognition algorithm can nicely move toward a higher recall without sacrificing precision. Our approach outperforms state-of-the-art on a challenging dataset including 35 household models obtained with the Kinect sensor, as well as on the standard 3D object recognition benchmark dataset."'),
('"A Globally Optimal Approach for 3D Elastic Motion Estimation from Stereo Sequences"', '"ECCV 2010"', '["Motion Estimation", "Edge Point", "Hide State", "Stereo Image", "Stereo Image Pair"]', '"https://doi.org/10.1007/978-3-642-15561-1_38"', '"Dense and markerless elastic 3D motion estimation based on stereo sequences is a challenge in computer vision. Solutions based on scene flow and 3D registration are mostly restricted to simple non-rigid motions, and suffer from the error accumulation. To address this problem, this paper proposes a globally optimal approach to non-rigid motion estimation which simultaneously recovers the 3D surface as well as its non-rigid motion over time. The instantaneous surface of the object is represented as a set of points which is reconstructed from the matched stereo images, meanwhile its deformation is captured by registering the points over time under spatio-temporal constraints. A global energy is defined on the constraints of stereo, spatial smoothness and temporal continuity, which is optimized via an iterative algorithm to approximate the minimum. Our extensive experiments on real video sequences including different facial expressions, cloth flapping, flag waves, etc. proved the robustness of our method and showed the method effectively handles complex nonrigid motions."'),
('"A Graph Based Subspace Semi-supervised Learning Framework for Dimensionality Reduction"', '"ECCV 2008"', '["Dimensionality Reduction", "Face Recognition", "Linear Discriminant Analysis", "Unlabeled Data", "', '"https://doi.org/10.1007/978-3-540-88688-4_49"', '"The key to the graph based semi-supervised learning algorithms for classification problems is how to construct the weight matrix of the p-nearest neighbor graph. A new method to construct the weight matrix is proposed and a graph based Subspace Semi-supervised Learning Framework (SSLF) is developed. The Framework aims to find an embedding transformation which respects the discriminant structure inferred from the labeled data, as well as the intrinsic geometrical structure inferred from both the labeled and unlabeled data. By utilizing this framework as a tool, we drive three semi-supervised dimensionality reduction algorithms: Subspace Semi-supervised Linear Discriminant Analysis (SSLDA), Subspace Semi-supervised Locality Preserving Projection (SSLPP), and Subspace Semi-supervised Marginal Fisher Analysis (SSMFA). The experimental results on face recognition demonstrate our subspace semi-supervised algorithms are able to use unlabeled samples effectively."'),
('"A Graph Theoretic Approach for Object Shape Representation in Compositional Hierarchies Using a Hyb', '"ECCV 2014"', '["Minimum Description Length", "Gabor Feature", "Object Graph", "Vocabulary Size", "Shape Retrieval"', '"https://doi.org/10.1007/978-3-319-10578-9_37"', '"A graph theoretic approach is proposed for object shape representation in a hierarchical compositional architecture called Compositional Hierarchy of Parts (CHOP). In the proposed approach, vocabulary learning is performed using a hybrid generative-descriptive model. First, statistical relationships between parts are learned using a Minimum Conditional Entropy Clustering algorithm. Then, selection of descriptive parts is defined as a frequent subgraph discovery problem, and solved using a Minimum Description Length (MDL) principle. Finally, part compositions are constructed using learned statistical relationships between parts and their description lengths. Shape representation and computational complexity properties of the proposed approach and algorithms are examined using six benchmark two-dimensional shape image datasets. Experiments show that CHOP can employ part shareability and indexing mechanisms for fast inference of part compositions using learned shape vocabularies. Additionally, CHOP provides better shape retrieval performance than the state-of-the-art shape retrieval methods."'),
('"A Grassmannian Framework for Face Recognition of 3D Dynamic Sequences with Challenging Conditions"', '"ECCV 2014"', '["Face recognition", "3d dynamic face sequences", "Grassmann manifold"]', '"https://doi.org/10.1007/978-3-319-16220-1_23"', '"Modern face recognition approaches target successful person identification in challenging scenarios, where uncooperative subjects are captured under unconstrained imaging conditions. With the introduction of a new generation of 3D acquisition devices capable of dynamic acquisitions, this trend is now emerging also in 3D based approaches. Motivated by these considerations, in this paper we propose an original and effective framework to address face recognition from 3D temporal sequences acquired in adverse conditions, including internal and external occlusions, pose and expression variations, and talking. Due to the novelty of the proposed scenario, a new database has been collected using a single-view structured light scanner with a large field of view, which allows free movement of the acquired subjects. The 3D temporal sequences are divided into fragments each modeled as a linear subspace in order to embody the shape and the motion of the facial surfaces. In virtue of the Riemannian geometry of the space of real \\\\(k\\\\)-dimensional linear subspaces, called Grassmann manifold, a new formulation of the matching between 3D temporal sequences has been developed. An unsupervised clustering over the Grassmann manifold is also introduced for efficient recognition. The proposed approach achieves promising results, without requiring any prior training or manual intervention."'),
('"A Hierarchical Framework for Spectral Correspondence"', '"ECCV 2002"', '["Modal Matrix", "Point Correspondence", "Modal Cluster", "Hierarchical Framework", "Proximity Matri', '"https://doi.org/10.1007/3-540-47969-4_18"', '"The modal correspondence method of Shapiro and Brady aims to match point-sets by comparing the eigenvectors of a pairwise point proximity matrix. Although elegant by means of its matrix representation, the method is notoriously susceptible to differences in the relational structure of the point-sets under consideration. In this paper we demonstrate how the method can be rendered robust to structural differences by adopting a hierarchical approach. We place the modal matching problem in a probabilistic setting in which the correspondences between pairwise clusters can be used to constrain the individual point correspondences. To meet this goal we commence by describing an iterative method which can be applied to the point proximity matrix to identify the locations of pairwise modal clusters. Once we have assigned points to clusters, we compute within-cluster and between-cluster proximity matrices. The modal co-efficients for these two sets of proximity matrices are used to compute cluster correspondence and cluster-conditional point correspondence probabilities. A sensitivity study on synthetic point-sets reveals that the method is considerably more robust than the conventional method to clutter or point-set contamination."'),
('"A Hierarchical Representation for Future Action Prediction"', '"ECCV 2014"', '["Video Clip", "Human Movement", "Motion Segment", "Future Action", "Dynamic Time Warping"]', '"https://doi.org/10.1007/978-3-319-10578-9_45"', '"We consider inferring the future actions of people from a still image or a short video clip. Predicting future actions before they are actually executed is a critical ingredient for enabling us to effectively interact with other humans on a daily basis. However, challenges are two fold: First, we need to capture the subtle details inherent in human movements that may imply a future action; second, predictions usually should be carried out as quickly as possible in the social world, when limited prior observations are available."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A High-Quality Video Denoising Algorithm Based on Reliable Motion Estimation"', '"ECCV 2010"', '["Motion Estimation", "Additive White Gaussian Noise", "Priority Queue", "Temporal Coherence", "Stru', '"https://doi.org/10.1007/978-3-642-15558-1_51"', '"Although the recent advances in the sparse representations of images have achieved outstanding denosing results, removing real, structured noise in digital videos remains a challenging problem. We show the utility of reliable motion estimation to establish temporal correspondence across frames in order to achieve high-quality video denoising. In this paper, we propose an adaptive video denosing framework that integrates robust optical flow into a non-local means (NLM) framework with noise level estimation. The spatial regularization in optical flow is the key to ensure temporal coherence in removing structured noise. Furthermore, we introduce approximate K-nearest neighbor matching to significantly reduce the complexity of classical NLM methods. Experimental results show that our system is comparable with the state of the art in removing AWGN, and significantly outperforms the state of the art in removing real, structured noise."'),
('"A Highly Efficient GPU Implementation for Variational Optic Flow Based on the Euler-Lagrange Framew', '"ECCV 2010"', '["Global Memory", "Data Term", "Smoothness Term", "British Machine Vision", "Constancy Assumption"]', '"https://doi.org/10.1007/978-3-642-35740-4_29"', '"The Euler-Lagrange (EL) framework is the most widely-used strategy for solving variational optic flow methods. We present the first approach that solves the EL equations of state-of-the-art methods on sequences with \\\\(640 \\\\!\\\\times\\\\! 480\\\\) pixels in near-realtime on GPUs. This performance is achieved by combining two ideas: (i) We extend the recently proposed Fast Explicit Diffusion (FED) scheme to optic flow, and additionally embed it into a coarse-to-fine strategy. (ii) We parallelise our complete algorithm on a GPU, where a careful optimisation of global memory operations and an efficient use of on-chip memory guarantee a good performance. Applying our approach to the variational \\u2018Complementary Optic Flow\\u2019 method (Zimmer et al. (2009)), we obtain highly accurate flow fields in less than a second. This currently constitutes the fastest method in the top 10 of the widely used Middlebury benchmark."'),
('"A Human vs. Machine Challenge in Fashion Color Classification"', '"ECCV 2012"', '["Human-Machine Interaction", "Image Retrieval", "Color Analysis", "Evaluation"]', '"https://doi.org/10.1007/978-3-642-33885-4_69"', '"For this demo, we present a set of stark applications designed to evaluate the performance of a color similarity retrieval system against human operators performance in the same tasks. The proposed series of tests give some interesting insights about the perception of color classes and the reliability of manual annotation in the fashion context."'),
('"A l1-Unified Variational Framework for Image Restoration"', '"ECCV 2004"', '["Besov Space", "Wavelet Packet", "Image Restoration", "Variational Framework", "Wavelet Shrinkage"]', '"https://doi.org/10.1007/978-3-540-24673-2_1"', '"Among image restoration literature, there are mainly two kinds of approach. One is based on a process over image wavelet coefficients, as wavelet shrinkage for denoising. The other one is based on a process over image gradient. In order to get an edge-preserving regularization, one usually assume that the image belongs to the space of functions of Bounded Variation (BV). An energy is minimized, composed of an observation term and the Total Variation (TV) of the image."'),
('"A Lattice-Preserving Multigrid Method for Solving the Inhomogeneous Poisson Equations Used in Image', '"ECCV 2008"', '["Diffusion Constant", "Multigrid Method", "Coarse Level", "Laplacian Matrix", "Bilinear Interpolati', '"https://doi.org/10.1007/978-3-540-88688-4_19"', '"The inhomogeneous Poisson (Laplace) equation with internal Dirichlet boundary conditions has recently appeared in several applications ranging from image segmentation [1, 2, 3] to image colorization [4], digital photo matting [5, 6] and image filtering [7, 8]. In addition, the problem we address may also be considered as the generalized eigenvector problem associated with Normalized Cuts [9], the linearized anisotropic diffusion problem [10, 11, 8] solved with a backward Euler method, visual surface reconstruction with discontinuities [12, 13] or optical flow [14]. Although these approaches have demonstrated quality results, the computational burden of finding a solution requires an efficient solver. Design of an efficient multigrid solver is difficult for these problems due to unpredictable inhomogeneity in the equation coefficients and internal Dirichlet boundary conditions with unpredictable location and value. Previous approaches to multigrid solvers have typically employed either a data-driven operator (with fast convergence) or the maintenance of a lattice structure at coarse levels (with low memory overhead). In addition to memory efficiency, a lattice structure at coarse levels is also essential to taking advantage of the power of a GPU implementation [15,16,5,3]. In this work, we present a multigrid method that maintains the low memory overhead (and GPU suitability) associated with a regular lattice while benefiting from the fast convergence of a data-driven coarse operator."'),
('"A Layered Motion Representation with Occlusion and Compact Spatial Support"', '"ECCV 2002"', '["Initial Guess", "Relative Depth", "Image Motion", "Motion Segmentation", "Occupancy Probability"]', '"https://doi.org/10.1007/3-540-47969-4_46"', '"We describe a 2.5D layered representation for visual motion analysis. The representation provides a global interpretation of image motion in terms of several spatially localized foreground regions along with a background region. Each of these regions comprises a parametric shape model and a parametric motion model. The representation also contains depth ordering so visibility and occlusion are rightly included in the estimation of the model parameters. Finally, because the number of objects, their positions, shapes and sizes, and their relative depths are all unknown, initial models are drawn from a proposal distribution, and then compared using a penalized likelihood criterion. This allows us to automatically initialize new models, and to compare different depth orderings."'),
('"A Learning Based Approach for 3D Segmentation and Colon Detagging"', '"ECCV 2006"', '["Grid Node", "Discriminative Model", "Input Volume", "Virtual Colonoscopy", "Boundary Evolution"]', '"https://doi.org/10.1007/11744078_34"', '"Foreground and background segmentation is a typical problem in computer vision and medical imaging. In this paper, we propose a new learning based approach for 3D segmentation, and we show its application on colon detagging. In many problems in vision, both the foreground and the background observe large intra-class variation and inter-class similarity. This makes the task of modeling and segregation of the foreground and the background very hard. The framework presented in this paper has the following key components: (1) We adopt probabilistic boosting tree [9] for learning discriminative models for the appearance of complex foreground and background. The discriminative model ratio is proved to be a pseudo-likelihood ratio modeling the appearances. (2) Integral volume and a set of 3D Haar filters are used to achieve efficient computation. (3) We devise a 3D topology representation, grid-line, to perform fast boundary evolution. The proposed algorithm has been tested on over 100 volumes of size 500 \\u00d7 512 \\u00d7 512 at the speed of 2 ~ 3 minutes per volume. The results obtained are encouraging."'),
('"A Linear Time Histogram Metric for Improved SIFT Matching"', '"ECCV 2008"', '["Scale Invariant Feature Transform", "JPEG Compression", "Linear Time Algorithm", "Oriented Gradien', '"https://doi.org/10.1007/978-3-540-88690-7_37"', '"We present a new metric between histograms such as SIFT descriptors and a linear time algorithm for its computation. It is common practice to use the L 2 metric for comparing SIFT descriptors. This practice assumes that SIFT bins are aligned, an assumption which is often not correct due to quantization, distortion, occlusion etc."'),
('"A Linguistic Feature Vector for the Visual Interpretation of Sign Language"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_30"', '"This paper presents a novel approach to sign language recognition that provides extremely high classification rates on minimal training data. Key to this approach is a 2 stage classification procedure where an initial classification stage extracts a high level description of hand shape and motion. This high level description is based upon sign linguistics and describes actions at a conceptual level easily understood by humans. Moreover, such a description broadly generalises temporal activities naturally overcoming variability of people and environments. A second stage of classification is then used to model the temporal transitions of individual signs using a classifier bank of Markov chains combined with Independent Component Analysis. We demonstrate classification rates as high as 97.67% for a lexicon of 43 words using only single instance training outperforming previous approaches where thousands of training examples are required."'),
('"A Local Bag-of-Features Model for Large-Scale Object Retrieval"', '"ECCV 2010"', '["Image Retrieval", "Visual Word", "Query Image", "Integral Image", "Object Retrieval"]', '"https://doi.org/10.1007/978-3-642-15567-3_22"', '"The so-called bag-of-features (BoF) representation for images is by now well-established in the context of large scale image and video retrieval. The BoF framework typically ranks database image according to a metric on the global histograms of the query and database images, respectively. Ranking based on global histograms has the advantage of being scalable with respect to the number of database images, but at the cost of reduced retrieval precision when the object of interest is small. Additionally, computationally intensive post-processing (such as RANSAC) is typically required to locate the object of interest in the retrieved images. To address these shortcomings, we propose a generalization of the global BoF framework to support scalable local matching. Specifically, we propose an efficient and accurate algorithm to accomplish local histogram matching and object localization simultaneously. The generalization is to represent each database image as a family of histograms that depend functionally on a bounding rectangle. Integral with the image retrieval process, we identify bounding rectangles whose histograms optimize query relevance, and rank the images accordingly. Through this localization scheme, we impose a weak spatial consistency constraint with low computational overhead. We validate our approach on two public image retrieval benchmarks: the University of Kentucky data set and the Oxford Building data set. Experiments show that our approach significantly improves on BoF-based retrieval, without requiring computationally expensive post-processing."'),
('"A Locally Linear Regression Model for Boundary Preserving Regularization in Stereo Matching"', '"ECCV 2012"', '["Data Term", "Stereo Match", "Temporal Consistency", "Local Linear Model", "Regularization Model"]', '"https://doi.org/10.1007/978-3-642-33715-4_8"', '"We propose a novel regularization model for stereo matching that uses large neighborhood windows. The model is based on the observation that in a local neighborhood there exists a linear relationship between pixel values and disparities. Compared to the traditional boundary preserving regularization models that use adjacent pixels, the proposed model is robust to image noise and captures higher level interactions. We develop a globally optimized stereo matching algorithm based on this regularization model. The algorithm alternates between finding a quadratic upper bound of the relaxed energy function and solving the upper bound using iterative reweighted least squares. To reduce the chance of being trapped in local minima, we propose a progressive convex-hull filter to tighten the data cost relaxation. Our evaluation on the Middlebury datasets shows the effectiveness of our method in preserving boundary sharpness while keeping regions smooth. We also evaluate our method on a wide range of challenging real-world videos. Experimental results show that our method outperforms existing methods in temporal consistency."'),
('"A Low-Level Active Vision Framework for Collaborative Unmanned Aircraft Systems"', '"ECCV 2014"', '["Visual tracking", "Visual surveillance", "Micro UAV", "Active vision"]', '"https://doi.org/10.1007/978-3-319-16178-5_15"', '"Micro unmanned aerial vehicles are becoming increasingly interesting for aiding and collaborating with human agents in myriads of applications, but in particular they are useful for monitoring inaccessible or dangerous areas. In order to interact with and monitor humans, these systems need robust and real-time computer vision subsystems that allow to detect and follow persons."'),
('"A MAP-Estimation Framework for Blind Deblurring Using High-Level Edge Priors"', '"ECCV 2014"', '["Latent Image", "Image Edge", "Edge Pixel", "Blind Deconvolution", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-319-10605-2_10"', '"In this paper we propose a general MAP-estimation framework for blind image deconvolution that allows the incorporation of powerful priors regarding predicting the edges of the latent image, which is known to be a crucial factor for the success of blind deblurring. This is achieved in a principled, robust and unified manner through the use of a global energy function that can take into account multiple constraints. Based on this framework, we show how to successfully make use of a particular prior of this type that is quite strong and also applicable to a wide variety of cases. It relates to the strong structural regularity that is exhibited by many scenes, and which affects the location and distribution of the corresponding image edges. We validate the excellent performance of our approach through an extensive set of experimental results and comparisons to the state-of-the-art."'),
('"A Markov Chain Monte Carlo Approach to Stereovision"', '"ECCV 2002"', '["stereoscopic vision", "digital terrain models", "disparity", "uncertainty estimation", "sampling a', '"https://doi.org/10.1007/3-540-47977-5_7"', '"We propose Markov chain Monte Carlo sampling methods to address uncertainty estimation in disparity computation. We consider this problem at a postprocessing stage, i.e. once the disparity map has been computed, and suppose that the only information available is the stereoscopic pair. The method, which consists of sampling from the posterior distribution given the stereoscopic pair, allows the prediction of large errors which occur with low probability, and accounts for spatial correlations. The model we use is oriented towards an application to mid-resolution stereo systems, but we give insights on how it can be extended. Moreover, we propose a new sampling algorithm relying on Markov chain theory and the use of importance sampling to speed up the computation. The efficiency of the algorithm is demonstrated, and we illustrate our method with the computation of confidence intervals and probability maps of large errors, which may be applied to optimize a trajectory in a three dimensional environment."'),
('"A Memory Efficient Discriminative Approach for Location Aided Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_19"', '"We propose a visual recognition approach aimed at fast recognition of urban landmarks on a GPS-enabled mobile device. While most existing methods offload their computation to a server, the latency of an image upload over a slow network can be a significant bottleneck. In this paper, we investigate a new approach to mobile visual recognition that would involve uploading only GPS coordinates to a server, following which a compact location specific classifier would be downloaded to the client and recognition would be computed completely on the client. To achieve this goal, we have developed an approach based on supervised learning that involves training very compact random forest classifiers based on labeled geo-tagged images. Our approach selectively chooses highly discriminative yet repeatable visual features in the database images during offline processing. Classification is efficient at query time as we first rectify the image based on vanishing points and then use random binary patterns to densely match a small set of downloaded features with min-hashing used to speedup the search. We evaluate our method on two public benchmarks and on two streetside datasets where we outperform standard bag-of-words retrieval as well as direct feature matching approaches, both of which are infeasible for client-side query processing."'),
('"A Method for Online Analysis of Structured Processes Using Bayesian Filters and Echo State Networks', '"ECCV 2012"', '["Hide Markov Model", "Gaussian Mixture Model", "Zernike Moment", "Online Analysis", "Task Duration"', '"https://doi.org/10.1007/978-3-642-33885-4_8"', '"We propose a Bayesian filtering framework for online analysis of visual structured processes, which can be combined with the Echo State Network (ESN) to capture prior information. With the proposed method we mitigate the effective Markovian Behavior of the ESN. We are able to keep a set of hypotheses about the entire history of behaviors and to evaluate them online based on new observations. The performance is evaluated under two complex visual behavior understanding scenarios using public datasets: a visual process for a kitchen table preparation and a real life manufacturing process."'),
('"A Minimal Case Solution to the Calibrated Relative Pose Problem for the Case of Two Known Orientati', '"ECCV 2010"', '["Orientation Angle", "Inertial Measurement Unit", "Independent Equation", "Point Correspondence", "', '"https://doi.org/10.1007/978-3-642-15561-1_20"', '"It this paper we present a novel minimal case solution to the calibrated relative pose problem using 3 point correspondences for the case of two known orientation angles. This case is relevant when a camera is coupled with an inertial measurement unit (IMU) and it recently gained importance with the omnipresence of Smartphones (iPhone, Nokia N900) that are equipped with accelerometers to measure the gravity normal. Similar to the 5-point (6-point), 7-point, and 8-point algorithm for computing the essential matrix in the unconstrained case, we derive a 3-point, 4-point and, 5-point algorithm for the special case of two known orientation angles. We investigate degenerate conditions and show that the new 3-point algorithm can cope with planes and even collinear points. We will show a detailed analysis and comparison on synthetic data and present results on cell phone images. As an additional application we demonstrate the algorithm on relative pose estimation for a micro aerial vehicle\\u2019s (MAV) camera-IMU system."'),
('"A Minimal Set of Constraints for the Trifocal Tensor"', '"ECCV 2000"', '["Image Plane", "Null Space", "Fundamental Matrix", "Fundamental Matrice", "Double Root"]', '"https://doi.org/10.1007/3-540-45054-8_6"', '"In this paper we derive a minimal set of sufficient constraints in order for 27 numbers to constitute a trifocal tensor. It is shown that, in general, eight nonlinear algebraic constraints are enough. This result is in accordance with the theoretically expected number of eight independent constraints and novel since the to date known sets of sufficient constraints contain at least 12 conditions. Up to now, research and formulation of constraints for the trifocal tensor has concentrated mainly on the correlation slices and has produced sets of constraints that are neither minimal (\\u2265 12) nor independent. We show that by turning attention from correlation to homographic slices, simple geometric considerations yield the desired result. Having the minimal set of constraints is important for constrained estimation of the tensor, as well as for deepening the understanding of the multiple view relations that are valid in the projective framework."'),
('"A Minimal Solution for Camera Calibration Using Independent Pairwise Correspondences"', '"ECCV 2012"', '["Camera Networks", "Multiple View Geometry", "Camera Calibration"]', '"https://doi.org/10.1007/978-3-642-33783-3_52"', '"We propose a minimal algorithm for fully calibrating a camera from 11 independent pairwise point correspondences with two other calibrated cameras. Unlike previous approaches, our method neither requires triple correspondences, nor prior knowledge about the viewed scene. This algorithm can be used to insert or re-calibrate a new camera into an existing network, without having to interrupt operation. Its main strength comes from the fact that it is often difficult to find triple correspondences in a camera network. This makes our algorithm, for the specified use cases, probably the most suited calibration solution that does not require a calibration target, and hence can be performed without human interaction."'),
('"A Minimally-Interactive Watershed Algorithm Designed for Efficient CTA Bone Removal"', '"CVAMIA 2006"', '["Medical Image Analysis", "Bone Removal", "Direct Volume Rendering", "Bone Segmentation", "Watershe', '"https://doi.org/10.1007/11889762_16"', '"We introduce a novel minimally-interactive watershed algorithm that needs no initial parameterization, but lets the user refine the automatic segmentation close to real-time. In contrast to previous proposals, our algorithm encapsulates all time consuming calculation in a processing step executed only once. Thereby, a hierarchical subdivision of the incoming image data is generated. This subdivision serves as a basis for computing automatic segmentation results according to a given multi-dimensional classification scheme as well as for interactive refinement according to local markers. We have successfully applied our algorithm to efficiently removing bone structures from computed tomography angiography data, which is among the very challenging segmentation problems in medical image analysis."'),
('"A Model of Volumetric Shape for the Analysis of Longitudinal Alzheimer\\u2019s Disease Data"', '"ECCV 2010"', '["Mild Cognitive Impairment", "Hippocampal Volume", "Hippocampal Atrophy", "Left Hippocampus", "Shap', '"https://doi.org/10.1007/978-3-642-15558-1_43"', '"We develop a multi-scale model of shape based on a volumetric representation of solids in 3D space. A signed energy function (SEF) derived from the model is designed to quantify the magnitude of regional shape changes that correlate well with local shrinkage and expansion. The methodology is applied to the analysis of longitudinal morphological data representing hippocampal volumes extracted from one-year repeat magnetic resonance scans of the brain of 381 subjects collected by the Alzheimer\\u2019s Disease Neuroimaging Initiative. We first establish a strong correlation between the SEFs and hippocampal volume loss over a one-year period and then use SEFs to characterize specific regions where hippocampal atrophy over the one-year period differ significantly among groups of normal controls and subjects with mild cognitive impairment and Alzheimer\\u2019s disease."'),
('"A Model-Based Approach to Recovering the Structure of a Plant from Images"', '"ECCV 2014"', '["Plant phenotyping", "Image processing", "Plant architecture"]', '"https://doi.org/10.1007/978-3-319-16220-1_16"', '"We present a method for recovering the structure of a plant directly from a small set of widely-spaced images for automated analysis of phenotype. Structure recovery is more complex than shape estimation, but the resulting structure estimate is more closely related to phenotype than is a 3D geometric model. The method we propose is applicable to a wide variety of plants, but is demonstrated on wheat. Wheat is composed of thin elements with few identifiable features, making it difficult to analyse using standard feature matching techniques. Our method instead analyses the structure of plants using only their silhouettes. We employ a generate-and-test method, using a database of manually modelled leaves and a model for their composition to synthesise plausible plant structures which are evaluated against the images. The method is capable of efficiently recovering accurate estimates of plant structure in a wide variety of imaging scenarios, without manual intervention."'),
('"A Modular Framework for 2D/3D and Multi-modal Segmentation with Joint Super-Resolution"', '"ECCV 2012"', '["Segmentation", "Image Processing", "Range Imaging", "Time-of-Flight (ToF)", "Photonic Mixer Device', '"https://doi.org/10.1007/978-3-642-33868-7_2"', '"A versatile multi-image segmentation framework for 2D/3D or multi-modal segmentation is introduced in this paper with possible application in a wide range of machine vision problems. The framework performs a joint segmentation and super-resolution to account for images of unequal resolutions gained from different imaging sensors. This allows to combine high resolution details of one modality with the distinctiveness of another modality. A set of measures is introduced to weight measurements according to their expected reliability and it is utilized in the segmentation as well as the super-resolution. The approach is demonstrated with different experimental setups and the effect of additional modalities as well as of the parameters of the framework are shown."'),
('"A Multi-scale Boosted Detector for Efficient and Robust Gesture Recognition"', '"ECCV 2014"', '["Gesture recognition", "Boosting methods", "One-vs-all", "Multi-modal fusion", "Feature pooling"]', '"https://doi.org/10.1007/978-3-319-16178-5_34"', '"We present an approach to detecting and recognizing gestures in a stream of multi-modal data. Our approach combines a sliding-window gesture detector with features drawn from skeleton data, color imagery, and depth data produced by a first-generation Kinect sensor. The detector consists of a set of one-versus-all boosted classifiers, each tuned to a specific gesture. Features are extracted at multiple temporal scales, and include descriptive statistics of normalized skeleton joint positions, angles, and velocities, as well as image-based hand descriptors. The full set of gesture detectors may be trained in under two hours on a single machine, and is extremely efficient at runtime, operating at 1700fps using only skeletal data, or at 100fps using fused skeleton and image features. Our method achieved a Jaccard Index score of 0.834 on the ChaLearn-2014 Gesture Recognition Test dataset, and was ranked 2nd overall in the competition."'),
('"A Multi-scale Geometric Flow for Segmenting Vasculature in MRI"', '"MMBIA 2004"', '["Tubular Structure", "Proton Density", "Medical Image Analysis", "Vessel Segmentation", "Transverse', '"https://doi.org/10.1007/978-3-540-27816-0_15"', '"Often in neurosurgical planning a dual echo acquisition is performed that yields proton density (PD) and T2-weighted images to evaluate edema near a tumour or lesion. The development of vessel segmentation algorithms for PD images is of general interest since this type of acquisition is widespread and is entirely noninvasive. Whereas vessels are signaled by black blood contrast in such images, extracting them is a challenge because other anatomical structures also yield similar contrasts at their boundaries. In this paper we present a novel multi-scale geometric flow for segmenting vasculature from PD images which can also be applied to the easier cases of computed tomography (CT) angiography data or Gadolinium enhanced MRI. The key idea is to first apply Frangi\\u2019s vesselness measure [4] to find putative centerlines of tubular structures along with their estimated radii. This multi-scale measure is then distributed to create a vector field which is orthogonal to vessel boundaries so that the flux maximizing flow algorithm of [17] can be applied to recover them. We validate the approach qualitatively with PD, angiography and Gadolinium enhanced MRI volumes."'),
('"A Multi-scale Vector Spline Method for Estimating the Fluids Motion on Satellite Images"', '"ECCV 2008"', '["Control Point", "Conservation Equation", "Motion Estimation", "Observation Operator", "Iterative M', '"https://doi.org/10.1007/978-3-540-88693-8_49"', '"Satellite image sequences visualize important patterns of the atmospheric and oceanographic circulation. Assessing motion from these data thus has a strong potential for improving the performances of the forecast models. Representing a vector field by a vector spline has been proven efficient for fluid motion assessment: the vector spline formulation makes it possible to initially select the locations where the conservation equation has to be taken into account; it efficiently implements the 2nd order div-curl regularity, advocated for turbulent fluids. The scientific contribution of this article is to formulate vector splines in a multiscale scheme, with the double objective of assessing motion even in the case of large displacements and capturing the spectrum of spatial scales associated to turbulent flows. The proposed method only requires the inversion of a band matrix, which is performed by an efficient numerical scheme making the method tractable for large satellite image sequences."'),
('"A Multi-stage Approach to Curve Extraction"', '"ECCV 2014"', '["IEEE Computer Society", "Curve Stability", "Zebra Pattern", "Curve Extraction", "Human Annotation"', '"https://doi.org/10.1007/978-3-319-10590-1_43"', '"We propose a multi-stage approach to curve extraction where the curve fragment search space is iteratively reduced by removing unlikely candidates using geometric constrains, but without affecting recall, to a point where the application of an objective functional becomes appropriate. The motivation in using multiple stages is to avoid the drawback of using a global functional directly on edges, which can result in non-salient but high scoring curve fragments, which arise from non-uniformly distributed edge evidence. The process progresses in stages from local to global: (i) edges, (ii) curvelets, (iii) unambiguous curve fragments, (iv) resolving ambiguities to generate a full set of curve fragment candidates, (v) merging curve fragments based on a learned photometric and geometric cues as well a novel lateral edge sparsity cue, and (vi) the application of a learned objective functional to get a final selection of curve fragments. The resulting curve fragments are typically visually salient and have been evaluated in two ways. First, we measure the stability of curve fragments when images undergo visual transformations such as change in viewpoints, illumination, and noise, a critical factor for curve fragments to be useful to later visual processes but one often ignored in evaluation. Second, we use a more traditional comparison against human annotation, but using the CFGD dataset and CFGD evaluation strategy rather than the standard BSDS counterpart, which is shown to be not appropriate for evaluating curve fragments. Under both evaluation schemes our results are significantly better than those state of the art algorithms whose implementations are publicly available."'),
('"A Multi-stage Linear Approach to Structure from Motion"', '"ECCV 2010"', '["Bundle Adjustment", "Reprojection Error", "Omnidirectional Image", "Camera Rotation", "Camera Pair', '"https://doi.org/10.1007/978-3-642-35740-4_21"', '"We present a new structure from motion (Sfm) technique based on point and vanishing point (VP) matches in images. First, all global camera rotations are computed from VP matches as well as relative rotation estimates obtained from pairwise image matches. A new multi-staged linear technique is then used to estimate all camera translations and 3D points simultaneously. The proposed method involves first performing pairwise reconstructions, then robustly aligning these in pairs, and finally aligning all of them globally by simultaneously estimating their unknown relative scales and translations. In doing so, measurements inconsistent in three views are efficiently removed. Unlike sequential Sfm, the proposed method treats all images equally, is easy to parallelize and does not require intermediate bundle adjustments. There is also a reduction of drift and significant speedups up to two order of magnitude over sequential Sfm. We compare our method with a standard Sfm pipeline [1] and demonstrate that our linear estimates are accurate on a variety of datasets, and can serve as good initializations for final bundle adjustment. Because we exploit VPs when available, our approach is particularly well-suited to the reconstruction of man-made scenes."'),
('"A Multi-transformational Model for Background Subtraction with Moving Cameras"', '"ECCV 2014"', '["Background subtraction", "moving camera", "moving object detection"]', '"https://doi.org/10.1007/978-3-319-10590-1_52"', '"We introduce a new approach to perform background subtraction in moving camera scenarios. Unlike previous treatments of the problem, we do not restrict the camera motion or the scene geometry. The proposed approach relies on Bayesian selection of the transformation that best describes the geometric relation between consecutive frames. Based on the selected transformation, we propagate a set of learned background and foreground appearance models using a single or a series of homography transforms. The propagated models are subjected to MAP-MRF optimization framework that combines motion, appearance, spatial, and temporal cues; the optimization process provides the final background/foreground labels. Extensive experimental evaluation with challenging videos shows that the proposed method outperforms the baseline and state-of-the-art methods in most cases."'),
('"A Multiview Approach to Tracking People in Crowded Scenes Using a Planar Homography Constraint"', '"ECCV 2006"', '["Ground Plane", "Foreground Object", "Foreground Region", "Ground Location", "Dense Crowd"]', '"https://doi.org/10.1007/11744085_11"', '"Occlusion and lack of visibility in dense crowded scenes make it very difficult to track individual people correctly and consistently. This problem is particularly hard to tackle in single camera systems. We present a multi-view approach to tracking people in crowded scenes where people may be partially or completely occluding each other. Our approach is to use multiple views in synergy so that information from all views is combined to detect objects. To achieve this we present a novel planar homography constraint to resolve occlusions and robustly determine locations on the ground plane corresponding to the feet of the people. To find tracks we obtain feet regions over a window of frames and stack them creating a space time volume. Feet regions belonging to the same person form contiguous spatio-temporal regions that are clustered using a graph cuts segmentation approach. Each cluster is the track of a person and a slice in time of this cluster gives the tracked location. Experimental results are shown in scenes of dense crowds where severe occlusions are quite common. The algorithm is able to accurately track people in all views maintaining correct correspondences across views. Our algorithm is ideally suited for conditions when occlusions between people would seriously hamper tracking performance or if there simply are not enough features to distinguish between different people."'),
('"A Naturalistic Open Source Movie for Optical Flow Evaluation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_44"', '"Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a new optical flow data set derived from the open source 3D animated short film Sintel. This data set has important features not present in the popular Middlebury flow evaluation: long sequences, large motions, specular reflections, motion blur, defocus blur, and atmospheric effects. Because the graphics data that generated the movie is open source, we are able to render scenes under conditions of varying complexity to evaluate where existing flow algorithms fail. We evaluate several recent optical flow algorithms and find that current highly-ranked methods on the Middlebury evaluation have difficulty with this more complex data set suggesting further research on optical flow estimation is needed. To validate the use of synthetic data, we compare the image- and flow-statistics of Sintel to those of real films and videos and show that they are similar. The data set, metrics, and evaluation website are publicly available."'),
('"A New 3-D Model Retrieval System Based on Aspect-Transition Descriptor"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_42"', '"In this paper, we propose a new 3-D model retrieval system using the Aspect-Transition Descriptor which is based on the aspect graph representation [1,2] approach. The proposed method differs from the conventional aspect graph representation in that we utilize transitions as well as aspects. The process of generating the Aspect-Transition Descriptor is as follows: First, uniformly sampled views of a 3-D model are separated into a stable and an unstable view sets according to the local variation of their 2-D shape. Next, adjacent stable views and unstable views are grouped into clusters and we select the characteristic aspects and transitions by finding the representative view from each cluster. The 2-D descriptors of the selected characteristic aspects and transitions are concatenated to form the 3-D descriptor. Matching the Aspect-Transition Descriptors is done using a modified Hausdorff distance. To evaluate the proposed 3-D descriptor, we have evaluated the retrieval performance on the Princeton benchmark database [3] and found that our method outperforms other retrieval techniques."'),
('"A New Algorithmic Approach for Contrast Enhancement"', '"ECCV 2010"', '["Transfer Function", "Contrast Enhancement", "Gray Level", "Histogram Equalization", "Gamma Correct', '"https://doi.org/10.1007/978-3-642-15567-3_26"', '"A novel algorithmic approach for optimal contrast enhancement is proposed. A measure of expected contrast and a sister measure of tone subtlety are defined for gray level transform functions. These definitions allow us to depart from the current practice of histogram equalization and formulate contrast enhancement as a problem of maximizing the expected contrast measure subject to a limit on tone distortion and possibly other constraints that suppress artifacts. The resulting contrast-tone optimization problem can be solved efficiently by linear programming. The proposed constrained optimization framework for contrast enhancement is general, and the user can add and fine tune the constraints to achieve desired visual effects. Experimental results demonstrate clearly superior performance of the new technique over histogram equalization."'),
('"A New Application of Smart Walker for Quantitative Analysis of Human Walking"', '"ECCV 2014"', '["Smart walker", "Gait analysis", "Step detection", "Turning", "Elderly"]', '"https://doi.org/10.1007/978-3-319-16199-0_33"', '"This paper presents a new nonintrusive device for everyday gait analysis and health monitoring. The system is a standard rollator equipped with encoders and inertial sensors. The assisted walking of \\\\(25\\\\) healthy elderly and \\\\(23\\\\) young adults are compared to develop walking quality index. The subjects were asked to walk on a straight trajectory and an L-shaped trajectory respectively. The walking trajectory, which is missing in other gait analysis methods, is calculated based on the encoder data. The obtained trajectory and steps are compared with the results of a motion capture system. The gait analysis results show that new index obtained by using the walker measurements, and not available otherwise, are very discriminating, e.g., the elderly have larger lateral motion and maneuver area, smaller angular velocity during turning, their walking accuracy is lower and turning ability is weaker although they have almost the same walking velocity as the young people."'),
('"A New Approach on Multimodal Biometrics Based on Combining Neural Networks Using AdaBoost"', '"BioAW 2004"', '["Discrete Cosine Transform", "Weak Learner", "Verification System", "Biometric System", "Decision F', '"https://doi.org/10.1007/978-3-540-25976-3_30"', '"In this paper, we propose a new approach in the field of multimodal biometrics, based on AdaBoost. AdaBoost has been used to overcome the prob-lem of limited number of training data in unimodal systems, by combining neu-ral networks as weak learners.The simplest possible neural network, i.e. only one neuron, plays the role of a weak learner in our system. We have conducted different experiments with different number of AdaBoost iterations (experts) and input features. We compared the results of our AdaBoost based multimodal system, using features of three different unimodal systems, with the results obtained separately from these unimodal systems, i.e. a GMM based speaker veri-fication, an HMM based face verification and a SVM based face verification. It has been shown that even the average FAR of the multimodal system (%0.058) is much less than the lowest FARs of each of the unimodal systems (%0.32 for SV, %4 for HMM based face verification and %1 for SVM based face verification systems), while the average FRR of the multimodal system is acceptable (%2.1)."'),
('"A New Baseline for Image Annotation"', '"ECCV 2008"', '["Image Retrieval", "Baseline Method", "Haar Wavelet", "Image Annotation", "Annotation Method"]', '"https://doi.org/10.1007/978-3-540-88690-7_24"', '"Automatically assigning keywords to images is of great interest as it allows one to index, retrieve, and understand large collections of image data. Many techniques have been proposed for image annotation in the last decade that give reasonable performance on standard datasets. However, most of these works fail to compare their methods with simple baseline techniques to justify the need for complex models and subsequent training. In this work, we introduce a new baseline technique for image annotation that treats annotation as a retrieval problem. The proposed technique utilizes low-level image features and a simple combination of basic distances to find nearest neighbors of a given image. The keywords are then assigned using a greedy label transfer mechanism. The proposed baseline outperforms the current state-of-the-art methods on two standard and one large Web dataset. We believe that such a baseline measure will provide a strong platform to compare and better understand future annotation techniques."'),
('"A New Biologically Inspired Color Image Descriptor"', '"ECCV 2012"', '["image descriptor", "color", "Hmax", "sift", "bag-of-words", "gist", "object recognition", "scene c', '"https://doi.org/10.1007/978-3-642-33715-4_23"', '"We describe a novel framework for the joint processing of color and shape information in natural images. A hierarchical non-linear spatio-chromatic operator yields spatial and chromatic opponent channels, which mimics processing in the primate visual cortex. We extend two popular object recognition systems (i.e., the Hmax hierarchical model of visual processing and a sift-based bag-of-words approach) to incorporate color information along with shape information. We further use the framework in combination with the gist algorithm for scene categorization as well as the Berkeley segmentation algorithm. In all cases, the proposed approach is shown to outperform standard grayscale/shape-based descriptors as well as alternative color processing schemes on several datasets."'),
('"A New Image Registration Technique with Free Boundary Constraints: Application to Mammography"', '"ECCV 2002"', '["Free Boundary", "Image Registration", "Free Boundary Condition", "Wrong Initialization", "Geometri', '"https://doi.org/10.1007/3-540-47979-1_36"', '"In this paper, a new image-matching mathematical model is presented for the mammogram registration. In avariational framework, an energy minimization problem is formulated and a multigrid resolution algorithm is designed. The model focuses on the matching of regions of interest. It also combines several constraints which are both intensity and segmentation based. A new feature of our model is combining region matching and segmentation by formulation of the energy minimization problem with free boundary conditions. Moreover, the energy has a new registration constraint. The performances of models with and without free boundary are compared on a simulated mammogram pair. It is shown that the new model with free boundary is more robust to initialization inaccuracies than the one without. The interest of the new model for the real mammogram registration is also illustrated."'),
('"A New Quadratic Classifier Applied to Biometric Recognition"', '"BioAW 2002"', '["Face Recognition", "Covariance Information", "Recognition Error", "Linear Discriminant Function", ', '"https://doi.org/10.1007/3-540-47917-1_19"', '"In biometric recognition applications, the number of training examples per class is limited and consequently the conventional quadratic classifier either performs poorly or cannot be calculated. Other non-conventional quadratic classifiers have been used in limited sample and high dimensional classification problems. In this paper, a new quadratic classifier called Maximum Entropy Covariance Selection (MECS) is presented. This classifier combines the sample group covariance matrices and the pooled covariance matrix under the principle of maximum entropy. This approach is a direct method that not only deals with the singularity and instability of the maximum likelihood covariance estimator, but also does not require an optimisation procedure. In order to evaluate the MECS effectiveness, experiments on face and fingerprint recognition were carried out and compared with other similar classifiers, including the Reguralized Discriminant Analysis (RDA), the Leave-One-Out Covariance estimator (LOOC) and the Simplified Quadratic Discriminant Function (SQDF). In both applications, using the publicly released databases FERET and NIST-4, the MECS classifier achieved the lowest classification error."'),
('"A New Robust Technique for Stabilizing Brightness Fluctuations in Image Sequences"', '"SMVP 2004"', '["Motion Estimation", "Graphic Hardware", "Robust Technique", "Severe Distortion", "Global Motion Es', '"https://doi.org/10.1007/978-3-540-30212-4_14"', '"Temporal random variation of luminance in images can manifest in film and video due to a wide variety of sources. Typical in archived films, it also affects scenes recorded simultaneously with different cameras (e.g. for film special effect), and scenes affected by illumination problems. Many applications in Computer Vision and Image Processing that try to match images (e.g. for motion estimation, stereo vision, etc.) have to cope with this problem. The success of current techniques for dealing with this is limited by the non-linearity of severe distortion, the presence of motion and missing data (yielding outliers in the estimation process) and the lack of fast implementations in reconfigurable systems. This paper proposes a new process for stabilizing brightness fluctuations that improves the existing models. The article also introduces a new estimation method able to cope with outliers in the joint distribution of pairs images. The system implementation is based on the novel use of general purpose PC graphics hardware. The overall system presented here is able to deal with much more severe distortion than previously was the case, and in addition can operate at 7 fps on a 1.6GHz PC with broadcast standard definition images."'),
('"A New Set of Quartic Trivariate Polynomial Equations for Stratified Camera Self-calibration under Z', '"ECCV 2012"', '["Camera Self-calibration", "Multiple View Geometry"]', '"https://doi.org/10.1007/978-3-642-33783-3_51"', '"This paper deals with the problem of self-calibrating a moving camera with constant parameters. We propose a new set of quartic trivariate polynomial equations in the unknown coordinates of the plane at infinity derived under the no-skew assumption. Our new equations allow to further enforce the constancy of the principal point across all images while retrieving the plane at infinity. Six such polynomials, four of which are independent, are obtained for each triplet of images. The proposed equations can be solved along with the so-called modulus constraints and allow to improve the performance of existing methods."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A New Variational Framework for Multiview Surface Reconstruction"', '"ECCV 2014"', '["Surface reconstruction", "surface fairing", "multiview stereo", "Gauss-Newton", "finite element me', '"https://doi.org/10.1007/978-3-319-10599-4_46"', '"The creation of surfaces from overlapping images taken from different vantages is a hard and important problem in computer vision. Recent developments fall primarily into two categories: the use of dense matching to produce point clouds from which surfaces are built, and the construction of surfaces from images directly. This paper presents a new method for surface reconstruction falling in the second category. First, a strongly motivated variational framework is built from the ground up based on a limiting case of photo-consistency. The framework includes a powerful new edge preserving smoothness term and exploits the input images exhaustively, directly yielding high quality surfaces instead of dealing with issues (such as noise or misalignment) after the fact. Numeric solution is accomplished with a combination of Gauss-Newton descent and the finite element method, yielding deep convergence in few iterates. The method is fast, robust, very insensitive to view/scene configurations, and produces state-of-the-art results in the Middlebury evaluation."'),
('"A Non-Linear Filter for Gyroscope-Based Video Stabilization"', '"ECCV 2014"', '["video stabilization", "rolling-shutter", "gyroscopes"]', '"https://doi.org/10.1007/978-3-319-10593-2_20"', '"We present a method for video stabilization and rolling-shutter correction for videos captured on mobile devices. The method uses the data from an on-board gyroscope to track the camera\\u2019s angular velocity, and can run in real time within the camera capture pipeline. We remove small motions and rolling-shutter distortions due to hand shake, creating the impression of a video shot on a tripod. For larger motions, we filter the camera\\u2019s angular velocity to produce a smooth output. To meet the latency constraints of a real-time camera capture pipeline, our filter operates on a small temporal window of three to five frames. Our algorithm performs better than the previous work that uses a gyroscope to stabilize a video stream, and at a similar level with respect to current feature-based methods."'),
('"A Non-local Method for Robust Noisy Image Completion"', '"ECCV 2014"', '["Matrix Completion", "Nuclear Norm", "Completion Problem", "Similar Patch", "Corrupted Image"]', '"https://doi.org/10.1007/978-3-319-10593-2_5"', '"The problem of noisy image completion refers to recovering an image from a random subset of its noisy intensities. In this paper, we propose a non-local patch-based algorithm to settle the noisy image completion problem following the methodology \\u201cgrouping and collaboratively filtering\\u201d. The target of \\u201cgrouping\\u201d is to form patch matrices by matching and stacking similar image patches. And the \\u201ccollaboratively filtering\\u201d is achieved by transforming the tasks of simultaneously estimating missing values and removing noises for the stacked patch matrices into low-rank matrix completion problems, which can be efficiently solved by minimizing the nuclear norm of the matrix with linear constraints. The final output is produced by synthesizing all the restored patches. To improve the robustness of our algorithm, we employ an efficient and accurate patch matching method with adaptations including pre-completion and outliers removal, etc. Experiments demonstrate that our approach achieves state-of-the-art performance for the noisy image completion problem in terms of both PSNR and subjective visual quality."'),
('"A Non-parametric Hierarchical Model to Discover Behavior Dynamics from Tracks"', '"ECCV 2012"', '["Anomaly Detection", "Latent Dirichlet Allocation", "Dynamic Time Warping", "Behavior Class", "Test', '"https://doi.org/10.1007/978-3-642-33783-3_20"', '"We present a novel non-parametric Bayesian model to jointly discover the dynamics of low-level actions and high-level behaviors of tracked people in open environments. Our model represents behaviors as Markov chains of actions which capture high-level temporal dynamics. Actions may be shared by various behaviors and represent spatially localized occurrences of a person\\u2019s low-level motion dynamics using Switching Linear Dynamics Systems. Since the model handles real-valued features directly, we do not lose information by quantizing measurements to \\u2018visual words\\u2019 and can thus discover variations in standing, walking and running without discrete thresholds. We describe inference using Gibbs sampling and validate our approach on several artificial and real-world tracking datasets. We show that our model can distinguish relevant behavior patterns that an existing state-of-the-art method for hierarchical clustering cannot."'),
('"A Novel Digitizing Pen for the Analysis of Pen Pressure and Inclination in Handwriting Biometrics"', '"BioAW 2004"', '["Feature Vector", "Handwriting Recognition", "Person Authentication", "Biometric Information", "Aut', '"https://doi.org/10.1007/978-3-540-25976-3_26"', '"In this paper we introduce a novel multi-functional digitizing pen for the verification and identification of individuals by means of handwritten signatures, text or figures (biometric smart pen, BiSP). Different from most conventional graphics tablets the device measures the kinematics and the dynamics of hand movement during the writing process by recording the pressure (Px, Py, Pz) and inclination (\\u03b1, \\u03b2) of the pen. Characteristic behavioral patterns are numerically extracted from the signals and stored in a database. Biometric test samples are compared against reference templates using a rapid feature classification and matching algorithm. A first BiSP prototype based on pressure sensors is available for presentation. Preliminary results obtained from 3D pressure signals demonstrate a remarkable potential of the system for person verification and identification, and suggest further applications in the areas of neurology and psychiatry."'),
('"A Novel Fast Method for L\\u2009\\u221e\\u2009 Problems in Multiview Geometry"', '"ECCV 2012"', '["Feasible Region", "Central Path", "Convex Problem", "Algebraic Solution", "Triangulation Problem"]', '"https://doi.org/10.1007/978-3-642-33715-4_9"', '"Optimization using the L \\u2009\\u221e\\u2009 norm is an increasingly important area in multiview geometry. Previous work has shown that globally optimal solutions can be computed reliably using the formulation of generalized fractional programming, in which algorithms solve a sequence of convex problems independently to approximate the optimal L \\u2009\\u221e\\u2009 norm error. We found the sequence of convex problems are highly related and we propose a method to derive a Newton-like step from any given point. In our method, the feasible region of the current involved convex problem is contracted gradually along with the Newton-like steps, and the updated point locates on the boundary of the new feasible region. We propose an effective strategy to make the boundary point become an interior point through one dimension augmentation and relaxation. Results are presented and compared to the state of the art algorithms on simulated and real data for some multiview geometry problems with improved performance on both runtime and Newton-like iterations."'),
('"A Novel Graph Embedding Framework for Object Recognition"', '"ECCV 2014"', '["Image classification", "Object recognition", "Graph based image representation", "Graph embedding"', '"https://doi.org/10.1007/978-3-319-16220-1_24"', '"A great deal of research works have been devoted to understand image contents. In this field many well-known methods exploit Bag of Words (BoW) features describing image contents as appearance frequency histogram of visual words. These approaches have a main drawback, the location information and the relationships between features are lost. To overcame this limitation we propose a novel methodology for the Object recognition task. A digital image is described as a feature vector computed by means of a new graph embedding paradigm on the Attributed Relational SIFT Regions Graph. The final classification is performed by using Logistic Label Propagation classifier. Our framework is evaluated on standard databases (such as ETH-\\\\(80\\\\), COIL-\\\\(100\\\\) and ALOI) and the achieved results compared with those obtained by well-known methodologies confirm its quality."'),
('"A Novel Material-Aware Feature Descriptor for Volumetric Image Registration in Diffusion Tensor Spa', '"ECCV 2012"', '["Probability Density Function", "Feature Point", "Feature Descriptor", "Structure Tensor", "Distanc', '"https://doi.org/10.1007/978-3-642-33765-9_36"', '"This paper advocates a novel material-aware feature descriptor for volumetric image registration. We rigorously formulate a novel probability density function (PDF) based distance metric to devise a compact local feature descriptor supporting invariance of full 3D orientation and isometric deformation. The central idea is to employ anisotropic heat diffusion to characterize the detected local volumetric features. It is achieved by the elegant unification of diffusion tensor (DT) space construction based on local Hessian eigen-system, multi-scale feature extraction based on DT-weighted dyadic wavelet transform, and local distance definition based on PDF formulated in DT space. The diffusion, intrinsic structure-aware nature makes our volumetric feature descriptor more robust to noise. With volumetric images registration as verifiable application, various experiments on different volumetric images demonstrate the superiority of our descriptor."'),
('"A Novel Parameter Estimation Algorithm for the Multivariate t-Distribution and Its Application to C', '"ECCV 2010"', '["Approximate Algorithm", "Multivariate Gaussian Distribution", "Incremental Algorithm", "Scale Matr', '"https://doi.org/10.1007/978-3-642-15552-9_43"', '"We present a novel algorithm for approximating the parameters of a multivariate t-distribution. At the expense of a slightly decreased accuracy in the estimates, the proposed algorithm is significantly faster and easier to implement compared to the maximum likelihood estimates computed using the expectation-maximization algorithm. The formulation of the proposed algorithm also provides theoretical guidance for solving problems that are intractable with the maximum likelihood equations. In particular, we show how the proposed algorithm can be modified to give an incremental solution for fast online parameter estimation. Finally, we validate the effectiveness of the proposed algorithm by using the approximated t-distribution as a drop in replacement for the conventional Gaussian distribution in two computer vision applications: object recognition and tracking. In both cases the t-distribution gives better performance with no increase in computation."'),
('"A Novel Topic-Level Random Walk Framework for Scene Image Co-segmentation"', '"ECCV 2014"', '["Image co-segmentation", "voting", "random walk", "link analysis"]', '"https://doi.org/10.1007/978-3-319-10590-1_45"', '"Image co-segmentation is popular with its ability to detour supervisory data by exploiting the common information in multiple images. In this paper, we aim at a more challenging branch called scene image co-segmentation, which jointly segments multiple images captured from the same scene into regions corresponding to their respective classes. We first put forward a novel representation named Visual Relation Network (VRN) to organize multiple segments, and then search for meaningful segments for every image through voting on the network. Scalable topic-level random walk is then used to solve the voting problem. Experiments on the benchmark MSRC-v2, the more difficult LabelMe and SUN datasets show the superiority over the state-of-the-art methods."'),
('"A Novel Visual Word Co-occurrence Model for Person Re-identification"', '"ECCV 2014"', '["Visual Word", "Appearance Model", "Reproduce Kernel Hilbert Space", "Camera View", "Codebook Size"', '"https://doi.org/10.1007/978-3-319-16199-0_9"', '"Person re-identification aims to maintain the identity of an individual in diverse locations through different non-overlapping camera views. The problem is fundamentally challenging due to appearance variations resulting from differing poses, illumination and configurations of camera views. To deal with these difficulties, we propose a novel visual word co-occurrence model. We first map each pixel of an image to a visual word using a codebook, which is learned in an unsupervised manner. The appearance transformation between camera views is encoded by a co-occurrence matrix of visual word joint distributions in probe and gallery images. Our appearance model naturally accounts for spatial similarities and variations caused by pose, illumination & configuration change across camera views. Linear SVMs are then trained as classifiers using these co-occurrence descriptors. On the VIPeR [1] and CUHK Campus [2] benchmark datasets, our method achieves 83.86% and 85.49% at rank-15 on the Cumulative Match Characteristic (CMC) curves, and beats the state-of-the-art results by 10.44% and 22.27%."'),
('"A Novel Wearable System for Capturing User View Images"', '"CVHCI 2004"', '["Audio Data", "Hand Gesture Recognition", "Wearable Computer", "Wearable System", "Wearable Camera"', '"https://doi.org/10.1007/978-3-540-24837-8_16"', '"In this paper, we propose a body attached system to capture the experience of a person in sequence as audio/visual information. The proposed system consists of two cameras (one IR (infra-red) camera and one wide-angle color camera) and a microphone. The IR camera image is used for capturing the user\\u2019s head motions. The wide-angle color camera is used for capturing frontal view images, and an image region approximately corresponding to the users\\u2019 view is selected according to the estimated human head motions. The selected image and head motion data are stored in a storage device with audio data. This system can overcome the disadvantages of systems using head-mounted cameras in terms of the ease in putting on/taking off the device and its less obtrusive visual impact on third persons. Using the proposed system, we can record audio data, images in the user\\u2019s view and head gestures (nodding, shaking, etc.) simultaneously. These data contain significant information for recording/analyzing human activities and can be used in wider application domains (such as a digital diary or interaction analysis). Experimental results show the effectiveness of the proposed system."'),
('"A Parameterless Line Segment and Elliptical Arc Detector with Enhanced Ellipse Fitting"', '"ECCV 2012"', '["ellipse detection", "a contrario approach", "ellipse fitting"]', '"https://doi.org/10.1007/978-3-642-33709-3_41"', '"We propose a combined line segment and elliptical arc detector, which formally guarantees the control of the number of false positives and requires no parameter tuning. The accuracy of the detected elliptical features is improved by using a novel non-iterative ellipse fitting technique, which merges the algebraic distance with the gradient orientation. The performance of the detector is evaluated on computer-generated images and on natural images."'),
('"A Particle Filter Framework for Contour Detection"', '"ECCV 2012"', '["Particle Filter", "Hide State", "Transition Distribution", "Contour Detection", "Tracking Procedur', '"https://doi.org/10.1007/978-3-642-33718-5_56"', '"We investigate the contour detection task in complex natural images. We propose a novel contour detection algorithm which locally tracks small pieces of edges called edgelets. The combination of the Bayesian modeling and the edgelets enables the use of semi-local prior information and image-dependent likelihoods. We use a mixed offline and online learning strategy to detect the most relevant edgelets. The detection problem is then modeled as a sequential Bayesian tracking task, estimated using a particle filtering technique. Experiments on the Berkeley Segmentation Datasets show that the proposed Particle Filter Contour Detector method performs well compared to competing state-of-the-art methods."'),
('"A PDE Approach for Thickness, Correspondence, and Gridding of Annular Tissues"', '"ECCV 2002"', '["Harmonic Function", "Length Function", "Seed Point", "Rectangular Grid", "Point Correspondence"]', '"https://doi.org/10.1007/3-540-47979-1_39"', '"A novel approach for computing point correspondences and grids within annular tissues is presented based on a recently introduced technique for computing thickness in such regions. The solution of Laplace\\u2019s equation provides implicit correspondence trajectories between the bounding surfaces. Pairs of partial differential equations are then efficiently solved within an Eulerian framework for thickness, from which concentric surfaces can be constructed. Point correspondences are then computed between the outer surfaces and any surface within, providing a gridding of the annular tissue. Examples are shown for two-dimensional short-axis images of the left ventricle and three-dimensional images of the cortex."'),
('"A PDE Solution of Brownian Warping"', '"ECCV 2004"', '["Thin Plate Spline", "Brownian Motion Model", "Destination Image", "Matching Constraint", "Plate Er', '"https://doi.org/10.1007/978-3-540-24673-2_15"', '"A Brownian motion model in the group of diffeomorphisms has been introduced as creating a least committed prior on warps. This prior is source destination symmetric, fulfills a natural semi-group property for warps, and with probability 1 create invertible warps. In this paper, we formulate a Partial Differential Equation for obtaining the maximum likelihood warp given matching constraints derived from the images. We solve for the free boundary conditions, and the bias toward smaller areas in the finite domain setting. Furthermore, we demonstrate the technique on 2D images, and show that the obtained warps are also in practice source-destination symmetric."'),
('"A Perceptual Comparison of Distance Measures for Color Constancy Algorithms"', '"ECCV 2008"', '["Distance Measure", "Color Space", "Human Observer", "Angular Error", "Hyperspectral Data"]', '"https://doi.org/10.1007/978-3-540-88682-2_17"', '"Color constancy is the ability to measure image features independent of the color of the scene illuminant and is an important topic in color and computer vision. As many color constancy algorithms exist, different distance measures are used to compute their accuracy. In general, these distances measures are based on mathematical principles such as the angular error and Euclidean distance. However, it is unknown to what extent these distance measures correlate to human vision."'),
('"A Physically-Based Statistical Deformable Model for Brain Image Analysis"', '"ECCV 2000"', '["Registration Error", "Deformable Model", "Brain Surface", "Training Population", "Rigid Registrati', '"https://doi.org/10.1007/3-540-45053-X_34"', '"A probabilistic deformable model for the representation of brain structures is described. The statistically learned deformable model represents the relative location of head (skull and scalp) and brain surfaces in Magnetic Resonance Images (MRIs) and accommodates their significant variability across different individuals. The head and brain surfaces of each volume are parameterized by the amplitudes of the vibration modes of a deformable spherical mesh. For a given MRI in the training set, a vector containing the largest vibration modes describing the head and the brain is created. This random vector is statistically constrained by retaining the most significant variation modes of its Karhunen-Loeve expansion on the training population. By these means, the conjunction of surfaces are deformed according to the anatomical variability observed in the training set. Two applications of the probabilistic deformable model are presented: the deformable model-based registration of 3D multimodal (MR/SPECT) brain images without removing non-brain structures and the segmentation of the brain in MRI using the probabilistic constraints embedded in the deformable model. The multi-object deformable model may be considered as a first step towards the development of a general purpose probabilistic anatomical brain atlas."'),
('"A Physically-Motivated Deformable Model Based on Fluid Dynamics"', '"ECCV 2006"', '["Image Segmentation", "Input Image", "Active Contour", "Object Boundary", "Noisy Image"]', '"https://doi.org/10.1007/11744023_39"', '"A novel deformable model for image segmentation and shape recovery is presented. The model is inspired by fluid dynamics and is based on a flooding simulation similar to the watershed paradigm. Unlike most watershed methods, our model has a continuous formulation, being described by two partial differential equations. In this model, different fluids, added by placing density (dye) sources manually or automatically, are attracted towards the contours of the objects of interest by an image force. In contrast to the watershed method, when different fluids meet they may mix. When the topographical relief of the image is flooded, the interfaces separating homogeneous fluid regions can be traced to yield the object contours. We demonstrate the flexibility and potential of our model in two experimental settings: shape recovery using manual initializations and automated segmentation."'),
('"A Polynomial-Time Metric for Attributed Trees"', '"ECCV 2004"', '["Editing Distance", "Rand Index", "Attribute Tree", "Unrooted Tree", "Pattern Recognition Letter"]', '"https://doi.org/10.1007/978-3-540-24673-2_34"', '"We address the problem of comparing attributed trees and propose a novel distance measure centered around the notion of a maximal similarity common subtree. The proposed measure is general and defined on trees endowed with either symbolic or continuous-valued attributes, and can be equally applied to ordered and unordered, rooted and unrooted trees. We prove that our measure satisfies the metric constraints and provide a polynomial-time algorithm to compute it. This is a remarkable and attractive property since the computation of traditional edit-distance-based metrics is NP-complete, except for ordered structures. We experimentally validate the usefulness of our metric on shape matching tasks, and compare it with edit-distance measures."'),
('"A Pose-Invariant Descriptor for Human Detection and Segmentation"', '"ECCV 2008"', '["Object Detection", "Image Patch", "Contour Point", "Human Detection", "Edge Orientation"]', '"https://doi.org/10.1007/978-3-540-88693-8_31"', '"We present a learning-based, sliding window-style approach for the problem of detecting humans in still images. Instead of traditional concatenation-style image location-based feature encoding, a global descriptor more invariant to pose variation is introduced. Specifically, we propose a principled approach to learning and classifying human/non-human image patterns by simultaneously segmenting human shapes and poses, and extracting articulation-insensitive features. The shapes and poses are segmented by an efficient, probabilistic hierarchical part-template matching algorithm, and the features are collected in the context of poses by tracing around the estimated shape boundaries. Histograms of oriented gradients are used as a source of low-level features from which our pose-invariant descriptors are computed, and kernel SVMs are adopted as the test classifiers. We evaluate our detection and segmentation approach on two public pedestrian datasets."'),
('"A Pot of Gold: Rainbows as a Calibration Cue"', '"ECCV 2014"', '["Focal Length", "Camera Calibration", "Photometric Stereo", "Horizon Line", "Probabilistic Principa', '"https://doi.org/10.1007/978-3-319-10602-1_53"', '"Rainbows are a natural cue for calibrating outdoor imagery. While ephemeral, they provide unique calibration cues because they are centered exactly opposite the sun and have an outer radius of 42 degrees. In this work, we define the geometry of a rainbow and describe minimal sets of constraints that are sufficient for estimating camera calibration. We present both semi-automatic and fully automatic methods to calibrate a camera using an image of a rainbow. To demonstrate our methods, we have collected a large database of rainbow images and use these to evaluate calibration accuracy and to create an empirical model of rainbow appearance. We show how this model can be used to edit rainbow appearance in natural images and how rainbow geometry, in conjunction with a horizon line and capture time, provides an estimate of camera location. While we focus on rainbows, many of the geometric properties and algorithms we present also apply to other solar-refractive phenomena, such as parhelion, often called sun dogs, and the 22 degree solar halo."'),
('"A Probabilistic Approach to Integrating Multiple Cues in Visual Tracking"', '"ECCV 2008"', '["Hide Markov Model", "Visual Tracking", "Target Representation", "Face Tracking", "Importance Funct', '"https://doi.org/10.1007/978-3-540-88688-4_17"', '"This paper presents a novel probabilistic approach to integrating multiple cues in visual tracking. We perform tracking in different cues by interacting processes. Each process is represented by a Hidden Markov Model, and these parallel processes are arranged in a chain topology. The resulting Linked Hidden Markov Models naturally allow the use of particle filters and Belief Propagation in a unified framework. In particular, a target is tracked in each cue by a particle filter, and the particle filters in different cues interact via a message passing scheme. The general framework of our approach allows a customized combination of different cues in different situations, which is desirable from the implementation point of view. Our examples selectively integrate four visual cues including color, edges, motion and contours. We demonstrate empirically that the ordering of the cues is nearly inconsequential, and that our approach is superior to other approaches such as Independent Integration and Hierarchical Integration in terms of flexibility and robustness."'),
('"A Probabilistic Approach to Large Displacement Optical Flow and Occlusion Detection"', '"SMVP 2004"', '["Probability Density Function", "Diffusion Equation", "Optical Flow", "Visibility Versus", "Smoothn', '"https://doi.org/10.1007/978-3-540-30212-4_7"', '"This paper deals with the computation of optical flow and occlusion detection in the case of large displacements. We propose a Bayesian approach to the optical flow problem and solve it by means of differential techniques. The images are regarded as noisy measurements of an underlying \\u2019true\\u2019 image-function. Additionally, the image data is considered incomplete, in the sense that we do not know which pixels from a particular image are occluded in the other images. We describe an EM-algorithm, which iterates between estimating values for all hidden quantities, and optimizing the current optical flow estimates by differential techniques. The Bayesian way of describing the problem leads to more insight in existing differential approaches, and offers some natural extensions to them. The resulting system involves less parameters and gives an interpretation to the remaining ones. An important new feature is the photometric detection of occluded pixels. We compare the algorithm with existing optical flow methods on ground truth data. The comparison shows that our algorithm generates the most accurate optical flow estimates. We further illustrate the approach with some challenging real-world examples."'),
('"A Probabilistic Approach to Robust Matrix Factorization"', '"ECCV 2012"', '["Online Algorithm", "Laplace Distribution", "Structure From Motion", "Nuclear Norm", "Computer Visi', '"https://doi.org/10.1007/978-3-642-33786-4_10"', '"Matrix factorization underlies a large variety of computer vision applications. It is a particularly challenging problem for large-scale applications and when there exist outliers and missing data. In this paper, we propose a novel probabilistic model called Probabilistic Robust Matrix Factorization (PRMF) to solve this problem. In particular, PRMF is formulated with a Laplace error and a Gaussian prior which correspond to an \\u21131 loss and an \\u21132 regularizer, respectively. For model learning, we devise a parallelizable expectation-maximization (EM) algorithm which can potentially be applied to large-scale applications. We also propose an online extension of the algorithm for sequential data to offer further scalability. Experiments conducted on both synthetic data and some practical computer vision applications show that PRMF is comparable to other state-of-the-art robust matrix factorization methods in terms of accuracy and outperforms them particularly for large data matrices."'),
('"A Probabilistic Background Model for Tracking"', '"ECCV 2000"', '["Partition Function", "Hide Markov Model", "Importance Sampling", "Observation Window", "Shadow Reg', '"https://doi.org/10.1007/3-540-45053-X_22"', '"A new probabilistic background model based on a Hidden Markov Model is presented. The hidden states of the model enable discrimination between foreground, background and shadow. This model functions as a low level process for a car tracker. A particle filter is employed as a stochastic filter for the car tracker. The use of a particle filter allows the incorporation of the information from the low level process via importance sampling. A novel observation density for the particle filter which models the statistical dependence of neighboring pixels based on a Markov random field is presented. The effectiveness of both the low level process and the observation likelihood are demonstrated."'),
('"A Probabilistic Cascade of Detectors for Individual Object Recognition"', '"ECCV 2008"', '["False Alarm", "Test Image", "Test Feature", "Model Vote", "Candidate Match"]', '"https://doi.org/10.1007/978-3-540-88690-7_32"', '"A probabilistic system for recognition of individual objects is presented. The objects to recognize are composed of constellations of features, and features from a same object share the common reference frame of the image in which they are detected. Features appearance and pose are modeled by probabilistic distributions, the parameters of which are shared across features in order to allow training from few examples."'),
('"A Probabilistic Derivative Measure Based on the Distribution of Intensity Difference"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_11"', '"In this paper, we propose a novel derivative measure based on the probability of intensity difference that is defined by observed intensities and their true intensities. Because the true intensity cannot be estimated accurately only using two observed intensities, the probability is marginalized to consider an entire set of possible true values. The proposed measure not only considers intensity dependent noise effectively using a distribution of intensity difference, but also computes the relevant difference of two corresponding pixels that have different true intensities by extending the same intensity assumption in previous works. Using the proposed measure, the estimation result of image derivative for synthetic noisy signals is closer to the ground truth than most of previous measures. We apply the proposed measure for block matching and corner detection that compute intensity similarity in the temporal domain and image derivative in the spatial domain, respectively."'),
('"A Probabilistic Framework for Correspondence and Egomotion"', '"WDV 2006"', '["Feature Point", "Probabilistic Framework", "Rotational Error", "Epipolar Line", "Independent Motio', '"https://doi.org/10.1007/978-3-540-70932-9_18"', '"This paper is an argument for two assertions: First, that by representing correspondence probabilistically, drastically more correspondence information can be extracted from images. Second, that by increasing the amount of correspondence information used, more accurate egomotion estimation is possible. We present a novel approach illustrating these principles."'),
('"A Probabilistic Framework for Spatio-Temporal Video Representation & Indexing"', '"ECCV 2002"', '["Feature Space", "Video Sequence", "Gaussian Mixture Model", "Video Content", "Minimum Description ', '"https://doi.org/10.1007/3-540-47979-1_31"', '"In this work we describe a novel statistical video representation and modeling scheme. Video representation schemes are needed to enable segmenting a video stream into meaningful video-objects, useful for later indexing and retrieval applications. In the proposed methodology, unsupervised clustering via Guassian mixture modeling extracts coherent space-time regions in feature space, and corresponding coherent segments (video-regions) in the video content. A key feature of the system is the analysis of video input as a single entity as opposed to a sequence of separate frames. Space and time are treated uniformly. The extracted space-time regions allow for the detection and recognition of video events. Results of segmenting video content into static vs. dynamic video regions and video content editing are presented."'),
('"A Probabilistic Interpretation of the Saliency Network"', '"ECCV 2000"', '["Feature Point", "Length Distribution", "Human Visual System", "Perceptual Organization", "Probabil', '"https://doi.org/10.1007/3-540-45053-X_17"', '"The calculation of salient structures is one of the early and basic ideas of perceptual organization in Computer Vision. Saliency algorithms aim to find image curves, maximizing some deterministic quality measure which grows with the length of the curve, its smoothness, and its continuity. This note proposes a modified saliency estimation mechanism, which is based on probabilistically specified grouping cues and on length estimation. In the context of the proposed method, the wellknown saliency mechanism, proposed by Shaashua and Ullman [SU88], may be interpreted as a process trying to detect the curve with maximal expected length."'),
('"A Probabilistic Multi-scale Model for Contour Completion Based on Image Statistics"', '"ECCV 2002"', '["Information Gain", "Natural Image", "Prior Model", "Coarse Scale", "Edge Pixel"]', '"https://doi.org/10.1007/3-540-47969-4_21"', '"We derive a probabilistic multi-scale model for contour completion based on image statistics. The boundaries of human segmented images are used as \\u201cground truth\\u201d. A probabilistic formulation of contours demands a prior model and a measurement model. From the image statistics of boundary contours, we derive both the prior model of contour shape and the local likelihood model of image measurements. We observe multi-scale phenomena in the data, and accordingly propose a higher-order Markov model over scales for the contour continuity prior. Various image cues derived from orientation energy are evaluated and incorporated into the measurement model. Based on these models, we have designed a multi-scale algorithm for contour completion, which exploits both contour continuity and texture. Experimental results are shown on a wide range of images."'),
('"A Probabilistic Sensor for the Perception and the Recognition of Activities"', '"ECCV 2000"', '["Hide Markov Model", "Recognition Rate", "Activity Recognition", "Probabilistic Sensor", "Signal De', '"https://doi.org/10.1007/3-540-45054-8_32"', '"This paper presents a new technique for the perception and recognition of activities using statistical descriptions of their spatiotemporal properties. A set of motion energy receptive fields is designed in order to sample the power spectrum of a moving texture. Their structure relates to the spatio-temporal energy models of Adelson and Bergen where measures of local visual motion information are extracted by comparing the outputs of a triad of Gabor energy filters. Then the probability density function required for Bayes rule is estimated for each class of activity by computing multi-dimensional histograms from the outputs from the set of receptive fields. The perception of activities is achieved according to Bayes rule. The result at each instant of time is the map of the conditional probabilities that each pixel belongs to each one of the activities of the training set. Since activities are perceived over a short integration time, a temporal analysis of outputs is done using Hidden Markov Models."'),
('"A Probabilistic Theory of Occupancy and Emptiness"', '"ECCV 2002"', '["Visibility State", "Probabilistic Dependency", "Occupancy Probability", "Fair Sample", "Input Phot', '"https://doi.org/10.1007/3-540-47977-5_8"', '"This paper studies the inference of 3D shape from a set of n noisy photos. We derive a probabilistic framework to specify what one can infer about 3D shape for arbitrarily-shaped, Lambertian scenes and arbitrary viewpoint configurations. Based on formal definitions of visibility, occupancy, emptiness, and photo-consistency, the theoretical development yields a formulation of the Photo Hull Distribution, the tightest probabilistic bound on the scene\\u2019s true shape that can be inferred from the photos. We show how to (1) express this distribution in terms of image measurements, (2) represent it compactly by assigning an occupancy probability to each point in space, and (3) design a stochastic reconstruction algorithm that draws fair samples (i.e., 3D photo hulls) from it. We also present experimental results for complex 3D scenes."'),
('"A Pseudo-Metric for Weighted Point Sets"', '"ECCV 2002"', '["pseudo-metric", "weighted point set", "shape recognition", "indexing", "triangle inequality"]', '"https://doi.org/10.1007/3-540-47977-5_47"', '"We derive a pseudo-metric for weighted point sets. There are numerous situations, for example in the shape description domain, where the individual points in a feature point set have an associated attribute, a weight. A distance function that incorporates this extra information apart from the points\\u2019 position can be very useful for matching and retrieval purposes. There are two main approaches to do this. One approach is to interpret the point sets as fuzzy sets. However, a distance measure for fuzzy sets that is a metric, invariant under rigid motion and respects scaling of the underlying ground distance, does not exist. In addition, a Hausdorff-like pseudo-metric fails to differentiate between fuzzy sets with arbitrarily different maximum membership values. The other approach is the Earth Mover\\u2019s Distance. However, for sets of unequal total weights, it gives zero distance for arbitrarily different sets, and does not obey the triangle inequality. In this paper we derive a distance measure, based on weight transportation, that is invariant under rigid motion, respects scaling, and obeys the triangle inequality, so that it can be used in efficient database searching. Moreover, our pseudo-metric identifies only weight-scaled versions of the same set. We demonstrate its potential use by testing it on two different collections, one of company logos and another one of fish contours."'),
('"A QCQP Approach to Triangulation"', '"ECCV 2012"', '["Singular Value Decomposition", "Camera Center", "Global Optimality Condition", "Epipolar Constrain', '"https://doi.org/10.1007/978-3-642-33718-5_47"', '"Triangulation of a three-dimensional point from n\\u2009\\u2265\\u20092 two-dimensional images can be formulated as a quadratically constrained quadratic program. We propose an algorithm to extract candidate solutions to this problem from its semidefinite programming relaxations. We then describe a sufficient condition and a polynomial time test for certifying when such a solution is optimal. This test has no false positives. Experiments indicate that false negatives are rare, and the algorithm has excellent performance in practice. We explain this phenomenon in terms of the geometry of the triangulation problem."'),
('"A Rao-Blackwellized Parts-Constellation Tracker"', '"WDV 2006"', '["Training Image", "Motion Model", "Shape Model", "Appearance Model", "Image Registration Technique"', '"https://doi.org/10.1007/978-3-540-70932-9_14"', '"We present a method for efficiently tracking objects represented as constellations of parts by integrating out the shape of the model. Parts-based models have been successfully applied to object recognition and tracking. However, the high dimensionality of such models present an obstacle to traditional particle filtering approaches. We can efficiently use parts-based models in a particle filter by applying Rao-Blackwellization to integrate out continuous parameters such as shape. This allows us to maintain multiple hypotheses for the pose of an object without the need to sample in the high-dimensional spaces in which parts-based models live. We present experimental results for a challenging biological tracking task."'),
('"A Real-Time Scene Text to Speech System"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_66"', '"An end-to-end real-time scene text localization and recognition method is demonstrated. The method localizes textual content in images, a video or a webcam stream, performs character recognition (OCR) and \\u201creads\\u201d it out loud using a text-to-speech engine. The method has been recently published, achieves state-of-the-art results on public datasets and is able to recognize different fonts and scripts including non-latin ones."'),
('"A Real-Time System for Head Tracking and Pose Estimation"', '"ECCV 2010"', '["Support Vector Regression", "Ridge Regression", "Active Appearance Model", "Face Tracking", "Head ', '"https://doi.org/10.1007/978-3-642-35749-7_26"', '"Driver\\u2019s visual attention provides important clues about his/ her activities and awareness. To monitor driver\\u2019s awareness, this paper proposes a real-time person-independent head tracking and pose estimation system using a monochromatic camera. The tracking and head-pose estimation tasks are formulated as regression problems. Three regression methods are proposed: (i) individual mapping on images for head tracking, (ii) direct mapping to subspace for head tracking, which predicts a subspace from one sample, and (iii) semantic piecewise regression for head-pose estimation. The approaches are evaluated on standard databases, and on several videos collected in vehicle environments."'),
('"A Rectilinearity Measurement for Polygons"', '"ECCV 2002"', '["Shape", "polygons", "rectilinearity", "measurement"]', '"https://doi.org/10.1007/3-540-47967-8_50"', '"In this paper we define a function \\\\( \\\\mathcal{R} \\\\) (P) which is defined for any polygon P and which maps a given polygon P into a number from the interval (0, 1]. The number \\\\( \\\\mathcal{R} \\\\) (P) can be used as an estimate of the rectilinearity of P. The mapping \\\\( \\\\mathcal{R} \\\\) (P) has the following desirable properties: any polygon P has the estimated rectilinearity \\\\( \\\\mathcal{R} \\\\) (P) which is a number from (0,1]; \\\\( \\\\mathcal{R} \\\\)(P)=1 if and only if P is a rectilinear polygon, i.e., all interior angles of P belong to the set \\u03c0/2, 3\\u03c0/2; inf \\\\( \\\\mathcal{R} \\\\) (P) = 0, where \\u03a0 denotes the set of all polygons; p\\u2208II a polygon\\u2019s rectilinearity measure is invariant under similarity transformations."'),
('"A Reflective Symmetry Descriptor"', '"ECCV 2002"', '["Shape Descriptor", "North Pole", "Voxel Model", "Symmetry Detection", "Voxel Grid"]', '"https://doi.org/10.1007/3-540-47967-8_43"', '"Computing reflective symmetries of 2D and 3D shapes is a classical problem in computer vision and computational geometry. Most prior work has focused on finding the main axes of symmetry, or determining that none exists. In this paper, we introduce a new reflective symmetry descriptor that represents a measure of reflective symmetry for an arbitrary 3D voxel model for all planes through the model\\u2019s center of mass (even if they are not planes of symmetry). The main benefits of this new shape descriptor are that it is defined over a canonical parameterization (the sphere) and describes global properties of a 3D shape. Using Fourier methods, our algorithm computes the symmetry descriptor in O(N 4 log N) time for an N \\u00d7 N \\u00d7 N voxel grid, and computes a multiresolution approximation in O(N 3 log N) time. In our initial experiments, we have found the symmetry descriptor to be useful for registration, matching, and classification of shapes."'),
('"A Robust Algorithm for Characterizing Anisotropic Local Structures"', '"ECCV 2004"', '["Robust Algorithm", "Shift Vector", "Validation Framework", "Shift Procedure", "Robust Statistical ', '"https://doi.org/10.1007/978-3-540-24670-1_42"', '"This paper proposes a robust estimation and validation framework for characterizing local structures in a positive multi-variate continuous function approximated by a Gaussian-based model. The new solution is robust against data with large deviations from the model and margin-truncations induced by neighboring structures. To this goal, it unifies robust statistical estimation for parametric model fitting and multi-scale analysis based on continuous scale-space theory. The unification is realized by formally extending the mean shift-based density analysis towards continuous signals whose local structure is characterized by an anisotropic fully-parameterized covariance matrix. A statistical validation method based on analyzing residual error of the chi-square fitting is also proposed to complement this estimation framework. The strength of our solution is the aforementioned robustness. Experiments with synthetic 1D and 2D data clearly demonstrate this advantage in comparison with the \\u03b3-normalized Laplacian approach [12] and the standard sample estimation approach [13, p.179]. The new framework is applied to 3D volumetric analysis of lung tumors. A 3D implementation is evaluated with high-resolution CT images of 14 patients with 77 tumors, including 6 part-solid or ground-glass opacity nodules that are highly non-Gaussian and clinically significant. Our system accurately estimated 3D anisotropic spread and orientation for 82% of the total tumors and also correctly rejected all the failures without any false rejection and false acceptance. This system processes each 32-voxel volume-of-interest by an average of two seconds with a 2.4GHz Intel CPU. Our framework is generic and can be applied for the analysis of blob-like structures in various other applications."'),
('"A Robust and Efficient Doubly Regularized Metric Learning Approach"', '"ECCV 2012"', '["Regularized metric learning", "boosting", "PSD matrix"]', '"https://doi.org/10.1007/978-3-642-33765-9_46"', '"A proper distance metric is fundamental in many computer vision and pattern recognition applications such as classification, image retrieval, face recognition and so on. However, it is usually not clear what metric is appropriate for specific applications, therefore it becomes more reliable to learn a task oriented metric. Over the years, many metric learning approaches have been reported in literature. A typical one is to learn a Mahalanobis distance which is parameterized by a positive semidefinite (PSD) matrix M. An efficient method of estimating M is to treat M as a linear combination of rank-one matrices that can be learned using a boosting type approach. However, such approaches have two main drawbacks. First, the weight change across the training samples may be non-smooth. Second, the learned rank-one matrices might be redundant. In this paper, we propose a doubly regularized metric learning algorithm, termed by DRMetric, which imposes two regularizations on the conventional metric learning method. First, a regularization is applied on the weight of the training examples, which prevents unstable change of the weights and also prevents outlier examples from being weighed too much. Besides, a regularization is applied on the rank-one matrices to make them independent. This greatly reduces the redundancy of the rank-one matrices. We present experiments depicting the performance of the proposed method on a variety of datasets for various applications."'),
('"A Robust and Scalable Approach to Face Identification"', '"ECCV 2010"', '["Face Recognition", "Recognition Rate", "Local Binary Pattern", "Partial Little Square Regression",', '"https://doi.org/10.1007/978-3-642-15567-3_35"', '"The problem of face identification has received significant attention over the years. For a given probe face, the goal of face identification is to match this unknown face against a gallery of known people. Due to the availability of large amounts of data acquired in a variety of conditions, techniques that are both robust to uncontrolled acquisition conditions and scalable to large gallery sizes, which may need to be incrementally built, are challenges. In this work we tackle both problems. Initially, we propose a novel approach to robust face identification based on Partial Least Squares (PLS) to perform multi-channel feature weighting. Then, we extend the method to a tree-based discriminative structure aiming at reducing the time required to evaluate novel probe samples. The method is evaluated through experiments on FERET and FRGC datasets. In most of the comparisons our method outperforms state-of-art face identification techniques. Furthermore, our method presents scalability to large datasets."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A Robust PCA Algorithm for Building Representations from Panoramic Images"', '"ECCV 2002"', '["Principal Axis", "Principle Component Analysis", "Training Image", "Reconstruction Error", "Expect', '"https://doi.org/10.1007/3-540-47979-1_51"', '"Appearance-based modeling of objects and scenes using PCA has been successfully applied in many recognition tasks. Robust methods which have made the recognition stage less susceptible to outliers, occlusions, and varying illumination have further enlarged the domain of applicability. However, much less research has been done in achieving robustness in the learning stage. In this paper, we propose a novel robust PCA method for obtaining a consistent subspace representation in the presence of outlying pixels in the training images. The method is based on the EM algorithm for estimation of principal subspaces in the presence of missing data. By treating the outlying points as missing pixels, we arrive at a robust PCA representation. We demonstrate experimentally that the proposed method is efficient. In addition, we apply the method to a set of panoramic images to build a representation that enables surveillance and view-based mobile robot localization."'),
('"A Robust Probabilistic Estimation Framework for Parametric Image Models"', '"ECCV 2004"', '["Image Model", "Range Image", "Recursive Little Square", "Less Square Estimator", "Kernel Density E', '"https://doi.org/10.1007/978-3-540-24670-1_39"', '"Models of spatial variation in images are central to a large number of low-level computer vision problems including segmentation, registration, and 3D structure detection. Often, images are represented using parametric models to characterize (noise-free) image variation, and, additive noise. However, the noise model may be unknown and parametric models may only be valid on individual segments of the image. Consequently, we model noise using a nonparametric kernel density estimation framework and use a locally or globally linear parametric model to represent the noise-free image pattern. This results in a novel, robust, redescending, M- parameter estimator for the above image model which we call the Kernel Maximum Likelihood estimator (KML). We also provide a provably convergent, iterative algorithm for the resultant optimization problem. The estimation framework is empirically validated on synthetic data and applied to the task of range image segmentation."'),
('"A Robust Vision-Based Framework for Screen Readers"', '"ECCV 2014"', '["Computer vision", "Screen reader", "Visually impaired", "Sensory substitution"]', '"https://doi.org/10.1007/978-3-319-16199-0_39"', '"With the increasingly rich display of media on the Internet, screen reading technology that mainly considers website source code can become ineffective. We aim to present a solution that remains robust in the face of dynamically displayed web content, regardless of the underlying web framework. To do this, we consider techniques used in computer vision to determine semantic information about the web pages. We consider existing screen reading technologies to see where such techniques can help, and discuss our analytical model to show how this approach can benefit low vision users."'),
('"A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration"', '"ECCV 2014"', '["Visual Tracking", "Correlation Filter", "Kernel Learning"]', '"https://doi.org/10.1007/978-3-319-16181-5_18"', '"Although the correlation filter-based trackers achieve the competitive results both on accuracy and robustness, there is still a need to improve the overall tracking capability. In this paper, we presented a very appealing tracker based on the correlation filter framework. To tackle the problem of the fixed template size in kernel correlation filter tracker, we suggest an effective scale adaptive scheme. Moreover, the powerful features including HoG and color-naming are integrated together to further boost the overall tracking performance. The extensive empirical evaluations on the benchmark videos and VOT 2014 dataset demonstrate that the proposed tracker is very promising for the various challenging scenarios. Our method successfully tracked the targets in about 72% videos and outperformed the state-of-the-art trackers on the benchmark dataset with 51 sequences."'),
('"A Secure Protocol for Data Hiding in Compressed Fingerprint Images"', '"BioAW 2004"', '["Random Number Generator", "Image Compression", "Wavelet Packet", "Secure Protocol", "Data Hiding"]', '"https://doi.org/10.1007/978-3-540-25976-3_19"', '"Data hiding techniques can be employed to enhance the security of biometrics-based authentication systems. In our previous work, we proposed a method to hide challenge responses in a WSQ-compressed fingerprint image. In this paper, we extend the work to analyze the security holes in the data hiding method and enhance the hiding technique to thwart attacks on the system. We employ several proven IT security techniques to provide a secure method of hiding responses in fingerprint images. The technique presented here can be used in hiding responses in other biometrics with minor changes to suit the domain."'),
('"A Segmentation Based Variational Model for Accurate Optical Flow Estimation"', '"ECCV 2008"', '["Optical Flow", "Motion Estimation", "Stereo Match", "Color Constancy", "Motion Segmentation"]', '"https://doi.org/10.1007/978-3-540-88682-2_51"', '"Segmentation has gained in popularity in stereo matching. However, it is not trivial to incorporate it in optical flow estimation due to the possible non-rigid motion problem. In this paper, we describe a new optical flow scheme containing three phases. First, we partition the input images and integrate the segmentation information into a variational model where each of the segments is constrained by an affine motion. Then the errors brought in by segmentation are measured and stored in a confidence map. The final flow estimation is achieved through a global optimization phase that minimizes an energy function incorporating the confidence map. Extensive experiments show that the proposed method not only produces quantitatively accurate optical flow estimates but also preserves sharp motion boundaries, which makes the optical flow result usable in a number of computer vision applications, such as image/video segmentation and editing."'),
('"A Selective Weighted Late Fusion for Visual Concept Recognition"', '"ECCV 2012"', '["Visual concept recognition", "multimodality", "feature fusion"]', '"https://doi.org/10.1007/978-3-642-33885-4_43"', '"We propose in this paper a novel multimodal approach to automatically predict the visual concepts of images through an effective fusion of visual and textual features. It relies on a Selective Weighted Late Fusion (SWLF) scheme which, in optimizing an overall Mean interpolated Average Precision (MiAP), learns to automatically select and weight the best experts for each visual concept to be recognized. Experiments were conducted on the MIR Flickr image collection within the ImageCLEF 2011 Photo Annotation challenge. The results have brought to the fore the effectiveness of SWLF as it achieved a MiAP of 43.69 % for the detection of the 99 visual concepts which ranked 2 nd out of the 79 submitted runs, while our new variant of SWLF allows to reach a MiAP of 43.93 %."'),
('"A Shrinkage Learning Approach for Single Image Super-Resolution with Overcomplete Representations"', '"ECCV 2010"', '["Sparse Representation", "Noisy Image", "Shrinkage Function", "Overcomplete Representation", "Image', '"https://doi.org/10.1007/978-3-642-15552-9_45"', '"We present a novel approach for online shrinkage functions learning in single image super-resolution. The proposed approach leverages the classical Wavelet Shrinkage denoising technique where a set of scalar shrinkage functions is applied to the wavelet coefficients of a noisy image. In the proposed approach, a unique set of learned shrinkage functions is applied to the overcomplete representation coefficients of the interpolated input image. The super-resolution image is reconstructed from the post-shrinkage coefficients. During the learning stage, the low-resolution input image is treated as a reference high-resolution image and a super-resolution reconstruction process is applied to a scaled-down version of it. The shapes of all shrinkage functions are jointly learned by solving a Least Squares optimization problem that minimizes the sum of squared errors between the reference image and its super-resolution approximation. Computer simulations demonstrate superior performance compared to state-of-the-art results."'),
('"A Simple Solution to the Six-Point Two-View Focal-Length Problem"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_16"', '"This paper presents a simple and practical solution to the 6-point 2-view focal-length estimation problem. Based on the hidden-variable technique we have derived a 15th degree polynomial in the unknown focal-length. During this course, a simple and constructive algorithm is established. To make use of multiple redundant measurements and then select the best solution, we suggest a kernel-voting scheme. The algorithm has been tested on both synthetic data and real images. Satisfactory results are obtained for both cases. For reference purpose we include our Matlab implementation in the paper, which is quite concise, consisting of 20 lines of code only. The result of this paper will make a small but useful module in many computer vision systems."'),
('"A Six Point Solution for Structure and Motion"', '"ECCV 2000"', '["Image Point", "Singular Vector", "Geometric Error", "Point Solution", "Bundle Adjustment"]', '"https://doi.org/10.1007/3-540-45054-8_41"', '"The paper has two main contributions: The first is a set of methods for computing structure and motion for m\\u2265 3 views of 6 points. It is shown that a geometric image error can be minimized over all views by a simple three parameter numerical optimization. Then, that an algebraic image error can be minimized over all views by computing the solution to a cubic in one variable. Finally, a minor point, is that this \\u201cquasi-linear\\u201d linear solution enables a more concise algorithm, than any given previously, for the reconstruction of 6 points in 3 views."'),
('"A Spherical Harmonics Shape Model for Level Set Segmentation"', '"ECCV 2010"', '["Active Contour", "Shape Model", "Signed Distance Function", "Active Shape Model", "Unique Decompos', '"https://doi.org/10.1007/978-3-642-15558-1_42"', '"We introduce a segmentation framework which combines and shares advantages of both an implicit surface representation and a parametric shape model based on spherical harmonics. Besides the elegant surface representation it also inherits the power and flexibility of variational level set methods with respect to the modeling of data terms. At the same time it provides all advantages of parametric shape models such as a sparse and multiscale shape representation. Additionally, we introduce a regularizer that helps to ensure a unique decomposition into spherical harmonics and thus the comparability of parameter values of multiple segmentations. We demonstrate the benefits of our method on medical and photometric data and present two possible extensions."'),
('"A Static SMC Sampler on Shapes for the Automated Segmentation of Aortic Calcifications"', '"ECCV 2010"', '["Shape Particle", "Shape Model", "Automate Segmentation", "Importance Weight", "Target Distribution', '"https://doi.org/10.1007/978-3-642-15561-1_48"', '"In this paper, we propose a sampling-based shape segmentation method that builds upon a global shape and a local appearance model. It is suited for challenging problems where there is high uncertainty about the correct solution due to a low signal-to-noise ratio, clutter, occlusions or an erroneous model. Our method suits for segmentation tasks where the number of objects is not known a priori, or where the object of interest is invisible and can only be inferred from other objects in the image. The method was inspired by shape particle filtering from de Bruijne and Nielsen, but shows substantial improvements to it. The principal contributions of this paper are as follows: (i) We introduce statistically motivated importance weights that lead to better performance and facilitate the application to new problems. (ii) We adapt the static sequential Monte Carlo (SMC) algorithm to the problem of image segmentation, where the algorithm proves to sample efficiently from high-dimensional static spaces. (iii) We evaluate the static SMC sampler on shapes on a medical problem of high relevance: the automated quantification of aortic calcifications on X-ray radiographs for the prognosis and diagnosis of cardiovascular disease and mortality. Our results suggest that the static SMC sampler on shapes is more generic, robust, and accurate than shape particle filtering, while being computationally equally costly."'),
('"A Statistical Confidence Measure for Optical Flows"', '"ECCV 2008"', '["Optical Flow", "Confidence Measure", "Endpoint Error", "Intrinsic Dimensionality", "Optical Flow E', '"https://doi.org/10.1007/978-3-540-88690-7_22"', '"Confidence measures are crucial to the interpretation of any optical flow measurement. Even though numerous methods for estimating optical flow have been proposed over the last three decades, a sound, universal, and statistically motivated confidence measure for optical flow measurements is still missing. We aim at filling this gap with this contribution, where such a confidence measure is derived, using statistical test theory and measurable statistics of flow fields from the regarded domain. The new confidence measure is computed from merely the results of the optical flow estimator and hence can be applied to any optical flow estimation method, covering the range from local parametric to global variational approaches. Experimental results using state-of-the-art optical flow estimators and various test sequences demonstrate the superiority of the proposed technique compared to existing \\u2019confidence\\u2019 measures."'),
('"A Statistical Model for General Contextual Object Recognition"', '"ECCV 2004"', '["Partition Function", "Object Recognition", "Spatial Context", "Translation Model", "Statistical Ma', '"https://doi.org/10.1007/978-3-540-24670-1_27"', '"We consider object recognition as the process of attaching meaningful labels to specific regions of an image, and propose a model that learns spatial relationships between objects. Given a set of images and their associated text (e.g. keywords, captions, descriptions), the objective is to segment an image, in either a crude or sophisticated fashion, then to find the proper associations between words and regions. Previous models are limited by the scope of the representation. In particular, they fail to exploit spatial context in the images and words. We develop a more expressive model that takes this into account. We formulate a spatially consistent probabilistic mapping between continuous image feature vectors and the supplied word tokens. By learning both word-to-region associations and object relations, the proposed model augments scene segmentations due to smoothing implicit in spatial consistency. Context introduces cycles to the undirected graph, so we cannot rely on a straightforward implementation of the EM algorithm for estimating the model parameters and densities of the unknown alignment variables. Instead, we develop an approximate EM algorithm that uses loopy belief propagation in the inference step and iterative scaling on the pseudo-likelihood approximation in the parameter update step. The experiments indicate that our approximate inference and learning algorithm converges to good local solutions. Experiments on a diverse array of images show that spatial context considerably improves the accuracy of object recognition. Most significantly, spatial context combined with a nonlinear discrete object representation allows our models to cope well with over-segmented scenes."'),
('"A Stochastic Algorithm for 3D Scene Segmentation and Reconstruction"', '"ECCV 2002"', '["Markov Chain Monte Carlo", "Segmentation Result", "Range Image", "Prior Model", "Stochastic Algori', '"https://doi.org/10.1007/3-540-47977-5_33"', '"In this paper, we present a stochastic algorithm by effective Markov chain Monte Carlo (MCMC) for segmenting and reconstructing 3D scenes. The objective is to segment a range image and its associated reflectance map into a number of surfaces which fit to various 3D surface models and have homogeneous reflectance (material) properties. In comparison to previous work on range image segmentation, the paper makes the following contributions. Firstly, it is aimed at generic natural scenes, indoor and outdoor, which are often much complexer than most of the existing experiments in the \\u201cpolyhedra world\\u201d. Natural scenes require the algorithm to automatically deal with multiple types (families) of surface models which compete to explain the data. Secondly, it integrates the range image with the reflectance map. The latter provides material properties and is especially useful for surface of high specularity, such as glass, metal, ceramics. Thirdly, the algorithm is designed by reversible jump and diffusion Markov chain dynamics and thus achieves globally optimal solutions under the Bayesian statistical framework. Thus it realizes the cue integration and multiple model switching. Fourthly, it adopts two techniques to improve the speed of the Markov chain search: One is a coarse-to-fine strategy and the other are data driven techniques such as edge detection and clustering. The data driven methods provide important information for narrowing the search spaces in a probabilistic fashion. We apply the algorithm to two data sets and the experiments demonstrate robust and satisfactory results on both. Based on the segmentation results, we extend the reconstruction of surfaces behind occlusions to fill in the occluded parts."'),
('"A Stochastic Graph Evolution Framework for Robust Multi-target Tracking"', '"ECCV 2010"', '["Markov Chain Monte Carlo", "Color Histogram", "Data Association", "Proposal Distribution", "Multip', '"https://doi.org/10.1007/978-3-642-15549-9_44"', '"Maintaining the stability of tracks on multiple targets in video over extended time periods remains a challenging problem. A few methods which have recently shown encouraging results in this direction rely on learning context models or the availability of training data. However, this may not be feasible in many application scenarios. Moreover, tracking methods should be able to work across different scenarios (e.g. multiple resolutions of the video) making such context models hard to obtain. In this paper, we consider the problem of long-term tracking in video in application domains where context information is not available a priori, nor can it be learned online. We build our solution on the hypothesis that most existing trackers can obtain reasonable short-term tracks (tracklets). By analyzing the statistical properties of these tracklets, we develop associations between them so as to come up with longer tracks. This is achieved through a stochastic graph evolution step that considers the statistical properties of individual tracklets, as well as the statistics of the targets along each proposed long-term track. On multiple real-life video sequences spanning low and high resolution data, we show the ability to accurately track over extended time periods (results are shown on many minutes of continuous video)."'),
('"A Streakline Representation of Flow in Crowded Scenes"', '"ECCV 2010"', '["Support Vector Machine", "Divergent Region", "Social Force Model", "Crowded Scene", "Crowd Motion"', '"https://doi.org/10.1007/978-3-642-15558-1_32"', '"Based on the Lagrangian framework for fluid dynamics, a streakline representation of flow is presented to solve computer vision problems involving crowd and traffic flow. Streaklines are traced in a fluid flow by injecting color material, such as smoke or dye, which is transported with the flow and used for visualization. In the context of computer vision, streaklines may be used in a similar way to transport information about a scene, and they are obtained by repeatedly initializing a fixed grid of particles at each frame, then moving both current and past particles using optical flow. Streaklines are the locus of points that connect particles which originated from the same initial position. In this paper, a streakline technique is developed to compute several important aspects of a scene, such as flow and potential functions using the Helmholtz decomposition theorem. This leads to a representation of the flow that more accurately recognizes spatial and temporal changes in the scene, compared with other commonly used flow representations. Applications of the technique to segmentation and behavior analysis provide comparison to previously employed techniques, showing that the streakline method outperforms the state-of-the-art in segmentation, and opening a new domain of application for crowd analysis based on potentials."'),
('"A Structural Filter Approach to Human Detection"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15567-3_18"', '"Occlusions and articulated poses make human detection much more difficult than common more rigid object detection like face or car. In this paper, a Structural Filter (SF) approach to human detection is presented in order to deal with occlusions and articulated poses. A three-level hierarchical object structure consisting of words, sentences and paragraphs in analog to text grammar is proposed and correspondingly each level is associated to a kind of SF, that is, Word Structural Filter (WSF), Sentences Structural Filter (SSF) and Paragraph Structural Filter (PSF). A SF is a set of detectors which is able to infer what structures a test window possesses, and specifically WSF is composed of all detectors for words, SSF is composed of all detectors for sentences, and so as PSF. WSF works on the most basic units of an object. SSF deals with meaningful sub structures of an object. Visible parts of human in crowded scene can be head-shoulder, left-part, right-part, upper-body or whole-body, and articulated human change a lot in pose especially in doing sports. Visible parts and different poses are the appearance statuses of detected humans handled by PSF. The three levels of SFs, WSF, SSF and PSF, are integrated in an embedded structure to form a powerful classifier, named as Integrated Structural Filter (ISF). Detection experiments on pedestrian in highly crowded scenes and articulated human show the effectiveness and efficiency of our approach."'),
('"A Study on the Influence of Image Dynamics and Noise on the JPEG 2000 Compression Performance for M', '"CVAMIA 2006"', '["Peak Signal Noise Ratio", "Image Compression", "Blind Source Separation", "JPEG2000 Compression", ', '"https://doi.org/10.1007/11889762_19"', '"This paper addresses two questions concerning JPEG2000 compression \\u2013 firstly \\u2013 how much has noise influence on compression performance \\u2013 secondly \\u2013 can compression performance be improved by applying a new complementary conception with introducing a denoising process before the application of compression Indeed, radiographic images are a combination between the relevant signal and noise, which is per definition not compressible. The noise behaves generally close to a mixture of Gaussian and/or Poisson statistics, which generally affects the compression performance. In this paper, the influence of noise on the compression performance of JPEG2000 images with investigating the parameters signal dynamic and spatial pattern frequency are considered; and the JPEG2000 compression scheme combined with a denoising process is analyzed on simulated and real dental ortho-pan-tomographic images. The test images are generated using Poisson statistics; the denoising utilizes a Monte Carlo noise modeling method. A hundred selected images are denoised and the compression ratio, using lossless and lossy JPEG 2000, is reported and evaluated."'),
('"A Superior Tracking Approach: Building a Strong Tracker through Fusion"', '"ECCV 2014"', '["Object Tracking", "Data Fusion"]', '"https://doi.org/10.1007/978-3-319-10584-0_12"', '"General object tracking is a challenging problem, where each tracking algorithm performs well on different sequences. This is because each of them has different strengths and weaknesses. We show that this fact can be utilized to create a fusion approach that clearly outperforms the best tracking algorithms in tracking performance. Thanks to dynamic programming based trajectory optimization we cannot only outperform tracking algorithms in accuracy but also in other important aspects like trajectory continuity and smoothness. Our fusion approach is very generic as it only requires frame-based tracking results in form of the object\\u2019s bounding box as input and thus can work with arbitrary tracking algorithms. It is also suited for live tracking. We evaluated our approach using 29 different algorithms on 51 sequences and show the superiority of our approach compared to state-of-the-art tracking methods."'),
('"A SURF-Based Spatio-Temporal Feature for Feature-Fusion-Based Action Recognition"', '"ECCV 2010"', '["Support Vector Machine", "Motion Vector", "Action Recognition", "Motion Feature", "Interest Point"', '"https://doi.org/10.1007/978-3-642-35749-7_12"', '"In this paper, we propose a novel spatio-temporal feature which is useful for feature-fusion-based action recognition with Multiple Kernel Learning (MKL). The proposed spatio-temporal feature is based on moving SURF interest points grouped by Delaunay triangulation and on their motion over time. Since this local spatio-temporal feature has different characteristics from holistic appearance features and motion features, it can boost action recognition performance for both controlled videos such as the KTH dataset and uncontrolled videos such as Youtube datasets, by combining it with visual and motion features with MKL. In the experiments, we evaluate our method using KTH dataset, and Youtube dataset. As a result, we obtain 94.5% as a classification rate for in KTH dataset which is almost equivalent to state-of-art, and 80.4% for Youtube dataset which outperforms state-of-the-art greatly."'),
('"A System for Assisting the Visually Impaired in Localization and Grasp of Desired Objects"', '"ECCV 2014"', '["Object recognition", "Attention", "Tracking", "Localization", "Grasp", "Auditory feedback", "Visua', '"https://doi.org/10.1007/978-3-319-16199-0_45"', '"A prototype wearable visual aid for helping visually impaired people find desired objects in their environment is described. The system is comprised of a head-worn camera to capture the scene, an Android phone interface to specify a desired object, and an attention-biasing-enhanced object recognition algorithm to identify three most likely object candidate regions, select the best-matching one, and pass its location to an object tracking algorithm. The object is tracked as the user\\u2019s head moves, and auditory feedback is provided to help the user maintain the object in the field of view, enabling easy reach and grasp. The implementation and integration of the system leading to testing of the working prototype with visually-impaired subjects at the Braille Institute in Los Angeles (demonstration in the accompanying video) is described. Results indicate that this system has clear potential to help visually-impaired users in achieving near-real-time object localization and grasp."'),
('"A System for Large Vocabulary Sign Search"', '"ECCV 2010"', '["American Sign Language recognition", "gesture recognition"]', '"https://doi.org/10.1007/978-3-642-35749-7_27"', '"A method is presented to help users look up the meaning of an unknown sign from American Sign Language (ASL). The user submits a video of the unknown sign as a query, and the system retrieves the most similar signs from a database of sign videos. The user then reviews the retrieved videos to identify the video displaying the sign of interest. Hands are detected in a semi-automatic way: the system performs some hand detection and tracking, and the user has the option to verify and correct the detected hand locations. Features are extracted based on hand motion and hand appearance. Similarity between signs is measured by combining dynamic time warping (DTW) scores, which are based on hand motion, with a simple similarity measure based on hand appearance. In user-independent experiments, with a system vocabulary of 1,113 signs, the correct sign was included in the top 10 matches for 78% of the test queries."'),
('"A Tai Chi Training System Based on Fast Skeleton Matching Algorithm"', '"ECCV 2012"', '["Tai Chi Training System", "Kinect", "Skeleton Matching"]', '"https://doi.org/10.1007/978-3-642-33885-4_78"', '"In this paper, we introduce a Tai Chi training system based on Microsoft\\u2019s Kinect, which automatically evaluates a user\\u2019s performance and provides real-time feedback for the user to refine his current posture. A novel method to measure posture is also described. The experimental results are promising, demonstrating the effectiveness of our approach."'),
('"A Tale of Two Classifiers: SNoW vs. SVM in Visual Recognition"', '"ECCV 2002"', '["Object Recognition", "Object Detection", "Target Node", "Feature Representation", "Linear Feature"', '"https://doi.org/10.1007/3-540-47979-1_46"', '"Numerous statistical learning methods have been developed for visual recognition tasks. Few attempts, however, have been made to address theoretical issues, and in particular, study the suitability of different learning algorithms for visual recognition. Large margin classifiers, such as SNoW and SVM, have recently demonstrated their success in object detection and recognition. In this paper, we present a theoretical account of these two learning approaches, and their suitability to visual recognition. Using tools from computational learning theory, we show that the main difference between the generalization bounds of SVM and SNoW depends on the properties of the data. We argue that learning problems in the visual domain have sparseness characteristics and exhibit them by analyzing data taken from face detection experiments. Experimental results exhibit good generalization and robustness properties of the SNoW-based method, and conform to the theoretical analysis."'),
('"A Tensor Voting Approach for Multi-view 3D Scene Flow Estimation and Refinement"', '"ECCV 2012"', '["Scene Flow", "Tensor Voting", "Multi-view"]', '"https://doi.org/10.1007/978-3-642-33765-9_21"', '"We introduce a framework to estimate and refine 3D scene flow which connects 3D structures of a scene across different frames. In contrast to previous approaches which compute 3D scene flow that connects depth maps from a stereo image sequence or from a depth camera, our approach takes advantage of full 3D reconstruction which computes the 3D scene flow that connects 3D point clouds from multi-view stereo system. Our approach uses a standard multi-view stereo and optical flow algorithm to compute the initial 3D scene flow. A unique two-stage refinement process regularizes the scene flow direction and magnitude sequentially. The scene flow direction is refined by utilizing 3D neighbor smoothness defined by tensor voting. The magnitude of the scene flow is refined by connecting the implicit surfaces across the consecutive 3D point clouds. Our estimated scene flow is temporally consistent. Our approach is efficient, model free, and it is effective in error corrections and outlier rejections. We tested our approach on both synthetic and real-world datasets. Our experimental results show that our approach out-performs previous algorithms quantitatively on synthetic dataset, and it improves the reconstructed 3D model from the refined 3D point cloud in real-world dataset."'),
('"A Testbed for Cross-Dataset Analysis"', '"ECCV 2014"', '["Dataset Bias", "Domain Adaptation", "Iterative Self-Labeling"]', '"https://doi.org/10.1007/978-3-319-16199-0_2"', '"Despite the increasing interest towards domain adaptation and transfer learning techniques to generalize over image collections and overcome their biases, the visual community misses a large scale testbed for cross-dataset analysis. In this paper we discuss the challenges faced when aligning twelve existing image databases in a unique corpus, and we propose two cross-dataset setups that introduce new interesting research questions. Moreover, we report on a first set of experimental domain adaptation tests showing the effectiveness of iterative self-labeling for large scale problems."'),
('"A Theoretical Analysis of Camera Response Functions in Image Deblurring"', '"ECCV 2012"', '["High Frequency Region", "Motion Blur", "Radiometric Calibration", "Ringing Artifact", "Blur Kernel', '"https://doi.org/10.1007/978-3-642-33786-4_25"', '"Motion deblurring is a long standing problem in computer vision and image processing. In most previous approaches, the blurred image is modeled as the convolution of a latent intensity image with a blur kernel. However, for images captured by a real camera, the blur convolution should be applied to scene irradiance instead of image intensity and the blurred results need to be mapped back to image intensity via the camera\\u2019s response function (CRF). In this paper, we present a comprehensive study to analyze the effects of CRFs on motion deblurring. We prove that the intensity-based model closely approximates the irradiance model at low frequency regions. However, at high frequency regions such as edges, the intensity-based approximation introduces large errors and directly applying deconvolution on the intensity image will produce strong ringing artifacts even if the blur kernel is invertible. Based on the approximation error analysis, we further develop a dual-image based solution that captures a pair of sharp/blurred images for both CRF estimation and motion deblurring. Experiments on synthetic and real images validate our theories and demonstrate the robustness and accuracy of our approach."'),
('"A Theory of Multiple Orientation Estimation"', '"ECCV 2006"', '["Local Orientation", "Symmetric Tensor", "Structure Tensor", "Orientation Vector", "Outer Product"]', '"https://doi.org/10.1007/11744047_6"', '"Estimation of local orientations in multivariate signals (including optical flow estimation as special case of orientation in space-time-volumes) is an important problem in image processing and computer vision. Modelling a signal using only a single orientation is often too restrictive, since occlusions and transparency happen frequently, thus necessitating the modelling and analysis of multiple orientations."'),
('"A Theory of Spherical Harmonic Identities for BRDF/Lighting Transfer and Image Consistency"', '"ECCV 2006"', '["Photometric Stereo", "Specular Component", "Image Consistency", "Lambertian Surface", "Change Mate', '"https://doi.org/10.1007/11744085_4"', '"We develop new mathematical results based on the spherical harmonic convolution framework for reflection from a curved surface. We derive novel identities, which are the angular frequency domain analogs to common spatial domain invariants such as reflectance ratios. They apply in a number of canonical cases, including single and multiple images of objects under the same and different lighting conditions. One important case we consider is two different glossy objects in two different lighting environments. Denote the spherical harmonic coefficients by \\\\(B_{lm}^{light,{material}}\\\\), where the subscripts refer to the spherical harmonic indices, and the superscripts to the lighting (1 or 2) and object or material (again 1 or 2). We derive a basic identity, \\\\(B^{\\\\rm 1,1}_{lm}\\\\) \\\\(B^{\\\\rm 2,2}_{lm}\\\\) = \\\\(B^{\\\\rm 1,2}_{lm}\\\\) \\\\(B^{\\\\rm 2,1}_{lm}\\\\), independent of the specific lighting configurations or BRDFs. While this paper is primarily theoretical, it has the potential to lay the mathematical foundations for two important practical applications. First, we can develop more general algorithms for inverse rendering problems, which can directly relight and change material properties by transferring the BRDF or lighting from another object or illumination. Second, we can check the consistency of an image, to detect tampering or image splicing."'),
('"A Three-Layered Approach to Facade Parsing"', '"ECCV 2012"', '["Parse Tree", "Window Detector", "Shape Grammar", "Semantic Vector", "Building Facade"]', '"https://doi.org/10.1007/978-3-642-33786-4_31"', '"We propose a novel three-layered approach for semantic segmentation of building facades. In the first layer, starting from an oversegmentation of a facade, we employ the recently introduced machine learning technique Recursive Neural Networks (RNN) to obtain a probabilistic interpretation of each segment. In the second layer, initial labeling is augmented with the information coming from specialized facade component detectors. The information is merged using a Markov Random Field. In the third layer, we introduce weak architectural knowledge, which enforces the final reconstruction to be architecturally plausible and consistent. Rigorous tests performed on two existing datasets of building facades demonstrate that we significantly outperform the current-state of the art, even when using outputs from earlier layers of the pipeline. Also, we show how the final output of the third layer can be used to create a procedural reconstruction."'),
('"A Topology Preserving Non-rigid Registration Method Using a Symmetric Similarity Function-Applicati', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24672-5_43"', '"3-D non-rigid brain image registration aims at estimating consistently long-distance and highly nonlinear deformations corresponding to anatomical variability between individuals. A consistent mapping is expected to preserve the integrity of warped structures and not to be dependent on the arbitrary choice of a reference image: the estimated transformation from A to B should be equal to the inverse transformation from B to A. This paper addresses these two issues in the context of a hierarchical parametric modeling of the mapping, based on B-spline functions. The parameters of the model are estimated by minimizing a symmetric form of the standard sum of squared differences criterion. Topology preservation is ensured by constraining the Jacobian of the transformation to remain positive on the whole continuous domain of the image as a non trivial 3-D extension of a previous work [1] dealing with the 2-D case. Results on synthetic and real-world data are shown to illustrate the contribution of preserving topology and using a symmetric similarity function."'),
('"A Tree-Based Approach to Integrated Action Localization, Recognition and Segmentation"', '"ECCV 2010"', '["Leaf Node", "Action Recognition", "Action Category", "Test Frame", "Human Action Recognition"]', '"https://doi.org/10.1007/978-3-642-35749-7_9"', '"A tree-based approach to integrated action segmentation, localization and recognition is proposed. An action is represented as a sequence of joint hog-flow descriptors extracted independently from each frame. During training, a set of action prototypes is first learned based on a k-means clustering, and then a binary tree model is constructed from the set of action prototypes based on hierarchical k-means clustering. Each tree node is characterized by a shape-motion descriptor and a rejection threshold, and an action segmentation mask is defined for leaf nodes (corresponding to a prototype). During testing, an action is localized by mapping each test frame to a nearest neighbor prototype using a fast matching method to search the learned tree, followed by global filtering refinement. An action is recognized by maximizing the sum of the joint probabilities of the action category and action prototype over test frames. Our approach does not explicitly rely on human tracking and background subtraction, and enables action localization and recognition in realistic and challenging conditions (such as crowded backgrounds). Experimental results show that our approach can achieve recognition rates of 100% on the CMU action dataset and 100% on the Weizmann dataset."'),
('"A Tuned Eigenspace Technique for Articulated Motion Recognition"', '"ECCV 2006"', '["Recognition Rate", "Human Motion", "Motion Trajectory", "Motion Line", "Posture Matrix"]', '"https://doi.org/10.1007/11744023_14"', '"In this paper, we introduce a tuned eigenspace technique so as to classify human motion. The method presented here overcomes those problems related to articulated motion and dress texture effects by learning various human motions in terms of their sequential postures in an eigenspace. In order to cope with the variability inherent to articulated motion, we propose a method to tune the set of sequential eigenspaces. Once the learnt tuned eigenspaces are at hand, the recognition task then becomes a nearest-neighbor search over the eigenspaces. We show how our tuned eigenspace method can be used for purposes of real-world and synthetic pose recognition. We also discuss and overcome the problem related to clothing texture that occurs in real-world data, and propose a background subtraction method to employ the method in out-door environment. We provide results on synthetic imagery for a number of human poses and illustrate the utility of the method for the purposes of human motion recognition."'),
('"A TV Flow Based Local Scale Measure for Texture Discrimination"', '"ECCV 2004"', '["Texture Feature", "Scale Measure", "Feature Channel", "Texture Segmentation", "Moment Matrix"]', '"https://doi.org/10.1007/978-3-540-24671-8_46"', '"We introduce a technique for measuring local scale, based on a special property of the so-called total variational (TV) flow. For TV flow, pixels change their value with a speed that is inversely proportional to the size of the region they belong to. Exploiting this property directly leads to a region based measure for scale that is well-suited for texture discrimination. Together with the image intensity and texture features computed from the second moment matrix, which measures the orientation of a texture, a sparse feature space of dimension 5 is obtained that covers the most important descriptors of a texture: magnitude, orientation, and scale. A demonstration of the performance of these features is given in the scope of texture segmentation."'),
('"A Two-Stage Strategy for Real-Time Dense 3D Reconstruction of Large-Scale Scenes"', '"ECCV 2014"', '["Geometric Constraint", "Mask Image", "Planar Patch", "Drift Error", "Identity Constraint"]', '"https://doi.org/10.1007/978-3-319-16178-5_30"', '"The frame-to-global-model approach is widely used for accurate 3D modeling from sequences of RGB-D images. Because still no perfect camera tracking system exists, the accumulation of small errors generated when registering and integrating successive RGB-D images causes deformations of the 3D model being built up. In particular, the deformations become significant when the scale of the scene to model is large. To tackle this problem, we propose a two-stage strategy to build in details a large-scale 3D model with minimal deformations where the first stage creates accurate small-scale 3D scenes in real-time from short subsequences of RGB-D images while the second stage re-organises all the results from the first stage in a geometrically consistent manner to reduce deformations as much as possible. By employing planar patches as the 3D scene representation, our proposed method runs in real-time to build accurate 3D models with minimal deformations even for large-scale scenes. Our experiments using real data confirm the effectiveness of our proposed method."'),
('"A Unified Algebraic Approach to 2-D and 3-D Motion Segmentation"', '"ECCV 2004"', '["Expectation Maximization", "Motion Model", "Image Pair", "Image Measurement", "Algebraic Approach"', '"https://doi.org/10.1007/978-3-540-24670-1_1"', '"We present an analytic solution to the problem of estimating multiple 2-D and 3-D motion models from two-view correspondences or optical flow. The key to our approach is to view the estimation of multiple motion models as the estimation of a single multibody motion model. This is possible thanks to two important algebraic facts. First, we show that all the image measurements, regardless of their associated motion model, can be fit with a real or complex polynomial. Second, we show that the parameters of the motion model associated with an image measurement can be obtained from the derivatives of the polynomial at the measurement. This leads to a novel motion segmentation algorithm that applies to most of the two-view motion models adopted in computer vision. Our experiments show that the proposed algorithm outperforms existing algebraic methods in terms of efficiency and robustness, and provides a good initialization for iterative techniques, such as EM, which is strongly dependent on correct initialization."'),
('"A Unified Contour-Pixel Model for Figure-Ground Segmentation"', '"ECCV 2010"', '["Joint Model", "Appearance Model", "Foreground Object", "Landmark Point", "Foreground Segmentation"', '"https://doi.org/10.1007/978-3-642-15555-0_25"', '"The goal of this paper is to provide an accurate pixel-level segmentation of a deformable foreground object in an image. We combine state-of-the-art local image segmentation techniques with a global object-specific contour model to form a coherent energy function over the outline of the object and the pixels inside it. The energy function includes terms from a variant of the TextonBoost method, which labels each pixel as either foreground or background. It also includes terms over landmark points from a LOOPS model [1], which combines global object shape with landmark-specific detectors. We allow the pixel-level segmentation and object outline to inform each other through energy potentials so that they form a coherent object segmentation with globally consistent shape and appearance. We introduce an inference method to optimize this energy that proposes moves within the complex energy space based on multiple initial oversegmentations of the entire image. We show that this method achieves state-of-the-art results in precisely segmenting articulated objects in cluttered natural scenes."'),
('"A Unified Energy Minimization Framework for Model Fitting in Depth"', '"ECCV 2012"', '["Point Cloud", "Energy Function", "Depth Image", "Intrinsic Parameter", "Depth Camera"]', '"https://doi.org/10.1007/978-3-642-33868-7_8"', '"In this paper we present a unified energy minimization framework for model fitting and pose recovery problems in depth cameras. 3D level-set embedding functions are used to represent object models implicitly and a novel 3D chamfer matching based energy function is minimized by adjusting the generic projection matrix, which could be parameterized differently according to specific applications. Our proposed energy function takes the advantage of the gradient of 3D level-set embedding function and can be efficiently solved by gradients-based optimization methods. We show various real-world applications, including real-time 3D tracking in depth, simultaneous calibration and tracking, and 3D point cloud modeling. We perform experiments on both real data and synthetic data to show the superior performance of our method for all the applications above."'),
('"A Unified Framework for Multi-target Tracking and Collective Activity Recognition"', '"ECCV 2012"', '["Collective Activity Recognition", "Tracking", "Tracklet Association"]', '"https://doi.org/10.1007/978-3-642-33765-9_16"', '"We present a coherent, discriminative framework for simultaneously tracking multiple people and estimating their collective activities. Instead of treating the two problems separately, our model is grounded in the intuition that a strong correlation exists between a person\\u2019s motion, their activity, and the motion and activities of other nearby people. Instead of directly linking the solutions to these two problems, we introduce a hierarchy of activity types that creates a natural progression that leads from a specific person\\u2019s motion to the activity of the group as a whole. Our model is capable of jointly tracking multiple people, recognizing individual activities (atomic activities), the interactions between pairs of people (interaction activities), and finally the behavior of groups of people (collective activities). We also propose an algorithm for solving this otherwise intractable joint inference problem by combining belief propagation with a version of the branch and bound algorithm equipped with integer programming. Experimental results on challenging video datasets demonstrate our theoretical claims and indicate that our model achieves the best collective activity classification results to date."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"A Unified Model for Image Colorization"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-16199-0_21"', '"This paper addresses the topic of image colorization that consists in converting a gray-scale image into a color one. In the literature, there exist two main types of approaches to tackle this problem. The first one is the manual methods where the color information is given by some scribbles drawn by the user on the image. The interest of these approaches comes from the interactions with the user that can put any color he wants. Nevertheless, when the scene is complex many scribbles must be drawn and the interactive process becomes tedious and time-consuming. The second category of approaches is the exemplar-based methods that require a color image as input. Once the example image is given, the colorization is generally fully automatic. A limitation of these methods is that the example image needs to contain all the desired colors in the final result. In this paper, we propose a new framework that unifies these two categories of approaches into a joint variational model. Our approach is able to take into account information coming from any colorization method among these two categories. Experiments and comparisons demonstrate that the proposed approach provides competitive colorization results compared to state-of-the-art methods."'),
('"A Unified View on Deformable Shape Factorizations"', '"ECCV 2012"', '["Discrete Cosine Transform", "Basis Shape", "Tensor Formulation", "Multiple Camera", "Alternate Lit', '"https://doi.org/10.1007/978-3-642-33783-3_49"', '"Multiple-view geometry and structure-from-motion are well established techniques to compute the structure of a moving rigid object. These techniques are all based on strong algebraic constraints imposed by the rigidity of the object. Unfortunately, many scenes of interest, e.g. faces or cloths, are dynamic and the rigidity constraint no longer holds. Hence, there is a need for non-rigid structure-from-motion (NRSfM) methods which can deal with dynamic scenes. A prominent framework to model deforming and moving non-rigid objects is the factorization technique where the measurements are assumed to lie in a low-dimensional subspace. Many different formulations and variations for factorization-based NRSfM have been proposed in recent years. However, due to the complex interactions between several subspaces, the distinguishing properties between two seemingly related approaches are often unclear. For example, do two approaches just vary in the optimization method used or is really a different model beneath?"'),
('"A Unifying Framework for Mutual Information Methods for Use in Non-linear Optimisation"', '"ECCV 2006"', '["Mutual Information", "Image Registration", "Window Function", "Warp Function", "Medical Image Anal', '"https://doi.org/10.1007/11744023_29"', '"Many variants of MI exist in the literature. These vary primarily in how the joint histogram is populated. This paper places the four main variants of MI: Standard sampling, Partial Volume Estimation (PVE), In-Parzen Windowing and Post-Parzen Windowing into a single mathematical framework. Jacobians and Hessians are derived in each case. A particular contribution is that the non-linearities implicit to standard sampling and post-Parzen windowing are explicitly dealt with. These non-linearities are a barrier to their use in optimisation. Side-by-side comparison of the MI variants is made using eight diverse data-sets, considering computational expense and convergence. In the experiments, PVE was generally the best performer, although standard sampling often performed nearly as well (if a higher sample rate was used). The widely used sum of squared differences metric performed as well as MI unless large occlusions and non-linear intensity relationships occurred. The binaries and scripts used for testing are available online."'),
('"A Unifying Theory for Central Panoramic Systems and Practical Implications"', '"ECCV 2000"', '["Great Circle", "Image Center", "Stereographic Projection", "Perspective Projection", "Parabolic Mi', '"https://doi.org/10.1007/3-540-45053-X_29"', '"Omnidirectional vision systems can provide panoramic alertness in surveillance, improve navigational capabilities, and produce panoramic images for multimedia. Catadioptric realizations of omnidirectional vision combine reflective surfaces and lenses. A particular class of them, the central panoramic systems, preserve the uniqueness of the projection viewpoint. In fact, every central projection system including the well known perspective projection on a plane falls into this category."'),
('"A Unifying Theory of Active Discovery and Learning"', '"ECCV 2012"', '["Active Learning", "Active Discovery", "Unlabelled Data", "Minority Class", "Dirichlet Process"]', '"https://doi.org/10.1007/978-3-642-33715-4_33"', '"For learning problems where human supervision is expensive, active query selection methods are often exploited to maximise the return of each supervision. Two problems where this has been successfully applied are active discovery \\u2013 where the aim is to discover at least one instance of each rare class with few supervisions; and active learning \\u2013 where the aim is to maximise a classifier\\u2019s performance with least supervision. Recently, there has been interest in optimising these tasks jointly, i.e., active learning with undiscovered classes, to support efficient interactive modelling of new domains. Mixtures of active discovery and learning and other schemes have been exploited, but perform poorly due to heuristic objectives. In this study, we show with systematic theoretical analysis how the previously disparate tasks of active discovery and learning can be cleanly unified into a single problem, and hence are able for the first time to develop a unified query algorithm to directly optimise this problem. The result is a model which consistently outperforms previous attempts at active learning in the presence of undiscovered classes, with no need to tune parameters for different datasets."'),
('"A Variational Approach to Recovering a Manifold from Sample Points"', '"ECCV 2002"', '["Computer Vision", "Ambient Space", "Implicit Representation", "Discriminant Function Analysis", "S', '"https://doi.org/10.1007/3-540-47967-8_1"', '"We present a novel algorithm for recovering a smooth manifold of unknown dimension and topology from a set of points known to belong to it. Numerous applications in computer vision can be naturally interpreted as instanciations of this fundamental problem. Recently, a non-iterative discrete approach, tensor voting, has been introduced to solve this problem and has been applied successfully to various applications. As an alternative, we propose a variational formulation of this problem in the continuous setting and derive an iterative algorithm which approximates its solutions. This method and tensor voting are somewhat the differential and integral form of one another. Although iterative methods are slower in general, the strength of the suggested method is that it can easily be applied when the ambient space is not Euclidean, which is important in many applications. The algorithm consists in solving a partial differential equation that performs a special anisotropic diffusion on an implicit representation of the known set of points. This results in connecting isolated neighbouring points. This approach is very simple, mathematically sound, robust and powerful since it handles in a homogeneous way manifolds of arbitrary dimension and topology, embedded in Euclidean or non-Euclidean spaces, with or without border. We shall present this approach and demonstrate both its benefits and shortcomings in two different contexts: (i) data visual analysis, (ii) skin detection in color images."'),
('"A Variational Approach to Shape from Defocus"', '"ECCV 2002"', '["Variational Approach", "Tangent Plane", "Blind Deconvolution", "Focal Setting", "Defocused Image"]', '"https://doi.org/10.1007/3-540-47967-8_2"', '"We address the problem of estimating the three-dimensional shape and radiance of a surface in space from images obtained with different focal settings. We pose the problem as an infinite-dimensional optimization and seek for the global shape of the surface by numerically solving a partial differential equation (PDE). Our method has the advantage of being global (so that regularization can be imposed explicitly), efficient (we use level set methods to solve the PDE), and geometrically correct (we do not assume a shift-invariant imaging model, and therefore are not restricted to equifocal surfaces)."'),
('"A Variational Framework for Single Image Dehazing"', '"ECCV 2014"', '["Image dehazing", "Image defogging", "Color correction", "Contrast enhancement"]', '"https://doi.org/10.1007/978-3-319-16199-0_18"', '"Images captured under adverse weather conditions, such as haze or fog, typically exhibit low contrast and faded colors, which may severely limit the visibility within the scene. Unveiling the image structure under the haze layer and recovering vivid colors out of a single image remains a challenging task, since the degradation is depth-dependent and conventional methods are unable to handle this problem."'),
('"A Video-Based Drowning Detection System"', '"ECCV 2002"', '["Swimming Pool", "Cold Water Immersion", "Water Crisis", "Background Scene", "Shadow Pixel"]', '"https://doi.org/10.1007/3-540-47979-1_20"', '"This paper provides new insights into robust human tracking and semantic event detection within the context of a novel real-time video surveillance system capable of automatically detecting drowning incidents in a swimming pool. An effective background model that incorporates prior knowledge about swimming pools and aquatic environments enables swimmers to be reliably detected and tracked despite the significant presence of water ripples, splashes and shadows. Visual indicators of water crises are identified based on professional knowledge of water crisis recognition and modelled by a hierarchical set of carefully chosen swimmer descriptors. An effective alarm generation methodology is then developed to enable the timely detection of genuine water crises while minimizing the number of false alarms. The system has been tested on numerous instances of simulated water crises and potential false alarm scenarios with encouraging results."'),
('"A Virtual Environment Tool for Benchmarking Face Analysis Systems"', '"ECCV 2012"', '["Face analysis", "Face Recognition", "Face Recognition Benchmark", "Evaluation Methodologies", "Vir', '"https://doi.org/10.1007/978-3-642-33885-4_54"', '"In this article, a virtual environment for realistic testing of face analysis systems under uncontrolled conditions is proposed. The key elements of this tool are a simulator, and real face and background images taken under real-world conditions with different acquisition conditions, such as indoor or outdoor illumination. Inside the virtual environment, an observing agent, the one with the ability to recognize and detect faces, can navigate and observe the face images, at different distances, and angles. During the face analysis process, the agent can actively change its viewpoint and relative distance to the faces in order to improve the recognition results. The virtual environment provides all behaviors to the agent (navigation, positioning, face\\u2019s image composing under different angles, etc.), except the ones related with the analysis of faces (detection, recognition, pose estimation, etc.). In addition we describe different kinds of experiments that can be implemented for quantifying the face analysis capabilities of agents and provide usage example of the proposed tool in evaluating a face recognition system in a service robot."'),
('"A Vision-Based Gestural Guidance Interface for Mobile Robotic Platforms"', '"CVHCI 2004"', '["Gesture Recognition", "Mobile Platform", "Video Rate", "Trained Neural Network", "Static Gesture"]', '"https://doi.org/10.1007/978-3-540-24837-8_5"', '"This paper describes a gestural guidance interface that controls the motion of a mobile platform using a set of predefined static and dynamic hand gestures inspired by the marshalling code. Images captured by an onboard color camera are processed at video rate in order to track the operator\\u2019s head and hands. The camera pan, tilt and zoom are adjusted by a fuzzy-logic controller so as to track the operator\\u2019s head and maintain it centered and properly sized within the image plane. Gestural commands are defined as two-hand motion patterns, whose features are provided, at video rate, to a trained neural network. A command is considered recognized once the classifier has produced a series of consistent interpretations. A motion-modifying command is then issued in a way that ensures motion coherence and smoothness. The guidance system can be trained online."'),
('"A Vision-Based Navigation Facility for Planetary Entry Descent Landing"', '"ECCV 2012"', '["European Space Agency", "Landing Zone", "Planet Surface", "Mars Science Laboratory", "Mars Explora', '"https://doi.org/10.1007/978-3-642-33868-7_54"', '"This paper describes a facility set up as a test bed and a proof of concept to study open issues of future space missions. The final goal of such studies is to increase the on board autonomy, of primary importance for missions covering very high distances. We refer in particular to vision-based modules, in charge of acquiring and processing images during the Entry Descent and Landing (EDL) phases of a Lander, and contributing to a precise localization of the landing region and a safe landing. We will describe the vision-based algorithms already implemented on the facility, and a preliminary experimental analysis which allowed us to validate the approaches and provided very promising results."'),
('"A Visual Category Filter for Google Images"', '"ECCV 2004"', '["Training Image", "Unsupervised Learning", "Object Category", "Image Search", "Curve Segment"]', '"https://doi.org/10.1007/978-3-540-24670-1_19"', '"We extend the constellation model to include heterogeneous parts which may represent either the appearance or the geometry of a region of the object. The parts and their spatial configuration are learnt simultaneously and automatically, without supervision, from cluttered images."'),
('"A Visual SLAM System on Mobile Robot Supporting Localization Services to Visually Impaired People"', '"ECCV 2014"', '["Visual odometry", "Place recognition", "FAB-MAP algorithms", "Kalman filter"]', '"https://doi.org/10.1007/978-3-319-16199-0_50"', '"This paper describes a Visual SLAM system developed on a mobile robot in order to support localization services to visually impaired people. The proposed system aims to provide services in small or mid-scale environments such as inside a building or campus of school where conventional positioning data such as GPS, WIFI signals are often not available. Toward this end, we adapt and improve existing vision-based techniques in order to handle issues in the indoor environments. We firstly design an image acquisition system to collect visual data. On one hand, a robust visual odometry method is adjusted to precisely create the routes in the environment. On the other hand, we utilize the Fast-Appearance Based Mapping algorithm that is may be the most successful for matching places in large scenarios. In order to better estimate robot\\u2019s location, we utilize a Kalman Filter that combines the matching results of current observation and the estimation of robot states based on its kinematic model. The experimental results confirmed that the proposed system is feasible to navigate the visually impaired people in the indoor environments."'),
('"Abnormal Object Detection by Canonical Scene-Based Contextual Model"', '"ECCV 2012"', '["abnormal object detection", "generative model", "sampling"]', '"https://doi.org/10.1007/978-3-642-33712-3_47"', '"Contextual modeling is a critical issue in scene understanding. Object detection accuracy can be improved by exploiting tendencies that are common among object configurations. However, conventional contextual models only exploit the tendencies of normal objects; abnormal objects that do not follow the same tendencies are hard to detect through contextual model. This paper proposes a novel generative model that detects abnormal objects by meeting four proposed criteria of success. This model generates normal as well as abnormal objects, each following their respective tendencies. Moreover, this generation is controlled by a latent scene variable. All latent variables of the proposed model are predicted through optimization via population-based Markov Chain Monte Carlo, which has a relatively short convergence time. We present a new abnormal dataset classified into three categories to thoroughly measure the accuracy of the proposed model for each category; the results demonstrate the superiority of our proposed approach over existing methods."'),
('"Accelerated Convergence Using Dynamic Mean Shift"', '"ECCV 2006"', '["Step Length", "Segmentation Result", "Superlinear Convergence", "Shift Vector", "Linear Convergenc', '"https://doi.org/10.1007/11744047_20"', '"Mean shift is an iterative mode-seeking algorithm widely used in pattern recognition and computer vision. However, its convergence is sometimes too slow to be practical. In this paper, we improve the convergence speed of mean shift by dynamically updating the sample set during the iterations, and the resultant procedure is called dynamic mean shift (DMS). When the data is locally Gaussian, it can be shown that both the standard and dynamic mean shift algorithms converge to the same optimal solution. However, while standard mean shift only has linear convergence, the dynamic mean shift algorithm has superlinear convergence. Experiments on color image segmentation show that dynamic mean shift produces comparable results as the standard mean shift algorithm, but can significantly reduce the number of iterations for convergence and takes much less time."'),
('"Accelerated Hypothesis Generation for Multi-structure Robust Fitting"', '"ECCV 2010"', '["Image Pair", "Sampling Step", "Minimal Subset", "Sequential Probability Ratio Test", "Inlier Proba', '"https://doi.org/10.1007/978-3-642-15555-0_39"', '"Random hypothesis generation underpins many geometric model fitting techniques. Unfortunately it is also computationally expensive. We propose a fundamentally new approach to accelerate hypothesis sampling by guiding it with information derived from residual sorting. We show that residual sorting innately encodes the probability of two points to have arisen from the same model and is obtained without recourse to domain knowledge (e.g. keypoint matching scores) typically used in previous sampling enhancement methods. More crucially our approach is naturally capable of handling data with multiple model instances and excels in applications (e.g. multi-homography fitting) which easily frustrate other techniques. Experiments show that our method provides superior efficiency on various geometric model estimation tasks. Implementation of our algorithm is available on the authors\\u2019 homepage."'),
('"Accelerated Large Scale Optimization by Concomitant Hashing"', '"ECCV 2012"', '["Sparse Code", "Mean Average Precision", "Random Projection", "Orthogonal Match Pursuit", "Gaussian', '"https://doi.org/10.1007/978-3-642-33718-5_30"', '"Traditional locality-sensitive hashing (LSH) techniques aim to tackle the curse of explosive data scale by guaranteeing that similar samples are projected onto proximal hash buckets. Despite the success of LSH on numerous vision tasks like image retrieval and object matching, however, its potential in large-scale optimization is only realized recently. In this paper we further advance this nascent area. We first identify two common operations known as the computational bottleneck of numerous optimization algorithms in a large-scale setting, i.e., min/max inner product. We propose a hashing scheme for accelerating min/max inner product, which exploits properties of order statistics of statistically correlated random vectors. Compared with other schemes, our algorithm exhibits improved recall at a lower computational cost. The effectiveness and efficiency of the proposed method are corroborated by theoretic analysis and several important applications. Especially, we use the proposed hashing scheme to perform approximate \\u21131 regularized least squares with dictionaries with millions of elements, a scale which is beyond the capability of currently known exact solvers. Nonetheless, it is highlighted that the focus of this paper is not on a new hashing scheme for approximate nearest neighbor problem. It exploits a new application for the hashing techniques and proposes a general framework for accelerating a large variety of optimization procedures in computer vision."'),
('"Accelerating Visual Categorization with the GPU"', '"ECCV 2010"', '["Vector Quantization", "Thread Block", "Graphic Hardware", "Visual Categorization", "Sift Descripto', '"https://doi.org/10.1007/978-3-642-35740-4_34"', '"Visual categorization is important to manage large collections of digital images and video, where textual meta-data is often incomplete or simply unavailable. The bag-of-words model has become the most powerful method for visual categorization of images and video. Despite its high accuracy, a severe drawback of this model is its high computational cost. As the trend to increase computational power in newer CPU and GPU architectures is to increase their level of parallelism, exploiting this parallelism becomes an important direction to handle the computational cost of the bag-of-words approach. In this paper, we analyze the bag-of-words model for visual categorization in terms of computational cost and identify two major bottlenecks: the quantization step and the classification step. We address these two bottlenecks by proposing two efficient algorithms for quantization and classification by exploiting the GPU hardware and the CUDA parallel programming model. The algorithms are designed to keep categorization accuracy intact and give the same numerical results."'),
('"Accuracy of Spherical Harmonic Approximations for Images of Lambertian Objects under Far and Near L', '"ECCV 2004"', '["Approximation Accuracy", "Harmonic Approximation", "Cast Shadow", "Photometric Stereo", "Point Lig', '"https://doi.org/10.1007/978-3-540-24670-1_44"', '"Various problems in Computer Vision become difficult due to a strong influence of lighting on the images of an object. Recent work showed analytically that the set of all images of a convex, Lambertian object can be accurately approximated by the low-dimensional linear subspace constructed using spherical harmonic functions. In this paper we present two major contributions: first, we extend previous analysis of spherical harmonic approximation to the case of arbitrary objects; second, we analyze its applicability for near light. We begin by showing that under distant lighting, with uniform distribution of light sources, the average accuracy of spherical harmonic representation can be bound from below. This bound holds for objects of arbitrary geometry and color, and for general illuminations (consisting of any number of light sources). We further examine the case when light is coming from above and provide an analytic expression for the accuracy obtained in this case. Finally, we show that low-dimensional representations using spherical harmonics provide an accurate approximation also for fairly near light. Our analysis assumes Lambertian reflectance and accounts for attached, but not for cast shadows. We support this analysis by simulations and real experiments, including an example of a 3D shape reconstruction by photometric stereo under very close, unknown lighting."'),
('"Accurate Disparity Estimation for Plenoptic Images"', '"ECCV 2014"', '["Plenoptic camera", "Raw-data conversion", "Disparity estimation"]', '"https://doi.org/10.1007/978-3-319-16181-5_42"', '"In this paper we propose a post-processing pipeline to recover accurately the views (light-field) from the raw data of a plenoptic camera such as Lytro and to estimate disparity maps in a novel way from such a light-field. First, the microlens centers are estimated and then the raw image is demultiplexed without demosaicking it beforehand. Then, we present a new block-matching algorithm to estimate disparities for the mosaicked plenoptic views. Our algorithm exploits at best the configuration given by the plenoptic camera: (i) the views are horizontally and vertically rectified and have the same baseline, and therefore (ii) at each point, the vertical and horizontal disparities are the same. Our strategy of demultiplexing without demosaicking avoids image artifacts due to view cross-talk and helps estimating more accurate disparity maps. Finally, we compare our results with state-of-the-art methods."'),
('"Accurate Image Localization Based on Google Maps Street View"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_19"', '"Finding an image\\u2019s exact GPS location is a challenging computer vision problem that has many real-world applications. In this paper, we address the problem of finding the GPS location of images with an accuracy which is comparable to hand-held GPS devices.We leverage a structured data set of about 100,000 images build from Google Maps Street View as the reference images. We propose a localization method in which the SIFT descriptors of the detected SIFT interest points in the reference images are indexed using a tree. In order to localize a query image, the tree is queried using the detected SIFT descriptors in the query image. A novel GPS-tag-based pruning method removes the less reliable descriptors. Then, a smoothing step with an associated voting scheme is utilized; this allows each query descriptor to vote for the location its nearest neighbor belongs to, in order to accurately localize the query image. A parameter called Confidence of Localization which is based on the Kurtosis of the distribution of votes is defined to determine how reliable the localization of a particular image is. In addition, we propose a novel approach to localize groups of images accurately in a hierarchical manner. First, each image is localized individually; then, the rest of the images in the group are matched against images in the neighboring area of the found first match. The final location is determined based on the Confidence of Localization parameter. The proposed image group localization method can deal with very unclear queries which are not capable of being geolocated individually."'),
('"Accurate Intrinsic Calibration of Depth Camera with Cuboids"', '"ECCV 2014"', '["intrinsic calibration", "depth camera", "3D measurement", "depth map", "cuboids"]', '"https://doi.org/10.1007/978-3-319-10602-1_51"', '"Due to the low precision, the consumer-grade depth sensor is often calibrated jointly with a color camera, and the joint calibration sometimes presents undesired interactions. In this paper, we propose a novel method to carry out the high-accuracy intrinsic calibration of depth sensors merely by the depth camera, in which the traditional calibration rig, checker-board pattern, is replaced with a set of cuboids with known sizes, and the objective function for calibration is based on the length, width, and height of cuboids and its angle between the neighboring surfaces, which can be directly and robustly calculated from the depth-map. We experimentally evaluate the accuracy of the calibrated depth camera by measuring the angles and sizes of cubic object, and it is empirically shown that the resulting calibration accuracy is higher than that in the state-of-the-art calibration procedures, making the commodity depth sensors applicable to more interesting application scenarios such as 3D measurement and shape modeling etc."'),
('"Accurate Measurement of Cartilage Morphology Using a 3D Laser Scanner"', '"CVAMIA 2006"', '["Articular Cartilage", "Range Image", "Manual Segmentation", "Cartilage Volume", "Cadaver Knee"]', '"https://doi.org/10.1007/11889762_4"', '"We describe a method to accurately assess articular cartilage morphology using the three-dimensional laser scanning technology. Traditional methods to obtain ground truth for validating the assessment of cartilage morphology from MR images have relied on water displacement, anatomical sections obtained with a high precision band saw, stereophotogrammetry, manual segmentation, and phantoms of known geometry. However, these methods are either limited to overall measurements such as volume and area, require an extensive setup and a highly skilled operator, or are prone to artifacts due to tissue sectioning. Alternatively, 3D laser scanning is an established technology that can provide high resolution range images of cartilage and bone surfaces. We present a method to extract these surfaces from scanned range images, register them spatially, and combine them into a single surface representing the articular cartilage from which volume, area, and thickness can be computed. We validated the laser scanning approach using a knee model which was covered with a synthetic articular cartilage model and compared the computed volume against water displacement measurements. Using this method, the volume of articular cartilage in five sets of cadaver knees was compared to volume estimates obtained from segmentation of MR images."'),
('"Accurate Single Image Multi-modal Camera Pose Estimation"', '"ECCV 2010"', '["Multi-Modal Registration", "Pose Estimation", "Multi-Modal 2D/3D Correspondences", "Self-Similarit', '"https://doi.org/10.1007/978-3-642-35740-4_23"', '"A well known problem in photogrammetry and computer vision is the precise and robust determination of camera poses with respect to a given 3D model. In this work we propose a novel multi-modal method for single image camera pose estimation with respect to 3D models with intensity information (e.g., LiDAR data with reflectance information)."'),
('"Acquiring 4D Light Fields of Self-Luminous Light Sources Using Programmable Filter"', '"ECCV 2014"', '["Self-luminous light source", "Extended light source", "4D light field", "Programmable filter", "Mu', '"https://doi.org/10.1007/978-3-319-16181-5_45"', '"Self-luminous light sources in the real world often have nonnegligible sizes and radiate light inhomogeneously. Acquiring the model of such a light source is highly important for accurate image synthesis and understanding. In this paper, we propose a method for measuring 4D light fields of self-luminous extended light sources by using a liquid crystal (LC) panel, i.e. a programmable filter and a diffuse-reflection board. The proposed method recovers the 4D light field from the images of the board illuminated by the light radiated from a light source and passing through the LC panel. We make use of the feature that the transmittance of the LC panel can be controlled both spatially and temporally. The proposed method enables us to utilize multiplexed sensing, and therefore is able to acquire 4D light fields more efficiently and densely than the straightforward method. We implemented the prototype setup, and confirmed through a number of experiments that the proposed method is effective for modeling self-luminous extended light sources in the real world."'),
('"Action and Gesture Temporal Spotting with Super Vector Representation"', '"ECCV 2014"', '["Action recognition", "Gesture recognition", "Temporal spotting", "Super vector"]', '"https://doi.org/10.1007/978-3-319-16178-5_36"', '"This paper focuses on describing our method designed for both track 2 and track 3 at Looking at People (LAP) challenging [1]. We propose an action and gesture spotting system, which is mainly composed of three steps: (i) temporal segmentation, (ii) clip classification, and (iii) post processing. For track 2, we resort to a simple sliding window method to divide each video sequence into clips, while for track 3, we design a segmentation method based on the motion analysis of human hands. Then, for each clip, we choose a kind of super vector representation with dense features. Based on this representation, we train a linear SVM to conduct action and gesture recognition. Finally, we use some post processing techniques to void the detection of false positives. We demonstrate the effectiveness of our proposed method by participating the contests of both track 2 and track 3. We obtain the best performance on track 2 and rank \\\\(4^{th}\\\\) on track 3, which indicates that the designed system is effective for action and gesture recognition."'),
('"Action Detection with Improved Dense Trajectories and Sliding Window"', '"ECCV 2014"', '["Video analysis", "Action recognition", "Action detection", "Dense trajectories"]', '"https://doi.org/10.1007/978-3-319-16178-5_38"', '"In this paper we describe an action/interaction detection system based on improved dense trajectories [19], multiple visual descriptors and bag-of-features representation. Given that the actions/interactions are not mutual exclusive, we train a binary classifier for every predefined action/interaction. We rely on a non-overlapped temporal sliding window to enable the temporal localization. We have tested our system in ChaLearn Looking at People Challenge 2014 Track 2 dataset [1, 2]. We obtained 0.4226 average overlap, which is the 3rd place in the track of the challenge. Finally, we provide an extensive analysis of the performance of this system on different actions and provide possible ways to improve a general action detection system."'),
('"Action Recognition in Broadcast Tennis Video Using Optical Flow and Support Vector Machine"', '"ECCV 2006"', '["Support Vector Machine", "Optical Flow", "Action Recognition", "Broadcast Video", "Tennis Player"]', '"https://doi.org/10.1007/11754336_9"', '"Motion analysis in broadcast sports video is a challenging problem especially for player action recognition due to the low resolution of players in the frames. In this paper, we present a novel approach to recognize the basic player actions in broadcast tennis video where the player is about 30 pixels tall. Two research challenges, motion representation and action recognition, are addressed. A new motion descriptor, which is a group of histograms based on optical flow, is proposed for motion representation. The optical flow here is treated as spatial pattern of noisy measurement instead of precise pixel displacement. To recognize the action performed by the player, support vector machine is employed to train the classifier where the concatenation of histograms is formed as the input features. Experimental results demonstrate that our method is promising by integrating with the framework of multimodal analysis in sports video."'),
('"Action Recognition Robust to Background Clutter by Using Stereo Vision"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_33"', '"An action recognition algorithm which works with binocular videos is presented. The proposed method uses standard bag-of-words approach, where each action clip is represented as a histogram of visual words. However, instead of using classical monocular HoG/HoF features, we construct features from the scene-flow computed by a matching algorithm on the sequence of stereo images. The resulting algorithm has a comparable or slightly better recognition accuracy than standard monocular solution in controlled setup with a single actor present in the scene. However, we show its significantly improved performance in case of strong background clutter due to other people freely moving behind the actor."'),
('"Action Recognition Using Subtensor Constraint"', '"ECCV 2012"', '["Action Recognition", "Dynamic Time Warping", "Fundamental Matrix", "Testing Video", "Testing Frame', '"https://doi.org/10.1007/978-3-642-33712-3_55"', '"Human action recognition from videos draws tremendous interest in the past many years. In this work, we first find that the trifocal tensor resides in a twelve dimensional subspace of the original space if the first two views are already matched and the fundamental matrix between them is known, which we refer to as subtensor. Then we use the subtensor to perform the task of action recognition under three views. We find that treating the two template views separately or not considering the correspondence relation already known between the first two views omits a lot of useful information. Experiments and datasets are designed to demonstrate the effectiveness and improved performance of the proposed approach."'),
('"Action Recognition Using Super Sparse Coding Vector with Spatio-temporal Awareness"', '"ECCV 2014"', '["Gaussian Mixture Model", "Visual Word", "Action Recognition", "Sparse Code", "Human Action Recogni', '"https://doi.org/10.1007/978-3-319-10605-2_47"', '"This paper presents a novel framework for human action recognition based on sparse coding. We introduce an effective coding scheme to aggregate low-level descriptors into the super descriptor vector (SDV). In order to incorporate the spatio-temporal information, we propose a novel approach of super location vector (SLV) to model the space-time locations of local interest points in a much more compact way compared to the spatio-temporal pyramid representations. SDV and SLV are in the end combined as the super sparse coding vector (SSCV) which jointly models the motion, appearance, and location cues. This representation is computationally efficient and yields superior performance while using linear classifiers. In the extensive experiments, our approach significantly outperforms the state-of-the-art results on the two public benchmark datasets, i.e., HMDB51 and YouTube."'),
('"Action Recognition with a Bio\\u2013inspired Feedforward Motion Processing Model: The Richness of Ce', '"ECCV 2008"', '["Motion Estimation", "Action Recognition", "Direction Selectivity", "Motion Representation", "Avera', '"https://doi.org/10.1007/978-3-540-88693-8_14"', '"Here we show that reproducing the functional properties of MT cells with various center\\u2013surround interactions enriches motion representation and improves the action recognition performance. To do so, we propose a simplified bio\\u2013inspired model of the motion pathway in primates: It is a feedforward model restricted to V1-MT cortical layers, cortical cells cover the visual space with a foveated structure and, more importantly, we reproduce some of the richness of center-surround interactions of MT cells. Interestingly, as observed in neurophysiology, our MT cells not only behave like simple velocity detectors, but also respond to several kinds of motion contrasts. Results show that this diversity of motion representation at the MT level is a major advantage for an action recognition task. Defining motion maps as our feature vectors, we used a standard classification method on the Weizmann database: We obtained an average recognition rate of 98.9%, which is superior to the recent results by Jhuang et al. (2007). These promising results encourage us to further develop bio\\u2013inspired models incorporating other brain mechanisms and cortical layers in order to deal with more complex videos."'),
('"Action Recognition with Exemplar Based 2.5D Graph Matching"', '"ECCV 2012"', '["Action Class", "Training Image", "Action Recognition", "Action Image", "Human Action Recognition"]', '"https://doi.org/10.1007/978-3-642-33765-9_13"', '"This paper deals with recognizing human actions in still images. We make two key contributions. (1) We propose a novel, 2.5D representation of action images that considers both view-independent pose information and rich appearance information. A 2.5D graph of an action image consists of a set of nodes that are key-points of the human body, as well as a set of edges that are spatial relationships between the nodes. Each key-point is represented by view-independent 3D positions and local 2D appearance features. The similarity between two action images can then be measured by matching their corresponding 2.5D graphs. (2) We use an exemplar based action classification approach, where a set of representative images are selected for each action class. The selected images cover large within-action variations and carry discriminative information compared with the other classes. This exemplar based representation of action classes further makes our approach robust to pose variations and occlusions. We test our method on two publicly available datasets and show that it achieves very promising performance."'),
('"Action Recognition with Stacked Fisher Vectors"', '"ECCV 2014"', '["Action recognition", "Fisher vectors", "stacked Fisher vectors", "max-margin dimensionality reduct', '"https://doi.org/10.1007/978-3-319-10602-1_38"', '"Representation of video is a vital problem in action recognition. This paper proposes Stacked Fisher Vectors (SFV), a new representation with multi-layer nested Fisher vector encoding, for action recognition. In the first layer, we densely sample large subvolumes from input videos, extract local features, and encode them using Fisher vectors (FVs). The second layer compresses the FVs of subvolumes obtained in previous layer, and then encodes them again with Fisher vectors. Compared with standard FV, SFV allows refining the representation and abstracting semantic information in a hierarchical way. Compared with recent mid-level based action representations, SFV need not to mine discriminative action parts but can preserve mid-level information through Fisher vector encoding in higher layer. We evaluate the proposed methods on three challenging datasets, namely Youtube, J-HMDB, and HMDB51. Experimental results demonstrate the effectiveness of SFV, and the combination of the traditional FV and SFV outperforms state-of-the-art methods on these datasets with a large margin."'),
('"Action-Reaction: Forecasting the Dynamics of Human Interaction"', '"ECCV 2014"', '["Human Interaction", "Nearest Neighbor", "Reward Function", "Continuous State Space", "Feature Coun', '"https://doi.org/10.1007/978-3-319-10584-0_32"', '"Forecasting human activities from visual evidence is an emerging area of research which aims to allow computational systems to make predictions about unseen human actions. We explore the task of activity forecasting in the context of dual-agent interactions to understand how the actions of one person can be used to predict the actions of another. We model dual-agent interactions as an optimal control problem, where the actions of the initiating agent induce a cost topology over the space of reactive poses \\u2013 a space in which the reactive agent plans an optimal pose trajectory. The technique developed in this work employs a kernel-based reinforcement learning approximation of the soft maximum value function to deal with the high-dimensional nature of human motion and applies a mean-shift procedure over a continuous cost function to infer a smooth reaction sequence. Experimental results show that our proposed method is able to properly model human interactions in a high dimensional space of human poses. When compared to several baseline models, results show that our method is able to generate highly plausible simulations of human interaction."'),
('"Active Contour Based Segmentation of 3D Surfaces"', '"ECCV 2008"', '["Active Contour", "Active Contour Model", "Geodesic Curvature", "Implicit Surface", "Signed Distanc', '"https://doi.org/10.1007/978-3-540-88688-4_26"', '"Algorithms incorporating 3D information have proven to be superior to purely 2D approaches in many areas of computer vision including face biometrics and recognition. Still, the range of methods for feature extraction from 3D surfaces is limited. Very popular in 2D image analysis, active contours have been generalized to curved surfaces only recently. Current implementations require a global surface parametrisation. We show that a balloon force cannot be included properly in existing methods, making them unsuitable for applications with noisy data. To overcome this drawback we propose a new algorithm for evolving geodesic active contours on implicit surfaces. We also introduce a new narrowband scheme which results in linear computational complexity. The performance of our model is illustrated on various real and synthetic 3D surfaces."'),
('"Active Deformable Part Models Inference"', '"ECCV 2014"', '["Part Order", "Object Detection", "Average Precision", "Part Evaluation", "Score Likelihood"]', '"https://doi.org/10.1007/978-3-319-10584-0_19"', '"This paper presents an active approach for part-based object detection, which optimizes the order of part filter evaluations and the time at which to stop and make a prediction. Statistics, describing the part responses, are learned from training data and are used to formalize the part scheduling problem as an offline optimization. Dynamic programming is applied to obtain a policy, which balances the number of part evaluations with the classification accuracy. During inference, the policy is used as a look-up table to choose the part order and the stopping time based on the observed filter responses. The method is faster than cascade detection with deformable part models (which does not optimize the part order) with negligible loss in accuracy when evaluated on the PASCAL VOC 2007 and 2010 datasets."'),
('"Active Frame Selection for Label Propagation in Videos"', '"ECCV 2012"', '["Dynamic Programming", "Markov Random Field", "Foreground Object", "Label Propagation", "Video Obje', '"https://doi.org/10.1007/978-3-642-33715-4_36"', '"Manually segmenting and labeling objects in video sequences is quite tedious, yet such annotations are valuable for learning-based approaches to object and activity recognition. While automatic label propagation can help, existing methods simply propagate annotations from arbitrarily selected frames (e.g., the first one) and so may fail to best leverage the human effort invested. We define an active frame selection problem: select k frames for manual labeling, such that automatic pixel-level label propagation can proceed with minimal expected error. We propose a solution that directly ties a joint frame selection criterion to the predicted errors of a flow-based random field propagation model. It selects the set of k frames that together minimize the total mislabeling risk over the entire sequence. We derive an efficient dynamic programming solution to optimize the criterion. Further, we show how to automatically determine how many total frames k should be labeled in order to minimize the total manual effort spent labeling and correcting propagation errors. We demonstrate our method\\u2019s clear advantages over several baselines, saving hours of human effort per video."'),
('"Active Image Labeling and Its Application to Facial Action Labeling"', '"ECCV 2008"', '["Facial Expression", "Bayesian Network", "Facial Expression Recognition", "Facial Action", "Bayesia', '"https://doi.org/10.1007/978-3-540-88688-4_52"', '"For many tasks in computer vision, it is very important to produce the groundtruth data. At present, this is mostly done manually. Manual data labeling is labor-intensive and prone to the human errors. The training data it produces often lacks in both quantity and quality. Fully automatic data labeling, on the other hand, is not feasible and reliable. In this paper, we propose an interactive image labeling technique for efficient and accurate data labeling."'),
('"Active Mask Hierarchies for Object Detection"', '"ECCV 2010"', '["Visual Word", "Object Detection", "Average Precision", "Object Part", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-642-15555-0_4"', '"This paper presents a new object representation, Active Mask Hierarchies (AMH), for object detection. In this representation, an object is described using a mixture of hierarchical trees where the nodes represent the object and its parts in pyramid form. To account for shape variations at a range of scales, a dictionary of masks with varied shape patterns are attached to the nodes at different layers. The shape masks are \\u201cactive\\u201d in that they enable parts to move with different displacements. The masks in this active hierarchy are associated with histograms of words (HOWs) and oriented gradients (HOGs) to enable rich appearance representation of both structured (eg, cat face) and textured (eg, cat body) image regions. Learning the hierarchical model is a latent SVM problem which can be solved by the incremental concave-convex procedure (iCCCP). The resulting system is comparable with the state-of-the-art methods when evaluated on the challenging public PASCAL 2007 and 2009 datasets."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Active Matching"', '"ECCV 2008"', '["Mutual Information", "Template Match", "Search Region", "True Match", "Image Processing Operation"', '"https://doi.org/10.1007/978-3-540-88682-2_7"', '"In the matching tasks which form an integral part of all types of tracking and geometrical vision, there are invariably priors available on the absolute and/or relative image locations of features of interest. Usually, these priors are used post-hoc in the process of resolving feature matches and obtaining final scene estimates, via \\u2018first get candidate matches, then resolve\\u2019 consensus algorithms such as RANSAC. In this paper we show that the dramatically different approach of using priors dynamically to guide a feature by feature matching search can achieve global matching with much fewer image processing operations and lower overall computational cost. Essentially, we put image processing into the loop of the search for global consensus. In particular, our approach is able to cope with significant image ambiguity thanks to a dynamic mixture of Gaussians treatment. In our fully Bayesian algorithm, the choice of the most efficient search action at each step is guided intuitively and rigorously by expected Shannon information gain. We demonstrate the algorithm in feature matching as part of a sequential SLAM system for 3D camera tracking. Robust, real-time matching can be achieved even in the previously unmanageable case of jerky, rapid motion necessitating weak motion modelling and large search regions."'),
('"Active Random Forests: An Application to Autonomous Unfolding of Clothes"', '"ECCV 2014"', '["Active Vision", "Active Random Forests", "Deformable Object Recognition", "Robotic Vision"]', '"https://doi.org/10.1007/978-3-319-10602-1_42"', '"We present Active Random Forests, a novel framework to address active vision problems. State of the art focuses on best viewing parameters selection based on single view classifiers. We propose a multi-view classifier where the decision mechanism of optimally changing viewing parameters is inherent to the classification process. This has many advantages: a) the classifier exploits the entire set of captured images and does not simply aggregate probabilistically per view hypotheses; b) actions are based on learnt disambiguating features from all views and are optimally selected using the powerful voting scheme of Random Forests and c) the classifier can take into account the costs of actions. The proposed framework is applied to the task of autonomously unfolding clothes by a robot, addressing the problem of best viewpoint selection in classification, grasp point and pose estimation of garments. We show great performance improvement compared to state of the art methods."'),
('"Active Surface Reconstruction Using the Gradient Strategy"', '"ECCV 2002"', '["Image-features", "surface geometry", "structure-from-motion", "active vision", "autonomous robot n', '"https://doi.org/10.1007/3-540-47979-1_18"', '"This paper describes the design and implementation of an active surface reconstruction algorithm for two-frame image sequences using passive imaging. A novel strategy based on the statistical grouping of image gradient features is used. It is shown that the gradient of the intensity in an image can successfully be used to drive the direction of the viewer\\u2019s motion. As such, an increased efficiency in the accumulation of information is demonstrated through a significant increase in the convergence rate of the depth estimator (3 to 4 times for the presented results) over traditional passive depth-from-motion. The viewer is considered to be restricted to a short baseline. A maximal-estimation framework is adopted to provide a simple approach for propagating information in a bottom-up fashion in the system. A Kalman filtering scheme is used for accumulating information temporally. The paper provides results for real-textured data to support the findings."'),
('"Activities as Time Series of Human Postures"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_52"', '"This paper presents an exemplar-based approach to detecting and localizing human actions, such as running, cycling, and swinging, in realistic videos with dynamic backgrounds. We show that such activities can be compactly represented as time series of a few snapshots of human-body parts in their most discriminative postures, relative to other activity classes. This enables our approach to efficiently store multiple diverse exemplars per activity class, and quickly retrieve exemplars that best match the query by aligning their short time-series representations. Given a set of example videos of all activity classes, we extract multiscale regions from all their frames, and then learn a sparse dictionary of most discriminative regions. The Viterbi algorithm is then used to track detections of the learned codewords across frames of each video, resulting in their compact time-series representations. Dictionary learning is cast within the large-margin framework, wherein we study the effects of \\u21131 and \\u21132 regularization on the sparseness of the resulting dictionaries. Our experiments demonstrate robustness and scalability of our approach on challenging YouTube videos."'),
('"Activity Forecasting"', '"ECCV 2012"', '["activity forecasting", "inverse optimal control"]', '"https://doi.org/10.1007/978-3-642-33765-9_15"', '"We address the task of inferring the future actions of people from noisy visual input. We denote this task activity forecasting. To achieve accurate activity forecasting, our approach models the effect of the physical environment on the choice of human actions. This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from optimal control theory. Our unified model also integrates several other key elements of activity analysis, namely, destination forecasting, sequence smoothing and transfer learning. As proof-of-concept, we focus on the domain of trajectory-based activity analysis from visual input. Experimental results demonstrate that our model accurately predicts distributions over future actions of individuals. We show how the same techniques can improve the results of tracking algorithms by leveraging information about likely goals and trajectories."'),
('"Activity Group Localization by Modeling the Relations among Participants"', '"ECCV 2014"', '["Action recognition", "group localization", "graphical model"]', '"https://doi.org/10.1007/978-3-319-10590-1_48"', '"Beyond recognizing the actions of individuals, activity group localization aims to determine \\u2018\\u2018who participates in each group\\u2019\\u2019 and \\u2018\\u2018what activity the group performs\\u2019\\u2019. In this paper, we propose a latent graphical model to group participants while inferring each group\\u2019s activity by exploring the relations among them, thus simultaneously addressing the problems of group localization and activity recognition. Our key insight is to exploit the relational graph among the participants. Specifically, each group is represented as a tree with an activity label while relations among groups are modeled as a fully connected graph. Inference of such a graph is reduced into an extended minimum spanning forest problem, which is casted into a max-margin framework. It therefore avoids the limitation of high-ordered hierarchical model and can be solved efficiently. Our model is able to provide strong and discriminative contextual cues for activity recognition and to better interpret scene information for localization. Experiments on three datasets demonstrate that our model achieves significant improvements in activity group. localization and state-of-the-arts performance on activity recognition."'),
('"Activity Recognition in Still Images with Transductive Non-negative Matrix Factorization"', '"ECCV 2014"', '["Still image based action recognition", "Non-negative matrix factorization", "Transductive learning', '"https://doi.org/10.1007/978-3-319-16178-5_56"', '"Still image based activity recognition is a challenging problem due to changes in appearance of persons, articulation in poses, cluttered backgrounds, and absence of temporal features. In this paper, we proposed a novel method to recognize activities from still images based on transductive non-negative matrix factorization (TNMF). TNMF clusters the visual descriptors of each human action in the training images into fixed number of groups meanwhile learns to represent the visual descriptor of test image on the concatenated bases. Since TNMF learns these bases on both training images and test image simultaneously, it learns a more discriminative representation than standard NMF based methods. We developed a multiplicative update rule to solve TNMF and proved its convergence. Experimental results on both laboratory and real-world datasets demonstrate that TNMF consistently outperforms NMF."'),
('"Activity-Based Person Identification Using Discriminative Sparse Projections and Orthogonal Ensembl', '"ECCV 2014"', '["Human identification", "Activity analysis", "Subspace learning", "Sparse coding", "Metric learning', '"https://doi.org/10.1007/978-3-319-16181-5_61"', '"In this paper, we propose an activity-based human identification approach using discriminative sparse projections (DSP) and orthogonal ensemble metric learning (OEML). Unlike gait recognition which recognizes person only from his/her walking activity, this study aims to identify people from more general types of human activities such as eating, drinking, running, and so on. That is because people may not always walk in the scene and gait recognition fails to work in this scenario. Given an activity video, human body mask in each frame is first extracted by background substraction. Then, we propose a DSP method to map these body masks into a low-dimensional subspace and cluster them into a number of clusters to form a dictionary, simultaneously. Subsequently, each video clip is pooled as a histogram feature for activity representation. Lastly, we propose an OEML method to learn a similarity distance metric to exploit discriminative information for recognition. Experimental results show the effectiveness of our proposed approach and better recognition rate is achieved than state-of-the-art methods."'),
('"Adapted Vocabularies for Generic Visual Categorization"', '"ECCV 2006"', '["Feature Vector", "Gaussian Mixture Model", "Visual Word", "Speaker Recognition", "Visual Vocabular', '"https://doi.org/10.1007/11744085_36"', '"Several state-of-the-art Generic Visual Categorization (GVC) systems are built around a vocabulary of visual terms and characterize images with one histogram of visual word counts. We propose a novel and practical approach to GVC based on a universal vocabulary, which describes the content of all the considered classes of images, and class vocabularies obtained through the adaptation of the universal vocabulary using class-specific data. An image is characterized by a set of histograms \\u2013 one per class \\u2013 where each histogram describes whether the image content is best modeled by the universal vocabulary or the corresponding class vocabulary. It is shown experimentally on three very different databases that this novel representation outperforms those approaches which characterize an image with a single histogram."'),
('"Adapting Spectral Scale for Shape from Texture"', '"ECCV 2000"', '["Window Size", "Instantaneous Frequency", "Spectral Window", "Texture Gradient", "Slant Angle"]', '"https://doi.org/10.1007/3-540-45054-8_28"', '"Spectral analysis provides a powerful means of estimating the perspective pose of texture planes. Unfortunately, one of the problems that restricts the utility of the method is the need to set the size of the spectral window. For texture planes viewed under extreme perspective distortion, the spectral frequency density may vary rapidly across the image plane. If the size of the window is mismatched to the underlying texture distribution, then the estimated frequency spectrum may become severely defocussed. This in turn limits the accuracy of perspective pose estimation. The aim in this paper is to describe an adaptive method for setting the size of the spectral window. We provide an analysis which shows that there is a window size that minimises the degree of defocusing. The minimum is located through an analysis of the spectral covariance matrix. We experiment with the new method on both synthetic and real world imagery. This demonstrates that the method provides accurate pose angle estimates, even when the slant angle is large. We also provide a comparison of the accuracy of perspective pose estimation that results both from our adaptive scale method and with one of fixed scale."'),
('"Adapting Visual Category Models to New Domains"', '"ECCV 2010"', '["Support Vector Machine", "Target Domain", "Domain Adaptation", "Source Domain", "Visual Domain"]', '"https://doi.org/10.1007/978-3-642-15561-1_16"', '"Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the first studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the effect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target domain labels and moderate to large changes in the imaging conditions."'),
('"Adaptive and Generic Corner Detection Based on the Accelerated Segment Test"', '"ECCV 2010"', '["Decision Tree", "Corner Detection", "Mask Size", "Memory Access Time", "Ternary Tree"]', '"https://doi.org/10.1007/978-3-642-15552-9_14"', '"The efficient detection of interesting features is a crucial step for various tasks in Computer Vision. Corners are favored cues due to their two dimensional constraint and fast algorithms to detect them. Recently, a novel corner detection approach, FAST, has been presented which outperforms previous algorithms in both computational performance and repeatability. We will show how the accelerated segment test, which underlies FAST, can be significantly improved by making it more generic while increasing its performance.We do so by finding the optimal decision tree in an extended configuration space, and demonstrating how specialized trees can be combined to yield an adaptive and generic accelerated segment test. The resulting method provides high performance for arbitrary environments and so unlike FAST does not have to be adapted to a specific scene structure. We will also discuss how different test patterns affect the corner response of the accelerated segment test."'),
('"Adaptive Metric Registration of 3D Models to Non-rigid Image Trajectories"', '"ECCV 2010"', '["Rotation Error", "Rigid Registration", "Structure From Motion", "Reprojection Error", "Generalise ', '"https://doi.org/10.1007/978-3-642-15558-1_7"', '"This paper addresses the problem of registering a 3D model, represented as a cloud of points lying over a surface, to a set of 2D deforming image trajectories in the image plane. The proposed approach can adapt to a scenario where the 3D model to register is not an exact description of the measured image data. This results in finding the best 2D\\u20133D registration, given the complexity of having both 2D deforming data and a coarse description of the image observations. The method acts in two distinct phases. First, an affine step computes a factorization for both the 2D image data and the 3D model using a joint subspace decomposition. This initial solution is then upgraded by finding the best projection to the image plane complying with the metric constraints given by a scaled orthographic camera. Both steps are computed efficiently in closed-form with the additional feature of being robust to degenerate motions which may possibly affect the 2D image data (i.e. lack of relevant rigid motion). Moreover, we present an extension of the approach for the case of missing image data. Synthetic and real experiments show the robustness of the method in registration tasks such as pose estimation of a talking face using a single 3D model."'),
('"Adaptive Probabilistic Visual Tracking with Incremental Subspace Update"', '"ECCV 2004"', '["Singular Value Decomposition", "Target Object", "Training Image", "Tracking Algorithm", "Appearanc', '"https://doi.org/10.1007/978-3-540-24671-8_37"', '"Visual tracking, in essence, deals with non-stationary data streams that change over time. While most existing algorithms are able to track objects well in controlled environments, they usually fail if there is a significant change in object appearance or surrounding illumination. The reason being that these visual tracking algorithms operate on the premise that the models of the objects being tracked are invariant to internal appearance change or external variation such as lighting or viewpoint. Consequently most tracking algorithms do not update the models once they are built or learned at the outset. In this paper, we present an adaptive probabilistic tracking algorithm that updates the models using an incremental update of eigenbasis. To track objects in two views, we use an effective probabilistic method for sampling affine motion parameters with priors and predicting its location with a maximum a posteriori estimate. Borne out by experiments, we demonstrate the proposed method is able to track objects well under large lighting, pose and scale variation with close to real-time performance."'),
('"Adaptive Registration for Occlusion Robust 3D Face Recognition"', '"ECCV 2012"', '["3D face registration", "regional face registration", "face registration under occlusion"]', '"https://doi.org/10.1007/978-3-642-33885-4_56"', '"Occlusions over facial surfaces cause performance degradation for face registration and recognition systems. In this work, we propose an occlusion-resistant three-dimensional face registration method. First, the nose area is detected on a probe face using curvedness-weighted convex shape index map. Then, probable eye and mouth patches are detected and checked for validity. An adaptive model is constructed by selecting valid patches of the average face model. Finally, registration is handled with the Iterative Closest Point algorithm, where the adaptive model is used as the reference. The UMB-DB face database is used to evaluate the registration system: The nose detector has 100% and 93.90% accuracy, for the non-occluded and occluded images, respectively. A simple global depth-based recognition experiment is done to evaluate the registration performance: Our adaptive model-based registration scheme improves rank-1 recognition rate by 16%, when compared with the nose-based alignment approach."'),
('"Adaptive Regularization for Image Segmentation Using Local Image Curvature Cues"', '"ECCV 2010"', '["Active Contour", "Adaptive Weight", "Image Segmentation Technique", "Texture Edge", "Regularizatio', '"https://doi.org/10.1007/978-3-642-15561-1_47"', '"Image segmentation techniques typically require proper weighting of competing data fidelity and regularization terms. Conventionally, the associated parameters are set through tedious trial and error procedures and kept constant over the image. However, spatially varying structural characteristics, such as object curvature, combined with varying noise and imaging artifacts, significantly complicate the selection process of segmentation parameters. In this work, we propose a novel approach for automating the parameter selection by employing a robust structural cue to prevent excessive regularization of trusted (i.e. low noise) high curvature image regions. Our approach autonomously adapts local regularization weights by combining local measures of image curvature and edge evidence that are gated by a signal reliability measure. We demonstrate the utility and favorable performance of our approach within two major segmentation frameworks, graph cuts and active contours, and present quantitative and qualitative results on a variety of natural and medical images."'),
('"Adaptive Rendering for Large-Scale Skyline Characterization and Matching"', '"ECCV 2012"', '["Transformation Model", "Query Image", "Query Skyline", "Skyline Point", "Reprojection Error"]', '"https://doi.org/10.1007/978-3-642-33863-2_17"', '"We propose an adaptive rendering approach for large-scale skyline characterization and matching with applications to automated geo-tagging of photos and images. Given an image, our system automatically extracts the skyline and then matches it to a database of reference skylines extracted from rendered images using digital elevation data (DEM). The sampling density of these rendering locations determines both the accuracy and the speed of skyline matching. The proposed approach successfully combines global planning and local greedy search strategies to select new rendering locations incrementally. We report quantitative and qualitative results from synthesized and real experiments, where we achieve a computational speedup of around 4X."'),
('"Adaptive Rest Condition Potentials: Second Order Edge-Preserving Regularization"', '"ECCV 2002"', '["Edge-preserving regularization", "image restoration", "segmentation", "anisotropic diffusion"]', '"https://doi.org/10.1007/3-540-47969-4_8"', '"The propose of this paper is to introduce a new regularization formulation for inverse problems in computer vision and image processing that allows one to reconstruct second order piecewise smooth images, that is, images consisting of an assembly of regions with almost constant value, almost constant slope or almost constant curvature. This formulation is based on the idea of using potential functions that correspond to springs or thin plates with an adaptive rest condition. Efficient algorithms for computing the solution, and examples illustrating the performance of this scheme, compared with other known regularization schemes are presented as well."'),
('"Adaptive Visual Obstacle Detection for Mobile Robots Using Monocular Camera and Ultrasonic Sensor"', '"ECCV 2012"', '["Obstacle detection", "ViBe", "ultrasonic sensor", "mobile robot"]', '"https://doi.org/10.1007/978-3-642-33868-7_52"', '"This paper presents a novel vision based obstacle detection algorithm that is adapted from a powerful background subtraction algorithm: ViBe (VIsual Background Extractor). We describe an adaptive obstacle detection method using monocular color vision and an ultrasonic distance sensor. Our approach assumes an obstacle free region in front of the robot in the initial frame. However, the method dynamically adapts to its environment in the succeeding frames. The adaptation is performed using a model update rule based on using ultrasonic distance sensor reading. Our detailed experiments validate the proposed concept and ultrasonic sensor based model update."'),
('"Adasens Advanced Driver Assistance Systems Live Demo"', '"ECCV 2012"', '["Adaptive Cruise Control", "Advance Driver Assistance System", "Stereo System", "Vulnerable Road Us', '"https://doi.org/10.1007/978-3-642-33885-4_76"', '"Computer Vision is starting to play a major role in Automotive Industry. FICOSA-Adasens Automotive is developing Advanced Driver Assistance Systems (ADAS). ADAS are systems that help the driver with the aim to increase car and road safety. In this context, Adasens is proposing a live show consisting of a fully operational stereo vision obstacle detector on-board of an Adasens demo car."'),
('"ADICT: Accurate Direct and Inverse Color Transformation"', '"ECCV 2010"', '["Color Constancy", "Color Match", "Output Color", "Ideal Device", "Direct Transfer Function"]', '"https://doi.org/10.1007/978-3-642-15561-1_6"', '"A color transfer function describes the relationship between the input and the output colors of a device. Computing this function is difficult when devices do not follow traditionally coveted properties like channel independency or color constancy, as is the case with most commodity capture and display devices (like projectors, camerass and printers). In this paper we present a novel representation for the color transfer function of any device, using higher-dimensional B\\u00e9zier patches, that does not rely on any restrictive assumptions and hence can handle devices that do not behave in an ideal manner. Using this representation and a novel reparametrization technique, we design a color transformation method that is more accurate and free of local artifacts compared to existing color transformation methods. We demonstrate this method\\u2019s generality by using it for color management on a variety of input and output devices. Our method shows significant improvement in the appearance of seamlessness when used in the particularly demanding application of color matching across multi-projector displays or multi-camera systems. Finally we demonstrate that our color transformation method can be performed efficiently using a real-time GPU implementation."'),
('"Adjacency Matrix Construction Using Sparse Coding for Label Propagation"', '"ECCV 2012"', '["Adjacency Matrix", "Training Image", "Sparse Code", "Neural Information Processing System", "Label', '"https://doi.org/10.1007/978-3-642-33885-4_32"', '"Graph-based semi-supervised learning algorithms have attracted increasing attentions recently due to their superior performance in dealing with abundant unlabeled data and limited labeled data via the label propagation. The principle issue of constructing a graph is how to accurately measure the similarity between two data examples. In this paper, we propose a novel approach to measure the similarities among data points by means of the local linear reconstruction of their corresponding sparse codes. Clearly, the sparse codes of data examples not only preserve their local manifold semantics but can significantly boost the discriminative power among different classes. Moreover, the sparse property helps to dramatically reduce the intensive computation and storage requirements. The experimental results over the well-known dataset Caltech-101 demonstrate that our proposed similarity measurement method delivers better performance of the label propagation."'),
('"Adjustment Learning and Relevant Component Analysis"', '"ECCV 2002"', '["Face Recognition", "Class Label", "Image Retrieval", "Query Image", "Unsupervised Learning"]', '"https://doi.org/10.1007/3-540-47979-1_52"', '"We propose a new learning approach for image retrieval, which we call adjustment learning, and demonstrate its use for face recognition and color matching. Our approach is motivated by a frequently encountered problem, namely, that variability in the original data representation which is not relevant to the task may interfere with retrieval and make it very difficult. Our key observation is that in real applications of image retrieval, data sometimes comes in small chunks - small subsets of images that come from the same (but unknown) class. This is the case, for example, when a query is presented via a short video clip. We call these groups chunklets, and we call the paradigm which uses chunklets for unsupervised learning adjustment learning. Within this paradigm we propose a linear scheme, which we call Relevant Component Analysis; this scheme uses the information in such chunklets to reduce irrelevant variability in the data while amplifying relevant variability. We provide results using our method on two problems: face recognition (using a database publicly available on the web), and visual surveillance (using our own data). In the latter application chunklets are obtained automatically from the data without the need of supervision."'),
('"Affine Puzzle: Realigning Deformed Object Fragments without Correspondences"', '"ECCV 2010"', '["Iterative Close Point", "Partial Match", "Segmentation Error", "Iterative Close Point Algorithm", ', '"https://doi.org/10.1007/978-3-642-15552-9_56"', '"This paper is addressing the problem of realigning broken objects without correspondences. We consider linear transformations between the object fragments and present the method through 2D and 3D affine transformations. The basic idea is to construct and solve a polynomial system of equations which provides the unknown parameters of the alignment. We have quantitatively evaluated the proposed algorithm on a large synthetic dataset containing 2D and 3D images. The results show that the method performs well and robust against segmentation errors. We also present experiments on 2D real images as well as on volumetric medical images applied to surgical planning."'),
('"Affine Subspace Representation for Feature Description"', '"ECCV 2014"', '["Feature Description", "Multiple View", "Local Patch", "Warping Function", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-319-10584-0_7"', '"This paper proposes a novel Affine Subspace Representation (ASR) descriptor to deal with affine distortions induced by viewpoint changes. Unlike the traditional local descriptors such as SIFT, ASR inherently encodes local information of multi-view patches, making it robust to affine distortions while maintaining a high discriminative ability. To this end, PCA is used to represent affine-warped patches as PCA-patch vectors for its compactness and efficiency. Then according to the subspace assumption, which implies that the PCA-patch vectors of various affine-warped patches of the same keypoint can be represented by a low-dimensional linear subspace, the ASR descriptor is obtained by using a simple subspace-to-point mapping. Such a linear subspace representation could accurately capture the underlying information of a keypoint (local structure) under multiple views without sacrificing its distinctiveness. To accelerate the computation of ASR descriptor, a fast approximate algorithm is proposed by moving the most computational part (i.e., warp patch under various affine transformations) to an offline training stage. Experimental results show that ASR is not only better than the state-of-the-art descriptors under various image transformations, but also performs well without a dedicated affine invariant detector when dealing with viewpoint changes."'),
('"Affine-Invariant Multi-reference Shape Priors for Active Contours"', '"ECCV 2006"', '["Segmentation Result", "Active Contour", "Shape Descriptor", "Canonical Representation", "Normalize', '"https://doi.org/10.1007/11744047_46"', '"We present a new way of constraining the evolution of a region-based active contour with respect to a set of reference shapes. The approach is based on a description of shapes by the Legendre moments computed from their characteristic function. This provides a region-based representation that can handle arbitrary shape topologies. Moreover, exploiting the properties of moments, it is possible to include intrinsic affine invariance in the descriptor, which solves the issue of shape alignment without increasing the number of d.o.f. of the initial problem and allows introducing geometric shape variabilities. Our new shape prior is based on a distance between the descriptors of the evolving curve and a reference shape. The proposed model naturally extends to the case where multiple reference shapes are simultaneously considered. Minimizing the shape energy, leads to a geometric flow that does not rely on any particular representation of the contour and can be implemented with any contour evolution algorithm. We introduce our prior into a two-class segmentation functional, showing its benefits on segmentation results in presence of severe occlusions and clutter. Examples illustrate the ability of the model to deal with large affine deformation and to take into account a set of reference shapes of different topologies."'),
('"Affordance-Based Object Recognition Using Interactions Obtained from a Utility Maximization Princip', '"ECCV 2014"', '["Affordance", "Sensorimotor object recognition", "Information gain"]', '"https://doi.org/10.1007/978-3-319-16181-5_29"', '"The interaction of biological agents within the real world is based on their abilities and the affordances of the environment. By contrast, the classical view of perception considers only sensory features, as do most object recognition models. Only a few models make use of the information provided by the integration of sensory information as well as possible or executed actions. Neither the relations shaping such an integration nor the methods for using this integrated information in appropriate representations are yet entirely clear. We propose a probabilistic model integrating the two information sources in one system. The recognition process is equipped with an utility maximization principle to obtain optimal interactions with the environment"'),
('"Affordances in Video Surveillance"', '"ECCV 2014"', '["Surveillance", "Attention modeling", "Affordances"]', '"https://doi.org/10.1007/978-3-319-16181-5_28"', '"This paper articulates the concept of affordances use as the building block of an automated video surveillance system which learns and evolves over time. It grounds its arguments on the basis of a visual attention hardware and affordances."'),
('"Age Invariant Face Verification with Relative Craniofacial Growth Model"', '"ECCV 2012"', '["Face Recognition", "Facial Image", "Equal Error Rate", "Grassmann Manifold", "Facial Shape"]', '"https://doi.org/10.1007/978-3-642-33783-3_5"', '"Age-separated facial images usually have significant changes in both shape and texture. Although many face recognition algorithms have been proposed in the last two decades, the problem of recognizing facial images across aging remains an open problem. In this paper, we propose a relative craniofacial growth model which is based on the science of craniofacial anthropometry. Compared to the traditional craniofacial growth model, the proposed method introduces a set of linear equations on the relative growth parameters which can be easily applied for facial image verification across aging. We then integrate the relative growth model with the Grassmann manifold and the SVM classifier. We also demonstrate how knowing the age could improve shape-based face recognition algorithms. Experiments show that the proposed method is able to mitigate the variations caused by the aging progress and thus effectively improve the performance of open-set face verification across aging."'),
('"Algebraic Methods for Direct and Feature Based Registration of Diffusion Tensor Images"', '"ECCV 2006"', '["Singular Value Decomposition", "Deformation Model", "Point Correspondence", "Single Voxel", "Regis', '"https://doi.org/10.1007/11744078_40"', '"We present an algebraic solution to both direct and feature-based registration of diffusion tensor images under various local deformation models. In the direct case, we show how to linearly recover a local deformation from the partial derivatives of the tensor using the so-called Diffusion Tensor Constancy Constraint, a generalization of the brightness constancy constraint to diffusion tensor data. In the feature-based case, we show that the tensor reorientation map can be found in closed form by exploiting the spectral properties of the rotation group. Given this map, solving for an affine deformation becomes a linear problem. We test our approach on synthetic, brain and heart diffusion tensor images."'),
('"Alias-Free Interpolation"', '"ECCV 2006"', '["Spectral Component", "High Frequency Component", "Interpolate Image", "Interpolate Signal", "Itera', '"https://doi.org/10.1007/11744085_20"', '"In this paper we study the possibility of removing aliasing in a scene from a single observation by designing an alias-free upsampling scheme. We generate the unknown high frequency components of the given partially aliased (low resolution) image by minimizing the total variation of the interpolant subject to the constraint that part of unaliased spectral components in the low resolution observation are known precisely and under the assumption of sparsity in the data. This provides a mathematical basis for exact reproduction of high frequency components with probability approaching one, from their aliased observation. The primary application of the given approach would be in super-resolution imaging."'),
('"Aligning Sequences and Actions by Maximizing Space-Time Correlations"', '"ECCV 2006"', '["Video Sequence", "Input Sequence", "Alignment Algorithm", "Temporal Derivative", "Pyramid Level"]', '"https://doi.org/10.1007/11744078_42"', '"We introduced an algorithm for sequence alignment, based on maximizing local space-time correlations. Our algorithm aligns sequences of the same action performed at different times and places by different people, possibly at different speeds, and wearing different clothes. Moreover, the algorithm offers a unified approach to the problem of sequence alignment for a wide range of scenarios (e.g., sequence pairs taken with stationary or jointly moving cameras, with the same or different photometric properties, with or without moving objects). Our algorithm is applied directly to the dense space-time intensity information of the two sequences (or to filtered versions of them). This is done without prior segmentation of foreground moving objects, and without prior detection of corresponding features across the sequences. Examples of challenging sequences with complex actions are shown, including ballet dancing, actions in the presence of other complex scene dynamics (clutter), as well as multi-sensor sequence pairs."'),
('"Aligning Spatio-Temporal Signals on a Special Manifold"', '"ECCV 2010"', '["Dynamic Time Warping", "Optimal Alignment", "Point Trajectory", "Alignment Problem", "Spatial Alig', '"https://doi.org/10.1007/978-3-642-15555-0_40"', '"We investigate the spatio-temporal alignment of videos or features/signals extracted from them. Specifically, we formally define an alignment manifold and formulate the alignment problem as an optimization procedure on this non-linear space by exploiting its intrinsic geometry. We focus our attention on semantically meaningful videos or signals, e.g., those describing or capturing human motion or activities, and propose a new formalism for temporal alignment accounting for executing rate variations among realizations of the same video event. By construction, we address this static and deterministic alignment task in a dynamic and stochastic manner: we regard the search for optimal alignment parameters as a recursive state estimation problem for a particular dynamic system evolving on the alignment manifold. Consequently, a Sequential Importance Sampling iteration on the alignment manifold is designed for effective and efficient alignment. We demonstrate the performance on several types of input data that arise in vision problems."'),
('"All the Images of an Outdoor Scene"', '"ECCV 2002"', '["Ground Truth Data", "Outdoor Scene", "High Dynamic Range Image", "Scene Point", "Scene Structure"]', '"https://doi.org/10.1007/3-540-47977-5_10"', '"The appearance of an outdoor scene depends on a variety of factors such as viewing geometry, scene structure and reflectance (BRDF or BTF), illumination (sun, moon, stars, street lamps), atmospheric condition (clear air, fog, rain) and weathering (or aging) of materials. Over time, these factors change, altering the way a scene appears. A large set of images is required to study the entire variability in scene appearance. In this paper, we present a database of high quality registered and calibrated images of a fixed outdoor scene captured every hour for over 5 months. The dataset covers a wide range of daylight and night illumination conditions, weather conditions and seasons. We describe in detail the image acquisition and sensor calibration procedures. The images are tagged with a variety of ground truth data such as weather and illumination conditions and actual scene depths. This database has potential implications for vision, graphics, image processing and atmospheric sciences and can be a testbed for many algorithms. We describe an example application - image analysis in bad weather - and show how this method can be evaluated using the images in the database. The database is available online at http://www.cs.columbia.edu/CAVE/. The data collection is ongoing and we plan to acquire images for one year."'),
('"All-In-Focus Synthetic Aperture Imaging"', '"ECCV 2014"', '["occluded object imaging", "all-in-focus synthetic aperture imaging", "multiple layer visibility pr', '"https://doi.org/10.1007/978-3-319-10599-4_1"', '"Heavy occlusions in cluttered scenes impose significant challenges to many computer vision applications. Recent light field imaging systems provide new see-through capabilities through synthetic aperture imaging (SAI) to overcome the occlusion problem. Existing synthetic aperture imaging methods, however, emulate focusing at a specific depth layer but is incapable of producing an all-in-focus see-through image. Alternative in-painting algorithms can generate visually plausible results but can not guarantee the correctness of the result. In this paper, we present a novel depth free all-in-focus SAI technique based on light-field visibility analysis. Specifically, we partition the scene into multiple visibility layers to directly deal with layer-wise occlusion and apply an optimization framework to propagate the visibility information between multiple layers. On each layer, visibility and optimal focus depth estimation is formulated as a multiple label energy minimization problem. The energy integrates the visibility mask from previous layers, multi-view intensity consistency, and depth smoothness constraint. We compare our method with the state-of-the-art solutions. Extensive experimental results with qualitative and quantitative analysis demonstrate the effectiveness and superiority of our approach."'),
('"Alpha Matting of Motion-Blurred Objects in Bracket Sequence Images"', '"ECCV 2014"', '["Alpha matting", "motion blur", "exposure bracketing"]', '"https://doi.org/10.1007/978-3-319-10578-9_9"', '"We present a method that utilizes bracket sequence images to automatically extract the alpha matte of a motion-blurred object. This method makes use of a sharp, short-exposure snapshot in the sequence to help overcome major challenges in this task, including blurred object detection, spatially-variant object motion, and foreground/background color ambiguity. A key component of our matte estimation is the inference of approximate, spatially-varying motion of the blurred object with the help of the sharp snapshot, as this motion information provides important constraints on the aforementioned issues. In addition, we take advantage of other relationships that exist between a pair of consecutive short-exposure and long-exposure frames, such as common background areas and consistencies in foreground appearance. With this technique, we demonstrate successful alpha matting results on a variety of moving objects including non-rigid human motion."'),
('"Ambiguous Configurations for 3-View Projective Reconstruction"', '"ECCV 2000"', '["Critical Surface", "Critical Curve", "Quadric Surface", "Fundamental Matrice", "Projective Reconst', '"https://doi.org/10.1007/3-540-45054-8_60"', '"The critical configurations for projective reconstruction from three views are discussed. A set of cameras and points is said to be critical if the projected image points are insufficient to determine the placement of the points and cameras uniquely, up to projective transformation. For two views, the classification of critical configurations is well known - the configuration is critical if and only if the points and camera centres all lie on a ruled quadric. For three views the critical configurations have not been identified previously. In this paper it is shown that for any placement of three given cameras there always exists a critical set consisting of a fourth-degree curve - any number of points on the curve form a critical set for the three cameras. Dual to this result, for a set of seven points there exists a fourth-degree curve such that a configuration of any number of cameras placed on this curve is critical for the set of points. Other critical configurations exist in cases where the points all lie in a plane, or one of the cameras lies on a twisted cubic."'),
('"Ambrosio-Tortorelli Segmentation of Stochastic Images"', '"ECCV 2010"', '["Polynomial Chaos", "Polynomial Chaos Expansion", "Stochastic Collocation", "Street Scene", "Stocha', '"https://doi.org/10.1007/978-3-642-15555-0_19"', '"We present an extension of the classical Ambrosio-Tortorelli approximation of the Mumford-Shah approach for the segmentation of images with uncertain gray values resulting from measurement errors and noise. Our approach yields a reliable precision estimate for the segmentation result, and it allows to quantify the robustness of edges in noisy images and under gray value uncertainty. We develop an ansatz space for such images by identifying gray values with random variables. The use of these stochastic images in the minimization of energies of Ambrosio-Tortorelli type leads to stochastic partial differential equations for the stochastic smoothed image and a stochastic phase field for the edge set. For their discretization we utilize the generalized polynomial chaos expansion and the generalized spectral decomposition (GSD) method. We demonstrate the performance of the method on artificial data as well as real medical ultrasound data."'),
('"An Accuracy Certified Augmented Reality System for Therapy Guidance"', '"ECCV 2004"', '["Augmented Reality", "Video Image", "Compute Tomog", "Validation Index", "Registration Error"]', '"https://doi.org/10.1007/978-3-540-24672-5_7"', '"Our purpose is to provide an augmented reality system for Radio-Frequency guidance that could superimpose a 3D model of the liver, its vessels and tumors (reconstructed from CT images) on external video images of the patient. In this paper, we point out that clinical usability not only need the best affordable registration accuracy, but also a certification that the required accuracy is met, since clinical conditions change from one intervention to the other. Beginning by addressing accuracy performances, we show that a 3D/2D registration based on radio-opaque fiducials is more adapted to our application constraints than other methods. Then, we outline a lack in their statistical assumptions which leads us to the derivation of a new extended 3D/2D criterion. Careful validation experiments on real data show that an accuracy of 2 mm can be achieved in clinically relevant conditions, and that our new criterion is up to 9% more accurate, while keeping a computation time compatible with real-time at 20 to 40 Hz."'),
('"An Accurate and Efficient Bayesian Method for Automatic Segmentation of Brain MRI"', '"ECCV 2002"', '["Markov Random Field", "Automatic Segmentation", "Expectation Maximization Algorithm", "Intensity M', '"https://doi.org/10.1007/3-540-47979-1_38"', '"Automatic 3D segmentation of the brain from MR scans is a challenging problem that has received enormous amount of attention lately. Of the techniques reported in literature, very few are fully automatic. In this paper, we present an efficient and accurate, fully automatic 3D segmentation procedure for brain MR scans. It has several salient features namely, (1) instead of a single multiplicative bias field that affects all tissue intensities, separate parametric smooth models are used for the intensity of each class. This may be a more realistic model and avoids the need for a logarithmic transformation. (2) A brain atlas is used in conjunction with a robust registration procedure to find a non-rigid transformation that maps the standard brain to the specimen to be segmented. This transformation is then used to: segment the brain from non-brain tissue; compute prior probabilities for each class at each voxel location and find an appropriate automatic initialization. (3) Finally, a novel algorithm is presented which is a variant of the EM procedure, that incorporates a fast and accurate way to find optimal segmentations, given the intensity models along with the spatial coherence assumption. Experimental results with both synthetic and real data are included, as well as comparisons of the performance of our algorithm with that of other published methods."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"An Active Patch Model for Real World Appearance Reconstruction"', '"ECCV 2014"', '["Dense mapping", "Deformable patches", "Gauss-Newton optimization"]', '"https://doi.org/10.1007/978-3-319-16178-5_31"', '"Dense mapping has been a very active field of research in recent years, promising various new application in computer vision, computer graphics, robotics, etc. Most of the work done on dense mapping use low-level features, such as occupancy grid, with some very recent work using high-level features, such as objects. In our work we use an active patch model to learn the prominent, primitive shapes commonly found in indoor environments. This model is then fitted to coming data to reconstruct the 3D scene. We use Gauss-Newton method to jointly optimize for appearance reconstruction error and geometric transformation differences. Finally we compare our results with Kinect Fusion [6]."'),
('"An Active Patch Model for Real World Texture and Appearance Classification"', '"ECCV 2014"', '["Active Patch", "Texture Classification", "Appearance Recognition"]', '"https://doi.org/10.1007/978-3-319-10578-9_10"', '"This paper addresses the task of natural texture and appearance classification. Our goal is to develop a simple and intuitive method that performs at state of the art on datasets ranging from homogeneous texture (e.g., material texture), to less homogeneous texture (e.g., the fur of animals), and to inhomogeneous texture (the appearance patterns of vehicles). Our method uses a bag-of-words model where the features are based on a dictionary of active patches. Active patches are raw intensity patches which can undergo spatial transformations (e.g., rotation and scaling) and adjust themselves to best match the image regions. The dictionary of active patches is required to be compact and representative, in the sense that we can use it to approximately reconstruct the images that we want to classify. We propose a probabilistic model to quantify the quality of image reconstruction and design a greedy learning algorithm to obtain the dictionary. We classify images using the occurrence frequency of the active patches. Feature extraction is fast (about 100 ms per image) using the GPU. The experimental results show that our method improves the state of the art on a challenging material texture benchmark dataset (KTH-TIPS2). To test our method on less homogeneous or inhomogeneous images, we construct two new datasets consisting of appearance image patches of animals and vehicles cropped from the PASCAL VOC dataset. Our method outperforms competing methods on these datasets."'),
('"An Adaptive Window Approach for Image Smoothing and Structures Preserving"', '"ECCV 2004"', '["Mean Square Error", "Noise Variance", "Image Decomposition", "Image Smoothing", "Adaptive Choice"]', '"https://doi.org/10.1007/978-3-540-24672-5_11"', '"A novel adaptive smoothing approach is proposed for image smoothing and discontinuities preservation. The method is based on a locally piecewise constant modeling of the image with an adaptive choice of a window around each pixel. The adaptive smoothing technique associates with each pixel the weighted sum of data points within the window. We describe a statistical method for choosing the optimal window size, in a manner that varies at each pixel, with an adaptive choice of weights for every pair of pixels in the window. We further investigate how the I-divergence could be used to stop the algorithm. It is worth noting the proposed technique is data-driven and fully adaptive. Simulation results show that our algorithm yields promising smoothing results on a variety of real images."'),
('"An Affine Invariant Interest Point Detector"', '"ECCV 2002"', '["Image features", "matching", "recognition"]', '"https://doi.org/10.1007/3-540-47969-4_9"', '"This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images."'),
('"An Affine Invariant of Parallelograms and Its Application to Camera Calibration and 3D Reconstructi', '"ECCV 2006"', '["Constraint Equation", "Camera Calibration", "Intrinsic Parameter", "Camera Center", "Camera Coordi', '"https://doi.org/10.1007/11744047_15"', '"In this work, a new affine invariant of parallelograms is introduced, and the explicit constraint equations between the intrinsic matrix of a camera and the similar invariants of a parallelogram or a parallelepiped are established using this affine invariant. Camera calibration and 3D reconstruction from parallelograms are systematically studied based on these constraints. The proposed theoretical results and algorithms have wide applicability as parallelograms and parallelepipeds are not rare in man-made scenes. Experimental results on synthetic and real images validate the proposed approaches."'),
('"An Affine Invariant Salient Region Detector"', '"ECCV 2004"', '["Object Class", "Region Detector", "Saliency Detector", "Sampling Window", "Background Clutter"]', '"https://doi.org/10.1007/978-3-540-24670-1_18"', '"In this paper we describe a novel technique for detecting salient regions in an image. The detector is a generalization to affine invariance of the method introduced by Kadir and Brady [10]. The detector deems a region salient if it exhibits unpredictability in both its attributes and its spatial scale."'),
('"An Analysis of Errors in Graph-Based Keypoint Matching and Proposed Solutions"', '"ECCV 2014"', '["Reference Image", "Target Image", "Markov Random Field", "Stereo Match", "Graph Match"]', '"https://doi.org/10.1007/978-3-319-10584-0_10"', '"An error occurs in graph-based keypoint matching when keypoints in two different images are matched by an algorithm but do not correspond to the same physical point. Most previous methods acquire keypoints in a black-box manner, and focus on developing better algorithms to match the provided points. However to study the complete performance of a matching system one has to study errors through the whole matching pipeline, from keypoint detection, candidate selection to graph optimisation. We show that in the full pipeline there are six different types of errors that cause mismatches. We then present a matching framework designed to reduce these errors. We achieve this by adapting keypoint detectors to better suit the needs of graph-based matching, and achieve better graph constraints by exploiting more information from their keypoints. Our framework is applicable in general images and can handle clutter and motion discontinuities. We also propose a method to identify many mismatches a posteriori based on Left-Right Consistency inspired by stereo matching due to the asymmetric way we detect keypoints and define the graph."'),
('"An AR Human Computer Interface for Object Localization in a Cognitive Vision Framework"', '"CVHCI 2004"', '["3D interaction device", "active cameras", "real-time pose computation", "augmented reality", "mobi', '"https://doi.org/10.1007/978-3-540-24837-8_17"', '"In the European cognitive vision project VAMPIRE (IST-2001-34401), mobile AR-kits are used for interactive teaching of a visual active memory. This is achieved by 3D augmented pointing, which combines inside-out tracking for head pose recovery and 3D stereo HCI in an office environment. An artificial landmark is used to establish a global coordinate system, and a sparse reconstruction of the office provides natural landmarks (corners). This paper describes the basic idea of the 3D cursor. In addition to the mobile system, at least one camera is used to obtain different views of an object which could be employed to improve e.g. view based object recognition. Accuracy of the 3D cursor for pointing in a scene coordinate system is evaluated experimentally."'),
('"An Effective Approach to 3D Deformable Surface Tracking"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_57"', '"The key challenge with 3D deformable surface tracking arises from the difficulty in estimating a large number of 3D shape parameters from noisy observations. A recent state-of-the-art approach attacks this problem by formulating it as a Second Order Cone Programming (SOCP) feasibility problem. The main drawback of this solution is the high computational cost. In this paper, we first reformulate the problem into an unconstrained quadratic optimization problem. Instead of handling a large set of complicated SOCP constraints, our new formulation can be solved very efficiently by resolving a set of sparse linear equations. Based on the new framework, a robust iterative method is employed to handle large outliers. We have conducted an extensive set of experiments to evaluate the performance on both synthetic and real-world testbeds, from which the promising results show that the proposed algorithm not only achieves better tracking accuracy, but also executes significantly faster than the previous solution."'),
('"An Effective Method for Illumination-Invariant Representation of Color Images"', '"ECCV 2012"', '["Illumination-invariant", "color image", "highlight detection", "spectral reflectance estimation"]', '"https://doi.org/10.1007/978-3-642-33868-7_40"', '"This paper proposes a method for illumination-invariant representation of natural color images. The invariant representation is derived, not using spectral reflectance, but using only RGB camera outputs. We suppose that the materials of target objects are composed of dielectric or metal, and the surfaces include illumination effects such as highlight, gloss, or specularity. We preset the procedure for realizing the invariant representation in three steps: (1) detection of specular highlight, (2) illumination color estimation, and (3) invariant representation for reflectance color. The performance of the proposed method is examined in experiments using real-world objects including metals and dielectrics in detail. The limitation of the method is also discussed. Finally, the proposed representation is applied to the edge detection problem of natural color images."'),
('"An Efficient Dense and Scale-Invariant Spatio-Temporal Interest Point Detector"', '"ECCV 2008"', '["Action Recognition", "Interest Point", "Interest Point Detector", "Scale Selection", "Saliency Mea', '"https://doi.org/10.1007/978-3-540-88688-4_48"', '"Over the years, several spatio-temporal interest point detectors have been proposed. While some detectors can only extract a sparse set of scale-invariant features, others allow for the detection of a larger amount of features at user-defined scales. This paper presents for the first time spatio-temporal interest points that are at the same time scale-invariant (both spatially and temporally) and densely cover the video content. Moreover, as opposed to earlier work, the features can be computed efficiently. Applying scale-space theory, we show that this can be achieved by using the determinant of the Hessian as the saliency measure. Computations are speeded-up further through the use of approximative box-filter operations on an integral video structure. A quantitative evaluation and experimental results on action recognition show the strengths of the proposed detector in terms of repeatability, accuracy and speed, in comparison with previously proposed detectors."'),
('"An Efficient Graph Cut Algorithm for Computer Vision Problems"', '"ECCV 2010"', '["Sink Node", "Texture Synthesis", "Grid Graph", "Voronoi Region", "Distance Label"]', '"https://doi.org/10.1007/978-3-642-15558-1_40"', '"Graph cuts has emerged as a preferred method to solve a class of energy minimization problems in computer vision. It has been shown that graph cut algorithms designed keeping the structure of vision based flow graphs in mind are more efficient than known strongly polynomial time max-flow algorithms based on preflow push or shortest augmenting path paradigms [1]. We present here a new algorithm for graph cuts which not only exploits the structural properties inherent in image based grid graphs but also combines the basic paradigms of max-flow theory in a novel way. The algorithm has a strongly polynomial time bound. It has been bench-marked using samples from Middlebury [2] and UWO [3] database. It runs faster on all 2D samples and is at least two to three times faster on 70% of 2D and 3D samples in comparison to the algorithm reported in [1]."'),
('"An Efficient Method for Tensor Voting Using Steerable Filters"', '"ECCV 2006"', '["Graphical Processing Unit", "Noisy Image", "Graphical Processing Unit Implementation", "Local Imag', '"https://doi.org/10.1007/11744085_18"', '"In many image analysis applications there is a need to extract curves in noisy images. To achieve a more robust extraction, one can exploit correlations of oriented features over a spatial context in the image. Tensor voting is an existing technique to extract features in this way. In this paper, we present a new computational scheme for tensor voting on a dense field of rank-2 tensors. Using steerable filter theory, it is possible to rewrite the tensor voting operation as a linear combination of complex-valued convolutions. This approach has computational advantages since convolutions can be implemented efficiently. We provide speed measurements to indicate the gain in speed, and illustrate the use of steerable tensor voting on medical applications."'),
('"An Efficient Parallel Strategy for Matching Visual Self-similarities in Large Image Databases"', '"ECCV 2012"', '["Similar Image", "Ambiguous Word", "Template Image", "Photo Collection", "Large Image Database"]', '"https://doi.org/10.1007/978-3-642-33863-2_28"', '"Due to high interest of social online systems, there exists a huge and still increasing amount of image data in the web. In order to handle this massive amount of visual information, algorithms often need to be redesigned. In this work, we developed an efficient approach to find visual similarities between images that runs completely on GPU and is applicable to large image databases. Based on local self-similarity descriptors, the approach finds similarities even across modalities. Given a set of images, a database is created by storing all descriptors in an arrangement suitable for parallel GPU-based comparison. A novel voting-scheme further considers the spatial layout of descriptors with hardly any overhead. Thousands of images are searched in only a few seconds. We apply our algorithm to cluster a set of image responses to identify various senses of ambiguous words and re-tag similar images with missing tags."'),
('"An Evaluation of Two Automatic Landmark Building Discovery Algorithms for City Reconstruction"', '"ECCV 2010"', '["Ground Truth", "Visual Word", "Spectral Cluster", "Query Expansion", "Seed Image"]', '"https://doi.org/10.1007/978-3-642-35740-4_24"', '"An important part of large-scale city reconstruction systems is an image clustering algorithm that divides a set of images into groups that should cover only one building each. Those groups then serve as input for structure from motion systems. A variety of approaches for this mining step have been proposed recently, but there is a lack of comparative evaluations and realistic benchmarks. In this work, we want to fill this gap by comparing two state-of-the-art landmark mining algorithms: spectral clustering and min-hash. Furthermore, we introduce a new large-scale dataset for the evaluation of landmark mining algorithms consisting of 500k images from the inner city of Paris. We evaluate both algorithms on the well-known Oxford dataset and our Paris dataset and give a detailed comparison of the clustering quality and computation time of the algorithms."'),
('"An Experimental Analysis of Saliency Detection with Respect to Three Saliency Levels"', '"ECCV 2014"', '["Saliency detection", "Visual attention modelling", "Salient object detection", "Salient object seg', '"https://doi.org/10.1007/978-3-319-16199-0_56"', '"Saliency detection is a useful tool for video-based, real-time Computer Vision applications. It allows to select which locations of the scene are the most relevant and has been used in a number of related assistive technologies such as life-logging, memory augmentation and object detection for the visually impaired, as well as to study autism and the Parkinson\\u2019s disease. Many works focusing on different aspects of saliency have been proposed in the literature, defining saliency in different ways depending on the task. In this paper we perform an experimental analysis focusing on three levels where saliency is defined in different ways, namely visual attention modelling, salient object detection and salient object segmentation. We review the main evaluation datasets specifying the level of saliency which they best describe. Through the experiments we show that the performances of the saliency algorithms depend on the level with respect to which they are evaluated and on the nature of the stimuli used for the benchmark. Moreover, we show that the eye fixation maps can be effectively used to perform salient object detection and segmentation, which suggests that pre-attentive bottom-up information can be still exploited to improve high level tasks such as salient object detection. Finally, we show that benchmarking a saliency detection algorithm with respect to a single dataset/saliency level, can lead to erroneous results and conclude that many datasets/saliency levels should be considered in the evaluations."'),
('"An Experimental Comparison of Discrete and Continuous Shape Optimization Methods"', '"ECCV 2008"', '["Image Segmentation", "Volume Resolution", "Active Contour Model", "Continuous Setting", "Discrete ', '"https://doi.org/10.1007/978-3-540-88682-2_26"', '"Shape optimization is a problem which arises in numerous computer vision problems such as image segmentation and multiview reconstruction. In this paper, we focus on a certain class of binary labeling problems which can be globally optimized both in a spatially discrete setting and in a spatially continuous setting. The main contribution of this paper is to present a quantitative comparison of the reconstruction accuracy and computation times which allows to assess some of the strengths and limitations of both approaches. We also present a novel method to approximate length regularity in a graph cut based framework: Instead of using pairwise terms we introduce higher order terms. These allow to represent a more accurate discretization of the L 2-norm in the length term."'),
('"An Experimental Study of Color-Based Segmentation Algorithms Based on the Mean-Shift Concept"', '"ECCV 2010"', '["Kernel Function", "Color Space", "Segmentation Algorithm", "Segmentation Result", "Normal Kernel"]', '"https://doi.org/10.1007/978-3-642-15552-9_37"', '"We point out a difference between the original mean-shift formulation of Fukunaga and Hostetler and the common variant in the computer vision community, namely whether the pairwise comparison is performed with the original or with the filtered image of the previous iteration. This leads to a new hybrid algorithm, called Color Mean Shift, that roughly speaking, treats color as Fukunaga\\u2019s algorithm and spatial coordinates as Comaniciu\\u2019s algorithm. We perform experiments to evaluate how different kernel functions and color spaces affect the final filtering and segmentation results, and the computational speed, using the Berkeley and Weizmann segmentation databases. We conclude that the new method gives better results than existing mean shift ones on four standard comparison measures (\\\\(\\\\backsim15\\\\%,\\\\:22\\\\%\\\\) improvement on RAND and BDE measures respectively for color images), with slightly higher running times (\\\\(\\\\backsim10\\\\%\\\\)). Overall, the new method produces segmentations comparable in quality to the ones obtained with current state of the art segmentation algorithms."'),
('"An Extended Phase Field Higher-Order Active Contour Model for Networks and Its Application to Road ', '"ECCV 2008"', '["Road Network", "Tangent Vector", "Active Contour", "Arbitrary Topology", "Road Region"]', '"https://doi.org/10.1007/978-3-540-88690-7_38"', '"This paper addresses the segmentation from an image of entities that have the form of a \\u2018network\\u2019, i.e. the region in the image corresponding to the entity is composed of branches joining together at junctions, e.g. road or vascular networks. We present a new phase field higher-order active contour (HOAC) prior model for network regions, and apply it to the segmentation of road networks from very high resolution satellite images. This is a hard problem for two reasons. First, the images are complex, with much \\u2018noise\\u2019 in the road region due to cars, road markings, etc., while the background is very varied, containing many features that are locally similar to roads. Second, network regions are complex to model, because they may have arbitrary topology. In particular, we address a severe limitation of a previous model in which network branch width was constrained to be similar to maximum network branch radius of curvature, thereby providing a poor model of networks with straight narrow branches or highly curved, wide branches. To solve this problem, we propose a new HOAC prior energy term, and reformulate it as a nonlocal phase field energy. We analyse the stability of the new model, and find that in addition to solving the above problem by separating the interactions between points on the same and opposite sides of a network branch, the new model permits the modelling of two widths simultaneously. The analysis also fixes some of the model parameters in terms of network width(s). After adding a likelihood energy, we use the model to extract the road network quasi-automatically from pieces of a QuickBird image, and compare the results to other models in the literature. The results demonstrate the superiority of the new model, the importance of strong prior knowledge in general, and of the new term in particular."'),
('"An Eye Fixation Database for Saliency Detection in Images"', '"ECCV 2010"', '["Visual Attention", "Attentional Bias", "Semantic Category", "Salient Object", "Active Segmentation', '"https://doi.org/10.1007/978-3-642-15561-1_3"', '"To learn the preferential visual attention given by humans to specific image content, we present NUSEF- an eye fixation database compiled from a pool of 758 images and 75 subjects. Eye fixations are an excellent modality to learn semantics-driven human understanding of images, which is vastly different from feature-driven approaches employed by saliency computation algorithms. The database comprises fixation patterns acquired using an eye-tracker, as subjects free-viewed images corresponding to many semantic categories such as faces (human and mammal), nudes and actions (look, read and shoot). The consistent presence of fixation clusters around specific image regions confirms that visual attention is not subjective, but is directed towards salient objects and object-interactions."'),
('"An Improved Stereo Matching Algorithm with Ground Plane and Temporal Smoothness Constraints"', '"ECCV 2012"', '["Consecutive Frame", "Stereo Match", "Stereo Pair", "Permeability Filter", "Stereo Video"]', '"https://doi.org/10.1007/978-3-642-33868-7_14"', '"In this study, novel techniques are presented addressing the challenges of stereo matching algorithms for surveillance and vehicle control. For this purpose, one of the most efficient local stereo matching techniques, namely permeability filter, is modified in terms of road plane geometry and temporal consistency in order to take the major challenges of such a scenario into account. Relaxing smoothness assumption of the permeability filter along vertical axis enables extraction of road geometry with high accuracy, even for the cases where ground plane does not contain sufficient textural information. On the other hand, temporal smoothness is enforced by transferring reliable depth assignments against illumination changes, reflections and instant occlusions. According to the extensive experiments on a recent challenging stereo video dataset, the proposed modifications provide reliable disparity maps under severe challenges and low texture distribution, improving scene analyses for surveillance related applications. Although improvements are illustrated for a specific local stereo matching algorithm, the presented specifications and modifications can be applied for the other similar stereo algorithms as well."'),
('"An Incremental Learning Algorithm for Face Recognition"', '"BioAW 2002"', '["face recognition", "incremental learning", "face sequences"]', '"https://doi.org/10.1007/3-540-47917-1_1"', '"In face recognition, where high-dimensional representation spaces are generally used, it is very important to take advantage of all the available information. In particular, many labelled facial images will be accumulated while the recognition system is functioning, and due to practical reasons some of them are often discarded. In this paper, we propose an algorithm for using this information. The algorithm has the fundamental characteristic of being incremental. On the other hand, the algorithm makes use of a combination of classification results for the images in the input sequence. Experiments with sequences obtained with a real person detection and tracking system allow us to analyze the performance of the algorithm, as well as its potential improvements."'),
('"An Incremental Learning Method for Unconstrained Gaze Estimation"', '"ECCV 2008"', '["Incremental Learn", "Online Learning Algorithm", "Facial Mesh", "Desktop Environment", "Multiple L', '"https://doi.org/10.1007/978-3-540-88690-7_49"', '"This paper presents an online learning algorithm for appea- rance-based gaze estimation that allows free head movement in a casual desktop environment. Our method avoids the lengthy calibration stage using an incremental learning approach. Our system keeps running as a background process on the desktop PC and continuously updates the estimation parameters by taking user\\u2019s operations on the PC monitor as input. To handle free head movement of a user, we propose a pose-based clustering approach that efficiently extends an appearance manifold model to handle the large variations of the head pose. The effectiveness of the proposed method is validated by quantitative performance evaluation with three users."'),
('"An Information Fusion Method for the Automatic Delineation of the Bone-Soft Tissues Interface in Ul', '"MMBIA 2004"', '["Ultrasound Image", "Active Contour", "Membership Degree", "Automatic Segmentation", "Manual Segmen', '"https://doi.org/10.1007/978-3-540-27816-0_19"', '"We present a new method for delineating the osseous interface in ultrasound images. Automatic segmentation of the bone-soft tissues interface is achieved by mimicking the reasoning of the expert in charge of the manual segmentation. Information are modeled and fused by the use of fuzzy logic and the accurate delineation is then performed by using general a priori knowledge about osseous interface and ultrasound imaging physics. Results of the automatic segmentation are compared with the manual segmentation of an expert."'),
('"An Information-Based Measure for Grouping Quality"', '"ECCV 2004"', '["Quality Measure", "Group Quality", "Human Judgment", "Degradation Model", "Correct Grouping"]', '"https://doi.org/10.1007/978-3-540-24672-5_31"', '"We propose a method for measuring the quality of a grouping result, based on the following observation: a better grouping result provides more information about the true, unknown grouping. The amount of information is evaluated using an automatic procedure, relying on the given hypothesized grouping, which generates (homogeneity) queries about the true grouping and answers them using an oracle. The process terminates once the queries suffice to specify the true grouping. The number of queries is a measure of the hypothesis non-informativeness. A relation between the query count and the (probabilistically characterized) uncertainty of the true grouping, is established and experimentally supported. The proposed information-based quality measure is free from arbitrary choices, uniformly treats different types of grouping errors, and does not favor any algorithm. We also found that it approximates human judgment better than other methods and gives better results when used to optimize a segmentation algorithm."'),
('"An Integral Solution to Surface Evolution PDEs Via Geo-cuts"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744078_32"', '"We introduce a new approach to modelling gradient flows of contours and surfaces. While standard variational methods (e.g. level sets) compute local interface motion in a differential fashion by estimating local contour velocity via energy derivatives, we propose to solve surface evolution PDEs by explicitly estimating integral motion of the whole surface. We formulate an optimization problem directly based on an integral characterization of gradient flow as an infinitesimal move of the (whole) surface giving the largest energy decrease among all moves of equal size. We show that this problem can be efficiently solved using recent advances in algorithms for global hypersurface optimization [4,2,11]. In particular, we employ the geo-cuts method [4] that uses ideas from integral geometry to represent continuous surfaces as cuts on discrete graphs. The resulting interface evolution algorithm is validated on some 2D and 3D examples similar to typical demonstrations of level-set methods. Our method can compute gradient flows of hypersurfaces with respect to a fairly general class of continuous functionals and it is flexible with respect to distance metrics on the space of contours/surfaces. Preliminary tests for standard L 2 distance metric demonstrate numerical stability, topological changes and an absence of any oscillatory motion."'),
('"An Integrated Algorithm for MRI Brain Images Segmentation"', '"CVAMIA 2006"', '["Gray Matter", "Integrate Algorithm", "Transitional Region", "Fuzzy Cluster Algorithm", "Watershed ', '"https://doi.org/10.1007/11889762_12"', '"This paper presents an integrated algorithm for MRI (Magnetic Resonance Imaging) brain tissues segmentation. The method is composed of four stages. Noise in the MRI images is first reduced by a versatile wavelet-based filter. Then, the watershed algorithm is applied to brain tissues as an initial segmenting method. Because the result of classical watershed algorithm on grey-scale textured images such as tissue images is over-segmentation. The third stage is a merging process for the over-segmentation regions using fuzzy clustering algorithm (Fuzzy C-Means). But there are still some regions which are not divided completely due to the low contrast in them, particularly in the transitional regions of gray matter and white matter, or cerebrospinal fluid and gray matter. We exploited a method base on Minimum Covariance Determinant (MCD) estimator to detect the regions needed segmentation again, and then partition them by a supervised k-Nearest Neighbor (kNN) classifier. This integrated approach yields a robust and precise segmentation. The efficacy of the proposed algorithm is validated using extensive experiments."'),
('"An Integrated Model for Accurate Shape Alignment"', '"ECCV 2006"', '["Line Segment", "Feature Point", "Optimal Shape", "Shape Model", "Global Shape"]', '"https://doi.org/10.1007/11744085_26"', '"In this paper, we propose a two-level integrated model for accurate face shape alignment. At the low level, the shape is split into a set of line segments which serve as the nodes in the hidden layer of a Markov Network. At the high level, all the line segments are constrained by a global Gaussian point distribution model. Furthermore, those already accurately aligned points from the low level are detected and constrained using a constrained regularization algorithm. By analyzing the regularization result, a mask image of local minima is generated to guide the distribution of Markov Network states, which makes our algorithm more robust. Extensive experiments demonstrate the accuracy and effectiveness of our proposed approach."'),
('"An Intelligent Wheelchair to Enable Safe Mobility of the Disabled People with Motor and Cognitive I', '"ECCV 2014"', '["Intelligent wheelchair", "Obstacle avoidance", "Situation awareness", "Learning-based path generat', '"https://doi.org/10.1007/978-3-319-16199-0_49"', '"In this paper, we develop an Intelligent Wheelchair (IW) system to provide safe mobility to the disabled or elderly people with cognitive and motor impairments. Our IW provides two main functions: obstacle avoidance and situation awareness. Firstly, it detects a variety of obstacles by a combination of a camera and 8 range sensors, and finds the viable paths to avoid the collisions of obstacles based on learning-based classification. Secondly, it categorizes the current situation where a user is standing on as sidewalk, roadway and traffic intersection by analyzing the texture properties and shapes of the images, thus prevents the collisions of vehicle at the traffic intersection. The proposed system was tested on various environments then the results show that the proposed system can recognize the outdoor place types with an accuracy of 98.25% and produce the viable paths with an accuracy of 92.00% on outdoors."'),
('"An Intensity Similarity Measure in Low-Light Conditions"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_21"', '"In low-light conditions, it is known that Poisson noise and quantization noise become dominant sources of noise. While intensity difference is usually measured by Euclidean distance, it often breaks down due to an unnegligible amount of uncertainty in observations caused by noise. In this paper, we develop a new noise model based upon Poisson noise and quantization noise. We then propose a new intensity similarity function built upon the proposed noise model. The similarity measure is derived by maximum likelihood estimation based on the nature of Poisson noise and quantization process in digital imaging systems, and it deals with the uncertainty embedded in observations. The proposed intensity similarity measure is useful in many computer vision applications which involve intensity differencing, e.g., block matching, optical flow, and image alignment. We verified the correctness of the proposed noise model by comparisons with real-world noise data and confirmed superior robustness of the proposed similarity measure compared with the standard Euclidean norm."'),
('"An Iterative Method with General Convex Fidelity Term for Image Restoration"', '"ECCV 2010"', '["Image Restoration", "General Convex", "Separable Banach Space", "Proximal Point Algorithm", "Proxi', '"https://doi.org/10.1007/978-3-642-15549-9_14"', '"We propose a convergent iterative regularization procedure based on the square of a dual norm for image restoration with general (quadratic or non-quadratic) convex fidelity terms. Convergent iterative regularization methods have been employed for image deblurring or denoising in the presence of Gaussian noise, which use L 2 [1] and L 1 [2] fidelity terms. Iusem-Resmerita [3] proposed a proximal point method using inexact Bregman distance for minimizing a general convex function defined on a general non-reflexive Banach space which is the dual of a separable Banach space. Based on this, we investigate several approaches for image restoration (denoising-deblurring) with different types of noise. We test the behavior of proposed algorithms on synthetic and real images. We compare the results with other state-of-the-art iterative procedures as well as the corresponding existing one-step gradient descent implementations. The numerical experiments indicate that the iterative procedure yields high quality reconstructions and superior results to those obtained by one-step gradient descent and similar with other iterative methods."'),
('"An MCMC-Based Particle Filter for Tracking Multiple Interacting Targets"', '"ECCV 2004"', '["Markov Chain Monte Carlo", "Particle Filter", "Motion Model", "Markov Random Field", "Importance W', '"https://doi.org/10.1007/978-3-540-24673-2_23"', '"We describe a Markov chain Monte Carlo based particle filter that effectively deals with interacting targets, i.e., targets that are influenced by the proximity and/or behavior of other targets. Such interactions cause problems for traditional approaches to the data association problem. In response, we developed a joint tracker that includes a more sophisticated motion model to maintain the identity of targets throughout an interaction, drastically reducing tracker failures. The paper presents two main contributions: (1) we show how a Markov random field (MRF) motion prior, built on the fly at each time step, can substantially improve tracking when targets interact, and (2) we show how this can be done efficiently using Markov chain Monte Carlo (MCMC) sampling. We prove that incorporating an MRF to model interactions is equivalent to adding an additional interaction factor to the importance weights in a joint particle filter. Since a joint particle filter suffers from exponential complexity in the number of tracked targets, we replace the traditional importance sampling step in the particle filter with an MCMC sampling step. The resulting filter deals efficiently and effectively with complicated interactions when targets approach each other. We present both qualitative and quantitative results to substantiate the claims made in the paper, including a large scale experiment on a video-sequence of over 10,000 frames in length."'),
('"An Off-line Signature Verification System Based on Fusion of Local and Global Information"', '"BioAW 2004"', '["Hide Markov Model", "Fusion Strategy", "Genuine Signature", "Skilled Forgery", "Slant Direction"]', '"https://doi.org/10.1007/978-3-540-25976-3_27"', '"An off-line signature verification system based on fusion of two machine experts is presented. One of the experts is based on global image analysis and a statistical distance measure while the second one is based on local image analysis and Hidden Markov Models. Experimental results are given on a subcorpus of the large MCYT signature database for random and skilled forgeries. It is shown experimentally that the machine expert based on local information outperforms the system based on global analysis in all reported cases. The two proposed systems are also shown to give complementary recognition information which is exploited with a simple fusion strategy based on the sum rule."'),
('"An Open Source Framework for Standardized Comparisons of Face Recognition Algorithms"', '"ECCV 2012"', '["Face Recognition", "Discrete Cosine Transform", "Gaussian Mixture Model", "Local Binary Pattern", ', '"https://doi.org/10.1007/978-3-642-33885-4_55"', '"In this paper we introduce the facereclib, the first software library that allows to compare a variety of face recognition algorithms on most of the known facial image databases and that permits rapid prototyping of novel ideas and testing of meta-parameters of face recognition algorithms. The facereclib is built on the open source signal processing and machine learning library Bob. It uses well-specified face recognition protocols to ensure that results are comparable and reproducible. We show that the face recognition algorithms implemented in Bob as well as third party face recognition libraries can be used to run face recognition experiments within the framework of the facereclib. As a proof of concept, we execute four different state-of-the-art face recognition algorithms: local Gabor binary pattern histogram sequences (LGBPHS), Gabor graph comparisons with a Gabor phase based similarity measure, inter-session variability modeling (ISV) of DCT block features, and the linear discriminant analysis on two different color channels (LDA-IR) on two different databases: The Good, The Bad, and The Ugly, and the BANCA database, in all cases using their fixed protocols. The results show that there is not one face recognition algorithm that outperforms all others, but rather that the results are strongly dependent on the employed database."'),
('"An Oriented Flux Symmetry Based Active Contour Model for Three Dimensional Vessel Segmentation"', '"ECCV 2010"', '["Active Contour", "Object Boundary", "Image Gradient", "Projected Gradient", "Active Contour Model"', '"https://doi.org/10.1007/978-3-642-15558-1_52"', '"This paper proposes a novel approach to segment three dimensional curvilinear structures, particularly vessels in angiography, by inspecting the symmetry of image gradients. The proposed method stresses the importance of simultaneously considering both the gradient symmetry with respect to the curvilinear structure center, and the gradient antisymmetry with respect to the object boundary. Measuring the image gradient symmetry remarkably suppresses the disturbance introduced by rapid intensity changes along curvilinear structures. Meanwhile, considering the image gradient antisymmetry helps locate the structure boundary. The gradient symmetry and the gradient antisymmetry are evaluated based on the notion of oriented flux. By utilizing the aforementioned gradient symmetry information, an active contour model is tailored to perform segmentation. On the one hand, by exploiting the symmetric image gradient pattern observed at structure centers, the contours expand along curvilinear structures even through there exists intensity fluctuation along the structures. On the other hand, measuring the antisymmetry of the image gradient conveys strong detection responses to precisely drive contours to the structure boundaries, as well as avoiding contour leakages. The proposed method is capable of delivering promising segmentation results. This is validated in the experiments using synthetic data and real vascular images of different modalities, and through the comparison to two well founded and published methods for curvilinear structure segmentation."'),
('"An Overview of Research Activities in Facial Age Estimation Using the FG-NET Aging Database"', '"ECCV 2014"', '["Facial age estimation", "Aging databases", "FG-NET aging database"]', '"https://doi.org/10.1007/978-3-319-16181-5_56"', '"The FG-NET aging database was released in 2004 in an attempt to support research activities related to facial aging. Since then a number of researchers used the database for carrying out research in various disciplines related to facial aging. Based on the analysis of published work where the FG-NET aging database was used, conclusions related to the type of research carried out in relation to the impact of the dataset in shaping up the research topic of facial aging, are presented. In particular we focus our attention on the topic of age estimation that proved to be the most popular among users of the FG-NET aging database. Through the review of key papers in age estimation and the presentation of benchmark results the main approaches/directions in facial aging are outlined and future trends, requirements and research directions are drafted."'),
('"An Unified Approach to Model-Based and Model-Free Visual Servoing"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47979-1_29"', '"Standard vision-based control techniques can be classified into two groups: model-based and model-free visual servoing. Model-based visual servoing is used when a 3D model of the observed object is available. If the 3D model is completely unknown, robot positioning can still be achieved using a teaching-by-showing approach. This model-free technique needs a preliminary learning step during which a reference image of the scene is stored. The objective of this paper is to propose an unified approach to vision-based control which can be used with a zooming camera whether the model of the object is known or not. The key idea of the unified approach is to build a reference in a projective space invariant to camera intrinsic parameters which can be computed if the model is known or if an image of the object is available. Thus, only one low level visual servoing technique must be implemented at once."'),
('"Analysing User Visual Implicit Feedback in Enhanced TV Scenarios"', '"ECCV 2014"', '["Facial expressions", "Emotions", "Moods", "Global body movement", "Head pose"]', '"https://doi.org/10.1007/978-3-319-16178-5_22"', '"In this paper, we report on user behaviors by analyzing visual clues while users are watching various TV broadcast in pilot settings. We detail the first results of the empathic analysis of viewers watching four distinct videos in dedicated recording sessions. Viewers are sitting in front of a TV set in unconstrained position (free postures, free head poses and free body movements) on a chair and recorded by a regular webcam at both low and high resolutions. We have extracted metrics related to: head and global movement, changes in head orientation and facial expressions (happy, angry, surprise). We have conducted preliminary studies about how the extracted metrics can be employed in order to detect the interest, the amusement or the distraction of a viewer."'),
('"Analysis of Building Textures for Reconstructing Partially Occluded Facades"', '"ECCV 2008"', '["Markov Chain Monte Carlo", "Markov Random Field", "Reversible Jump Markov Chain Monte Carlo", "Mar', '"https://doi.org/10.1007/978-3-540-88682-2_28"', '"As part of an architectural modeling project, this paper investigates the problem of understanding and manipulating images of buildings. Our primary motivation is to automatically detect and seamlessly remove unwanted foreground elements from urban scenes. Without explicit handling, these objects will appear pasted as artifacts on the model. Recovering the building facade in a video sequence is relatively simple because parallax induces foreground/background depth layers, but here we consider static images only. We develop a series of methods that enable foreground removal from images of buildings or brick walls. The key insight is to use a priori knowledge about grid patterns on building facades that can be modeled as Near Regular Textures (NRT). We describe a Markov Random Field (MRF) model for such textures and introduce a Markov Chain Monte Carlo (MCMC) optimization procedure for discovering them. This simple spatial rule is then used as a starting point for inference of missing windows, facade segmentation, outlier identification, and foreground removal."'),
('"Analysis of KITTI Data for Stereo Analysis with Stereo Confidence Measures"', '"ECCV 2012"', '["Ground Truth", "Stereo Matcher", "Disparity Estimate", "Disparity Error", "Disparity Space"]', '"https://doi.org/10.1007/978-3-642-33868-7_16"', '"The recently published KITTI stereo dataset provides a new quality of stereo imagery with partial ground truth for benchmarking stereo matchers. Our aim is to test the value of stereo confidence measures (e.g. a left-right consistency check of disparity maps, or an analysis of the slope of a local interpolation of the cost function at the taken minimum) when applied to recorded datasets, such as published with KITTI. We choose popular measures as available in the stereo-analysis literature, and discuss a naive combination of these. Evaluations are carried out using a sparsification strategy. While the best single confidence measure proved to be the right-left consistency check for high disparity map densities, the best overall performance is achieved with the proposed naive measure combination. We argue that there is still demand for more challenging datasets and more comprehensive ground truth."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Analysis of Motion Blur with a Flutter Shutter Camera for Non-linear Motion"', '"ECCV 2010"', '["Motion Estimation", "Modulation Transfer Function", "Motion Blur", "Motion Estimation Algorithm", ', '"https://doi.org/10.1007/978-3-642-15549-9_2"', '"Motion blurs confound many computer vision problems. The fluttered shutter (FS) camera [1] tackles the motion deblurring problem by emulating invertible broadband blur kernels. However, existing FS methods assume known constant velocity motions, e.g., via user specifications. In this paper, we extend the FS technique to general 1D motions and develop an automatic motion-from-blur framework by analyzing the image statistics under the FS."'),
('"Analysis of Sampling Techniques for Learning Binarized Statistical Image Features Using Fixations a', '"ECCV 2014"', '["Binary operators", "Visual attention", "Salience modeling"]', '"https://doi.org/10.1007/978-3-319-16181-5_9"', '"This paper studies the role of different sampling techniques in the process of learning Binarized Statistical Image Features (BSIF). It considers various sampling approaches including random sampling and selective sampling. The selective sampling utilizes either human eye tracking data or artificially generated fixations. To generate artificial fixations, this paper exploits salience models which apply to key point localization. Therefore, it proposes a framework grounded on the hypothesis that the most salient point conveys important information. Furthermore, it investigates possible performance gain by training BSIF filters on class specific data. To summarize, the contribution of this paper are as follows: 1) it studies different sampling strategies to learn BSIF filters, 2) it employs human fixations in the design of a binary operator, 3) it proposes an attention model to replicate human fixations, and 4) it studies the performance of learning application specific BSIF filters using attention modeling."'),
('"Analytical Dynamic Programming Matching"', '"ECCV 2012"', '["Dynamic Time Warping", "Block Match", "Gaussian Likelihood", "Quadratic Nature", "Dynamic Programm', '"https://doi.org/10.1007/978-3-642-33863-2_10"', '"In this paper, we show that the truly two-dimensional elastic image matching problem can be solved analytically using dynamic programming (DP) in polynomial time if the problem is formulated as a maximum a posteriori problem using Gaussian distributions for the likelihood and prior. After giving the derivation of the analytical DP matching algorithm, we evaluate its performance on handwritten character images containing various nonlinear deformations, and compare other elastic image matching methods."'),
('"Analytical Forward Projection for Axial Non-central Dioptric and Catadioptric Cameras"', '"ECCV 2010"', '["Bundle Adjustment", "Epipolar Geometry", "Spherical Mirror", "Reprojection Error", "Scene Point"]', '"https://doi.org/10.1007/978-3-642-15558-1_10"', '"We present a technique for modeling non-central catadioptric cameras consisting of a perspective camera and a rotationally symmetric conic reflector. While previous approaches use a central approximation and/or iterative methods for forward projection, we present an analytical solution. This allows computation of the optical path from a given 3D point to the given viewpoint by solving a 6th degree forward projection equation for general conic mirrors. For a spherical mirror, the forward projection reduces to a 4th degree equation, resulting in a closed form solution. We also derive the forward projection equation for imaging through a refractive sphere (non-central dioptric camera) and show that it is a 10th degree equation. While central catadioptric cameras lead to conic epipolar curves, we show the existence of a quartic epipolar curve for catadioptric systems using a spherical mirror. The analytical forward projection leads to accurate and fast 3D reconstruction via bundle adjustment. Simulations and real results on single image sparse 3D reconstruction are presented. We demonstrate ~ 100 times speed up using the analytical solution over iterative forward projection for 3D reconstruction using spherical mirrors."'),
('"Analytical Image Models and Their Applications"', '"ECCV 2002"', '["Image features", "spectral analysis", "Bessel K forms", "clutter classification", "object recognit', '"https://doi.org/10.1007/3-540-47969-4_3"', '"In this paper, we study a family of analytical probability models for images within the spectral representation framework. First the input image is decomposed using a bank of filters, and probability models are imposed on the filter outputs (or spectral components). A two-parameter analytical form, called a Bessel K form, derived based on a generator model, is used to model the marginal probabilities of these spectral components. The Bessel K parameters can be estimated efficiently from the filtered images and extensive simulations using video, infrared, and range images have demonstrated Bessel K form\\u2019s fit to the observed histograms. The effectiveness of Bessel K forms is also demonstrated through texture modeling and synthesis. In contrast to numeric-based dimension reduction representations, which are derived purely based on numerical methods, the Bessel K representations are derived based on object representations and this enables us to establish relationships between the Bessel parameters and certain characteristics of the imaged objects. We have derived a pseudometric on the image space to quantify image similarities/differences using an analytical expression for L 2-metric on the set of Bessel K forms. We have applied the Bessel K representation to texture modeling and synthesis, clutter classification, pruning of hypotheses for object recognition, and object classification. Results show that Bessel K representation captures important image features, suggesting its role in building efficient image understanding paradigms and systems."'),
('"Analyzing and Evaluating Markerless Motion Tracking Using Inertial Sensors"', '"ECCV 2010"', '["Tracking Error", "Inertial Sensor", "Orientation Data", "Tracking Result", "Global Coordinate Syst', '"https://doi.org/10.1007/978-3-642-35749-7_11"', '"In this paper, we introduce a novel framework for automatically evaluating the quality of 3D tracking results obtained from markerless motion capturing. In our approach, we use additional inertial sensors to generate suitable reference information. In contrast to previously used marker-based evaluation schemes, inertial sensors are inexpensive, easy to operate, and impose comparatively weak additional constraints on the overall recording setup with regard to location, recording volume, and illumination. On the downside, acceleration and rate of turn data as obtained from such inertial systems turn out to be unsuitable representations for tracking evaluation. As our main contribution, we show how tracking results can be analyzed and evaluated on the basis of suitable limb orientations, which can be derived from 3D tracking results as well as from enhanced inertial sensors fixed on these limbs. Our experiments on various motion sequences of different complexity demonstrate that such limb orientations constitute a suitable mid-level representation for robustly detecting most of the tracking errors. In particular, our evaluation approach reveals also misconfigurations and twists of the limbs that can hardly be detected from traditional evaluation metrics."'),
('"Analyzing Depth from Coded Aperture Sets"', '"ECCV 2010"', '["Multiplicative Noise", "Discrimination Score", "Derivative Power", "Optical Transfer Function", "L', '"https://doi.org/10.1007/978-3-642-15549-9_16"', '"Computational depth estimation is a central task in computer vision and graphics. A large variety of strategies have been introduced in the past relying on viewpoint variations, defocus changes and general aperture codes. However, the tradeoffs between such designs are not well understood. Depth estimation from computational camera measurements is a highly non-linear process and therefore most research attempts to evaluate depth estimation strategies rely on numerical simulations. Previous attempts to design computational cameras with good depth discrimination optimized highly non-linear and non-convex scores, and hence it is not clear if the constructed designs are optimal. In this paper we address the problem of depth discrimination from J images captured using J arbitrary codes placed within one fixed lens aperture. We analyze the desired properties of discriminative codes under a geometric optics model and propose an upper bound on the best possible discrimination. We show that under a multiplicative noise model, the half ring codes discovered by Zhou et al. [1] are near-optimal. When a large number of images are allowed, a multi-aperture camera [2] dividing the aperture into multiple annular rings provides near-optimal discrimination. In contrast, the plenoptic camera of [5] which divides the aperture into compact support circles can achieve at most 50% of the optimal discrimination bound."'),
('"Analyzing the Performance of Multilayer Neural Networks for Object Recognition"', '"ECCV 2014"', '["convolutional neural networks", "object recognition", "empirical analysis"]', '"https://doi.org/10.1007/978-3-319-10584-0_22"', '"In the last two years, convolutional neural networks (CNNs) have achieved an impressive suite of results on standard recognition datasets and tasks. CNN-based features seem poised to quickly replace engineered representations, such as SIFT and HOG. However, compared to SIFT and HOG, we understand much less about the nature of the features learned by large CNNs. In this paper, we experimentally probe several aspects of CNN feature learning in an attempt to help practitioners gain useful, evidence-backed intuitions about how to apply CNNs to computer vision problems."'),
('"Analyzing the Subspace Structure of Related Images: Concurrent Segmentation of Image Sets"', '"ECCV 2012"', '["Visual Word", "Multiple Object", "Sparse Representation", "Appearance Model", "Subspace Cluster"]', '"https://doi.org/10.1007/978-3-642-33765-9_10"', '"We develop new algorithms to analyze and exploit the joint subspace structure of a set of related images to facilitate the process of concurrent segmentation of a large set of images. Most existing approaches for this problem are either limited to extracting a single similar object across the given image set or do not scale well to a large number of images containing multiple objects varying at different scales. One of the goals of this paper is to show that various desirable properties of such an algorithm (ability to handle multiple images with multiple objects showing arbitary scale variations) can be cast elegantly using simple constructs from linear algebra: this significantly extends the operating range of such methods. While intuitive, this formulation leads to a hard optimization problem where one must perform the image segmentation task together with appropriate constraints which enforce desired algebraic regularity (e.g., common subspace structure). We propose efficient iterative algorithms (with small computational requirements) whose key steps reduce to objective functions solvable by max-flow and/or nearly closed form identities. We study the qualitative, theoretical, and empirical properties of the method, and present results on benchmark datasets."'),
('"Anchored Deformable Face Ensemble Alignment"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_14"', '"At present, many approaches have been proposed for deformable face alignment with varying degrees of success. However, the common drawback to nearly all these approaches is the inaccurate landmark registrations. The registration errors which occur are predominantly heterogeneous (i.e. low error for some frames in a sequence and higher error for others). In this paper we propose an approach for simultaneously aligning an ensemble of deformable face images stemming from the same subject given noisy heterogeneous landmark estimates. We propose that these initial noisy landmark estimates can be used as an \\u201canchor\\u201d in conjunction with known state-of-the-art objectives for unsupervised image ensemble alignment. Impressive alignment performance is obtained using well known deformable face fitting algorithms as \\u201canchors\\u201d."'),
('"Anisotropic Geodesics for Perceptual Grouping and Domain Meshing"', '"ECCV 2008"', '["Voronoi Diagram", "Geodesic Distance", "Structure Tensor", "Voronoi Cell", "Riemannian Metrics"]', '"https://doi.org/10.1007/978-3-540-88688-4_10"', '"This paper shows how Voronoi diagrams and their dual Delaunay complexes, defined with geodesic distances over 2D Reimannian manifolds, can be used to solve two important problems encountered in computer vision and graphics. The first problem studied is perceptual grouping which is a curve reconstruction problem where one should complete in a meaningful way a sparse set of noisy curves. From this latter curves, our grouping algorithm first designs an anisotropic tensor field that corresponds to a Reimannian metric. Then, according to this metric, the Delaunay graph is constructed and pruned in order to correctly link together salient features. The second problem studied is planar domain meshing, where one should build a good quality triangulation of a given domain. Our meshing algorithm is a geodesic Delaunay refinement method that exploits an anisotropic tensor field in order to locally impose the orientation and aspect ratio of the created triangles."'),
('"Anisotropic Laplace-Beltrami Operators for Shape Analysis"', '"ECCV 2014"', '["Shape analysis", "Anisotropic diffusion", "Curvature", "Non-rigid matching", "Segmentation", "Lapl', '"https://doi.org/10.1007/978-3-319-16220-1_21"', '"This paper introduces an anisotropic Laplace-Beltrami operator for shape analysis. While keeping useful properties of the standard Laplace-Beltrami operator, it introduces variability in the directions of principal curvature, giving rise to a more intuitive and semantically meaningful diffusion process. Although the benefits of anisotropic diffusion have already been noted in the area of mesh processing (e.g. surface regularization), focusing on the Laplacian itself, rather than on the diffusion process it induces, opens the possibility to effectively replace the omnipresent Laplace-Beltrami operator in many shape analysis methods. After providing a mathematical formulation and analysis of this new operator, we derive a practical implementation on discrete meshes. Further, we demonstrate the effectiveness of our new operator when employed in conjunction with different methods for shape segmentation and matching."'),
('"Anisotropic Minimal Surfaces Integrating Photoconsistency and Normal Information for Multiview Ster', '"ECCV 2010"', '["Minimal Surface", "Surface Orientation", "Normal Information", "Photometric Stereo", "Anisotropic ', '"https://doi.org/10.1007/978-3-642-15558-1_39"', '"In this work the weighted minimal surface model traditionally used in multiview stereo is revisited. We propose to generalize the classical photoconsistency-weighted minimal surface approach by means of an anisotropic metric which allows to integrate a specified surface orientation into the optimization process. In contrast to the conventional isotropic case, where all spatial directions are treated equally, the anisotropic metric adaptively weights the regularization along different directions so as to favor certain surface orientations over others. We show that the proposed generalization preserves all properties and globality guarantees of continuous convex relaxation methods. We make use of a recently introduced efficient primal-dual algorithm to solve the arising saddle point problem. In multiple experiments on real image sequences we demonstrate that the proposed anisotropic generalization allows to overcome oversmoothing of small-scale surface details, giving rise to more precise reconstructions."'),
('"Annotation Propagation in Large Image Databases via Dense Image Correspondence"', '"ECCV 2012"', '["Gaussian Mixture Model", "Annotation Propagation", "Similar Image", "Image Annotation", "Semantic ', '"https://doi.org/10.1007/978-3-642-33712-3_7"', '"Our goal is to automatically annotate many images with a set of word tags and a pixel-wise map showing where each word tag occurs. Most previous approaches rely on a corpus of training images where each pixel is labeled. However, for large image databases, pixel labels are expensive to obtain and are often unavailable. Furthermore, when classifying multiple images, each image is typically solved for independently, which often results in inconsistent annotations across similar images. In this work, we incorporate dense image correspondence into the annotation model, allowing us to make do with significantly less labeled data and to resolve ambiguities by propagating inferred annotations from images with strong local visual evidence to images with weaker local evidence. We establish a large graphical model spanning all labeled and unlabeled images, then solve it to infer annotations, enforcing consistent annotations over similar visual patterns. Our model is optimized by efficient belief propagation algorithms embedded in an expectation-maximization (EM) scheme. Extensive experiments are conducted to evaluate the performance on several standard large-scale image datasets, showing that the proposed framework outperforms state-of-the-art methods."'),
('"Anomalous Behaviour Detection Using Spatiotemporal Oriented Energies, Subset Inclusion Histogram Co', '"ECCV 2010"', '["Anomaly Detection", "Anomalous Behaviour", "Partial Match", "Dynamic Texture", "Total Frame"]', '"https://doi.org/10.1007/978-3-642-15549-9_41"', '"This paper proposes a novel approach to anomalous behaviour detection in video. The approach is comprised of three key components. First, distributions of spatiotemporal oriented energy are used to model behaviour. This representation can capture a wide range of naturally occurring visual spacetime patterns and has not previously been applied to anomaly detection. Second, a novel method is proposed for comparing an automatically acquired model of normal behaviour with new observations. The method accounts for situations when only a subset of the model is present in the new observation, as when multiple activities are acceptable in a region yet only one is likely to be encountered at any given instant. Third, event driven processing is employed to automatically mark portions of the video stream that are most likely to contain deviations from the expected and thereby focus computational efforts. The approach has been implemented with real-time performance. Quantitative and qualitative empirical evaluation on a challenging set of natural image videos demonstrates the approach\\u2019s superior performance relative to various alternatives."'),
('"Another Way of Looking at Plane-Based Calibration: The Centre Circle Constraint"', '"ECCV 2002"', '["Calibration", "Homography", "Planar Scene", "Multiple View Geometry", "Poncelet"]', '"https://doi.org/10.1007/3-540-47979-1_17"', '"The plane-based calibration consists in recovering the internal parameters of the camera from the views of a planar pattern with a known geometric structure. The existing direct algorithms use a problem formulation based on the properties of basis vectors. They minimize algebraic distances and may require a \\u2018good\\u2019 choice of system normalization. Our contribution is to put this problem into a more intuitive geometric framework. A solution can be obtained by intersecting circles, called Centre Circles, whose parameters are computed from the world-to-image homographies. The Centre Circle is the camera centre locus when planar figures are in perpective correspondence, in accordance with a Poncelet\\u2019s theorem. An interesting aspect of our formulation, using the Centre Circle constraint, is that we can easily transform the cost function into a sum of squared Euclidean distances. The simulations on synthetic data and an application with real images confirm the strong points of our method."'),
('"Anti-Faces for Detection"', '"ECCV 2000"', '["Support Vector Machine", "False Alarm", "Input Image", "False Alarm Rate", "Machine Intelligence"]', '"https://doi.org/10.1007/3-540-45054-8_9"', '"This paper offers a novel detection method, which works well even in the case of a complicated image collection - for instance, a frontal face under a large class of linear transformations. It was also successfully applied to detect 3D objects under different views. Call the class of images, which should be detected, a multi-template."'),
('"Appearance Based Qualitative Image Description for Object Class Recognition"', '"ECCV 2004"', '["object recognition", "shape", "appearance"]', '"https://doi.org/10.1007/978-3-540-24671-8_41"', '"The problem of recognizing classes of objects as opposed to special instances requires methods of comparing images that capture the variation within the class while they discriminate against objects outside the class. We present a simple method for image description based on histograms of qualitative shape indexes computed from the combination of triplets of sampled locations and gradient directions in the image. We demonstrate that this method indeed is able to capture variation within classes of objects and we apply it to the problem of recognizing four different categories from a large database. Using our descriptor on the whole image, containing varying degrees of background clutter, we obtain results for two of the objects that are superior to the best results published so far for this database. By cropping images manually we demonstrate that our method has a potential to handle also the other objects when supplied with an algorithm for searching the image. We argue that our method, based on qualitative image properties, capture the large range of variation that is typically encountered within an object class. This means that our method can be used on substantially larger patches of images than existing methods based on simpler criteria for evaluating image similarity."'),
('"Appearances Can Be Deceiving: Learning Visual Tracking from Few Trajectory Annotations"', '"ECCV 2014"', '["Visual tracking", "Motion learning", "Event modelling"]', '"https://doi.org/10.1007/978-3-319-10602-1_11"', '"Visual tracking is the task of estimating the trajectory of an object in a video given its initial location. This is usually done by combining at each step an appearance and a motion model. In this work, we learn from a small set of training trajectory annotations how the objects in the scene typically move. We learn the relative weight between the appearance and the motion model. We call this weight: visual deceptiveness. At test time, we transfer the deceptiveness and the displacement from the closest trajectory annotation to infer the next location of the object. Further, we condition the transference on an event model. On a set of 161 manually annotated test trajectories, we show in our experiments that learning from just 10 trajectory annotations halves the center location error and improves the success rate by about 10%."'),
('"Applications of Image Registration in Human Genome Research"', '"MMBIA 2004"', '["Image Registration", "Correction Function", "Object Type", "Chromatic Aberration", "Colocalization', '"https://doi.org/10.1007/978-3-540-27816-0_32"', '"Fluorescence microscopy has some limitations and imperfections that can affect some types of study in human genome research. Image registration methods can help overcome or reduce the effect of some of them. This paper is concentrated on several applications of image registration that are carried out in our laboratory. The concerned areas are: 1. chromatic aberration correction, which improves precision of colocalization studies, 2. registration of images after repeated acquisitions, which helps to enlarge the number of object types studied simultaneously, and 3. registration of tilted images in micro-axial tomography, which helps to improve optical resolution of light microscopy."'),
('"Approximate Confidence Intervals for Estimation of Matching Error Rates of Biometric Identification', '"BioAW 2004"', '["Error Rate", "Minimum Coverage", "False Reject Rate", "Binomial Proportion", "Approximate Confiden', '"https://doi.org/10.1007/978-3-540-25976-3_17"', '"Assessing the matching error rates of a biometric identification devices is integral to understanding its performance. Here we propose and evaluate several methods for creating approximate confidence intervals for matching error rates. Testing of biometric identification devices is recognized as inducing intra-individual correlation. In order to estimate error rates associated with these devices, it is necessary to deal with these correlations. In this paper, we consider extensions of recent work on adjustments to confidence intervals for binomial proportions to correlated binary proportions. In particular we propose a Agresti-Coull type adjustment for estimation of a proportion. Here that proportion represents an error rate. We use an overdispersion model to account for intra-individual correlation. To evaluate this approach we simulate data from a Beta-binomial distribution and assess the coverage for nominally 95% confidence intervals."'),
('"Approximate Envelope Minimization for Curvature Regularity"', '"ECCV 2012"', '["Curvature", "segmentation", "convex conjugate", "convex envelope"]', '"https://doi.org/10.1007/978-3-642-33885-4_29"', '"We propose a method for minimizing a non-convex function, which can be split up into a sum of simple functions. The key idea of the method is the approximation of the convex envelopes of the simple functions, which leads to a convex approximation of the original function. A solution is obtained by minimizing this convex approximation. Cost functions, which fulfill such a splitting property are ubiquitous in computer vision, therefore we explain the method based on such a problem, namely the non-convex problem of binary image segmentation based on Euler\\u2019s Elastica."'),
('"Approximate Gaussian Mixtures for Large Scale Vocabularies"', '"ECCV 2012"', '["Gaussian mixtures", "expectation-maximization", "visual vocabularies", "large scale clustering", "', '"https://doi.org/10.1007/978-3-642-33712-3_2"', '"We introduce a clustering method that combines the flexibility of Gaussian mixtures with the scaling properties needed to construct visual vocabularies for image retrieval. It is a variant of expectation-maximization that can converge rapidly while dynamically estimating the number of components. We employ approximate nearest neighbor search to speed-up the E-step and exploit its iterative nature to make search incremental, boosting both speed and precision. We achieve superior performance in large scale retrieval, being as fast as the best known approximate k-means."'),
('"Approximate MRF Inference Using Bounded Treewidth Subgraphs"', '"ECCV 2012"', '["Greedy Algorithm", "Original Graph", "Tree Decomposition", "Signed Graph", "Exact Inference"]', '"https://doi.org/10.1007/978-3-642-33718-5_28"', '"Graph cut algorithms [9], commonly used in computer vision, solve a first-order MRF over binary variables. The state of the art for this NP-hard problem is QPBO [1,2], which finds the values for a subset of the variables in the global minimum. While QPBO is very effective overall there are still many difficult problems where it can only label a small subset of the variables. We propose a new approach that, instead of optimizing the original graphical model, instead optimizes a tractable sub-model, defined as an energy function that uses a subset of the pairwise interactions of the original, but for which exact inference can be done efficiently. Our Bounded Treewidth Subgraph (k-BTS) algorithm greedily computes a large weight treewidth-k subgraph of the signed graph, then solves the energy minimization problem for this subgraph by dynamic programming. The edges omitted by our greedy method provide a per-instance lower bound. We demonstrate promising experimental results for binary deconvolution, a challenging problem used to benchmark QPBO [2]: our algorithm performs an order of magnitude better than QPBO or its common variants [4], both in terms of energy and accuracy, and the visual quality of our output is strikingly better as well. We also obtain a significant improvement in energy and accuracy on a stereo benchmark with 2nd order priors [5], although the improvement in visual quality is more modest. Our method\\u2019s running time is comparable to QPBO."'),
('"Approximate N-View Stereo"', '"ECCV 2000"', '["Calibration Error", "Discrete Image", "Scene Point", "View Stereo", "Approximate Shape"]', '"https://doi.org/10.1007/3-540-45054-8_5"', '"This paper introduces a new multi-view reconstruction problem called approximate N-view stereo. The goal of this problem is to recover a one-parameter family of volumes that are increasingly tighter supersets of an unknown, arbitrarily-shaped 3D scene. By studying 3D shapes that reproduce the input photographs up to a special image transformation called a shuffle transformation, we prove that (1) these shapes can be organized hierarchically into nested supersets of the scene, and (2) they can be computed using a simple algorithm called Approximate Space Carving that is provably-correct for arbitrary discrete scenes (i.e., for unknown, arbitrarily-shaped Lambertian scenes that are defined by a finite set of voxels and are viewed from N arbitrarily-distributed viewpoints inside or around them). The approach is specifically designed to attack practical reconstruction problems, including (1) recovering shape from images with inaccurate calibration information, and (2) building coarse scene models from multiple views."'),
('"Approximate Thin Plate Spline Mappings"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47977-5_2"', '"The thin plate spline (TPS) is an effective tool for modeling coordinate transformations that has been applied successfully in several computer vision applications. Unfortunately the solution requires the inversion of a p \\u00d7 p matrix, where p is the number of points in the data set, thus making it impractical for large scale applications. As it turns out, a surprisingly good approximate solution is often possible using only a small subset of corresponding points. We begin by discussing the obvious approach of using the subsampled set to estimate a transformation that is then applied to all the points, and we show the drawbacks of this method. We then proceed to borrow a technique from the machine learning community for function approximation using radial basis functions (RBFs) and adapt it to the task at hand. Using this method, we demonstrate a significant improvement over the naive method. One drawback of this method, however, is that is does not allow for principal warp analysis, a technique for studying shape deformations introduced by Bookstein based on the eigenvectors of the p \\u00d7 p bending energy matrix. To address this, we describe a third approximation method based on a classic matrix completion technique that allows for principal warp analysis as a by-product. By means of experiments on real and synthetic data, we demonstrate the pros and cons of these different approximations so as to allow the reader to make an informed decision suited to his or her application."'),
('"Approximated Relative Pose Solvers for Efficient Camera Motion Estimation"', '"ECCV 2014"', '["Camera pose", "Relative pose", "Relative orientation", "Five-pt algorithm", "Essential matrix", "G', '"https://doi.org/10.1007/978-3-319-16178-5_12"', '"We propose simple and efficient methods for estimating the camera motion between two images when this motion is small. While current solutions are still either slow, or unstable in case of small translation, we show how to considerably speed up a recent stable but slow method. The reasons for this speed-up are twofold. First, by approximating the rotation matrix to first order, we obtain a smaller polynomial system to be solved. Second, because of the small rotation assumption, we can use linearization and truncation of higher-order terms to quickly obtain a single solution. Our experiments show that our approach is both stable and fast on challenging test sequences from vehicle-mounted cameras."'),
('"Approximation and Processing of Intensity Images with Discontinuity-Preserving Adaptive Triangular ', '"ECCV 2000"', '["Original Image", "Gray Level", "Intensity Image", "Triangular Mesh", "Range Image"]', '"https://doi.org/10.1007/3-540-45054-8_55"', '"A new algorithm for approximating intensity images with adaptive triangular meshes keeping image discontinuities and avoiding optimization is presented. The algorithm consists of two main stages. In the first stage, the original image is adaptively sampled at a set of points, taking into account both image discontinuities and curvatures. In the second stage, the sampled points are triangulated by applying a constrained 2D Delaunay algorithm. The obtained triangular meshes are compact representations that model the regions and discontinuities present in the original image with many fewer points. Thus, image processing operations applied upon those meshes can perform faster than upon the original images. As an example, four simple operations (translation, rotation, scaling and deformation) have been implemented in the 3D geometric domain and compared to their image domain counterparts.1"'),
('"AQUATICS Reconstruction Software: The Design of a Diagnostic Tool Based on Computer Vision Algorith', '"MMBIA 2004"', '["Abdominal Aortic Aneurysm", "Vessel Lumen", "Deformable Model", "Active Contour Model", "Computer ', '"https://doi.org/10.1007/978-3-540-27816-0_5"', '"Computer vision methods can be applied to a variety of medical and surgical applications, and many techniques and algorithms are available that can be used to recover 3D shapes and information from images range and volume data. Complex practical applications, however, are rarely approachable with a single technique, and require detailed analysis on how they can be subdivided in subtasks that are computationally treatable and that, at the same time, allow for the appropriate level of user-interaction. In this paper we show an example of a complex application where, following criteria of efficiency, reliability and user friendliness, several computer vision techniques have been selected and customized to build a system able to support diagnosis and endovascular treatment of Abdominal Aortic Aneurysms. The system reconstructs the geometrical representation of four different structures related to the aorta (vessel lumen, thrombus, calcifications and skeleton) from CT angiography data. In this way it supports the three dimensional measurements required for a careful geometrical evaluation of the vessel, that is fundamental to decide if the treatment is necessary and to perform, in this case, its planning. The system has been realized within the European trial AQUATICS (IST-1999-20226 EUTIST-M WP 12), and it has been widely tested on clinical data."'),
('"Architectural Style Classification Using Multinomial Latent Logistic Regression"', '"ECCV 2014"', '["Latent Variable Models", "Architectural Style Classification", "Architectural Style Dataset"]', '"https://doi.org/10.1007/978-3-319-10590-1_39"', '"Architectural style classification differs from standard classification tasks due to the rich inter-class relationships between different styles, such as re-interpretation, revival, and territoriality. In this paper, we adopt Deformable Part-based Models (DPM) to capture the morphological characteristics of basic architectural components and propose Multinomial Latent Logistic Regression (MLLR) that introduces the probabilistic analysis and tackles the multi-class problem in latent variable models. Due to the lack of publicly available datasets, we release a new large-scale architectural style dataset containing twenty-five classes. Experimentation on this dataset shows that MLLR in combination with standard global image features, obtains the best classification results. We also present interpretable probabilistic explanations for the results, such as the styles of individual buildings and a style relationship network, to illustrate inter-class relationships."'),
('"Architectures for Biometric Match-on-Token Solutions"', '"BioAW 2004"', '["Feature Vector", "Smart Card", "Security Mechanism", "Speaker Recognition", "Biometric Data"]', '"https://doi.org/10.1007/978-3-540-25976-3_18"', '"Biometric Authentication Systems face several problems related to the security and the users\\u2019 potential rejection. Most of those problems comes from the high sensitivity of biometric data (whose manipulation could damage the system security or the users\\u2019 privacy), and the feeling of governmental control when central databases are used. The authors have been working in providing solutions to these problems, by promoting the idea of using tokens that could perform the biometric verification internally, not allowing the template to travel through communication lines, nor the memory of an open terminal. To achieve this, a general architecture, based on a set of requirements previously defined, will be outlined. At the end, directives for the implementation of such an architecture using smartcards and USB-Tokens will be given."'),
('"Archive Film Restoration Based on Spatiotemporal Random Walks"', '"ECCV 2010"', '["Random Walk", "Mean Square Error", "Motion Vector", "Local Binary Pattern", "Texture Synthesis"]', '"https://doi.org/10.1007/978-3-642-15555-0_35"', '"We propose a novel restoration method for defects and missing regions in video sequences, particularly in application to archive film restoration. Our statistical framework is based on random walks to examine the spatiotemporal path of a degraded pixel, and uses texture features in addition to intensity and motion information traditionally used in previous restoration works. The degraded pixels within a frame are restored in a multiscale framework by updating their features (intensity, motion and texture) at each level with reference to the attributes of normal pixels and other defective pixels in the previous scale as long as they fall within the defective pixel\\u2019s random walk-based spatiotemporal neighbourhood. The proposed algorithm is compared against two state-of-the-art methods to demonstrate improved accuracy in restoring synthetic and real degraded image sequences."'),
('"Are Iterations and Curvature Useful for Tensor Voting?"', '"ECCV 2004"', '["Synthetic Aperture Radar", "Synthetic Aperture Radar Image", "Perceptual Grouping", "Curvature Cal', '"https://doi.org/10.1007/978-3-540-24672-5_13"', '"Tensor voting is an efficient algorithm for perceptual grouping and feature extraction, particularly for contour extraction. In this paper two studies on tensor voting are presented. First the use of iterations is investigated, and second, a new method for integrating curvature information is evaluated. In opposition to other grouping methods, tensor voting claims the advantage to be non-iterative. Although non-iterative tensor voting methods provide good results in many cases, the algorithm can be iterated to deal with more complex data configurations. The experiments conducted demonstrate that iterations substantially improve the process of feature extraction and help to overcome limitations of the original algorithm. As a further contribution we propose a curvature improvement for tensor voting. On the contrary to the curvature-augmented tensor voting proposed by Tang and Medioni, our method takes advantage of the curvature calculation already performed by the classical tensor voting and evaluates the full curvature, sign and amplitude. Some new curvature-modified voting fields are also proposed. Results show a lower degree of artifacts, smoother curves, a high tolerance to scale parameter changes and also more noise-robustness."'),
('"Are You Really Smiling at Me? Spontaneous versus Posed Enjoyment Smiles"', '"ECCV 2012"', '["Face analysis", "smile classification", "affective computing"]', '"https://doi.org/10.1007/978-3-642-33712-3_38"', '"Smiling is an indispensable element of nonverbal social interaction. Besides, automatic distinction between spontaneous and posed expressions is important for visual analysis of social signals. Therefore, in this paper, we propose a method to distinguish between spontaneous and posed enjoyment smiles by using the dynamics of eyelid, cheek, and lip corner movements. The discriminative power of these movements, and the effect of different fusion levels are investigated on multiple databases. Our results improve the state-of-the-art. We also introduce the largest spontaneous/posed enjoyment smile database collected to date, and report new empirical and conceptual findings on smile dynamics. The collected database consists of 1240 samples of 400 subjects. Moreover, it has the unique property of having an age range from 8 to 76 years. Large scale experiments on the new database indicate that eyelid dynamics are highly relevant for smile classification, and there are age-related differences in smile dynamics."'),
('"Arm-Pointer: 3D Pointing Interface for Real-World Interaction"', '"CVHCI 2004"', '["Human Computer Interaction", "Real Object", "Virtual Object", "Stereo Vision", "Weighted Vote"]', '"https://doi.org/10.1007/978-3-540-24837-8_8"', '"We propose a new real-world pointing interface, called Arm-Pointer, for user interaction with real objects. Pointing at objects for which a computer is to perform some operation is a fundamental, yet important, process in human-computer interaction (HCI). Arm-Pointer enables a user to point the computer to a real object directly by extending his arm towards the object. In conventional pointing methods, HCI studies have concentrated on pointing at virtual objects existing in computers. However, there are the vast number of real objects that requires user operation. Arm-Pointer enables users to point at objects in the real world to inform a computer to operate them without the user having to wear any special devices or making direct contacts with the objects. In order to identify the object the user specifies, the position and direction of the arm pointing are recognized by extracting the user\\u2019s shoulders and arms. Therefore, an easy-to-use real-world oriented interaction system is realized using the proposed method. We developed an algorithm which uses weighted voting for robust recognition. A prototype system using a stereo vision camera was developed and the real-time recognition was confirmed by experiment."'),
('"Articulated Motion Segmentation Using RANSAC with Priors"', '"WDV 2006"', '[]', '"https://doi.org/10.1007/978-3-540-70932-9_6"', '"Articulated motions are partially dependent. Most of the existing segmentation methods, e.g. Costeira and Kanade[2], can not be applied to articulated motions."'),
('"Articulated Multi-body Tracking under Egomotion"', '"ECCV 2008"', '["Marginal Likelihood", "Temporary Occlusion", "Scale Conjugate Gradient", "Body Articulation", "Dat', '"https://doi.org/10.1007/978-3-540-88688-4_60"', '"In this paper, we address the problem of 3D articulated multi-person tracking in busy street scenes from a moving, human-level observer. In order to handle the complexity of multi-person interactions, we propose to pursue a two-stage strategy. A multi-body detection-based tracker first analyzes the scene and recovers individual pedestrian trajectories, bridging sensor gaps and resolving temporary occlusions. A specialized articulated tracker is then applied to each recovered pedestrian trajectory in parallel to estimate the tracked person\\u2019s precise body pose over time. This articulated tracker is implemented in a Gaussian Process framework and operates on global pedestrian silhouettes using a learned statistical representation of human body dynamics. We interface the two tracking levels through a guided segmentation stage, which combines traditional bottom-up cues with top-down information from a human detector and the articulated tracker\\u2019s shape prediction. We show the proposed approach\\u2019s viability and demonstrate its performance for articulated multi-person tracking on several challenging video sequences of a busy inner-city scenario."'),
('"Articulated-Body Tracking Through Anisotropic Edge Detection"', '"WDV 2006"', '["Edge Detection", "Kinematic Model", "Image Patch", "Kinematic Chain", "Rigid Motion"]', '"https://doi.org/10.1007/978-3-540-70932-9_7"', '"This paper addresses the problem of articulated motion tracking from image sequences. We describe a method that relies on both an explicit parameterization of the extremal contours and on the prediction of the human boundary edges in the image. We combine extremal contour prediction and edge detection in a non linear minimization process. The error function that measures the discrepancy between observed image edges and predicted model contours is minimized using an analytical expression of the Jacobian that maps joint velocities onto extremal contour velocities. In practice, we model people both by their geometry (truncated elliptic cones) and their articulated structure \\u2013 a kinematic model with 40 rotational degrees of freedom. To overcome the flaws of standard edge detection, we introduce a model-based anisotropic Gaussian filter. The parameters of the anisotropic Gaussian are automatically derived from the kinematic model through the prediction of the extremal contours. The theory is validated by performing full body motion capture from six synchronized video sequences at 30 fps without markers."'),
('"Articulation-Invariant Representation of Non-planar Shapes"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_21"', '"Given a set of points corresponding to a 2D projection of a non-planar shape, we would like to obtain a representation invariant to articulations (under no self-occlusions). It is a challenging problem since we need to account for the changes in 2D shape due to 3D articulations, viewpoint variations, as well as the varying effects of imaging process on different regions of the shape due to its non-planarity. By modeling an articulating shape as a combination of approximate convex parts connected by non-convex junctions, we propose to preserve distances between a pair of points by (i) estimating the parts of the shape through approximate convex decomposition, by introducing a robust measure of convexity and (ii) performing part-wise affine normalization by assuming a weak perspective camera model, and then relating the points using the inner distance which is insensitive to planar articulations. We demonstrate the effectiveness of our representation on a dataset with non-planar articulations, and on standard shape retrieval datasets like MPEG-7."'),
('"Artificial Mosaics with Irregular Tiles Based on Gradient Vector Flow"', '"ECCV 2012"', '["Artificial Mosaics", "Gradient Vector Flow"]', '"https://doi.org/10.1007/978-3-642-33863-2_60"', '"Artificial mosaics can be generated making use of computational processes devoted to reproduce different artistic styles and related issues. One of the most challenging field is the generation of an artificial mosaic reproducing some ancient and well known techniques starting from any input image. In this paper we propose a mosaic generation approach based on gradient vector flow (GVF) properly integrated with a set of tile cutting heuristics. The various involved cutting strategies, namely subtractive and shared cuts, have been evaluated according to aesthetic criteria. Several tests and comparisons with a state-of-the-art method confirm the effectiveness of the proposed approach."'),
('"Artistic Image Analysis Using the Composition of Human Figures"', '"ECCV 2014"', '["Artistic image analysis", "Image feature", "Image annotation and retrieval"]', '"https://doi.org/10.1007/978-3-319-16178-5_8"', '"Artistic image understanding is an interdisciplinary research field of increasing importance for the computer vision and art history communities. One of the goals of this field is the implementation of a system that can automatically retrieve and annotate artistic images. The best approach in the field explores the artistic influence among different artistic images using graph-based learning methodologies that take into consideration appearance and label similarities, but the current state-of-the-art results indicate that there seems to be lots of room for improvements in terms of retrieval and annotation accuracy. In order to improve those results, we introduce novel human figure composition features that can compute the similarity between artistic images based on the location and number (i.e., composition) of human figures. Our main motivation for developing such features lies in the importance that composition (particularly the composition of human figures) has in the analysis of artistic images when defining the visual classes present in those images. We show that the introduction of such features in the current dominant methodology of the field improves significantly the state-of-the-art retrieval and annotation accuracies on the PRINTART database, which is a public database exclusively composed of artistic images."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Artistic Image Classification: An Analysis on the PRINTART Database"', '"ECCV 2012"', '["Training Image", "Mean Average Precision", "Image Annotation", "Matrix Completion", "Retrieval Pro', '"https://doi.org/10.1007/978-3-642-33765-9_11"', '"Artistic image understanding is an interdisciplinary research field of increasing importance for the computer vision and the art history communities. For computer vision scientists, this problem offers challenges where new techniques can be developed; and for the art history community new automatic art analysis tools can be developed. On the positive side, artistic images are generally constrained by compositional rules and artistic themes. However, the low-level texture and color features exploited for photographic image analysis are not as effective because of inconsistent color and texture patterns describing the visual classes in artistic images. In this work, we present a new database of monochromatic artistic images containing 988 images with a global semantic annotation, a local compositional annotation, and a pose annotation of human subjects and animal types. In total, 75 visual classes are annotated, from which 27 are related to the theme of the art image, and 48 are visual classes that can be localized in the image with bounding boxes. Out of these 48 classes, 40 have pose annotation, with 37 denoting human subjects and 3 representing animal types. We also provide a complete evaluation of several algorithms recently proposed for image annotation and retrieval. We then present an algorithm achieving remarkable performance over the most successful algorithm hitherto proposed for this problem. Our main goal with this paper is to make this database, the evaluation process, and the benchmark results available for the computer vision community."'),
('"As-Rigid-As-Possible Stereo under Second Order Smoothness Priors"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-10605-2_8"', '"Imposing smoothness priors is a key idea of the top-ranked global stereo models. Recent progresses demonstrated the power of second order priors which are usually defined by either explicitly considering three-pixel neighborhoods, or implicitly using a so-called 3D-label for each pixel. In contrast to the traditional first-order priors which only prefer fronto-parallel surfaces, second-order priors encourage arbitrary collinear structures. However, we still can find defective regions in matching results even under such powerful priors, e.g., large textureless regions. One reason is that most of the stereo models are non-convex, where pixel-wise smoothness priors, i.e., local constraints, are too flexible to prevent the solution from trapping in bad local minimums. On the other hand, long-range spatial constraints, especially the segment-based priors, have advantages on this problem. However, segment-based priors are too rigid to handle curved surfaces. We present a mixture model to combine the benefits of these two kinds of priors, whose energy function consists of two terms 1) a Laplacian operator on the disparity map which imposes pixel-wise second-order smoothness; 2) a segment-wise matching cost as a function of quadratic surface, which encourages \\u201cas-rigid-as-possible\\u201d smoothness. To effectively solve the problem, we introduce an intermediate term to decouple the two subenergies, which enables an alternated optimization algorithm that is about an order of magnitude faster than PatchMatch [1]. Our approach is one of the top ranked models on the Middlebury benchmark at sub-pixel accuracy."'),
('"Ask\\u2019nSeek: A New Game for Object Detection and Labeling"', '"ECCV 2012"', '["Object Detection", "Spatial Relation", "User Study", "Image Annotation", "Simulated Game"]', '"https://doi.org/10.1007/978-3-642-33863-2_25"', '"This paper proposes a novel approach to detect and label objects within images and describes a two-player web-based guessing game \\u2013 Ask\\u2019nSeek \\u2013 that supports these tasks in a fun and interactive way. Ask\\u2019nSeek asks users to guess the location of a hidden region within an image with the help of semantic and topological clues. The information collected from game logs is combined with results from content analysis algorithms and used to feed a machine learning algorithm that outputs the outline of the most relevant regions within the image and their names. Two noteworthy aspects of the proposed game are: (i) it solves two computer vision problems \\u2013 object detection and labeling \\u2013 in a single game; and (ii) it learns spatial relations within the image from game logs. The game has been evaluated through user studies, which confirmed that it was easy to understand, intuitive, and fun to play."'),
('"ASN: Image Keypoint Detection from Adaptive Shape Neighborhood"', '"ECCV 2008"', '["Zenith Angle", "Scale Invariant Feature Transform", "Integration Scale", "Moment Matrix", "Viewpoi', '"https://doi.org/10.1007/978-3-540-88682-2_35"', '"We describe an accurate keypoint detector that is stable under viewpoint change. In this paper, keypoints correspond to actual junctions in the image. The principle of ASN differs fundamentally from other keypoint detectors. At each position in the image and before any detection, it systematically estimates the position of a potential junction from the local gradient field. Keypoints then appear where multiple position estimates are attracted. This approach allows the detector to adapt in shape and size to the image content. One further obtains the area where the keypoint has attracted solutions. Comparative results with other detectors show the improved accuracy and stability with viewpoint change."'),
('"Assessing the Quality of Actions"', '"ECCV 2014"', '["Discrete Cosine Transform", "Discrete Fourier Transform", "Support Vector Regression", "Action Rec', '"https://doi.org/10.1007/978-3-319-10599-4_36"', '"While recent advances in computer vision have provided reliable methods to recognize actions in both images and videos, the problem of assessing how well people perform actions has been largely unexplored in computer vision. Since methods for assessing action quality have many real-world applications in healthcare, sports, and video retrieval, we believe the computer vision community should begin to tackle this challenging problem. To spur progress, we introduce a learning-based framework that takes steps towards assessing how well people perform actions in videos. Our approach works by training a regression model from spatiotemporal pose features to scores obtained from expert judges. Moreover, our approach can provide interpretable feedback on how people can improve their action. We evaluate our method on a new Olympic sports dataset, and our experiments suggest our framework is able to rank the athletes more accurately than a non-expert human. While promising, our method is still a long way to rivaling the performance of expert judges, indicating that there is significant opportunity in computer vision research to improve on this difficult yet important task."'),
('"Assessing the Suitability of the Microsoft Kinect for Calculating Person Specific Body Segment Para', '"ECCV 2014"', '["Body segment parameters", "BSP", "Kinect", "Depth camera", "Measurement", "Body scanning"]', '"https://doi.org/10.1007/978-3-319-16178-5_26"', '"Many biomechanical and medical analyses rely on the availability of reliable body segment parameter estimates. Current techniques typically take many manual measurements of the human body, in conjunction with geometric models or regression equations. However, such techniques are often criticised. 3D scanning offers many advantages, but current systems are prohibitively complex and costly. The recent interest in natural user interaction (NUI) has led to the development of low cost (~\\u00a3200) sensors capable of 3D body scanning, however, there has been little consideration of their validity. A scanning system comprising four Microsoft Kinect sensors (a typical NUI sensor) was used to scan twelve living male participants three times. Volume estimates from the system were compared to those from a geometric modelling technique. Results demonstrated high reliability (ICC>0.7, TEM<1 %) and presence of a systematic measurement offset (0.001m\\\\(^{3}\\\\)), suggesting the system would be well received by healthcare and sports communities."'),
('"Assessment of Intrathoracic Airway Trees: Methods and In Vivo Validation"', '"MMBIA 2004"', '["Reference Tree", "Airway Tree", "Skeletonization Algorithm", "Centerline Extraction", "Human Chest', '"https://doi.org/10.1007/978-3-540-27816-0_29"', '"A method for quantitative assessment of tree structures is reported allowing evaluation of airway tree morphology and its associated function. Our skeletonization and branch\\u2013point identification method provides a basis for tree quantification or tree matching, tree\\u2013branch diameter measurement in any orientation, and labeling individual branch segments. All main components of our method were specifically developed to deal with imaging artifacts typically present in volumetric medical image data. The proposed method has been tested in a computer phantom subjected to changes of its orientation as well as in repeatedly CT-scanned rigid and rubber plastic phantoms. In this paper, validation is reported in six in vivo scans of the human chest."'),
('"Associating Locations Between Indoor Journeys from Wearable Cameras"', '"ECCV 2014"', '["Video Sequence", "Receive Signal Strength Indication", "Visual Data", "Indoor Localization", "Brit', '"https://doi.org/10.1007/978-3-319-16220-1_3"', '"The main question we address is whether it is possible to crowdsource navigational data in the form of video sequences captured from wearable cameras. Without using geometric inference techniques (such as SLAM), we test video data for its location-discrimination content. Tracking algorithms do not form part of this assessment, because our goal is to compare different visual descriptors for the purpose of location inference in highly ambiguous indoor environments. The testing of these descriptors, and different encoding methods, is performed by measuring the positional error inferred during one journey with respect to other journeys along the same approximate path."'),
('"Assorted Pixels: Multi-sampled Imaging with Structural Models"', '"ECCV 2002"', '["Neighborhood Size", "Color Channel", "High Dynamic Range", "Image Detector", "Sampling Pattern"]', '"https://doi.org/10.1007/3-540-47979-1_43"', '"Multi-sampled imaging is a general framework for using pixels on an image detector to simultaneously sample multiple dimensions of imaging (space, time, spectrum, brightness, polarization, etc.). The mosaic of red, green and blue spectral filters found in most solid-state color cameras is one example of multi-sampled imaging. We briefly describe how multi-sampling can be used to explore other dimensions of imaging. Once such an image is captured, smooth reconstructions along the individual dimensions can be obtained using standard interpolation algorithms. Typically, this results in a substantial reduction of resolution (and hence image quality). One can extract significantly greater resolution in each dimension by noting that the light fields associated with real scenes have enormous redundancies within them, causing different dimensions to be highly correlated. Hence, multi-sampled images can be better interpolated using local structural models that are learned offline from a diverse set of training images. The specific type of structural models we use are based on polynomial functions of measured image intensities. They are very effective as well as computationally efficient. We demonstrate the benefits of structural interpolation using three specific applications. These are (a) traditional color imaging with a mosaic of color filters, (b) high dynamic range monochrome imaging using a mosaic of exposure filters, and (c) high dynamic range color imaging using a mosaic of overlapping color and exposure filters."'),
('"Atomic Action Features: A New Feature for Action Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_29"', '"We introduce an atomic action based features and demonstrate that it consistently improves performance on human activity recognition. The features are built using auxiliary atomic action data collected in our lab. We train a kernelized SVM classifier for each atomic action class. Then given a local spatio-temporal cuboid of a test video, we represent it using the responses of our atomic action classifiers. This new atomic action feature is discriminative, and has semantic meanings. We perform extensive experiments on four benchmark action recognition datasets. The results show that atomic action features either outperform the corresponding low level features or significantly boost the recognition performance by combining the two."'),
('"Attribute Discovery via Predictable Discriminative Binary Codes"', '"ECCV 2012"', '["Image Retrieval", "Binary Code", "Query Image", "Locality Sensitive Hashing", "Attribute Discovery', '"https://doi.org/10.1007/978-3-642-33783-3_63"', '"We present images with binary codes in a way that balances discrimination and learnability of the codes. In our method, each image claims its own code in a way that maintains discrimination while being predictable from visual data. Category memberships are usually good proxies for visual similarity but should not be enforced as a hard constraint. Our method learns codes that maximize separability of categories unless there is strong visual evidence against it. Simple linear SVMs can achieve state-of-the-art results with our short codes. In fact, our method produces state-of-the-art results on Caltech256 with only 128-dimensional bit vectors and outperforms state of the art by using longer codes. We also evaluate our method on ImageNet and show that our method outperforms state-of-the-art binary code methods on this large scale dataset. Lastly, our codes can discover a discriminative set of attributes."'),
('"Attribute Learning for Understanding Unstructured Social Activity"', '"ECCV 2012"', '["Attribute Space", "Latent Attribute", "Topic Model", "Latent Dirichlet Allocation", "Transfer Lear', '"https://doi.org/10.1007/978-3-642-33765-9_38"', '"The rapid development of social video sharing platforms has created a huge demand for automatic video classification and annotation techniques, in particular for videos containing social activities of a group of people (e.g. YouTube video of a wedding reception). Recently, attribute learning has emerged as a promising paradigm for transferring learning to sparsely labelled classes in object or single-object short action classification. In contrast to existing work, this paper for the first time, tackles the problem of attribute learning for understanding group social activities with sparse labels. This problem is more challenging because of the complex multi-object nature of social activities, and the unstructured nature of the activity context. To solve this problem, we (1) contribute an unstructured social activity attribute (USAA) dataset with both visual and audio attributes, (2) introduce the concept of semi-latent attribute space and (3) propose a novel model for learning the latent attributes which alleviate the dependence of existing models on exact and exhaustive manual specification of the attribute-space. We show that our framework is able to exploit latent attributes to outperform contemporary approaches for addressing a variety of realistic multi-media sparse data learning tasks including: multi-task learning, N-shot transfer learning, learning with label noise and importantly zero-shot learning."'),
('"Attribute Learning in Large-Scale Datasets"', '"ECCV 2010"', '["Object Class", "Object Category", "Striped Zebra", "Attribute Learn", "Semantic Hierarchy"]', '"https://doi.org/10.1007/978-3-642-35749-7_1"', '"We consider the task of learning visual connections between object categories using the ImageNet dataset, which is a large-scale dataset ontology containing more than 15 thousand object classes. We want to discover visual relationships between the classes that are currently missing (such as similar colors or shapes or textures). In this work we learn 20 visual attributes and use them in a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories."'),
('"Attribute-Based Transfer Learning for Object Categorization with Zero/One Training Example"', '"ECCV 2010"', '["Knowledge Transfer", "Visual Word", "Gibbs Sampling", "Object Attribute", "Attribute Model"]', '"https://doi.org/10.1007/978-3-642-15555-0_10"', '"This paper studies the one-shot and zero-shot learning problems, where each object category has only one training example or has no training example at all. We approach this problem by transferring knowledge from known categories (a.k.a source categories) to new categories (a.k.a target categories) via object attributes. Object attributes are high level descriptions of object categories, such as color, texture, shape, etc. Since they represent common properties across different categories, they can be used to transfer knowledge from source categories to target categories effectively. Based on this insight, we propose an attribute-based transfer learning framework in this paper. We first build a generative attribute model to learn the probabilistic distributions of image features for each attribute, which we consider as attribute priors. These attribute priors can be used to (1) classify unseen images of target categories (zero-shot learning), or (2) facilitate learning classifiers for target categories when there is only one training examples per target category (one-shot learning). We demonstrate the effectiveness of the proposed approaches using the Animal with Attributes data set and show state-of-the-art performance in both zero-shot and one-shot learning tests."'),
('"Attributes for Classifier Feedback"', '"ECCV 2012"', '["Query Image", "Category Label", "Attribute Predictor", "Large Vocab", "Visual Concept"]', '"https://doi.org/10.1007/978-3-642-33712-3_26"', '"Traditional active learning allows a (machine) learner to query the (human) teacher for labels on examples it finds confusing. The teacher then provides a label for only that instance. This is quite restrictive. In this paper, we propose a learning paradigm in which the learner communicates its belief (i.e. predicted label) about the actively chosen example to the teacher. The teacher then confirms or rejects the predicted label. More importantly, if rejected, the teacher communicates an explanation for why the learner\\u2019s belief was wrong. This explanation allows the learner to propagate the feedback provided by the teacher to many unlabeled images. This allows a classifier to better learn from its mistakes, leading to accelerated discriminative learning of visual concepts even with few labeled images. In order for such communication to be feasible, it is crucial to have a language that both the human supervisor and the machine learner understand. Attributes provide precisely this channel. They are human-interpretable mid-level visual concepts shareable across categories e.g. \\u201cfurry\\u201d, \\u201cspacious\\u201d, etc. We advocate the use of attributes for a supervisor to provide feedback to a classifier and directly communicate his knowledge of the world. We employ a straightforward approach to incorporate this feedback in the classifier, and demonstrate its power on a variety of visual recognition scenarios such as image classification and annotation. This application of attributes for providing classifiers feedback is very powerful, and has not been explored in the community. It introduces a new mode of supervision, and opens up several avenues for future research."'),
('"Attributes Make Sense on Segmented Objects"', '"ECCV 2014"', '["attributes", "segmentation", "zero-shot classification"]', '"https://doi.org/10.1007/978-3-319-10599-4_23"', '"In this paper we aim for object classification and segmentation by attributes. Where existing work considers attributes either for the global image or for the parts of the object, we propose, as our first novelty, to learn and extract attributes on segments containing the entire object. Object-level attributes suffer less from accidental content around the object and accidental image conditions such as partial occlusions, scale changes and viewpoint changes. As our second novelty, we propose joint learning for simultaneous object classification and segment proposal ranking, solely on the basis of attributes. This naturally brings us to our third novelty: object-level attributes for zero-shot, where we use attribute descriptions of unseen classes for localizing their instances in new images and classifying them accordingly. Results on the Caltech UCSD Birds, Leeds Butterflies, and an a-Pascal subset demonstrate that i) extracting attributes on oracle object-level brings substantial benefits ii) our joint learning model leads to accurate attribute-based classification and segmentation, approaching the oracle results and iii) object-level attributes also allow for zero-shot classification and segmentation.We conclude that attributes make sense on segmented objects."'),
('"Audio-Video Integration for Background Modelling"', '"ECCV 2004"', '["Gaussian Mixture Model", "Background Modelling", "Audio Signal", "Blind Source Separation", "Multi', '"https://doi.org/10.1007/978-3-540-24671-8_16"', '"This paper introduces a new concept of surveillance, namely, audio-visual data integration for background modelling. Actually, visual data acquired by a fixed camera can be easily supported by audio information allowing a more complete analysis of the monitored scene. The key idea is to build a multimodal model of the scene background, able to promptly detect single auditory or visual events, as well as simultaneous audio and visual foreground situations. In this way, it is also possible to tackle some open problems (e.g., the sleeping foreground problems) of standard visual surveillance systems, if they are also characterized by an audio foreground. The method is based on the probabilistic modelling of the audio and video data streams using separate sets of adaptive Gaussian mixture models, and on their integration using a coupled audio-video adaptive model working on the frame histogram, and the audio frequency spectrum. This framework has shown to be able to evaluate the time causality between visual and audio foreground entities. To the best of our knowledge, this is the first attempt to the multimodal modelling of scenes working on-line and using one static camera and only one microphone. Preliminary results show the effectiveness of the approach at facing problems still unsolved by only visual monitoring approaches."'),
('"Audio-Video Sensor Fusion with Probabilistic Graphical Models"', '"ECCV 2002"', '["Video Data", "Audio Signal", "Video Model", "Audio Data", "Probabilistic Graphical Model"]', '"https://doi.org/10.1007/3-540-47969-4_49"', '"We present a new approach to modeling and processing multimedia data. This approach is based on graphical models that combine audio and video variables. We demonstrate it by developing a new algorithm for tracking a moving object in a cluttered, noisy scene using two microphones and a camera. Our model uses unobserved variables to describe the data in terms of the process that generates them. It is therefore able to capture and exploit the statistical structure of the audio and video data separately, as well as their mutual dependencies. Model parameters are learned from data via an EM algorithm, and automatic calibration is performed as part of this procedure. Tracking is done by Bayesian inference of the object location from data. We demonstrate successful performance on multimedia clips captured in real world scenarios using off-the-shelf equipment."'),
('"Audiovisual Conflict Detection in Political Debates"', '"ECCV 2014"', '["Video Frame", "Political Debate", "Head Nodding", "Audio Feature", "Active Appearance Model"]', '"https://doi.org/10.1007/978-3-319-16178-5_21"', '"In this paper, the problem of conflict detection in audiovisual recordings of political debates is investigated. In contrast to the current state of the art in social signal processing, where only the audio modality is employed for analysing the human non-verbal behavior, we propose to use additionally visual features capturing certain facial behavioral cues such as head nodding, fidgeting, and frowning which are related to conflicts. To this end, a dataset with video excerpts from televised political debates, where conflicts naturally arise, is introduced. The prediction of conflict level (i.e., conflict/nonconflict) is performed by applying the linear support vector machine and the collaborative representation-based classifier onto audio, visual, and audiovisual features. The experimental results demonstrate that the fusion of audio and visual features, outperform the accuracy in conflict detection, obtained by features that resort to a single modality (i.e., either audio or video)."'),
('"Augmented Attribute Representations"', '"ECCV 2012"', '["Discriminative Autoencoder", "Hybrid Representations"]', '"https://doi.org/10.1007/978-3-642-33715-4_18"', '"We propose a new learning method to infer a mid-level feature representation that combines the advantage of semantic attribute representations with the higher expressive power of non-semantic features. The idea lies in augmenting an existing attribute-based representation with additional dimensions for which an autoencoder model is coupled with a large-margin principle. This construction allows a smooth transition between the zero-shot regime with no training example, the unsupervised regime with training examples but without class labels, and the supervised regime with training examples and with class labels. The resulting optimization problem can be solved efficiently, because several of the necessity steps have closed-form solutions. Through extensive experiments we show that the augmented representation achieves better results in terms of object categorization accuracy than the semantic representation alone."'),
('"Augmenting Vehicle Localization Accuracy with Cameras and 3D Road Infrastructure Database"', '"ECCV 2014"', '["Vehicle localization", "Road infrastructure database", "Road signs", "Road markings"]', '"https://doi.org/10.1007/978-3-319-16178-5_13"', '"Accurate and continuous vehicle localization in urban environments has been an important research problem in recent years. In this paper, we propose a landmark based localization method using road signs and road markings. The principle is to associate the online detections from onboard cameras with the landmarks in a pre-generated road infrastructure database, then to adjust the raw vehicle pose predicted by the inertial sensors. This method was evaluated with data sequences acquired in urban streets. The results prove the contribution of road signs and road markings for reducing the trajectory drift as absolute control points."'),
('"Authentic Emotion Detection in Real-Time Video"', '"CVHCI 2004"', '["Facial Expression", "Bayesian Network", "Emotional Intelligence", "Gesture Recognition", "Facial E', '"https://doi.org/10.1007/978-3-540-24837-8_10"', '"There is a growing trend toward emotional intelligence in human-computer interaction paradigms. In order to react appropriately to a human, the computer would need to have some perception of the emotional state of the human. We assert that the most informative channel for machine perception of emotions is through facial expressions in video. One current difficulty in evaluating automatic emotion detection is that there are currently no international databases which are based on authentic emotions. The current facial expression databases contain facial expressions which are not naturally linked to the emotional state of the test subject. Our contributions in this work are twofold: First, we create the first authentic facial expression database where the test subjects are showing the natural facial expressions based upon their emotional state. Second, we evaluate the several promising machine learning algorithms for emotion detection which include techniques such as Bayesian Networks, SVMs, and Decision trees."'),
('"Auto-Grouped Sparse Representation for Visual Analysis"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_46"', '"In this work, we investigate how to automatically uncover the underlying group structure of a feature vector such that each group characterizes certain object-specific patterns, e.g., visual pattern or motion trajectories from one object. By mining the group structure, we can effectively alleviate the mutual inference of multiple objects and improve the performance in various visual analysis tasks. To this end, we propose a novel auto-grouped sparse representation (ASR) method. ASR groups semantically correlated feature elements together through optimally fusing their multiple sparse representations. Due to the intractability of primal objective function, we also propose well-behaved convex relaxation and smooth approximation to guarantee obtaining a global optimal solution effectively. Finally, we apply ASR in two important visual analysis tasks: multi-label image classification and motion segmentation. Comprehensive experimental evaluations show that ASR is able to achieve superior performance compared with the state-of-the-arts on these two tasks."'),
('"Automated 3D Reconstruction and Segmentation from Optical Coherence Tomography"', '"ECCV 2010"', '["Optical Coherence Tomography", "Active Contour", "Active Contour Model", "Human Cornea", "Corneal ', '"https://doi.org/10.1007/978-3-642-15558-1_4"', '"Ultra-High Resolution Optical Coherence Tomography is a novel imaging technology that allows non-invasive, high speed, cellular resolution imaging of anatomical structures in the human eye, including the retina and the cornea."'),
('"Automated Delineation of Dendritic Networks in Noisy Image Stacks"', '"ECCV 2008"', '["Gaussian Mixture Model", "Minimum Span Tree", "Appearance Model", "Dendritic Tree", "Weak Learner"', '"https://doi.org/10.1007/978-3-540-88693-8_16"', '"We present a novel approach to 3D delineation of dendritic networks in noisy image stacks. We achieve a level of automation beyond that of state-of-the-art systems, which model dendrites as continuous tubular structures and postulate simple appearance models. Instead, we learn models from the data itself, which make them better suited to handle noise and deviations from expected appearance."'),
('"Automated Optic Disc Localization and Contour Detection Using Ellipse Fitting and Wavelet Transform', '"ECCV 2004"', '["Optic Disc", "Active Contour", "Retinal Image", "Active Contour Model", "Daubechies Wavelet"]', '"https://doi.org/10.1007/978-3-540-24671-8_11"', '"Optic disc detection is important in the computer-aided analysis of retinal images. It is crucial for the precise identification of the macula to enable successful grading of macular pathology such as diabetic maculopathy. However, the extreme variation of intensity features within the optic disc and intensity variations close to the optic disc boundary presents a major obstacle in automated optic disc detection. The presence of blood vessels, crescents and peripapillary chorioretinal atrophy seen in myopic patients also increase the complexity of detection. Existing techniques have not addressed these difficult cases, and are neither adaptable nor sufficiently sensitive and specific for real-life application. This work presents a novel algorithm to detect the optic disc based on wavelet processing and ellipse fitting. We first employ Daubechies wavelet transform to approximate the optic disc region. Next, an abstract representation of the optic disc is obtained using an intensity-based template. This yields robust results in cases where the optic disc intensity is highly non-homogenous. Ellipse fitting algorithm is then utilized to detect the optic disc contour from this abstract representation. Additional wavelet processing is performed on the more complex cases to improve the contour detection rate. Experiments on 279 consecutive retinal images of diabetic patients indicate that this approach is able to achieve an accuracy of 94% for optic disc detection."'),
('"Automated Textual Descriptions for a Wide Range of Video Events with 48 Human Actions"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_37"', '"Presented is a hybrid method to generate textual descriptions of video based on actions. The method includes an action classifier and a description generator. The aim for the action classifier is to detect and classify the actions in the video, such that they can be used as verbs for the description generator. The aim of the description generator is (1) to find the actors (objects or persons) in the video and connect these correctly to the verbs, such that these represent the subject, and direct and indirect objects, and (2) to generate a sentence based on the verb, subject, and direct and indirect objects. The novelty of our method is that we exploit the discriminative power of a bag-of-features action detector with the generative power of a rule-based action descriptor. Shown is that this approach outperforms a homogeneous setup with the rule-based action detector and action descriptor."'),
('"Automatic Attribute Discovery and Characterization from Noisy Web Data"', '"ECCV 2010"', '["Object Category", "Visual Appearance", "Visual Attribute", "Potential Attribute", "Multiple Instan', '"https://doi.org/10.1007/978-3-642-15549-9_48"', '"It is common to use domain specific terminology \\u2013 attributes \\u2013 to describe the visual appearance of objects. In order to scale the use of these describable visual attributes to a large number of categories, especially those not well studied by psychologists or linguists, it will be necessary to find alternative techniques for identifying attribute vocabularies and for learning to recognize attributes without hand labeled training data. We demonstrate that it is possible to accomplish both these tasks automatically by mining text and image data sampled from the Internet. The proposed approach also characterizes attributes according to their visual representation: global or local, and type: color, texture, or shape. This work focuses on discovering attributes and their visual appearance, and is as agnostic as possible about the textual description."'),
('"Automatic Camera Calibration from a Single Manhattan Image"', '"ECCV 2002"', '["Focal Length", "Single Image", "Importance Sampling", "Camera Calibration", "Principal Point"]', '"https://doi.org/10.1007/3-540-47979-1_12"', '"We present a completely automatic method for obtaining the approximate calibration of a camera (alignment to a world frame and focal length) from a single image of an unknown scene, provided only that the scene satisfies a Manhattan world assumption. This assumption states that the imaged scene contains three orthogonal, dominant directions, and is often satisfied by outdoor or indoor views of man-made structures and environments."'),
('"Automatic Detection and Tracking of Human Motion with a View-Based Representation"', '"ECCV 2002"', '["Visual motion", "motion detection and tracking", "human motion analysis", "probabilistic models", ', '"https://doi.org/10.1007/3-540-47969-4_32"', '"This paper proposes a solution for the automatic detection and tracking of human motion in image sequences. Due to the complexity of the human body and its motion, automatic detection of 3D human motion remains an open, and important, problem. Existing approaches for automatic detection and tracking focus on 2D cues and typically exploit object appearance (color distribution, shape) or knowledge of a static background. In contrast, we exploit 2D optical flow information which provides rich descriptive cues, while being independent of object and background appearance. To represent the optical flow patterns of people from arbitrary viewpoints, we develop a novel representation of human motion using low-dimensional spatio-temporal models that are learned using motion capture data of human subjects. In addition to human motion (the foreground) we probabilistically model the motion of generic scenes (the background); these statistical models are defined as Gibbsian fields specified from the first-order derivatives of motion observations. Detection and tracking are posed in a principled Bayesian framework which involves the computation of a posterior probability distribution over the model parameters (i.e., the location and the type of the human motion) given a sequence of optical flow observations. Particle filtering is used to represent and predict this non-Gaussian posterior distribution over time. The model parameters of samples from this distribution are related to the pose parameters of a 3D articulated model (e.g. the approximate joint angles and movement direction). Thus the approach proves suitable for initializing more complex probabilistic models of human motion. As shown by experiments on real image sequences, our method is able to detect and track people under different viewpoints with complex backgrounds."'),
('"Automatic Detection of the Optimal Acceptance Threshold in a Face Verification System"', '"BioAW 2004"', '["Support Vector Machine", "Radial Basis Function", "Neural Network Radial Basis Function", "Equal E', '"https://doi.org/10.1007/978-3-540-25976-3_7"', '"We present a face verification system with an acceptance threshold automatically computed. The user is allowed to provide the rate between the costs assumed for a false acceptance and false rejection. This rate between costs can be intuitively known by the system responsible and are a starting point to fulfil user security requirements. With this user-friendly data, an algorithm based on screening techniques to compute the acceptance threshold is presented in this paper. This algorithm is applied to an original and competitive face verification system based on principal component analysis and two classifiers (neural network radial basis function and support vector machine). Experimental results with a 100 people face database are shown. This method can be also applied into other biometric applications in which this threshold should be calculated."'),
('"Automatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation"', '"ECCV 2014"', '["Dataset expansion", "Food image", "Foodness", "Domain adaptation", "Crowd-sourcing", "Adaptive SVM', '"https://doi.org/10.1007/978-3-319-16199-0_1"', '"In this paper, we propose a novel effective framework to expand an existing image dataset automatically leveraging existing categories and crowdsourcing. Especially, in this paper, we focus on expansion on food image data set. The number of food categories is uncountable, since foods are different from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for different food cultures have been built independently so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic \\u201cfoodness\\u201d classifier and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the effectiveness of the proposed method over the baselines."'),
('"Automatic Exposure Correction of Consumer Photographs"', '"ECCV 2012"', '["Input Image", "Exposure Evaluation", "Histogram Equalization", "Zone Region", "Tone Mapping"]', '"https://doi.org/10.1007/978-3-642-33765-9_55"', '"We study the problem of automatically correcting the exposure of an input image. Generic auto-exposure correction methods usually fail in individual over-/under-exposed regions. Interactive corrections may fix this issue, but adjusting every photograph requires skill and time. This paper will automate the interactive correction technique by estimating the image specific S-shaped non-linear tone curve that best fits the input image. Our first contribution is a new Zone-based region-level optimal exposure evaluation, which would consider both the visibility of individual regions and relative contrast between regions. Then a detail-preserving S-curve adjustment is applied based on the optimal exposure to obtain the final output. We show that our approach enables better corrections comparing with popular image editing tools and other automatic methods."'),
('"Automatic Facial Landmark Tracking in Video Sequences Using Kalman Filter Assisted Active Shape Mod', '"ECCV 2010"', '["Kalman Filter", "Video Sequence", "Tracking Method", "Face Detection", "Previous Frame"]', '"https://doi.org/10.1007/978-3-642-35749-7_7"', '"In this paper we address the problem of automatically locating the facial landmarks of a single person across frames of a video sequence. We propose two methods that utilize Kalman filter based approaches to assist an Active Shape Model (ASM) in achieving this goal. The use of Kalman filtering not only aids in better initialization of the ASM by predicting landmark locations in the next frame but also helps in refining its search results and hence in producing improved fitting accuracy. We evaluate our tracking methods on frames from three video sequences and quantitatively demonstrate their reliability and accuracy."'),
('"Automatic Generator of Minimal Problem Solvers"', '"ECCV 2008"', '["Polynomial Equation", "Automatic Generator", "Minimal Problem Solver", "Quotient Ring", "Basis Sel', '"https://doi.org/10.1007/978-3-540-88690-7_23"', '"Finding solutions to minimal problems for estimating epipolar geometry and camera motion leads to solving systems of algebraic equations. Often, these systems are not trivial and therefore special algorithms have to be designed to achieve numerical robustness and computational efficiency. The state of the art approach for constructing such algorithms is the Gr\\u00f6bner basis method for solving systems of polynomial equations. Previously, the Gr\\u00f6bner basis solvers were designed ad hoc for concrete problems and they could not be easily applied to new problems. In this paper we propose an automatic procedure for generating Gr\\u00f6bner basis solvers which could be used even by non-experts to solve technical problems. The input to our solver generator is a system of polynomial equations with a finite number of solutions. The output of our solver generator is the Matlab or C code which computes solutions to this system for concrete coefficients. Generating solvers automatically opens possibilities to solve more complicated problems which could not be handled manually or solving existing problems in a better and more efficient way. We demonstrate that our automatic generator constructs efficient and numerically stable solvers which are comparable or outperform known manually constructed solvers. The automatic generator is available at http://cmp.felk.cvut.cz/minimal"'),
('"Automatic Image Colorization Via Multimodal Predictions"', '"ECCV 2008"', '["Color Space", "Support Vector Regression", "Color Variation", "Local Description", "Greyscale Imag', '"https://doi.org/10.1007/978-3-540-88690-7_10"', '"We aim to color greyscale images automatically, without any manual intervention. The color proposition could then be interactively corrected by user-provided color landmarks if necessary. Automatic colorization is nontrivial since there is usually no one-to-one correspondence between color and local texture. The contribution of our framework is that we deal directly with multimodality and estimate, for each pixel of the image to be colored, the probability distribution of all possible colors, instead of choosing the most probable color at the local level. We also predict the expected variation of color at each pixel, thus defining a non-uniform spatial coherency criterion. We then use graph cuts to maximize the probability of the whole colored image at the global level. We work in the L-a-b color space in order to approximate the human perception of distances between colors, and we use machine learning tools to extract as much information as possible from a dataset of colored examples. The resulting algorithm is fast, designed to be more robust to texture noise, and is above all able to deal with ambiguity, in contrary to previous approaches."'),
('"Automatic Image Segmentation by Positioning a Seed"', '"ECCV 2006"', '["Image Segmentation", "Seed Position", "Automatic Image Segmentation", "Combine Boundary", "Human S', '"https://doi.org/10.1007/11744047_36"', '"We present a method that automatically partitions a single image into non-overlapping regions coherent in texture and colour. An assumption that each textured or coloured region can be represented by a small template, called the seed, is used. Positioning of the seed across the input image gives many possible sub-segmentations of the image having same texture and colour property as the pixels behind the seed. A probability map constructed during the sub-segmentations helps to assign each pixel to just one most probable region and produce the final pyramid representing various detailed segmentations at each level. Each sub-segmentation is obtained as the min-cut/max-flow in the graph built from the image and the seed. One segment may consist of several isolated parts. Compared to other methods our approach does not need a learning process or a priori information about the textures in the image. Performance of the method is evaluated on images from the Berkeley database."'),
('"Automatic Learning of Background Semantics in Generic Surveilled Scenes"', '"ECCV 2010"', '["Ground Truth", "Segmentation Accuracy", "Parking Space", "Smoothness Constraint", "Ground Truth Im', '"https://doi.org/10.1007/978-3-642-15552-9_49"', '"Advanced surveillance systems for behavior recognition in outdoor traffic scenes depend strongly on the particular configuration of the scenario. Scene-independent trajectory analysis techniques statistically infer semantics in locations where motion occurs, and such inferences are typically limited to abnormality. Thus, it is interesting to design contributions that automatically categorize more specific semantic regions. State-of-the-art approaches for unsupervised scene labeling exploit trajectory data to segment areas like sources, sinks, or waiting zones. Our method, in addition, incorporates scene-independent knowledge to assign more meaningful labels like crosswalks, sidewalks, or parking spaces. First, a spatiotemporal scene model is obtained from trajectory analysis. Subsequently, a so-called GI-MRF inference process reinforces spatial coherence, and incorporates taxonomy-guided smoothness constraints. Our method achieves automatic and effective labeling of conceptual regions in urban scenarios, and is robust to tracking errors. Experimental validation on 5 surveillance databases has been conducted to assess the generality and accuracy of the segmentations. The resulting scene models are used for model-based behavior analysis."'),
('"Automatic Localization of Balloon Markers and Guidewire in Rotational Fluoroscopy with Application ', '"ECCV 2012"', '["Percutaneous Coronary Interven", "Viterbi Algorithm", "Marker Pair", "Marker Tracking", "Target Ma', '"https://doi.org/10.1007/978-3-642-33783-3_31"', '"A fully automatic framework is proposed to identify consistent landmarks and wire structures in a rotational X-ray scan. In our application, we localize the balloon marker pair and the guidewire in between the marker pair on each projection angle from a rotational fluoroscopic sequence. We present an effective offline balloon marker tracking algorithm that leverages learning based detectors and employs the Viterbi algorithm to track the balloon markers in a globally optimal manner. Localizing the guidewire in between the tracked markers is formulated as tracking the middle control point of the spline fitting the guidewire. The experimental studies demonstrate that our methods achieve a marker tracking accuracy of 96.33% and a mean guidewire localization error of 0.46 mm, suggesting a great potential of our methods for clinical applications. The proposed offline marker tracking method is also successfully applied to the problem of automatic self-initialization of generic online marker trackers for 2D live fluoroscopy stream, demonstrating a success rate of 95.9% on 318 sequences. Its potential applications also include localization of landmarks in a generic rotational scan."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Automatic Model Selection by Modelling the Distribution of Residuals"', '"ECCV 2002"', '["Model Selection", "Gaussian Noise", "Measurement Noise", "Shape Model", "Unseen Data"]', '"https://doi.org/10.1007/3-540-47979-1_42"', '"Many problems in computer vision involve a choice of the most suitable model for a set of data. Typically one wishes to choose a model which best represents the data in a way that generalises to unseen data without overfitting. We propose an algorithm in which the quality of a model match can be determined by calculating how well the distribution of model residuals matches a distribution estimated from the noise on the data. The distribution of residuals has two components - the measurement noise, and the noise caused by the uncertainty in the model parameters. If the model is too complex to be supported by the data, then there will be large uncertainty in the parameters. We demonstrate that the algorithm can be used to select appropriate model complexity in a variety of problems, including polynomial fitting, and selecting the number of modes to match a shape model to noisy data."'),
('"Automatic Non-rigid 3D Modeling from Video"', '"ECCV 2004"', '["Video Sequence", "Outlier Probability", "Rigid Motion", "Color Constancy", "Shape Reconstruction"]', '"https://doi.org/10.1007/978-3-540-24671-8_24"', '"We present a robust framework for estimating non-rigid 3D shape and motion in video sequences. Given an input video sequence, and a user-specified region to reconstruct, the algorithm automatically solves for the 3D time-varying shape and motion of the object, and estimates which pixels are outliers, while learning all system parameters, including a PDF over non-rigid deformations. There are no user-tuned parameters (other than initialization); all parameters are learned by maximizing the likelihood of the entire image stream. We apply our method to both rigid and non-rigid shape reconstruction, and demonstrate it in challenging cases of occlusion and variable illumination."'),
('"Automatic Reconstruction of Dendrite Morphology from Optical Section Stacks"', '"CVAMIA 2006"', '["Point Spread Function", "Medial Axis", "Dendrite Morphology", "Distance Transform", "Parseval Fram', '"https://doi.org/10.1007/11889762_17"', '"The function of the human brain arises from computations that occur within and among billions of nerve cells known as neurons. A neuron is composed primarily of a cell body (soma) from which emanates a collection of branching structures (dendrites). How neuronal signals are processed is dependent on the dendrites\\u2019 specific morphology and distribution of voltage-gated ion channels. To understand this processing, it is necessary to acquire an accurate structural analysis of the cell. Toward this end, we present an automated reconstruction system which extracts the morphology of neurons imaged from confocal and multi-photon microscopes. As we place emphasis on this being a rapid (and therefore automated) process, we have developed several techniques that provide high-quality reconstructions with minimal human interaction. In addition to generating a tree of connected cylinders representing the reconstructed neuron, a computational model is also created for purposes of performing functional simulations. We present visual and statistical results from reconstructions performed both on real image volumes and on noised synthetic data from the Duke-Southampton archive."'),
('"Automatic Registration of Large-Scale Multi-sensor Datasets"', '"ECCV 2010"', '["different sensors registration", "2D-3D matching", "LiDAR data"]', '"https://doi.org/10.1007/978-3-642-35740-4_18"', '"This paper proposes an automatic method for registering images from different sensors, particularly 2D optical sensors and 3D range sensors, without any assumption about initial alignment."'),
('"Automatic Registration of Oblique Aerial Images with Cadastral Maps"', '"ECCV 2010"', '["Aerial Image", "Building Height", "Initial Registration", "Automatic Registration", "Terrain Eleva', '"https://doi.org/10.1007/978-3-642-35740-4_20"', '"In recent years, oblique aerial images of urban regions have become increasingly popular for 3D city modeling, texturing, and various cadastral applications. In contrast to images taken vertically to the ground, they provide information on building heights, appearance of facades, and terrain elevation. Despite their widespread availability for many cities, the processing pipeline for oblique images is not fully automatic yet. Especially the process of precisely registering oblique images with map vector data can be a tedious manual process. We address this problem with a registration approach for oblique aerial images that is fully automatic and robust against discrepancies between map and image data. As input, it merely requires a cadastral map and an arbitrary number of oblique images. Besides rough initial registrations usually available from GPS/INS measurements, no further information is required, in particular no information about the terrain elevation."'),
('"Automatic Rib Segmentation in CT Data"', '"MMBIA 2004"', '["Feature Selection", "Intravenous Contrast Agent", "Sequential Forward Selection", "Local Image Str', '"https://doi.org/10.1007/978-3-540-27816-0_17"', '"A supervised method is presented for the detection and segmentation of ribs in computed tomography (ct) data. In a first stage primitives are extracted that represent parts of the centerlines of elongated structures. Each primitive is characterized by a number of features computed from local image structure. For a number of training cases, the primitives are labeled by a human observer into two classes (rib vs. non-rib). This data is used to train a classifier. Now, primitives obtained from any image can be labeled automatically. In a final stage the primitives classified as ribs are used to initialize a seeded region growing process to obtain the complete rib cage."'),
('"Automatic Segmentation of Unknown Objects, with Application to Baggage Security"', '"ECCV 2012"', '["Ground Truth", "Image Segmentation", "Gaussian Mixture Model", "Segmentation Algorithm", "Object S', '"https://doi.org/10.1007/978-3-642-33709-3_31"', '"Computed tomography (CT) is used widely to image patients for medical diagnosis and to scan baggage for threatening materials. Automated reading of these images can be used to reduce the costs of a human operator, extract quantitative information from the images or support the judgements of a human operator. Object quantification requires an image segmentation to make measurements about object size, material composition and morphology. Medical applications mostly require the segmentation of prespecified objects, such as specific organs or lesions, which allows the use of customized algorithms that take advantage of training data to provide orientation and anatomical context of the segmentation targets. In contrast, baggage screening requires the segmentation algorithm to provide segmentation of an unspecified number of objects with enormous variability in size, shape, appearance and spatial context. Furthermore, security systems demand 3D segmentation algorithms that can quickly and reliably detect threats. To address this problem, we present a segmentation algorithm for 3D CT images that makes no assumptions on the number of objects in the image or on the composition of these objects. The algorithm features a new Automatic QUality Measure (AQUA) model that measures the segmentation confidence for any single object (from any segmentation method) and uses this confidence measure to both control splitting and to optimize the segmentation parameters at runtime for each dataset. The algorithm is tested on 27 bags that were packed with a large variety of different objects."'),
('"Automatic Single-View Calibration and Rectification from Parallel Planar Curves"', '"ECCV 2014"', '["camera calibration", "projective rectification", "contour grouping", "traffic surveillance"]', '"https://doi.org/10.1007/978-3-319-10593-2_53"', '"Typical methods for camera calibration and image rectification from a single view assume the existence of straight parallel lines from which vanishing points can be computed, or orthogonal structure known to exist in the scene. However, there are practical situations where these assumptions do not apply. Moreover, from a single family of parallel lines on the ground plane there is insufficient information to recover a complete rectification. Here we study a generalization of these methods to scenes known to contain parallel curves. Our method is based on establishing an association between pairs of corresponding points lying on the image projection of these curves. We show how this method can be used to compute a least-squares estimate of the focal length and the camera pose from the tangent lines of the associated points, allowing complete rectification of the image. We evaluate the method on highway and sports track imagery, and demonstrate its accuracy relative to a state-of-the-art vanishing point method."'),
('"Automatic Tracking of a Large Number of Moving Targets in 3D"', '"ECCV 2012"', '["Stereo Match", "Automatic Tracking", "Epipolar Constraint", "Multiple Hypothesis Tracking", "Joint', '"https://doi.org/10.1007/978-3-642-33765-9_52"', '"This paper addresses the problem of tracking a large number of targets moving in 3D space using multiple calibrated video cameras. Most visual details of the targets are lost in the captured images because of limited image resolution, and the remainder can be easily corrupted due to frequent occlusion, which makes it difficult to determine both across-view and temporal correspondences. We propose a fully automatic tracking system that is capable of detecting and tracking a large number of flying targets in a 3D volume. The system includes a 3D tracking method in the framework of particle filter. Different from previous 2D tracking methods, the proposed method models the 3D attributes of targets and furthest collects weak visual information from multiple views, which makes the tracker robust against occlusion and distraction. The ambiguities in stereo matching when initializing trackers are handled by an effective multiple hypothesis generation and verification mechanism. The whole system is fully automatic in dealing with variable number of targets and robust against detection and matching errors. Our system has successfully been used by biologists to recover the 3D trajectories of hundreds of fruit flies flying freely in a 3D volume."'),
('"Autonomous Approach and Landing for a Low-Cost Quadrotor Using Monocular Cameras"', '"ECCV 2014"', '["Approach and landing", "Ellipse detection", "Conic sections", "Pose estimation", "MAV", "PTAM", "S', '"https://doi.org/10.1007/978-3-319-16178-5_14"', '"In this paper, we propose a monocular vision system for approach and landing using a low-cost micro aerial vehicle (MAV). The system enables an off-the-shelf Parrot AR.Drone 2.0 quadrotor MAV to autonomously detect a landpad, approach it, and land on it. Particularly, we exploit geometric properties of a circular landpad marker in order to estimate the exact flight distance between the quadrotor and the landing spot. We then employ monocular simultaneous localization and mapping (SLAM) to fly towards the landpad while accurately following a trajectory. Notably, our system does not require the landpad to be located directly underneath the MAV."'),
('"Avoiding Confusing Features in Place Recognition"', '"ECCV 2010"', '["Visual Word", "Query Image", "Query Expansion", "Object Retrieval", "Place Recognition"]', '"https://doi.org/10.1007/978-3-642-15549-9_54"', '"We seek to recognize the place depicted in a query image using a database of \\u201cstreet side\\u201d images annotated with geolocation information. This is a challenging task due to changes in scale, viewpoint and lighting between the query and the images in the database. One of the key problems in place recognition is the presence of objects such as trees or road markings, which frequently occur in the database and hence cause significant confusion between different places. As the main contribution, we show how to avoid features leading to confusion of particular places by using geotags attached to database images as a form of supervision. We develop a method for automatic detection of image-specific and spatially-localized groups of confusing features, and demonstrate that suppressing them significantly improves place recognition performance while reducing the database size. We show the method combines well with the state of the art bag-of-features model including query expansion, and demonstrate place recognition that generalizes over wide range of viewpoints and lighting conditions. Results are shown on a geotagged database of over 17K images of Paris downloaded from Google Street View."'),
('"Background Cut"', '"ECCV 2006"', '["Segmentation Result", "Background Image", "Color Model", "Foreground Object", "Segmentation Bounda', '"https://doi.org/10.1007/11744047_48"', '"In this paper, we introduce background cut, a high quality and real-time foreground layer extraction algorithm. From a single video sequence with a moving foreground object and stationary background, our algorithm combines background subtraction, color and contrast cues to extract a foreground layer accurately and efficiently. The key idea in background cut is background contrast attenuation, which adaptively attenuates the contrasts in the background while preserving the contrasts across foreground/background boundaries. Our algorithm builds upon a key observation that the contrast (or more precisely, color image gradient) in the background is dissimilar to the contrast across foreground/background boundaries in most cases. Using background cut, the layer extraction errors caused by background clutter can be substantially reduced. Moreover, we present an adaptive mixture model of global and per-pixel background colors to improve the robustness of our system under various background changes. Experimental results of high quality composite video demonstrate the effectiveness of our background cut algorithm."'),
('"Background Inpainting for Videos with Dynamic Objects and a Free-Moving Camera"', '"ECCV 2012"', '["video processing", "video completion", "video inpainting", "image alignment", "background estimati', '"https://doi.org/10.1007/978-3-642-33718-5_49"', '"We propose a method for removing marked dynamic objects from videos captured with a free-moving camera, so long as the objects occlude parts of the scene with a static background. Our approach takes as input a video, a mask marking the object to be removed, and a mask marking the dynamic objects to remain in the scene. To inpaint a frame, we align other candidate frames in which parts of the missing region are visible. Among these candidates, a single source is chosen to fill each pixel so that the final arrangement is color-consistent. Intensity differences between sources are smoothed using gradient domain fusion. Our frame alignment process assumes that the scene can be approximated using piecewise planar geometry: A set of homographies is estimated for each frame pair, and one each is selected for aligning pixels such that the color-discrepancy is minimized and the epipolar constraints are maintained. We provide experimental validation with several real-world video sequences to demonstrate that, unlike in previous work, inpainting videos shot with free-moving cameras does not necessarily require estimation of absolute camera positions and per-frame per-pixel depth maps."'),
('"Background Subtraction on Distributions"', '"ECCV 2008"', '["Background Subtraction", "Background Model", "Background Image", "Foreground Object", "Dynamic Tex', '"https://doi.org/10.1007/978-3-540-88690-7_21"', '"Environmental monitoring applications present a challenge to current background subtraction algorithms that analyze the temporal variability of pixel intensities, due to the complex texture and motion of the scene. They also present a challenge to segmentation algorithms that compare intensity or color distributions between the foreground and the background in each image independently, because objects of interest such as animals have adapted to blend in. Therefore, we have developed a background modeling and subtraction scheme that analyzes the temporal variation of intensity or color distributions, instead of either looking at temporal variation of point statistics, or the spatial variation of region statistics in isolation. Distributional signatures are less sensitive to movements of the textured background, and at the same time they are more robust than individual pixel statistics in detecting foreground objects. They also enable slow background update, which is crucial in monitoring applications where processing power comes at a premium, and where foreground objects, when present, may move less than the background and therefore disappear into it when a fast update scheme is used. Our approach compares favorably with the state of the art both in generic low-level detection metrics, as well as in application-dependent criteria."'),
('"Background Subtraction Using Low Rank and Group Sparsity Constraints"', '"ECCV 2012"', '["Background Subtraction", "General Rank", "Foreground Object", "Pixel Level", "Sparsity Constraint"', '"https://doi.org/10.1007/978-3-642-33718-5_44"', '"Background subtraction has been widely investigated in recent years. Most previous work has focused on stationary cameras. Recently, moving cameras have also been studied since videos from mobile devices have increased significantly. In this paper, we propose a unified and robust framework to effectively handle diverse types of videos, e.g., videos from stationary or moving cameras. Our model is inspired by two observations: 1) background motion caused by orthographic cameras lies in a low rank subspace, and 2) pixels belonging to one trajectory tend to group together. Based on these two observations, we introduce a new model using both low rank and group sparsity constraints. It is able to robustly decompose a motion trajectory matrix into foreground and background ones. After obtaining foreground and background trajectories, the information gathered on them is used to build a statistical model to further label frames at the pixel level. Extensive experiments demonstrate very competitive performance on both synthetic data and real videos."'),
('"Background Subtraction with Dirichlet Processes"', '"ECCV 2012"', '["Gaussian Mixture Model", "Background Subtraction", "Mixture Component", "Kernel Density Estimate",', '"https://doi.org/10.1007/978-3-642-33765-9_8"', '"Background subtraction is an important first step for video analysis, where it is used to discover the objects of interest for further processing. Such an algorithm often consists of a background model and a regularisation scheme. The background model determines a per-pixel measure of if a pixel belongs to the background or the foreground, whilst the regularisation brings in information from adjacent pixels. A new method is presented that uses a Dirichlet process Gaussian mixture model to estimate a per-pixel background distribution, which is followed by probabilistic regularisation. Key advantages include inferring the per-pixel mode count, such that it accurately models dynamic backgrounds, and that it updates its model continuously in a principled way."'),
('"Backprojection Revisited: Scalable Multi-view Object Detection and Similarity Metrics for Detection', '"ECCV 2010"', '["Training Image", "Object Detection", "Image Domain", "Object Hypothesis", "Vote Space"]', '"https://doi.org/10.1007/978-3-642-15549-9_45"', '"Hough transform based object detectors learn a mapping from the image domain to a Hough voting space. Within this space, object hypotheses are formed by local maxima. The votes contributing to a hypothesis are called support. In this work, we investigate the use of the support and its backprojection to the image domain for multi-view object detection. To this end, we create a shared codebook with training and matching complexities independent of the number of quantized views. We show that since backprojection encodes enough information about the viewpoint all views can be handled together. In our experiments, we demonstrate that superior accuracy and efficiency can be achieved in comparison to the popular one-vs-the-rest detectors by treating views jointly especially with few training examples and no view annotations. Furthermore, we go beyond the detection case and based on the support we introduce a part-based similarity measure between two arbitrary detections which naturally takes spatial relationships of parts into account and is insensitive to partial occlusions. We also show that backprojection can be used to efficiently measure the similarity of a detection to all training examples. Finally, we demonstrate how these metrics can be used to estimate continuous object parameters like human pose and object\\u2019s viewpoint. In our experiment, we achieve state-of-the-art performance for view-classification on the PASCAL VOC\\u201906 dataset."'),
('"Balanced Exploration and Exploitation Model Search for Efficient Epipolar Geometry Estimation"', '"ECCV 2006"', '["Fundamental Matrix", "Singularity Constraint", "Inlier Rate", "Point Correspondence", "Sift Descri', '"https://doi.org/10.1007/11744047_12"', '"The estimation of the epipolar geometry is especially difficult where the putative correspondences include a low percentage of inlier correspondences and/or a large subset of the inliers is consistent with a degenerate configuration of the epipolar geometry that is totally incorrect. This work presents the Balanced Exploration and Exploitation Model Search (BEEM) algorithm that works very well especially for these difficult scenes."'),
('"Balanced Recovery of 3D Structure and Camera Motion from Uncalibrated Image Sequences"', '"ECCV 2002"', '["Camera Motion", "Camera Parameter", "Projection Matrice", "Bundle Adjustment", "Total Little Squar', '"https://doi.org/10.1007/3-540-47967-8_20"', '"Metric reconstruction of a scene viewed by an uncalibrated camera undergoing an unknown motion is a fundamental task in computer vision. To obtain accurate results all the methods rely on bundle adjustment, a nonlinear optimization technique which minimizes the reprojection error over the structural and camera parameters. Bundle adjustment is optimal for normally distributed measurement noise, however, its performance depends on the starting point. The initial solution is usually obtained by solving a linearized constraint through a total least squares procedure, which yields a biased estimate. We present a more balanced approach where in main computational modules of an uncalibrated reconstruction system, the initial solution is obtained from a statistically justified estimator which assures its unbiasedness. Since the quality of the new initial solution is already comparable with that of the result of bundle adjustment, the burden on the latter is drastically reduced while its reliability is significantly increased. The performance of our system was assessed for both synthetic data and standard image sequences."'),
('"Balancing Deformability and Discriminability for Shape Matching"', '"ECCV 2010"', '["Pattern Anal", "Geodesic Distance", "Scale Invariant Feature Transform", "Shape Descriptor", "Retr', '"https://doi.org/10.1007/978-3-642-15558-1_30"', '"We propose a novel framework, aspect space, to balance deformability and discriminability, which are often two competing factors in shape and image representations. In this framework, an object is embedded as a surface in a higher dimensional space with a parameter named aspect weight, which controls the importance of intensity in the embedding. We show that this framework naturally unifies existing important shape and image representations by adjusting the aspect weight and the embedding. More importantly, we find that the aspect weight implicitly controls the degree to which a representation handles deformation. Based on this idea, we present the aspect shape context, which extends shape context-based descriptors and adaptively selects the \\u201cbest\\u201d aspect weight for shape comparison. Another observation we have is the proposed descriptor nicely fits context-sensitive shape retrieval. The proposed methods are evaluated on two public datasets, MPEG7-CE-Shape-1 and Tari 1000, in comparison to state-of-the-art solutions. In the standard shape retrieval experiment using the MPEG7 CE-Shape-1 database, the new descriptor with context information achieves a bull\\u2019s eye score of 95.96%, which surpassed all previous results. In the Tari 1000 dataset, our methods significantly outperform previous tested methods as well."'),
('"Barcode Imaging Using a Light Field Camera"', '"ECCV 2014"', '["Light field camera", "Barcode imaging", "Spatial frequency"]', '"https://doi.org/10.1007/978-3-319-16181-5_40"', '"We present a method to capture sharp barcode images, using a microlens-based light field camera. Relative to standard barcode readers, which typically use fixed-focus cameras in order to reduce mechanical complexity and shutter lag, employing a light field camera significantly increases the scanner\\u2019s depth of field. However, the increased computational complexity that comes with software-based focusing is a major limitation on these approaches. Whereas traditional light field rendering involves time-consuming steps intended to produce a focus stack in which all objects appear sharply-focused, a scanner only needs to produce an image of the barcode region that falls within the decoder\\u2019s inherent robustness to defocus. With this in mind, we speed up image processing by segmenting the barcode region before refocus is applied. We then estimate the barcode\\u2019s depth directly from the raw sensor image, using a lookup table characterizing a relationship between depth and the code\\u2019s spatial frequency. Real image experiments with the Lytro camera illustrate that our system can produce a decodable image with a fraction of the computational complexity."'),
('"Base Materials for Photometric Stereo"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33868-7_35"', '"Image-based capture of material appearance has been extensively studied, but the quality of the results and generality of the applied methods leave a lot of room for improvement. Most existing methods rely on parametric models of reflectance and require complex hardware systems or accurate geometric models that are not always available or practical. Rather than independently estimating reflectance properties for each surface point, it is common to express the reflectance as a combination of base materials inherent to each particular object or scene."'),
('"Bayesian Blind Deconvolution with General Sparse Image Priors"', '"ECCV 2012"', '["Blind Deconvolution", "Image Prior", "Blur Kernel", "Unknown Image", "Blurry Image"]', '"https://doi.org/10.1007/978-3-642-33783-3_25"', '"We present a general method for blind image deconvolution using Bayesian inference with super-Gaussian sparse image priors. We consider a large family of priors suitable for modeling natural images, and develop the general procedure for estimating the unknown image and the blur. Our formulation includes a number of existing modeling and inference methods as special cases while providing additional flexibility in image modeling and algorithm design. We also present an analysis of the proposed inference compared to other methods and discuss its advantages. Theoretical and experimental results demonstrate that the proposed formulation is very effective, efficient, and flexible."'),
('"Bayesian Correction of Image Intensity with Spatial Consideration"', '"ECCV 2004"', '["Color Space", "Bayesian Framework", "Histogram Equalization", "Color Statistic", "Motion Blur"]', '"https://doi.org/10.1007/978-3-540-24672-5_27"', '"Under dimly lit condition, it is difficult to take a satisfactory image in long exposure time with a hand-held camera. Despite the use of a tripod, moving objects in the scene still generate ghosting and blurring effect. In this paper, we propose a novel approach to recover a high-quality image by exploiting the tradeoff between exposure time and motion blur, which considers color statistics and spatial constraints simultaneously, by using only two defective input images. A Bayesian framework is adopted to incorporate the factors to generate an optimal color mapping function. No estimation of PSF is performed. Our new approach can be readily extended to handle high contrast scenes to reveal fine details in saturated or highlight regions. An image acquisition system deploying off-the-shelf digital cameras and camera control softwares was built. We present our results on a variety of defective images: global and local motion blur due to camera shake or object movement, and saturation due to high contrast scenes."'),
('"Bayesian Estimation of Layers from Multiple Images"', '"ECCV 2002"', '["Ground Truth", "Bayesian Estimation", "Multiple Image", "Virtual Object", "Foreground Object"]', '"https://doi.org/10.1007/3-540-47977-5_32"', '"When estimating foreground and background layers (or equivalently an alpha matte), it is often the case that pixel measurements contain mixed colours which are a combination of foreground and background. Object boundaries, especially at thin sub-pixel structures like hair, pose a serious problem."'),
('"Bayesian Face Revisited: A Joint Formulation"', '"ECCV 2012"', '["Face Recognition", "Linear Discriminant Analysis", "Mahalanobis Distance", "Discriminative Informa', '"https://doi.org/10.1007/978-3-642-33712-3_41"', '"In this paper, we revisit the classical Bayesian face recognition method by Baback Moghaddam et al. and propose a new joint formulation. The classical Bayesian method models the appearance difference between two faces. We observe that this \\u201cdifference\\u201d formulation may reduce the separability between classes. Instead, we model two faces jointly with an appropriate prior on the face representation. Our joint formulation leads to an EM-like model learning at the training time and an efficient, closed-formed computation at the test time. On extensive experimental evaluations, our method is superior to the classical Bayesian face and many other supervised approaches. Our method achieved 92.4% test accuracy on the challenging Labeled Face in Wild (LFW) dataset. Comparing with current best commercial system, we reduced the error rate by 10%."'),
('"Bayesian Multimodal Fusion in Forensic Applications"', '"ECCV 2012"', '["Support Vector Machine", "Bayesian Network", "Surveillance Video", "Fusion Technique", "Partial De', '"https://doi.org/10.1007/978-3-642-33885-4_47"', '"The public location of CCTV cameras and their connexion with public safety demand high robustness and reliability from surveillance systems. This paper focuses on the development of a multimodal fusion technique which exploits the benefits of a Bayesian inference scheme to enhance surveillance systems\\u2019 reliability. Additionally, an automatic object classifier is proposed based on the multimodal fusion technique, addressing semantic indexing and classification for forensic applications. The proposed Bayesian-based Multimodal Fusion technique, and particularly, the proposed object classifier are evaluated against two state-of-the-art automatic object classifiers on the i-LIDS surveillance dataset."'),
('"Bayesian Nonparametric Intrinsic Image Decomposition"', '"ECCV 2014"', '["Intrinsic images", "Dirichlet process", "Gaussian process", "MCMC"]', '"https://doi.org/10.1007/978-3-319-10593-2_46"', '"We present a generative, probabilistic model that decomposes an image into reflectance and shading components. The proposed approach uses a Dirichlet process Gaussian mixture model where the mean parameters evolve jointly according to a Gaussian process. In contrast to prior methods, we eliminate the Retinex term and adopt more general smoothness assumptions for the shading image. Markov chain Monte Carlo sampling techniques are used for inference, yielding state-of-the-art results on the MIT Intrinsic Image Dataset."'),
('"Bayesian Self-Calibration of a Moving Camera"', '"ECCV 2002"', '["Posterior Distribution", "Camera Motion", "Motion Sequence", "Intrinsic Parameter", "Structure Fro', '"https://doi.org/10.1007/3-540-47967-8_19"', '"In this paper, a Bayesian self-calibration approach is proposed using sequential importance sampling (SIS). Given a set of feature correspondences tracked through an image sequence, the joint posterior distributions of both camera extrinsic and intrinsic parameters as well as the scene structure are approximated by a set of samples and their corresponding weights. The critical motion sequences are explicitly considered in the design of the algorithm. The probability of the existence of the critical motion sequence is inferred from the sample and weight set obtained from the SIS procedure. No initial guess for the calibration parameters is required. The proposed approach has been extensively tested on both synthetic and real image sequences and satisfactory performance has been observed."'),
('"Bayesian Tracking with Auxiliary Discrete Processes. Application to Detection and Tracking of Objec', '"WDV 2006"', '["Auxiliary Variable", "Observation Model", "Visual Tracking", "Proposal Distribution", "Sequential ', '"https://doi.org/10.1007/978-3-540-70932-9_15"', '"A number of Bayesian tracking models involve auxiliary discrete variables beside the main hidden state of interest. These discrete variables usually follow a Markovian process and interact with the hidden state either via its evolution model or via the observation process, or both. We consider here a general model that encompasses all these situations, and show how Bayesian filtering can be rigorously conducted with it. The resulting approach facilitates easy re-use of existing tracking algorithms designed in the absence of the auxiliary process. In particular we show how particle filters can be obtained based on sampling only in the original state space instead of sampling in the augmented space, as it is usually done. We finally demonstrate how this framework facilitates solutions to the critical problem of appearance and disappearance of targets, either upon scene entering and exiting, or due to temporary occlusions. This is illustrated in the context of color-based tracking with particle filters."'),
('"Behind the Depth Uncertainty: Resolving Ordinal Depth in SFM"', '"ECCV 2008"', '["Motion Estimate", "Image Point", "Motion Error", "Discrimination Threshold", "Depth Estimate"]', '"https://doi.org/10.1007/978-3-540-88690-7_25"', '"Structure from Motion(SFM) is beset by the noise sensitivity problem. Previous works show that some motion ambiguities are inherent and errors in the motion estimates are inevitable. These errors may render accurate metric depth estimate difficult to obtain. However, can we still extract some valid and useful depth information from the inaccurate metric depth estimates? In this paper, the resolution of ordinal depth extracted from the inaccurate metric depth is investigated. Based on a general depth distortion model, a sufficient condition is derived for ordinal depth to be extracted validly. By studying the geometry and statistics of the image regions satisfying this condition, we found that although metric depth estimates are inaccurate, ordinal depth can still be discerned locally if physical metric depth difference is beyond certain discrimination threshold. The resolution level of discernible ordinal depth decreases as the visual angle subtended by the points increases, as the speed of the motion carrying the depth information decreases, and as points recede from the camera. These findings suggest that accurate knowledge of qualitative 3D structure is ensured in a small local image neighborhood, which might account for biological foveated vision and shed light on the nature of the perceived visual space."'),
('"Being John Malkovich"', '"ECCV 2010"', '["Image Retrieval", "Local Binary Pattern", "Target Image", "Target Face", "Mouth Region"]', '"https://doi.org/10.1007/978-3-642-15549-9_25"', '"Given a photo of person A, we seek a photo of person B with similar pose and expression. Solving this problem enables a form of puppetry, in which one person appears to control the face of another. When deployed on a webcam-equipped computer, our approach enables a user to control another person\\u2019s face in real-time. This image-retrieval-inspired approach employs a fully-automated pipeline of face analysis techniques, and is extremely general\\u2014we can puppet anyone directly from their photo collection or videos in which they appear. We show several examples using images and videos of celebrities from the Internet."'),
('"Belief Propagation with Directional Statistics for Solving the Shape-from-Shading Problem"', '"ECCV 2008"', '["Ground Truth", "Belief Propagation", "Directional Statistics", "Fisher Distribution", "Smoothness ', '"https://doi.org/10.1007/978-3-540-88690-7_58"', '"The Shape-from-Shading [SfS] problem infers shape from reflected light, collected using a camera at a single point in space only. Reflected light alone does not provide sufficient constraint and extra information is required; typically a smoothness assumption is made. A surface with Lambertian reflectance lit by a single infinitely distant light source is also typical."'),
('"Benchmarking of Fingerprint Sensors"', '"BioAW 2004"', '["Quality Score", "Skin Type", "Fingerprint Image", "Usable Range", "Image Quality Score"]', '"https://doi.org/10.1007/978-3-540-25976-3_9"', '"At present, there are many competing fingerprint sensors available. Thus, fingerprint sensor benchmarking is necessary but unfortunately no proper methodology is available. This paper attempts to address this deficiency by proposing a new methodology to benchmark the fingerprint sensors. The methodology consists of three metrics and the associated procedures to collect the data in order to compute the proposed metrics. Two small scale experiments are conducted to show the validity and efficacy of the proposed method. These include comparison of the image acquisition performance among various sensors with different skin type and under different weather condition. The effect of number of usage with acquisition performance is also analyzed. Analysis of the results shows that the proposed method does provide a basic sensor benchmarking capability."'),
('"Beyond Bounding-Boxes: Learning Object Shape by Model-Driven Grouping"', '"ECCV 2012"', '["Object Detection", "Average Precision", "Object Shape", "Multiple Instance Learn", "Mercer Kernel"', '"https://doi.org/10.1007/978-3-642-33712-3_42"', '"Visual recognition requires to learn object models from training data. Commonly, training samples are annotated by marking only the bounding-box of objects, since this appears to be the best trade-off between labeling information and effectiveness. However, objects are typically not box-shaped. Thus, the usual parametrization of object hypotheses by only their location, scale and aspect ratio seems inappropriate since the box contains a significant amount of background clutter. Most important, however, is that object shape becomes only explicit once objects are segregated from the background. Segmentation is an ill-posed problem and so we propose an approach for learning object models for detection while, simultaneously, learning to segregate objects from clutter and extracting their overall shape. For this purpose, we exclusively use bounding-box annotated training data. The approach groups fragmented object regions using the Multiple Instance Learning (MIL) framework to obtain a meaningful representation of object shape which, at the same time, crops away distracting background clutter to improve the appearance representation."'),
('"Beyond Feature Points: Structured Prediction for Monocular Non-rigid 3D Reconstruction"', '"ECCV 2012"', '["Feature Point", "Structure Prediction", "Mesh Vertex", "Reprojection Error", "Pairwise Potential"]', '"https://doi.org/10.1007/978-3-642-33765-9_18"', '"Existing approaches to non-rigid 3D reconstruction either are specifically designed for feature point correspondences, or require a good shape initialization to exploit more complex image likelihoods. In this paper, we formulate reconstruction as inference in a graphical model, where the variables encode the rotations and translations of the facets of a surface mesh. This lets us exploit complex likelihoods even in the absence of a good initialization. In contrast to existing approaches that set the weights of the likelihood terms manually, our formulation allows us to learn them from as few as a single training example. To improve efficiency, we combine our structured prediction formalism with a gradient-based scheme. Our experiments show that our approach yields tremendous improvement over state-of-the-art gradient-based methods."'),
('"Beyond Loose LP-Relaxations: Optimizing MRFs by Repairing Cycles"', '"ECCV 2008"', '["Dual Solution", "Anchor Node", "Linear Programming Relaxation", "Tight Link", "Dual Objective"]', '"https://doi.org/10.1007/978-3-540-88690-7_60"', '"This paper presents a new MRF optimization algorithm, which is derived from Linear Programming and manages to go beyond current state-of-the-art techniques (such as those based on graph-cuts or belief propagation). It does so by relying on a much tighter class of LP-relaxations, called cycle-relaxations. With the help of this class of relaxations, our algorithm tries to deal with a difficulty lying at the heart of MRF optimization: the existence of inconsistent cycles. To this end, it uses an operation called cycle-repairing. The goal of that operation is to fix any inconsistent cycles that may appear during optimization, instead of simply ignoring them as usually done up to now. The more the repaired cycles, the tighter the underlying LP relaxation becomes. As a result of this procedure, our algorithm is capable of providing almost optimal solutions even for very general MRFs with arbitrary potentials. Experimental results verify its effectiveness on difficult MRF problems, as well as its better performance compared to the state of the art."'),
('"Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers"', '"ECCV 2008"', '["Image Region", "Image Annotation", "Correspondence Problem", "Binary Relationship", "Automatic Ima', '"https://doi.org/10.1007/978-3-540-88682-2_3"', '"Learning visual classifiers for object recognition from weakly labeled data requires determining correspondence between image regions and semantic object classes. Most approaches use co-occurrence of \\u201cnouns\\u201d and image features over large datasets to determine the correspondence, but many correspondence ambiguities remain. We further constrain the correspondence problem by exploiting additional language constructs to improve the learning process from weakly labeled data. We consider both \\u201cprepositions\\u201d and \\u201ccomparative adjectives\\u201d which are used to express relationships between objects. If the models of such relationships can be determined, they help resolve correspondence ambiguities. However, learning models of these relationships requires solving the correspondence problem. We simultaneously learn the visual features defining \\u201cnouns\\u201d and the differential visual features defining such \\u201cbinary-relationships\\u201d using an EM-based approach."'),
('"Beyond Spatial Pyramids: A New Feature Extraction Framework with Dense Spatial Sampling for Image C', '"ECCV 2012"', '["Image Classification", "Spatial Pyramid", "Sliding Window", "Multiple Kernel Learning", "Adapted C', '"https://doi.org/10.1007/978-3-642-33765-9_34"', '"We introduce a new framework for image classification that extends beyond the window sampling of fixed spatial pyramids to include a comprehensive set of windows densely sampled over location, size and aspect ratio. To effectively deal with this large set of windows, we derive a concise high-level image feature using a two-level extraction method. At the first level, window-based features are computed from local descriptors (e.g., SIFT, spatial HOG, LBP) in a process similar to standard feature extractors. Then at the second level, the new image feature is determined from the window-based features in a manner analogous to the first level. This higher level of abstraction offers both efficient handling of dense samples and reduced sensitivity to misalignment. More importantly, our simple yet effective framework can readily accommodate a large number of existing pooling/coding methods, allowing them to extract features beyond the spatial pyramid representation."'),
('"Beyond the Line of Sight: Labeling the Underlying Surfaces"', '"ECCV 2012"', '["Training Image", "Query Image", "Foreground Object", "Visible Surface", "Occlude Region"]', '"https://doi.org/10.1007/978-3-642-33715-4_55"', '"Scene understanding requires reasoning about both what we can see and what is occluded. We offer a simple and general approach to infer labels of occluded background regions. Our approach incorporates estimates of visible surrounding background, detected objects, and shape priors from transferred training regions. We demonstrate the ability to infer the labels of occluded background regions in both the outdoor StreetScenes dataset and an indoor scene dataset using the same approach. Our experiments show that our method outperforms competent baselines."'),
('"Bi-affinity Filter: A Bilateral Type Filter for Color Images"', '"ECCV 2010"', '["Bilateral filter", "RGB color filtering", "image matting", "matting Laplacian"]', '"https://doi.org/10.1007/978-3-642-35740-4_3"', '"We propose a new filter called Bi-affinity filter for color images. This filter is similar in structure to the bilateral filter. The proposed filter is based on the color line model, which does not require the explicit conversion of the RGB values to perception based spaces such as CIELAB. The bi-affinity filter measures the affinity of a pixel to a small neighborhood around it and weighs the filter term accordingly. We show that this method can perform at par with standard bilateral filters for color images. The small edges of the image are usually enhanced leading to a very easy image enhancement filter."'),
('"Bias in Shape Estimation"', '"ECCV 2004"', '["Sensor Noise", "Psychophysical Experiment", "Shape Estimation", "Camera Center", "Texture Plane"]', '"https://doi.org/10.1007/978-3-540-24672-5_32"', '"This paper analyses the uncertainty in the estimation of shape from motion and stereo. It is shown that there are computational limitations of a statistical nature that previously have not been recognized. Because there is noise in all the input parameters, we cannot avoid bias. The analysis rests on a new constraint which relates image lines and rotation to shape. Because the human visual system has to cope with bias as well, it makes errors. This explains the underestimation of slant found in computational and psychophysical experiments, and demonstrated here for an illusory display. We discuss properties of the best known estimators with regard to the problem, as well as possible avenues for visual systems to deal with the bias."'),
('"Bias in the Localization of Curved Edges"', '"ECCV 2004"', '["Edge Detection", "Lookup Table", "Smoothing Kernel", "Edge Localization", "Canny Edge Detector"]', '"https://doi.org/10.1007/978-3-540-24671-8_44"', '"This paper presents a theoretical and experimental analysis of the bias in the localization of edges detected from the zeros of the second derivative of the image in the direction of its gradient, such as the Canny edge detector. Its contributions over previous art are: a quantification of the localization bias as a function of the scale \\u03c3 of the smoothing filter and the radius of curvature R of the edge, which unifies, without any approximation, previous results that independently studied the case of R\\u226b\\u03c3 or \\u03c3\\u226b R; the determination of an optimal scale at which edge curvature can be accurately recovered for circular objects; and a technique to compensate for the localization bias which can be easily incorporated into existing algorithms for edge detection. The theoretical results are validated by experiments with synthetic data, and the bias correction algorithm introduced here is reduced to practice on real images."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Bidirectional Texture Contrast Function"', '"ECCV 2002"', '["Texture", "shading and colour", "Image features", "Surface geometry", "Inverse rendering"]', '"https://doi.org/10.1007/3-540-47979-1_54"', '"We consider image texture due to the illumination of 3D surface corrugations on globally smooth curved surfaces. The same surface corrugations give rise to different image textures depending on illumination and viewing geometries. We study surfaces that are on the average approximately Lambertian. The surface roughness gives rise to luminance modulations of the global shading pattern. The extreme values of the luminance depend on simple geometrical factors such as whether surface micro facets exist that squarely face the light source or are in shadow. We find that a simple microfacet-based model suffices to describe textures in natural scenes robustly in a semi-quantitative manner. Robust statistical measures of the texture yield the parameters for simple models that allow prediction of the BRDF. Thus texture analysis allows the input parameters for inverse rendering and material recognition to be estimated."'),
('"Bilateral Filtering-Based Optical Flow Estimation with Occlusion Detection"', '"ECCV 2006"', '["Gaussian Kernel", "Anisotropic Diffusion", "Input Frame", "Occlude Region", "Smoothness Term"]', '"https://doi.org/10.1007/11744023_17"', '"Using the variational approaches to estimate optical flow between two frames, the flow discontinuities between different motion fields are usually not distinguished even when an anisotropic diffusion operator is applied. In this paper, we propose a multi-cue driven adaptive bilateral filter to regularize the flow computation, which is able to achieve the smoothly varied optical flow field with highly desirable motion discontinuities. First, we separate the traditional one-step variational updating model into a two-step filtering-based updating model. Then, employing our occlusion detector, we reformulate the energy functional of optical flow estimation by explicitly introducing an occlusion term to balance the energy loss due to the occlusion or mismatches. Furthermore, based on the two-step updating framework, a novel multi-cue driven bilateral filter is proposed to substitute the original anisotropic diffusion process, and it is able to adaptively control the diffusion process according to the occlusion detection, image intensity dissimilarity, and motion dissimilarity. After applying our approach on various video sources (movie and TV) in the presence of occlusion, motion blurring, non-rigid deformation, and weak textureness, we generate a spatial-coherent flow field between each pair of input frames and detect more accurate flow discontinuities along the motion boundaries."'),
('"Bilateral Functions for Global Motion Modeling"', '"ECCV 2014"', '["Image Pair", "Outlier Removal", "Bilateral Model", "Motion Coherence", "Feature Correspondence"]', '"https://doi.org/10.1007/978-3-319-10593-2_23"', '"This paper proposes modeling motion in a bilateral domain that augments spatial information with the motion itself. We use the bilateral domain to reformulate a piecewise smooth constraint as continuous global modeling constraint. The resultant model can be robustly computed from highly noisy scattered feature points using a global minimization. We demonstrate how the model can reliably obtain large numbers of good quality correspondences over wide baselines, while keeping outliers to a minimum."'),
('"Bilinear Factorization via Augmented Lagrange Multipliers"', '"ECCV 2010"', '["Constrain Optimization Problem", "Structure From Motion", "Photometric Stereo", "Augmented Lagrang', '"https://doi.org/10.1007/978-3-642-15561-1_21"', '"This paper presents a unified approach to solve different bilinear factorization problems in Computer Vision in the presence of missing data in the measurements. The problem is formulated as a constrained optimization problem where one of the factors is constrained to lie on a specific manifold. To achieve this, we introduce an equivalent reformulation of the bilinear factorization problem. This reformulation decouples the core bilinear aspect from the manifold specificity. We then tackle the resulting constrained optimization problem with Bilinear factorization via Augmented Lagrange Multipliers (BALM). The mechanics of our algorithm are such that only a projector onto the manifold constraint is needed. That is the strength and the novelty of our approach: it can handle seamlessly different Computer Vision problems. We present experiments and results for two popular factorization problems: Non-rigid Structure from Motion and Photometric Stereo."'),
('"Bilinear Kernel Reduced Rank Regression for Facial Expression Synthesis"', '"ECCV 2010"', '["Facial Expression", "Neutral Face", "Facial Animation", "Alternate Little Square", "Ground Truth I', '"https://doi.org/10.1007/978-3-642-15552-9_27"', '"In the last few years, Facial Expression Synthesis (FES) has been a flourishing area of research driven by applications in character animation, computer games, and human computer interaction. This paper proposes a photo-realistic FES method based on Bilinear Kernel Reduced Rank Regression (BKRRR). BKRRR learns a high-dimensional mapping between the appearance of a neutral face and a variety of expressions (e.g. smile, surprise, squint). There are two main contributions in this paper: (1) Propose BKRRR for FES. Several algorithms for learning the parameters of BKRRR are evaluated. (2) Propose a new method to preserve subtle person-specific facial characteristics (e.g. wrinkles, pimples). Experimental results on the CMU Multi-PIE database and pictures taken with a regular camera show the effectiveness of our approach."'),
('"Binary Codes Embedding for Fast Image Tagging with Incomplete Labels"', '"ECCV 2014"', '["Image Tagging", "Binary Codes", "Hashing"]', '"https://doi.org/10.1007/978-3-319-10605-2_28"', '"Tags have been popularly utilized for better annotating, organizing and searching for desirable images. Image tagging is the problem of automatically assigning tags to images. One major challenge for image tagging is that the existing/training labels associated with image examples might be incomplete and noisy. Valuable prior work has focused on improving the accuracy of the assigned tags, but very limited work tackles the efficiency issue in image tagging, which is a critical problem in many large scale real world applications. This paper proposes a novel Binary Codes Embedding approach for Fast Image Tagging (BCE-FIT) with incomplete labels. In particular, we construct compact binary codes for both image examples and tags such that the observed tags are consistent with the constructed binary codes. We then formulate the problem of learning binary codes as a discrete optimization problem. An efficient iterative method is developed to solve the relaxation problem, followed by a novel binarization method based on orthogonal transformation to obtain the binary codes from the relaxed solution. Experimental results on two large scale datasets demonstrate that the proposed approach can achieve similar accuracy with state-of-the-art methods while using much less time, which is important for large scale applications."'),
('"Binary Coherent Edge Descriptors"', '"ECCV 2010"', '["Principal Component Analysis", "Image Patch", "Equal Error Rate", "Gradient Magnitude", "Jaccard S', '"https://doi.org/10.1007/978-3-642-15552-9_13"', '"Patch descriptors are used for a variety of tasks ranging from finding corresponding points across images, to describing object category parts. In this paper, we propose an image patch descriptor based on edge position, orientation and local linear length. Unlike previous works using histograms of gradients, our descriptor does not encode relative gradient magnitudes. Our approach locally normalizes the patch gradients to remove relative gradient information, followed by orientation dependent binning. Finally, the edge histogram is binarized to encode edge locations, orientations and lengths. Two additional extensions are proposed for fast PCA dimensionality reduction, and a min-hash approach for fast patch retrieval. Our algorithm produces state-of-the-art results on previously published object instance patch data sets, as well as a new patch data set modeling intra-category appearance variations."'),
('"Binocular Self-Alignment and Calibration from Planar Scenes"', '"ECCV 2000"', '["Ground Plane", "Projective Structure", "Bundle Adjustment", "Invariant Line", "Real Scene"]', '"https://doi.org/10.1007/3-540-45053-X_30"', '"We consider the problem of aligning and calibrating a binocular pan-tilt device using visual information from controlled motions, while viewing a degenerate (planar) scene."'),
('"Biometric Face Authentication Using Pixel-Based Weak Classifiers"', '"BioAW 2004"', '["Face Image", "Face Detection", "Authentication System", "Identity Authentication", "Benchmark Data', '"https://doi.org/10.1007/978-3-540-25976-3_3"', '"The performance of face authentication systems has steadily improved over the last few years. State-of-the-art methods use the projection of the gray-scale face image into a Linear Discriminant subspace as input of a classifier such as Support Vector Machines or Multi-layer Perceptrons. Unfortunately, these classifiers involve thousands of parameters that are difficult to store on a smart-card for instance. Recently, boosting algorithms has emerged to boost the performance of simple (weak) classifiers by combining them iteratively. The famous AdaBoost algorithm have been proposed for object detection and applied successfully to face detection. In this paper, we investigate the use of AdaBoost for face authentication to boost weak classifiers based simply on pixel values. The proposed approach is tested on a benchmark database, namely XM2VTS. Results show that boosting only hundreds of classifiers achieved near state-of-the-art results. Furthermore, the proposed approach outperforms similar work on face authentication using boosting algorithms on the same database."'),
('"Biometric Identification in Forensic Cases According to the Bayesian Approach"', '"BioAW 2002"', '["Bayesian Approach", "Equal Error Rate", "Speaker Recognition", "Biometric System", "Speaker Verifi', '"https://doi.org/10.1007/3-540-47917-1_18"', '"On the one hand, commercial biometric systems and forensic identification require different approaches in order to evaluate system outputs. On the other hand, bayesian approach for evidence analysis and forensic reporting perfectly suits the needs of the court and the forensic scientist. Inside this bayesian framework, any biometric system can be adapted to provide its results in the form of likelihood ratios (LR) (being so converted in a forensic identification system), and performance of the forensic system can be then assessed according to the bayesian approach. We will focus on a specific biometric characteristic, showing how forensic speaker recognition can be reported by means of bayesian technique. Results including NIST-Ahumada and providing LR scores in the form of Tippet plots (and compared with DET plots) will be finally presented."'),
('"Biometric Sensor Interoperability: A Case Study in Fingerprints"', '"BioAW 2004"', '["Optical Sensor", "Speaker Recognition", "Biometric System", "Fingerprint Image", "Biometric Trait"', '"https://doi.org/10.1007/978-3-540-25976-3_13"', '"The problem of biometric sensor interoperability has received limited attention in the literature. Most biometric systems operate under the assumption that the data (viz., images) to be compared are obtained using the same sensor and, hence, are restricted in their ability to match or compare biometric data originating from different sensors. Although progress has been made in the development of common data exchange formats to facilitate the exchange of feature sets between vendors, very little effort has been invested in the actual development of algorithms and techniques to match these feature sets. In the Fingerprint Verification Competition (FVC 2002), for example, the evaluation protocol only matched images originating from the same sensor although fingerprint data from 3 different commercial sensors was available. This is an indication of the difficulty in accommodating sensor interoperability in biometric systems. In this paper we discuss this problem and present a case study involving two different fingerprint sensors."'),
('"Blind Correction of Optical Aberrations"', '"ECCV 2012"', '["Graphic Processing Unit", "Point Spread Function", "Chromatic Aberration", "Blind Deconvolution", ', '"https://doi.org/10.1007/978-3-642-33712-3_14"', '"Camera lenses are a critical component of optical imaging systems, and lens imperfections compromise image quality. While traditionally, sophisticated lens design and quality control aim at limiting optical aberrations, recent works [1,2,3] promote the correction of optical flaws by computational means. These approaches rely on elaborate measurement procedures to characterize an optical system, and perform image correction by non-blind deconvolution."'),
('"Blind Deblurring Using Internal Patch Recurrence"', '"ECCV 2014"', '["Blind deblurring", "blind deconvolution", "blur kernel estimation", "internal patch recurrence", "', '"https://doi.org/10.1007/978-3-319-10578-9_51"', '"Recurrence of small image patches across different scales of a natural image has been previously used for solving ill-posed problems (e.g. super- resolution from a single image). In this paper we show how this multi-scale property can also be used for \\u201cblind-deblurring\\u201d, namely, removal of an unknown blur from a blurry image. While patches repeat \\u2018as is\\u2019 across scales in a sharp natural image, this cross-scale recurrence significantly diminishes in blurry images. We exploit these deviations from ideal patch recurrence as a cue for recovering the underlying (unknown) blur kernel. More specifically, we look for the blur kernel k, such that if its effect is \\u201cundone\\u201d (if the blurry image is deconvolved with k), the patch similarity across scales of the image will be maximized. We report extensive experimental evaluations, which indicate that our approach compares favorably to state-of-the-art blind deblurring methods, and in particular, is more robust than them."'),
('"Blind Reflectometry"', '"ECCV 2010"', '["Ground Truth", "Independent Component Analysis", "Color Constancy", "High Dynamic Range Image", "E', '"https://doi.org/10.1007/978-3-642-15549-9_4"', '"We address the problem of inferring homogeneous reflectance (BRDF) from a single image of a known shape in an unknown real-world lighting environment. With appropriate representations of lighting and reflectance, the image provides bilinear constraints on the two signals, and our task is to blindly isolate the latter. We achieve this by leveraging the statistics of real-world illumination and estimating the reflectance that is most likely under a distribution of probable illumination environments. Experimental results with a variety of real and synthetic images suggest that useable reflectance information can be inferred in many cases, and that these estimates are stable under changes in lighting."'),
('"Blind Vision"', '"ECCV 2006"', '["Hash Function", "Face Detection", "Secure Protocol", "Detection Window", "Vision Algorithm"]', '"https://doi.org/10.1007/11744078_1"', '"Alice would like to detect faces in a collection of sensitive surveillance images she own. Bob has a face detection algorithm that he is willing to let Alice use, for a fee, as long as she learns nothing about his detector. Alice is willing to use Bob\\u2019s detector provided that he will learn nothing about her images, not even the result of the face detection operation. Blind vision is about applying secure multi-party techniques to vision algorithms so that Bob will learn nothing about the images he operates on, not even the result of his own operation and Alice will learn nothing about the detector. The proliferation of surveillance cameras raises privacy concerns that can be addressed by secure multi-party techniques and their adaptation to vision algorithms."'),
('"Block-Sparse RPCA for Consistent Foreground Detection"', '"ECCV 2012"', '["Illumination Change", "Foreground Object", "Dynamic Background", "Foreground Region", "Move Object', '"https://doi.org/10.1007/978-3-642-33715-4_50"', '"Recent evaluation of representative background subtraction techniques demonstrated the drawbacks of these methods, with hardly any approach being able to reach more than 50% precision at recall level higher than 90%. Challenges in realistic environment include illumination change causing complex intensity variation, background motions (trees, waves, etc.) whose magnitude can be greater than the foreground, poor image quality under low light, camouflage etc. Existing methods often handle only part of these challenges; we address all these challenges in a unified framework which makes little specific assumption of the background. We regard the observed image sequence as being made up of the sum of a low-rank background matrix and a sparse outlier matrix and solve the decomposition using the Robust Principal Component Analysis method. We dynamically estimate the support of the foreground regions via a motion saliency estimation step, so as to impose spatial coherence on these regions. Unlike smoothness constraint such as MRF, our method is able to obtain crisply defined foreground regions, and in general, handles large dynamic background motion much better. Extensive experiments on benchmark and additional challenging datasets demonstrate that our method significantly outperforms the state-of-the-art approaches and works effectively on a wide range of complex scenarios."'),
('"Blocks World Revisited: Image Understanding Using Qualitative Geometry and Mechanics"', '"ECCV 2010"', '["Physical Stability", "Support Relationship", "Density Class", "Segmentation Performance", "Outdoor', '"https://doi.org/10.1007/978-3-642-15561-1_35"', '"Since most current scene understanding approaches operate either on the 2D image or using a surface-based representation, they do not allow reasoning about the physical constraints within the 3D scene. Inspired by the \\u201cBlocks World\\u201d work in the 1960\\u2019s, we present a qualitative physical representation of an outdoor scene where objects have volume and mass, and relationships describe 3D structure and mechanical configurations. Our representation allows us to apply powerful global geometric constraints between 3D volumes as well as the laws of statics in a qualitative manner. We also present a novel iterative \\u201cinterpretation-by-synthesis\\u201d approach where, starting from an empty ground plane, we progressively \\u201cbuild up\\u201d a physically-plausible 3D interpretation of the image. For surface layout estimation, our method demonstrates an improvement in performance over the state-of-the-art [9]. But more importantly, our approach automatically generates 3D parse graphs which describe qualitative geometric and mechanical properties of objects and relationships between objects within an image."'),
('"Blur-Kernel Estimation from Spectral Irregularities"', '"ECCV 2012"', '["Natural Image", "Phase Retrieval", "Blind Deconvolution", "Blur Kernel", "Blurry Image"]', '"https://doi.org/10.1007/978-3-642-33715-4_45"', '"We describe a new method for recovering the blur kernel in motion-blurred images based on statistical irregularities their power spectrum exhibits. This is achieved by a power-law that refines the one traditionally used for describing natural images. The new model better accounts for biases arising from the presence of large and strong edges in the image. We use this model together with an accurate spectral whitening formula to estimate the power spectrum of the blur. The blur kernel is then recovered using a phase retrieval algorithm with improved convergence and disambiguation capabilities. Unlike many existing methods, the new approach does not perform a maximum a posteriori estimation, which involves repeated reconstructions of the latent image, and hence offers attractive running times."'),
('"Bony Structure Suppression in Chest Radiographs"', '"CVAMIA 2006"', '["Chest Radiograph", "Lung Nodule", "Explicit Scheme", "Bony Structure", "Subtraction Image"]', '"https://doi.org/10.1007/11889762_15"', '"Many computer aided diagnosis schemes in chest radiography start with preprocessing steps that try to remove or suppress normal anatomical structures from the image. Examples of normal structures in posteroanterior chest radiographs are bony structures. Removing these kinds of structures can be done quite effectively if the right dual energy images\\u2014two radiographic images from the same patient taken with different energies\\u2014are available. Subtracting these two radiographs gives a soft-tissue image with most of the rib and other bony structures removed. In general, however, dual energy images are not readily available."'),
('"Boosting Chamfer Matching by Learning Chamfer Distance Normalization"', '"ECCV 2010"', '["Target Object", "Object Detection", "IEEE Conf", "Weak Learner", "Cluttered Background"]', '"https://doi.org/10.1007/978-3-642-15555-0_33"', '"We propose a novel technique that significantly improves the performance of oriented chamfer matching on images with cluttered background. Different to other matching methods, which only measures how well a template fits to an edge map, we evaluate the score of the template in comparison to auxiliary contours, which we call normalizers. We utilize AdaBoost to learn a Normalized Oriented Chamfer Distance (NOCD). Our experimental results demonstrate that it boosts the detection rate of the oriented chamfer distance. The simplicity and ease of training of NOCD on a small number of training samples promise that it can replace chamfer distance and oriented chamfer distance in any template matching application."'),
('"Boosting VLAD with Supervised Dictionary Learning and High-Order Statistics"', '"ECCV 2014"', '["Visual Word", "Action Recognition", "Local Descriptor", "Sparse Code", "Convolutional Neural Netwo', '"https://doi.org/10.1007/978-3-319-10578-9_43"', '"Recent studies show that aggregating local descriptors into super vector yields effective representation for retrieval and classification tasks. A popular method along this line is vector of locally aggregated descriptors (VLAD), which aggregates the residuals between descriptors and visual words. However, original VLAD ignores high-order statistics of local descriptors and its dictionary may not be optimal for classification tasks. In this paper, we address these problems by utilizing high-order statistics of local descriptors and peforming supervised dictionary learning. The main contributions are twofold. Firstly, we propose a high-order VLAD (H-VLAD) for visual recognition, which leverages two kinds of high-order statistics in the VLAD-like framework, namely diagonal covariance and skewness. These high-order statistics provide complementary information for VLAD and allow for efficient computation. Secondly, to further boost the performance of H-VLAD, we design a supervised dictionary learning algorithm to discriminatively refine the dictionary, which can be also extended for other super vector based encoding methods. We examine the effectiveness of our methods in image-based object categorization and video-based action recognition. Extensive experiments on PASCAL VOC 2007, HMDB51, and UCF101 datasets exhibit that our method achieves the state-of-the-art performance on both tasks."'),
('"Bootstrap Initialization of Nonparametric Texture Models for Tracking"', '"ECCV 2000"', '["Model Point", "Dirichlet Distribution", "Head Orientation", "Texture Model", "Acquisition Function', '"https://doi.org/10.1007/3-540-45053-X_8"', '"Inbootstrap initialization for tracking, we exploit a weak prior model used to track a target to learn a stronger model, without manual intervention. We define a general formulation of this problem and present a simple taxonomy of such tasks."'),
('"Bottom-Up Perceptual Organization of Images into Object Part Hypotheses"', '"ECCV 2012"', '["IEEE Computer Society", "Illusory Contour", "Object Part", "Good Continuation", "British Machine V', '"https://doi.org/10.1007/978-3-642-33718-5_19"', '"The demise of \\u201csegmentation-then-recognition\\u201d strategy led to a paradigm shift toward feature-based discriminative recognition with significant success. However, increased complexity in multi-class datasets reveals that local low-level features may not be sufficiently discriminative, requiring the construction and use of more complex structural features which are necessarily category independent. The paper proposes a bottom-up procedure for generating fragment features which are intended to be object part hypotheses. Suggesting that the demise of segmentation to generate a representation suitable for recognition was due to prematurely committing to a grouping option in the face of ambiguities, the proposed framework considers and tracks multiple alternate grouping options. This approach is made tractable by (i) using a medial fragment representation which allows for the simultaneous use of multiple cues, (ii) a set of transforms to effect grouping operations, (iii) a containment graph representation which avoids duplicate consideration of possibilities, and the estimation of the likelihood of a grouping sequence to retain only plausible groupings. The resulting hypotheses are evaluated intrinsically by measuring their ability to represent objects with a few fragments. They are also evaluated by comparison to algorithms which aim to generate full object segments, with results that match or exceed the state of art, thus demonstrating the suitability of the proposed mid-level representation."'),
('"Boundary Detection Using F-Measure-, Filter- and Feature- (F3) Boost"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_47"', '"In this work we propose a boosting-based approach to boundary detection that advances the current state-of-the-art. To achieve this we introduce the following novel ideas: (a) we use a training criterion that approximates the F-measure of the classifier, instead of the exponential loss that is commonly used in boosting. We optimize this criterion using Anyboost. (b) We deal with the ambiguous information about orientation of the boundary in the annotation by treating it as a hidden variable, and train our classifier using Multiple-Instance Learning. (c) We adapt the Filterboost approach of [1] to leverage information from the whole training set to train our classifier, instead of using a fixed subset of points. (d) We extract discriminative features from appearance descriptors that are computed densely over the image. We demonstrate the performance of our approach on the Berkeley Segmentation Benchmark."'),
('"Bounding Part Scores for Rapid Detection with Deformable Part Models"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_5"', '"Computing part scores is the main computational bottleneck in object detection with Deformable Part Models. In this work we introduce an efficient method to obtain bounds on part scores, which we then integrate with deformable model detection. As in [1] we rapidly approximate the inner product between a weight vector and HOG-based features by quantizing the HOG cells onto a codebook and replace their inner product with the lookup of a precomputed score. The novelty in our work consists in combining this lookup-based estimate with the codebook quantization error so as to construct probabilistic bounds to the exact inner product."'),
('"Brain Hallucination"', '"ECCV 2008"', '["Regularization Term", "Regularization Approach", "Face Hallucination", "Reconstructed High Resolut', '"https://doi.org/10.1007/978-3-540-88682-2_38"', '"In this paper, we investigate brain hallucination, or generating a high resolution brain image from an input low-resolution image, with the help of another high resolution brain image. Contrary to interpolation techniques, the reconstruction process is based on a physical model of image acquisition. Our contribution is a new regularization approach that uses an example-based framework integrating non-local similarity constraints to handle in a better way repetitive structures and texture. The effectiveness of our approach is demonstrated by experiments on realistic Magnetic Resonance brain images generating automatically high-quality hallucinated brain images from low-resolution input."'),
('"BRIEF: Binary Robust Independent Elementary Features"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_56"', '"We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L 2 norm as is usually done."'),
('"Building a Compact Relevant Sample Coverage for Relevance Feedback in Content-Based Image Retrieval', '"ECCV 2008"', '["Local Binary Pattern", "Relevance Feedback", "Query Point", "Query Expansion", "Relevance Score"]', '"https://doi.org/10.1007/978-3-540-88682-2_53"', '"Conventional approaches to relevance feedback in content-based image retrieval are based on the assumption that relevant images are physically close to the query image, or the query regions can be identified by a set of clustering centers. However, semantically related images are often scattered across the visual space. It is not always reliable that the refined query point or the clustering centers are capable of representing a complex query region."'),
('"Building Architectural Models from Many Views Using Map Constraints"', '"ECCV 2002"', '["Projection Matrix", "Camera Calibration", "Camera Parameter", "Camera Position", "Projection Matri', '"https://doi.org/10.1007/3-540-47967-8_11"', '"This paper describes an interactive system for creating geometric models from many uncalibrated images of architectural scenes. In this context, we must solve the structure from motion problem given only few and noisy feature correspondences in non-sequential views. By exploiting the strong constraints obtained by modelling a map as a single affine view of the scene, we are able to compute all 3D points and camera positions simultaneously as the solution of a set of linear equations. Reconstruction is achieved without making restrictive assumptions about the scene (such as that reference points or planes are visible in all views). We have implemented a practical interactive system, which has been used to make large-scale models of a variety of architectural scenes. We present quantitative and qualitative results obtained by this system."'),
('"Building Compact Local Pairwise Codebook with Joint Feature Space Clustering"', '"ECCV 2010"', '["Feature Selection", "Visual Word", "Cluster Assignment", "Sift Descriptor", "Codebook Size"]', '"https://doi.org/10.1007/978-3-642-15549-9_50"', '"This paper presents a simple, yet effective method of building a codebook for pairs of spatially close SIFT descriptors. Integrating such codebook into the popular bag-of-words model encodes local spatial information which otherwise cannot be represented with just individual SIFT descriptors. Many previous pairing techniques first quantize the descriptors to learn a set of visual words before they are actually paired. Our approach contrasts with theirs in that each pair of spatially close descriptors is represented as a data point in a joint feature space first and then clustering is applied to build a codebook called Local Pairwise Codebook (LPC). It is advantageous over the previous approaches in that feature selection over quadratic number of possible pairs of visual words is not required and feature aggregation is implicitly performed to achieve a compact codebook. This is all done in an unsupervised manner. Experimental results on challenging datasets, namely 15 Scenes, 67 Indoors, Caltech-101, Caltech-256 and MSRCv2 demonstrate that LPC outperforms the baselines and performs competitively against the state-of-the-art techniques in scene and object categorization tasks where a large number of categories need to be recognized."'),
('"Building Roadmaps of Local Minima of Visual Models"', '"ECCV 2002"', '["Model based vision", "global optimization", "saddle points", "3D human tracking"]', '"https://doi.org/10.1007/3-540-47969-4_38"', '"Getting trapped in suboptimal local minima is a perennial problem in model based vision, especially in applications like monocular human body tracking where complex nonlinear parametric models are repeatedly fitted to ambiguous image data. We show that the trapping problem can be attacked by building \\u2018roadmaps\\u2019 of nearby minima linked by transition pathways \\u2014 paths leading over low \\u2018cols\\u2019 or \\u2018passes\\u2019 in the cost surface, found by locating the transition state (codimension-1 saddle point) at the top of the pass and then sliding downhill to the next minimum. We know of no previous vision or optimization work on numerical methods for locating transition states, but such methods do exist in computational chemistry, where transitions are critical for predicting reaction parameters. We present two families of methods, originally derived in chemistry, but here generalized, clarified and adapted to the needs of model based vision: eigenvector tracking is a modified form of damped Newton minimization, while hypersurface sweeping sweeps a moving hypersurface through the space, tracking minima within it. Experiments on the challenging problem of estimating 3D human pose from monocular images show that our algorithms find nearby transition states and minima very efficiently, but also underline the disturbingly large number of minima that exist in this and similar model based vision problems."'),
('"Building Rome on a Cloudless Day"', '"ECCV 2010"', '["Bundle Adjustment", "Structure From Motion", "Epipolar Geometry", "Locality Sensitive Hash", "Phot', '"https://doi.org/10.1007/978-3-642-15561-1_27"', '"This paper introduces an approach for dense 3D reconstruction from unregistered Internet-scale photo collections with about 3 million images within the span of a day on a single PC (\\u201ccloudless\\u201d). Our method advances image clustering, stereo, stereo fusion and structure from motion to achieve high computational performance. We leverage geometric and appearance constraints to obtain a highly parallel implementation on modern graphics processors and multi-core architectures. This leads to two orders of magnitude higher performance on an order of magnitude larger dataset than competing state-of-the-art approaches."'),
('"Bundle Adjustment in the Large"', '"ECCV 2010"', '["Conjugate Gradient", "Levenberg Marquardt", "Newton Step", "Bundle Adjustment", "Leaf Image"]', '"https://doi.org/10.1007/978-3-642-15552-9_3"', '"We present the design and implementation of a new inexact Newton type algorithm for solving large-scale bundle adjustment problems with tens of thousands of images. We explore the use of Conjugate Gradients for calculating the Newton step and its performance as a function of some simple and computationally efficient preconditioners. We show that the common Schur complement trick is not limited to factorization-based methods and that it can be interpreted as a form of preconditioning. Using photos from a street-side dataset and several community photo collections, we generate a variety of bundle adjustment problems and use them to evaluate the performance of six different bundle adjustment algorithms. Our experiments show that truncated Newton methods, when paired with relatively simple preconditioners, offer state of the art performance for large-scale bundle adjustment. The code, test problems and detailed performance data are available at http://grail.cs.washington.edu/projects/bal ."'),
('"Calculating Reachable Workspace Volume for Use in Quantitative Medicine"', '"ECCV 2014"', '["Kinect", "Muscular dystrophy", "Functional workspace", "Rehabilitation", "Assessment", "Diagnosis"', '"https://doi.org/10.1007/978-3-319-16199-0_40"', '"Quantitative measures of the space an individual can reach is essential for tracking the progression of a disease and the effects of therapeutic intervention. The reachable workspace can be used to track an individuals\\u2019 ability to perform activities of daily living, such as feeding and grooming. There are few methods for quantifying upper limb performance, none of which are able to generate a reachable workspace volume from motion capture data. We introduce a method to estimate the reachable workspace volume for an individual by capturing their observed joint limits using a low cost depth camera. This method is then tested on seven individuals with varying upper limb performance. Based on these initial trials, we found that the reachable workspace volume decreased as muscular impairment increased. This shows the potential for this method to be used as a quantitative clinical assessment tool."'),
('"Calibrating Parameters of Cost Functionals"', '"ECCV 2000"', '["Learning", "variational method", "parameter estimation", "image reconstruction", "Bayesian image m', '"https://doi.org/10.1007/3-540-45053-X_14"', '"We propose a new framework for calibrating parameters of energy functionals, as used in image analysis. The method learns parameters from a family of correct examples, and given a probabilistic construct for generating wrong examples from correct ones. We introduce a measure of frustration to penalize cases in which wrong responses are preferred to correct ones, and we design a stochastic gradient algorithm which converges to parameters which minimize this measure of frustration. We also present a first set of experiments in this context, and introduce extensions to deal with data-dependent energies."'),
('"Calibration from Statistical Properties of the Visual World"', '"ECCV 2008"', '["Conditional Entropy", "Angular Separation", "Sensor Element", "Information Distance", "Visual Worl', '"https://doi.org/10.1007/978-3-540-88693-8_17"', '"What does a blind entity need in order to determine the geometry of the set of photocells that it carries through a changing lightfield? In this paper, we show that very crude knowledge of some statistical properties of the environment is sufficient for this task."'),
('"Calibration Methodology for Distant Surveillance Cameras"', '"ECCV 2014"', '["Video surveillance", "Networked cameras", "Calibration"]', '"https://doi.org/10.1007/978-3-319-16199-0_12"', '"We present a practical method for video surveillance networks to calibrate their cameras which have mostly non-overlapping field of views and might be tens of meters apart. The calibration or estimating the camera pose, focal length and radial distortion is an essential requirement in video surveillance systems for any further automated tasks like person tracking or flow monitoring. The proposed methodology casts the calibration as a localization problem of an image with respect to a 3D model which is built a priori with a moving camera. The method comprises state-of-the-art functioning blocks, the Structure from Motion (SfM) and minimal Perspective-n-Point (PnP) solvers, which were proved stable in 3D computer vision community and applies them in context of video surveillance. We demonstrate that the calibration method is effective in difficult repetitive, reflective and texture less large indoor environments like an airport."'),
('"Calibration of a Moving Camera Using a Planar Pattern: Optimal Computation, Reliability Evaluation,', '"ECCV 2000"', '["Focal Length", "Camera Motion", "Camera Calibration", "Newton Iteration", "Grid Pattern"]', '"https://doi.org/10.1007/3-540-45053-X_38"', '"We present a scheme for simultaneous calibration of a continuously moving and continuously zooming camera: placing an easily distinguishable pattern in the scene, we calibrate the camera from an unoccluded portion of the pattern image in each frame. We describe an optimal method which provides an evaluation of the reliability of the solution. We then propose a technique for avoiding the inherent degeneracy and statistical fluctuations by model selection using the geometric AIC and the geometric MDL."'),
('"Camera Calibration and Shape Recovery from Videos of Two Mirrors"', '"ECCV 2014"', '["Video", "Motion and shape recovery", "Camera calibration", "Two-mirror system", "Circular motion"]', '"https://doi.org/10.1007/978-3-319-16178-5_53"', '"This paper addresses the problem of motion and shape recovery from a two-mirror system which is able to generate five views of an object. Different from existing methods, this paper uses a short video instead of static snapshots so that it can help with action recognition once the 3D visual hull model is reconstructed. In order to solve the problem, this paper shows the geometry relationship between the two-mirror system and circular motion, so that the two-mirror system can be solved as circular motions. Different from the approach of Zhang et al. [22], we avoid using the vanishing point of X-axis which would cause accumulate error when calculating the epipoles of two views. Results of comparative experiments and the 3D visual hull of model show the feasibility and the accuracy of the proposed approach."'),
('"Camera Calibration from the Quasi-affine Invariance of Two Parallel Circles"', '"ECCV 2004"', '["Camera Calibration", "Intrinsic Parameter", "Pinhole Camera", "Absolute Conic", "Circular Point"]', '"https://doi.org/10.1007/978-3-540-24670-1_15"', '"In this paper, a new camera calibration algorithm is proposed, which is from the quasi-affine invariance of two parallel circles. Two parallel circles here mean two circles in one plane, or in two parallel planes. They are quite common in our life."'),
('"Camera Calibration with One-Dimensional Objects"', '"ECCV 2002"', '["Camera calibratio"]', '"https://doi.org/10.1007/3-540-47979-1_11"', '"Camera calibration has been studied extensively in computer vision and photogrammetry, and the proposed techniques in the literature include those using 3D apparatus (two or three planes orthogonal to each other, or a plane undergoing a pure translation, etc.), 2D objects (planar patterns undergoing unknown motions), and 0D features (self-calibration using unknown scene points). This paper yet proposes a new calibration technique using 1D objects (points aligned on a line), thus filling the missing dimension in calibration. In particular, we show that camera calibration is not possible with free-moving 1D objects, but can be solved if one point is fixed. A closed-form solution is developed if six or more observations of such a 1D object are made. For higher accuracy, a nonlinear technique based on the maximum likelihood criterion is then used to refine the estimate. Besides the theoretical aspect, the proposed technique is also important in practice especially when calibrating multiple cameras mounted apart from each other, where the calibration objects are required to be visible simultaneously."'),
('"Camera Calibration with Two Arbitrary Coaxial Circles"', '"ECCV 2006"', '["Camera Calibration", "Principal Point", "Static Scene", "Calibration Approach", "Camera Center"]', '"https://doi.org/10.1007/11744023_21"', '"We present an approach for camera calibration from the image of at least two circles arranged in a coaxial way. Such a geometric configuration arises in static scenes of objects with rotational symmetry or in scenes including generic objects undergoing rotational motion around a fixed axis. The approach is based on the automatic localization of a surface of revolution (SOR) in the image, and its use as a calibration artifact. The SOR can either be a real object in a static scene, or a \\u201cvirtual surface\\u201d obtained by frame superposition in a rotational sequence. This provides a unified framework for calibration from single images of SORs or from turntable sequences. Both the internal and external calibration parameters (square pixels model) are obtained from two or more imaged cross sections of the SOR, whose apparent contour is also exploited to obtain a better calibration accuracy. Experimental results show that this calibration approach is accurate enough for several vision applications, encompassing 3D realistic model acquisition from single images, and desktop 3D object scanning."'),
('"Camera Calibration with Two Arbitrary Coplanar Circles"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24672-5_41"', '"In this paper, we describe a novel camera calibration method to estimate the extrinsic parameters and the focal length of a camera by using only one single image of two coplanar circles with arbitrary radius."'),
('"Camera Pose Estimation and Reconstruction from Image Profiles under Circular Motion"', '"ECCV 2000"', '[]', '"https://doi.org/10.1007/3-540-45053-X_55"', '"This paper addresses the problem of motion estimation and reconstruction of 3D models from profiles of an object rotating on a turntable, obtained from a single camera. Its main contribution is the development of a practical and accurate technique for solving this problem from profiles alone, which is, for the first time, precise enough to allow the reconstruction of the object. No correspondence between points or lines are necessary, although the method proposed can be equally used when these features are available, without any further adaptation. Symmetry properties of the surface of revolution swept out by the rotating object are exploited to obtain the image of the rotation axis and the homography relating epipolar lines, in a robust and elegant way. These, together with geometric constraints for images of rotating objects, are then used to obtain first the image of the horizon, which is the projection of the plane that contains the camera centres, and then the epipoles, thus fully determining the epipolar geometry of the sequence of images. The estimation of the epipolar geometry by this sequential approach (image of rotation axis - homography - image of the horizon - epipoles) avoids many of the problems usually found in other algorithms for motion recovery from profiles. In particular, the search for the epipoles, by far the most critical step, is carried out as a simple one-dimensional optimisation problem. The initialisation of the parameters is trivial and completely automatic for all stages of the algorithm. After the estimation of the epipolar geometry, the Euclidean motion is recovered using the fixed intrinsic parameters of the camera, obtained either from a calibration grid or from self-calibration techniques. Finally, the spinning object is reconstructed from its profiles, using the motion estimated in the previous stage. Results from real data are presented, demonstrating the efficiency and usefulness of the proposed methods."'),
('"Camera Pose Estimation Using First-Order Curve Differential Geometry"', '"ECCV 2012"', '["Pose Estimation", "Camera Resectioning", "Differential Geometry"]', '"https://doi.org/10.1007/978-3-642-33765-9_17"', '"This paper considers and solves the problem of estimating camera pose given a pair of point-tangent correspondences between the 3D scene and the projected image. The problem arises when considering curve geometry as the basis of forming correspondences, computation of structure and calibration, which in its simplest form is a point augmented with the curve tangent. We show that while the standard resectioning problem is solved with a minimum of three points given the intrinsic parameters, when points are augmented with tangent information only two points are required, leading to substantial computational savings, e.g., when used as a minimal engine within ransac. In addition, computational algorithms are developed to find a practical and efficient solution shown to effectively recover camera pose using both synthetic and realistic datasets. The resolution of this problem is intended as a basic building block of future curve-based structure from motion systems, allowing new views to be incrementally registered to a core set of views for which relative pose has already been computed."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Camera Pose Estimation Using Images of Planar Mirror Reflections"', '"ECCV 2010"', '["Planar Motion", "Planar Mirror", "Rigid Transformation", "Virtual View", "Static Camera"]', '"https://doi.org/10.1007/978-3-642-15561-1_28"', '"The image of a planar mirror reflection (IPMR) can be interpreted as a virtual view of the scene, acquired by a camera with a pose symmetric to the pose of the real camera with respect to the mirror plane. The epipolar geometry of virtual views associated with different IPMRs is well understood, and it is possible to recover the camera motion and perform 3D scene reconstruction by applying standard structure-from-motion methods that use image correspondences as input. In this article we address the problem of estimating the pose of the real camera, as well as the positions of the mirror plane, by assuming that the rigid motion between N virtual views induced by planar mirror reflections is known. The solution of this problem enables the registration of objects lying outside the camera field-of-view, which can have important applications in domains like non-overlapping camera network calibration and robot vision. We show that the positions of the mirror planes can be uniquely determined by solving a system of linear equations. This enables to estimate the pose of the real camera in a straightforward closed-form manner using a minimum of N\\u2009=\\u20093 virtual views. Both synthetic tests and real experiments show the superiority of our approach with respect to current state-of-the-art methods."'),
('"Can a Continuity Heuristic Be Used to Resolve the Inclination Ambiguity of Polarized Light Imaging?', '"MMBIA 2004"', '["Simulated Annealing", "Fiber Orientation", "Simulated Annealing Algorithm", "Large Array", "Coarse', '"https://doi.org/10.1007/978-3-540-27816-0_31"', '"We propose the use of a continuity heuristic for solving the inclination ambiguity of polarized light imaging, which is a high resolution method of mapping the spatially varying pattern of anisotropy in biological and non-biological samples. Applied to the white matter of the brain, solving the inclination ambiguity of polarized light imaging will allow the creation of a 3D model of fibers. We use the continuity heuristic in several methods, some of which employ the simulated annealing algorithm to reinforce the heuristic, while others proceed deterministically to solve the inclination ambiguity. We conclude by explaining the limitations of the continuity heuristic approach."'),
('"Can We Calibrate a Camera Using an Image of a Flat,Textureless Lambertian Surface?"', '"ECCV 2000"', '["Focal Length", "Intensity Noise", "Real Image", "Camera Calibration", "Calibration Technique"]', '"https://doi.org/10.1007/3-540-45053-X_41"', '"In this paper, we show that it is possible to calibrate a camera using just a flat, textureless Lambertian surface and constant illumination. This is done using the effects of off-axis illumination and vignetting, which result in reduction of light into the camera at off-axis angles. We use these imperfections to our advantage. The intrinsic parameters that we consider are the focal length, principal point, aspect ratio, and skew. We also consider the effect of the tilt of the camera. Preliminary results from simulated and real experiments show that the focal length can be recovered relatively robustly under certain conditions."'),
('"Can We Consider Central Catadioptric Cameras and Fisheye Cameras within a Unified Imaging Model"', '"ECCV 2004"', '["Conic Section", "Line Image", "Quadric Surface", "Perspective Projection", "Perspective Image"]', '"https://doi.org/10.1007/978-3-540-24670-1_34"', '"There are two kinds of omnidirectional cameras often used in computer vision: central catadioptric cameras and fisheye cameras. Previous literatures use different imaging models to describe them separately. A unified imaging model is however presented in this paper. The unified model in this paper can be considered as an extension of the unified imaging model for central catadioptric cameras proposed by Geyer and Daniilidis. We show that our unified model can cover some existing models for fisheye cameras and fit well for many actual fisheye cameras used in previous literatures. Under our unified model, central catadioptric cameras and fisheye cameras can be classified by the model\\u2019s characteristic parameter, and a fisheye image can be transformed into a central catadioptric one, vice versa. An important merit of our new unified model is that existing calibration methods for central catadioptric cameras can be directly applied to fisheye cameras. Furthermore, the metric calibration from single fisheye image only using projections of lines becomes possible via our unified model but the existing methods for fisheye cameras in the literatures till now are all non-metric under the same conditions. Experimental results of calibration from some central catadioptric and fisheye images confirm the validity and usefulness of our new unified model."'),
('"Canonical Correlation Analysis on Riemannian Manifolds and Its Applications"', '"ECCV 2014"', '["Riemannian Manifold", "Canonical Correlation Analysis", "Parallel Transport", "Geodesic Curve", "D', '"https://doi.org/10.1007/978-3-319-10605-2_17"', '"Canonical correlation analysis (CCA) is a widely used statistical technique to capture correlations between two sets of multi-variate random variables and has found a multitude of applications in computer vision, medical imaging and machine learning. The classical formulation assumes that the data live in a pair of vector spaces which makes its use in certain important scientific domains problematic. For instance, the set of symmetric positive definite matrices (SPD), rotations and probability distributions, all belong to certain curved Riemannian manifolds where vector-space operations are in general not applicable. Analyzing the space of such data via the classical versions of inference models is rather sub-optimal. But perhaps more importantly, since the algorithms do not respect the underlying geometry of the data space, it is hard to provide statistical guarantees (if any) on the results. Using the space of SPD matrices as a concrete example, this paper gives a principled generalization of the well known CCA to the Riemannian setting. Our CCA algorithm operates on the product Riemannian manifold representing SPD matrix-valued fields to identify meaningful statistical relationships on the product Riemannian manifold. As a proof of principle, we present results on an Alzheimer\\u2019s disease (AD) study where the analysis task involves identifying correlations across diffusion tensor images (DTI) and Cauchy deformation tensor fields derived from T1-weighted magnetic resonance (MR) images."'),
('"Capacity and Examples of Template-Protecting Biometric Authentication Systems"', '"BioAW 2004"', '["Authentication System", "Enrollment Phase", "Biometric System", "Authentication Phase", "Biometric', '"https://doi.org/10.1007/978-3-540-25976-3_15"', '"In this paper, we formulate precisely the requirements for privacy protecting biometric authentication systems. The secrecy capacity Cs is investigated for the discrete and the continuous case. We present, furthermore, a general algorithm that meets the requirements and achieves Cs as well as Cid (the identification capacity). Finally, we present some practical constructions of the general algorithm and analyze their properties."'),
('"Carved Visual Hulls for Image-Based Modeling"', '"ECCV 2006"', '["Image Discrepancy", "Surface Detail", "Visual Hull", "Frontier Point", "Apparent Contour"]', '"https://doi.org/10.1007/11744023_44"', '"This article presents a novel method for acquiring high-quality solid models of complex 3D shapes from multiple calibrated photographs. After the purely geometric constraints associated with the silhouettes found in each image have been used to construct a coarse surface approximation in the form of a visual hull, photoconsistency constraints are enforced in three consecutive steps: (1) the rims where the surface grazes the visual hull are first identified through dynamic programming; (2) with the rims now fixed, the visual hull is carved using graph cuts to globally optimize the photoconsistency of the surface and recover its main features; (3) an iterative (local) refinement step is finally used to recover fine surface details. The proposed approach has been implemented, and experiments with six real data sets are presented, along with qualitative comparisons with several state-of-the-art image-based-modeling algorithms."'),
('"Cascaded Confidence Filtering for Improved Tracking-by-Detection"', '"ECCV 2010"', '["Object Detection", "Ground Plane", "Background Model", "Human Detection", "Scene Structure"]', '"https://doi.org/10.1007/978-3-642-15549-9_27"', '"We propose a novel approach to increase the robustness of object detection algorithms in surveillance scenarios. The cascaded confidence filter successively incorporates constraints on the size of the objects, on the preponderance of the background and on the smoothness of trajectories. In fact, the continuous detection confidence scores are analyzed locally to adapt the generic detector to the specific scene. The approach does not learn specific object models, reason about complete trajectories or scene structure, nor use multiple cameras. Therefore, it can serve as preprocessing step to robustify many tracking-by-detection algorithms. Our real-world experiments show significant improvements, especially in the case of partial occlusions, changing backgrounds, and similar distractors."'),
('"Cascaded Models for Articulated Pose Estimation"', '"ECCV 2010"', '["State Space", "Part Axis", "Pictorial Structure", "Rich Feature", "Pairwise Term"]', '"https://doi.org/10.1007/978-3-642-15552-9_30"', '"We address the problem of articulated human pose estimation by learning a coarse-to-fine cascade of pictorial structure models. While the fine-level state-space of poses of individual parts is too large to permit the use of rich appearance models, most possibilities can be ruled out by efficient structured models at a coarser scale. We propose to learn a sequence of structured models at different pose resolutions, where coarse models filter the pose space for the next level via their max-marginals. The cascade is trained to prune as much as possible while preserving true poses for the final level pictorial structure model. The final level uses much more expensive segmentation, contour and shape features in the model for the remaining filtered set of candidates. We evaluate our framework on the challenging Buffy and PASCAL human pose datasets, improving the state-of-the-art."'),
('"Cat Head Detection - How to Effectively Exploit Shape and Texture Features"', '"ECCV 2008"', '["Texture Feature", "Object Detection", "Face Detection", "Human Detection", "Texture Detector"]', '"https://doi.org/10.1007/978-3-540-88693-8_59"', '"In this paper, we focus on the problem of detecting the head of cat-like animals, adopting cat as a test case. We show that the performance depends crucially on how to effectively utilize the shape and texture features jointly. Specifically, we propose a two step approach for the cat head detection. In the first step, we train two individual detectors on two training sets. One training set is normalized to emphasize the shape features and the other is normalized to underscore the texture features. In the second step, we train a joint shape and texture fusion classifier to make the final decision. We demonstrate that a significant improvement can be obtained by our two step approach. In addition, we also propose a set of novel features based on oriented gradients, which outperforms existing leading features, e. g., Haar, HoG, and EoH. We evaluate our approach on a well labeled cat head data set with 10,000 images and PASCAL 2007 cat data."'),
('"Categorizing Turn-Taking Interactions"', '"ECCV 2012"', '["Video Sequence", "Visual Word", "Interest Point", "Camera Motion", "Negative Instance"]', '"https://doi.org/10.1007/978-3-642-33715-4_28"', '"We address the problem of categorizing turn-taking interactions between individuals. Social interactions are characterized by turn-taking and arise frequently in real-world videos. Our approach is based on the use of temporal causal analysis to decompose a space-time visual word representation of video into co-occuring independent segments, called causal sets [1]. These causal sets then serves the input to a multiple instance learning framework to categorize turn-taking interactions. We introduce a new turn-taking interactions dataset consisting of social games and sports rallies. We demonstrate that our formulation of multiple instance learning (QP-MISVM) is better able to leverage the repetitive structure in turn-taking interactions and demonstrates superior performance relative to a conventional bag of words model."'),
('"Category Independent Object Proposals"', '"ECCV 2010"', '["Appearance Model", "Object Region", "Boost Decision Tree", "Hierarchical Segmentation", "Occlusion', '"https://doi.org/10.1007/978-3-642-15555-0_42"', '"We propose a category-independent method to produce a bag of regions and rank them, such that top-ranked regions are likely to be good segmentations of different objects. Our key objectives are completeness and diversity: every object should have at least one good proposed region, and a diverse set should be top-ranked. Our approach is to generate a set of segmentations by performing graph cuts based on a seed region and a learned affinity function. Then, the regions are ranked using structured learning based on various cues. Our experiments on BSDS and PASCAL VOC 2008 demonstrate our ability to find most objects within a small bag of proposed regions."'),
('"Category-Specific Video Summarization"', '"ECCV 2014"', '["video summarization", "temporal segmentation", "video classification"]', '"https://doi.org/10.1007/978-3-319-10599-4_35"', '"In large video collections with clusters of typical categories, such as \\u201cbirthday party\\u201d or \\u201cflash-mob\\u201d, category-specific video summarization can produce higher quality video summaries than unsupervised approaches that are blind to the video category."'),
('"Causal Camera Motion Estimation by Condensation and Robust Statistics Distance Measures"', '"ECCV 2004"', '["Motion Estimation", "Extend Kalman Filter", "Geometric Error", "Camera Motion", "Robust Motion"]', '"https://doi.org/10.1007/978-3-540-24672-5_10"', '"The problem of Simultaneous Localization And Mapping (SLAM) originally arose from the robotics community and is closely related to the problems of camera motion estimation and structure recovery in computer vision. Recent work in the vision community addressed the SLAM problem using either active stereo or a single passive camera. The precision of camera based SLAM was tested in indoor static environments. However the extended Kalman filters (EKF) as used in these tests are highly sensitive to outliers. For example, even a single mismatch of some feature point could lead to catastrophic collapse in both motion and structure estimates. In this paper we employ a robust-statistics-based condensation approach to the camera motion estimation problem. The condensation framework maintains multiple motion hypotheses when ambiguities exist. Employing robust distance functions in the condensation measurement stage enables the algorithm to discard a considerable fraction of outliers in the data. The experimental results demonstrate the accuracy and robustness of the proposed method."'),
('"CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching"', '"ECCV 2008"', '["Image Match", "Integral Image", "Visual Odometry", "Center Surround", "Viewpoint Change"]', '"https://doi.org/10.1007/978-3-540-88693-8_8"', '"We explore the suitability of different feature detectors for the task of image registration, and in particular for visual odometry, using two criteria: stability (persistence across viewpoint change) and accuracy (consistent localization across viewpoint change). In addition to the now-standard SIFT, SURF, FAST, and Harris detectors, we introduce a suite of scale-invariant center-surround detectors (CenSurE) that outperform the other detectors, yet have better computational characteristics than other scale-space detectors, and are capable of real-time implementation."'),
('"ChaLearn Looking at People Challenge 2014: Dataset and Results"', '"ECCV 2014"', '["Human pose recovery", "Behavior analysis", "Action and interactions", "Multi-modal gestures", "Rec', '"https://doi.org/10.1007/978-3-319-16178-5_32"', '"This paper summarizes the ChaLearn Looking at People 2014 challenge data and the results obtained by the participants. The competition was split into three independent tracks: human pose recovery from RGB data, action and interaction recognition from RGB data sequences, and multi-modal gesture recognition from RGB-Depth sequences. For all the tracks, the goal was to perform user-independent recognition in sequences of continuous images using the overlapping Jaccard index as the evaluation measure. In this edition of the ChaLearn challenge, two large novel data sets were made publicly available and the Microsoft Codalab platform were used to manage the competition. Outstanding results were achieved in the three challenge tracks, with accuracy results of 0.20, 0.50, and 0.85 for pose recovery, action/interaction recognition, and multi-modal gesture recognition, respectively."'),
('"Change Detection in the Presence of Motion Blur and Rolling Shutter Effect"', '"ECCV 2014"', '["Change Detection", "Reference Image", "Camera Motion", "Distorted Image", "Motion Blur"]', '"https://doi.org/10.1007/978-3-319-10584-0_9"', '"The coalesced presence of motion blur and rolling shutter effect is unavoidable due to the sequential exposure of sensor rows in CMOS cameras. We address the problem of detecting changes in an image affected by motion blur and rolling shutter artifacts with respect to a reference image. Our framework bundles modelling of motion blur in global shutter and rolling shutter cameras into a single entity. We leverage the sparsity of the camera trajectory in the pose space and the sparsity of occlusion in spatial domain to propose an optimization problem that not only registers the reference image to the observed distorted image but detects occlusions as well, both within a single framework."'),
('"Characterization of Human Faces under Illumination Variations Using Rank, Integrability, and Symmet', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_45"', '"Photometric stereo algorithms use a Lambertian reflectance model with a varying albedo field and involve the appearances of only one object. This paper extends photometric stereo algorithms to handle all the appearances of all the objects in a class, in particular the class of human faces. Similarity among all facial appearances motivates a rank constraint on the albedos and surface normals in the class. This leads to a factorization of an observation matrix that consists of exemplar images of different objects under different illuminations, which is beyond what can be analyzed using bilinear analysis. Bilinear analysis requires exemplar images of different objects under same illuminations. To fully recover the class-specific albedos and surface normals, integrability and face symmetry constraints are employed. The proposed linear algorithm takes into account the effects of the varying albedo field by approximating the integrability terms using only the surface normals. As an application, face recognition under illumination variation is presented. The rank constraint enables an algorithm to separate the illumination source from the observed appearance and keep the illuminant-invariant information that is appropriate for recognition. Good recognition results have been obtained using the PIE dataset."'),
('"Characterization of Partial Intrinsic Symmetries"', '"ECCV 2014"', '["Symmetry", "Shape analysis", "Shape matching", "Intrinsic geometry", "Slippability analysis"]', '"https://doi.org/10.1007/978-3-319-16220-1_19"', '"We present a mathematical framework and algorithm for characterizing and extracting partial intrinsic symmetries of surfaces, which is a fundamental building block for many modern geometry processing algorithms. Our goal is to compute all \\u201csignificant\\u201d symmetry information of the shape, which we define as \\\\(r\\\\)-symmetries, i.e., we report all isometric self-maps within subsets of the shape that contain at least an intrinsic circle or radius \\\\(r\\\\). By specifying \\\\(r\\\\), the user has direct control over the scale at which symmetry should be detected. Unlike previous techniques, we do not rely on feature points, voting or probabilistic schemes. Rather than that, we bound computational efforts by splitting our algorithm into two phases. The first detects infinitesimal \\\\(r\\\\)-symmetries directly using a local differential analysis, and the second performs direct matching for the remaining discrete symmetries. We show that our algorithm can successfully characterize and extract intrinsic symmetries from a number of example shapes."'),
('"Characterizing Depth Distortion Due to Calibration Uncertainty"', '"ECCV 2000"', '["Structure from motion", "Depth distortion", "Space perception", "Uncalibrated motion analysis"]', '"https://doi.org/10.1007/3-540-45054-8_43"', '"There have been relatively little works to shed light on the effects of errors in the intrinsic parameters on motion estimation and scene reconstruction. Given that the estimation of the extrinsic and intrinsic parameters from uncalibrated motion apts to be imprecise, it is important to study the resulting distortion on the recovered structure. By making use of the iso-distortion framework, we explicitly characterize the geometry of the distorted space recovered from 3-D motion with freely varying focal length. This characterization allows us: 1) to investigate the effectiveness of the visibility constraint in disambiguating uncalibrated motion by studying the negative distortion regions, and 2) to make explicit those ambiguous error situations under which the visibility constraint is not effective. An important finding is that under these ambiguous situations, the direction of heading can nevertheless be accurately recovered and the structure recovered experienced a well-behaved distortion. The distortion is given by a relief transformation which preserves ordinal depth relations. Thus in the case where the only unknown intrinsic parameter is the focal length, structure information in the form of depth relief can be obtained. Experiments were presented to support the use of the visibility constraint in obtaining such partial motion and structure solutions."'),
('"Characterizing Predicate Arity and Spatial Structure for Inductive Learning of Game Rules"', '"ECCV 2014"', '["Predicate discovery", "Spatial structure discovery", "Game rule learning", "Semantic graphs", "Mul', '"https://doi.org/10.1007/978-3-319-16181-5_23"', '"Where do the predicates in a game ontology come from? We use RGBD vision to learn a) the spatial structure of a board, and b) the number of parameters in a move or transition. These are used to define state-transition predicates for a logical description of each game state. Given a set of videos for a game, we use an improved 3D multi-object tracking to obtain the positions of each piece in games such as 4-peg solitaire or Towers of Hanoi. The spatial positions occupied by pieces over the entire game is clustered, revealing the structure of the board. Each frame is represented as a Semantic Graph with edges encoding spatial relations between pieces. Changes in the graphs between game states reveal the structure of a \\u201cmove\\u201d. Knowledge from spatial structure and semantic graphs is mapped to FOL descriptions of the moves and used in an Inductive Logic framework to infer the valid moves and other rules of the game. Discovered predicate structures and induced rules are demonstrated for several games with varying board layouts and move structures."'),
('"Chinese Shadow Puppetry with an Interactive Interface Using the Kinect Sensor"', '"ECCV 2012"', '["Kinect", "shadow puppetry", "gesture"]', '"https://doi.org/10.1007/978-3-642-33863-2_35"', '"This paper addresses the problem of using body gestures to control the Chinese shadow puppets with the Microsoft Kinect sensor. By analyzing the motion of the actors in the Chinese famous drama, Wusong Fights the Tiger, we propose a general framework for controlling two shadow puppets, a human model and an animal model. A performer can conduct simple actions such as turning the head, stretching the arms or kicking the legs. However, it is more difficult for a normal performer to simulate more complicated movements, for example, back flips and splits. Therefore we define some special postures to represent these difficult movements. Besides, in order to be compatible with the Chinese drama style, we use water color to paint the background scenery and the foreground characters. We show some preliminary results which demonstrate the effectiveness of this work."'),
('"Chrono-Gait Image: A Novel Temporal Template for Gait Recognition"', '"ECCV 2010"', '["Foreground Pixel", "Individual Recognition", "Baseline Algorithm", "Gait Recognition", "Period Det', '"https://doi.org/10.1007/978-3-642-15549-9_19"', '"In this paper, we propose a novel temporal template, called Chrono-Gait Image (CGI), to describe the spatio-temporal walking pattern for human identification by gait. The CGI temporal template encodes the temporal information among gait frames via color mapping to improve the recognition performance. Our method starts with the extraction of the contour in each gait image, followed by utilizing a color mapping function to encode each of gait contour images in the same gait sequence and compositing them to a single CGI. We also obtain the CGI-based real templates by generating CGI for each period of one gait sequence and utilize contour distortion to generate the CGI-based synthetic templates. In addition to independent recognition using either of individual templates, we combine the real and synthetic temporal templates for refining the performance of human recognition. Extensive experiments on the USF HumanID database indicate that compared with the recently published gait recognition approaches, our CGI-based approach attains better performance in gait recognition with considerable robustness to gait period detection."'),
('"Class-Specific, Top-Down Segmentation"', '"ECCV 2002"', '["Grouping and segmentation", "Figure-ground", "Top-down processing", "Object classification"]', '"https://doi.org/10.1007/3-540-47967-8_8"', '"In this paper we present a novel class-based segmentation method, which is guided by a stored representation of the shape of objects within a general class (such as horse images). The approach is different from bottom-up segmentation methods that primarily use the continuity of grey-level, texture, and bounding contours. We show that the method leads to markedly improved segmentation results and can deal with significant variation in shape and varying backgrounds. We discuss the relative merits of class-specific and general image-based segmentation methods and suggest how they can be usefully combined."'),
('"ClassCut for Unsupervised Class Segmentation"', '"ECCV 2010"', '["Reference Frame", "Location Model", "Object Class", "Shape Model", "Appearance Model"]', '"https://doi.org/10.1007/978-3-642-15555-0_28"', '"We propose a novel method for unsupervised class segmentation on a set of images. It alternates between segmenting object instances and learning a class model. The method is based on a segmentation energy defined over all images at the same time, which can be optimized efficiently by techniques used before in interactive segmentation. Over iterations, our method progressively learns a class model by integrating observations over all images. In addition to appearance, this model captures the location and shape of the class with respect to an automatically determined coordinate frame common across images. This frame allows us to build stronger shape and location models, similar to those used in object class detection. Our method is inspired by interactive segmentation methods [1], but it is fully automatic and learns models characteristic for the object class rather than specific to one particular object/image. We experimentally demonstrate on the Caltech4, Caltech101, and Weizmann horses datasets that our method (a) transfers class knowledge across images and this improves results compared to segmenting every image independently; (b) outperforms Grabcut [1] for the task of unsupervised segmentation; (c) offers competitive performance compared to the state-of-the-art in unsupervised segmentation and in particular it outperforms the topic model [2]."'),
('"Classification and Localisation of Diabetic-Related Eye Disease"', '"ECCV 2002"', '["Optic Disk", "Retinal Image", "Gradient Vector Flow", "Scale Conjugate Gradient", "Gradient Vector', '"https://doi.org/10.1007/3-540-47979-1_34"', '"Retinal exudates are a characteristic feature of many retinal diseases such as Diabetic Retinopathy. We address the development of a method to quantitatively diagnose these random yellow patches in colour retinal images automatically. After a colour normalisation and contrast enhancement preprocessing step, the colour retinal image is segmented using Fuzzy C-Means clustering. We then classify the segmented regions into two disjoint classes, exudates and non-exudates, comparing the performance of various classifiers. We also locate the optic disk both to remove it as a candidate region and to measure its boundaries accurately since it is a significant landmark feature for ophthalmologists. Three different approaches are reported for optic disk localisation based on template matching, least squares are estimation and snakes. The system could achieve an overall diagnostic accuracy of 90.1% for identification of the exudate pathologies and 90.7% for optic disk localisation."'),
('"Classification of Artistic Styles Using Binarized Features Derived from a Deep Neural Network"', '"ECCV 2014"', '["Local Binary Pattern", "Convolutional Neural Network", "Deep Neural Network", "Feature Fusion", "B', '"https://doi.org/10.1007/978-3-319-16178-5_5"', '"With the vast expansion of digital contemporary painting collections, automatic theme stylization has grown in demand in both academic and commercial fields. The recent interest in deep neural networks has provided powerful visual features that achieve state-of-the-art results in various visual classification tasks. In this work, we examine the perceptiveness of these features in identifying artistic styles in paintings, and suggest a compact binary representation of the paintings. Combined with the PiCoDes descriptors, these features show excellent classification results on a large scale collection of paintings."'),
('"Classifier Ensemble Recommendation"', '"ECCV 2012"', '["Action Recognition", "Weak Learner", "AdaBoost Algorithm", "Rating Store", "Model Recommendation"]', '"https://doi.org/10.1007/978-3-642-33863-2_21"', '"The problem of training classifiers from limited data is one that particularly affects large-scale and social applications, and as a result, although carefully trained machine learning forms the backbone of many current techniques in research, it sees dramatically fewer applications for end-users. Recently we demonstrated a technique for selecting or recommending a single good classifier from a large library even with highly impoverished training data. We consider alternatives for extending our recommendation technique to sets of classifiers, including a modification to the AdaBoost algorithm that incorporates recommendation. Evaluating on an action recognition problem, we present two viable methods for extending model recommendation to sets."'),
('"Classifying Images of Materials: Achieving Viewpoint and Illumination Independence"', '"ECCV 2002"', '["Greedy Algorithm", "Training Image", "Texture Class", "Viewpoint Change", "Photometric Stereo"]', '"https://doi.org/10.1007/3-540-47977-5_17"', '"In this paper we present a new approach to material classification under unknown viewpoint and illumination. Our texture model is based on the statistical distribution of clustered filter responses. However, unlike previous 3D texton representations, we use rotationally invariant filters and cluster in an extremely low dimensional space. Having built a texton dictionary, we present a novel method of classifying a single image without requiring any a priori knowledge about the viewing or illumination conditions under which it was photographed. We argue that using rotationally invariant filters while clustering in such a low dimensional space improves classification performance and demonstrate this claim with results on all 61 textures in the Columbia-Utrecht database. We then proceed to show how texture models can be further extended by compensating for viewpoint changes using weak isotropy."'),
('"Classifying Materials from Their Reflectance Properties"', '"ECCV 2004"', '["Recognition Rate", "Close Manifold", "High Recognition Rate", "Matte Material", "Zernike Polynomia', '"https://doi.org/10.1007/978-3-540-24673-2_30"', '"We explore the possibility of recognizing the surface material from a single image with unknown illumination, given the shape of the surface. Model-based PCA is used to create a low-dimensional basis to represent the images. Variations in the illumination create manifolds in the space spanned by this basis. These manifolds are learnt using captured illumination maps and the CUReT database. Classification of the material is done by finding the manifold closest to the point representing the image of the material. Testing on synthetic data shows that the problem is hard. The materials form groups where the materials in a group often are mis-classifed as one of the other materials in the group. With a grouping algorithm we find a grouping of the materials in the CUReT database. Tests on images of real materials in natural illumination settings show promising results."'),
('"Climbing: A Unified Approach for Global Constraints on Hierarchical Segmentation"', '"ECCV 2012"', '["Global Constraint", "Texture Segmentation", "Binary Energy", "Optimal Segmentation", "Partial Part', '"https://doi.org/10.1007/978-3-642-33885-4_33"', '"The paper deals with global constraints for hierarchical segmentations. The proposed framework associates, with an input image, a hierarchy of segmentations and an energy, and the subsequent optimization problem. It is the first paper that compiles the different global constraints and unifies them as Climbing energies. The transition from global optimization to local optimization is attained by the h-increasingness property, which allows to compare parent and child partition energies in hierarchies. The laws of composition of such energies are established and examples are given over the Berkeley Dataset for colour and texture segmentation."'),
('"Closed-Form Approximate CRF Training for Scalable Image Segmentation"', '"ECCV 2014"', '["Image Segmentation", "Training Image", "Conditional Random Field", "Probabilistic Inference", "Seg', '"https://doi.org/10.1007/978-3-319-10578-9_36"', '"We present LS-CRF, a new method for training cyclic Conditional Random Fields (CRFs) from large datasets that is inspired by classical closed-form expressions for the maximum likelihood parameters of a generative graphical model with tree topology. Training a CRF with LS-CRF requires only solving a set of independent regression problems, each of which can be solved efficiently in closed form or by an iterative solver. This makes LS-CRF orders of magnitude faster than classical CRF training based on probabilistic inference, and at the same time more flexible and easier to implement than other approximate techniques, such as pseudolikelihood or piecewise training. We apply LS-CRF to the task of semantic image segmentation, showing that it achieves on par accuracy to other training techniques at higher speed, thereby allowing efficient CRF training from very large training sets. For example, training a linearly parameterized pairwise CRF on 150,000 images requires less than one hour on a modern workstation."'),
('"Closed-Form Solution to Non-rigid 3D Surface Registration"', '"ECCV 2008"', '["Extend Linearization", "Individual Image", "Active Appearance Model", "Reprojection Error", "Defor', '"https://doi.org/10.1007/978-3-540-88693-8_43"', '"We present a closed-form solution to the problem of recovering the 3D shape of a non-rigid inelastic surface from 3D-to-2D correspondences. This lets us detect and reconstruct such a surface by matching individual images against a reference configuration, which is in contrast to all existing approaches that require initial shape estimates and track deformations from image to image."'),
('"Closed-Loop Adaptation for Robust Tracking"', '"ECCV 2010"', '["Current Frame", "Tracking Result", "Salient Point", "Robust Tracking", "Accurate Boundary"]', '"https://doi.org/10.1007/978-3-642-15549-9_30"', '"Model updating is a critical problem in tracking. Inaccurate extraction of the foreground and background information in model adaptation would cause the model to drift and degrade the tracking performance. The most direct but yet difficult solution to the drift problem is to obtain accurate boundaries of the target. We approach such a solution by proposing a novel closed-loop model adaptation framework based on the combination of matting and tracking. In our framework, the scribbles for matting are all automatically generated, which makes matting applicable in a tracking system. Meanwhile, accurate boundaries of the target can be obtained from matting results even when the target has large deformation. An effective model is further constructed and successfully updated based on such accurate boundaries. Extensive experiments show that our closed-loop adaptation scheme largely avoids model drift and significantly outperforms other discriminative tracking models as well as video matting approaches."'),
('"Clustering Complex Data with Group-Dependent Feature Selection"', '"ECCV 2010"', '["Feature Selection", "Local Binary Pattern", "Spectral Cluster", "Normalize Mutual Information", "W', '"https://doi.org/10.1007/978-3-642-15567-3_7"', '"We describe a clustering approach with the emphasis on detecting coherent structures in a complex dataset, and illustrate its effectiveness with computer vision applications. By complex data, we mean that the attribute variations among the data are too extensive such that clustering based on a single feature representation/descriptor is insufficient to faithfully divide the data into meaningful groups. The proposed method thus assumes the data are represented with various feature representations, and aims to uncover the underlying cluster structure. To that end, we associate each cluster with a boosting classifier derived from multiple kernel learning, and apply the cluster-specific classifier to feature selection across various descriptors to best separate data of the cluster from the rest. Specifically, we integrate the multiple, correlative training tasks of the cluster-specific classifiers into the clustering procedure, and cast them as a joint constrained optimization problem. Through the optimization iterations, the cluster structure is gradually revealed by these classifiers, while their discriminant power to capture similar data would be progressively improved owing to better data labeling."'),
('"Clustering Local Motion Estimates for Robust and Efficient Object Tracking"', '"ECCV 2014"', '["Visual object tracking", "Optical flow", "Motion-based", "Texture-less tracking"]', '"https://doi.org/10.1007/978-3-319-16181-5_17"', '"We present a new short-term tracking algorithm called Best Displacement Flow (BDF). This approach is based on the idea of \\u2018Flock of Trackers\\u2019 with two main contributions. The first contribution is the adoption of an efficient clustering approach to identify what we term the \\u2018Best Displacement\\u2019 vector, used to update the object\\u2019s bounding box. This clustering procedure is more robust than the median filter to high percentage of outliers. The second contribution is a procedure that we term \\u2018Consensus-Based Reinitialization\\u2019 used to reinitialize trackers that have previously been classified as outliers. For this reason we define a new tracker state called \\u2018transition\\u2019 used to sample new trackers in according to the current inlier trackers."'),
('"Clustering with Hypergraphs: The Case for Large Hyperedges"', '"ECCV 2014"', '["Hypergraph clustering", "model fitting", "guided sampling"]', '"https://doi.org/10.1007/978-3-319-10593-2_44"', '"The extension of conventional clustering to hypergraph clustering, which involves higher order similarities instead of pairwise similarities, is increasingly gaining attention in computer vision. This is due to the fact that many grouping problems require an affinity measure that must involve a subset of data of size more than two, i.e., a hyperedge. Almost all previous works, however, have considered the smallest possible hyperedge size, due to a lack of study into the potential benefits of large hyperedges and effective algorithms to generate them. In this paper, we show that large hyperedges are better from both theoretical and empirical standpoints. We then propose a novel guided sampling strategy for large hyperedges, based on the concept of random cluster models. Our method can generate pure large hyperedges that significantly improve grouping accuracy without exponential increases in sampling costs. In the important applications of face clustering and motion segmentation, our method demonstrates substantially better accuracy and efficiency."'),
('"Co-inference for Multi-modal Scene Analysis"', '"ECCV 2012"', '["Point Cloud", "Local Binary Pattern", "Contextual Feature", "Point Cloud Data", "Global Reference ', '"https://doi.org/10.1007/978-3-642-33783-3_48"', '"We address the problem of understanding scenes from multiple sources of sensor data (e.g., a camera and a laser scanner) in the case where there is no one-to-one correspondence across modalities (e.g., pixels and 3-D points). This is an important scenario that frequently arises in practice not only when two different types of sensors are used, but also when the sensors are not co-located and have different sampling rates. Previous work has addressed this problem by restricting interpretation to a single representation in one of the domains, with augmented features that attempt to encode the information from the other modalities. Instead, we propose to analyze all modalities simultaneously while propagating information across domains during the inference procedure. In addition to the immediate benefit of generating a complete interpretation in all of the modalities, we demonstrate that this co-inference approach also improves performance over the canonical approach."'),
('"Co-operative Multi-target Tracking and Classification"', '"ECCV 2004"', '["Bayesian Network", "Camera Calibration", "Tracking Result", "Foreground Object", "Pattern Match Al', '"https://doi.org/10.1007/978-3-540-24670-1_29"', '"This paper describes a real-time system for multi-target tracking and classification in image sequences from a single stationary camera. Several targets can be tracked simultaneously in spite of splits and merges amongst the foreground objects and presence of clutter in the segmentation results. In results we show tracking of upto 17 targets simultaneously. The algorithm combines Kalman filter-based motion and shape tracking with an efficient pattern matching algorithm. The latter facilitates the use of a dynamic programming strategy to efficiently solve the data association problem in presence of multiple splits and merges. The system is fully automatic and requires no manual input of any kind for initialization of tracking. The initialization for tracking is done using attributed graphs. The algorithm gives stable and noise free track initialization. The image based tracking results are used as inputs to a Bayesian network based classifier to classify the targets into different categories. After classification a simple 3D model for each class is used along with camera calibration to obtain 3D tracking results for the targets. We present results on a large number of real world image sequences, and accurate 3D tracking results compared with the readings from the speedometer of the vehicle. The complete tracking system including segmentation of moving targets works at about 25Hz for 352\\u00d7288 resolution color images on a 2.8 GHz pentium-4 desktop."'),
('"Co-recognition of Image Pairs by Data-Driven Monte Carlo Image Exploration"', '"ECCV 2008"', '["Reference Image", "Image Retrieval", "Image Pair", "Query Image", "Common Object"]', '"https://doi.org/10.1007/978-3-540-88693-8_11"', '"We introduce a new concept of \\u2018co-recognition\\u2019 for object-level image matching between an arbitrary image pair. Our method augments putative local region matches to reliable object-level correspondences without any supervision or prior knowledge on common objects. It provides the number of reliable common objects and the dense correspondences between the image pair. In this paper, generative model for co-recognition is presented. For inference, we propose data-driven Monte Carlo image exploration which clusters and propagates local region matches by Markov chain dynamics. The global optimum is achieved by a guiding force of our data-driven sampling and posterior probability model. In the experiments, we demonstrate the power and utility on image retrieval and unsupervised recognition and segmentation of multiple common objects."'),
('"Co-Sparse Textural Similarity for Interactive Segmentation"', '"ECCV 2014"', '["Image Segmentation", "Natural Image", "Image Patch", "Textural Similarity", "Graphic Hardware"]', '"https://doi.org/10.1007/978-3-319-10599-4_19"', '"We propose an algorithm for segmenting natural images based on texture and color information, which leverages the co-sparse analysis model for image segmentation. As a key ingredient of this method, we introduce a novel textural similarity measure, which builds upon the co-sparse representation of image patches. We propose a statistical MAP inference approach to merge textural similarity with information about color and location. Combined with recently developed convex multilabel optimization methods this leads to an efficient algorithm for interactive segmentation, which is easily parallelized on graphics hardware. The provided approach outperforms state-of-the-art interactive segmentation methods on the Graz Benchmark."'),
('"Co-transduction for Shape Retrieval"', '"ECCV 2010"', '["Unlabeled Data", "Image Search", "Retrieval Rate", "Query Object", "Database Object"]', '"https://doi.org/10.1007/978-3-642-15558-1_24"', '"In this paper, we propose a new shape/object retrieval algorithm, co-transduction. The performance of a retrieval system is critically decided by the accuracy of adopted similarity measures (distances or metrics). Different types of measures may focus on different aspects of the objects: e.g. measures computed based on contours and skeletons are often complementary to each other. Our goal is to develop an algorithm to fuse different similarity measures for robust shape retrieval through a semi-supervised learning framework. We name our method co-transduction which is inspired by the co-training algorithm [1]. Given two similarity measures and a query shape, the algorithm iteratively retrieves the most similar shapes using one measure and assigns them to a pool for the other measure to do a re-ranking, and vice-versa. Using co-transduction, we achieved a significantly improved result of 97.72% on the MPEG-7 dataset [2] over the state-of-the-art performances (91% in [3], 93.4% in [4]). Our algorithm is general and it works directly on any given similarity measures/metrics; it is not limited to object shape retrieval and can be applied to other tasks for ranking/retrieval."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Coarse Registration of Surface Patches with Local Symmetries"', '"ECCV 2002"', '["Surface Registration", "Surface geometry", "Shape"]', '"https://doi.org/10.1007/3-540-47967-8_38"', '"Most 3D recording methods generate multiple partial reconstructions that must be integrated to form a complete model. The coarse registration step roughly aligns the parts with each other. Several methods for coarse registration have been developed that are based on matching points between different parts. These methods look for interest points and use a point signature that encodes the local surface geometry to find corresponding points. We developed a technique that is complementary to these methods. Local descriptions can fail or can be highly inefficient when the surfaces contain local symmetries. In stead of discarding these regions, we introduce a method that first uses the Gaussian image to detect planar, cylindrical and conical regions and uses this information to compute the rigid motion between the patches. For combining the information from multiple regions to a single solution, we use a a Hough space that accumulates votes for candidate transformations. Due to their symmetry, they update a subspace of parameter space in stead of a single bin. Experiments on real range data from different views of the same object show that the method can find the rigid motion to put the patches in the same coordinates system."'),
('"Coarse to Fine Face Detection Based on Skin Color Adaption"', '"BioAW 2002"', '["Skin Color", "Gaussian Model", "Face Detection", "Color Distribution", "Skin Region"]', '"https://doi.org/10.1007/3-540-47917-1_12"', '"In this paper we present a skin color approach for fast and accurate face detection which combines skin color learning and image segmentation. This approach starts from a coarse segmentation which provides regions of homogeneous statistical color distribution. Some regions represent parts of human skin and are selected by minimizing an error between the color distribution of each region and the output of a compression decompression neural network, which learns skin color distribution for several populations of different ethnicity. This ANN is used to find a collection of skin regions which are used to estimate the new parameters of the Gaussian models using a 2-means fuzzy clustering in order to adapt these parameters to the context of the input image. A Bayesian framework is used to perform a finer classification and makes the skin and face detection process invariant to scale and lighting conditions. Finally, a face shape based model is used to validate or not the face hypothesis on each skin region."'),
('"Coarse-to-Fine Auto-Encoder Networks (CFAN) for Real-Time Face Alignment"', '"ECCV 2014"', '["Face Alignment", "Nonlinear", "Deep Learning", "Stacked Auto-encoder", "Coarse-to-Fine", "Real-tim', '"https://doi.org/10.1007/978-3-319-10605-2_1"', '"Accurate face alignment is a vital prerequisite step for most face perception tasks such as face recognition, facial expression analysis and non-realistic face re-rendering. It can be formulated as the nonlinear inference of the facial landmarks from the detected face region. Deep network seems a good choice to model the nonlinearity, but it is nontrivial to apply it directly. In this paper, instead of a straightforward application of deep network, we propose a Coarse-to-Fine Auto-encoder Networks (CFAN) approach, which cascades a few successive Stacked Auto-encoder Networks (SANs). Specifically, the first SAN predicts the landmarks quickly but accurately enough as a preliminary, by taking as input a low-resolution version of the detected face holistically. The following SANs then progressively refine the landmark by taking as input the local features extracted around the current landmarks (output of the previous SAN) with higher and higher resolution. Extensive experiments conducted on three challenging datasets demonstrate that our CFAN outperforms the state-of-the-art methods and performs in real-time(40+fps excluding face detection on a desktop)."'),
('"Coaxial Omnidirectional Stereopsis"', '"ECCV 2004"', '["Stereo Match", "Panoramic Image", "Perspective Projection", "Epipolar Line", "Radial Distortion"]', '"https://doi.org/10.1007/978-3-540-24673-2_29"', '"Catadioptric omnidirectional sensors, consisting of a camera and a mirror, can track objects even when their bearings change suddenly, usually due to the observer making a significant turn. There has been much debate concerning the relative merits of several possible shapes of mirrors to be used by such sensors."'),
('"Coherent Filtering: Detecting Coherent Motions from Crowd Clutters"', '"ECCV 2012"', '["Normalize Mutual Information", "Coherent Motion", "Velocity Correlation", "Motion Segmentation", "', '"https://doi.org/10.1007/978-3-642-33709-3_61"', '"Coherent motions, which describe the collective movements of individuals in crowd, widely exist in physical and biological systems. Understanding their underlying priors and detecting various coherent motion patterns from background clutters have both scientific values and a wide range of practical applications, especially for crowd motion analysis. In this paper, we propose and study a prior of coherent motion called Coherent Neighbor Invariance, which characterizes the local spatiotemporal relationships of individuals in coherent motion. Based on the coherent neighbor invariance, a general technique of detecting coherent motion patterns from noisy time-series data called Coherent Filtering is proposed. It can be effectively applied to data with different distributions at different scales in various real-world problems, where the environments could be sparse or extremely crowded with heavy noise. Experimental evaluation and comparison on synthetic and real data show the existence of Coherence Neighbor Invariance and the effectiveness of our Coherent Filtering."'),
('"Collaborative Facial Landmark Localization for Transferring Annotations Across Datasets"', '"ECCV 2014"', '["Target Image", "Target Face", "Active Appearance Model", "Facial Landmark", "Target Dataset"]', '"https://doi.org/10.1007/978-3-319-10599-4_6"', '"In this paper we make the first effort, to the best of our knowledge, to combine multiple face landmark datasets with different landmark definitions into a super dataset, with a union of all landmark types computed in each image as output. Our approach is flexible, and our system can optionally use known landmarks in the target dataset to constrain the localization. Our novel pipeline is built upon variants of state-of-the-art facial landmark localization methods. Specifically, we propose to label images in the target dataset jointly rather than independently and exploit exemplars from both the source datasets and the target dataset. This approach integrates nonparametric appearance and shape modeling and graph matching together to achieve our goal."'),
('"CollageParsing: Nonparametric Scene Parsing by Adaptive Overlapping Windows"', '"ECCV 2014"', '["image parsing", "semantic segmentation", "scene understanding"]', '"https://doi.org/10.1007/978-3-319-10599-4_33"', '"Scene parsing is the problem of assigning a semantic label to every pixel in an image. Though an ambitious task, impressive advances have been made in recent years, in particular in scalable nonparametric techniques suitable for open-universe databases. This paper presents the CollageParsing algorithm for scalable nonparametric scene parsing. In contrast to common practice in recent nonparametric approaches, CollageParsing reasons about mid-level windows that are designed to capture entire objects, instead of low-level superpixels that tend to fragment objects. On a standard benchmark consisting of outdoor scenes from the LabelMe database, CollageParsing achieves state-of-the-art nonparametric scene parsing results with 7 to 11% higher average per-class accuracy than recent nonparametric approaches."'),
('"Collective Activity Localization with Contextual Spatial Pyramid"', '"ECCV 2012"', '["Collective Activity", "Activity Localization", "Activity Recognition", "Activity Category", "Spati', '"https://doi.org/10.1007/978-3-642-33885-4_25"', '"In this paper, we propose an activity localization method with contextual information of person relationships. Activity localization is a task to determine \\u201cwho participates to an activity group\\u201d, such as detecting \\u201cwalking in a group\\u201d or \\u201ctalking in a group\\u201d. Usage of contextual information has been providing promising results in the previous activity recognition methods, however, the contextual information has been limited to the local information extracted from one person or only two people relationship. We propose a new context descriptor named \\u201ccontextual spatial pyramid model (CSPM)\\u201d, which represents the global relationships extracted from the whole of activities in single images. CSPM encodes useful relationships for activity localization, such as \\u201cfacing each other\\u201d. The experimental result shows CSPM improve activity localization performance, therefore CSPM provides strong contextual cues for activity recognition in complex scenes."'),
('"Color and Scale: The Spatial Structure of Color Images"', '"ECCV 2000"', '["Color Image", "Human Visual System", "Spectral Energy Distribution", "Color Match", "Shadow Bounda', '"https://doi.org/10.1007/3-540-45054-8_22"', '"For grey-value images, it is well accepted that the neighborhood rather than the pixel carries the geometrical interpretation. Interestingly the spatial configuration of the neighborhood is the basis for the perception of humans. Common practise in color image processing, is to use the color information without considering the spatial structure. We aim at a physical basis for the local interpretation of color images. We propose a framework for spatial color measurement, based on the Gaussian scale-space theory. We consider a Gaussian color model, which inherently uses the spatial and color information in an integrated model. The framework is well-founded in physics as well as in measurement science. The framework delivers sound and robust spatial color invariant features. The usefulness of the proposed measurement framework is illustrated by edge detection, where edges are discriminated as shadow, highlight, or object boundary. Other applications of the framework include color invariant image retrieval and color constant edge detection."'),
('"Color Barcode Decoding in the Presence of Specular Reflection"', '"ECCV 2014"', '["Color barcode decoding", "Dichromatic reflection model", "Subspace classification"]', '"https://doi.org/10.1007/978-3-319-16199-0_19"', '"Color barcodes enable higher information density with respect to traditional black and white barcodes. Existing technologies use small color palettes and display the colors in the palette in the barcode itself for easy and robust decoding. This solution comes at the cost of reduced information density due to the fact that the displayed reference colors cannot be used to encode information. We introduce a new approach to color barcode decoding that uses a relatively large palettes (up to 24 colors) and a small number of reference colors (2 to 6) to be displayed in a barcode. Our decoding method specifically accounts for specular reflections using a dichromatic model. The experimental results show that our decoding algorithm achieves higher information rate with a very low probability of decoding error compared to previous approaches that use a color palette for decoding."'),
('"Color Constancy Using Local Color Shifts"', '"ECCV 2004"', '["Processing Element", "Human Visual System", "Color Constancy", "Local Color", "Current Pixel"]', '"https://doi.org/10.1007/978-3-540-24672-5_22"', '"The human visual system is able to correctly determine the color of objects in view irrespective of the illuminant. This ability to compute color constant descriptors is known as color constancy. We have developed a parallel algorithm for color constancy. This algorithm is based on the computation of local space average color using a grid of processing elements. We have one processing element per image pixel. Each processing element has access to the data stored in neighboring elements. Local space average color is used to shift the color of the input pixel in the direction of the gray vector. The computations are executed inside the unit color cube. The color of the input pixel as well as local space average color is simply a vector inside this Euclidean space. We compute the component of local space average color which is orthogonal to the gray vector. This component is subtracted from the color of the input pixel to compute a color corrected image. Before performing the color correction step we can also normalize both colors. In this case, the resulting color is rescaled to the original intensity of the input color such that the image brightness remains unchanged."'),
('"Color Constancy Using Single Colors"', '"ECCV 2012"', '["Color Constancy", "Single Color", "Colored Patch", "Diagonal Mapping", "White Surface"]', '"https://doi.org/10.1007/978-3-642-33868-7_39"', '"This work investigates if the von Kries adaptation can be generalized to deal with single colored patches. We investigate which colored patches can give statistically equivalent performance to a white patch for von Kries adaptation. The investigation is then extended to couples of colors, and the analysis of the characteristics of the colors forming the couples is carried out. We focus here on single and couples of colors since common objects and logos are usually composed by a small number of colors."'),
('"Color Constancy, Intrinsic Images, and Shape Estimation"', '"ECCV 2012"', '["Color Constancy", "Natural Illumination", "Laplacian Pyramid", "Shadow Removal", "Intrinsic Image"', '"https://doi.org/10.1007/978-3-642-33765-9_5"', '"We present SIRFS (shape, illumination, and reflectance from shading), the first unified model for recovering shape, chromatic illumination, and reflectance from a single image. Our model is an extension of our previous work [1], which addressed the achromatic version of this problem. Dealing with color requires a modified problem formulation, novel priors on reflectance and illumination, and a new optimization scheme for dealing with the resulting inference problem. Our approach outperforms all previously published algorithms for intrinsic image decomposition and shape-from-shading on the MIT intrinsic images dataset [1, 2] and on our own \\u201cnaturally\\u201d illuminated version of that dataset."'),
('"Color Invariant SURF in Discriminative Object Tracking"', '"ECCV 2010"', '["tracking", "surf", "color", "invariant"]', '"https://doi.org/10.1007/978-3-642-35740-4_6"', '"Tracking can be seen as an online learning problem, where the focus is on discriminating object from background. From this point of view, features play a key role as the tracking accuracy depends on how well the feature distinguishes object and background. Current discriminative trackers use traditional features such as intensity, RGB and full body shape features. In this paper, we propose to use color invariant SURF features in the discriminative tracking. This set of invariant features has been shown to be of increased invariance and discriminative power. The resulting tracker inherits a good discrimination between object and background while keeping advantages of the discriminative tracking framework. Experiments on a dataset of 80 videos covering a wide range of tracking circumstances show that the tracker is robust to changes in object appearance, lighting conditions and able to track objects under cluttered scenes and partial occlusion."'),
('"Color-Based Probabilistic Tracking"', '"ECCV 2002"', '["Gaussian Mixture Model", "Color Histogram", "Visual Tracking", "Multiple Object Tracking", "Face T', '"https://doi.org/10.1007/3-540-47969-4_44"', '"Color-based trackers recently proposed in [3,4,5] have been proved robust and versatile for a modest computational cost. They are especially appealing for tracking tasks where the spatial structure of the tracked objects exhibits such a dramatic variability that trackers based on a space-dependent appearance reference would break down very fast. Trackers in [3,4,5] rely on the deterministic search of a window whose color content matches a reference histogram color model."'),
('"Color-Constant Information Embedding"', '"ECCV 2010"', '["Reference Surface", "Bilinear Model", "Rendering Error", "Unknown Surface", "Color Compensation"]', '"https://doi.org/10.1007/978-3-642-35740-4_2"', '"We propose a technique to embed information in the color of a printed surface. One or more reference surfaces are used to help compensate for the color changes due to varying illuminants. Seven different techniques, some of which are novel, are considered for color compensation. Experiments using different performance metrics are presented, providing a comparative assessment of the various algorithms and highlighting the importance of the correct choice of reference surfaces."'),
('"Coloring Local Feature Extraction"', '"ECCV 2006"', '["Shape Descriptor", "Sift Descriptor", "Color Descriptor", "Opponent Color", "Spherical Angle"]', '"https://doi.org/10.1007/11744047_26"', '"Although color is commonly experienced as an indispensable quality in describing the world around us, state-of-the art local feature-based representations are mostly based on shape description, and ignore color information. The description of color is hampered by the large amount of variations which causes the measured color values to vary significantly. In this paper we aim to extend the description of local features with color information. To accomplish a wide applicability of the color descriptor, it should be robust to : 1. photometric changes commonly encountered in the real world, 2. varying image quality, from high quality images to snap-shot photo quality and compressed internet images. Based on these requirements we derive a set of color descriptors. The set of proposed descriptors are compared by extensive testing on multiple applications areas, namely, matching, retrieval and classification, and on a wide variety of image qualities. The results show that color descriptors remain reliable under photometric and geometrical changes, and with decreasing image quality. For all experiments a combination of color and shape outperforms a pure shape-based approach."'),
('"Colorization for Single Image Super Resolution"', '"ECCV 2010"', '["Markov Random Field", "IEEE Conf", "Seed Point", "Color Assignment", "Super Resolution"]', '"https://doi.org/10.1007/978-3-642-15567-3_24"', '"This paper introduces a new procedure to handle color in single image super resolution (SR). Most existing SR techniques focus primarily on enforcing image priors or synthesizing image details; less attention is paid to the final color assignment. As a result, many existing SR techniques exhibit some form of color aberration in the final upsampled image. In this paper, we outline a procedure based on image colorization and back-projection to perform color assignment guided by the super-resolution luminance channel. We have found that our procedure produces better results both quantitatively and qualitatively than existing approaches. In addition, our approach is generic and can be incorporated into any existing SR techniques."'),
('"Colour by Correlation in a Three-Dimensional Colour Space"', '"ECCV 2000"', '["Specular Reflection", "Discretization Problem", "Colour Constancy", "Pixel Brightness", "Brightnes', '"https://doi.org/10.1007/3-540-45054-8_25"', '"We improve the promising Colour by Correlation method for computational colour constancy by modifying it to work in a three dimensional colour space. The previous version of the algorithm uses only the chromaticity of the input, and thus cannot make use of the information inherent in the pixel brightness which previous work suggests is useful. We develop the algorithm for the Mondrian world (matte surfaces), the Mondrian world with fluorescent surfaces, and the Mondrian world with specularities. We test the new algorithm on synthetic data, and on a data set of 321 carefully calibrated images. We find that on the synthetic data, the new algorithm significantly out-performs all other colour constancy algorithms. In the case of image data, the results are also promising. The new algorithm does significantly better than its chromaticity counter-part, and its performance approaches that of the best algorithms. Since the research into the method is still young, we are hopeful that the performance gap between the real and synthetic case can be narrowed."'),
('"Colour Image Retrieval and Object Recognition Using the Multimodal Neighbourhood Signature"', '"ECCV 2000"', '["Object Recognition", "Image Retrieval", "Invariant Feature", "Illumination Change", "Partial Occlu', '"https://doi.org/10.1007/3-540-45054-8_4"', '"A novel approach to colour-based object recognition and image retrieval -the multimodal neighbourhood signature- is proposed. Object appearance is represented by colour-based features computed from image neighbourhoods with multi-modal colour density function. Stable invariants are derived from modes of the density function that are robustly located by the mean shift algorithm. The problem of extracting local invariant colour features is addressed directly, without a need for prior segmentation or edge detection. The signature is concise - an image is typically represented by a few hundred bytes, a few thousands for very complex scenes."'),
('"Colour Texture Segmentation by Region-Boundary Cooperation"', '"ECCV 2004"', '["Image Segmentation", "Texture Feature", "Machine Intelligence", "Kernel Density Estimation", "Colo', '"https://doi.org/10.1007/978-3-540-24671-8_20"', '"A colour texture segmentation method which unifies region and boundary information is presented in this paper. The fusion of several approaches which integrate both information sources allows us to exploit the benefits of each one. We propose a segmentation method which uses a coarse detection of the perceptual (colour and texture) edges of the image to adequately place and initialise a set of active regions. Colour texture of regions is modelled by the conjunction of non-parametric techniques of kernel density estimation, which allow to estimate the colour behaviour, and classical co-occurrence matrix based texture features. When the region information is defined, accurate boundary information can be extracted. Afterwards, regions concurrently compete for the image pixels in order to segment the whole image taking both information sources into account. In contrast with other approaches, our method achieves relevant results on images with regions with the same texture and different colour (as well as with regions with the same colour and different texture), demonstrating the performance of our proposal. Furthermore, the method has been quantitatively evaluated and compared on a set of mosaic images, and results on real images are shown and analysed."'),
('"Combined Motion Estimation and Reconstruction in Tomography"', '"ECCV 2012"', '["Computed tomography", "motion correction"]', '"https://doi.org/10.1007/978-3-642-33863-2_2"', '"If objects or patients move during a CT scan, reconstructions suffer from severe motion artifacts. Time dependent computed tomography (4DCT) tries to minimize these artifacts by estimating motion and/or reconstruction simultaneously. Most current methods assume a known deformation or a reconstruction without artifacts at a certain time point. This work explores the possibilities of estimating the motion model and reconstruction simultaneously. It does so by modifying the simultaneous iterative reconstruction technique (SIRT) to incorporate motion (trans-SIRT) and uses this method in an optimization routine that computes motion and reconstruction at the same time. Results show that the optimization routine is able to estimate motion accurately, assuming only the type of parametrization for the motion model. Our approach can potentially be extended to more complex motion models."'),
('"Combining Appearance and Topology for Wide Baseline Matching"', '"ECCV 2002"', '["Interest Point", "String Match", "Left Image", "Cyclic Order", "Longe Common Subsequence"]', '"https://doi.org/10.1007/3-540-47969-4_5"', '"The problem of establishing image-to-image correspondences is fundamental in computer vision. Recently, several wide baseline matching algorithms capable of handling large changes of viewpoint have appeared. By computing feature values from image data, these algorithms mainly use appearance as a cue for matching. Topological information, i.e. spatial relations between features, has also been used, but not nearly to the same extent as appearance. In this paper, we incorporate topological constraints into an existing matching algorithm [1] which matches image intensity profiles between interest points. We show that the algorithm can be improved by exploiting the constraint that the intensity profiles around each interest point should be cyclically ordered. String matching techniques allows for an efficient implementation of the ordering constraint. Experiments with real data indicate that the modified algorithm indeed gives superior results to the original one. The method of enforcing the spatial constraints is not limited to the presented case, but can be used on any algorithm where interest point correspondences are sought."'),
('"Combining Elastic and Statistical Models of Appearance Variation"', '"ECCV 2000"', '["Face Image", "Image Registration", "Local Deformation", "Model Point", "Appearance Model"]', '"https://doi.org/10.1007/3-540-45054-8_10"', '"We propose a model of appearance and a matching method which combines \\u2018global\\u2019 models (in which a few parameters control global appearance) with local elastic or optical-flow-based methods, in which deformation is described by many local parameters together with some regularisation constraints. We use an Active Appearance Model (AAM) as the global model, which can match a statistical model of appearance to a new image rapidly. However, the amount of variation allowed is constrained by the modes of the model, which may be too restrictive (for instance when insufficient training examples are available, or the number of modes is deliberately truncated for effciency or memory conservation). To compensate for this, after global AAM convergence, we allow further local model deformation, driven by local AAMs around each model node. This is analogous to optical flow or \\u2018demon\\u2019 methods of non-linear image registration. We describe the technique in detail, and demonstrate that allowing this extra freedom can improve the accuracy of object location with only a modest increase in search time. We show the combined method is more accurate than either pure local or pure global model search."'),
('"Combining Geometric and Appearance Priors for Robust Homography Estimation"', '"ECCV 2010"', '["Gaussian Mixture Model", "Model Point", "Candidate Selection", "Repetitive Pattern", "Potential Ma', '"https://doi.org/10.1007/978-3-642-15558-1_5"', '"The homography between pairs of images are typically computed from the correspondence of keypoints, which are established by using image descriptors. When these descriptors are not reliable, either because of repetitive patterns or large amounts of clutter, additional priors need to be considered. The Blind PnP algorithm makes use of geometric priors to guide the search for matches while computing camera pose. Inspired by this, we propose a novel approach for homography estimation that combines geometric priors with appearance priors of ambiguous descriptors. More specifically, for each point we retain its best candidates according to appearance. We then prune the set of potential matches by iteratively shrinking the regions of the image that are consistent with the geometric prior. We can then successfully compute homographies between pairs of images containing highly repetitive patterns and even under oblique viewing conditions."'),
('"Combining Geometric- and View-Based Approaches for Articulated Pose Estimation"', '"ECCV 2004"', '["Tracking Algorithm", "Support Point", "Rigid Motion", "Appearance Model", "Spherical Joint"]', '"https://doi.org/10.1007/978-3-540-24672-5_15"', '"In this paper we propose an efficient real-time approach that combines vision-based tracking and a view-based model to estimate the pose of a person. We introduce an appearance model that contains views of a person under various articulated poses. The appearance model is built and updated online. The main contribution consists of modeling, in each frame, the pose changes as a linear transformation of the view change. This linear model allows (i) for predicting the pose in a new image, and (ii) for obtaining a better estimate of the pose corresponding to a key frame. Articulated pose is computed by merging the estimation provided by the tracking-based algorithm and the linear prediction given by the view-based model."'),
('"Combining Language Sources and Robust Semantic Relatedness for Attribute-Based Knowledge Transfer"', '"ECCV 2010"', '["Knowledge Transfer", "Semantic Relatedness", "Object Class", "Language Source", "Solid Blue Curve"', '"https://doi.org/10.1007/978-3-642-35749-7_2"', '"Knowledge transfer between object classes has been identified as an important tool for scalable recognition. However, determining which knowledge to transfer where remains a key challenge. While most approaches employ varying levels of human supervision, we follow the idea of mining linguistic knowledge bases to automatically infer transferable knowledge. In contrast to previous work, we explicitly aim to design robust semantic relatedness measures and to combine different language sources for attribute-based knowledge transfer. On the challenging Animals with Attributes (AwA) data set, we report largely improved attribute-based zero-shot object class recognition performance that matches the performance of human supervision."'),
('"Combining Monocular Geometric Cues with Traditional Stereo Cues for Consumer Camera Stereo"', '"ECCV 2012"', '["narrow baseline stereo", "consumer stereo camera"]', '"https://doi.org/10.1007/978-3-642-33868-7_11"', '"This paper presents an algorithm for considering both stereo cues and structural priors to obtain a geometrically representative depth map from a narrow baseline stereo pair. We use stereo pairs captured with a consumer stereo camera and observe that traditional depth estimation using stereo matching techniques encounters difficulties related to the narrow baseline relative to the depth of the scene. However, monocular geometric cues based on attributes such as lines and the horizon provide additional hints about the global structure that stereo matching misses. We merge both monocular and stereo matching features in a piecewise planar reconstruction framework that is initialized with a discrete inference step, and refined with a continuous optimization to encourage the intersections of hypothesized planes to coincide with observed image lines. We show through our results on stereo pairs of manmade structures captured outside of the lab that our algorithm exploits the advantages of both approaches to infer a better depth map of the scene."'),
('"Combining Per-frame and Per-track Cues for Multi-person Action Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_9"', '"We propose a model to combine per-frame and per-track cues for action recognition. With multiple targets in a scene, our model simultaneously captures the natural harmony of an individual\\u2019s action in a scene and the flow of actions of an individual in a video sequence, inferring valid tracks in the process. Our motivation is based on the unlikely discordance of an action in a structured scene, both at the track level and the frame level (e.g., a person dancing in a crowd of joggers). While we can utilize sampling approaches for inference in our model, we instead devise a global inference algorithm by decomposing the problem and solving the subproblems exactly and efficiently, recovering a globally optimal joint solution in several cases. Finally, we improve on the state-of-the-art action recognition results for two publicly available datasets."'),
('"Combining Semi-autonomous Navigation with Manned Behaviour in a Cooperative Driving System for Mobi', '"ECCV 2014"', '["Human-robot interaction", "Mobile robotic telepresence", "Teleoperation", "User interfaces"]', '"https://doi.org/10.1007/978-3-319-16220-1_2"', '"This paper presents an image-based cooperative driving system for telepresence robot, which allows safe operation in indoor environments and is meant to minimize the burden on novice users operating the robot. The paper focuses on one emerging telepresence robot, namely, mobile remote presence systems for social interaction. Such systems brings new opportunities for applications in healthcare and elderly care by allowing caregivers to communicate with patients and elderly from remote locations. However, using such systems can be a difficult task particularly for caregivers without proper training. The paper presents a first implementation of a vision-based cooperative driving enhancement to a telepresence robot. A preliminary evaluation in the laboratory environment is presented."'),
('"Combining Simple Discriminators for Object Discrimination"', '"ECCV 2002"', '["Feature Space", "Training Image", "Voronoi Diagram", "Image Space", "Exponential Family"]', '"https://doi.org/10.1007/3-540-47977-5_51"', '"We propose to combine simple discriminators for object discrimination under the maximum entropy framework or equivalently under the maximum likelihood framework for the exponential family. The duality between the maximum entropy framework and maximum likelihood framework allows us to relate two selection criteria for the discriminators that were proposed in the literature. We illustrate our approach by combining nearest prototype discriminators that are simple to implement and widely applicable as they can be constructed in any feature space with a distance function. For efficient run-time performance we adapt the work on \\u201calternating trees\\u201d for multi-class discrimination tasks. We report results on a multi-class discrimination task in which significant gains in performance are seen by combining discriminators under our framework from a variety of easy to construct feature spaces."'),
('"Combining Simple Models to Approximate Complex Dynamics"', '"SMVP 2004"', '["Exact Inference", "Human Body Part", "Constituent Model", "Observation Likelihood", "Simple Dynami', '"https://doi.org/10.1007/978-3-540-30212-4_9"', '"Stochastic tracking of structured models in monolithic state spaces often requires modeling complex distributions that are difficult to represent with either parametric or sample-based approaches. We show that if redundant representations are available, the individual state estimates may be improved by combining simpler dynamical systems, each of which captures some aspect of the complex behavior. For example, human body parts may be robustly tracked individually, but the resulting pose combinations may not satisfy articulation constraints. Conversely, the results produced by full-body trackers satisfy such constraints, but such trackers are usually fragile due to the presence of clutter. We combine constituent dynamical systems in a manner similar to a Product of HMMs model. Hidden variables are introduced to represent system appearance. While the resulting model contains loops, making the inference hard in general, we present an approximate non-loopy filtering algorithm based on sequential application of Belief Propagation to acyclic subgraphs of the model."'),
('"Combining Textural and Geometrical Descriptors for Scene Recognition"', '"ECCV 2012"', '["2D/3D description", "feature fusion", "localization"]', '"https://doi.org/10.1007/978-3-642-33868-7_4"', '"Local description of images is a common technique in many computer vision related research. Due to recent improvements in RGB-D cameras, local description of 3D data also becomes practical. The number of studies that make use of this extra information is increasing. However, their applicabilities are limited due to the need for generic combination methods. In this paper, we propose combining textural and geometrical descriptors for scene recognition of RGB-D data. The methods together with the normalization stages proposed in this paper can be applied to combine any descriptors obtained from 2D and 3D domains. This study represents and evaluates different ways of combining multi-modal descriptors within the BoW approach in the context of indoor scene localization. Query\\u2019s rough location is determined from the pre-recorded images and depth maps in an unsupervised image matching manner."'),
('"Compact Video Description for Copy Detection with Precise Temporal Alignment"', '"ECCV 2010"', '["Video Frame", "Average Precision", "Interest Point", "Dynamic Time Warping", "Local Descriptor"]', '"https://doi.org/10.1007/978-3-642-15549-9_38"', '"This paper introduces a very compact yet discriminative video description, which allows example-based search in a large number of frames corresponding to thousands of hours of video. Our description extracts one descriptor per indexed video frame by aggregating a set of local descriptors. These frame descriptors are encoded using a time-aware hierarchical indexing structure. A modified temporal Hough voting scheme is used to rank the retrieved database videos and estimate segments in them that match the query. If we use a dense temporal description of the videos, matched video segments are localized with excellent precision."'),
('"Comparative Analysis of Kernel Methods for Statistical Shape Learning"', '"CVAMIA 2006"', '["Feature Space", "Kernel Method", "Locally Linear Embedding", "Signed Distance Function", "Kernel S', '"https://doi.org/10.1007/11889762_9"', '"Prior knowledge about shape may be quite important for image segmentation. In particular, a number of different methods have been proposed to compute the statistics on a set of training shapes, which are then used for a given image segmentation task to provide the shape prior. In this work, we perform a comparative analysis of shape learning techniques such as linear PCA, kernel PCA, locally linear embedding and propose a new method, kernelized locally linear embedding for doing shape analysis. The surfaces are represented as the zero level set of a signed distance function and shape learning is performed on the embeddings of these shapes. We carry out some experiments to see how well each of these methods can represent a shape, given the training set."'),
('"Comparative Evaluation of Binary Features"', '"ECCV 2012"', '["binary features", "comparison", "evaluation"]', '"https://doi.org/10.1007/978-3-642-33709-3_54"', '"Performance evaluation of salient features has a long-standing tradition in computer vision. In this paper, we fill the gap of evaluation for the recent wave of binary feature descriptors, which aim to provide robustness while achieving high computational efficiency. We use established metrics to embed our assessment into the body of existing evaluations, allowing us to provide a novel taxonomy unifying both traditional and novel binary features. Moreover, we analyze the performance of different detector and descriptor pairings, which are often used in practice but have been infrequently analyzed. Additionally, we complement existing datasets with novel data testing for illumination change, pure camera rotation, pure scale change, and the variety present in photo-collections. Our performance analysis clearly demonstrates the power of the new class of features. To benefit the community, we also provide a website for the automatic testing of new description methods using our provided metrics and datasets ( www.cs.unc.edu/feature-evaluation )."'),
('"Comparing Ensembles of Learners: Detecting Prostate Cancer from High Resolution MRI"', '"CVAMIA 2006"', '["Receiver Operating Characteristic Curve", "Ensemble Method", "Training Instance", "Weighted Linear', '"https://doi.org/10.1007/11889762_3"', '"While learning ensembles have been widely used for various pattern recognition tasks, surprisingly, they have found limited application in problems related to medical image analysis and computer-aided diagnosis (CAD). In this paper we investigate the performance of several state-of-the-art machine-learning methods on a CAD method for detecting prostatic adenocarcinoma from high resolution (4 Tesla) ex vivo MRI studies. A total of 14 different feature ensemble methods from 4 different families of ensemble methods were compared: Bayesian learning, Boosting, Bagging, and the k-Nearest Neighbor (kNN) classifier. Quantitative comparison of the methods was done on a total of 33 2D sections obtained from 5 different 3D MRI prostate studies. The tumor ground truth was determined on histologic sections and the regions manually mapped onto the corresponding individual MRI slices. All methods considered were found to be robust to changes in parameter settings and showed significantly less classification variability compared to inter-observer agreement among 5 experts. The kNN classifier was the best in terms of accuracy and ease of training, thus validating the principle of Occam\\u2019s Razor. The success of a simple non-parametric classifier requiring minimal training is significant for medical image analysis applications where large amounts of training data are usually unavailable."'),
('"Comparing Intensity Transformations and Their Invariants in the Context of Color Pattern Recognitio', '"ECCV 2002"', '["Canonical Variable", "Invariant Feature", "Model Selection Criterion", "Color Band", "Color Consta', '"https://doi.org/10.1007/3-540-47979-1_30"', '"In this paper we compare different ways of representing the photometric changes in image intensities caused by changes in illumination and viewpoint, aiming at a balance between goodness-of-fit and low complexity. We derive invariant features based on generalized color moment invariants - that can deal with geometric and photometric changes of a planar pattern - corresponding to the chosen photometric models. The geometric changes correspond to a perspective skew. We compare the photometric models also in terms of the invariants\\u2019 discriminative power and classification performance in a pattern recognition system."'),
('"Comparing Salient Object Detection Results without Ground Truth"', '"ECCV 2014"', '["Input Image", "Salient Object", "Salient Region", "Saliency Detection", "Visual Saliency"]', '"https://doi.org/10.1007/978-3-319-10578-9_6"', '"A wide variety of methods have been developed to approach the problem of salient object detection. The performance of these methods is often image-dependent. This paper aims to develop a method that is able to select for an input image the best salient object detection result from many results produced by different methods. This is a challenging task as different salient object detection results need to be compared without any ground truth. This paper addresses this challenge by designing a range of features to measure the quality of salient object detection results. These features are then used in various machine learning algorithms to rank different salient object detection results. Our experiments show that our method is promising for ranking salient object detection results and our method is also able to pick the best salient object detection result such that the overall salient object detection performance is better than each individual method."'),
('"Comparison of Dense Stereo Using CUDA"', '"ECCV 2010"', '["Shared Memory", "Global Memory", "Thread Block", "Epipolar Line", "Cost Aggregation"]', '"https://doi.org/10.1007/978-3-642-35740-4_31"', '"In this paper, a local and a global dense stereo matching method, implemented using Compute Unified Device Architecture (CUDA), are presented, analyzed and compared. The purposed work shows the general strategy of the parallelization of matching methods on GPUs and the tradeoff between accuracy and run-time on current GPU hardware. Two representative and widely-used methods, the Sum of Absolute Differences (SAD) method and the Semi-Global Matching (SGM) method, are used and their results are compared using the Middlebury test sets."'),
('"Comparison of Energy Minimization Algorithms for Highly Connected Graphs"', '"ECCV 2006"', '["Ground Truth", "Belief Propagation", "Message Passing", "Stereo Match", "Sequential Schedule"]', '"https://doi.org/10.1007/11744047_1"', '"Algorithms for discrete energy minimization play a fundamental role for low-level vision. Known techniques include graph cuts, belief propagation (BP) and recently introduced tree-reweighted message passing (TRW). So far, the standard benchmark for their comparison has been a 4-connected grid-graph arising in pixel-labelling stereo. This minimization problem, however, has been largely solved: recent work shows that for many scenes TRW finds the global optimum. Furthermore, it is known that a 4-connected grid-graph is a poor stereo model since it does not take occlusions into account."'),
('"Complex Bingham Distribution for Facial Feature Detection"', '"ECCV 2012"', '["Facial Feature", "Local Binary Pattern", "Texture Model", "Active Appearance Model", "Active Shape', '"https://doi.org/10.1007/978-3-642-33868-7_33"', '"We present a novel method for facial feature point detection on images captured from severe uncontrolled environments based on a combination of regularized boosted classifiers and mixture of complex Bingham distributions. The complex Bingham distribution is a rotation-invariant shape representation that can handle pose, in-plane rotation and occlusion better than existing models. Additionally, we regularized a boosted classifier with a variance normalization factor to reduce false positives. Using the proposed two models, we formulate our facial features detection approach in a Bayesian framework of a maximum a-posteriori estimation. This approach allows for the inclusion of the uncertainty of the regularized boosted classifier and complex Bingham distribution. The proposed detector is tested on different datasets and results show comparable performance to the state-of-the-art with the BioID database and outperform them in uncontrolled datasets."'),
('"Complex Events Detection Using Data-Driven Concepts"', '"ECCV 2012"', '["Action Recognition", "Independent Component Analysis", "Hide Unit", "Sparse Code", "Mean Average P', '"https://doi.org/10.1007/978-3-642-33712-3_52"', '"Automatic event detection in a large collection of unconstrained videos is a challenging and important task. The key issue is to describe long complex video with high level semantic descriptors, which should find the regularity of events in the same category while distinguish those from different categories. This paper proposes a novel unsupervised approach to discover data-driven concepts from multi-modality signals (audio, scene and motion) to describe high level semantics of videos. Our methods consists of three main components: we first learn the low-level features separately from three modalities. Secondly we discover the data-driven concepts based on the statistics of learned features mapped to a low dimensional space using deep belief nets (DBNs). Finally, a compact and robust sparse representation is learned to jointly model the concepts from all three modalities. Extensive experimental results on large in-the-wild dataset show that our proposed method significantly outperforms state-of-the-art methods."'),
('"Complex Filters Applied to Fingerprint Images Detecting Prominent Symmetry Points Used for Alignmen', '"BioAW 2002"', '["Global Structure", "Equal Error Rate", "Fingerprint Image", "Symmetry Detection", "Gaussian Pyrami', '"https://doi.org/10.1007/3-540-47917-1_5"', '"For the alignment of two fingerprints position of certain landmarks are needed. These should be automatically extracted with low misidentification rate. As landmarks we suggest the prominent symmetry points (core-points) in the fingerprint. They are extracted from the complex orientation field estimated from the global structure of the fingerprint, i.e. the overall pattern of the ridges and valleys. Complex filters, applied to the orientation field in multiple resolution scales, are used to detect the symmetry and the type of symmetry. Experimental results are reported."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Composite Texture Descriptions"', '"ECCV 2002"', '["Texture Synthesis", "Texture Model", "Neighborhood System", "Verbatim Copying", "Single Texture"]', '"https://doi.org/10.1007/3-540-47977-5_12"', '"Textures can often more easily be described as a composition of subtextures than as a single texture. The paper proposes a way to model and synthesize such \\u201ccomposite textures\\u201d, where the layout of the different subtextures is itself modeled as a texture, which can be generated automatically. Examples are shown for building materials with an intricate structure and for the automatic creation of landscape textures. First, a model of the composite texture is generated. This procedure comprises manual or unsupervised texture segmentation to learn the spatial layout of the composite texture and the extraction of models for each of the subtextures. Synthesis of a composite texture includes the generation of a layout texture, which is subsequently filled in with the appropriate subtextures. This scheme is refined further by also including interactions between neighboring subtextures."'),
('"Compressive Acquisition of Dynamic Scenes"', '"ECCV 2010"', '["Compressive Sense", "State Sequence", "Linear Dynamical System", "Video Model", "Dynamic Scene"]', '"https://doi.org/10.1007/978-3-642-15549-9_10"', '"Compressive sensing (CS) is a new approach for the acquisition and recovery of sparse signals and images that enables sampling rates significantly below the classical Nyquist rate. Despite significant progress in the theory and methods of CS, little headway has been made in compressive video acquisition and recovery. Video CS is complicated by the ephemeral nature of dynamic events, which makes direct extensions of standard CS imaging architectures and signal models infeasible. In this paper, we develop a new framework for video CS for dynamic textured scenes that models the evolution of the scene as a linear dynamical system (LDS). This reduces the video recovery problem to first estimating the model parameters of the LDS from compressive measurements, from which the image frames are then reconstructed. We exploit the low-dimensional dynamic parameters (the state sequence) and high-dimensional static parameters (the observation matrix) of the LDS to devise a novel compressive measurement strategy that measures only the dynamic part of the scene at each instant and accumulates measurements over time to estimate the static parameters. This enables us to considerably lower the compressive measurement rate considerably. We validate our approach with a range of experiments including classification experiments that highlight the effectiveness of the proposed approach."'),
('"Compressive Sensing for Background Subtraction"', '"ECCV 2008"', '["Background Subtraction", "Compressive Sense", "Difference Image", "Foreground Object", "Tile Size"', '"https://doi.org/10.1007/978-3-540-88688-4_12"', '"Compressive sensing (CS) is an emerging field that provides a framework for image recovery using sub-Nyquist sampling rates. The CS theory shows that a signal can be reconstructed from a small set of random projections, provided that the signal is sparse in some basis, e.g., wavelets. In this paper, we describe a method to directly recover background subtracted images using CS and discuss its applications in some communication constrained multi-camera computer vision problems. We show how to apply the CS theory to recover object silhouettes (binary background subtracted images) when the objects of interest occupy a small portion of the camera view, i.e., when they are sparse in the spatial domain. We cast the background subtraction as a sparse approximation problem and provide different solutions based on convex optimization and total variation. In our method, as opposed to learning the background, we learn and adapt a low dimensional compressed representation of it, which is sufficient to determine spatial innovations; object silhouettes are then estimated directly using the compressive samples without any auxiliary image reconstruction. We also discuss simultaneous appearance recovery of the objects using compressive measurements. In this case, we show that it may be necessary to reconstruct one auxiliary image. To demonstrate the performance of the proposed algorithm, we provide results on data captured using a compressive single-pixel camera. We also illustrate that our approach is suitable for image coding in communication constrained problems by using data captured by multiple conventional cameras to provide 2D tracking and 3D shape reconstruction results with compressive measurements."'),
('"Compressive Structured Light for Recovering Inhomogeneous Participating Media"', '"ECCV 2008"', '["Attenuation Correction", "Compressive Sensing", "Volume Density", "Measurement Cost", "Spatial Cod', '"https://doi.org/10.1007/978-3-540-88693-8_62"', '"We propose a new method named compressive structured light for recovering inhomogeneous participating media. Whereas conventional structured light methods emit coded light patterns onto the surface of an opaque object to establish correspondence for triangulation, compressive structured light projects patterns into a volume of participating medium to produce images which are integral measurements of the volume density along the line of sight. For a typical participating medium encountered in the real world, the integral nature of the acquired images enables the use of compressive sensing techniques that can recover the entire volume density from only a few measurements. This makes the acquisition process more efficient and enables reconstruction of dynamic volumetric phenomena. Moreover, our method requires the projection of multiplexed coded illumination, which has the added advantage of increasing the signal-to-noise ratio of the acquisition. Finally, we propose an iterative algorithm to correct for the attenuation of the participating medium during the reconstruction process. We show the effectiveness of our method with simulations as well as experiments on the volumetric recovery of multiple translucent layers, 3D point clouds etched in glass, and the dynamic process of milk drops dissolving in water."'),
('"Computation of the Mid-Sagittal Plane in 3D Images of the Brain"', '"ECCV 2000"', '["Tilted Head", "Block Match", "Rigid Transformation", "Midsagittal Plane", "Least Trim Square"]', '"https://doi.org/10.1007/3-540-45053-X_44"', '"We present a new symmetry-based method allowing to automatically compute, reorient and recenter the mid-sagittal plane in anatomical and functional 3D images of the brain. Our approach is composed of two steps. At first, the computation of local similarity measures between the two hemispheres of the brain allows to match homologous anatomical structures or functional areas, by way of a block matching procedure. The output is a set of point-to-point correspondences: the centers of homologous blocks. Subsequently, we define the mid-sagittal plane as the one best superposing the points in one side of the brain and their counterparts in the other side by reflective symmetry. The estimation of the parameters characterizing the plane is performed by a least trimmed squares optimization scheme. This robust technique allows normal or abnormal asymmetrical areas to be treated as outliers, and the plane to be mainly computed from the underlying gross symmetry of the brain. We show on a large database of synthetic images that we can obtain a subvoxel accuracy in a CPU time of about 3 minutes, for strongly tilted heads, noisy and biased images. We present results on anatomical (MR, CT), and functional (SPECT and PET) images."'),
('"Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science"', '"ECCV 2014"', '["Computer Vision", "Aesthetic Judgment", "Aesthetic Theory", "Critical Theory", "Formalism"]', '"https://doi.org/10.1007/978-3-319-16178-5_3"', '"In part one of the Critique of Judgment, Immanuel Kant wrote that \\u201cthe judgment of taste ... is not a cognitive judgment, and so not logical, but is aesthetic [1].\\u201d While the condition of aesthetic discernment has long been the subject of philosophical discourse, the role of the arbiters of that judgment has more often been assumed than questioned. The art historian, critic, connoisseur, and curator have long held the esteemed position of the aesthetic judge, their training, instinct, and eye part of the inimitable subjective processes that Kant described as occurring upon artistic evaluation. Although the concept of intangible knowledge in regard to aesthetic theory has been much explored, little discussion has arisen in response to the development of new types of artificial intelligence as a challenge to the seemingly ineffable abilities of the human observer. This paper examines the developments in the field of computer vision analysis of paintings from canonical movements within the history of Western art and the reaction of art historians to the application of this technology in the field. Through an investigation of the ethical consequences of this innovative technology, the unquestioned authority of the art expert is challenged and the subjective nature of aesthetic judgment is brought to philosophical scrutiny once again."'),
('"Computer-Aided Measurement of Solid Breast Tumor Features on Ultrasound Images"', '"MMBIA 2004"', '["Ultrasound Image", "Malignant Nodule", "Ellipsoid Shape", "Computer Vision Technique", "Hypoechoic', '"https://doi.org/10.1007/978-3-540-27816-0_30"', '"This paper presents a new approach in the application of computer vision techniques to the diagnosis of solid breast tumors on ultrasound images. Most works related to medical image analysis for breast cancer detection refer to mammography. However, radiologists have proved the significance of some aspects observed on ultrasound images, among which are spiculation, calcifications, ellipsoid shape, dimensions, echogenicity, capsule, angular margins, lobulations, shadowing and ramifications. We have developed a common framework for the analysis of these criteria, so that a series of parameters are available for the physicians to decide whether the biopsy is necessary or not. We present a set of mathematical methods to extract objective evidence of the presence or absence of the diagnostic criteria. This system is able to extract the relevant features for solid breast nodules with high accuracy and represents a very valuable help in the assessment of radiologists."'),
('"Computer-Aided Reclamation of Lost Art"', '"ECCV 2012"', '["Fine Art", "Restoration", "Super-Resolution", "Colour Correction"]', '"https://doi.org/10.1007/978-3-642-33863-2_57"', '"There are numerous approaches towards restoration of art, including computer applications as aid to manual performance. However, to our knowledge, it has not been attempted to recuperate high quality images of missing or presumably destroyed works of art. While these works will never again be available in their original form, it may be feasible to considerably enhance the quality of preserved photographic reproductions. A pioneering combination of super-resolution and colour correction is presented here, targeting the reclamation of high quality images of lost works of art. The techniques are performed by example, utilising correspondence between artworks of similar nature, currently available both in low and high quality. With extensive prior knowledge in the domains of super-resolution and colour correction, selected approaches were studied, implemented and tested, concluding to the most efficient. Experimental results are highly promising, revealing a new research path in colour imaging for fine art."'),
('"Computing Content-Plots for Video"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47979-1_33"', '"The content-plot of a video clip is created by positioning several key frames in two-dimensions and connecting them with lines. It is constructed so that it should be possible to follow the events shown in the video by moving along the lines. Content plots were previously computed by clustering together frames that are contiguous in time. We propose to cluster together frames if they are related by a short chain of similarly looking frames even if they are not adjacent on the time-line. The computational problem can be formulated as a graph clustering problem that we solve by extending the classic k-means technique to graphs. This new graph clustering algorithm is the main technical contribution of this paper."'),
('"Computing Emotion Awareness Through Facial Electromyography"', '"ECCV 2006"', '["Emotion Category", "Affective Computing", "Psychophysiological Measure", "Mixed Emotion", "Corruga', '"https://doi.org/10.1007/11754336_6"', '"To improve human-computer interaction (HCI), computers need to recognize and respond properly to their user\\u2019s emotional state. This is a fundamental application of affective computing, which relates to, arises from, or deliberately influences emotion. As a first step to a system that recognizes emotions of individual users, this research focuses on how emotional experiences are expressed in six parameters (i.e., mean, absolute deviation, standard deviation, variance, skewness, and kurtosis) of physiological measurements of three electromyography signals: frontalis (EMG1), corrugator supercilii (EMG2), and zygomaticus major (EMG3). The 24 participants were asked to watch film scenes of 120 seconds, which they rated afterward. These ratings enabled us to distinguish four categories of emotions: negative, positive, mixed, and neutral. The skewness of the EMG2 and four parameters of EMG3, discriminate between the four emotion categories. This, despite the coarse time windows that were used. Moreover, rapid processing of the signals proved to be possible. This enables tailored HCI facilitated by an emotional awareness of systems."'),
('"Computing the Physical Parameters of Rigid-Body Motion from Video"', '"ECCV 2002"', '["Rigid Body", "Video Sequence", "Video Clip", "Optimization Framework", "Ordinary Differential Equa', '"https://doi.org/10.1007/3-540-47969-4_37"', '"This paper presents an optimization framework for estimating the motion and underlying physical parameters of a rigid body in free flight from video. The algorithm takes a video clip of a tumbling rigid body of known shape and generates a physical simulation of the object observed in the video clip. This solution is found by optimizing the simulation parameters to best match the motion observed in the video sequence. These simulation parameters include initial positions and velocities, environment parameters like gravity direction and parameters of the camera. A global objective function computes the sum squared difference between the silhouette of the object in simulation and the silhouette obtained from video at each frame. Applications include creating interesting rigid body animations, tracking complex rigid body motions in video and estimating camera parameters from video."'),
('"ConceptMap: Mining Noisy Web Data for Concept Learning"', '"ECCV 2014"', '["Weakly-labelled data", "Clustering and outlier detection", "Semi- supervised model learning", "Con', '"https://doi.org/10.1007/978-3-319-10584-0_29"', '"We attack the problem of learning concepts automatically from noisy Web image search results. The idea is based on discovering common characteristics shared among subsets of images by posing a method that is able to organise the data while eliminating irrelevant instances. We propose a novel clustering and outlier detection method, namely Concept Map (CMAP). Given an image collection returned for a concept query, CMAP provides clusters pruned from outliers. Each cluster is used to train a model representing a different characteristics of the concept. The proposed method outperforms the state-of-the-art studies on the task of learning from noisy web data for low-level attributes, as well as high level object categories. It is also competitive with the supervised methods in learning scene concepts. Moreover, results on naming faces support the generalisation capability of the CMAP framework to different domains. CMAP is capable to work at large scale with no supervision through exploiting the available sources."'),
('"Conditional Infomax Learning: An Integrated Framework for Feature Extraction and Fusion"', '"ECCV 2006"', '["Feature Selection", "Feature Extraction", "Mutual Information", "Face Recognition", "Extraction St', '"https://doi.org/10.1007/11744023_6"', '"The paper introduces a new framework for feature learning in classification motivated by information theory. We first systematically study the information structure and present a novel perspective revealing the two key factors in information utilization: class-relevance and redundancy. We derive a new information decomposition model where a novel concept called class-relevant redundancy is introduced. Subsequently a new algorithm called Conditional Informative Feature Extraction is formulated, which maximizes the joint class-relevant information by explicitly reducing the class-relevant redundancies among features. To address the computational difficulties in information-based optimization, we incorporate Parzen window estimation into the discrete approximation of the objective function and propose a Local Active Region method which substantially increases the optimization efficiency. To effectively utilize the extracted feature set, we propose a Bayesian MAP formulation for feature fusion, which unifies Laplacian Sparse Prior and Multivariate Logistic Regression to learn a fusion rule with good generalization capability. Realizing the inefficiency caused by separate treatment of the extraction stage and the fusion stage, we further develop an improved design of the framework to coordinate the two stages by introducing a feedback from the fusion stage to the extraction stage, which significantly enhances the learning efficiency. The results of the comparative experiments show remarkable improvements achieved by our framework."'),
('"Cone-Beam Image Reconstruction by Moving Frames"', '"MMBIA 2004"', '["Image Plane", "Projective Space", "Critical Plane", "Radon Transform", "Radial Derivative"]', '"https://doi.org/10.1007/978-3-540-27816-0_4"', '"In this paper, we present a new algorithmic paradigm for cone-beam image reconstruction. The new class of algorithms, referred to as cone-beam reconstruction by moving frames, enables numerical implementation of exact cone-beam inversion using its intrinsic geometry. In particular, our algorithm allows a 3-D discrete approach to the differentiation-backprojection operator on the curved manifolds appearing in all analytical cone-beam inverse formulations. The enabling technique, called the method of moving frames, has been popular in the computer vision community for many years [3]. Although cone-beam image reconstruction has come from a different origin and has been until now developed along very different lines from computer vision algorithms, we can find analogies in their line-and-plane geometry. We demonstrate how the moving frame technique can be made into a ubiquitous and powerful computational tool for designing and implementing more robust and more accurate cone-beam reconstruction algorithms."'),
('"Confocal Stereo"', '"ECCV 2006"', '["Image Alignment", "Outgoing Radiance", "Scene Point", "Sensor Plane", "Focus Setting"]', '"https://doi.org/10.1007/11744023_48"', '"We present confocal stereo, a new method for computing 3D shape by controlling the focus and aperture of a lens. The method is specifically designed for reconstructing scenes with high geometric complexity or fine-scale texture. To achieve this, we introduce the confocal constancy property, which states that as the lens aperture varies, the pixel intensity of a visible in-focus scene point will vary in a scene-independent way, that can be predicted by prior radiometric lens calibration. The only requirement is that incoming radiance within the cone subtended by the largest aperture is nearly constant. First, we develop a detailed lens model that factors out the distortions in high resolution SLR cameras (12MP or more) with large-aperture lenses (e.g., f1.2). This allows us to assemble an A \\u00d7 F aperture-focus image (AFI) for each pixel, that collects the undistorted measurements over all A apertures and F focus settings. In the AFI representation, confocal constancy reduces to color comparisons within regions of the AFI, and leads to focus metrics that can be evaluated separately for each pixel. We propose two such metrics and present initial reconstruction results for complex scenes."'),
('"Conjugate Gradient Bundle Adjustment"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_9"', '"Bundle adjustment for multi-view reconstruction is traditionally done using the Levenberg-Marquardt algorithm with a direct linear solver, which is computationally very expensive. An alternative to this approach is to apply the conjugate gradients algorithm in the inner loop. This is appealing since the main computational step of the CG algorithm involves only a simple matrix-vector multiplication with the Jacobian. In this work we improve on the latest published approaches to bundle adjustment with conjugate gradients by making full use of the least squares nature of the problem. We employ an easy-to-compute QR factorization based block preconditioner and show how a certain property of the preconditioned system allows us to reduce the work per iteration to roughly half of the standard CG algorithm."'),
('"Connecting Missing Links: Object Discovery from Sparse Observations Using 5 Million Product Images"', '"ECCV 2012"', '["Input Image", "Image Segment", "Foreground Object", "Computer Mouse", "Object Discovery"]', '"https://doi.org/10.1007/978-3-642-33783-3_57"', '"Object discovery algorithms group together image regions that originate from the same object. This process is effective when the input collection of images contains a large number of densely sampled views of each object, thereby creating strong connections between nearby views. However, existing approaches are less effective when the input data only provide sparse coverage of object views."'),
('"Consensus of Regression for Occlusion-Robust Facial Feature Localization"', '"ECCV 2014"', '["Facial feature localization", "Consensus of Regression", "Occlusion detection", "Face alignment"]', '"https://doi.org/10.1007/978-3-319-10593-2_8"', '"We address the problem of robust facial feature localization in the presence of occlusions, which remains a lingering problem in facial analysis despite intensive long-term studies. Recently, regression-based approaches to localization have produced accurate results in many cases, yet are still subject to significant error when portions of the face are occluded. To overcome this weakness, we propose an occlusion-robust regression method by forming a consensus from estimates arising from a set of occlusion-specific regressors. That is, each regressor is trained to estimate facial feature locations under the precondition that a particular pre-defined region of the face is occluded. The predictions from each regressor are robustly merged using a Bayesian model that models each regressor\\u2019s prediction correctness likelihood based on local appearance and consistency with other regressors with overlapping occlusion regions. After localization, the occlusion state for each landmark point is estimated using a Gaussian MRF semi-supervised learning method. Experiments on both non-occluded and occluded face databases demonstrate that our approach achieves consistently better results over state-of-the-art methods for facial landmark localization and occlusion detection."'),
('"Consistency Conditions on the Medial Axis"', '"ECCV 2004"', '["Boundary Surface", "Consistency Condition", "Principal Curvature", "Principal Direction", "Medial ', '"https://doi.org/10.1007/978-3-540-24671-8_42"', '"The medial axis in 3D consists of 2D sheets, meeting in 1D curves and special points. In this paper we investigate the consistency conditions which must hold on a collection of sheets meeting in curves and points in order that they could be the medial axis of a smooth surface."'),
('"Consistent and Elastic Registration of Histological Sections Using Vector-Spline Regularization"', '"CVAMIA 2006"', '[]', '"https://doi.org/10.1007/11889762_8"', '"Here we present a new image registration algorithm for the alignment of histological sections that combines the ideas of B-spline based elastic registration and consistent image registration, to allow simultaneous registration of images in two directions (direct and inverse). In principle, deformations based on B-splines are not invertible. The consistency term overcomes this limitation and allows registration of two images in a completely symmetric way. This extension of the elastic registration method simplifies the search for the optimum deformation and allows registering with no information about landmarks or deformation regularization. This approach can also be used as the first step to solve the problem of group-wise registration."'),
('"Consistent Matting for Light Field Images"', '"ECCV 2014"', '["Image Matting", "Light field image", "EPI"]', '"https://doi.org/10.1007/978-3-319-10593-2_7"', '"We present a new image matting algorithm to extract consistent alpha mattes across sub-images of a light field image. Instead of matting each sub-image individually, our approach utilizes the epipolar plane image (EPI) to construct comprehensive foreground and background sample sets across the sub-images without missing a true sample. The sample sets represent all color variation of foreground and background in a light field image, and the optimal alpha matte is obtained by choosing the best combination of foreground and background samples that minimizes the linear composite error subject to the EPI correspondence constraint. To further preserve consistency of the estimated alpha mattes across different sub-images, we impose a smoothness constraint along the EPI of alpha mattes. In experimental evaluations, we have created a dataset where the ground truth alpha mattes of light field images were obtained by using the blue screen technique. A variety of experiments show that our proposed algorithm produces both visually and quantitatively high-quality matting results for light field images."'),
('"Consistent Re-identification in a Camera Network"', '"ECCV 2014"', '["Person re-identification", "Network consistency"]', '"https://doi.org/10.1007/978-3-319-10605-2_22"', '"Most existing person re-identification methods focus on finding similarities between persons between pairs of cameras (camera pairwise re-identification) without explicitly maintaining consistency of the results across the network. This may lead to infeasible associations when results from different camera pairs are combined. In this paper, we propose a network consistent re-identification (NCR) framework, which is formulated as an optimization problem that not only maintains consistency in re-identification results across the network, but also improves the camera pairwise re-identification performance between all the individual camera pairs. This can be solved as a binary integer programing problem, leading to a globally optimal solution. We also extend the proposed approach to the more general case where all persons may not be present in every camera. Using two benchmark datasets, we validate our approach and compare against state-of-the-art methods."'),
('"Constrained Clustering with Local Constraint Propagation"', '"ECCV 2012"', '["constrained clustering", "constraint propagation", "image segmentation"]', '"https://doi.org/10.1007/978-3-642-33885-4_23"', '"We consider the problem of multi-class constrained clustering given pairwise constraints, which specify the pairs of data belonging to the same or different clusters. In this paper, we present a new constrained clustering algorithm, Local Constraint Propagation (LCP), which can propagate the influence of each pairwise constraint to the unconstrained data with sufficient smoothness. It not only reveals the underlying structures of the clusters, but also integrates the influence of all the pairwise constraints on every data point. Promising experiments on image segmentations demonstrate the effectiveness of our method."'),
('"Constrained Dichromatic Colour Constancy"', '"ECCV 2000"', '["Colour Signal", "Colour Constancy", "Single Surface", "Spectral Power Distribution", "Colour Cast"', '"https://doi.org/10.1007/3-540-45054-8_23"', '"Statistics-based colour constancy algorithms work well as long as there are many colours in a scene, they fail however when the encountering scenes comprise few surfaces. In contrast, physics-based algorithms, based on an understanding of physical processes such as highlights and interreflections, are theoretically able to solve for colour constancy even when there are as few as two surfaces in a scene. Unfortunately, physics-based theories rarely work outside the lab. In this paper we show that a combination of physical and statistical knowledge leads to a surprisingly simple and powerful colour constancy algorithm, one that also works well for images of natural scenes."'),
('"Constrained Flows of Matrix-Valued Functions: Application to Diffusion Tensor Regularization"', '"ECCV 2002"', '["Tangent Space", "Homogeneous Space", "Tensor Orientation", "Constraint Preserve", "Image Inpaintin', '"https://doi.org/10.1007/3-540-47969-4_17"', '"Nonlinear partial differential equations (PDE) are now widely used to regularize images. They allow to eliminate noise and artifacts while preserving large global features, such as object contours. In this context, we propose a geometric framework to design PDE flows acting on constrained datasets. We focus our interest on flows of matrix-valued functions undergoing orthogonal and spectral constraints. The corresponding evolution PDE\\u2019s are found by minimization of cost functionals, and depend on the natural metrics of the underlying constrained manifolds (viewed as Lie groups or homogeneous spaces). Suitable numerical schemes that fit the constraints are also presented. We illustrate this theoretical framework through a recent and challenging problem in medical imaging: the regularization of diffusion tensor volumes (DTMRI)."'),
('"Constrained Maximum Likelihood Learning of Bayesian Networks for Facial Action Recognition"', '"ECCV 2008"', '["Bayesian Network", "Facial Expression Recognition", "Parameter Learning", "Probabilistic Graphical', '"https://doi.org/10.1007/978-3-540-88690-7_13"', '"Probabilistic graphical models such as Bayesian Networks have been increasingly applied to many computer vision problems. Accuracy of inferences in such models depends on the quality of network parameters. Learning reliable parameters of Bayesian networks often requires a large amount of training data, which may be hard to acquire and may contain missing values. On the other hand, qualitative knowledge is available in many computer vision applications, and incorporating such knowledge can improve the accuracy of parameter learning. This paper describes a general framework based on convex optimization to incorporate constraints on parameters with training data to perform Bayesian network parameter estimation. For complete data, a global optimum solution to maximum likelihood estimation is obtained in polynomial time, while for incomplete data, a modified expectation-maximization method is proposed. This framework is applied to real image data from a facial action unit recognition problem and produces results that are similar to those of state-of-the-art methods."'),
('"Constrained Semi-Supervised Learning Using Attributes and Comparative Attributes"', '"ECCV 2012"', '["Unlabeled Data", "Mutual Exclusion", "Comparative Attribute", "Pairwise Constraint", "Scene Catego', '"https://doi.org/10.1007/978-3-642-33712-3_27"', '"We consider the problem of semi-supervised bootstrap learning for scene categorization. Existing semi-supervised approaches are typically unreliable and face semantic drift because the learning task is under-constrained. This is primarily because they ignore the strong interactions that often exist between scene categories, such as the common attributes shared across categories as well as the attributes which make one scene different from another. The goal of this paper is to exploit these relationships and constrain the semi-supervised learning problem. For example, the knowledge that an image is an auditorium can improve labeling of amphitheaters by enforcing constraint that an amphitheater image should have more circular structures than an auditorium image. We propose constraints based on mutual exclusion, binary attributes and comparative attributes and show that they help us to constrain the learning problem and avoid semantic drift. We demonstrate the effectiveness of our approach through extensive experiments, including results on a very large dataset of one million images."'),
('"Constrained Spectral Clustering via Exhaustive and Efficient Constraint Propagation"', '"ECCV 2010"', '["Spectral Cluster", "Constraint Propagation", "Adjusted Rand Index", "Pairwise Constraint", "Scene ', '"https://doi.org/10.1007/978-3-642-15567-3_1"', '"This paper presents an exhaustive and efficient constraint propagation approach to exploiting pairwise constraints for spectral clustering. Since traditional label propagation techniques cannot be readily generalized to propagate pairwise constraints, we tackle the constraint propagation problem inversely by decomposing it to a set of independent label propagation subproblems which are further solved in quadratic time using semi-supervised learning based on k-nearest neighbors graphs. Since this time complexity is proportional to the number of all possible pairwise constraints, our approach gives a computationally efficient solution for exhaustively propagating pairwise constraint throughout the entire dataset. The resulting exhaustive set of propagated pairwise constraints are then used to adjust the weight (or similarity) matrix for spectral clustering. It is worth noting that this paper first clearly shows how pairwise constraints are propagated independently and then accumulated into a conciliatory closed-form solution. Experimental results on real-life datasets demonstrate that our approach to constrained spectral clustering outperforms the state-of-the-art techniques."'),
('"Constraints on Coplanar Moving Points"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_14"', '"Configurations of dynamic points viewed by one or more cameras have not been studied much. In this paper, we present several view and time-independent constraints on different configurations of points moving on a plane. We show that 4 points with constant independent velocities or accelerations under affine projection can be characterized in a view independent manner using 2 views. Under perspective projection, 5 coplanar points under uniform linear velocity observed for 3 time instants in a single view have a view-independent characterization. The best known constraint for this case involves 6 points observed for 35 frames. Under uniform acceleration, 5 points in 5 time instants have a view-independent characterization. We also present constraints on a point undergoing arbitrary planar motion under affine projections in the Fourier domain. The constraints introduced in this paper involve fewer points or views than similar results reported in the literature and are simpler to compute in most cases. The constraints developed can be applied to many aspects of computer vision. Recognition constraints for several planar point configurations of moving points can result from them. We also show how time-alignment of views captured independently can follow from the constraints on moving point configurations."'),
('"Constructing Category Hierarchies for Visual Recognition"', '"ECCV 2008"', '["Support Vector Machine", "Image Representation", "Object Category", "Visual Recognition", "Class H', '"https://doi.org/10.1007/978-3-540-88693-8_35"', '"Class hierarchies are commonly used to reduce the complexity of the classification problem. This is crucial when dealing with a large number of categories. In this work, we evaluate class hierarchies currently constructed for visual recognition. We show that top-down as well as bottom-up approaches, which are commonly used to automatically construct hierarchies, incorporate assumptions about the separability of classes. Those assumptions do not hold for visual recognition of a large number of object categories. We therefore propose a modification which is appropriate for most top-down approaches. It allows to construct class hierarchies that postpone decisions in the presence of uncertainty and thus provide higher recognition accuracy. We also compare our method to a one-against-all approach and show how to control the speed-for-accuracy trade-off with our method. For the experimental evaluation, we use the Caltech-256 visual object classes dataset and compare to state-of-the-art methods."'),
('"Constructing Illumination Image Basis from Object Motion"', '"ECCV 2002"', '["Reference Frame", "Input Image", "Principle Component Analysis", "Motion Parameter", "Object Motio', '"https://doi.org/10.1007/3-540-47977-5_13"', '"We propose to construct a 3D linear image basis which spans an image space of arbitrary illumination conditions, from images of a moving object observed under a static lighting condition. The key advance is to utilize the object motion which causes illumination variance on the object surface, rather than varying the lighting, and thereby simplifies the environment for acquiring the input images. Since we then need to re-align the pixels of the images so that the same view of the object can be seen, the correspondence between input images must be solved despite the illumination variance. In order to overcome the problem, we adapt the recently introduced geotensity constraint that accurately governs the relationship between four or more images of a moving object. Through experiments we demonstrate that equivalent 3D image basis is indeed computable and available for recognition or image rendering."'),
('"Content-Based Retrieval of Functional Objects in Video Using Scene Context"', '"ECCV 2010"', '["Functional Model", "Defense Advance Research Project Agency", "Functional Object", "Defense Advanc', '"https://doi.org/10.1007/978-3-642-15549-9_40"', '"Functional object recognition in video is an emerging problem for visual surveillance and video understanding problem. By functional objects, we mean objects with specific purpose such as postman and delivery truck, which are defined more by their actions and behaviors than by appearance. In this work, we present an approach for content-based learning and recognition of the function of moving objects given video-derived tracks. In particular, we show that semantic behaviors of movers can be captured in location-independent manner by attributing them with features which encode their relations and actions w.r.t. scene contexts. By scene context, we mean local scene regions with different functionalities such as doorways and parking spots which moving objects often interact with. Based on these representations, functional models are learned from examples and novel instances are identified from unseen data afterwards. Furthermore, recognition in the presence of track fragmentation, due to imperfect tracking, is addressed by a boosting-based track linking classifier. Our experimental results highlight both promising and practical aspects of our approach."'),
('"Context as Supervisory Signal: Discovering Objects with Predictable Context"', '"ECCV 2014"', '["Context", "prediction", "unsupervised object discovery", "mining"]', '"https://doi.org/10.1007/978-3-319-10578-9_24"', '"This paper addresses the well-established problem of unsupervised object discovery with a novel method inspired by weakly-supervised approaches. In particular, the ability of an object patch to predict the rest of the object (its context) is used as supervisory signal to help discover visually consistent object clusters. The main contributions of this work are: 1) framing unsupervised clustering as a leave-one-out context prediction task; 2) evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models; and 3) an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences. The proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches, and is shown to provide correspondences detailed enough to transfer keypoint annotations."'),
('"Context-Aided Human Recognition \\u2013 Clustering"', '"ECCV 2006"', '["Face Recognition", "Context Information", "Face Detection", "Spectral Cluster", "Color Histogram"]', '"https://doi.org/10.1007/11744078_30"', '"Context information other than faces, such as clothes, picture-taken-time and some logical constraints, can provide rich cues for recognizing people. This aim of this work is to automatically cluster pictures according to person\\u2019s identity by exploiting as much context information as possible in addition to faces. Toward that end, a clothes recognition algorithm is first developed, which is effective for different types of clothes (smooth or highly textured). Clothes recognition results are integrated with face recognition to provide similarity measurements for clustering. Picture-taken-time is used when combining faces and clothes, and the cases of faces or clothes missing are handled in a principle way. A spectral clustering algorithm which can enforce hard constraints (positive and negative) is presented to incorporate logic-based cues (e.g. two persons in one picture must be different individuals) and user feedback. Experiments on real consumer photos show the effectiveness of the algorithm."'),
('"Context-Based Automatic Local Image Enhancement"', '"ECCV 2012"', '["Input Image", "User Study", "Image Enhancement", "High Dynamic Range", "Enhancement Method"]', '"https://doi.org/10.1007/978-3-642-33718-5_41"', '"In this paper, we describe a technique to automatically enhance the perceptual quality of an image. Unlike previous techniques, where global statistics of the image are used to determine enhancement operation, our method is local and relies on local scene descriptors and context in addition to high-level image statistics. We cast the problem of image enhancement as searching for the best transformation for each pixel in the given image and then discovering the enhanced image using a formulation based on Gaussian Random Fields. The search is done in a coarse-to-fine manner, namely by finding the best candidate images, followed by pixels. Our experiments indicate that such context-based local enhancement is better than global enhancement schemes. A user study using Mechanical Turk shows that the subjects prefer contextual and local enhancements over the ones provided by existing schemes."'),
('"Context-Based Pedestrian Path Prediction"', '"ECCV 2014"', '["intelligent vehicles", "path prediction", "situational awareness", "visual focus of attention", "D', '"https://doi.org/10.1007/978-3-319-10599-4_40"', '"We present a novel Dynamic Bayesian Network for pedestrian path prediction in the intelligent vehicle domain. The model incorporates the pedestrian situational awareness, situation criticality and spatial layout of the environment as latent states on top of a Switching Linear Dynamical System (SLDS) to anticipate changes in the pedestrian dynamics. Using computer vision, situational awareness is assessed by the pedestrian head orientation, situation criticality by the distance between vehicle and pedestrian at the expected point of closest approach, and spatial layout by the distance of the pedestrian to the curbside. Our particular scenario is that of a crossing pedestrian, who might stop or continue walking at the curb. In experiments using stereo vision data obtained from a vehicle, we demonstrate that the proposed approach results in more accurate path prediction than only SLDS, at the relevant short time horizon (1 s), and slightly outperforms a computationally more demanding state-of-the-art method."'),
('"Contextual Object Detection Using Set-Based Classification"', '"ECCV 2012"', '["Average Precision", "Object Class", "Context Model", "Reference Object", "Contextual Relationship"', '"https://doi.org/10.1007/978-3-642-33783-3_4"', '"We propose a new model for object detection that is based on set representations of the contextual elements. In this formulation, relative spatial locations and relative scores between pairs of detections are considered as sets of unordered items. Directly training classification models on sets of unordered items, where each set can have varying cardinality can be difficult. In order to overcome this problem, we propose SetBoost, a discriminative learning algorithm for building set classifiers. The SetBoost classifiers are trained to rescore detected objects based on object-object and object-scene context. Our method is able to discover composite relationships, as well as intra-class and inter-class spatial relationships between objects. The experimental evidence shows that our set-based formulation performs comparable to or better than existing contextual methods on the SUN and the VOC 2007 benchmark datasets."'),
('"Continuous Conditional Neural Fields for Structured Regression"', '"ECCV 2014"', '["Structured regression", "Landmark detection", "Face tracking"]', '"https://doi.org/10.1007/978-3-319-10593-2_39"', '"An increasing number of computer vision and pattern recognition problems require structured regression techniques. Problems like human pose estimation, unsegmented action recognition, emotion prediction and facial landmark detection have temporal or spatial output dependencies that regular regression techniques do not capture. In this paper we present continuous conditional neural fields (CCNF) \\u2013 a novel structured regression model that can learn non-linear input-output dependencies, and model temporal and spatial output relationships of varying length sequences. We propose two instances of our CCNF framework: Chain-CCNF for time series modelling, and Grid-CCNF for spatial relationship modelling. We evaluate our model on five public datasets spanning three different regression problems: facial landmark detection in the wild, emotion prediction in music and facial action unit recognition. Our CCNF model demonstrates state-of-the-art performance on all of the datasets used."'),
('"Continuous Energy Minimization Via Repeated Binary Fusion"', '"ECCV 2008"', '["Markov Random Field", "Optical Flow Estimation", "Average Angular Error", "Propose Optimization St', '"https://doi.org/10.1007/978-3-540-88693-8_50"', '"Variational problems, which are commonly used to solve low-level vision tasks, are typically minimized via a local, iterative optimization strategy, e.g. gradient descent. Since every iteration is restricted to a small, local improvement, the overall convergence can be slow and the algorithm may get stuck in an undesirable local minimum. In this paper, we propose to approximate the minimization by solving a series of binary subproblems to facilitate large optimization moves. The proposed method can be interpreted as an extension of discrete graph-cut based methods such as \\u03b1-expansion or LogCut to a spatially continuous setting. In order to demonstrate the viability of the approach, we evaluated the novel optimization strategy in the context of optical flow estimation, yielding excellent results on the Middlebury optical flow datasets."'),
('"Continuous Gesture Recognition from Articulated Poses"', '"ECCV 2014"', '["Gaussian Mixture Model", "Action Recognition", "Gesture Recognition", "Convolutional Neural Networ', '"https://doi.org/10.1007/978-3-319-16178-5_42"', '"This paper addresses the problem of continuous gesture recognition from articulated poses. Unlike the common isolated recognition scenario, the gesture boundaries are here unknown, and one has to solve two problems: segmentation and recognition. This is cast into a labeling framework, namely every site (frame) must be assigned a label (gesture ID). The inherent constraint for a piece-wise constant labeling is satisfied by solving a global optimization problem with a smoothness term. For efficiency reasons, we suggest a dynamic programming (DP) solver that seeks the optimal path in a recursive manner. To quantify the consistency between the labels and the observations, we build on a recent method that encodes sequences of articulated poses into Fisher vectors using short skeletal descriptors. A sliding window allows to frame-wise build such Fisher vectors that are then classified by a multi-class SVM, whereby each label is assigned to each frame at some cost. The evaluation in the ChalearnLAP-2014 challenge shows that the method outperforms other participants that rely only on skeleton data. We also show that the proposed method competes with the top-ranking methods when colour and skeleton features are jointly used."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Continuous Learning of Human Activity Models Using Deep Nets"', '"ECCV 2014"', '["Continuous Learning", "Active Learning", "Deep Learning", "Action Recognition"]', '"https://doi.org/10.1007/978-3-319-10578-9_46"', '"Learning activity models continuously from streaming videos is an immensely important problem in video surveillance, video indexing, etc. Most of the research on human activity recognition has mainly focused on learning a static model considering that all the training instances are labeled and present in advance, while in streaming videos new instances continuously arrive and are not labeled. In this work, we propose a continuous human activity learning framework from streaming videos by intricately tying together deep networks and active learning. This allows us to automatically select the most suitable features and to take the advantage of incoming unlabeled instances to improve the existing model incrementally. Given the segmented activities from streaming videos, we learn features in an unsupervised manner using deep networks and use active learning to reduce the amount of manual labeling of classes. We conduct rigorous experiments on four challenging human activity datasets to demonstrate the effectiveness of our framework for learning human activity models continuously."'),
('"Continuous Markov Random Fields for Robust Stereo Estimation"', '"ECCV 2012"', '["Belief Propagation", "Stereo Vision", "Stereo Match", "Quadratic Potential", "Autonomous Driving"]', '"https://doi.org/10.1007/978-3-642-33715-4_4"', '"In this paper we present a novel slanted-plane model which reasons jointly about occlusion boundaries as well as depth. We formulate the problem as one of inference in a hybrid MRF composed of both continuous (i.e., slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables. This allows us to define potentials encoding the ownership of the pixels that compose the boundary between segments, as well as potentials encoding which junctions are physically possible. Our approach outperforms the state-of-the-art on Middlebury high resolution imagery [1] as well as in the more challenging KITTI dataset [2], while being more efficient than existing slanted plane MRF methods, taking on average 2 minutes to perform inference on high resolution imagery."'),
('"Continuous Regression for Non-rigid Image Alignment"', '"ECCV 2012"', '["Training Image", "Motion Parameter", "Canonical Correlation Analysis", "Appearance Model", "Active', '"https://doi.org/10.1007/978-3-642-33786-4_19"', '"Parameterized Appearance Models (PAMs) such as Active Appearance Models (AAMs), Morphable Models and Boosted Appearance Models have been extensively used for face alignment. Broadly speaking, PAMs methods can be classified into generative and discriminative. Discriminative methods learn a mapping between appearance features and motion parameters (rigid and non-rigid). While discriminative approaches have some advantages (e.g., feature weighting, improved generalization), they suffer from two major drawbacks: (1) they need large amounts of perturbed samples to train a regressor or classifier, making the training process computationally expensive in space and time. (2) It is not practical to uniformly sample the space of motion parameters. In practice, there are regions of the motion space that are more densely sampled than others, resulting in biased models and lack of generalization. To solve these problems, this paper proposes a computationally efficient continuous regressor that does not require the sampling stage. Experiments on real data show the improvement in memory and time requirements to train a discriminative appearance model, as well as improved generalization."'),
('"Contour Context Selection for Object Detection: A Set-to-Set Contour Matching Approach"', '"ECCV 2008"', '["Control Point", "Object Detection", "Shape Match", "Background Clutter", "Shape Context"]', '"https://doi.org/10.1007/978-3-540-88688-4_57"', '"We introduce a shape detection framework called Contour Context Selection for detecting objects in cluttered images using only one exemplar. Shape based detection is invariant to changes of object appearance, and can reason with geometrical abstraction of the object. Our approach uses salient contours as integral tokens for shape matching. We seek a maximal, holistic matching of shapes, which checks shape features from a large spatial extent, as well as long-range contextual relationships among object parts. This amounts to finding the correct figure/ground contour labeling, and optimal correspondences between control points on/around contours. This removes accidental alignments and does not hallucinate objects in background clutter, without negative training examples. We formulate this task as a set-to-set contour matching problem. Naive methods would require searching over \\u2019exponentially\\u2019 many figure/ground contour labelings. We simplify this task by encoding the shape descriptor algebraically in a linear form of contour figure/ground variables. This allows us to use the reliable optimization technique of Linear Programming. We demonstrate our approach on the challenging task of detecting bottles, swans and other objects in cluttered images."'),
('"Contour Grouping and Abstraction Using Simple Part Models"', '"ECCV 2010"', '["Perceptual Grouping", "Active Shape Model", "Image Contour", "Abstract Part", "Initial Edge"]', '"https://doi.org/10.1007/978-3-642-15555-0_44"', '"We address the problem of contour-based perceptual grouping using a user-defined vocabulary of simple part models. We train a family of classifiers on the vocabulary, and apply them to a region oversegmentation of the input image to detect closed contours that are consistent with some shape in the vocabulary. Given such a set of consistent cycles, they are both abstracted and categorized through a novel application of an active shape model also trained on the vocabulary. From an image of a real object, our framework recovers the projections of the abstract surfaces that comprise an idealized model of the object. We evaluate our framework on a newly constructed dataset annotated with a set of ground truth abstract surfaces."'),
('"Contour-Based Correspondence for Stereo"', '"ECCV 2000"', '["Image Plane", "Camera Parameter", "Stereo Pair", "Unit Speed", "Circular Cone"]', '"https://doi.org/10.1007/3-540-45054-8_21"', '"In stereoscopic images, the behavior of a curve in space is related to the appearance of the curve in the left and right image planes. Formally, this relationship is governed by the projective geometry induced by the stereo camera configuration and by the differential structure of the curve in the scene. We propose that the correspondence problem-matching corresponding points in the image planes-can be solved by relating the differential structure in the left and right image planes to the geometry of curves in space. Specifically, the compatibility between two pairs of corresponding points and tangents at those points is related to the local approximation of a space curve using an osculating helix. To guarantee robustness against small changes in the camera parameters, we select a specific osculating helix. A relaxation labeling network demonstrates that the compatibilities can be used to infer the appropriate correspondences in a scene. Examples on which standard approaches fail are demonstrated."'),
('"Contraction Moves for Geometric Model Fitting"', '"ECCV 2012"', '["Point Cloud", "Markov Random Field", "Point Cloud Data", "Label Cost", "Pairwise Term"]', '"https://doi.org/10.1007/978-3-642-33786-4_14"', '"This paper presents a new class of moves, called \\u03b1-expansion-contraction, which generalizes \\u03b1-expansion graph cuts for multi-label energy minimization problems. The new moves are particularly useful for optimizing the assignments in model fitting frameworks whose energies include Label Cost (LC), as well as Markov Random Field (MRF) terms. These problems benefit from the contraction moves\\u2019 greater scope for removing instances from the model, reducing label costs. We demonstrate this effect on the problem of fitting sets of geometric primitives to point cloud data, including real-world point clouds containing millions of points, obtained by multi-view reconstruction."'),
('"Controlling Sparseness in Non-negative Tensor Factorization"', '"ECCV 2006"', '["Nonnegative Matrix Factorization", "Positive Matrix Factorization", "Tensor Factorization", "Spars', '"https://doi.org/10.1007/11744023_5"', '"Non-negative tensor factorization (NTF) has recently been proposed as sparse and efficient image representation (Welling and Weber, Patt. Rec. Let., 2001). Until now, sparsity of the tensor factorization has been empirically observed in many cases, but there was no systematic way to control it. In this work, we show that a sparsity measure recently proposed for non-negative matrix factorization (Hoyer, J. Mach. Learn. Res., 2004) applies to NTF and allows precise control over sparseness of the resulting factorization. We devise an algorithm based on sequential conic programming and show improved performance over classical NTF codes on artificial and on real-world data sets."'),
('"Converting Level Set Gradients to Shape Gradients"', '"ECCV 2010"', '["Active Contour", "Projection Line", "Active Contour Model", "Signed Distance Function", "Velocity ', '"https://doi.org/10.1007/978-3-642-15555-0_52"', '"The level set representation of shapes is useful for shape evolution and is widely used for the minimization of energies with respect to shapes. Many algorithms consider energies depending explicitly on the signed distance function (SDF) associated with a shape, and differentiate these energies with respect to the SDF directly in order to make the level set representation evolve. This framework is known as the \\u201cvariational level set method\\u201d. We show that this gradient computation is actually mathematically incorrect, and can lead to undesirable performance in practice. Instead, we derive the expression of the gradient with respect to the shape, and show that it can be easily computed from the gradient of the energy with respect to the SDF. We discuss some problematic gradients from the literature, show how they can easily be fixed, and provide experimental comparisons illustrating the improvement."'),
('"Convex Relaxation for Multilabel Problems with Product Label Spaces"', '"ECCV 2010"', '["Convex Relaxation", "Convex Envelope", "Label Space", "Occlusion Detection", "Occlude Pixel"]', '"https://doi.org/10.1007/978-3-642-15555-0_17"', '"Convex relaxations for continuous multilabel problems have attracted a lot of interest recently [1,2,3,4,5]. Unfortunately, in previous methods, the runtime and memory requirements scale linearly in the total number of labels, making them very inefficient and often unapplicable for problems with higher dimensional label spaces. In this paper, we propose a reduction technique for the case that the label space is a product space, and introduce proper regularizers. The resulting convex relaxation requires orders of magnitude less memory and computation time than previously, which enables us to apply it to large-scale problems like optic flow, stereo with occlusion detection, and segmentation into a very large number of regions. Despite the drastic gain in performance, we do not arrive at less accurate solutions than the original relaxation. Using the novel method, we can for the first time efficiently compute solutions to the optic flow functional which are within provable bounds of typically 5% of the global optimum."'),
('"Convexity Shape Prior for Segmentation"', '"ECCV 2014"', '["segmentation", "convexity shape prior", "high-order functionals", "trust region", "graph cuts"]', '"https://doi.org/10.1007/978-3-319-10602-1_44"', '"Convexity is known as an important cue in human vision. We propose shape convexity as a new high-order regularization constraint for binary image segmentation. In the context of discrete optimization, object convexity is represented as a sum of 3-clique potentials penalizing any 1-0-1 configuration on all straight lines. We show that these non-submodular interactions can be efficiently optimized using a trust region approach. While the quadratic number of all 3-cliques is prohibitively high, we designed a dynamic programming technique for evaluating and approximating these cliques in linear time. Our experiments demonstrate general usefulness of the proposed convexity constraint on synthetic and real image segmentation examples. Unlike standard second-order length regularization, our convexity prior is scale invariant, does not have shrinking bias, and is virtually parameter-free."'),
('"Convolutional Learning of Spatio-temporal Features"', '"ECCV 2010"', '["Action Recognition", "Interest Point", "Image Patch", "Sparse Code", "Human Activity Recognition"]', '"https://doi.org/10.1007/978-3-642-15567-3_11"', '"We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent \\u201cflow fields\\u201d which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets."'),
('"Coplanar Common Points in Non-centric Cameras"', '"ECCV 2014"', '["Mirror Surface", "Symmetric Axis", "Vertical Slit", "Cylindrical Mirror", "Catadioptric Camera"]', '"https://doi.org/10.1007/978-3-319-10590-1_15"', '"Discovering and extracting new image features pertaining to scene geometry is important to 3D reconstruction and scene understanding. Examples include the classical vanishing points observed in a centric camera and the recent coplanar common points (CCPs) in a crossed-slit camera [21,17]. A CCP is a point in the image plane corresponding to the intersection of the projections of all lines lying on a common 3D plane. In this paper, we address the problem of determining CCP existence in general non-centric cameras. We first conduct a ray-space analysis to show that finding the CCP of a 3D plane is equivalent to solving an array of ray constraint equations. We then derive the necessary and sufficient conditions for CCP to exist in an arbitrary non-centric camera such as non-centric catadioptric mirrors. Finally, we present robust algorithms for extracting the CCPs from a single image and validate our theories and algorithms through experiments."'),
('"Coregistration: Simultaneous Alignment and Modeling of Articulated 3D Shape"', '"ECCV 2012"', '["Body Shape", "Shape Model", "Regularization Term", "Body Scan", "Template Mesh"]', '"https://doi.org/10.1007/978-3-642-33783-3_18"', '"Three-dimensional (3D) shape models are powerful because they enable the inference of object shape from incomplete, noisy, or ambiguous 2D or 3D data. For example, realistic parameterized 3D human body models have been used to infer the shape and pose of people from images. To train such models, a corpus of 3D body scans is typically brought into registration by aligning a common 3D human-shaped template to each scan. This is an ill-posed problem that typically involves solving an optimization problem with regularization terms that penalize implausible deformations of the template. When aligning a corpus, however, we can do better than generic regularization. If we have a model of how the template can deform then alignments can be regularized by this model. Constructing a model of deformations, however, requires having a corpus that is already registered. We address this chicken-and-egg problem by approaching modeling and registration together. By minimizing a single objective function, we reliably obtain high quality registration of noisy, incomplete, laser scans, while simultaneously learning a highly realistic articulated body model. The model greatly improves robustness to noise and missing data. Since the model explains a corpus of body scans, it captures how body shape varies across people and poses."'),
('"Correcting for Duplicate Scene Structure in Sparse 3D Reconstruction"', '"ECCV 2014"', '["Structure from motion", "duplicate structure disambiguation"]', '"https://doi.org/10.1007/978-3-319-10593-2_51"', '"Structure from motion (SfM) is a common technique to recover 3D geometry and camera poses from sets of images of a common scene. In many urban environments, however, there are symmetric, repetitive, or duplicate structures that pose challenges for SfM pipelines. The result of these ambiguous structures is incorrectly placed cameras and points within the reconstruction. In this paper, we present a post-processing method that can not only detect these errors, but successfully resolve them. Our novel approach proposes the strong and informative measure of conflicting observations, and we demonstrate that it is robust to a large variety of scenes."'),
('"Correlation-Based Intrinsic Image Extraction from a Single Image"', '"ECCV 2010"', '["Ground Truth", "Single Image", "Illumination Change", "Texture Modulation", "Ground Truth Image"]', '"https://doi.org/10.1007/978-3-642-15561-1_5"', '"Intrinsic images represent the underlying properties of a scene such as illumination (shading) and surface reflectance. Extracting intrinsic images is a challenging, ill-posed problem. Human performance on tasks such as shadow detection and shape-from-shading is improved by adding colour and texture to surfaces. In particular, when a surface is painted with a textured pattern, correlations between local mean luminance and local luminance amplitude promote the interpretation of luminance variations as illumination changes. Based on this finding, we propose a novel feature, local luminance amplitude, to separate illumination and reflectance, and a framework to integrate this cue with hue and texture to extract intrinsic images. The algorithm uses steerable filters to separate images into frequency and orientation components and constructs shading and reflectance images from weighted combinations of these components. Weights are determined by correlations between corresponding variations in local luminance, local amplitude, colour and texture. The intrinsic images are further refined by ensuring the consistency of local texture elements. We test this method on surfaces photographed under different lighting conditions. The effectiveness of the algorithm is demonstrated by the correlation between our intrinsic images and ground truth shading and reflectance data. Luminance amplitude was found to be a useful cue. Results are also presented for natural images."'),
('"Correspondences of Persistent Feature Points on Near-Isometric Surfaces"', '"ECCV 2012"', '["Feature Point", "Gaussian Curvature", "Markov Random Field", "Partial Match", "Heat Kernel Signatu', '"https://doi.org/10.1007/978-3-642-33863-2_11"', '"We present a full pipeline for finding corresponding points between two surfaces based on conceptually simple and computationally efficient components. Our pipeline begins with robust and stable extraction of feature points from the surfaces. We then find a set of near isometric correspondences between the feature points by solving an optimization problem using established components. The performance is evaluated on a large number of 3D models from the following perspectives: robustness w.r.t. isometric deformation, robustness w.r.t. noise and incomplete surfaces, partial matching, and anisometric deformation."'),
('"Cosegmentation Revisited: Models and Optimization"', '"ECCV 2010"', '["Color Histogram", "Appearance Model", "Foreground Object", "Subgradient Method", "Global Term"]', '"https://doi.org/10.1007/978-3-642-15552-9_34"', '"The problem of cosegmentation consists of segmenting the same object (or objects of the same class) in two or more distinct images. Recently a number of different models have been proposed for this problem. However, no comparison of such models and corresponding optimization techniques has been done so far. We analyze three existing models: the L1 norm model of Rother et al. [1], the L2 norm model of Mukherjee et al. [2] and the \\u201creward\\u201d model of Hochbaum and Singh [3]. We also study a new model, which is a straightforward extension of the Boykov-Jolly model for single image segmentation [4]."'),
('"Cost-Sensitive Top-Down/Bottom-Up Inference for Multiscale Activity Recognition"', '"ECCV 2012"', '["Group Activity", "Activity Recognition", "Child Node", "Coarse Scale", "Descriptor Vector"]', '"https://doi.org/10.1007/978-3-642-33765-9_14"', '"This paper addresses a new problem, that of multiscale activity recognition. Our goal is to detect and localize a wide range of activities, including individual actions and group activities, which may simultaneously co-occur in high-resolution video. The video resolution allows for digital zoom-in (or zoom-out) for examining fine details (or coarser scales), as needed for recognition. The key challenge is how to avoid running a multitude of detectors at all spatiotemporal scales, and yet arrive at a holistically consistent video interpretation. To this end, we use a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. The AND-OR graph allows a principled formulation of efficient, cost-sensitive inference via an explore-exploit strategy. Our inference optimally schedules the following computational processes: 1) direct application of activity detectors \\u2013 called \\u03b1 process; 2) bottom-up inference based on detecting activity parts \\u2013 called \\u03b2 process; and 3) top-down inference based on detecting activity context \\u2013 called \\u03b3 process. The scheduling iteratively maximizes the log-posteriors of the resulting parse graphs. For evaluation, we have compiled and benchmarked a new dataset of high-resolution videos of group and individual activities co-occurring in a courtyard of the UCLA campus."'),
('"Coupled Gaussian Process Regression for Pose-Invariant Facial Expression Recognition"', '"ECCV 2010"', '["Facial Expression", "Facial Expression Recognition", "Target Pair", "Landmark Point", "Facial Land', '"https://doi.org/10.1007/978-3-642-15552-9_26"', '"We present a novel framework for the recognition of facial expressions at arbitrary poses that is based on 2D geometric features. We address the problem by first mapping the 2D locations of landmark points of facial expressions in non-frontal poses to the corresponding locations in the frontal pose. Then, recognition of the expressions is performed by using any state-of-the-art facial expression recognition method (in our case, multi-class SVM). To learn the mappings that achieve pose normalization, we use a novel Gaussian Process Regression (GPR) model which we name Coupled Gaussian Process Regression (CGPR) model. Instead of learning single GPR model for all target pairs of poses at once, or learning one GPR model per target pair of poses independently of other pairs of poses, we propose CGPR model, which also models the couplings between the GPR models learned independently per target pairs of poses. To the best of our knowledge, the proposed method is the first one satisfying all: (i) being face-shape-model-free, (ii) handling expressive faces in the range from \\u2212\\u200945\\u00b0 to +\\u200945\\u00b0 pan rotation and from \\u2212\\u200930\\u00b0 to +\\u200930\\u00b0 tilt rotation, and (iii) performing accurately for continuous head pose despite the fact that the training was conducted only on a set of discrete poses."'),
('"Coupled Geodesic Active Regions for Image Segmentation: A Level Set Approach"', '"ECCV 2000"', '["Image Segmentation", "Minimum Description Length", "Active Contour Model", "Intensity Property", "', '"https://doi.org/10.1007/3-540-45053-X_15"', '"This paper presents a novel variational method for im age segmentation that unifies boundary and region-based information sources under the Geodesic Active Region framework. A statistical analysis based on the Minimum Description Length criterion and the Maximum Likelihood Principle for the observed density function (image histogram) using a mixture of Gaussian elements, indicates the number of the different regions and their intensity properties. Then, the boundary information is determined using a probabilistic edge detector, while the region information is estimated using the Gaussian components of the mixture model. The defined objective function is mini mized using a gradientdescent method where a level set approach is used to implement the resulting PDE system. According to the motion equations, the set of initial curves is propagated toward the segmentation result under the influence of boundary and region-based segmentation forces, and being constrained by a regularity force. The changes of topology are naturally handled thanks to the level set implementation, while a coupled multi-phase propagation is adopted that increases the robustness and the convergence rate by imposing the idea of mutually exclusive propagating curves. Finally, to reduce the required computational cost and the risk of convergence to local minima, a multi-scale approach is also considered. The performance of our method is demonstrated on a variety of real images."'),
('"Coupled Marginal Fisher Analysis for Low-Resolution Face Recognition"', '"ECCV 2012"', '["Face Recognition", "Linear Discriminant Analysis", "Recognition Rate", "Local Binary Pattern", "Pr', '"https://doi.org/10.1007/978-3-642-33868-7_24"', '"Many scenarios require that face recognition be performed at conditions that are not optimal. Traditional face recognition algorithms are not best suited for matching images captured at a low-resolution to a set of high-resolution gallery images. To perform matching between images of different resolutions, this work proposes a method of learning two sets of projections, one for high-resolution images and one for low-resolution images, based on local relationships in the data. Subsequent matching is done in a common subspace. Experiments show that our algorithm yields higher recognition rates than other similar methods."'),
('"Coupled-Contour Tracking through Non-orthogonal Projections and Fusion for Echocardiography"', '"ECCV 2004"', '["Motion Estimation", "Information Fusion", "Initial Contour", "Endocardial Border", "Tracking Frame', '"https://doi.org/10.1007/978-3-540-24670-1_26"', '"Existing methods for incorporating subspace model constraints in contour tracking use only partial information from the measurements and model distribution. We propose a complete fusion formulation for robust contour tracking, optimally resolving uncertainties from heteroscedastic measurement noise, system dynamics, and a subspace model. The resulting non-orthogonal subspace projection is a natural extension of the traditional model constraint using orthogonal projection. We build models for coupled double-contours, and exploit information from the ground truth initialization through a strong model adaptation. Our framework is applied for tracking in echocardiograms where the noise is heteroscedastic, each heart has distinct shape, and the relative motions of epi- and endocardial borders reveal crucial diagnostic features. The proposed method significantly outperforms the traditional shape-space-constrained tracking algorithm. Due to the joint fusion of heteroscedastic uncertainties, the strong model adaptation, and the coupled tracking of double-contours, robust performance is observed even on the most challenging cases."'),
('"Covariance Propagation and Next Best View Planning for 3D Reconstruction"', '"ECCV 2012"', '["Structure and motion", "covariance propagation", "next best view planning"]', '"https://doi.org/10.1007/978-3-642-33709-3_39"', '"This paper examines the potential benefits of applying next best view planning to sequential 3D reconstruction from unordered image sequences. A standard sequential structure-and-motion pipeline is extended with active selection of the order in which cameras are resectioned. To this end, approximate covariance propagation is implemented throughout the system, providing running estimates of the uncertainties of the reconstruction, while also enhancing robustness and accuracy. Experiments show that the use of expensive global bundle adjustment can be reduced throughout the process, while the additional cost of propagation is essentially linear in the problem size."'),
('"Covariant Derivatives and Vision"', '"ECCV 2006"', '["Covariant Derivative", "Human Visual System", "Image Space", "Texture Image", "Grayscale Image"]', '"https://doi.org/10.1007/11744085_5"', '"We describe a new theoretical approach to Image Processing and Vision. Expressed in mathemetical terminology, in our formalism image space is a fibre bundle, and the image itself is the graph of a section on it. This mathematical model has advantages to the conventional view of the image as a function on the plane: Based on the new method we are able to do image processing of the image as viewed by the human visual system, which includes adaptation and perceptual correctness of the results. Our formalism is invariant to relighting and handles seamlessly illumination change. It also explains simultaneous contrast visual illusions, which are intrinsically related to the new covariant approach."'),
('"Creating Summaries from User Videos"', '"ECCV 2014"', '["Video analysis", "video summarization", "temporal segmentation"]', '"https://doi.org/10.1007/978-3-319-10584-0_33"', '"This paper proposes a novel approach and a new benchmark for video summarization. Thereby we focus on user videos, which are raw videos containing a set of interesting events. Our method starts by segmenting the video by using a novel \\u201csuperframe\\u201d segmentation, tailored to raw videos. Then, we estimate visual interestingness per superframe using a set of low-, mid- and high-level features. Based on this scoring, we select an optimal subset of superframes to create an informative and interesting summary. The introduced benchmark comes with multiple human created summaries, which were acquired in a controlled psychological experiment. This data paves the way to evaluate summarization methods objectively and to get new insights in video summarization. When evaluating our method, we find that it generates high-quality results, comparable to manual, human-created summaries."'),
('"Crisp Boundary Detection Using Pointwise Mutual Information"', '"ECCV 2014"', '["Edge/Contour Detection", "Segmentation"]', '"https://doi.org/10.1007/978-3-319-10578-9_52"', '"Detecting boundaries between semantically meaningful objects in visual scenes is an important component of many vision algorithms. In this paper, we propose a novel method for detecting such boundaries based on a simple underlying principle: pixels belonging to the same object exhibit higher statistical dependencies than pixels belonging to different objects. We show how to derive an affinity measure based on this principle using pointwise mutual information, and we show that this measure is indeed a good predictor of whether or not two pixels reside on the same object. Using this affinity with spectral clustering, we can find object boundaries in the image \\u2013 achieving state-of-the-art results on the BSDS500 dataset. Our method produces pixel-level accurate boundaries while requiring minimal feature engineering."'),
('"Critical Curves and Surfaces for Euclidean Reconstruction"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_30"', '"The problem of recovering scene structure and camera motion from images has a number of inherent ambiguities. In this paper, configurations of points and cameras are analyzed for which the image points alone are insufficient to recover the scene geometry uniquely. Such configurations are said to be critical. For two views, it is well-known that a configuration is critical only if the two camera centres and all points lie on a ruled quadric. However, this is only a necessary condition. We give a complete characterization of the critical surfaces for two calibrated cameras and any number of points. Both algebraic and geometric characterizations of such surfaces are given. The existence of critical sets for n-view projective reconstruction has recently been reported in the literature. We show that there are critical sets for n-view Euclidean reconstruction as well. For example, it is shown that for any placement of three calibrated cameras, there always exists a critical set consisting of any number of points on a fourth-degree curve."'),
('"Critical Nets and Beta-Stable Features for Image Matching"', '"ECCV 2010"', '["Image Pair", "Common Structure", "Image Match", "Convex Region", "Image Part"]', '"https://doi.org/10.1007/978-3-642-15558-1_48"', '"We propose new ideas and efficient algorithms towards bridging the gap between bag-of-features and constellation descriptors for image matching. Specifically, we show how to compute connections between local image features in the form of a critical net whose construction is repeatable across changes of viewing conditions or scene configuration. Arcs of the net provide a more reliable frame of reference than individual features do for the purpose of invariance. In addition, regions associated with either small stars or loops in the critical net can be used as parts for recognition or retrieval, and subgraphs of the critical net that are matched across images exhibit common structures shared by different images. We also introduce the notion of beta-stable features, a variation on the notion of feature lifetime from the literature of scale space. Our experiments show that arc-based SIFT-like descriptors of beta-stable features are more repeatable and more accurate than competing descriptors. We also provide anecdotal evidence of the usefulness of image parts and of the structures that are found to be common across images."'),
('"Cross-Age Reference Coding for Age-Invariant Face Recognition and Retrieval"', '"ECCV 2014"', '["Face Recognition", "Aging"]', '"https://doi.org/10.1007/978-3-319-10599-4_49"', '"Recently, promising results have been shown on face recognition researches. However, face recognition and retrieval across age is still challenging. Unlike prior methods using complex models with strong parametric assumptions to model the aging process, we use a data-driven method to address this problem. We propose a novel coding framework called Cross-Age Reference Coding (CARC). By leveraging a large-scale image dataset freely available on the Internet as a reference set, CARC is able to encode the low-level feature of a face image with an age-invariant reference space. In the testing phase, the proposed method only requires a linear projection to encode the feature and therefore it is highly scalable. To thoroughly evaluate our work, we introduce a new large-scale dataset for face recognition and retrieval across age called Cross-Age Celebrity Dataset (CACD). The dataset contains more than 160,000 images of 2,000 celebrities with age ranging from 16 to 62. To the best of our knowledge, it is by far the largest publicly available cross-age face dataset. Experimental results show that the proposed method can achieve state-of-the-art performance on both our dataset as well as the other widely used dataset for face recognition across age, MORPH dataset."'),
('"Cross-View Action Recognition from Temporal Self-similarities"', '"ECCV 2008"', '["Recognition Accuracy", "Action Recognition", "Human Action Recognition", "Gait Recognition", "Moti', '"https://doi.org/10.1007/978-3-540-88688-4_22"', '"This paper concerns recognition of human actions under view changes. We explore self-similarities of action sequences over time and observe the striking stability of such measures across views. Building upon this key observation we develop an action descriptor that captures the structure of temporal similarities and dissimilarities within an action sequence. Despite this descriptor not being strictly view-invariant, we provide intuition and experimental validation demonstrating the high stability of self-similarities under view changes. Self-similarity descriptors are also shown stable under action variations within a class as well as discriminative for action recognition. Interestingly, self-similarities computed from different image features possess similar properties and can be used in a complementary fashion. Our method is simple and requires neither structure recovery nor multi-view correspondence estimation. Instead, it relies on weak geometric properties and combines them with machine learning for efficient cross-view action recognition. The method is validated on three public datasets, it has similar or superior performance compared to related methods and it performs well even in extreme conditions such as when recognizing actions from top views while using side views for training only."'),
('"Crosstalk Cascades for Frame-Rate Pedestrian Detection"', '"ECCV 2012"', '["Object Detection", "Miss Rate", "Pedestrian Detection", "Unsupervised Approach", "Rejection Thresh', '"https://doi.org/10.1007/978-3-642-33709-3_46"', '"Cascades help make sliding window object detection fast, nevertheless, computational demands remain prohibitive for numerous applications. Currently, evaluation of adjacent windows proceeds independently; this is suboptimal as detector responses at nearby locations and scales are correlated. We propose to exploit these correlations by tightly coupling detector evaluation of nearby windows. We introduce two opposing mechanisms: detector excitation of promising neighbors and inhibition of inferior neighbors. By enabling neighboring detectors to communicate, crosstalk cascades achieve major gains (4-30\\u00d7 speedup) over cascades evaluated independently at each image location. Combined with recent advances in fast multi-scale feature computation, for which we provide an optimized implementation, our approach runs at 35-65 fps on 640\\u00d7480 images while attaining state-of-the-art accuracy."'),
('"Crowd Detection with a Multiview Sampler"', '"ECCV 2010"', '["Camera View", "Foreground Pixel", "Pedestrian Detection", "Crowd Density", "Angular Extent"]', '"https://doi.org/10.1007/978-3-642-15555-0_24"', '"We present a Bayesian approach for simultaneously estimating the number of people in a crowd and their spatial locations by sampling from a posterior distribution over crowd configurations. Although this framework can be naturally extended from single to multiview detection, we show that the naive extension leads to an inefficient sampler that is easily trapped in local modes. We therefore develop a set of novel proposals that leverage multiview geometry to propose global moves that jump more efficiently between modes of the posterior distribution. We also develop a statistical model of crowd configurations that can handle dependencies among people and while not requiring discretization of their spatial locations. We quantitatively evaluate our algorithm on a publicly available benchmark dataset with different crowd densities and environmental conditions, and show that our approach outperforms other state-of-the-art methods for detecting and counting people in crowds."'),
('"Crowd Segmentation Through Emergent Labeling"', '"SMVP 2004"', '["Interest Point", "Maximal Clique", "Acceptance Probability", "Edge Strength", "Blob Detection"]', '"https://doi.org/10.1007/978-3-540-30212-4_17"', '"As an alternative to crowd segmentation using model-based object detection methods which depend on learned appearance models, we propose a paradigm that only makes use of low-level interest points. Here the detection of objects of interest is formulated as a clustering problem. The set of feature points are associated with vertices of a graph. Edges connect vertices based on the plausibility that the two vertices could have been generated from the same object. The task of object detection amounts to identifying a specific set of cliques of this graph. Since the topology of the graph is constrained by a geometric appearance model the maximal cliques can be enumerated directly. Each vertex of the graph can be a member of multiple maximal cliques. We need to find an assignment such that every vertex is only assigned to a single clique. An optimal assignment with respect to a global score function is estimated though a technique akin to soft-assign which can be viewed as a form of relaxation labeling that propagates constraints from regions of low to high ambiguity. No prior knowledge regarding the number of people in the scene is required."'),
('"Crowd Tracking with Dynamic Evolution of Group Structures"', '"ECCV 2014"', '["Spatial Constraint", "Coherent Motion", "Scene Structure", "Social Force Model", "Crowd Scene"]', '"https://doi.org/10.1007/978-3-319-10599-4_10"', '"Crowd tracking generates trajectories of a set of particles for further analysis of crowd motion patterns. In this paper, we try to answer the following questions: what are the particles appropriate for crowd tracking and how to track them robustly through crowd. Different than existing approaches of computing optical flows, tracking keypoints or pedestrians, we propose to discover distinctive and stable mid-level patches and track them jointly with dynamic evolution of group structures. This is achieved through the integration of low-level keypoint tracking, mid-level patch tracking, and high-level group evolution. Keypoint tracking guides the generation of patches with stable internal motions, and also organizes patches into hierarchical groups with collective motions. Patches are tracked together through occlusions with spatial constraints imposed by hierarchical tree structures within groups. Coherent groups are dynamically updated through merge and split events guided by keypoint tracking. The dynamically structured patches not only substantially improve the tracking for themselves, but also can assist the tracking of any other target in the crowd. The effectiveness of the proposed approach is shown through experiments and comparison with state-of-the-art trackers."'),
('"CSDD Features: Center-Surround Distribution Distance for Feature Extraction and Matching"', '"ECCV 2008"', '["Salience Detection", "Interest Region", "Interest Point Detector", "Salient Region Detector", "Blo', '"https://doi.org/10.1007/978-3-540-88690-7_11"', '"We present an interest region operator and feature descriptor called Center-Surround Distribution Distance (CSDD) that is based on comparing feature distributions between a central foreground region and a surrounding ring of background pixels. In addition to finding the usual light(dark) blobs surrounded by a dark(light) background, CSDD also detects blobs with arbitrary color distribution that \\u201cstand out\\u201d perceptually because they look different from the background. A proof-of-concept implementation using an isotropic scale-space extracts feature descriptors that are invariant to image rotation and covariant with change of scale. Detection repeatability is evaluated and compared with other state-of-the-art approaches using a standard dataset, while use of CSDD features for image registration is demonstrated within a RANSAC procedure for affine image matching."'),
('"CT from an Unmodified Standard Fluoroscopy Machine Using a Non-reproducible Path"', '"MMBIA 2004"', '["Tomographic Reconstruction", "Optical Tracker", "Reconstructed Volume", "World Coordinate System",', '"https://doi.org/10.1007/978-3-540-27816-0_2"', '"3D reconstruction from image data is required in many medical procedures. Recently, the use of fluoroscopy data to generate these 3D models has been explored. Most existing methods require knowledge of the scanning path either from precise hardware, or pre-calibration procedures. We propose an alternative of obtaining this needed pose information without the need of additional hardware or pre-calibration so that many existing fluoroscopes can be used."'),
('"Curvature-Preserving Regularization of Multi-valued Images Using PDE\\u2019s"', '"ECCV 2006"', '["Integral Curve", "Anisotropic Diffusion", "Integral Curf", "Smoothing Process", "Line Integral Con', '"https://doi.org/10.1007/11744047_23"', '"We are interested in diffusion PDE\\u2019s for smoothing multi-valued images in an anisotropic manner. By pointing out the pros and cons of existing tensor-driven regularization methods, we introduce a new constrained diffusion PDE that regularizes image data while taking curvatures of image structures into account. Our method has a direct link with a continuous formulation of the Line Integral Convolutions, allowing us to design a very fast and stable algorithm for its implementation. Besides, our smoothing scheme numerically performs with a sub-pixel accuracy and is then able to preserves very thin image structures contrary to classical PDE discretizations based on finite difference approximations. We illustrate our method with different applications on color images."'),
('"Cyclostationary Processes on Shape Spaces for Gait-Based Recognition"', '"ECCV 2006"', '["Gait Analysis", "Gait Cycle", "Dynamic Time Warping", "Shape Space", "Geodesic Path"]', '"https://doi.org/10.1007/11744047_34"', '"We present a novel approach to gait recognition that considers gait sequences as cyclostationary processes on a shape space of simple closed curves. Consequently, gait analysis reduces to quantifying differences between statistics underlying these stochastic processes. The main steps in the proposed approach are: (i) off-line extraction of human silhouettes from IR video data, (ii) use of piecewise-geodesic paths, connecting the observed shapes, to smoothly interpolate between them, (iii) computation of an average gait cycle within class (i.e. associated with a person) using average shapes, (iv) registration of average cycles using linear and nonlinear time scaling, (iv) comparisons of average cycles using geodesic lengths between the corresponding registered shapes. We illustrate this approach on infrared video clips involving 26 subjects."'),
('"CYKLS: Detect Pedestrian\\u2019s Dart Focusing on an Appearance Change"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33868-7_55"', '"We propose a new method for detecting \\u201cpedestrians\\u2019 dart\\u201d to support drivers cognition in real traffic scenario. The main idea is to detect sudden appearance change of pedestrians before their consequent actions happen. Our new algorithm, called \\u201cChronologically Yielded values of Kullback-Leibler divergence between Separate frames\\u201d (CYKLS), is a combination of two main procedures: (1) calculation of appearance change by Kullback-Leibler divergence between descriptors in some time interval frames, and (2) detection of non-periodic sequence by a new smoothing method in the field of time series analysis. We can detect pedestrians\\u2019 dart with 22% Equal Error Rate, using a dataset which includes 144 dart scenes."'),
('"DaMN \\u2013 Discriminative and Mutually Nearest: Exploiting Pairwise Category Proximity for Video A', '"ECCV 2014"', '["Computer Vision", "Action Recognition", "Semantic Attribute", "Late Fusion", "Strong Baseline"]', '"https://doi.org/10.1007/978-3-319-10578-9_47"', '"We propose a method for learning discriminative category-level features and demonstrate state-of-the-art results on large-scale action recognition in video. The key observation is that one-vs-rest classifiers, which are ubiquitously employed for this task, face challenges in separating very similar categories (such as running vs. jogging). Our proposed method automatically identifies such pairs of categories using a criterion of mutual pairwise proximity in the (kernelized) feature space, using a category-level similarity matrix where each entry corresponds to the one-vs-one SVM margin for pairs of categories. We then exploit the observation that while splitting such \\u201cSiamese Twin\\u201d categories may be difficult, separating them from the remaining categories in a two-vs-rest framework is not. This enables us to augment one-vs-rest classifiers with a judicious selection of \\u201ctwo-vs-rest\\u201d classifier outputs, formed from such discriminative and mutually nearest (DaMN) pairs. By combining one-vs-rest and two-vs-rest features in a principled probabilistic manner, we achieve state-of-the-art results on the UCF101 and HMDB51 datasets. More importantly, the same DaMN features, when treated as a mid-level representation also outperform existing methods in knowledge transfer experiments, both cross-dataset from UCF101 to HMDB51 and to new categories with limited training data (one-shot and few-shot learning). Finally, we study the generality of the proposed approach by applying DaMN to other classification tasks; our experiments show that DaMN outperforms related approaches in direct comparisons, not only on video action recognition but also on their original image dataset tasks."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Data-Driven Extraction of Curved Intersection Lanemarks from Road Traffic Image Sequences"', '"ECCV 2000"', '["Curve Section", "Curve Intersection", "Edge Element", "Spatial Description", "Vehicle Trajectory"]', '"https://doi.org/10.1007/3-540-45053-X_27"', '"Segmentation of optical flow fields, estimated by spatio-temporally adaptive methods, is - under favourable conditions - reliable enough to track moving vehicles at intersections without using vehicle or road models. Already a single image plane trajectory per lane obtained in this manner offers valuable information about where lane markers should be searched for. Fitting a hyperbola to an image plane trajectory of a vehicle which crosses an intersection thus provides concise geometric hints. These allow to separate images of direction indicators and of stop marks painted onto the road surface from side marks delimiting a lane. Such a \\u2018lane spine hyperbola\\u2019, moreover, facilitates to link side marks even across significant gaps in cluttered areas of a complex intersection. Data-driven extraction of trajectory information thus facilitates to link local spatial descriptions practically across the entire field of view in order to create global spatial descriptions. These results are important since they allow to extract required information from image sequences of traffic scenes without the necessity to obtain a map of the road structure and to make this information (interactively) available to a machine-vision-based traffic surveillance system."'),
('"Data-Driven Vehicle Identification by Image Matching"', '"ECCV 2012"', '["Gaussian Mixture Model", "Image Retrieval", "Plate Image", "Image Match", "License Plate"]', '"https://doi.org/10.1007/978-3-642-33868-7_53"', '"Vehicle identification from images has been predominantly addressed through automatic license plate recognition (ALPR) techniques which detect and recognize the characters in the plate region of the image. We move away from traditional ALPR techniques and advocate for a data-driven approach for vehicle identification. Here, given a plate image region, the idea is to search for a near-duplicate image in an annotated database; if found, the identity of the near-duplicate is transferred to the input region. Although this approach could be perceived as impractical, we actually demonstrate that it is feasible with state-of-the-art image representations, and that it presents some advantages in terms of speed, and time-to-deploy. To overcome the issue of identifying previously unseen identities, we propose an image simulation approach where photo-realistic images of license plates are generated for desired plate numbers. We demonstrate that there is no perceivable performance difference between using synthetic and real plates. We also improve the matching accuracy using similarity learning, which is in the spirit of domain adaptation."'),
('"Database-Guided Simultaneous Multi-slice 3D Segmentation for Volumetric Data"', '"ECCV 2006"', '["Segmentation Method", "Volumetric Data", "Active Appearance Model", "Object Appearance", "Multiple', '"https://doi.org/10.1007/11744085_31"', '"Automatic delineation of anatomical structures in 3-D volumetric data is a challenging task due to the complexity of the object appearance as well as the quantity of information to be processed. This makes it increasingly difficult to encode prior knowledge about the object segmentation in a traditional formulation as a perceptual grouping task. We introduce a fast shape segmentation method for 3-D volumetric data by extending the 2-D database-guided segmentation paradigm which directly exploits expert annotations of the interest object in large medical databases. Rather than dealing with 3-D data directly, we take advantage of the observation that the information about position and appearance of a 3-D shape can be characterized by a set of 2-D slices. Cutting these multiple slices simultaneously from the 3-D shape allows us to represent and process 3-D data as efficiently as 2-D images while keeping most of the information about the 3-D shape. To cut slices consistently for all shapes, an iterative 3-D non-rigid shape alignment method is also proposed for building local coordinates for each shape. Features from all the slices are jointly used to learn to discriminate between the object appearance and background and to learn the association between appearance and shape. The resulting procedure is able to perform shape segmentation in only a few seconds. Extensive experiments on cardiac ultrasound images demonstrate the algorithm\\u2019s accuracy and robustness in the presence of large amounts of noise."'),
('"Dating Historical Color Images"', '"ECCV 2012"', '["Color Process", "Scene Recognition", "Historical Image", "Amazon Mechanical Turk", "Joint Histogra', '"https://doi.org/10.1007/978-3-642-33783-3_36"', '"We introduce the task of automatically estimating the age of historical color photographs. We suggest features which attempt to capture temporally discriminative information based on the evolution of color imaging processes over time and evaluate the performance of both these novel features and existing features commonly utilized in other problem domains on a novel historical image data set. For the challenging classification task of sorting historical color images into the decade during which they were photographed, we demonstrate significantly greater accuracy than that shown by untrained humans on the same data set. Additionally, we apply the concept of data-driven camera response function estimation to historical color imagery, demonstrating its relevance to both the age estimation task and the popular application of imitating the appearance of vintage color photography."'),
('"Deblurring Face Images with Exemplars"', '"ECCV 2014"', '["Face Image", "Kernel Estimation", "Error Ratio", "Ringing Artifact", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-319-10584-0_4"', '"The human face is one of the most interesting subjects involved in numerous applications. Significant progress has been made towards the image deblurring problem, however, existing generic deblurring methods are not able to achieve satisfying results on blurry face images. The success of the state-of-the-art image deblurring methods stems mainly from implicit or explicit restoration of salient edges for kernel estimation. When there is not much texture in the blurry image (e.g., face images), existing methods are less effective as only few edges can be used for kernel estimation. Moreover, recent methods are usually jeopardized by selecting ambiguous edges, which are imaged from the same edge of the object after blur, for kernel estimation due to local edge selection strategies. In this paper, we address these problems of deblurring face images by exploiting facial structures. We propose a maximum a posteriori (MAP) deblurring algorithm based on an exemplar dataset, without using the coarse-to-fine strategy or ad-hoc edge selections. Extensive evaluations against state-of-the-art methods demonstrate the effectiveness of the proposed algorithm for deblurring face images. We also show the extendability of our method to other specific deblurring tasks."'),
('"Decision Theoretic Modeling of Human Facial Displays"', '"ECCV 2004"', '["Video Sequence", "Dynamic Bayesian Network", "Facial Motion", "Conditional Probability Distributio', '"https://doi.org/10.1007/978-3-540-24672-5_3"', '"We present a vision based, adaptive, decision theoretic model of human facial displays in interactions. The model is a partially observable Markov decision process, or POMDP. A POMDP is a stochastic planner used by an agent to relate its actions and utility function to its observations and to other context. Video observations are integrated into the POMDP using a dynamic Bayesian network that creates spatial and temporal abstractions of the input sequences. The parameters of the model are learned from training data using an a-posteriori constrained optimization technique based on the expectation-maximization algorithm. The training does not require facial display labels on the training data. The learning process discovers clusters of facial display sequences and their relationship to the context automatically. This avoids the need for human intervention in training data collection, and allows the models to be used without modification for facial display learning in any context without prior knowledge of the type of behaviors to be used. We present an experimental paradigm in which we record two humans playing a game, and learn the POMDP model of their behaviours. The learned model correctly predicts human actions during a simple cooperative card game based, in part, on their facial displays."'),
('"Deconvolving PSFs for a Better Motion Deblurring Using Multiple Images"', '"ECCV 2012"', '["Blind deconvolution", "motion blur", "PSF"]', '"https://doi.org/10.1007/978-3-642-33715-4_46"', '"Blind deconvolution of motion blur is hard, but it can be made easier if multiple images are available. This observation, and an algorithm using two differently-blurred images of a scene are the subject of this paper. While this idea is not new, existing methods have so far not delivered very practical results. In practice, the PSFs corresponding to the two given images are estimated and assumed to be close to the latent motion blurs. But in actual fact, these estimated blurs are often far from the truth, for a simple reason: They often share a common, and unidentified PSF that goes unaccounted for. That is, the estimated PSFs are themselves \\u201cblurry\\u201d. While this can be due to any number of other blur sources including shallow depth of field, out of focus, lens aberrations, diffraction effects, and the like, it is also a mathematical artifact of the ill-posedness of the deconvolution problem. In this paper, instead of estimating the PSFs directly and only once from the observed images, we first generate a rough estimate of the PSFs using a robust multichannel deconvolution algorithm, and then \\u201cdeconvolve the PSFs\\u201d to refine the outputs. Simulated and real data experiments show that this strategy works quite well for motion blurred images, producing state of the art results."'),
('"Deep Dynamic Neural Networks for Gesture Segmentation and Recognition"', '"ECCV 2014"', '["Deep Belief Networks", "3D Convolutional Neural Networks", "Gesture recognition", "ChaLearn"]', '"https://doi.org/10.1007/978-3-319-16178-5_39"', '"The purpose of this paper is to describe a novel method called Deep Dynamic Neural Networks(DDNN) for the Track 3 of the Chalearn Looking at People 2014 challenge [1]. A generalised semi-supervised hierarchical dynamic framework is proposed for simultaneous gesture segmentation and recognition taking both skeleton and depth images as input modules. First, Deep Belief Networks(DBN) and 3D Convolutional Neural Networks (3DCNN) are adopted for skeletal and depth data accordingly to extract high level spatio-temporal features. Then the learned representations are used for estimating emission probabilities of the Hidden Markov Models to infer an action sequence. The framework can be easily extended by including an ergodic state to segment and recognise video sequences by a frame-to-frame mechanism, rendering it possible for online segmentation and recognition for diverse input modules. Some normalisation details pertaining to preprocessing raw features are also discussed. This purely data-driven approach achieves 0.8162 score in this gesture spotting challenge. The performance is on par with a variety of the state-of-the-art hand-tuned-feature approaches and other learning-based methods, opening the doors for using deep learning techniques to explore time series multimodal data."'),
('"Deep Features for Text Spotting"', '"ECCV 2014"', '["Word Recognition", "Convolutional Neural Network", "Text Line", "Text Detection", "Street View"]', '"https://doi.org/10.1007/978-3-319-10593-2_34"', '"The goal of this work is text spotting in natural images. This is divided into two sequential tasks: detecting words regions in the image, and recognizing the words within these regions. We make the following contributions: first, we develop a Convolutional Neural Network (CNN) classifier that can be used for both tasks. The CNN has a novel architecture that enables efficient feature sharing (by using a number of layers in common) for text detection, character case-sensitive and insensitive classification, and bigram classification. It exceeds the state-of-the-art performance for all of these. Second, we make a number of technical changes over the traditional CNN architectures, including no downsampling for a per-pixel sliding window, and multi-mode learning with a mixture of linear models (maxout). Third, we have a method of automated data mining of Flickr, that generates word and character level annotations. Finally, these components are used together to form an end-to-end, state-of-the-art text spotting system. We evaluate the text-spotting system on two standard benchmarks, the ICDAR Robust Reading data set and the Street View Text data set, and demonstrate improvements over the state-of-the-art on multiple measures."'),
('"Deep Learning of Scene-Specific Classifier for Pedestrian Detection"', '"ECCV 2014"', '["Domain Adaptation", "Convolutional Neural Network", "Transfer Learning", "Pedestrian Detection", "', '"https://doi.org/10.1007/978-3-319-10578-9_31"', '"The performance of a detector depends much on its training dataset and drops significantly when the detector is applied to a new scene due to the large variations between the source training dataset and the target scene. In order to bridge this appearance gap, we propose a deep model to automatically learn scene-specific features and visual patterns in static video surveillance without any manual labels from the target scene. It jointly learns a scene-specific classifier and the distribution of the target samples. Both tasks share multi-scale feature representations with both discriminative and representative power. We also propose a cluster layer in the deep model that utilizes the scene-specific visual patterns for pedestrian detection. Our specifically designed objective function not only incorporates the confidence scores of target training samples but also automatically weights the importance of source training samples by fitting the marginal distributions of target samples. It significantly improves the detection rates at 1 FPPI by 10% compared with the state-of-the-art domain adaptation methods on MIT Traffic Dataset and CUHK Square Dataset."'),
('"Deep Network Cascade for Image Super-resolution"', '"ECCV 2014"', '["Super-resolution", "Auto-encoder", "Deep learning"]', '"https://doi.org/10.1007/978-3-319-10602-1_4"', '"In this paper, we propose a new model called deep network cascade (DNC) to gradually upscale low-resolution images layer by layer, each layer with a small scale factor. DNC is a cascade of multiple stacked collaborative local auto-encoders. In each layer of the cascade, non-local self-similarity search is first performed to enhance high-frequency texture details of the partitioned patches in the input image. The enhanced image patches are then input into a collaborative local auto-encoder (CLA) to suppress the noises as well as collaborate the compatibility of the overlapping patches. By closing the loop on non-local self-similarity search and CLA in a cascade layer, we can refine the super-resolution result, which is further fed into next layer until the required image scale. Experiments on image super-resolution demonstrate that the proposed DNC can gradually upscale a low-resolution image with the increase of network layers and achieve more promising results in visual quality as well as quantitative performance."'),
('"Defocus Inpainting"', '"ECCV 2006"', '["Input Image", "Point Spread Function", "Image Restoration", "Deblurred Image", "Corneal Imaging"]', '"https://doi.org/10.1007/11744047_27"', '"In this paper, we propose a method to restore a single image affected by space-varying blur. The main novelty of our method is the use of recurring patterns as regularization during the restoration process. We postulate that restored patterns in the deblurred image should resemble other sharp details in the input image. To this purpose, we establish the correspondence of regions that are similar up to Gaussian blur. When two regions are in correspondence, one can perform deblurring by using the sharpest of the two as a proposal. Our solution consists of two steps: First, estimate correspondence of similar patches and their relative amount of blurring; second, restore the input image by imposing the similarity of such recurring patterns as a prior. Our approach has been successfully tested on both real and synthetic data."'),
('"Deformable Image Registration by Adaptive Gaussian Forces"', '"MMBIA 2004"', '["Control Point", "Radial Basis Function", "Image Registration", "Radial Basis Function Neural Netwo', '"https://doi.org/10.1007/978-3-540-27816-0_27"', '"This paper introduces a novel physics-based approach to elastic image registration. It is based on applying Gaussian-shaped forces at irregularly distributed control points in the image, which is considered to be an infinite elastic continuum. The positions of the control points, the directions and magnitudes of the applied forces as well as their influence areas, and the elastic material properties are optimized to reach a maximum of the similarity measure between the images. The use of the adaptive irregular grid potentially allows to achieve good registration quality by using fewer parameters as compared to regular grids, e.g. B-splines. The feasibility of the proposed approach is tested on clinical images, and open problems and directions for future work are discussed."'),
('"Deformable Model with Non-euclidean Metrics"', '"ECCV 2002"', '["image segmentation", "deformable model", "non-Euclidean geometry", "topology adaptation", "optimiz', '"https://doi.org/10.1007/3-540-47977-5_29"', '"Deformable models like snakes are a classical tool for image segmentation. Highly deformable models extend them with the ability to handle dynamic topological changes, and therefore to extract arbitrary complex shapes. However, the resolution of these models largely depends on the resolution of the image. As a consequence, their time and memory complexity increases at least as fast as the size of input data. In this paper we extend an existing highly deformable model, so that it is able to locally adapt its resolution with respect to its position. With this property, a significant precision is achieved in the interesting parts of the image, while a coarse resolution is maintained elsewhere. The general idea is to replace the Euclidean metric of the image space by a deformed non-Euclidean metric, which geometrically expands areas of interest. With this approach, we obtain a new model that follows the robust framework of classical deformable models, while offering a significant independence from both the size of input data and the geometric complexity of image components."'),
('"Deformed Lattice Discovery Via Efficient Mean-Shift Belief Propagation"', '"ECCV 2008"', '["Belief Propagation", "Interest Point", "Real Image", "Markov Random Field", "Texture Element"]', '"https://doi.org/10.1007/978-3-540-88688-4_35"', '"We introduce a novel framework for automatic detection of repeated patterns in real images. The novelty of our work is to formulate the extraction of an underlying deformed lattice as a spatial, multi-target tracking problem using a new and efficient Mean-Shift Belief Propagation (MSBP) method. Compared to existing work, our approach has multiple advantages, including: 1) incorporating higher order constraints early-on to propose highly plausible lattice points; 2) growing a lattice in multiple directions simultaneously instead of one at a time sequentially; and 3) achieving more efficient and more accurate performance than state-of-the-art algorithms. These advantages are demonstrated by quantitative experimental results on a diverse set of real world photos."'),
('"DEFORMOTION Deforming Motion, Shape Average and the Joint Registration and Segmentation of Images"', '"ECCV 2002"', '["Group Action", "Shape Average", "Signed Distance Function", "General Deformation", "Euclidean Grou', '"https://doi.org/10.1007/3-540-47977-5_3"', '"What does it mean for a deforming object to be \\u201cmoving\\u201d (see Fig.1)? How can we separate the overall motion (a finite-dimensional group action) from the more general deformation (a diffeomorphism)? In this paper we propose a definition of motion for a deforming object and introduce a notion of \\u201cshape average\\u201d as the entity that separates the motion from the deformation. Our definition allows us to derive novel and efficient algorithms to register non-equivalent shapes using region-based methods, and to simultaneously approximate and register structures in grey-scale images. We also extend the notion of shape average to that of a \\u201cmoving average\\u201d in order to track moving and deforming objects through time."'),
('"Degen Generalized Cylinders and Their Properties"', '"ECCV 2006"', '["Computer Vision", "Tangent Plane", "Machine Intelligence", "Invariant Property", "Projective Geome', '"https://doi.org/10.1007/11744023_7"', '"Generalized cylinder (GC) has played an important role in computer vision since it was introduced in the 1970s. While studying GC models in human visual perception of shapes from contours, Marr assumed that GC\\u2019s limbs are planar curves. Later, Koenderink and Ponce pointed out that this assumption does not hold in general by giving some examples. In this paper, we show that straight homogeneous generalized cylinders (SHGCs) and tori (a kind of curved GCs) have planar limbs when viewed from points on specific straight lines. This property leads us to the definition and investigation of a new class of GCs, with the help of the surface model proposed by Degen for geometric modeling. We call them Degen generalized cylinders (DGCs), which include SHGCs, tori, quadrics, cyclides, and more other GCs into one model. Our rigorous discussion is based on projective geometry and homogeneous coordinates. We present some invariant properties of DGCs that reveal the relations among the planar limbs, axes, and contours of DGCs. These properties are useful for recovering DGC descriptions from image contours as well as for some other tasks in computer vision."'),
('"Dense Motion Analysis in Fluid Imagery"', '"ECCV 2002"', '["Singular Point", "Motion Estimation", "Velocity Potential", "Bhattacharyya Distance", "Dense Motio', '"https://doi.org/10.1007/3-540-47969-4_45"', '"Analyzing fluid motion is essential in number of domains and can rarely be handled using generic computer vision techniques. In this particular application context, we address two distinct problems. First we describe a dedicated dense motion estimator. The approach relies on constraints issuing from fluid motion properties and allows us to recover dense motion fields of good quality. Secondly, we address the problem of analyzing such velocity fields. We present a kind of motion-based segmentation relying on an analytic representation of the motion field that permits to extract important quantities such as singularities, stream-functions or velocity potentials. The proposed method has the advantage to be robust, simple, and fast."'),
('"Dense Photometric Stereo by Expectation Maximization"', '"ECCV 2006"', '["Expectation Maximization", "Markov Random Field", "Expectation Maximization Algorithm", "Light Dir', '"https://doi.org/10.1007/11744085_13"', '"We formulate a robust method using Expectation Maximization (EM) to address the problem of dense photometric stereo. Previous approaches using Markov Random Fields (MRF) utilized a dense set of noisy photometric images for estimating an initial normal to encode the matching cost at each pixel, followed by normal refinement by considering the neighborhood of the pixel. In this paper, we argue that they had not fully utilized the inherent data redundancy in the dense set and that its full exploitation leads to considerable improvement. Using the same noisy and dense input, this paper contributes in learning relevant observations, recovering accurate normals and very good surface albedos, and inferring optimal parameters in an unifying EM framework that converges to an optimal solution and has no free user-supplied parameter to set. Experiments show that our EM approach for dense photometric stereo outperforms the previous approaches using the same input."'),
('"Dense Point Trajectories by GPU-Accelerated Large Displacement Optical Flow"', '"ECCV 2010"', '["Conjugate Gradient", "Large Displacement", "Linear Solver", "Point Tracking", "Point Iteration"]', '"https://doi.org/10.1007/978-3-642-15549-9_32"', '"Dense and accurate motion tracking is an important requirement for many video feature extraction algorithms. In this paper we provide a method for computing point trajectories based on a fast parallel implementation of a recent optical flow algorithm that tolerates fast motion. The parallel implementation of large displacement optical flow runs about 78\\u00d7 faster than the serial C++ version. This makes it practical to use in a variety of applications, among them point tracking. In the course of obtaining the fast implementation, we also proved that the fixed point matrix obtained in the optical flow technique is positive semi-definite. We compare the point tracking to the most commonly used motion tracker - the KLT tracker - on a number of sequences with ground truth motion. Our resulting technique tracks up to three orders of magnitude more points and is 46% more accurate than the KLT tracker. It also provides a tracking density of 48% and has an occlusion error of 3% compared to a density of 0.1% and occlusion error of 8% for the KLT tracker. Compared to the Particle Video tracker, we achieve 66% better accuracy while retaining the ability to handle large displacements while running an order of magnitude faster."'),
('"Dense Semi-rigid Scene Flow Estimation from RGBD Images"', '"ECCV 2014"', '["motion", "scene flow", "RGBD image"]', '"https://doi.org/10.1007/978-3-319-10584-0_37"', '"Scene flow is defined as the motion field in 3D space, and can be computed from a single view when using an RGBD sensor. We propose a new scene flow approach that exploits the local and piecewise rigidity of real world scenes. By modeling the motion as a field of twists, our method encourages piecewise smooth solutions of rigid body motions. We give a general formulation to solve for local and global rigid motions by jointly using intensity and depth data. In order to deal efficiently with a moving camera, we model the motion as a rigid component plus a non-rigid residual and propose an alternating solver. The evaluation demonstrates that the proposed method achieves the best results in the most commonly used scene flow benchmark. Through additional experiments we indicate the general applicability of our approach in a variety of different scenarios."'),
('"Dense Structure-from-Motion: An Approach Based on Segment Matching"', '"ECCV 2002"', '["Motion Vector", "Camera Motion", "Camera Calibration", "Error Curve", "Relaxation Algorithm"]', '"https://doi.org/10.1007/3-540-47967-8_15"', '"For 3-D video applications, dense depth maps are required. We present a segment-based structure-from-motion technique. After image segmentation, we estimate the motion of each segment. With knowledge of the camera motion, this can be translated into depth. The optimal depth is found by minimizing a suitable error norm, which can handle occlusions as well. This method combines the advantages of motion estimation on the one hand, and structure-from-motion algorithms on the other hand. The resulting depth maps are pixel-accurate due to the segmentation, and have a high accuracy: depth differences corresponding to motion differences of 1/8th of a pixel can be recovered."'),
('"Dense, Robust, and Accurate Motion Field Estimation from Stereo Image Sequences in Real-Time"', '"ECCV 2010"', '["Root Mean Square", "Motion Vector", "Disparity Estimation", "Scene Flow", "Dense Stereo"]', '"https://doi.org/10.1007/978-3-642-15561-1_42"', '"In this paper a novel approach for estimating the three dimensional motion field of the visible world from stereo image sequences is proposed. This approach combines dense variational optical flow estimation, including spatial regularization, with Kalman filtering for temporal smoothness and robustness. The result is a dense, robust, and accurate reconstruction of the three-dimensional motion field of the current scene that is computed in real-time. Parallel implementation on a GPU and an FPGA yields a vision-system which is directly applicable in real-world scenarios, like automotive driver assistance systems or in the field of surveillance. Within this paper we systematically show that the proposed algorithm is physically motivated and that it outperforms existing approaches with respect to computation time and accuracy."'),
('"Density Estimation Using Mixtures of Mixtures of Gaussians"', '"ECCV 2006"', '["Bayesian Information Criterion", "Expectation Maximization", "Segmentation Result", "Mixture Compo', '"https://doi.org/10.1007/11744085_32"', '"In this paper we present a new density estimation algorithm using mixtures of mixtures of Gaussians. The new algorithm overcomes the limitations of the popular Expectation Maximization algorithm. The paper first introduces a new model selection criterion called the Penalty-less Information Criterion, which is based on the Jensen-Shannon divergence. Mean-shift is used to automatically initialize the means and covariances of the Expectation Maximization in order to obtain better structure inference. Finally, a locally linear search is performed using the Penalty-less Information Criterion in order to infer the underlying density of the data. The validity of the algorithm is verified using real color images."'),
('"Depth and Arbitrary Motion Deblurring Using Integrated PSF"', '"ECCV 2014"', '["Coded imaging", "PSF", "Deblur", "Motion Blur", "All-in-Focus"]', '"https://doi.org/10.1007/978-3-319-16181-5_44"', '"In recent years, research for recovering depth blur and motion blur in images has been making a significant progress. In particular, the progress in computational photography enabled us to generate all-in-focus images and control depth of field in images. However, the simultaneous recovery of depth and motion blurs is still a big problem, and recoverable motion blurs are limited."'),
('"Depth and Deblurring from a Spectrally-Varying Depth-of-Field"', '"ECCV 2012"', '["Color Channel", "Depth Estimation", "Green Channel", "Sharp Image", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-642-33715-4_47"', '"We propose modifying the aperture of a conventional color camera so that the effective aperture size for one color channel is smaller than that for the other two. This produces an image where different color channels have different depths-of-field, and from this we can computationally recover scene depth, reconstruct an all-focus image and achieve synthetic re-focusing, all from a single shot. These capabilities are enabled by a spatio-spectral image model that encodes the statistical relationship between gradient profiles across color channels. This approach substantially improves depth accuracy over alternative single-shot coded-aperture designs, and since it avoids introducing additional spatial distortions and is light efficient, it allows high-quality deblurring and lower exposure times. We demonstrate these benefits with comparisons on synthetic data, as well as results on images captured with a prototype lens."'),
('"Depth Based Object Detection from Partial Pose Estimation of Symmetric Objects"', '"ECCV 2014"', '["Object detection", "3D computer vision", "Range data", "Partial pose estimation"]', '"https://doi.org/10.1007/978-3-319-10602-1_25"', '"Category-level object detection, the task of locating object instances of a given category in images, has been tackled with many algorithms employing standard color images. Less attention has been given to solving it using range and depth data, which has lately become readily available using laser and RGB-D cameras. Exploiting the different nature of the depth modality, we propose a novel shape-based object detector with partial pose estimation for axial or reflection symmetric objects. We estimate this partial pose by detecting target\\u2019s symmetry, which as a global mid-level feature provides us with a robust frame of reference with which shape features are represented for detection. Results are shown on a particularly challenging depth dataset and exhibit significant improvement compared to the prior art."'),
('"Depth Enhancement by Fusion for Passive and Active Sensing"', '"ECCV 2012"', '["depth enhancement", "data fusion", "passive sensing", "active sensing"]', '"https://doi.org/10.1007/978-3-642-33885-4_51"', '"This paper presents a general refinement procedure that enhances any given depth map obtained by passive or active sensing. Given a depth map, either estimated by triangulation methods or directly provided by the sensing system, and its corresponding 2-D image, we correct the depth values by separately treating regions with undesired effects such as empty holes, texture copying or edge blurring due to homogeneous regions, occlusions, and shadowing. In this work, we use recent depth enhancement filters intended for Time-of-Flight cameras, and adapt them to alternative depth sensing modalities, both active using an RGB-D camera and passive using a dense stereo camera. To that end, we propose specific masks to tackle areas in the scene that require a special treatment. Our experimental results show that such areas are satisfactorily handled by replacing erroneous depth measurements with accurate ones."'),
('"Depth Estimation for Glossy Surfaces with Light-Field Cameras"', '"ECCV 2014"', '["Markov Random Fields", "Depth Estimation", "Color Constancy", "Specular Surface", "Specular Compon', '"https://doi.org/10.1007/978-3-319-16181-5_41"', '"Light-field cameras have now become available in both consumer and industrial applications, and recent papers have demonstrated practical algorithms for depth recovery from a passive single-shot capture. However, current light-field depth estimation methods are designed for Lambertian objects and fail or degrade for glossy or specular surfaces. Because light-field cameras have an array of micro-lenses, the captured data allows modification of both focus and perspective viewpoints. In this paper, we develop an iterative approach to use the benefits of light-field data to estimate and remove the specular component, improving the depth estimation. The approach enables light-field data depth estimation to support both specular and diffuse scenes. We present a physically-based method that estimates one or multiple light source colors. We show our method outperforms current state-of-the-art diffuse and specular separation and depth estimation algorithms in multiple real world scenarios."'),
('"Depth Extraction from Video Using Non-parametric Sampling"', '"ECCV 2012"', '["Input Image", "Depth Estimation", "Motion Parallax", "Dynamic Scene", "Motion Segmentation"]', '"https://doi.org/10.1007/978-3-642-33715-4_56"', '"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade."'),
('"Depth Matters: Influence of Depth Cues on Visual Saliency"', '"ECCV 2012"', '["Visual Attention", "Saliency Detection", "Visual Saliency", "Depth Plane", "Saliency Model"]', '"https://doi.org/10.1007/978-3-642-33709-3_8"', '"Most previous studies on visual saliency have only focused on static or dynamic 2D scenes. Since the human visual system has evolved predominantly in natural three dimensional environments, it is important to study whether and how depth information influences visual saliency. In this work, we first collect a large human eye fixation database compiled from a pool of 600 2D-vs-3D image pairs viewed by 80 subjects, where the depth information is directly provided by the Kinect camera and the eye tracking data are captured in both 2D and 3D free-viewing experiments. We then analyze the major discrepancies between 2D and 3D human fixation data of the same scenes, which are further abstracted and modeled as novel depth priors. Finally, we evaluate the performances of state-of-the-art saliency detection models over 3D images, and propose solutions to enhance their performances by integrating the depth priors."'),
('"Depth Recovery Using an Adaptive Color-Guided Auto-Regressive Model"', '"ECCV 2012"', '["Depth recovery", "AR model", "nonlocal filtering", "depth camera"]', '"https://doi.org/10.1007/978-3-642-33715-4_12"', '"This paper proposes an adaptive color-guided auto-regressive (AR) model for high quality depth recovery from low quality measurements captured by depth cameras. We formulate the depth recovery task into a minimization of AR prediction errors subject to measurement consistency. The AR predictor for each pixel is constructed according to both the local correlation in the initial depth map and the nonlocal similarity in the accompanied high quality color image. Experimental results show that our method outperforms existing state-of-the-art schemes, and is versatile for both mainstream depth sensors: ToF camera and Kinect."'),
('"Depth-Encoded Hough Voting for Joint Object Detection and Shape Recovery"', '"ECCV 2010"', '["Image Patch", "Object Instance", "Pascal VOC07", "Object Hypothesis", "Object Depth"]', '"https://doi.org/10.1007/978-3-642-15555-0_48"', '"Detecting objects, estimating their pose and recovering 3D shape information are critical problems in many vision and robotics applications. This paper addresses the above needs by proposing a new method called DEHV - Depth-Encoded Hough Voting detection scheme. Inspired by the Hough voting scheme introduced in [13], DEHV incorporates depth information into the process of learning distributions of image features (patches) representing an object category. DEHV takes advantage of the interplay between the scale of each object patch in the image and its distance (depth) from the corresponding physical patch attached to the 3D object. DEHV jointly detects objects, infers their categories, estimates their pose, and infers/decodes objects depth maps from either a single image (when no depth maps are available in testing) or a single image augmented with depth map (when this is available in testing). Extensive quantitative and qualitative experimental analysis on existing datasets [6,9,22] and a newly proposed 3D table-top object category dataset shows that our DEHV scheme obtains competitive detection and pose estimation results as well as convincing 3D shape reconstruction from just one single uncalibrated image. Finally, we demonstrate that our technique can be successfully employed as a key building block in two application scenarios (highly accurate 6 degrees of freedom (6 DOF) pose estimation and 3D object modeling)."'),
('"Depth-of-Field and Coded Aperture Imaging on XSlit Lens"', '"ECCV 2014"', '["Point Spread Function", "Cylindrical Lens", "Spherical Lens", "Blur Kernel", "Iteratively Reweight', '"https://doi.org/10.1007/978-3-319-10578-9_49"', '"Recent coded aperture imaging systems have shown great success in scene reconstruction, extended depth-of-field and light field imaging. By far nearly all solutions are built on top of commodity cameras equipped with a single spherical lens. In this paper, we explore coded aperture solutions on a special non-centric lens called the crossed-slit (XSlit) lens. An XSlit lens uses a relay of two orthogonal cylindrical lenses, each coupled with a slit-shaped aperture. Through ray geometry analysis, we first show that the XSlit lens produces a different and potentially advantageous depth-of-field than the regular spherical lens. We then present a coded aperture strategy that individually encodes each slit aperture, one with broadband code and the other with high depth discrepancy code, for scene recovery. Synthetic and real experiments validate our theory and demonstrate the advantages of XSlit coded aperture solutions over the spherical lens ones."'),
('"Descattering Transmission via Angular Filtering"', '"ECCV 2010"', '["Tomographic Reconstruction", "Direct Component", "Normal Photo", "Microlens Array", "Algebraic Rec', '"https://doi.org/10.1007/978-3-642-15549-9_7"', '"We describe a single-shot method to differentiate unscattered and scattered components of light transmission through a heterogeneous translucent material. Directly-transmitted components travel in a straight line from the light source, while scattered components originate from multiple scattering centers in the volume. Computer vision methods deal with participating media via 2D contrast enhancing software techniques. On the other hand, optics techniques treat scattering as noise and use elaborate methods to reduce the scattering or its impact on the direct unscattered component. We observe the scattered component on its own provides useful information because the angular variation is low frequency. We propose a method to strategically capture angularly varying scattered light and compute the unscattered direct component. We capture the scattering from a single light source via a lenslet array placed close to the image plane. As an application, we demonstrate enhanced tomographic reconstruction of scattering objects using estimated direct transmission images."'),
('"Descending Stairs Detection with Low-Power Sensors"', '"ECCV 2014"', '["Descending stair detection", "Stereo vision", "Elderly care", "Rehabilitation", "Visual impairment', '"https://doi.org/10.1007/978-3-319-16199-0_46"', '"With the increasing proportion of senior citizens, many mobility aid devices were developed such as the rollator. However among walker\\u2019s users, 87% of their falls is attributed to rollators. The EyeWalker project aims at developing a small device for rollators to protect elderly people from such dangers. Descending stairs are ones of the potential hazards rollator users have to daily face. We propose a method to detect them in real-time using a passive stereo camera. To meet the requirements of low-power consumption, we examined the performance of our stereo vision based detector with regard to the camera resolution. It succeeds in differentiating dangerously approaching stairs from safe situations at low resolutions. In the future, our detector will be ported on an embedded platform equipped with a pair of low-resolution and high dynamic range stereo camera for both indoor and outdoor usage with a battery-life of several days."'),
('"Describing and Matching 2D Shapes by Their Points of Mutual Symmetry"', '"ECCV 2006"', '["Medial Axis", "Shape Descriptor", "Tangency Point", "Equidistant Point", "Maximal Circle"]', '"https://doi.org/10.1007/11744078_17"', '"A novel shape descriptor is introduced. It groups pairs of points that share a geometrical property that is based on their mutual symmetry. The descriptor is visualized as a diagonally symmetric diagram with binary valued regions. This diagram is a fingerprint of global symmetry between pairs of points along the shape. The descriptive power of the method is tested on a well-known shape data base containing several classes of shapes and partially occluded shapes. First tests with simple, elementary matching algorithms show good results."'),
('"Describing Clothing by Semantic Attributes"', '"ECCV 2012"', '["Semantic Attribute", "Sift Descriptor", "Attribute Prediction", "Gender Recognition", "Solid Patte', '"https://doi.org/10.1007/978-3-642-33712-3_44"', '"Describing clothing appearance with semantic attributes is an appealing technique for many important applications. In this paper, we propose a fully automated system that is capable of generating a list of nameable attributes for clothes on human body in unconstrained images. We extract low-level features in a pose-adaptive manner, and combine complementary features for learning attribute classifiers. Mutual dependencies between the attributes are then explored by a Conditional Random Field to further improve the predictions from independent classifiers. We validate the performance of our system on a challenging clothing attribute dataset, and introduce a novel application of dressing style analysis that utilizes the semantic attributes produced by our system."'),
('"Description-Discrimination Collaborative Tracking"', '"ECCV 2014"', '["Descriptive model", "discriminative model", "collaborative tracking", "SVDD", "structural predicti', '"https://doi.org/10.1007/978-3-319-10590-1_23"', '"Appearance model is one of the most important components for online visual tracking. An effective appearance model needs to strike the right balance between being adaptive, to account for appearance change, and being conservative, to re-track the object after it loses tracking (e.g., due to occlusion). Most conventional appearance models focus on one aspect out of the two, and hence are not able to achieve the right balance. In this paper, we approach this problem by a max-margin learning framework collaborating a descriptive component and a discriminative component. Particularly, the two components are for different purposes and with different lifespans. One forms a robust object model, and the other tries to distinguish the object from the current background. Taking advantages of their complementary roles, the components improve each other and collaboratively contribute to a shared score function. Besides, for realtime implementation, we also propose a series of optimization and sample-management strategies. Experiments over 30 challenging videos demonstrate the effectiveness and robustness of the proposed tracker. Our method generally outperforms the existing state-of-the-art methods."'),
('"Descriptor Learning for Efficient Retrieval"', '"ECCV 2010"', '["Visual Word", "Retrieval Performance", "Point Pair", "Stochastic Gradient Descent", "Sift Descript', '"https://doi.org/10.1007/978-3-642-15558-1_49"', '"Many visual search and matching systems represent images using sparse sets of \\u201cvisual words\\u201d: descriptors that have been quantized by assignment to the best-matching symbol in a discrete vocabulary. Errors in this quantization procedure propagate throughout the rest of the system, either harming performance or requiring correction using additional storage or processing. This paper aims to reduce these quantization errors at source, by learning a projection from descriptor space to a new Euclidean space in which standard clustering techniques are more likely to assign matching descriptors to the same cluster, and non-matching descriptors to different clusters."'),
('"Descriptor Learning Using Convex Optimisation"', '"ECCV 2012"', '["Image Retrieval", "Convex Optimisation", "Convex Optimisation Problem", "Pooling Region", "Feature', '"https://doi.org/10.1007/978-3-642-33718-5_18"', '"The objective of this work is to learn descriptors suitable for the sparse feature detectors used in viewpoint invariant matching. We make a number of novel contributions towards this goal: first, it is shown that learning the pooling regions for the descriptor can be formulated as a convex optimisation problem selecting the regions using sparsity; second, it is shown that dimensionality reduction can also be formulated as a convex optimisation problem, using the nuclear norm to reduce dimensionality. Both of these problems use large margin discriminative learning methods. The third contribution is a new method of obtaining the positive and negative training data in a weakly supervised manner. And, finally, we employ a state-of-the-art stochastic optimizer that is efficient and well matched to the non-smooth cost functions proposed here. It is demonstrated that the new learning methods improve over the state of the art in descriptor learning for large scale matching, Brown et al. [2], and large scale object retrieval, Philbin et al. [10]."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Destination Flow for Crowd Simulation"', '"ECCV 2012"', '["User Study", "Motion Behavior", "Virtual Agent", "Real Trajectory", "Crowd Simulation"]', '"https://doi.org/10.1007/978-3-642-33885-4_17"', '"We present a crowd simulation that captures some of the semantics of a specific scene by partly reproducing its motion behaviors, both at a lower level using a steering model and at the higher level of goal selection. To this end, we use and generalize a steering model based on linear velocity prediction, termed LTA. From a goal selection perspective, we reproduce many of the motion behaviors of the scene without explicitly specifying them. Behaviors like \\u201cwait at the tram stop\\u201d or \\u201cstroll-around\\u201d are not explicitly modeled, but learned from real examples. To this end, we process real data to extract information that we use in our simulation. As a consequence, we can easily integrate real and virtual agents in a mixed reality simulation. We propose two strategies to achieve this goal and validate the results by a user study."'),
('"Detecting Actions, Poses, and Objects with Relational Phraselets"', '"ECCV 2012"', '["Action Class", "Action Recognition", "Compositional Model", "Occlude Part", "Pictorial Structure"]', '"https://doi.org/10.1007/978-3-642-33765-9_12"', '"We present a novel approach to modeling human pose, together with interacting objects, based on compositional models of local visual interactions and their relations. Skeleton models, while flexible enough to capture large articulations, fail to accurately model self-occlusions and interactions. Poselets and Visual Phrases address this limitation, but do so at the expense of requiring a large set of templates. We combine all three approaches with a compositional model that is flexible enough to model detailed articulations but still captures occlusions and object interactions. Unlike much previous work on action classification, we do not assume test images are labeled with a person, and instead present results for \\u201caction detection\\u201d in an unlabeled image. Notably, for each detection, our model reports back a detailed description including an action label, articulated human pose, object poses, and occlusion flags. We demonstrate that modeling occlusion is crucial for recognizing human-object interactions. We present results on the PASCAL Action Classification challenge that shows our unified model advances the state-of-the-art for detection, action classification, and articulated pose estimation."'),
('"Detecting and Reconstructing 3D Mirror Symmetric Objects"', '"ECCV 2012"', '["Symmetry detection", "3D reconstruction", "curve matching"]', '"https://doi.org/10.1007/978-3-642-33709-3_42"', '"We present a system that detects 3D mirror-symmetric objects in images and then reconstructs their visible symmetric parts. Our detection stage is based on matching mirror symmetric feature points and descriptors and then estimating the symmetry direction using RANSAC. We enhance this step by augmenting feature descriptors with their affine-deformed versions and matching these extended sets of descriptors. The reconstruction stage uses a novel edge matching algorithm that matches symmetric pairs of curves that are likely to be counterparts. This allows the algorithm to reconstruct lightly textured objects, which are problematic for traditional feature-based and intensity-based stereo matchers."'),
('"Detecting Carried Objects in Short Video Sequences"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_12"', '"We propose a new method for detecting objects such as bags carried by pedestrians depicted in short video sequences. In common with earlier work [1,2] on the same problem, the method starts by averaging aligned foreground regions of a walking pedestrian to produce a representation of motion and shape (known as a temporal template) that has some immunity to noise in foreground segmentations and phase of the walking cycle. Our key novelty is for carried objects to be revealed by comparing the temporal templates against view-specific exemplars generated offline for unencumbered pedestrians. A likelihood map obtained from this match is combined in a Markov random field with a map of prior probabilities for carried objects and a spatial continuity assumption, from which we obtain a segmentation of carried objects using the MAP solution. We have re-implemented the earlier state of the art method [1] and demonstrate a substantial improvement in performance for the new method on the challenging PETS2006 dataset [3]. Although developed for a specific problem, the method could be applied to the detection of irregularities in appearance for other categories of object that move in a periodic fashion."'),
('"Detecting Doctored JPEG Images Via DCT Coefficient Analysis"', '"ECCV 2006"', '["Discrete Cosine Transform", "JPEG Compression", "Discrete Cosine Transform Coefficient", "Quantiza', '"https://doi.org/10.1007/11744078_33"', '"The steady improvement in image/video editing techniques has enabled people to synthesize realistic images/videos conveniently. Some legal issues may occur when a doctored image cannot be distinguished from a real one by visual examination. Realizing that it might be impossible to develop a method that is universal for all kinds of images and JPEG is the most frequently used image format, we propose an approach that can detect doctored JPEG images and further locate the doctored parts, by examining the double quantization effect hidden among the DCT coefficients. Up to date, this approach is the only one that can locate the doctored part automatically. And it has several other advantages: the ability to detect images doctored by different kinds of synthesizing methods (such as alpha matting and inpainting, besides simple image cut/paste), the ability to work without fully decompressing the JPEG images, and the fast speed. Experiments show that our method is effective for JPEG images, especially when the compression quality is high."'),
('"Detecting Faint Curved Edges in Noisy Images"', '"ECCV 2010"', '["Edge Detection", "Noisy Image", "False Detection", "Beam Curve", "General Quadrangle"]', '"https://doi.org/10.1007/978-3-642-15561-1_54"', '"A fundamental question for edge detection is how faint an edge can be and still be detected. In this paper we offer a formalism to study this question and subsequently introduce a hierarchical edge detection algorithm designed to detect faint curved edges in noisy images. In our formalism we view edge detection as a search in a space of feasible curves, and derive expressions to characterize the behavior of the optimal detection threshold as a function of curve length and the combinatorics of the search space. We then present an algorithm that efficiently searches for edges through a very large set of curves by hierarchically constructing difference filters that match the curves traced by the sought edges. We demonstrate the utility of our algorithm in simulations and in applications to challenging real images."'),
('"Detecting Fine-Grained Affordances with an Anthropomorphic Agent Model"', '"ECCV 2014"', '["Affordances", "Fine-grained affordances", "Visual affordance detection", "Object classification"]', '"https://doi.org/10.1007/978-3-319-16181-5_30"', '"In this paper we propose an approach to distinguish affordances on a fine-grained scale. We define an anthropomorphic agent model and parameterized affordance models. The agent model is transformed according to affordance parameters to detect affordances in the input data. We present first results on distinguishing two closely related affordances derived from sitting. The promising results support our concept of fine-grained affordance detection."'),
('"Detecting Ground Shadows in Outdoor Consumer Photographs"', '"ECCV 2010"', '["Outdoor Scene", "Shadow Detection", "Ground Color", "Shadow Boundary", "Invariant Image"]', '"https://doi.org/10.1007/978-3-642-15552-9_24"', '"Detecting shadows from images can significantly improve the performance of several vision tasks such as object detection and tracking. Recent approaches have mainly used illumination invariants which can fail severely when the qualities of the images are not very good, as is the case for most consumer-grade photographs, like those on Google or Flickr. We present a practical algorithm to automatically detect shadows cast by objects onto the ground, from a single consumer photograph. Our key hypothesis is that the types of materials constituting the ground in outdoor scenes is relatively limited, most commonly including asphalt, brick, stone, mud, grass, concrete, etc. As a result, the appearances of shadows on the ground are not as widely varying as general shadows and thus, can be learned from a labelled set of images. Our detector consists of a three-tier process including (a) training a decision tree classifier on a set of shadow sensitive features computed around each image edge, (b) a CRF-based optimization to group detected shadow edges to generate coherent shadow contours, and (c) incorporating any existing classifier that is specifically trained to detect grounds in images. Our results demonstrate good detection accuracy (85%) on several challenging images. Since most objects of interest to vision applications (like pedestrians, vehicles, signs) are attached to the ground, we believe that our detector can find wide applicability."'),
('"Detecting Instances of Shape Classes That Exhibit Variable Structure"', '"ECCV 2006"', '["Hide Markov Model", "Edge Pixel", "Viterbi Algorithm", "Shape Part", "Active Shape Model"]', '"https://doi.org/10.1007/11744023_10"', '"This paper proposes a method for detecting shapes of variable structure in images with clutter. The term \\u201cvariable structure\\u201d means that some shape parts can be repeated an arbitrary number of times, some parts can be optional, and some parts can have several alternative appearances. The particular variation of the shape structure that occurs in a given image is not known a priori. Existing computer vision methods, including deformable model methods, were not designed to detect shapes of variable structure; they may only be used to detect shapes that can be decomposed into a fixed, a priori known, number of parts. The proposed method can handle both variations in shape structure and variations in the appearance of individual shape parts. A new class of shape models is introduced, called Hidden State Shape Models, that can naturally represent shapes of variable structure. A detection algorithm is described that finds instances of such shapes in images with large amounts of clutter by finding globally optimal correspondences between image features and shape models. Experiments with real images demonstrate that our method can localize plant branches that consist of an a priori unknown number of leaves and can detect hands more accurately than a hand detector based on the chamfer distance."'),
('"Detecting Interesting Events Using Unsupervised Density Ratio Estimation"', '"ECCV 2012"', '["Video Summarization", "Density Ratio Estimation"]', '"https://doi.org/10.1007/978-3-642-33885-4_16"', '"Generating meaningful digests of videos by extracting interesting frames remains a difficult task. In this paper, we define interesting events as unusual events which occur rarely in the entire video and we propose a novel interesting event summarization framework based on the technique of density ratio estimation recently introduced in machine learning. Our proposed framework is unsupervised and it can be applied to general video sources, including videos from moving cameras. We evaluated the proposed approach on a publicly available dataset in the context of anomalous crowd behavior and with a challenging personal video dataset. We demonstrated competitive performance both in accuracy relative to human annotation and computation time."'),
('"Detecting Keypoints with Stable Position, Orientation, and Scale under Illumination Changes"', '"ECCV 2004"', '["keypoint", "point of interest", "corner detection", "feature based vision", "F\\u00f6rstner-Harris ', '"https://doi.org/10.1007/978-3-540-24673-2_9"', '"Local feature approaches to vision geometry and object recognition are based on selecting and matching sparse sets of visually salient image points, known as \\u2018keypoints\\u2019 or \\u2018points of interest\\u2019. Their performance depends critically on the accuracy and reliability with which corresponding keypoints can be found in subsequent images. Among the many existing keypoint selection criteria, the popular F\\u00f6rstner-Harris approach explicitly targets geometric stability, defining keypoints to be points that have locally maximal self-matching precision under translational least squares template matching. However, many applications require stability in orientation and scale as well as in position. Detecting translational keypoints and verifying orientation/scale behaviour post hoc is suboptimal, and can be misleading when different motion variables interact. We give a more principled formulation, based on extending the F\\u00f6rstner-Harris approach to general motion models and robust template matching. We also incorporate a simple local appearance model to ensure good resistance to the most common illumination variations. We illustrate the resulting methods and quantify their performance on test images."'),
('"Detecting Large Repetitive Structures with Salient Boundaries"', '"ECCV 2010"', '["Scale Invariant Feature Transform", "Boundary Detection", "Repetition Interval", "Matching Distanc', '"https://doi.org/10.1007/978-3-642-15552-9_11"', '"This paper presents a novel robust and efficient framework to analyze large repetitive structures in urban scenes. A particular contribution of the proposed approach is that it finds the salient boundaries of the repeating elements even when the repetition exists along only one direction. A perspective image is rectified based on vanishing points computed jointly from edges and repeated features detected in the original image by maximizing its overall symmetry. Then a feature-based method is used to extract hypotheses of repetition and symmetry from the rectified image, and initial repetition regions are obtained from the supporting features of each repetition interval. To maximize the local symmetry of each element, their boundaries along the repetition direction are determined from the repetition of local symmetry axes. For any image patch, we define its repetition quality for each repetition interval conditionally with a suppression of integer multiples of repetition intervals. We determine the boundary along the non-repeating direction by finding strong decreases of the repetition quality. Experiments demonstrate the robustness and repeatability of our repetition detection."'),
('"Detecting Liveness in Fingerprint Scanners Using Wavelets: Results of the Test Dataset"', '"BioAW 2004"', '["Wavelet Packet", "Multiresolution Analysis", "Daubechies Wavelet", "Wavelet Packet Analysis", "Bas', '"https://doi.org/10.1007/978-3-540-25976-3_10"', '"A novel method is proposed to detect \\u201cliveness\\u201d associated with fingerprint devices. The physiological phenomenon of perspiration, observed only in live people, is used as a measure to classify \\u2018live\\u2019 fingers from \\u2018not live\\u2019 fingers. Pre-processing involves filtering of the images using different image processing techniques. Wavelet analysis of the images is performed using Daubechies wavelet. Multiresolution analysis is performed to extract information from the low frequency content, while wavelet packet analysis is performed to analyze the high frequency information content. A threshold is applied to the first difference of the information in all the sub-bands. The energy content of the changing wavelet coefficients, which are directly associated with the perspiration pattern, is used as a quantified measure to differentiate live fingers from others. The proposed algorithm was applied to a data set of approximately 30 live, 30 spoof and 14 cadaver fingerprint images from three different types of scanners. The algorithm was able to completely classify \\u2018live\\u2019 fingers from \\u2018not live\\u2019 fingers providing a method for improved spoof protection."'),
('"Detecting People in Cubist Art"', '"ECCV 2014"', '["Object detection", "Perception", "Abstract art", "Cubism"]', '"https://doi.org/10.1007/978-3-319-16178-5_7"', '"Although the human visual system is surprisingly robust to extreme distortion when recognizing objects, most evaluations of computer object detection methods focus only on robustness to natural form deformations such as people\\u2019s pose changes. To determine whether algorithms truly mirror the flexibility of human vision, they must be compared against human vision at its limits. For example, in Cubist abstract art, painted objects are distorted by object fragmentation and part-reorganization, sometimes to the point that human vision often fails to recognize them. In this paper, we evaluate existing object detection methods on these abstract renditions of objects, comparing human annotators to four state-of-the-art object detectors on a corpus of Picasso paintings. Our results demonstrate that while human perception significantly outperforms current methods, human perception and part-based models exhibit a similarly graceful degradation in object detection performance as the objects become increasingly abstract and fragmented, corroborating the theory of part-based object representation in the brain."'),
('"Detecting People Using Mutually Consistent Poselet Activations"', '"ECCV 2010"', '["Object Detection", "Image Patch", "Visible Bound", "People Detection", "Human Torso"]', '"https://doi.org/10.1007/978-3-642-15567-3_13"', '"Bourdev and Malik (ICCV 09) introduced a new notion of parts, poselets, constructed to be tightly clustered both in the configuration space of keypoints, as well as in the appearance space of image patches. In this paper we develop a new algorithm for detecting people using poselets. Unlike that work which used 3D annotations of keypoints, we use only 2D annotations which are much easier for naive human annotators. The main algorithmic contribution is in how we use the pattern of poselet activations. Individual poselet activations are noisy, but considering the spatial context of each can provide vital disambiguating information, just as object detection can be improved by considering the detection scores of nearby objects in the scene. This can be done by training a two-layer feed-forward network with weights set using a max margin technique. The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions. Finally, bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation. To the best of our knowledge, the resulting system is the current best performer on the task of people detection and segmentation with an average precision of 47.8% and 40.5% respectively on PASCAL VOC 2009."'),
('"Detecting Regions from Single Scale Edges"', '"ECCV 2010"', '["Maximally Stable Extremal Region", "Salient Region Detector", "Edge Fragment", "Euclidean Distance', '"https://doi.org/10.1007/978-3-642-35749-7_23"', '"We believe that the potential of edges in local feature detection has not been fully exploited and therefore propose a detector that starts from single scale edges and produces reliable and interpretable blob-like regions and groups of regions of arbitrary shape. The detector is based on merging local maxima of the distance transform guided by the gradient strength of the surrounding edges. Repeatability and matching score are evaluated and compared to state-of-the-art detectors on standard benchmarks. Furthermore, we demonstrate the potential application of our method to wide-baseline matching and feature detection in sequences involving human activity."'),
('"Detecting Snap Points in Egocentric Video with a Web Photo Prior"', '"ECCV 2014"', '["Ground Truth", "Object Detection", "Salient Object", "Label Image", "Video Summarization"]', '"https://doi.org/10.1007/978-3-319-10602-1_19"', '"Wearable cameras capture a first-person view of the world, and offer a hands-free way to record daily experiences or special events. Yet, not every frame is worthy of being captured and stored. We propose to automatically predict \\u201csnap points\\u201d in unedited egocentric video\\u2014that is, those frames that look like they could have been intentionally taken photos. We develop a generative model for snap points that relies on a Web photo prior together with domain-adapted features. Critically, our approach avoids strong assumptions about the particular content of snap points, focusing instead on their composition. Using 17 hours of egocentric video from both human and mobile robot camera wearers, we show that the approach accurately isolates those frames that human judges would believe to be intentionally snapped photos. In addition, we demonstrate the utility of snap point detection for improving object detection and keyframe selection in egocentric video."'),
('"Detecting Social Actions of Fruit Flies"', '"ECCV 2014"', '["Confusion Matrix", "Confusion Matrice", "Bout Duration", "Wing Angle", "Wing Extension"]', '"https://doi.org/10.1007/978-3-319-10605-2_50"', '"We describe a system that tracks pairs of fruit flies and automatically detects and classifies their actions. We compare experimentally the value of a frame-level feature representation with the more elaborate notion of \\u2018bout features\\u2019 that capture the structure within actions. Similarly, we compare a simple sliding window classifier architecture with a more sophisticated structured output architecture, and find that window based detectors outperform the much slower structured counterparts, and approach human performance. In addition we test our top performing detector on the CRIM13 mouse dataset, finding that it matches the performance of the best published method. Our Fly-vs-Fly dataset contains 22 hours of video showing pairs of fruit flies engaging in 10 social interactions in three different contexts; it is fully annotated by experts, and published with articulated pose trajectory features."'),
('"Detecting Symmetry and Symmetric Constellations of Features"', '"ECCV 2006"', '["Feature Point", "Rotational Symmetry", "Bilateral Symmetry", "Symmetric Pair", "Sift Descriptor"]', '"https://doi.org/10.1007/11744047_39"', '"A novel and efficient method is presented for grouping feature points on the basis of their underlying symmetry and characterising the symmetries present in an image. We show how symmetric pairs of features can be efficiently detected, how the symmetry bonding each pair is extracted and evaluated, and how these can be grouped into symmetric constellations that specify the dominant symmetries present in the image. Symmetries over all orientations and radii are considered simultaneously, and the method is able to detect local or global symmetries, locate symmetric figures in complex backgrounds, detect bilateral or rotational symmetry, and detect multiple incidences of symmetry."'),
('"Detection and Modelling of Staircases Using a Wearable Depth Sensor"', '"ECCV 2014"', '["Stair detection", "Obstacle detection", "Segmentation", "Visually impaired", "RGB-D"]', '"https://doi.org/10.1007/978-3-319-16199-0_32"', '"In this paper we deal with the perception task of a wearable navigation assistant. Specifically, we have focused on the detection of staircases because of the important role they play in indoor navigation due to the multi-floor reaching possibilities they bring and the lack of security they cause, specially for those who suffer from visual deficiencies. We use the depth sensing capacities of the modern RGB-D cameras to segment and classify the different elements that integrate the scene and then carry out the stair detection and modelling algorithm to retrieve all the information that might interest the user, i.e. the location and orientation of the staircase, the number of steps and the step dimensions. Experiments prove that the system is able to perform in real-time and works even under partial occlusions of the stairway."'),
('"Detection and Tracking of Large Number of Targets in Wide Area Surveillance"', '"ECCV 2010"', '["Grid Cell", "Background Model", "Registration Error", "Tracking Problem", "Object Context"]', '"https://doi.org/10.1007/978-3-642-15558-1_14"', '"In this paper, we tackle the problem of object detection and tracking in a new and challenging domain of wide area surveillance. This problem poses several challenges: large camera motion, strong parallax, large number of moving objects, small number of pixels on target, single channel data and low framerate of video. We propose a method that overcomes these challenges and evaluate it on CLIF dataset. We use median background modeling which requires few frames to obtain a workable model. We remove false detections due to parallax and registration errors using gradient information of the background image. In order to keep complexity of the tracking problem manageable, we divide the scene into grid cells, solve the tracking problem optimally within each cell using bipartite graph matching and then link tracks across cells. Besides tractability, grid cells allow us to define a set of local scene constraints such as road orientation and object context. We use these constraints as part of cost function to solve the tracking problem which allows us to track fast-moving objects in low framerate videos. In addition to that, we manually generated groundtruth for four sequences and performed quantitative evaluation of the proposed algorithm."'),
('"Detection and Tracking Scheme for Line Scratch Removal in an Image Sequence"', '"ECCV 2004"', '["Motion Picture", "Viterbi Algorithm", "Tracking Scheme", "Multiple Hypothesis Tracker", "Trellis D', '"https://doi.org/10.1007/978-3-540-24672-5_21"', '"A detection and tracking approach is proposed for line scratch removal in a digital film restoration process. Unlike random impulsive distortions such as dirt spots, line scratch artifacts persist across several frames. Hence, motion compensated methods will fail, as well as single-frame methods if scratches are unsteady or fragmented."'),
('"Detection of Connective Tissue Disorders from 3D Aortic MR Images Using Independent Component Analy', '"CVAMIA 2006"', '["Independent Component Analysis", "Independent Component", "Abdominal Aortic Aneurysm", "Segmentati', '"https://doi.org/10.1007/11889762_2"', '"A computer-aided diagnosis (CAD) method is reported that allows the objective identification of subjects with connective tissue disorders from 3D aortic MR images using segmentation and independent component analysis (ICA). The first step to extend the model to 4D (3D + time) has also been taken. ICA is an effective tool for connective tissue disease detection in the presence of sparse data using prior knowledge to order the components, and the components can be inspected visually."'),
('"Detection of Frontal Faces in Video Streams"', '"BioAW 2002"', '["Face detection", "tracking", "active vision", "feature detection", "HCI"]', '"https://doi.org/10.1007/3-540-47917-1_10"', '"This paper describes an approach for detection of frontal faces in real time (20\\u201335Hz) for further processing. This approach makes use of a combination of previous detection tracking and color for selecting interest areas. On those areas, later facial features such as eyes, nose and mouth are searched based on geometric tests, appearance verification, temporal and spatial coherence. The system makes use of very simple techniques applied in a cascade approach, combined and coordinated with temporal information for improving performance. This module is a component of a complete system designed for detection, tracking and identification of individuals [1]."'),
('"Detection of Independently Moving Objects in Non-planar Scenes via Multi-Frame Monocular Epipolar C', '"ECCV 2012"', '["Camera Motion", "False Detection", "Fundamental Matrix", "Plane Object", "Static Scene"]', '"https://doi.org/10.1007/978-3-642-33715-4_62"', '"In this paper we present a novel approach for detection of independently moving foreground objects in non-planar scenes captured by a moving camera. We avoid the traditional assumptions that the stationary background of the scene is planar, or that it can be approximated by dominant single or multiple planes, or that the camera used to capture the video is orthographic. Instead we utilize a multiframe monocular epipolar constraint of camera motion derived for monocular moving cameras defined by an evolving epipolar plane between the moving camera center and 3D scene points. This constraint is parameterized as a polynomial function of time, and unlike repeated computations of inter-frame fundamental matrix, requires the estimation of fewer unknowns, and provides a more consistent separation between moving and static objects for different levels of noise. This constraint allows us to segment out moving objects in a general 3D scene where other approaches fail because their initial assumptions do not hold, and provides a natural way of fusing temporal information across multiple frames. We use a combination of optical flow and particle advection to capture all motion in the video across a number of frames, in the form of particle trajectories. We then apply the derived multi-frame epipolar constraint to these trajectories to determine which trajectories violate it, thus segmenting out the independently moving objects. We show superior results on a number of moving camera sequences observing non-planar scenes, where other methods fail."'),
('"Determining Correspondences for Statistical Models of Appearance"', '"ECCV 2000"', '["Image features", "Statistical models of appearance", "correspondence"]', '"https://doi.org/10.1007/3-540-45054-8_54"', '"In order to build a statistical model of appearance we require a set of images, each with a consistent set of landmarks. We address the problem of automatically placing a set of landmarks to define the correspondences across an image set. We can estimate correspondences between any pair of images by locating salient points on one and finding their corresponding position in the second. However, we wish to determine a globally consistent set of correspondences across all the images. We present an iterative scheme in which these pair-wise correspondences are used to determine a global correspondence across the entire set. We show results on several training sets, and demonstrate that an Appearance Model trained on the correspondences can be of higher quality than one built from hand marked images."'),
('"Determining Patch Saliency Using Low-Level Context"', '"ECCV 2008"', '["Object Recognition", "Recognition Accuracy", "Interest Point", "Image Patch", "Salient Region"]', '"https://doi.org/10.1007/978-3-540-88688-4_33"', '"The increased use of context for high level reasoning has been popular in recent works to increase recognition accuracy. In this paper, we consider an orthogonal application of context. We explore the use of context to determine which low-level appearance cues in an image are salient or representative of an image\\u2019s contents. Existing classes of low-level saliency measures for image patches include those based on interest points, as well as supervised discriminative measures. We propose a new class of unsupervised contextual saliency measures based on co-occurrence and spatial information between image patches. For recognition, image patches are sampled using a weighted random sampling based on saliency, or using a sequential approach based on maximizing the likelihoods of the image patches. We compare the different classes of saliency measures, along with a baseline uniform measure, for the task of scene and object recognition using the bag-of-features paradigm. In our results, the contextual saliency measures achieve improved accuracies over the previous methods. Moreover, our highest accuracy is achieved using a sparse sampling of the image, unlike previous approaches who\\u2019s performance increases with the sampling density."'),
('"Deterministic 3D Human Pose Estimation Using Rigid Structure"', '"ECCV 2010"', '["Rigid Structure", "Bone Length", "Deterministic Structure", "Rigid Constraint", "Motion Approach"]', '"https://doi.org/10.1007/978-3-642-15558-1_34"', '"This paper explores a method, first proposed by Wei and Chai [1], for estimating 3D human pose from several frames of uncalibrated 2D point correspondences containing projected body joint locations. In their work Wei and Chai boldly claimed that, through the introduction of rigid constraints to the torso and hip, camera scales, bone lengths and absolute depths could be estimated from a finite number of frames (i.e. \\u2265\\u20095). In this paper we show this claim to be false, demonstrating in principle one can never estimate these parameters in a finite number of frames. Further, we demonstrate their approach is only valid for rigid sub-structures of the body (e.g. torso). Based on this analysis we propose a novel approach using deterministic structure from motion based on assumptions of rigidity in the body\\u2019s torso. Our approach provides notably more accurate estimates and is substantially faster than Wei and Chai\\u2019s approach, and unlike the original, can be solved as a deterministic least-squares problem."'),
('"Diagnosing Error in Object Detectors"', '"ECCV 2012"', '["Localization Error", "Object Detector", "Average Precision", "Object Category", "Similar Object"]', '"https://doi.org/10.1007/978-3-642-33712-3_25"', '"This paper shows how to analyze the influences of object characteristics on detection performance and the frequency and impact of different types of false positives. In particular, we examine effects of occlusion, size, aspect ratio, visibility of parts, viewpoint, localization error, and confusion with semantically similar objects, other labeled objects, and background. We analyze two classes of detectors: the Vedaldi et al. multiple kernel learning detector and different versions of the Felzenszwalb et al. detector. Our study shows that sensitivity to size, localization error, and confusion with similar objects are the most impactful forms of error. Our analysis also reveals that many different kinds of improvement are necessary to achieve large gains, making more detailed analysis essential for the progress of recognition research. By making our software and annotations available, we make it effortless for future researchers to perform similar analysis."'),
('"Dictionary-Based Face Recognition from Video"', '"ECCV 2012"', '["Receiver Operating Characteristic Curve", "Face Recognition", "Video Sequence", "Face Image", "Fal', '"https://doi.org/10.1007/978-3-642-33783-3_55"', '"The main challenge in recognizing faces in video is effectively exploiting the multiple frames of a face and the accompanying dynamic signature. One prominent method is based on extracting joint appearance and behavioral features. A second method models a person by temporal correlations of features in a video. Our approach introduces the concept of video-dictionaries for face recognition, which generalizes the work in sparse representation and dictionaries for faces in still images. Video-dictionaries are designed to implicitly encode temporal, pose, and illumination information. We demonstrate our method on the Face and Ocular Challenge Series (FOCS) Video Challenge, which consists of unconstrained video sequences. We show that our method is efficient and performs significantly better than many competitive video-based face recognition algorithms."'),
('"Diffeomorphic Matching Problems in One Dimension: Designing and Minimizing Matching Functionals"', '"ECCV 2000"', '["Shape", "Shape representation and recognition", "Elastic matching", "Calculus of variations", "dyn', '"https://doi.org/10.1007/3-540-45054-8_37"', '"This paper focuses on matching 1D structures by variational methods. We provide rigorous rules for the construction of the cost function, on the basis of an analysis of properties which should be satisfied by the optimal matching. A new, exact, dynamic programming algorithm is then designed for the minimization. We conclude with experimental results on shape comparison."'),
('"Differential Geometric Consistency Extends Stereo to Curved Surfaces"', '"ECCV 2006"', '["Tangent Plane", "Shape Operator", "Stereo Pair", "Candidate Match", "Stereo Correspondence"]', '"https://doi.org/10.1007/11744078_4"', '"Traditional stereo algorithms implicitly use the frontal parallel plane assumption when exploiting contextual information, since the smoothness prior biases towards constant disparity (depth) over a neighborhood. For curved surfaces these algorithms introduce systematic errors to the matching process. These errors are non-negligible for detailed geometric modeling of natural objects (e.g. a human face). We propose to use contextual information geometrically. In particular, we perform a differential geometric study of smooth surfaces and argue that geometric contextual information should be encoded in Cartan\\u2019s moving frame model over local quadratic approximations of the smooth surfaces. The result enforces geometric consistency for both depth and surface normal. We develop a simple stereo algorithm to illustrate the importance of using such geometric contextual information and demonstrate its power on images of the human face."'),
('"Differential Spatial Resection - Pose Estimation Using a Single Local Image Feature"', '"ECCV 2008"', '["Point Correspondence", "Epipolar Geometry", "Camera Center", "Feature Correspondence", "Omnidirect', '"https://doi.org/10.1007/978-3-540-88693-8_23"', '"Robust local image features have been used successfully in robot localization and camera pose estimation; region tracking using affine warps is considered state of the art also for many years. Although such correspondences provide a warp of the local image region and are quite powerful, in direct pose estimation they are so far only considered as points and therefore three of them are required to construct a camera pose. In this contribution we show how it is possible to directly compute a pose based upon one such feature, given the plane in space where it lies. This differential correspondence concept exploits the texture warp and has recently gained attention in estimation of conjugate rotations. The approach can also be considered as the limiting case of the well-known spatial resection problem when the three 3D points approach each other infinitesimally close. We show that the differential correspondence is more powerful than conic correspondences while its exploitation requires nothing more complicated than the roots of a third order polynomial. We give a detailed sensitivity analysis, a comparison against state-of-the-art pose estimators and demonstrate real-world applicability of the algorithm based on automatic region recognition."'),
('"Diffuse-Specular Separation and Depth Recovery from Image Sequences"', '"ECCV 2002"', '["Color Histogram", "Epipolar Geometry", "Continuity Constraint", "Scene Point", "True Depth"]', '"https://doi.org/10.1007/3-540-47977-5_14"', '"Specular reflections present difficulties for many areas of computer vision such as stereo and segmentation. To separate specular and diffuse reflection components, previous approaches generally require accurate segmentation, regionally uniform reflectance or structured lighting. To overcome these limiting assumptions, we propose a method based on color analysis and multibaseline stereo that simultaneously estimates the separation and the true depth of specular reflections. First, pixels with a specular component are detected by a novel form of color histogram differencing that utilizes the epipolar constraint. This process uses relevant data from all the stereo images for robustness, and addresses the problem of color occlusions. Based on the Lambertian model of diffuse reflectance, stereo correspondence is then employed to compute for specular pixels their corresponding diffuse components in other views. The results of color-based detection aid the stereo correspondence, which determines both separation and true depth of specular pixels. Our approach integrates color analysis and multibaseline stereo in a synergistic manner to yield accurate separation and depth, as demonstrated by our results on synthetic and real image sequences."'),
('"Dilated Divergence Based Scale-Space Representation for Curve Analysis"', '"ECCV 2012"', '["Minor Radius", "Object Scale", "Scale Selection", "Neighboring Object", "Spiral Image"]', '"https://doi.org/10.1007/978-3-642-33709-3_40"', '"This study proposes the novel dilated divergence scale-space representation for multidimensional curve-like image structure analysis. In the proposed framework, image structures are modeled as curves with arbitrary thickness. The dilated divergence analyzes the structure boundaries along the curve normal space in a multi-scale fashion. The dilated divergence based detection is formulated so as to 1) sustain the disturbance introduced by neighboring objects, 2) recognize the curve normal and tangent spaces. The latter enables the innovative formulation of structure eccentricity analysis and curve tangent space-based structure motion analysis, which have been scarcely investigated in literature. The proposed method is validated using 2D, 3D and 4D images. The structure principal direction estimation accuracies, structure scale detection accuracies and detection stabilities are quantified and compared against two scale-space approaches, showing a competitive performance of the proposed approach, under the disturbance introduced by image noise and neighboring objects. Moreover, as an application example employing the dilated divergence detection responses, an automated approach is tailored for spinal cord centerline extraction. The proposed method is shown to be versatile to well suit a wide range of applications."'),
('"Dimensionality Reduction by Canonical Contextual Correlation Projections"', '"ECCV 2004"', '["Feature Vector", "Dimensionality Reduction", "Linear Discriminant Analysis", "Class Label", "Canon', '"https://doi.org/10.1007/978-3-540-24670-1_43"', '"A linear, discriminative, supervised technique for reducing feature vectors extracted from image data to a lower-dimensional representation is proposed. It is derived from classical Fisher linear discriminant analysis (LDA) and useful, for example, in supervised segmentation tasks in which high-dimensional feature vector describes the local structure of the image. In general, the main idea of the technique is applicable in discriminative and statistical modelling that involves contextual data."'),
('"Direct Bundle Estimation for Recovery of Shape, Reflectance Property and Light Position"', '"ECCV 2008"', '["Input Image", "Preconditioned Conjugate Gradient", "Bundle Adjustment", "Photometric Stereo", "Poi', '"https://doi.org/10.1007/978-3-540-88690-7_31"', '"Given a set of images captured with a fixed camera while a point light source moves around an object, we can estimate the shape, reflectance property and texture of the object, as well as the positions of the light source. Our formulation is a large-scale nonlinear optimization that allows us to adjust the parameters so that the images synthesized from all of the parameters optimally fit the input images. This type of optimization, which is a variation of the bundle adjustment for structure and motion reconstruction, is often employed to refine a carefully constructed initial estimation. However, the initialization task often requires a great deal of labor, several special devices, or both. In the present paper, we describe (i) an easy method of initialization that does not require any special devices or a precise calibration and (ii) an efficient algorithm for the optimization. The efficiency of the optimization method enables us to use a simple initialization. For a set of synthesized images, the proposed method decreases the residual to zero. In addition, we show that various real objects, including toy models and human faces, can be successfully recovered."'),
('"Direct Energy Minimization for Super-Resolution on Nonlinear Manifolds"', '"ECCV 2006"', '["Reconstruction Error", "Image Patch", "Locally Linear Embedding", "Markov Network", "Nonlinear Man', '"https://doi.org/10.1007/11744085_22"', '"We address the problem of single image super-resolution by exploring the manifold properties. Given a set of low resolution image patches and their corresponding high resolution patches, we assume they respectively reside on two non-linear manifolds that have similar locally-linear structure. This manifold correlation can be realized by a three-layer Markov network that connects performing super-resolution with energy minimization. The main advantage of our approach is that by working directly with the network model, there is no need to actually construct the mappings for the underlying manifolds. To achieve such efficiency, we establish an energy minimization model for the network that directly accounts for the expected property entailed by the manifold assumption. The resulting energy function has two nice properties for super-resolution. First, the function is convex so that the optimization can be efficiently done. Second, it can be shown to be an upper bound of the reconstruction error by our algorithm. Thus, minimizing the energy function automatically guarantees a lower reconstruction error\\u2014 an important characteristic for promising stable super-resolution results."'),
('"Direct Segmentation of Multiple 2-D Motion Models of Different Types"', '"WDV 2006"', '["Motion Model", "Segmentation Result", "Translational Motion", "Image Measurement", "Translational ', '"https://doi.org/10.1007/978-3-540-70932-9_2"', '"We propose a closed form solution for segmenting mixtures of 2-D translational and 2-D affine motion models directly from the image intensities. Our approach exploits the fact that the spatial-temporal image derivatives generated by a mixture of these motion models must satisfy a bi-homogeneous polynomial called the multibody brightness constancy constraint (MBCC). We show that the degrees of the MBCC are related to the number of motions models of each kind. Such degrees can be automatically computed using a one-dimensional search. We then demonstrate that a sub-matrix of the Hessian of the MBCC encodes information about the type of motion models. For instance, the matrix is rank-1 for 2-D translational models and rank-3 for 2-D affine models. Once the type of motion model has been identified, one can obtain the parameters of each type of motion model at every image measurement from the cross products of the derivatives of the MBCC. We then demonstrate that accounting for a 2-D translational motion model as a 2-D affine one would result in erroneous estimation of the motion models, thus motivating our aim to account for different types of motion models. We apply our method to segmenting various dynamic scenes."'),
('"Direct Solutions for Computing Cylinders from Minimal Sets of 3D Points"', '"ECCV 2006"', '["Point Cloud", "Range Data", "Cylinder Axis", "Direct Solution", "Bernstein Polynomial"]', '"https://doi.org/10.1007/11744023_11"', '"Efficient direct solutions for the determination of a cylinder from points are presented. The solutions range from the well known direct solution of a quadric to the minimal solution of a cylinder with five points. In contrast to the approach of G. Roth and M. D. Levine (1990), who used polynomial bases for representing the geometric entities, we use algebraic constraints on the quadric representing the cylinder. The solutions for six to eight points directly determine all the cylinder parameters in one step: (1) The eight-point-solution, similar to the estimation of the fundamental matrix, requires to solve for the roots of a 3rd-order-polynomial. (2) The seven-point-solution, similar to the six-point-solution for the relative orientation by J. Philip (1996), yields a linear equation system. (3) The six-point-solution, similar to the five-point-solution for the relative orientation by D. Nister (2003), yields a ten-by-ten eigenvalue problem. The new minimal five-point-solution first determines the direction and then the position and the radius of the cylinder. The search for the zeros of the resulting 6th order polynomials is efficiently realized using 2D-Bernstein polynomials. Also direct solutions for the special cases with the axes of the cylinder parallel to a coordinate plane or axis are given. The method is used to find cylinders in range data of an industrial site."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Direction Control for an Active Docking Behaviour Based on the Rotational Component of Log-Polar Op', '"ECCV 2000"', '["Active vision and real-time vision", "vision-guided mobile robots", "and docking"]', '"https://doi.org/10.1007/3-540-45053-X_11"', '"Docking is a fundamental requirement for a mobile robot in order to be able to interact with objects in its environment. In this paper we present an algorithm and implementation for a special case of the docking problem for ground-based robots. We require the robot to dock with a fixated environment point where only visual information is available. Specifically, camera pan/tilt information is unknown, as is the direction of motion with respect to the object and the robot\\u2019s velocity. Further, camera calibration is unavailable. The aim is to minimise the difference between the camera optical axis and the robot heading direction. This constitutes a behaviour for controlling robot direction based on fixation. This paper presents a full mathematical derivation of the method and implementation used. In its most general form, the method requires partial segmentation of the optical flow field. The experiments presented, however, assume partial knowledge as to whether points are closer to the camera than the fixation point or further away. There are many scenarios in robotic navigation where such assumptions are typical working conditions. We examine two cases: convex objects; and distant background/floor. The solution presented uses only the rotational component of optical flow from a log-polar sensor. Results are presented with real image and ray-traced image sequences. The robot is controlled based on a single component of optical flow over a small portion of the image, and thus is suited to real-time implementation."'),
('"Directional Space-Time Oriented Gradients for 3D Visual Pattern Analysis"', '"ECCV 2012"', '["Action Recognition", "Multiple Kernel Learn", "Dynamic Texture", "Multiple Instance Learning", "Sp', '"https://doi.org/10.1007/978-3-642-33712-3_53"', '"Various visual tasks such as the recognition of human actions, gestures, facial expressions, and classification of dynamic textures require modeling and the representation of spatio-temporal information. In this paper, we propose representing space-time patterns using directional spatio-temporal oriented gradients. In the proposed approach, a 3D video patch is represented by a histogram of oriented gradients over nine symmetric spatio-temporal planes. Video comparison is achieved through a positive definite similarity kernel that is learnt by multiple kernel learning. A rich spatio-temporal descriptor with a simple trade-off between discriminatory power and invariance properties is thereby obtained. To evaluate the proposed approach, we consider three challenging visual recognition tasks, namely the classification of dynamic textures, human gestures and human actions. Our evaluations indicate that the proposed approach attains significant classification improvements in recognition accuracy in comparison to state-of-the-art methods such as LBP-TOP, 3D-SIFT, HOG3D, tensor canonical correlation analysis, and dynamical fractal analysis."'),
('"Discovering a Lexicon of Parts and Attributes"', '"ECCV 2012"', '["Latent Dirichlet Allocation", "Object Category", "Visual Attribute", "Front Wheel", "Sentence Pair', '"https://doi.org/10.1007/978-3-642-33885-4_3"', '"We propose a framework to discover a lexicon of visual attributes that supports fine-grained visual discrimination. It consists of a novel annotation task where annotators are asked to describe differences between pairs of images. This captures the intuition that for a lexicon to be useful, it should achieve twin goals of discrimination and communication. Next, we show that such comparative text collected for many pairs of images can be analyzed to discover topics that encode nouns and modifiers, as well as relations that encode attributes of parts. The model also provides an ordering of attributes based on their discriminative ability, which can be used to create a shortlist of attributes to collect for a dataset. Experiments on Caltech-UCSD birds, PASCAL VOC person, and a dataset of airplanes, show that the discovered lexicon of parts and their attributes is comparable to those created by experts."'),
('"Discovering Groups of People in Images"', '"ECCV 2014"', '["Group discovery", "Social interaction", "Activity recognition"]', '"https://doi.org/10.1007/978-3-319-10593-2_28"', '"Understanding group activities from images is an important yet challenging task. This is because there is an exponentially large number of semantic and geometrical relationships among individuals that one must model in order to effectively recognize and localize the group activities. Rather than focusing on directly recognizing group activities as most of the previous works do, we advocate the importance of introducing an intermediate representation for modeling groups of humans which we call structure groups. Such groups define the way people spatially interact with each other. People might be facing each other to talk, while others sit on a bench side by side, and some might stand alone. In this paper we contribute a method for identifying and localizing these structured groups in a single image despite their varying viewpoints, number of participants, and occlusions. We propose to learn an ensemble of discriminative interaction patterns to encode the relationships between people in 3D and introduce a novel efficient iterative augmentation algorithm for solving this complex inference problem. A nice byproduct of the inference scheme is an approximate 3D layout estimate of the structured groups in the scene. Finally, we contribute an extremely challenging new dataset that contains images each showing multiple people performing multiple activities. Extensive evaluation confirms our theoretical findings."'),
('"Discovering Latent Domains for Multisource Domain Adaptation"', '"ECCV 2012"', '["Local Cluster", "Target Domain", "Domain Adaptation", "Object Category", "Source Domain"]', '"https://doi.org/10.1007/978-3-642-33709-3_50"', '"Recent domain adaptation methods successfully learn cross-domain transforms to map points between source and target domains. Yet, these methods are either restricted to a single training domain, or assume that the separation into source domains is known a priori. However, most available training data contains multiple unknown domains. In this paper, we present both a novel domain transform mixture model which outperforms a single transform model when multiple domains are present, and a novel constrained clustering method that successfully discovers latent domains. Our discovery method is based on a novel hierarchical clustering technique that uses available object category information to constrain the set of feasible domain separations. To illustrate the effectiveness of our approach we present experiments on two commonly available image datasets with and without known domain labels: in both cases our method outperforms baseline techniques which use no domain adaptation or domain adaptation methods that presume a single underlying domain shift."'),
('"Discovering Multipart Appearance Models from Captioned Images"', '"ECCV 2010"', '["Part Model", "Appearance Model", "Image Annotation", "Part Detection", "National Hockey League"]', '"https://doi.org/10.1007/978-3-642-15555-0_14"', '"Even a relatively unstructured captioned image set depicting a variety of objects in cluttered scenes contains strong correlations between caption words and repeated visual structures. We exploit these correlations to discover named objects and learn hierarchical models of their appearance. Revising and extending a previous technique for finding small, distinctive configurations of local features, our method assembles these co-occurring parts into graphs with greater spatial extent and flexibility. The resulting multipart appearance models remain scale, translation and rotation invariant, but are more reliable detectors and provide better localization. We demonstrate improved annotation precision and recall on datasets to which the non-hierarchical technique was previously applied and show extended spatial coverage of detected objects."'),
('"Discovering Object Classes from Activities"', '"ECCV 2014"', '["Object Discovery", "Human-Object Interaction", "RGBD Videos"]', '"https://doi.org/10.1007/978-3-319-10599-4_27"', '"In order to avoid an expensive manual labelling process or to learn object classes autonomously without human intervention, object discovery techniques have been proposed that extract visually similar objects from weakly labelled videos. However, the problem of discovering small or medium sized objects is largely unexplored. We observe that videos with activities involving human-object interactions can serve as weakly labelled data for such cases. Since neither object appearance nor motion is distinct enough to discover objects in such videos, we propose a framework that samples from a space of algorithms and their parameters to extract sequences of object proposals. Furthermore, we model similarity of objects based on appearance and functionality, which is derived from human and object motion. We show that functionality is an important cue for discovering objects from activities and demonstrate the generality of the model on three challenging RGB-D and RGB datasets."'),
('"Discovering Texture Regularity as a Higher-Order Correspondence Problem"', '"ECCV 2006"', '["Texture Element", "Optimal Assignment", "Normalize Cross Correlation", "Pairwise Constraint", "Cor', '"https://doi.org/10.1007/11744047_40"', '"Understanding texture regularity in real images is a challenging computer vision task. We propose a higher-order feature matching algorithm to discover the lattices of near-regular textures in real images. The underlying lattice of a near-regular texture identifies all of the texels as well as the global topology among the texels. A key contribution of this paper is to formulate lattice-finding as a correspondence problem. The algorithm finds a plausible lattice by iteratively proposing texels and assigning neighbors between the texels. Our matching algorithm seeks assignments that maximize both pair-wise visual similarity and higher-order geometric consistency. We approximate the optimal assignment using a recently developed spectral method. We successfully discover the lattices of a diverse set of unsegmented, real-world textures with significant geometric warping and large appearance variation among texels."'),
('"Discovering Video Clusters from Visual Features and Noisy Tags"', '"ECCV 2014"', '["Visual Feature", "Spectral Cluster", "Event Category", "Home Video", "Video Category"]', '"https://doi.org/10.1007/978-3-319-10599-4_34"', '"We present an algorithm for automatically clustering tagged videos. Collections of tagged videos are commonplace, however, it is not trivial to discover video clusters therein. Direct methods that operate on visual features ignore the regularly available, valuable source of tag information. Solely clustering videos on these tags is error-prone since the tags are typically noisy. To address these problems, we develop a structured model that considers the interaction between visual features, video tags and video clusters. We model tags from visual features, and correct noisy tags by checking visual appearance consistency. In the end, videos are clustered from the refined tags as well as the visual features. We learn the clustering through a max-margin framework, and demonstrate empirically that this algorithm can produce more accurate clustering results than baseline methods based on tags or visual features, or both. Further, qualitative results verify that the clustering results can discover sub-categories and more specific instances of a given video category."'),
('"Discriminant Analysis on Embedded Manifold"', '"ECCV 2004"', '["Discriminant Analysis", "Face Recognition", "Near Neighbor", "Locality Preserve Projection", "Nonl', '"https://doi.org/10.1007/978-3-540-24670-1_10"', '"Previous manifold learning algorithms mainly focus on uncovering the low dimensional geometry structure from a set of samples that lie on or nearly on a manifold in an unsupervised manner. However, the representations from unsupervised learning are not always optimal in discriminating capability. In this paper, a novel algorithm is introduced to conduct discriminant analysis in term of the embedded manifold structure. We propose a novel clustering algorithm, called Intra-Cluster Balanced K-Means (ICBKM), which ensures that there are balanced samples for the classes in a cluster; and the local discriminative features for all clusters are simultaneously calculated by following the global Fisher criterion. Compared to the traditional linear/kernel discriminant analysis algorithms, ours has the following characteristics: 1) it is approximately a locally linear yet globally nonlinear discriminant analyzer; 2) it can be considered a special Kernel-DA with geometry-adaptive-kernel, in contrast to traditional KDA whose kernel is independent to the samples; and 3) its computation and memory cost are reduced a great deal compared to traditional KDA, especially for the cases with large number of samples. It does not need to store the original samples for computing the low dimensional representation for new data. The evaluation on toy problem shows that it is effective in deriving discriminative representations for the problem with nonlinear classification hyperplane. When applied to the face recognition problem, it is shown that, compared with LDA and traditional KDA on YALE and PIE databases, the proposed algorithm significantly outperforms LDA and"'),
('"Discriminative Bayesian Active Shape Models"', '"ECCV 2012"', '["Root Mean Square", "Linear Dynamical System", "Kernel Density Esti", "Active Appearance Model", "B', '"https://doi.org/10.1007/978-3-642-33712-3_5"', '"This work presents a simple and very efficient solution to align facial parts in unseen images. Our solution relies on a Point Distribution Model (PDM) face model and a set of discriminant local detectors, one for each facial landmark. The patch responses can be embedded into a Bayesian inference problem, where the posterior distribution of the global warp is inferred in a \\u0131maximum a posteriori (MAP) sense. However, previous formulations do not model explicitly the covariance of the latent variables, which represents the confidence in the current solution. In our Discriminative Bayesian Active Shape Model (DBASM) formulation, the MAP global alignment is inferred by a Linear Dynamical System (LDS) that takes this information into account. The Bayesian paradigm provides an effective fitting strategy, since it combines in the same framework both the shape prior and multiple sets of patch alignment classifiers to further improve the accuracy. Extensive evaluations were performed on several datasets including the challenging Labeled Faces in the Wild (LFW). Face parts descriptors were also evaluated, including the recently proposed Minimum Output Sum of Squared Error (MOSSE) filter. The proposed Bayesian optimization strategy improves on the state-of-the-art while using the same local detectors. We also show that MOSSE filters further improve on these results."'),
('"Discriminative Decorrelation for Clustering and Classification"', '"ECCV 2012"', '["Linear Discriminant Analysis", "Object Detection", "Average Precision", "Image Patch", "Object Cat', '"https://doi.org/10.1007/978-3-642-33765-9_33"', '"Object detection has over the past few years converged on using linear SVMs over HOG features. Training linear SVMs however is quite expensive, and can become intractable as the number of categories increase. In this work we revisit a much older technique, viz. Linear Discriminant Analysis, and show that LDA models can be trained almost trivially, and with little or no loss in performance. The covariance matrices we estimate capture properties of natural images. Whitening HOG features with these covariances thus removes naturally occuring correlations between the HOG features. We show that these whitened features (which we call WHO) are considerably better than the original HOG features for computing similarities, and prove their usefulness in clustering. Finally, we use our findings to produce an object detection system that is competitive on PASCAL VOC 2007 while being considerably easier to train and test."'),
('"Discriminative Indexing for Probabilistic Image Patch Priors"', '"ECCV 2014"', '["Gaussian Mixture Model", "Markov Random Field", "Image Patch", "Conditional Random Field", "Tree D', '"https://doi.org/10.1007/978-3-319-10593-2_14"', '"Newly emerged probabilistic image patch priors, such as Expected Patch Log-Likelihood (EPLL), have shown excellent performance on image restoration tasks, especially deconvolution, due to its rich expressiveness. However, its applicability is limited by the heavy computation involved in the associated optimization process. Inspired by the recent advances on using regression trees to index priors defined on a Conditional Random Field, we propose a novel discriminative indexing approach on patch-based priors to expedite the optimization process. Specifically, we propose an efficient tree indexing structure for EPLL, and overcome its training tractability challenges in high-dimensional spaces by utilizing special structures of the prior. Experimental results show that our approach accelerates state-of-the-art EPLL-based deconvolution methods by up to 40 times, with very little quality compromise."'),
('"Discriminative Learning for Deformable Shape Segmentation: A Comparative Study"', '"ECCV 2008"', '["Ground Truth", "Model Space", "Ranking Function", "Weak Learner", "Active Appearance Model"]', '"https://doi.org/10.1007/978-3-540-88682-2_54"', '"We present a comparative study on how to use discriminative learning methods such as classification, regression, and ranking to address deformable shape segmentation. Traditional generative models and energy minimization methods suffer from local minima. By casting the segmentation into a discriminative framework, the target fitting function can be steered to possess a desired shape for ease of optimization yet better characterize the relationship between shape and appearance. To address the high-dimensional learning challenge present in the learning framework, we use a multi-level approach to learning discriminative models. Our experimental results on left ventricle segmentation from ultrasound images and facial feature point localization demonstrate that the discriminative models outperform generative models and energy minimization methods by a large margin."'),
('"Discriminative Learning with Latent Variables for Cluttered Indoor Scene Understanding"', '"ECCV 2010"', '["Latent Variable", "Prior Constraint", "Inference Method", "Indoor Scene", "Major Face"]', '"https://doi.org/10.1007/978-3-642-15552-9_32"', '"We address the problem of understanding an indoor scene from a single image in terms of recovering the layouts of the faces (floor, ceiling, walls) and furniture. A major challenge of this task arises from the fact that most indoor scenes are cluttered by furniture and decorations, whose appearances vary drastically across scenes, and can hardly be modeled (or even hand-labeled) consistently. In this paper we tackle this problem by introducing latent variables to account for clutters, so that the observed image is jointly explained by the face and clutter layouts. Model parameters are learned in the maximum margin formulation, which is constrained by extra prior energy terms that define the role of the latent variables. Our approach enables taking into account and inferring indoor clutter layouts without hand-labeling of the clutters in the training set. Yet it outperforms the state-of-the-art method of Hedau et al. [4] that requires clutter labels."'),
('"Discriminative Locality Alignment"', '"ECCV 2008"', '["Linear Discriminant Analysis", "Face Image", "Locality Preserve Projection", "Unlabeled Sample", "', '"https://doi.org/10.1007/978-3-540-88682-2_55"', '"Fisher\\u2019s linear discriminant analysis (LDA), one of the most popular dimensionality reduction algorithms for classification, has three particular problems: it fails to find the nonlinear structure hidden in the high dimensional data; it assumes all samples contribute equivalently to reduce dimension for classification; and it suffers from the matrix singularity problem. In this paper, we propose a new algorithm, termed Discriminative Locality Alignment (DLA), to deal with these problems. The algorithm operates in the following three stages: first, in part optimization, discriminative information is imposed over patches, each of which is associated with one sample and its neighbors; then, in sample weighting, each part optimization is weighted by the margin degree, a measure of the importance of a given sample; and finally, in whole alignment, the alignment trick is used to align all weighted part optimizations to the whole optimization. Furthermore, DLA is extended to the semi-supervised case, i.e., semi-supervised DLA (SDLA), which utilizes unlabeled samples to improve the classification performance. Thorough empirical studies on the face recognition demonstrate the effectiveness of both DLA and SDLA."'),
('"Discriminative Mixture-of-Templates for Viewpoint Classification"', '"ECCV 2010"', '["Discriminative Learning", "Viewpoint Model", "Aspect Ratio Criterion", "Object Viewpoint", "Positi', '"https://doi.org/10.1007/978-3-642-15555-0_30"', '"Object viewpoint classification aims at predicting an approximate 3D pose of objects in a scene and is receiving increasing attention. State-of-the-art approaches to viewpoint classification use generative models to capture relations between object parts. In this work we propose to use a mixture of holistic templates (e.g. HOG) and discriminative learning for joint viewpoint classification and category detection. Inspired by the work of Felzenszwalb et al 2009, we discriminatively train multiple components simultaneously for each object category. A large number of components are learned in the mixture and they are associated with canonical viewpoints of the object through different levels of supervision, being fully supervised, semi-supervised, or unsupervised. We show that discriminative learning is capable of producing mixture components that directly provide robust viewpoint classification, significantly outperforming the state of the art: we improve the viewpoint accuracy on the Savarese et al 3D Object database from 57% to 74%, and that on the VOC 2006 car database from 73% to 86%. In addition, the mixture-of-templates approach to object viewpoint/pose has a natural extension to the continuous case by discriminatively learning a linear appearance model locally at each discrete view. We evaluate continuous viewpoint estimation on a dataset of everyday objects collected using IMUs for groundtruth annotation: our mixture model shows great promise comparing to a number of baselines including discrete nearest neighbor and linear regression."'),
('"Discriminative Nonorthogonal Binary Subspace Tracking"', '"ECCV 2010"', '["Tracking Error", "Object Tracking", "Object Representation", "Visual Tracking", "Image Template"]', '"https://doi.org/10.1007/978-3-642-15558-1_19"', '"Visual tracking is one of the central problems in computer vision. A crucial problem of tracking is how to represent the object. Traditional appearance-based trackers are using increasingly more complex features in order to be robust. However, complex representations typically will not only require more computation for feature extraction, but also make the state inference complicated. In this paper, we show that with a careful feature selection scheme, extremely simple yet discriminative features can be used for robust object tracking. The central component of the proposed method is a succinct and discriminative representation of image template using discriminative non-orthogonal binary subspace spanned by Haar-like features. These Haar-like bases are selected from the over-complete dictionary using a variation of the OOMP (optimized orthogonal matching pursuit). Such a representation inherits the merits of original NBS in that it can be used to efficiently describe the object. It also incorporates the discriminative information to distinguish the foreground and background. We apply the discriminative NBS to object tracking through SSD-based template matching. An update scheme of the discriminative NBS is devised in order to accommodate object appearance changes. We validate the effectiveness of our method through extensive experiments on challenging videos and demonstrate its capability to track objects in clutter and moving background."'),
('"Discriminative Sparse Image Models for Class-Specific Edge Detection and Image Interpretation"', '"ECCV 2008"', '["Edge Detection", "Sparse Representation", "Reconstruction Error", "Sparse Code", "Orthogonal Match', '"https://doi.org/10.1007/978-3-540-88690-7_4"', '"Sparse signal models learned from data are widely used in audio, image, and video restoration. They have recently been generalized to discriminative image understanding tasks such as texture segmentation and feature selection. This paper extends this line of research by proposing a multiscale method to minimize least-squares reconstruction errors and discriminative cost functions under \\u21130 or \\u21131 regularization constraints. It is applied to edge detection, category-based edge selection and image classification tasks. Experiments on the Berkeley edge detection benchmark and the PASCAL VOC\\u201905 and VOC\\u201907 datasets demonstrate the computational efficiency of our algorithm and its ability to learn local image descriptions that effectively support demanding computer vision tasks."'),
('"Discriminative Spatial Attention for Robust Tracking"', '"ECCV 2010"', '["Discriminative Power", "Attentional Region", "Robust Tracking", "Discriminative Score", "Spatial S', '"https://doi.org/10.1007/978-3-642-15549-9_35"', '"A major reason leading to tracking failure is the spatial distractions that exhibit similar visual appearances as the target, because they also generate good matches to the target and thus distract the tracker. It is in general very difficult to handle this situation. In a selective attention tracking paradigm, this paper advocates a new approach of discriminative spatial attention that identifies some special regions on the target, called attentional regions (ARs). The ARs show strong discriminative power in their discriminative domains where they do not observe similar things. This paper presents an efficient two-stage method that divides the discriminative domain into a local and a semi-local one. In the local domain, the visual appearance of an attentional region is locally linearized and its discriminative power is closely related to the property of the associated linear manifold, so that a gradient-based search is designed to locate the set of local ARs. Based on that, the set of semi-local ARs are identified through an efficient branch-and-bound procedure that guarantees the optimality. Extensive experiments show that such discriminative spatial attention leads to superior performances in many challenging target tracking tasks."'),
('"Discriminative Tracking by Metric Learning"', '"ECCV 2010"', '["Visual Target", "Appearance Model", "Visual Tracking", "Tracking Result", "Discriminative Model"]', '"https://doi.org/10.1007/978-3-642-15558-1_15"', '"We present a discriminative model that casts appearance modeling and visual matching into a single objective for visual tracking. Most previous discriminative models for visual tracking are formulated as supervised learning of binary classifiers. The continuous output of the classification function is then utilized as the cost function for visual tracking. This may be less desirable since the function is optimized for making binary decision. Such a learning objective may make it not to be able to well capture the manifold structure of the discriminative appearances. In contrast, our unified formulation is based on a principled metric learning framework, which seeks for a discriminative embedding for appearance modeling. In our formulation, both appearance modeling and visual matching are performed online by efficient gradient based optimization. Our formulation is also able to deal with multiple targets, where the exclusive principle is naturally reinforced to handle occlusions. Its efficacy is validated in a wide variety of challenging videos. It is shown that our algorithm achieves more persistent results, when compared with previous appearance model based tracking algorithms."'),
('"Discriminatively Trained Dense Surface Normal Estimation"', '"ECCV 2014"', '["Computer Vision", "Ground Truth", "Random Forest", "Visual Word", "Feature Representation"]', '"https://doi.org/10.1007/978-3-319-10602-1_31"', '"In this work we propose the method for a rather unexplored problem of computer vision - discriminatively trained dense surface normal estimation from a single image. Our method combines contextual and segment-based cues and builds a regressor in a boosting framework by transforming the problem into the regression of coefficients of a local coding. We apply our method to two challenging data sets containing images of man-made environments, the indoor NYU2 data set and the outdoor KITTI data set. Our surface normal predictor achieves results better than initially expected, significantly outperforming state-of-the-art."'),
('"Disentangling Factors of Variation for Facial Expression Recognition"', '"ECCV 2012"', '["emotion recognition", "contractive", "convolution", "deep learning", "auto-encoder", "TFD"]', '"https://doi.org/10.1007/978-3-642-33783-3_58"', '"We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a Contractive Discriminative Analysis (CDA) feature extractor, a novel variant of the Contractive Auto-Encoder (CAE), designed to learn a representation separating out the emotion-related factors from the others (which mostly capture the subject identity, and what is left of pose after the CCNET). This system beats the state-of-the-art on a recently proposed dataset for facial expression recognition, the Toronto Face Database, moving the state-of-art accuracy from 82.4% to 85.0%, while the CCNET and CDA improve accuracy of a standard CAE by 8%."'),
('"Disparity Statistics for Pedestrian Detection: Combining Appearance, Motion and Stereo"', '"ECCV 2010"', '["Ground Plane", "Motion Information", "Human Detection", "Pedestrian Detection", "Disparity Statist', '"https://doi.org/10.1007/978-3-642-15567-3_14"', '"Pedestrian detection is an important problem in computer vision due to its importance for applications such as visual surveillance, robotics, and automotive safety. This paper pushes the state-of-the-art of pedestrian detection in two ways. First, we propose a simple yet highly effective novel feature based on binocular disparity, outperforming previously proposed stereo features. Second, we show that the combination of different classifiers often improves performance even when classifiers are based on the same feature or feature combination. These two extensions result in significantly improved performance over the state-of-the-art on two challenging datasets."'),
('"Displacement Template with Divide-&-Conquer Algorithm for Significantly Improving Descriptor Based ', '"ECCV 2012"', '["Descriptor Approach", "Template", "Face Recognition"]', '"https://doi.org/10.1007/978-3-642-33715-4_16"', '"This paper proposes a displacement template structure for improving descriptor based face recognition approaches. With this template structure, a face is represented by a template consisting of a set of piled blocks; each block pile consists of a few heavily overlapped blocks from the face image. An ensemble of blocks, one from each pile, is taken as a candidate image of the face. When a descriptor based approach is used, we are able to generate a displacement description template for the face by replacing each block in the template with its local description, where a concatenation of the local descriptions of the blocks, one from each pile, is taken to be a candidate description of the face. Using the description template together with a divide-and-conquer algorithm for computing the similarities between description templates, we have demonstrated the significantly improved performance of LBP, TPLBP and FPLBP templates over original LBP, TPLBP and FPLBP approaches by the experiments on benchmark face databases."'),
('"Distance Estimation of an Unknown Person from a Portrait"', '"ECCV 2014"', '["Camera-subject distance", "Perspective distortion", "Pose estimation", "Face recognition"]', '"https://doi.org/10.1007/978-3-319-10590-1_21"', '"We propose the first automated method for estimating distance from frontal pictures of unknown faces. Camera calibration is not necessary, nor is the reconstruction of a 3D representation of the shape of the head. Our method is based on estimating automatically the position of face and head landmarks in the image, and then using a regressor to estimate distance from such measurements. We collected and annotated a dataset of frontal portraits of 53 individuals spanning a number of attributes (sex, age, race, hair), each photographed from seven distances. We find that our proposed method outperforms humans performing the same task. We observe that different physiognomies will bias systematically the estimate of distance, i.e. some people look closer than others. We expire which landmarks are more important for this task."'),
('"Distortion Correction in 3D-Modeling of Root Systems for Plant Phenotyping"', '"ECCV 2014"', '["Root phenotyping", "Gel-based media", "Hydroponicsubstrate", "Glass cylinder", "Distortion correct', '"https://doi.org/10.1007/978-3-319-16220-1_11"', '"Root Phenotyping is an important tool in predicting the life and growth of plants. Many systems have been developed to automate the process of extracting root traits using 3D imaging system, however, not many of those systems corrected for the distortions that frequently appear during this process. In this paper we present a new method to compensate for light refractions that occur due to hydroponic substrates \\u2013 gel-based platforms for growing plants. As our results demonstrate, our method provides an accurate 3D point cloud containing the coordinates of the surface of the root system with error smaller than 0.16 mm in average and standard deviation of less than 0.13 mm."'),
('"Divergence-Based Medial Surfaces"', '"ECCV 2000"', '["Hamiltonian System", "Voronoi Diagram", "Gradient Vector", "Medial Surface", "Energy Principle"]', '"https://doi.org/10.1007/3-540-45054-8_39"', '"The medial surface of a volumetric object is of significant interest for shape analysis. However, its numerical computation can be subtle. Methods based on Voronoi techniques preserve the object\\u2019s topology, but heuristic pruning measures are introduced to remove unwanted faces. Approaches based on Euclidean distance functions can localize medial surface points accurately, but often at the cost of altering the object\\u2019s topology. In this paper we introduce a new algorithm for computing medial surfaces which addresses these concerns. The method is robust and accurate, has low computational complexity, and preserves topology. The key idea is to measure the net outward flux of a vector field per unit volume, and to detect locations where a conservation of energy principle is violated. This is done in conjunction with a thinning process applied in a cubic lattice. We illustrate the approach with examples of medial surfaces of synthetic objects and complex anatomical structures obtained from medical images."'),
('"Divergence-Free Motion Estimation"', '"ECCV 2012"', '["State Vector", "Image Sequence", "Data Assimilation", "Motion Estimation", "Image Observation"]', '"https://doi.org/10.1007/978-3-642-33765-9_2"', '"This paper describes an innovative approach to estimate motion from image observations of divergence-free flows. Unlike most state-of-the-art methods, which only minimize the divergence of the motion field, our approach utilizes the vorticity-velocity formalism in order to construct a motion field in the subspace of divergence free functions. A 4DVAR-like image assimilation method is used to generate an estimate of the vorticity field given image observations. Given that vorticity estimate, the motion is obtained solving the Poisson equation. Results are illustrated on synthetic image observations and compared to those obtained with state-of-the-art methods, in order to quantify the improvements brought by the presented approach. The method is then applied to ocean satellite data to demonstrate its performance on the real images."'),
('"Diverse M-Best Solutions in Markov Random Fields"', '"ECCV 2012"', '["Lagrangian Relaxation", "Probable Solution", "Markov Random", "Dissimilarity Function", "Diverse S', '"https://doi.org/10.1007/978-3-642-33715-4_1"', '"Much effort has been directed at algorithms for obtaining the highest probability (MAP) configuration in probabilistic (random field) models. In many situations, one could benefit from additional high-probability solutions. Current methods for computing the M most probable configurations produce solutions that tend to be very similar to the MAP solution and each other. This is often an undesirable property. In this paper we propose an algorithm for the Diverse M-Best problem, which involves finding a diverse set of highly probable solutions under a discrete probabilistic model. Given a dissimilarity function measuring closeness of two solutions, our formulation involves maximizing a linear combination of the probability and dissimilarity to previous solutions. Our formulation generalizes the M-Best MAP problem and we show that for certain families of dissimilarity functions we can guarantee that these solutions can be found as easily as the MAP solution."'),
('"Djinn: Interaction Framework for Home Environment Using Speech and Vision"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_15"', '"In this paper we describe an interaction framework that uses speech recognition and computer vision to model new generation of interfaces in the residential environment. We outline the blueprints of the architecture and describe the main building blocks. We show a concrete prototype platform where this novel architecture has been deployed and will be tested at the user field trials. EC co-funds this work as part of HomeTalk IST-2001-33507 project."'),
('"Dog Breed Classification Using Part Localization"', '"ECCV 2012"', '["Face Detection", "Query Image", "Color Histogram", "Part Localization", "Sift Feature"]', '"https://doi.org/10.1007/978-3-642-33718-5_13"', '"We propose a novel approach to fine-grained image classification in which instances from different classes share common parts but have wide variation in shape and appearance. We use dog breed identification as a test case to show that extracting corresponding parts improves classification performance. This domain is especially challenging since the appearance of corresponding parts can vary dramatically, e.g., the faces of bulldogs and beagles are very different. To find accurate correspondences, we build exemplar-based geometric and appearance models of dog breeds and their face parts. Part correspondence allows us to extract and compare descriptors in like image locations. Our approach also features a hierarchy of parts (e.g., face and eyes) and breed-specific part localization. We achieve 67% recognition rate on a large real-world dataset including 133 dog breeds and 8,351 images, and experimental results show that accurate part localization significantly increases classification performance compared to state-of-the-art approaches."'),
('"Domain Adaptation with a Domain Specific Class Means Classifier"', '"ECCV 2014"', '["Domain adaptation", "Self-adative metric learning", "NCM"]', '"https://doi.org/10.1007/978-3-319-16199-0_3"', '"We consider the problem of learning a classifier when we dispose little training data from the target domain but abundant training data from several source domains. We make two contributions to the domain adaptation problem. First we extend the Nearest Class Mean (NCM) classifier by introducing for each class domain-dependent mean parameters as well as domain-specific weights. Second, we propose a generic adaptive semi-supervised metric learning technique that iteratively curates the training set by adding unlabeled samples with high prediction confidence and by removing labeled samples for which the prediction confidence is low. These two complementary techniques are evaluated on two public benchmarks: the ImageClef Domain Adaptation Challenge and the Office-CalTech datasets. Both contributions are shown to yield improvements and to be complementary to each other."'),
('"Domain Adaptive Dictionary Learning"', '"ECCV 2012"', '["Face Recognition", "Source Image", "Sparse Representation", "Sparse Code", "Domain Parameter"]', '"https://doi.org/10.1007/978-3-642-33765-9_45"', '"Many recent efforts have shown the effectiveness of dictionary learning methods in solving several computer vision problems. However, when designing dictionaries, training and testing domains may be different, due to different view points and illumination conditions. In this paper, we present a function learning framework for the task of transforming a dictionary learned from one visual domain to the other, while maintaining a domain-invariant sparse representation of a signal. Domain dictionaries are modeled by a linear or non-linear parametric function. The dictionary function parameters and domain-invariant sparse codes are then jointly learned by solving an optimization problem. Experiments on real datasets demonstrate the effectiveness of our approach for applications such as face recognition, pose alignment and pose estimation."'),
('"Domain-Adaptive Discriminative One-Shot Learning of Gestures"', '"ECCV 2014"', '["Sign Language", "Gesture Recognition", "Domain Adaptation", "Dynamic Time Warping", "Hand Shape"]', '"https://doi.org/10.1007/978-3-319-10599-4_52"', '"The objective of this paper is to recognize gestures in videos \\u2013 both localizing the gesture and classifying it into one of multiple classes."'),
('"Dramatic Improvements to Feature Based Stereo"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_17"', '"The camera registration extracted from feature based stereo is usually considered sufficient to accurately localize the 3D points. However, for natural scenes the feature localization is not as precise as in man-made environments. This results in small camera registration errors. We show that even very small registration errors result in large errors in dense surface reconstruction."'),
('"Drawing an Automatic Sketch of Deformable Objects Using Only a Few Images"', '"ECCV 2012"', '["Salient Contours", "Part-based Deformable Contour Matching", "Contour Completion"]', '"https://doi.org/10.1007/978-3-642-33863-2_7"', '"We propose a method to automatically extract a sketch of a common object structure present in a small set of real world weakly-labeled images. Applying a part-based deformable contour matching technique gives the location of repeatable contours. An initial deformable search strategy selects a set of salient, repeatable contours robust to a large range of non-rigid deformations. A contour completion technique based on a locally greedy bi-directional search strategy is adopted to merge the repeatable contour fragments for obtaining a complete shape. The output of our algorithm is used as an input to a sketch-based object-recognizer with results that are either better, or on par with those obtained with the ground truth sketches provided with the dataset."'),
('"DREAM2S: Deformable Regions Driven by an Eulerian Accurate Minimization Method for Image and Video ', '"ECCV 2002"', '["Evolution Equation", "Video Sequence", "Additional Term", "Active Contour", "Homogeneous Region"]', '"https://doi.org/10.1007/3-540-47977-5_24"', '"In this paper, we propose a general Eulerian framework for region-based active contours named DREAM2S. We introduce a general criterion including both region-based and boundary-based terms where the information on a region is named \\u201cdescriptor\\u201d. The originality of this work is twofold. Firstly we propose to use shape optimization principles to compute the evolution equation of the active contour that will make it evolve as fast as possible towards a minimum of the criterion. Secondly, we take into account the variation of the descriptors during the propagation of the curve. Indeed, a descriptor is generally globally attached to the region and thus \\u201cregion-dependent\\u201d. This case arises for example if the mean or the variance of a region are chosen as descriptors. We show that the dependence of the descriptors with the region induces additional terms in the evolution equation of the active contour that have never been previously computed. DREAM2S gives an easy way to take such a dependence into account and to compute the resulting additional terms. Experimental results point out the importance of the additional terms to reach a true minimum of the criterion and so to obtain accurate results. The covariance matrix determinant appears to be a very relevant tool for homogeneous color regions segmentation. As an example, it has been successfully applied to face detection in real video sequences."'),
('"Dual-Force Metric Learning for Robust Distracter-Resistant Tracker"', '"ECCV 2012"', '["Visual tracking", "distracter", "distance metric", "similarity propagation"]', '"https://doi.org/10.1007/978-3-642-33718-5_37"', '"In this paper, we propose a robust distracter-resistant tracking approach by learning a discriminative metric that adaptively learns the importance of features on-the-fly. The proposed metric is elaborately designed for the tracking problem by forming a margin objective function which systematically includes distance margin maximization and reconstruction error constraint that acts as a force to push distracters away from the positive space and into the negative space. Due to the variety of negative samples in the tracking problem, we specifically introduce the similarity propagation technique that gives distracters a second force from the negative space. Consequently, the discriminative metric obtained helps to preserve the most discriminative information to separate the target from distracters while ensuring the stability of the optimal metric. We seamlessly combine it with the popular L1 minimization tracker. Our tracker is therefore not only resistant to distracters, but also inherits the merit of occlusion robustness from the L1 tracker. Quantitative comparisons with several state-of-the-art algorithms have been conducted in many challenging video sequences. The results show that our method resists distracters excellently and achieves superior performance."'),
('"Duality and the Continuous Graphical Model"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-10578-9_18"', '"Inspired by the Linear Programming based algorithms for discrete MRFs, we show how a corresponding infinite-dimensional dual for continuous-state MRFs can be approximated by a hierarchy of tractable relaxations. This hierarchy of dual programs includes as a special case the methods of Peng et al. [17] and Zach & Kohli [33]. We give approximation bounds for the tightness of our construction, study their relationship to discrete MRFs and give a generic optimization algorithm based on Nesterov\\u2019s dual-smoothing method [16]."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Duals, Invariants, and the Recognition of Smooth Objects from Their Occluding Contour"', '"ECCV 2000"', '["Tangent Line", "Image View", "Initial Curve", "Rigid Transformation", "Image Contour"]', '"https://doi.org/10.1007/3-540-45054-8_51"', '"This paper presents a new geometric relation between a solid bounded by a smooth surface and its silhouette in images formed under weak perspective projection. The relation has the potential to be used for recognizing complex 3-D objects from a single image. Objects are modeled by showing them to a camera without any knowledge of their motion. The main idea is to consider the dual of the 3-D surface and the family of dual curves of the silhouettes over all viewing directions. Occluding contours correspond to planar slices of the dual surface. We introduce an affine-invariant representation of this surface that can constructed from a sequence of images and allows an object to be recognized from arbitrary viewing directions. We illustrate the proposed object representation scheme through synthetic examples and image contours detected in real images."'),
('"Dynamic Color Flow: A Motion-Adaptive Color Model for Object Segmentation in Video"', '"ECCV 2010"', '["Motion Estimation", "Color Model", "Object Segmentation", "Video Object", "Video Segmentation"]', '"https://doi.org/10.1007/978-3-642-15555-0_45"', '"Accurately modeling object colors, and features in general, plays a critical role in video segmentation and analysis. Commonly used color models, such as global Gaussian mixtures, localized Gaussian mixtures, and pixel-wise adaptive ones, often fail to accurately represent the object appearance in complicated scenes, thereby leading to segmentation errors. We introduce a new color model, Dynamic Color Flow, which unlike previous approaches, incorporates motion estimation into color modeling in a probabilistic framework, and adaptively changes model parameters to match the local properties of the motion. The proposed model accurately and reliably describes changes in the scene\\u2019s appearance caused by motion across frames. We show how to apply this color model to both foreground and background layers in a balanced way for efficient object segmentation in video. Experimental results show that when compared with previous approaches, our model provides more accurate foreground and background estimations, leading to more efficient video object cutout systems."'),
('"Dynamic Context for Tracking behind Occlusions"', '"ECCV 2012"', '["dynamics-based tracking", "occlusion", "context"]', '"https://doi.org/10.1007/978-3-642-33715-4_42"', '"Tracking objects in the presence of clutter and occlusion remains a challenging problem. Current approaches often rely on a priori target dynamics and/or use nearly rigid image context to determine the target position. In this paper, a novel algorithm is proposed to estimate the location of a target while it is hidden due to occlusion. The main idea behind the algorithm is to use contextual dynamical cues from multiple supporter features which may move with the target, move independently of the target, or remain stationary. These dynamical cues are learned directly from the data without making prior assumptions about the motions of the target and/or the support features. As illustrated through several experiments, the proposed algorithm outperforms state of the art approaches under long occlusions and severe camera motion."'),
('"Dynamic Eye Movement Datasets and Learnt Saliency Models for Visual Action Recognition"', '"ECCV 2012"', '["Action Recognition", "Interest Point", "Central Bias", "Multiple Kernel Learn", "Saliency Model"]', '"https://doi.org/10.1007/978-3-642-33709-3_60"', '"Systems based on bag-of-words models operating on image features collected at maxima of sparse interest point operators have been extremely successful for both computer-based visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in \\u201dsaccade and fixate\\u201d regimes, the knowledge, methodology, and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large-scale dynamic computer vision datasets like Hollywood-2[1] and UCF Sports[2] with human eye movements collected under the ecological constraints of the visual action recognition task. To our knowledge these are the first massive human eye tracking datasets of significant size to be collected for video (497,107 frames, each viewed by 16 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as opposed to free-viewing. Second, we introduce novel dynamic consistency and alignment models, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the massive amounts of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the most advanced computer vision practice, can lead to state of the art results."'),
('"Dynamic Facial Expression Recognition Using Longitudinal Facial Expression Atlases"', '"ECCV 2012"', '["Facial Expression", "Local Binary Pattern", "Facial Expression Recognition", "Average Recognition ', '"https://doi.org/10.1007/978-3-642-33709-3_45"', '"In this paper, we propose a new scheme to formulate the dynamic facial expression recognition problem as a longitudinal atlases construction and deformable groupwise image registration problem. The main contributions of this method include: 1) We model human facial feature changes during the facial expression process by a diffeomorphic image registration framework; 2) The subject-specific longitudinal change information of each facial expression is captured by building an expression growth model; 3) Longitudinal atlases of each facial expression are constructed by performing groupwise registration among all the corresponding expression image sequences of different subjects. The constructed atlases can reflect overall facial feature changes of each expression among the population, and can suppress the bias due to inter-personal variations. The proposed method was extensively evaluated on the Cohn-Kanade, MMI, and Oulu-CASIA VIS dynamic facial expression databases and was compared with several state-of-the-art facial expression recognition approaches. Experimental results demonstrate that our method consistently achieves the highest recognition accuracies among other methods under comparison on all the databases."'),
('"Dynamic Integration of Generalized Cues for Person Tracking"', '"ECCV 2008"', '["Partial Occlusion", "Proposal Distribution", "Layered Sampling", "Dynamic Integration", "Limited C', '"https://doi.org/10.1007/978-3-540-88693-8_38"', '"We present an approach for the dynamic combination of multiple cues in a particle filter-based tracking framework. The proposed algorithm is based on a combination of democratic integration and layered sampling. It is capable of dealing with deficiencies of single features as well as partial occlusion using the very same dynamic fusion mechanism. A set of simple but fast cues is defined, which allow us to cope with limited computational resources. The system is capable of automatic track initialization by means of a dedicated attention tracker permanently scanning the surroundings."'),
('"Dynamic Markov Random Field Model for Visual Tracking"', '"ECCV 2012"', '["Markov random field", "Dynamic Markov random field", "Visual tracking"]', '"https://doi.org/10.1007/978-3-642-33885-4_21"', '"We propose a new dynamic Markov random field (DMRF) model to track a heavily occluded object. The DMRF model is a bidirectional graph which consists of three random variables: hidden, observation, and validity. It temporally prunes invalid nodes and links edges among valid nodes by verifying validities of all nodes. In order to apply the proposed DMRF model to the object tracking framework, we use an image block lattice model exactly correspond to nodes and edges in the DMRF model and utilize the mean-shift belief propagation (MSBP). The proposed object tracking method using the DMRF surprisingly tracks a heavily occluded object even if the occluded region is more than 70~80%. Experimental results show that the proposed tracking method gives good tracking performance even on various tracking image sequences(ex. human and face) with heavy occlusion."'),
('"Dynamic Probabilistic CCA for Analysis of Affective Behaviour"', '"ECCV 2012"', '["Ground Truth", "Latent Space", "Dynamic Time Warping", "Observation Sequence", "Relevance Vector M', '"https://doi.org/10.1007/978-3-642-33786-4_8"', '"Fusing multiple continuous expert annotations is a crucial problem in machine learning and computer vision, particularly when dealing with uncertain and subjective tasks related to affective behaviour. Inspired by the concept of inferring shared and individual latent spaces in probabilistic CCA (PCCA), we firstly propose a novel, generative model which discovers temporal dependencies on the shared/individual spaces (DPCCA). In order to accommodate for temporal lags which are prominent amongst continuous annotations, we further introduce a latent warping process. We show that the resulting model (DPCTW) (i) can be used as a unifying framework for solving the problems of temporal alignment and fusion of multiple annotations in time, and (ii) that by incorporating dynamics, modelling annotation/sequence specific biases, noise estimation and time warping, DPCTW outperforms state-of-the-art methods for both the aggregation of multiple, yet imperfect expert annotations as well as the alignment of affective behavior."'),
('"Dynamic Programming for Approximate Expansion Algorithm"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33712-3_61"', '"Expansion algorithm is a popular optimization method for labeling problems. For many common energies, each expansion step can be optimally solved with a min-cut/max flow algorithm. While the observed performance of max-flow for the expansion algorithm is fast, its theoretical time complexity is worse than linear in the number of pixels. Recently, Dynamic Programming (DP) was shown to be useful for 2D labeling problems via a \\u201ctiered labeling\\u201d algorithm, although the structure of allowed (tiered) is quite restrictive. We show another use of DP in a 2D labeling case. Namely, we use DP for an approximate expansion step. Our expansion-like moves are more limited in the structure than the max-flow expansion moves. In fact, our moves are more restrictive than the tiered labeling structure, but their complexity is linear in the number of pixels, making them extremely efficient in practice. We illustrate the performance of our DP-expansion on the Potts energy, but our algorithm can be used for any pairwise energies. We achieve better efficiency with almost the same energy compared to the max-flow expansion moves."'),
('"Dynamic Texture Recognition Using Volume Local Binary Patterns"', '"WDV 2006"', '["Dynamic Texture", "Neighboring Frame", "Temporal Texture", "Normal Flow Feature", "Dynamic Texture', '"https://doi.org/10.1007/978-3-540-70932-9_13"', '"Dynamic texture is an extension of texture to the temporal domain. Description and recognition of dynamic textures has attracted growing attention. In this paper, a new method for recognizing dynamic textures is proposed. The textures are modeled with volume local binary patterns (VLBP), which are an extension of the LBP operator widely used in still texture analysis, combining the motion and appearance together. A rotation invariant VLBP is also proposed. Our approach has many advantages compared with the earlier approaches, providing a better performance for two test databases. Due to its rotation invariance and robustness to gray-scale variations, the method is very promising for practical applications."'),
('"Dynamic Trees: Learning to Model Outdoor Scenes"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47979-1_6"', '"This paper considers the dynamic tree (DT) model, first introduced in [1]. A dynamic tree specifies a prior over structures of trees, each of which is a forest of one or more tree-structured belief networks (TSBN). In the literature standard tree-structured belief network models have been found to produce \\u201cblocky\\u201d segmentations when naturally occurring boundaries within an image did not coincide with those of the subtrees in the fixed structure of the network. Dynamic trees have a flexible architecture which allows the structure to vary to create configurations where the subtree and image boundaries align, and experimentation with the model has shown significant improvements."'),
('"Dynamic Visual Search Using Inner-Scene Similarity: Algorithms and Inherent Limitations"', '"ECCV 2004"', '["Feature Vector", "Visual Search", "Search Task", "Stochastic Approach", "Partial Description"]', '"https://doi.org/10.1007/978-3-540-24671-8_5"', '"A dynamic visual search framework based mainly on inner-scene similarity is proposed. Algorithms as well as measures quantifying the difficulty of search tasks are suggested. Given a number of candidates (e.g. sub-images), our basic hypothesis is that more visually similar candidates are more likely to have the same identity. Both deterministic and stochastic approaches, relying on this hypothesis, are used to quantify this intuition. Under the deterministic approach, we suggest a measure similar to Kolmogorov\\u2019s \\u03b5-covering that quantifies the difficulty of a search task and bounds the performance of all search algorithms. We also suggest a simple algorithm that meets this bound. Under the stochastic approach, we model the identities of the candidates as correlated random variables and characterize the task using its second order statistics. We derive a search procedure based on minimum MSE linear estimation. Simple extensions enable the algorithm to use top-down and/or bottom-up information, when available."'),
('"D\\u00e9j\\u00e0 Vu:"', '"ECCV 2014"', '["Static Image", "Patch Size", "Motion Vector", "Action Recognition", "Camera Motion"]', '"https://doi.org/10.1007/978-3-319-10578-9_12"', '"This paper proposes motion prediction in single still images by learning it from a set of videos. The building assumption is that similar motion is characterized by similar appearance. The proposed method learns local motion patterns given a specific appearance and adds the predicted motion in a number of applications. This work (i) introduces a novel method to predict motion from appearance in a single static image, (ii) to that end, extends of the Structured Random Forest with regression derived from first principles, and (iii) shows the value of adding motion predictions in different tasks such as: weak frame-proposals containing unexpected events, action recognition, motion saliency. Illustrative results indicate that motion prediction is not only feasible, but also provides valuable information for a number of applications."'),
('"Easy Minimax Estimation with Random Forests for Human Pose Estimation"', '"ECCV 2014"', '["Human pose estimation", "Regression", "Regression forests", "Minimax"]', '"https://doi.org/10.1007/978-3-319-16178-5_47"', '"We describe a method for human parsing that is straightforward and competes with state-of-the-art performance on standard datasets. Unlike the state-of-the-art, our method does not search for individual body parts or poselets. Instead, a regression forest is used to predict a body configuration in body-space. The output of this regression forest is then combined in a novel way. Instead of averaging the output of each tree in the forest we use minimax to calculate optimal weights for the trees. This optimal weighting improves performance on rare poses and improves the generalization of our method to different datasets. Our paper demonstrates the unique advantage of random forest representations: minimax estimation is straightforward with no significant retraining burden."'),
('"Edge Boxes: Locating Object Proposals from Edges"', '"ECCV 2014"', '["object proposals", "object detection", "edge detection"]', '"https://doi.org/10.1007/978-3-319-10602-1_26"', '"The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box\\u2019s boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96% object recall at overlap threshold of 0.5 and over 75% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy."'),
('"Edge-Preserving Smoothing and Mean-Shift Segmentation of Video Streams"', '"ECCV 2008"', '["Video Stream", "Video Streaming", "Anisotropic Diffusion", "Partial Derivative Equation", "Tempora', '"https://doi.org/10.1007/978-3-540-88688-4_34"', '"Video streams are ubiquitous in applications such as surveillance, games, and live broadcast. Processing and analyzing these data is challenging because algorithms have to be efficient in order to process the data on the fly. From a theoretical standpoint, video streams have their own specificities \\u2013 they mix spatial and temporal dimensions, and compared to standard video sequences, half of the information is missing, i.e. the future is unknown. The theoretical part of our work is motivated by the ubiquitous use of the Gaussian kernel in tools such as bilateral filtering and mean-shift segmentation. We formally derive its equivalent for video streams as well as a dedicated expression of isotropic diffusion. Building upon this theoretical ground, we adapt a number of classical algorithms to video streams: bilateral filtering, mean-shift segmentation, and anisotropic diffusion."'),
('"Effective and Efficient Image Copy Detection Based on GPU"', '"ECCV 2010"', '["image copy detection", "CUDA", "GPU", "local feature", "semilocal spatial coherent verification"]', '"https://doi.org/10.1007/978-3-642-35740-4_26"', '"To improve the accuracy and efficiency of image copy detection, a novel system is proposed based on Graphics Processing Units (GPU). We combine two complementary local features, Harris-Laplace and SURF, to provide a compact representation of an image. By using complementary features, the image is better covered and the detection accuracy becomes less dependent on the actual image content. Moreover, ordinal measure (OM) is applied as semilocal spatial coherent verification. To improve time performance, the process of local features generation and OM calculating are implemented on the GPU through NVIDIA CUDA. Experiments show that our system achieves a 15% precision improvement over the baseline Hamming embedding approach. Compared to the CPU-based method, the GPU realization reaches up to a 30-40x speedup, having real-time performance."'),
('"Effective Appearance Model and Similarity Measure for Particle Filtering and Visual Tracking"', '"ECCV 2006"', '["Similarity Measure", "Target Object", "Particle Filter", "Color Histogram", "Visual Tracking"]', '"https://doi.org/10.1007/11744078_47"', '"In this paper, we adaptively model the appearance of objects based on Mixture of Gaussians in a joint spatial-color space (the approach is called SMOG). We propose a new SMOG-based similarity measure. SMOG captures richer information than the general color histogram because it incorporates spatial layout in addition to color. This appearance model and the similarity measure are used in a framework of Bayesian probability for tracking natural objects. In the second part of the paper, we propose an Integral Gaussian Mixture (IGM) technique, as a fast way to extract the parameters of SMOG for target candidate. With IGM, the parameters of SMOG can be computed efficiently by using only simple arithmetic operations (addition, subtraction, division) and thus the computation is reduced to linear complexity. Experiments show that our method can successfully track objects despite changes in foreground appearance, clutter, occlusion, etc.; and that it outperforms several color-histogram based methods."'),
('"Effective Use of Frequent Itemset Mining for Image Classification"', '"ECCV 2012"', '["Association Rule", "Visual Word", "Pattern Mining", "Image Representation", "Equal Error Rate"]', '"https://doi.org/10.1007/978-3-642-33718-5_16"', '"In this paper we propose a new and effective scheme for applying frequent itemset mining to image classification tasks. We refer to the new set of obtained patterns as Frequent Local Histograms or FLHs. During the construction of the FLHs, we pay special attention to keep all the local histogram information during the mining process and to select the most relevant reduced set of FLH patterns for classification. The careful choice of the visual primitives and some proposed extensions to exploit other visual cues such as colour or global spatial information allow us to build powerful bag-of-FLH-based image representations. We show that these bag-of-FLHs are more discriminative than traditional bag-of-words and yield state-of-the art results on various image classification benchmarks."'),
('"Efficient Articulated Trajectory Reconstruction Using Dynamic Programming and Filters"', '"ECCV 2012"', '["Basis Size", "Perspective Camera", "Articulation Constraint", "Trajectory Reconstruction", "Trajec', '"https://doi.org/10.1007/978-3-642-33718-5_6"', '"This paper considers the problem of reconstructing the motion of a 3D articulated tree from 2D point correspondences subject to some temporal prior. Hitherto, smooth motion has been encouraged using a trajectory basis, yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames. Branch and bound strategies have previously attempted to curb this complexity whilst maintaining global optimality. However, they provide no guarantee of being more efficient than exhaustive search. Inspired by recent work which reconstructs general trajectories using compact high-pass filters, we develop a dynamic programming approach which scales linearly in the number of frames, leveraging the intrinsically local nature of filter interactions. Extension to affine projection enables reconstruction without estimating cameras."'),
('"Efficient Belief Propagation with Learned Higher-Order Markov Random Fields"', '"ECCV 2006"', '["Belief Propagation", "Variable Node", "Image Denoising", "Factor Node", "Factor Graph"]', '"https://doi.org/10.1007/11744047_21"', '"Belief propagation (BP) has become widely used for low-level vision problems and various inference techniques have been proposed for loopy graphs. These methods typically rely on ad hoc spatial priors such as the Potts model. In this paper we investigate the use of learned models of image structure, and demonstrate the improvements obtained over previous ad hoc models for the image denoising problem. In particular, we show how both pairwise and higher-order Markov random fields with learned clique potentials capture rich image structures that better represent the properties of natural images. These models are learned using the recently proposed Fields-of-Experts framework. For such models, however, traditional BP is computationally expensive. Consequently we propose some approximation methods that make BP with learned potentials practical. In the case of pairwise models we propose a novel approximation of robust potentials using a finite family of quadratics. In the case of higher order MRFs, with 2\\u00d7 2 cliques, we use an adaptive state space to handle the increased complexity. Extensive experiments demonstrate the power of learned models, the benefits of higher-order MRFs and the practicality of BP for these problems with the use of simple principled approximations."'),
('"Efficient Camera Smoothing in Sequential Structure-from-Motion Using Approximate Cross-Validation"', '"ECCV 2008"', '["Smoothing Parameter", "Reconstruction Process", "Bundle Adjustment", "Reprojection Error", "Britis', '"https://doi.org/10.1007/978-3-540-88690-7_15"', '"In the sequential approach to three-dimensional reconstruction, adding prior knowledge about camera pose improves reconstruction accuracy. We add a smoothing penalty on the camera trajectory. The smoothing parameter, usually fixed by trial and error, is automatically estimated using Cross-Validation. This technique is extremely expensive in its basic form. We derive Gauss-Newton Cross-Validation, which closely approximates Cross-Validation, while being much cheaper to compute. The method is substantiated by experimental results on synthetic and real data. They show that it improves accuracy and stability in the reconstruction process, preventing several failure cases."'),
('"Efficient Closed-Form Solution to Generalized Boundary Detection"', '"ECCV 2012"', '["Window Size", "Boundary Detection", "Image Channel", "Window Center", "Occlusion Boundary"]', '"https://doi.org/10.1007/978-3-642-33765-9_37"', '"Boundary detection is essential for a variety of computer vision tasks such as segmentation and recognition. We propose a unified formulation for boundary detection, with closed-form solution, which is applicable to the localization of different types of boundaries, such as intensity edges and occlusion boundaries from video and RGB-D cameras. Our algorithm simultaneously combines low- and mid-level image representations, in a single eigenvalue problem, and we solve over an infinite set of putative boundary orientations. Moreover, our method achieves state of the art results at a significantly lower computational cost than current methods. We also propose a novel method for soft-segmentation that can be used in conjunction with our boundary detection algorithm and improve its accuracy at a negligible extra computational cost."'),
('"Efficient Color Constancy with Local Surface Reflectance Statistics"', '"ECCV 2014"', '["color constancy", "illuminant estimation", "reflectance", "retina"]', '"https://doi.org/10.1007/978-3-319-10605-2_11"', '"The aim of computational color constancy is to estimate the actual surface color in an acquired scene disregarding its illuminant. Many solutions try to first estimate the illuminant and then correct the image with the illuminant estimate. Based on the linear image formation model, we propose in this work a new strategy to estimate the illuminant. Inspired by the feedback modulation from horizontal cells to the cones in the retina, we first normalize each local patch with its local maximum to obtain the so-called locally normalized reflectance estimate (LNRE). Then, we experimentally found that the ratio of the global summation of true surface reflectance to the global summation of LNRE in a scene is approximately achromatic for both indoor and outdoor scenes. Based on this substantial observation, we estimate the illuminant by computing the ratio of the global summation of the intensities to the global summation of the locally normalized intensities of the color-biased image. The proposed model has only one free parameter and requires no explicit training with learning-based approach. Experimental results on four commonly used datasets show that our model can produce competitive or even better results compared to the state-of-the-art approaches with low computational cost."'),
('"Efficient Computation of Scale-Space Features for Deformable Shape Correspondences"', '"ECCV 2010"', '["Scale Space", "Scale Invariant Feature Transformation", "Correct Match", "Deformable Surface", "Ir', '"https://doi.org/10.1007/978-3-642-15558-1_28"', '"With the rapid development of fast data acquisition techniques, 3D scans that record the geometric and photometric information of deformable objects are routinely acquired nowadays. To track surfaces in temporal domain or stitch partially-overlapping scans to form a complete model in spatial domain, robust and efficient feature detection for deformable shape correspondences, as an enabling method, becomes fundamentally critical with pressing needs. In this paper, we propose an efficient method to extract local features in scale spaces of both texture and geometry for deformable shape correspondences. We first build a hierarchical scale space on surface geometry based on geodesic metric, and the pyramid representation of surface geometry naturally engenders the rapid computation of scale-space features. Analogous to the SIFT, our features are found as local extrema in the scale space. We then propose a new feature descriptor for deformable surfaces, which is a gradient histogram within a local region computed by a local parameterization. Both the detector and the descriptor are invariant to isometric deformation, which makes our method a powerful tool for deformable shape correspondences. The performance of the proposed method is evaluated by feature matching on a sequence of deforming surfaces with ground truth correspondences."'),
('"Efficient Dense Scene Flow from Sparse or Dense Stereo Data"', '"ECCV 2008"', '["Ground Truth", "Root Mean Square", "Motion Estimation", "Stereo Image", "Disparity Estimation"]', '"https://doi.org/10.1007/978-3-540-88682-2_56"', '"This paper presents a technique for estimating the three-dimensional velocity vector field that describes the motion of each visible scene point (scene flow). The technique presented uses two consecutive image pairs from a stereo sequence. The main contribution is to decouple the position and velocity estimation steps, and to estimate dense velocities using a variational approach. We enforce the scene flow to yield consistent displacement vectors in the left and right images. The decoupling strategy has two main advantages: Firstly, we are independent in choosing a disparity estimation technique, which can yield either sparse or dense correspondences, and secondly, we can achieve frame rates of 5 fps on standard consumer hardware. The approach provides dense velocity estimates with accurate results at distances up to 50 meters."'),
('"Efficient Discriminative Projections for Compact Binary Descriptors"', '"ECCV 2012"', '["Image Patch", "Stepwise Approach", "Integral Image", "Random Projection", "Stochastic Gradient Des', '"https://doi.org/10.1007/978-3-642-33718-5_17"', '"Binary descriptors of image patches are increasingly popular given that they require less storage and enable faster processing. This, however, comes at a price of lower recognition performances. To boost these performances, we project the image patches to a more discriminative subspace, and threshold their coordinates to build our binary descriptor. However, applying complex projections to the patches is slow, which negates some of the advantages of binary descriptors. Hence, our key idea is to learn the discriminative projections so that they can be decomposed into a small number of simple filters for which the responses can be computed fast. We show that with as few as 32 bits per descriptor we outperform the state-of-the-art binary descriptors in terms of both accuracy and efficiency."'),
('"Efficient Edge-Based Methods for Estimating Manhattan Frames in Urban Imagery"', '"ECCV 2008"', '["Ground Truth", "Camera Parameter", "Urban Scene", "Vanishing Point", "Gauss Sphere"]', '"https://doi.org/10.1007/978-3-540-88688-4_15"', '"We address the problem of efficiently estimating the rotation of a camera relative to the canonical 3D Cartesian frame of an urban scene, under the so-called \\u201cManhattan World\\u201d assumption [1,2]. While the problem has received considerable attention in recent years, it is unclear how current methods stack up in terms of accuracy and efficiency, and how they might best be improved. It is often argued that it is best to base estimation on all pixels in the image [2]. However, in this paper, we argue that in a sense, less can be more: that basing estimation on sparse, accurately localized edges, rather than dense gradient maps, permits the derivation of more accurate statistical models and leads to more efficient estimation. We also introduce and compare several different search techniques that have advantages over prior approaches. A cornerstone of the paper is the establishment of a new public groundtruth database which we use to derive required statistics and to evaluate and compare algorithms."'),
('"Efficient Exact Inference for 3D Indoor Scene Understanding"', '"ECCV 2012"', '["Integral Geometry", "Layout Data", "Indoor Scene", "Ground Truth Label", "Exact Inference"]', '"https://doi.org/10.1007/978-3-642-33783-3_22"', '"In this paper we propose the first exact solution to the problem of estimating the 3D room layout from a single image. This problem is typically formulated as inference in a Markov random field, where potentials count image features (e.g., geometric context, orientation maps, lines in accordance with vanishing points) in each face of the layout. We present a novel branch and bound approach which splits the label space in terms of candidate sets of 3D layouts, and efficiently bounds the potentials in these sets by restricting the contribution of each individual face. We employ integral geometry in order to evaluate these bounds in constant time, and as a consequence, we not only obtain the exact solution, but also in less time than approximate inference tools such as message-passing. We demonstrate the effectiveness of our approach in two benchmarks and show that our bounds are tight, and only a few evaluations are necessary."'),
('"Efficient Fingerprint Image Enhancement for Mobile Embedded Systems"', '"BioAW 2004"', '["Singular Value Decomposition", "Gabor Filter", "Image Block", "Fingerprint Image", "False Acceptan', '"https://doi.org/10.1007/978-3-540-25976-3_14"', '"Fingerprint image enhancement is an important step in a fingerprint verification system. The enhancement process, however, are often not applied to mobile embedded devices in which floating-point processing units (FPU) are absent. Earlier Hong and Jain reported a fingerprint enhancement algorithm based on the Gabor Filter. This algorithm and its derivatives have been proved to be quite effective in improving the fingerprint verification reliability. In this paper, we present an efficient implementation of this algorithm in an embedded system environment. In our implementation, fixed-point arithmetic is used to replace the floating-point operations. Moreover, a special Gabor filter parameter selection constraint is also proposed to reduce the computing complexity of the kernel generation step. Experimental results show that our new approach achieves significant speed improvement and is almost as effective as the traditional floating-point based implementation."'),
('"Efficient Highly Over-Complete Sparse Coding Using a Mixture Model"', '"ECCV 2010"', '["Gaussian Mixture Model", "Sparse Representation", "Local Descriptor", "Sparse Code", "Descriptor S', '"https://doi.org/10.1007/978-3-642-15555-0_9"', '"Sparse coding of sensory data has recently attracted notable attention in research of learning useful features from the unlabeled data. Empirical studies show that mapping the data into a significantly higher-dimensional space with sparse coding can lead to superior classification performance. However, computationally it is challenging to learn a set of highly over-complete dictionary bases and to encode the test data with the learned bases. In this paper, we describe a mixture sparse coding model that can produce high-dimensional sparse representations very efficiently. Besides the computational advantage, the model effectively encourages data that are similar to each other to enjoy similar sparse representations. What\\u2019s more, the proposed model can be regarded as an approximation to the recently proposed local coordinate coding (LCC), which states that sparse coding can approximately learn the nonlinear manifold of the sensory data in a locally linear manner. Therefore, the feature learned by the mixture sparse coding model works pretty well with linear classifiers. We apply the proposed model to PASCAL VOC 2007 and 2009 datasets for the classification task, both achieving state-of-the-art performances."'),
('"Efficient Image and Video Co-localization with Frank-Wolfe Algorithm"', '"ECCV 2014"', '["Object Tracking", "Image Model", "Video Model", "Temporal Consistency", "Adjacent Frame"]', '"https://doi.org/10.1007/978-3-319-10599-4_17"', '"In this paper, we tackle the problem of performing efficient co-localization in images and videos. Co-localization is the problem of simultaneously localizing (with bounding boxes) objects of the same class across a set of distinct images or videos. Building upon recent state-of-the-art methods, we show how we are able to naturally incorporate temporal terms and constraints for video co-localization into a quadratic programming framework. Furthermore, by leveraging the Frank-Wolfe algorithm (or conditional gradient), we show how our optimization formulations for both images and videos can be reduced to solving a succession of simple integer programs, leading to increased efficiency in both memory and speed. To validate our method, we present experimental results on the PASCAL VOC 2007 dataset for images and the YouTube-Objects dataset for videos, as well as a joint combination of the two."'),
('"Efficient Inference with Multiple Heterogeneous Part Detectors for Human Pose Estimation"', '"ECCV 2010"', '["Part Detector", "Pictorial Structure", "Region Template", "Active Branch", "Perceptron Algorithm"]', '"https://doi.org/10.1007/978-3-642-15558-1_23"', '"We address the problem of estimating human pose in a single image using a part based approach. Pose accuracy is directly affected by the accuracy of the part detectors but more accurate detectors are likely to be also more computationally expensive. We propose to use multiple, heterogeneous part detectors with varying accuracy and computation requirements, ordered in a hierarchy, to achieve more accurate and efficient pose estimation. For inference, we propose an algorithm to localize articulated objects by exploiting an ordered hierarchy of detectors with increasing accuracy. The inference uses branch and bound method to search for each part and use kinematics from neighboring parts to guide the branching behavior and compute bounds on the best part estimate. We demonstrate our approach on a publicly available People dataset and outperform the state-of-art methods. Our inference is 3 times faster than one based on using a single, highly accurate detector."'),
('"Efficient Initialization for Constrained Active Surfaces, Applications in 3D Medical Images"', '"MMBIA 2004"', '[]', '"https://doi.org/10.1007/978-3-540-27816-0_18"', '"A novel method allowing simplified and efficient active surface initialization for 3D images segmentation is presented. Our method allows to initialize an active surface through simple objects like points and curves and ensures that the further evolution of the active object will not be trapped by unwanted local minima. Our approach is based on minimal paths that integrate the information coming from the user given curves and from the image volume. The minimal paths build a network representing a first approximation of the initialization surface. An interpolation method is then used to build a mesh or an implicit representation based on the information retrieved from the network of paths. From this initialization, an active surface converges quickly to the expected solution. Our paper describes a fast construction obtained by exploiting the Fast Marching algorithm. The algorithm has been successfully applied to synthetic images and 3D medical images."'),
('"Efficient Joint Segmentation, Occlusion Labeling, Stereo and Flow Estimation"', '"ECCV 2014"', '["Flow Estimation", "Stereo Pair", "Boundary Pixel", "Autonomous Driving", "Trifocal Tensor"]', '"https://doi.org/10.1007/978-3-319-10602-1_49"', '"In this paper we propose a slanted plane model for jointly recovering an image segmentation, a dense depth estimate as well as boundary labels (such as occlusion boundaries) from a static scene given two frames of a stereo pair captured from a moving vehicle. Towards this goal we propose a new optimization algorithm for our SLIC-like objective which preserves connecteness of image segments and exploits shape regularization in the form of boundary length. We demonstrate the performance of our approach in the challenging stereo and flow KITTI benchmarks and show superior results to the state-of-the-art. Importantly, these results can be achieved an order of magnitude faster than competing approaches."'),
('"Efficient k-Support Matrix Pursuit"', '"ECCV 2014"', '["k-support norm", "subspace segmentation", "semi-supervised classification", "sparse coding"]', '"https://doi.org/10.1007/978-3-319-10605-2_40"', '"In this paper, we study the k-support norm regularized matrix pursuit problem, which is regarded as the core formulation for several popular computer vision tasks. The k-support matrix norm, a convex relaxation of the matrix sparsity combined with the \\u21132-norm penalty, generalizes the recently proposed k-support vector norm. The contributions of this work are two-fold. First, the proposed k-support matrix norm does not suffer from the disadvantages of existing matrix norms towards sparsity and/or low-rankness: 1) too sparse/dense, and/or 2) column independent. Second, we present an efficient procedure for k-support norm optimization, in which the computation of the key proximity operator is substantially accelerated by binary search. Extensive experiments on subspace segmentation, semi-supervised classification and sparse coding well demonstrate the superiority of the new regularizer over existing matrix-norm regularizers, and also the orders-of-magnitude speedup compared with the existing optimization procedure for the k-support norm."'),
('"Efficient Mining of Repetitions in Large-Scale TV Streams with Product Quantization Hashing"', '"ECCV 2012"', '["Hash Function", "Neighbor Search", "Hash Table", "Locality Sensitive Hash", "Hash Code"]', '"https://doi.org/10.1007/978-3-642-33863-2_27"', '"Duplicates or near-duplicates mining in video sequences is of broad interest to many multimedia applications. How to design an effective and scalable system, however, is still a challenge to the community. In this paper, we present a method to detect recurrent sequences in large-scale TV streams in an unsupervised manner and with little a priori knowledge on the content. The method relies on a product k-means quantizer that efficiently produces hash keys adapted to the data distribution for frame descriptors. This hashing technique combined with a temporal consistency check allows the detection of meaningful repetitions in TV streams. When considering all frames (about 47 millions) of a 22-day long TV broadcast, our system detects all repetitions in 15 minutes, excluding the computation of the frame descriptors. Experimental results show that our approach is a promising way to deal with very large video databases."'),
('"Efficient Misalignment-Robust Representation for Real-Time Face Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_61"', '"Sparse representation techniques for robust face recognition have been widely studied in the past several years. Recently face recognition with simultaneous misalignment, occlusion and other variations has achieved interesting results via robust alignment by sparse representation (RASR). In RASR, the best alignment of a testing sample is sought subject by subject in the database. However, such an exhaustive search strategy can make the time complexity of RASR prohibitive in large-scale face databases. In this paper, we propose a novel scheme, namely misalignment robust representation (MRR), by representing the misaligned testing sample in the transformed face space spanned by all subjects. The MRR seeks the best alignment via a two-step optimization with a coarse-to-fine search strategy, which needs only two deformation-recovery operations. Extensive experiments on representative face databases show that MRR has almost the same accuracy as RASR in various face recognition and verification tasks but it runs tens to hundreds of times faster than RASR. The running time of MRR is less than 1 second in the large-scale Multi-PIE face database, demonstrating its great potential for real-time face recognition."'),
('"Efficient Monte Carlo Sampler for Detecting Parametric Objects in Large Scenes"', '"ECCV 2012"', '["Point Cloud", "Markov Chain Monte Carlo", "Point Process", "Markov Random Fields", "Monte Carlo Sa', '"https://doi.org/10.1007/978-3-642-33712-3_39"', '"Point processes have demonstrated efficiency and competitiveness when addressing object recognition problems in vision. However, simulating these mathematical models is a difficult task, especially on large scenes. Existing samplers suffer from average performances in terms of computation time and stability. We propose a new sampling procedure based on a Monte Carlo formalism. Our algorithm exploits Markovian properties of point processes to perform the sampling in parallel. This procedure is embedded into a data-driven mechanism such that the points are non-uniformly distributed in the scene. The performances of the sampler are analyzed through a set of experiments on various object recognition problems from large scenes, and through comparisons to the existing algorithms."'),
('"Efficient NCC-Based Image Matching in Walsh-Hadamard Domain"', '"ECCV 2008"', '["pattern matching", "image matching", "image alignment", "normalized cross correlation", "winner up', '"https://doi.org/10.1007/978-3-540-88690-7_35"', '"In this paper, we proposed a fast image matching algorithm based on the normalized cross correlation (NCC) by applying the winner-update strategy on the Walsh-Hadamard transform. Walsh-Hadamard transform is an orthogonal transformation that is easy to compute and has nice energy packing capability. Based on the Cauchy-Schwarz inequality, we derive a novel upper bound for the cross-correlation of image matching in the Walsh-Hadamard domain. Applying this upper bound with the winner update search strategy can skip unnecessary calculation, thus significantly reducing the computational burden of NCC-based pattern matching. Experimental results show the proposed algorithm is very efficient for NCC-based image matching under different lighting conditions and noise levels."'),
('"Efficient Non-consecutive Feature Tracking for Structure-from-Motion"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15555-0_31"', '"Structure-from-motion (SfM) is an important computer vision problem and largely relies on the quality of feature tracking. In image sequences, if disjointed tracks caused by objects moving in and out of the view, occasional occlusion, or image noise, are not handled well, the corresponding SfM could be significantly affected. In this paper, we address the non-consecutive feature point tracking problem and propose an effective method to match interrupted tracks. Our framework consists of steps of solving the feature \\u2018dropout\\u2019 problem when indistinctive structures, noise or even large image distortion exist, and of rapidly recognizing and joining common features located in different subsequences. Experimental results on several challenging and large-scale video sets show that our method notably improves SfM."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Efficient Nonlocal Regularization for Optical Flow"', '"ECCV 2012"', '["Optical Flow", "Penalty Function", "Regularization Term", "Data Term", "Angular Error"]', '"https://doi.org/10.1007/978-3-642-33718-5_26"', '"Dense optical flow estimation in images is a challenging problem because the algorithm must coordinate the estimated motion across large regions in the image, while avoiding inappropriate smoothing over motion boundaries. Recent works have advocated for the use of nonlocal regularization to model long-range correlations in the flow. However, incorporating nonlocal regularization into an energy optimization framework is challenging due to the large number of pairwise penalty terms. Existing techniques either substitute intermediate filtering of the flow field for direct optimization of the nonlocal objective, or suffer substantial performance penalties when the range of the regularizer increases. In this paper, we describe an optimization algorithm that efficiently handles a general type of nonlocal regularization objectives for optical flow estimation. The computational complexity of the algorithm is independent of the range of the regularizer. We show that nonlocal regularization improves estimation accuracy at longer ranges than previously reported, and is complementary to intermediate filtering of the flow field. Our algorithm is simple and is compatible with many optical flow models."'),
('"Efficient Object Category Recognition Using Classemes"', '"ECCV 2010"', '["Training Image", "Category Label", "Image Search", "Linear Support Vector Machine", "Multiple Kern', '"https://doi.org/10.1007/978-3-642-15549-9_56"', '"We introduce a new descriptor for images which allows the construction of efficient and compact classifiers with good accuracy on object category recognition. The descriptor is the output of a large number of weakly trained object category classifiers on the image. The trained categories are selected from an ontology of visual concepts, but the intention is not to encode an explicit decomposition of the scene. Rather, we accept that existing object category classifiers often encode not the category per se but ancillary image characteristics; and that these ancillary characteristics can combine to represent visual classes unrelated to the constituent categories\\u2019 semantic meanings."'),
('"Efficient Online Spatio-Temporal Filtering for Video Event Detection"', '"ECCV 2014"', '["Video Sequence", "Salient Object", "Pedestrian Detection", "Video Event", "Baseline Detector"]', '"https://doi.org/10.1007/978-3-319-16178-5_54"', '"We propose a novel spatio-temporal filtering technique to improve the per-pixel prediction map, by leveraging the spatio-temporal smoothness of the video signal. Different from previous techniques that perform spatio-temporal filtering in an offline/batch mode, e.g., through graphical model, our filtering can be implemented online and in real-time, with provable lowest computational complexity. Moreover, it is compatible to any image analysis module that can produce per-pixel map of detection scores or multi-class prediction distributions. For each pixel, our filtering finds the optimal spatio-temporal trajectory in the past frames that has the maximum accumulated detection score. Pixels with small accumulated detection score will be treated as false alarm thus suppressed. To demonstrate the effectiveness of our online spatio-temporal filtering, we perform three video event tasks: salient action discovery, walking pedestrian detection, and sports event detection, all in an online/causal way. The experimental results on the three datasets demonstrate the excellent performances of our filtering scheme when compared with the state-of-the-art methods."'),
('"Efficient Optimization for Low-Rank Integrated Bilinear Classifiers"', '"ECCV 2012"', '["Feature Vector", "Canonical Correlation Analysis", "Reproduce Kernel Hilbert Space", "Hard Constra', '"https://doi.org/10.1007/978-3-642-33709-3_34"', '"In pattern classification, it is needed to efficiently treat two-way data (feature matrices) while preserving the two-way structure such as spatio-temporal relationships, etc. The classifier for the feature matrix is generally formulated by multiple bilinear forms which result in a matrix. The rank of the matrix, i.e., the number of bilinear forms, should be low from the viewpoint of generalization performance and computational cost. For that purpose, we propose a low-rank bilinear classifier based on the efficient optimization. In the proposed method, the classifier is optimized by minimizing the trace norm of the classifier (matrix), which contributes to the rank reduction for an efficient classifier without any hard constraint on the rank. We formulate the optimization problem in a tractable convex form and propose the procedure to solve it efficiently with the global optimum. In addition, by considering a kernel-based extension of the bilinear method, we induce a novel multiple kernel learning (MKL), called heterogeneous MKL. The method combines both inter kernels between heterogeneous types of features and the ordinary kernels within homogeneous features into a new discriminative kernel in a unified manner using the bilinear model. In the experiments on various classification problems using feature arrays, co-occurrence feature matrices, and multiple kernels, the proposed method exhibits favorable performances compared to the other methods."'),
('"Efficient Point-to-Subspace Query in \\u21131 with Application to Robust Face Recognition"', '"ECCV 2012"', '["\\u21131 point-to-subspace distance", "nearest subspace search", "Cauchy projection", "face recogni', '"https://doi.org/10.1007/978-3-642-33765-9_30"', '"Motivated by vision tasks such as robust face and object recognition, we consider the following general problem: given a collection of low-dimensional linear subspaces in a high-dimensional ambient (image) space, and a query point (image), efficiently determine the nearest subspace to the query in \\u21131 distance. We show in theory this problem can be solved with a simple two-stage algorithm: (1) random Cauchy projection of query and subspaces into low-dimensional spaces followed by efficient distance evaluation (\\u21131 regression); (2) getting back to the high-dimensional space with very few candidates and performing exhaustive search. We present preliminary experiments on robust face recognition to corroborate our theory."'),
('"Efficient Recursive Algorithms for Computing the Mean Diffusion Tensor and Applications to DTI Segm', '"ECCV 2012"', '["Segmentation Result", "Active Contour", "Structure Tensor", "Recursive Algorithm", "Recursive Esti', '"https://doi.org/10.1007/978-3-642-33786-4_29"', '"Computation of the mean of a collection of symmetric positive definite (SPD) matrices is a fundamental ingredient of many algorithms in diffusion tensor image (DTI) processing. For instance, in DTI segmentation, clustering, etc. In this paper, we present novel recursive algorithms for computing the mean of a set of diffusion tensors using several distance/divergence measures commonly used in DTI segmentation and clustering such as the Riemannian distance and symmetrized Kullback-Leibler divergence. To the best of our knowledge, to date, there are no recursive algorithms for computing the mean using these measures in literature. Recursive algorithms lead to a gain in computation time of several orders in magnitude over existing non-recursive algorithms. The key contributions of this paper are: (i) we present novel theoretical results on a recursive estimator for Karcher expectation in the space of SPD matrices, which in effect is a proof of the law of large numbers (with some restrictions) for the manifold of SPD matrices. (ii) We also present a recursive version of the symmetrized KL-divergence for computing the mean of a collection of SPD matrices. (iii) We present comparative timing results for computing the mean of a group of SPD matrices (diffusion tensors) depicting the gains in compute time using the proposed recursive algorithms over existing non-recursive counter parts. Finally, we also show results on gains in compute times obtained by applying these recursive algorithms to the task of DTI segmentation."'),
('"Efficient Similarity Derived from Kernel-Based Transition Probability"', '"ECCV 2012"', '["Label Sample", "Label Propagation", "Multiple Kernel", "Multiple Kernel Learning", "Unlabeled Samp', '"https://doi.org/10.1007/978-3-642-33783-3_27"', '"Semi-supervised learning effectively integrates labeled and unlabeled samples for classification, and most of the methods are founded on the pair-wise similarities between the samples. In this paper, we propose methods to construct similarities from the probabilistic viewpoint, whilst the similarities have so far been formulated in a heuristic manner such as by k-NN. We first propose the kernel-based formulation of transition probabilities via considering kernel least squares in the probabilistic framework. The similarities are consequently derived from the kernel-based transition probabilities which are efficiently computed, and the similarities are inherently sparse without applying k-NN. In the case of multiple types of kernel functions, the multiple transition probabilities are also obtained correspondingly. From the probabilistic viewpoint, they can be integrated with prior probabilities, i.e., linear weights, and we propose a computationally efficient method to optimize the weights in a discriminative manner, as in multiple kernel learning. The novel similarity is thereby constructed by the composite transition probability and it benefits the semi-supervised learning methods as well. In the various experiments on semi-supervised learning problems, the proposed methods demonstrate favorable performances, compared to the other methods, in terms of classification performances and computation time."'),
('"Efficient Sparsity Estimation via Marginal-Lasso Coding"', '"ECCV 2014"', '["Sparsity estimation", "marginal regression", "sparse coding", "lasso", "dictionary learning", "ada', '"https://doi.org/10.1007/978-3-319-10593-2_38"', '"This paper presents a generic optimization framework for efficient feature quantization using sparse coding which can be applied to many computer vision tasks. While there are many works working on sparse coding and dictionary learning, none of them has exploited the advantages of the marginal regression and the lasso simultaneously to provide more efficient and effective solutions. In our work, we provide such an approach with a theoretical support. Therefore, the computational complexity of the proposed method can be two orders faster than that of the lasso with sacrificing the inevitable quantization error. On the other hand, the proposed method is more robust than the conventional marginal regression based methods. We also provide an adaptive regularization parameter selection scheme and a dictionary learning method incorporated with the proposed sparsity estimation algorithm. Experimental results and detailed model analysis are presented to demonstrate the efficacy of our proposed methods."'),
('"Efficient Structure from Motion by Graph Optimization"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_8"', '"We present an efficient structure from motion algorithm that can deal with large image collections in a fraction of time and effort of previous approaches while providing comparable quality of the scene and camera reconstruction. First, we employ fast image indexing using large image vocabularies to measure visual overlap of images without running actual image matching. Then, we select a small subset from the set of input images by computing its approximate minimal connected dominating set by a fast polynomial algorithm. Finally, we use task prioritization to avoid spending too much time in a few difficult matching problems instead of exploring other easier options. Thus we avoid wasting time on image pairs with low chance of success and avoid matching of highly redundant images of landmarks. We present results for several challenging sets of thousands of perspective as well as omnidirectional images."'),
('"Efficiently Learning Random Fields for Stereo Vision with Sparse Message Passing"', '"ECCV 2008"', '["Message Passing", "Stereo Vision", "Stereo Match", "Approximate Inference", "Smoothness Term"]', '"https://doi.org/10.1007/978-3-540-88682-2_47"', '"As richer models for stereo vision are constructed, there is a growing interest in learning model parameters. To estimate parameters in Markov Random Field (MRF) based stereo formulations, one usually needs to perform approximate probabilistic inference. Message passing algorithms based on variational methods and belief propagation are widely used for approximate inference in MRFs. Conditional Random Fields (CRFs) are discriminative versions of traditional MRFs and have recently been applied to the problem of stereo vision. However, CRF parameter training typically requires expensive inference steps for each iteration of optimization. Inference is particularly slow when there are many discrete disparity levels, due to high state space cardinality. We present a novel CRF for stereo matching with an explicit occlusion model and propose sparse message passing to dramatically accelerate the approximate inference needed for parameter optimization. We show that sparse variational message passing iteratively minimizes the KL divergence between the approximation and model distributions by optimizing a lower bound on the partition function. Our experimental results show reductions in inference time of one order of magnitude with no loss in approximation quality. Learning using sparse variational message passing improves results over prior work using graph cuts."'),
('"Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_44"', '"Accurately annotating entities in video is labor intensive and expensive. As the quantity of online video grows, traditional solutions to this task are unable to scale to meet the needs of researchers with limited budgets. Current practice provides a temporary solution by paying dedicated workers to label a fraction of the total frames and otherwise settling for linear interpolation. As budgets and scale require sparser key frames, the assumption of linearity fails and labels become inaccurate. To address this problem we have created a public framework for dividing the work of labeling video data into micro-tasks that can be completed by huge labor pools available through crowdsourced marketplaces. By extracting pixel-based features from manually labeled entities, we are able to leverage more sophisticated interpolation between key frames to maximize performance given a budget. Finally, by validating the power of our framework on difficult, real-world data sets we demonstrate an inherent trade-off between the mix of human and cloud computing used vs. the accuracy and cost of the labeling."'),
('"Egocentric Object Recognition Leveraging the 3D Shape of the Grasping Hand"', '"ECCV 2014"', '["Mobile and wearable systems", "Egocentric and first-person vision", "Activity monitoring systems",', '"https://doi.org/10.1007/978-3-319-16199-0_52"', '"We present a systematic study on the relationship between the 3D shape of a hand that is about to grasp an object and recognition of the object to be grasped. In this paper, we investigate the direction from the shape of the hand to object recognition for unimpaired users. Our work shows that the 3D shape of a grasping hand from an egocentric point of view can help improve recognition of the objects being grasped. Previous work has attempted to exploit hand interactions or gaze information in the egocentric setting to guide object segmentation. However, all such analyses are conducted in 2D. We hypothesize that the 3D shape of a grasping hand is highly correlated to the physical attributes of the object being grasped. Hence, it can provide very beneficial visual information for object recognition. We validate this hypothesis by first building a 3D, egocentric vision pipeline to segment and reconstruct dense 3D point clouds of the grasping hands. Then, visual descriptors are extracted from the point cloud and subsequently fed into an object recognition system to recognize the object being grasped. Our experiments demonstrate that the 3D hand shape can indeed greatly help improve the visual recognition accuracy, when compared with the baseline where only 2D image features are utilized."'),
('"Egomotion Estimation Using Quadruples of Collinear Image Points"', '"ECCV 2000"', '["Optical Flow", "Rotational Component", "Image Motion", "Translational Component", "Scene Structure', '"https://doi.org/10.1007/3-540-45053-X_53"', '"This paper considers a fundamental problem in visual motion perception, namely the problem of egomotion estimation based on visual input. Many of the existing techniques for solving this problem rely on restrictive assumptions regarding the observer\\u2019s motion or even the scene structure. Moreover, they often resort to searching the high dimensional space of possible solutions, a strategy which might be inefficient in terms of computational complexity and exhibit convergence problems if the search is initiated far away from the correct solution. In this work, a novel linear constraint that involves quantities that depend on the egomotion parameters is developed. The constraint is defined in terms of the optical flow vectors pertaining to four collinear image points and is applicable regardless of the egomotion or the scene structure. In addition, it is exact in the sense that no approximations are made for deriving it. Combined with robust linear regression techniques, the constraint enables the recovery of the FOE, thereby decoupling the 3D motion parameters. Extensive simulations as well as experiments with real optical flow fields provide evidence regarding the performance of the proposed method under varying noise levels and camera motions."'),
('"EigenExpress Approach in Recognition of Facial Expression Using GPU"', '"ECCV 2006"', '["Facial Expression", "Graphic Process Unit", "Expression Recognition", "Facial Expression Recogniti', '"https://doi.org/10.1007/11754336_2"', '"The automatic recognition of facial expression presents a significant challenge to the pattern analysis and man-machine interaction research community. In this paper, a novel system is proposed to recognize human facial expressions based on the expression sketch. Firstly, facial expression sketch is extracted by an GPU-based real-time edge detection and sharpening algorithm from original gray image. Then, a statistical method, which is called Eigenexpress, is introduced to obtain the expression feature vectors for sketches. Finally, Modified Hausdorff distance(MHD) was used to perform the expression classification. In contrast to performing feature vector extraction from the gray image directly, the sketch based expression recognition reduces the feature vector\\u2019s dimension first, which leads to a concise representation of the facial expression. Experiment shows our method is appreciable and convincible."'),
('"EigenSegments: A Spatio-Temporal Decomposition of an Ensemble of Images"', '"ECCV 2002"', '["Principal Component Analysis", "Support Vector Machine", "Test Image", "Receiver Operator Characte', '"https://doi.org/10.1007/3-540-47977-5_49"', '"Eigensegments combine image segmentation and Principal Component Analysis (PCA) to obtain a spatio-temporal decomposition of an ensemble of images. The image plane is spatially decomposed into temporally correlated regions. Each region is independently decomposed temporally using PCA. Thus, each image is modeled by several low-dimensional segment-spaces, instead of a single high-dimensional image-space. Experiments show the proposed method gives better classification results, gives smaller reconstruction errors, can handle local changes in appearance and is faster to compute. Results for faces and vehicles are shown."'),
('"Elastic Shape Matching of Parameterized Surfaces Using Square Root Normal Fields"', '"ECCV 2012"', '["Linear Interpolation", "Spherical Surface", "Shape Analysis", "Surface Graph", "Riemannian Metrics', '"https://doi.org/10.1007/978-3-642-33715-4_58"', '"In this paper we define a new methodology for shape analysis of parameterized surfaces, where the main issues are: (1) choice of metric for shape comparisons and (2) invariance to reparameterization. We begin by defining a general elastic metric on the space of parameterized surfaces. The main advantages of this metric are twofold. First, it provides a natural interpretation of elastic shape deformations that are being quantified. Second, this metric is invariant under the action of the reparameterization group. We also introduce a novel representation of surfaces termed square root normal fields or SRNFs. This representation is convenient for shape analysis because, under this representation, a reduced version of the general elastic metric becomes the simple \\\\(\\\\ensuremath{\\\\mathbb{L}^2}\\\\) metric. Thus, this transformation greatly simplifies the implementation of our framework. We validate our approach using multiple shape analysis examples for quadrilateral and spherical surfaces. We also compare the current results with those of Kurtek et al. [1]. We show that the proposed method results in more natural shape matchings, and furthermore, has some theoretical advantages over previous methods."'),
('"Element-Wise Factorization for N-View Projective Reconstruction"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_29"', '"Sturm-Triggs iteration is a standard method for solving the projective factorization problem. Like other iterative algorithms, this method suffers from some common drawbacks such as requiring a good initialization, the iteration may not converge or only converge to a local minimum, etc. None of the published works can offer any sort of global optimality guarantee to the problem. In this paper, an optimal solution to projective factorization for structure and motion is presented, based on the same principle of low-rank factorization. Instead of formulating the problem as matrix factorization, we recast it as element-wise factorization, leading to a convenient and efficient semi-definite program formulation. Our method is thus global, where no initial point is needed, and a globally-optimal solution can be found (up to some relaxation gap). Unlike traditional projective factorization, our method can handle real-world difficult cases like missing data or outliers easily, and all in a unified manner. Extensive experiments on both synthetic and real image data show comparable or superior results compared with existing methods."'),
('"Elevation Angle from Reflectance Monotonicity: Photometric Stereo for General Isotropic Reflectance', '"ECCV 2012"', '["Elevation Angle", "Azimuth Angle", "Lighting Direction", "Photometric Stereo", "Surface Normal Dir', '"https://doi.org/10.1007/978-3-642-33712-3_33"', '"This paper exploits the monotonicity of general isotropic reflectances for estimating elevation angles of surface normal given the azimuth angles. With an assumption that the reflectance includes at least one lobe that is a monotonic function of the angle between the surface normal and half-vector (bisector of lighting and viewing directions), we prove that elevation angles can be uniquely determined when the surface is observed under varying directional lights densely and uniformly distributed over the hemisphere. We evaluate our method by experiments using synthetic and real data to show its wide applicability, even when the assumption does not strictly hold. By combining an existing method for azimuth angle estimation, our method derives complete surface normal estimates for general isotropic reflectances."'),
('"Ellipse Fitting with Hyperaccuracy"', '"ECCV 2006"', '["Thick Solid Line", "Thin Solid Line", "Order Error", "Point Sequence", "True Shape"]', '"https://doi.org/10.1007/11744023_38"', '"For fitting an ellipse to a point sequence, ML (maximum likelihood) has been regarded as having the highest accuracy. In this paper, we demonstrate the existence of a \\u201chyperaccurate\\u201d method which outperforms ML. This is made possible by error analysis of ML followed by subtraction of high-order bias terms. Since ML nearly achieves the theoretical accuracy bound (the KCR lower bound), the resulting improvement is very small. Nevertheless, our analysis has theoretical significance, illuminating the relationship between ML and the KCR lower bound."'),
('"EM Enhancement of 3D Head Pose Estimated by Perspective Invariance"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_18"', '"In this paper, a new approach is proposed for estimating 3D head pose from a monocular image. The approach assumes the more difficult full perspective projection camera model as against most previous approaches that approximate the non-linear perspective projection via linear affine assumption. Perspective-invariance is used to estimate the head pose from a face image. Our approach employs a general prior knowledge of face structure and the corresponding geometrical constraints provided by the location of a certain vanishing point to determine the pose of human faces. To achieve this, eye-lines, formed from the far and near eye corners, and mouth-line of the mouth corners are assumed parallel in 3D space. Then the vanishing point of these parallel lines found by the intersection of the eye-line and mouth-line in the image can be used to infer the 3D orientation and location of the human face. Perspective invariance of cross ratio and harmonic range are used to locate the vanishing point stably. In order to deal with the variance of the facial model parameters, e.g. ratio between the eye-line and the mouth line, an EM framework is applied to update the parameters iteratively. We use the EM strategy to first compute the 3D pose using some initially learned (PCA) parameters, e.g. ratio and length, then update iteratively the parameters for individual persons and their facial expressions till convergence. The EM technique models data uncertainty as Gaussian defined over positions and orientation of facial plane. The resulting weighted parameters estimation problem is solved using the Levenberg-Marquardt method. The robustness analysis of the algorithm with synthetic data and real face images are included."'),
('"EMD-L1: An Efficient and Robust Algorithm for Comparing Histogram-Based Descriptors"', '"ECCV 2006"', '["Image Retrieval", "Interest Point", "Robust Algorithm", "Shape Match", "Shape Context"]', '"https://doi.org/10.1007/11744078_26"', '"We propose a fast algorithm, EMD-L 1, for computing the Earth Mover\\u2019s Distance (EMD) between a pair of histograms. Compared to the original formulation, EMD-L 1 has a largely simplified structure. The number of unknown variables in EMD-L 1 is O(N) that is significantly less than O(N 2) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function is also simplified. We prove that the EMD-L 1 is formally equivalent to the original EMD with L 1 ground distance without approximation. Exploiting the L 1 metric structure, an efficient tree-based algorithm is designed to solve the EMD-L 1 computation. An empirical study demonstrates that the new algorithm has the time complexity of O(N 2), which is much faster than previously reported algorithms with super-cubic complexities. The proposed algorithm thus allows the EMD to be applied for comparing histogram-based features, which is practically impossible with previous algorithms. We conducted experiments for shape recognition and interest point matching. EMD-L 1 is applied to compare shape contexts on the widely tested MPEG7 shape dataset and SIFT image descriptors on a set of images with large deformation, illumination change and heavy noise. The results show that our EMD-L 1-based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks."'),
('"Emotion Mirror: A Novel Intervention for Autism Based on Real-Time Expression Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_79"', '"Facial expression perception and production are crucial for social functioning. Children with autism spectrum disorders (ASD) are impaired in their ability to produce and perceive dynamic facial expressions, which may contribute to social deficits. Here we present a novel intervention system for improving facial expression perception and production in children with ASD based on computer vision. We present a live demo of the Emotion Mirror, a game where the children make facial expressions of basic emotions (anger, disgust, fear, happiness, sadness, and surprise) that are \\u201cmirrored\\u201d by a cartoon character on the screen who responds dynamically in real-time. In the reverse mirror condition, the character makes an expression and children are rewarded when they successfully copy the expression of the character. This application demonstrates a novel intersection of computer vision and medicine enabled by real-time facial expression processing."'),
('"Emotion Recognition from Arbitrary View Facial Images"', '"ECCV 2010"', '["Linear Discriminant Analysis", "Gaussian Mixture Model", "Emotion Recognition", "Facial Expression', '"https://doi.org/10.1007/978-3-642-15567-3_36"', '"Emotion recognition from facial images is a very active research topic in human computer interaction (HCI). However, most of the previous approaches only focus on the frontal or nearly frontal view facial images. In contrast to the frontal/nearly-frontal view images, emotion recognition from non-frontal view or even arbitrary view facial images is much more difficult yet of more practical utility. To handle the emotion recognition problem from arbitrary view facial images, in this paper we propose a novel method based on the regional covariance matrix (RCM) representation of facial images. We also develop a new discriminant analysis theory, aiming at reducing the dimensionality of the facial feature vectors while preserving the most discriminative information, by minimizing an estimated multiclass Bayes error derived under the Gaussian mixture model (GMM). We further propose an efficient algorithm to solve the optimal discriminant vectors of the proposed discriminant analysis method. We render thousands of multi-view 2D facial images from the BU-3DFE database and conduct extensive experiments on the generated database to demonstrate the effectiveness of the proposed method. It is worth noting that our method does not require face alignment or facial landmark points localization, making it very attractive."'),
('"Energy Minimization under Constraints on Label Counts"', '"ECCV 2010"', '["Energy Minimization", "Energy Function", "Grid Graph", "Labelling Problem", "Graph Decomposition"]', '"https://doi.org/10.1007/978-3-642-15552-9_39"', '"Many computer vision problems such as object segmentation or reconstruction can be formulated in terms of labeling a set of pixels or voxels. In certain scenarios, we may know the number of pixels or voxels which can be assigned to a particular label. For instance, in the reconstruction problem, we may know size of the object to be reconstructed. Such label count constraints are extremely powerful and have recently been shown to result in good solutions for many vision problems."'),
('"Energy-Aware Real-Time Face Recognition System on Mobile CPU-GPU Platform"', '"ECCV 2010"', '["Fast Fourier Transform", "Face Recognition", "Graphic Processing Unit", "Gabor Wavelet", "Face Rec', '"https://doi.org/10.1007/978-3-642-35740-4_32"', '"The Graphics Processor Unit (GPU) has expanded its role from an accelerator for rendering graphics into an efficient parallel processor for general purpose computing. The GPU, an indispensable component in desktop and server-class computers as well as game consoles, has also become an integrated component in handheld devices, such as smartphones. Since the handheld devices are mostly powered by battery, the mobile GPU is usually designed with an emphasis on low-power rather than on performance. In addition, the memory bus architecture of mobile devices is also quite different from those of desktops, servers, and game consoles. In this paper, we try to provide answers to the following two questions: (1) Can a mobile GPU be used as a powerful accelerator in the mobile platform for general purpose computing, similar to its role in the desktop and server platforms? (2) What is the role of a mobile GPU in energy-optimized real-time mobile applications? We use face recognition as an application driver which is a compute-intensive task and is a core process for several mobile applications. The experiments of our investigation were performed on an Nvidia Tegra development board which consists of a dual-core ARM Cortex A9 CPU and a Nvidia mobile GPU integrated in a SoC. The experiment results show that, utilizing the mobile GPU can achieve a 4.25x speedup in performance and 3.98x reduction in energy consumption, in comparison with a CPU-only implementation on the same platform."'),
('"Enforcing Temporal Consistency in Real-Time Stereo Estimation"', '"ECCV 2006"', '["Stereo Match", "Graphic Hardware", "Temporal Consistency", "Adjacent Frame", "Match Cost"]', '"https://doi.org/10.1007/11744078_44"', '"Real-time stereo matching has many important applications in areas such as robotic navigation and immersive teleconferencing. When processing stereo sequences most existing real-time stereo algorithms calculate disparity maps for different frames independently without considering temporal consistency between adjacent frames. While it is known that temporal consistency information can help to produce better results, there is no efficient way to enforce temporal consistency in real-time applications."'),
('"Enhancing Interactive Image Segmentation with Automatic Label Set Augmentation"', '"ECCV 2010"', '["Dirichlet Process", "Diffusion Signature", "Average Error Rate", "Label Vertex", "Adaptive Thresho', '"https://doi.org/10.1007/978-3-642-15567-3_42"', '"We address the problem of having insufficient labels in an interactive image segmentation framework, for which most current methods would fail without further user interaction. To minimize user interaction, we use the appearance and boundary information synergistically. Specifically, we perform distribution propagation on the image graph constructed with color features to derive an initial estimate of the segment labels. Following that, we include automatically estimated segment distributions at \\u201ccritical pixels\\u201d with uncertain labels to improve the segmentation performance. Such estimation is realized by incorporating boundary information using a non-parametric Dirichlet process for modeling diffusion signatures derived from the salient boundaries. Our main contribution is fusion of image appearance with probabilistic modeling of boundary information to segment the whole-object with a limited number of labeled pixels. Our proposed framework is extensively tested on a standard dataset, and is shown to achieve promising results both quantitatively and qualitatively."'),
('"Enhancing Particle Filters Using Local Likelihood Sampling"', '"ECCV 2004"', '["Importance Sampling", "Visual Tracking", "License Plate", "Proposal Density", "Test Video Sequence', '"https://doi.org/10.1007/978-3-540-24670-1_2"', '"Particle filters provide a means to track the state of an object even when the dynamics and the observations are non-linear/non-Gaussian. However, they can be very inefficient when the observation noise is low as compared to the system noise, as it is often the case in visual tracking applications. In this paper we propose a new two-stage sampling procedure to boost the performance of particle filters under this condition. We provide conditions under which the new procedure is proven to reduce the variance of the weights. Synthetic and real-world visual tracking experiments are used to confirm the validity of the theoretical analysis."'),
('"Enhancing Semantic Features with Compositional Analysis for Scene Recognition"', '"ECCV 2012"', '["Semantic Feature", "Compositional Feature", "Scene Categorization", "Outdoor Scene", "Scene Recogn', '"https://doi.org/10.1007/978-3-642-33885-4_45"', '"Scene recognition systems are generally based on features that represent the image semantics by modeling the content depicted in a given image. In this paper we propose a framework for scene recognition that goes beyond the mere visual content analysis by exploiting a new cue for categorization: the image composition, namely its photographic style and layout. We extract information about the image composition by storing the values of affective, aesthetic and artistic features in a compositional vector. We verify the discriminative power of our compositional vector for scene categorization by using it for the classification of images from various, diverse, large scale scene understanding datasets. We then combine the compositional features with traditional semantic features in a complete scene recognition framework. Results show that, due to the complementarity of compositional and semantic features, scene categorization systems indeed benefit from the incorporation of descriptors representing the image photographic layout (+ 13-15% over semantic-only categorization)."'),
('"Enhancing the Point Feature Tracker by Adaptive Modelling of the Feature Support"', '"ECCV 2006"', '["Median Absolute Deviation", "Adaptive Modelling", "Visual Servoing", "Error Image", "Feature Suppo', '"https://doi.org/10.1007/11744047_9"', '"We consider the problem of tracking a given set of point features over large sequences of image frames. A classic procedure for monitoring the tracking quality consists in requiring that the current features nicely warp towards their reference appearances. The procedure recommends focusing on features projected from planar 3D patches (planar features), by enforcing a conservative threshold on the residual of the difference between the warped current feature and the reference. However, in some important contexts, there are many features for which the planarity assumption is only partially satisfied, while the true planar features are not so abundant. This is especially true when the motion of the camera is mainly translational and parallel to the optical axis (such as when driving a car along straight sections of the road), which induces a permanent increase of the apparent feature size. Tracking features containing occluding boundaries then becomes an interesting goal, for which we propose a multi-scale monitoring solution striving to maximize the lifetime of the feature, while also detecting the tracking failures. The devised technique infers the parts of the reference which are not projected from the same 3D surface as the patch which has been consistently tracked until the present moment. The experiments on real sequences taken from cars driving through urban environments show that the technique is effective in increasing the average feature lifetimes, especially in sequences with occlusions and large photometric variations."'),
('"Ensemble Partitioning for Unsupervised Image Categorization"', '"ECCV 2012"', '["Random Forest", "Spectral Cluster", "Killer Whale", "Ensemble Learning", "Weak Learner"]', '"https://doi.org/10.1007/978-3-642-33712-3_35"', '"While the quality of object recognition systems can strongly benefit from more data, human annotation and labeling can hardly keep pace. This motivates the usage of autonomous and unsupervised learning methods. In this paper, we present a simple, yet effective method for unsupervised image categorization, which relies on discriminative learners. Since automatically obtaining error-free labeled training data for the learners is infeasible, we propose the concept of weak training (WT) set. WT sets have various deficiencies, but still carry useful information. Training on a single WT set cannot result in good performance, thus we design a random walk sampling scheme to create a series of diverse WT sets. This naturally allows our categorization learning to leverage ensemble learning techniques. In particular, for each WT set, we train a max-margin classifier to further partition the whole dataset to be categorized. By doing so, each WT set leads to a base partitioning of the dataset and all the base partitionings are combined into an ensemble proximity matrix. The final categorization is completed by feeding this proximity matrix into a spectral clustering algorithm. Experiments on a variety of challenging datasets show that our method outperforms competing methods by a considerable margin."'),
('"Error-Tolerant Image Compositing"', '"ECCV 2010"', '["Poisson Equation", "Texture Region", "Visual Masking", "Sparse Linear System", "Shadow Removal"]', '"https://doi.org/10.1007/978-3-642-15549-9_3"', '"Gradient-domain compositing is an essential tool in computer vision and its applications, e.g., seamless cloning, panorama stitching, shadow removal, scene completion and reshuffling. While easy to implement, these gradient-domain techniques often generate bleeding artifacts where the composited image regions do not match. One option is to modify the region boundary to minimize such mismatches. However, this option may not always be sufficient or applicable, e.g., the user or algorithm may not allow the selection to be altered. We propose a new approach to gradient-domain compositing that is robust to inaccuracies and prevents color bleeding without changing the boundary location. Our approach improves standard gradient-domain compositing in two ways. First, we define the boundary gradients such that the produced gradient field is nearly integrable. Second, we control the integration process to concentrate residuals where they are less conspicuous. We show that our approach can be formulated as a standard least-squares problem that can be solved with a sparse linear system akin to the classical Poisson equation. We demonstrate results on a variety of scenes. The visual quality and run-time complexity compares favorably to other approaches."'),
('"Estimating 3D Face Model and Facial Deformation from a Single Image Based on Expression Manifold Op', '"ECCV 2008"', '["3D Modeling", "Facial Expression", "Manifold"]', '"https://doi.org/10.1007/978-3-540-88682-2_45"', '"Facial expression modeling is central to facial expression recognition and expression synthesis for facial animation. Previous works reported that modeling the facial expression with low-dimensional manifold is more appropriate than using a linear subspace. In this paper, we propose a manifold-based 3D face reconstruction approach to estimating the 3D face model and the associated expression deformation from a single face image. In the training phase, we build a nonlinear 3D expression manifold from a large set of 3D facial expression models to represent the facial shape deformations due to facial expressions. Then a Gaussian mixture model in this manifold is learned to represent the distribution of expression deformation. By combining the merits of morphable neutral face model and the low-dimensional expression manifold, we propose a new algorithm to reconstruct the 3D face geometry as well as the 3D shape deformation from a single face image with expression in an energy minimization framework. Experimental results on CMU-PIE image database and FG-Net video database are shown to validate the effectiveness and accuracy of the proposed algorithm."'),
('"Estimating 3D Trajectories of Periodic Motions from Stationary Monocular Views"', '"ECCV 2008"', '["Periodic Motion", "Image Sample", "Periodic Trajectory", "Local Optimization Method", "Ground Trut', '"https://doi.org/10.1007/978-3-540-88690-7_41"', '"We present a method for estimating the 3D trajectory of an object undergoing periodic motion in world coordinates by observing its apparent trajectory in a video taken from a single stationary camera. Periodicity in 3D is used here as a physical constraint, from which accurate solutions can be obtained. A detailed analysis is performed, from which we gain significant insight regarding the nature of the problem and the information that is required to arrive at a unique solution. Subsequently, a robust, numerical approach is proposed, and it is demonstrated that the cost function exhibits strong local convexity which is amenable to local optimization methods. Experimental results indicate the effectiveness of the proposed method for reconstructing periodic trajectories in 3D."'),
('"Estimating Gaze Direction from Low-Resolution Faces in Video"', '"ECCV 2006"', '["Skin Detection", "British Machine Vision", "Skin Pixel", "Body Direction", "Head Pose"]', '"https://doi.org/10.1007/11744047_31"', '"In this paper we describe a new method for automatically estimating where a person is looking in images where the head is typically in the range 20 to 40 pixels high. We use a feature vector based on skin detection to estimate the orientation of the head, which is discretised into 8 different orientations, relative to the camera. A fast sampling method returns a distribution over previously-seen head-poses. The overall body pose relative to the camera frame is approximated using the velocity of the body, obtained via automatically-initiated colour-based tracking in the image sequence. We show that, by combining direction and head-pose information gaze is determined more robustly than using each feature alone. We demonstrate this technique on surveillance and sports footage."'),
('"Estimating Geo-temporal Location of Stationary Cameras Using Shadow Trajectories"', '"ECCV 2008"', '["Ground Plane", "Camera Calibration", "Stationary Camera", "Horizon Line", "Declination Angle"]', '"https://doi.org/10.1007/978-3-540-88682-2_25"', '"Using only shadow trajectories of stationary objects in a scene, we demonstrate that using a set of six or more photographs are sufficient to accurately calibrate the camera. Moreover, we present a novel application where, using only three points from the shadow trajectory of the objects, one can accurately determine the geo-location of the camera, up to a longitude ambiguity, and also the date of image acquisition without using any GPS or other special instruments. We refer to this as \\u201cgeo-temporal localization\\u201d. We consider possible cases where ambiguities can be removed if additional information is available. Our method does not require any knowledge of the date or the time when the pictures are taken, and geo-temporal information is recovered directly from the images. We demonstrate the accuracy of our technique for both steps of calibration and geo-temporal localization using synthetic and real data."'),
('"Estimating Human Body Configurations Using Shape Context Matching"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47977-5_44"', '"The problem we consider in this paper is to take a single two-dimensional image containing a human body, locate the joint positions, and use these to estimate the body configuration and pose in three-dimensional space. The basic approach is to store a number of exemplar 2D views of the human body in a variety of different configurations and viewpoints with respect to the camera. On each of these stored views, the locations of the body joints (left elbow, right knee, etc.) are manually marked and labelled for future use. The test shape is then matched to each stored view, using the technique of shape context matching in conjunction with a kinematic chain-based deformation model. Assuming that there is a stored view sufficiently similar in configuration and pose, the correspondence process will succeed. The locations of the body joints are then transferred from the exemplar view to the test shape. Given the joint locations, the 3D body configuration and pose are then estimated. We can apply this technique to video by treating each frame independently - tracking just becomes repeated recognition! We present results on a variety of datasets."'),
('"Estimating Intrinsic Images from Image Sequences with Biased Illumination"', '"ECCV 2004"', '["Adjacent Pixel", "Cast Shadow", "Illumination Direction", "Smoothness Constraint", "Photometric St', '"https://doi.org/10.1007/978-3-540-24671-8_22"', '"We present a method for estimating intrinsic images from a fixed-viewpoint image sequence captured under changing illumination directions. Previous work on this problem reduces the influence of shadows on reflectance images, but does not address shading effects which can significantly degrade reflectance image estimation under the typically biased sampling of illumination directions. In this paper, we describe how biased illumination sampling leads to biased estimates of reflectance image derivatives. To avoid the effects of illumination bias, we propose a solution that explicitly models spatial and temporal constraints over the image sequence. With this constraint network, our technique minimizes a regularization function that takes advantage of the biased image derivatives to yield reflectance images less influenced by shading."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Estimating Radiometric Response Functions from Image Noise Variance"', '"ECCV 2008"', '["Response Function", "Noise Variance", "Shot Noise", "Noise Distribution", "Radiometric Calibration', '"https://doi.org/10.1007/978-3-540-88693-8_46"', '"We propose a method for estimating radiometric response functions from observation of image noise variance, not profile of its distribution. The relationship between radiance intensity and noise variance is affine, but due to the non-linearity of response functions, this affinity is not maintained in the observation domain. We use the non-affinity relationship between the observed intensity and noise variance to estimate radiometric response functions. In addition, we theoretically derive how the response function alters the intensity-variance relationship. Since our method uses noise variance as input, it is fundamentally robust against noise. Unlike prior approaches, our method does not require images taken with different and known exposures. Real-world experiments demonstrate the effectiveness of our method."'),
('"Estimating Shadows with the Bright Channel Cue"', '"ECCV 2010"', '["Machine Intelligence", "Markov Random Field", "Color Channel", "Cast Shadow", "Shadow Detection"]', '"https://doi.org/10.1007/978-3-642-35740-4_1"', '"In this paper, we introduce a simple but efficient cue for the extraction of shadows from a single color image, the bright channel cue. We discuss its limitations and offer two methods to refine the bright channel: by computing confidence values for the cast shadows, based on a shadow-dependent feature, such as hue; and by combining the bright channel with illumination invariant representations of the original image in a flexible way using an MRF model. We present qualitative and quantitative results for shadow detection, as well as results in illumination estimation from shadows. Our results show that our method achieves satisfying results despite the simplicity of the approach."'),
('"Estimating Surface Normals from Spherical Stokes Reflectance Fields"', '"ECCV 2012"', '["Incident Lighting", "Circular Polarization", "Stokes Parameter", "Photometric Stereo", "Stokes Vec', '"https://doi.org/10.1007/978-3-642-33868-7_34"', '"In this paper we introduce a novel technique for estimating surface normals from the four Stokes polarization parameters of specularly reflected light under a single spherical incident lighting condition that is either unpolarized or circularly polarized. We illustrate the practicality of our technique by estimating surface normals under uncontrolled outdoor illumination from just four observations from a fixed viewpoint."'),
('"Estimating the Jacobian of the Singular Value Decomposition: Theory and Applications"', '"ECCV 2000"', '["Singular Value Decomposition", "Machine Intelligence", "Image Watermark", "Fundamental Matrix", "I', '"https://doi.org/10.1007/3-540-45054-8_36"', '"The Singular Value Decomposition (SVD) of a matrix is a linear algebra tool that has been successfully applied to a wide variety of domains. The present paper is concerned with the problem of estimating the Jacobian of the SVD components of a matrix with respect to the matrix itself. An exact analytic technique is developed that facilitates the estimation of the Jacobian using calculations based on simple linear algebra. Knowledge of the Jacobian of the SVD is very useful in certain applications involving multivariate regression or the computation of the uncertainty related to estimates obtained through the SVD. The usefulness and generality of the proposed technique is demonstrated by applying it to the estimation of the uncertainty for three different vision problems, namely self-calibration, epipole computation and rigid motion estimation."'),
('"Estimating the Pose of a 3D Sensor in a Non-rigid Environment"', '"WDV 2006"', '["Residual Error", "Shape Model", "Basis Shape", "Reprojection Error", "Shape Vector"]', '"https://doi.org/10.1007/978-3-540-70932-9_19"', '"Estimating the pose of an imaging sensor is a central research problem. Many solutions have been proposed for the case of a rigid environment. In contrast, we tackle the case of a non-rigid environment observed by a 3D sensor, which has been neglected in the literature. We represent the environment as sets of time-varying 3D points explained by a low-rank shape model, that we derive in its implicit and explicit forms. The parameters of this model are learnt from data gathered by the 3D sensor. We propose a learning algorithm based on minimal 3D non-rigid tensors that we introduce. This is followed by a Maximum Likelihood nonlinear refinement performed in a bundle adjustment manner. Given the learnt environment model, we compute the pose of the 3D sensor, as well as the deformations of the environment, that is, the non-rigid counterpart of pose, from new sets of 3D points. We validate our environment learning and pose estimation modules on simulated and real data."'),
('"Estimation of 3D Object Structure, Motion and Rotation Based on 4D Affine Optical Flow Using a Mult', '"ECCV 2010"', '["Motion Estimate", "Surface Patch", "Rotational Model", "Translational Model", "Total Little Square', '"https://doi.org/10.1007/978-3-642-15561-1_43"', '"In this paper we extend a standard affine optical flow model to 4D and present how affine parameters can be used for estimation of 3D object structure, 3D motion and rotation using a 1D camera grid. Local changes of the projected motion vector field are modelled not only on the image plane as usual for affine optical flow, but also in camera displacement direction, and in time. We identify all parameters of this 4D fully affine model with terms depending on scene structure, scene motion, and camera displacement. We model the scene by planar, translating, and rotating surface patches and project them with a pinhole camera grid model. Imaged intensities of the projected surface points are then modelled by a brightness change model handling illumination changes. Experiments demonstrate the accuracy of the new model. It outperforms not only 2D affine optical flow models but range flow for varying illumination. Moreover we are able to estimate surface normals and rotation parameters. Experiments on real data of a plant physiology experiment confirm the applicability of our model."'),
('"Estimation of Illuminant Direction and Intensity of Multiple Light Sources"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47979-1_14"', '"This paper presents a novel scheme for locating multiple light sources and estimating their intensities from a pair of stereo images of a sphere. No prior knowledge of the location and radius of the sphere is necessary. The sphere surface is not assumed to be a pure Lambertian surface, instead, it has both Lambertian and specular properties. The light source locating algorithm is based on the fact that the Lambertian intensity is not dependent on the direction of view point, while the specular intensity is highly dependent on the direction of the view point. From this fact, we can use a pair of stereo images whose view point changes can be utilized to separate the image of the sphere into two images, one with Lambertian intensities, and the other with specular intensities. The specular image is used to find the directions of the light sources, and then Lambertian image model is used to find the intensities of the light sources. Experiments on both synthetic and real images show that the scheme is successful and robust in finding the directions of the light sources accurately with accurate intensity estimation."'),
('"Estimation of Intrinsic Image Sequences from Image+Depth Video"', '"ECCV 2012"', '["Surface Point", "Temporal Constraint", "Adjacent Pixel", "Lighting Direction", "Baseline Algorithm', '"https://doi.org/10.1007/978-3-642-33783-3_24"', '"We present a technique for estimating intrinsic images from image+depth video, such as that acquired from a Kinect camera. Intrinsic image decomposition in this context has importance in applications like object modeling, in which surface colors need to be recovered without illumination effects. The proposed method is based on two new types of decomposition constraints derived from the multiple viewpoints and reconstructed 3D scene geometry of the video data. The first type provides shading constraints that enforce relationships among the shading components of different surface points according to their similarity in surface orientation. The second type imposes temporal constraints that favor consistency in the intrinsic color of a surface point seen in different video frames, which improves decomposition in cases of view-dependent non-Lambertian reflections. Local and non-local variants of the two constraints are employed in a manner complementary to local and non-local reflectance constraints used in previous works. Together they are formulated within a linear system that allows for efficient optimization. Experimental results demonstrate that each of the new constraints appreciably elevates the quality of intrinsic image estimation, and that they jointly yield decompositions that compare favorably to current techniques."'),
('"Estimation of Multiple Illuminants from a Single Image of Arbitrary Known Geometry"', '"ECCV 2002"', '["Critical Boundary", "Sphere Image", "Angle Threshold", "Directional Light Source", "Consecutive Wi', '"https://doi.org/10.1007/3-540-47977-5_18"', '"We present a new method for the detection and estimation of multiple illuminants, using one image of any object with known geometry and Lambertian reflectance. Our method obviates the need to modify the imaged scene by inserting calibration objects of any particular geometry, relying instead on partial knowledge of the geometry of the scene. Thus, the recovered multiple illuminants can be used both for image-based rendering and for shape reconstruction. We first develop our method for the case of a sphere with known size, illuminated by a set of directional light sources. In general, each point of such a sphere will be illuminated by a subset of these sources. We propose a novel, robust way to segment the surface into regions, with each region illuminated by a different set of sources. The regions are separated by boundaries consisting of critical points (points where one illuminant is perpendicular to the normal). Our region-based recursive least-squares method is impervious to noise and missing data and significantly outperforms a previous boundary-based method using spheres[21]. This robustness to missing data is crucial to extending the method to surfaces of arbitrary smooth geometry, other than spheres. We map the normals of the arbitrary shape onto a sphere, which we can then segment, even when only a subset of the normals is available on the scene. We demonstrate experimentally the accuracy of our method, both in detecting the number of light sources and in estimating their directions, by testing on images of a variety of synthetic and real objects."'),
('"Estimation of Multiple Periodic Motions from Video"', '"ECCV 2006"', '["Video Sequence", "Motion Estimation", "Periodic Motion", "Real Sequence", "Period Estimate"]', '"https://doi.org/10.1007/11744023_12"', '"The analysis of periodic or repetitive motions is useful in many applications, both in the natural and the man-made world. An important example is the recognition of human and animal activities. Existing methods for the analysis of periodic motions first extract motion trajectories, e.g. via correlation, or feature point matching. We present a new approach, which takes advantage of both the frequency and spatial information of the video. The 2D spatial Fourier transform is applied to each frame, and time-frequency distributions are then used to estimate the time-varying object motions. Thus, multiple periodic trajectories are extracted and their periods are estimated. The period information is finally used to segment the periodically moving objects. Unlike existing methods, our approach estimates multiple periodicities simultaneously, it is robust to deviations from strictly periodic motion, and estimates periodicities superposed on translations. Experiments with synthetic and real sequences display the capabilities and limitations of this approach. Supplementary material is provided, showing the video sequences used in the experiments."'),
('"Euclidean Group Invariant Computation of Stochastic Completion Fields Using Shiftable-Twistable Fun', '"ECCV 2000"', '["Visual Cortex", "Primary Visual Cortex", "Simple Cell", "Illusory Contour", "Occlude Object"]', '"https://doi.org/10.1007/3-540-45053-X_7"', '"We describe a method for computing the likelihood that a completion joining two contour fragments passes through any given position and orientation in the image plane, that is, a method for completing the boundaries of partially occluded objects. Like computations in primary visual cortex (and unlike all previous models of contour completion in the human visual system), our computation is Euclidean invariant. This invariance is achieved in a biologically plausible manner by representing the input, output, and intermediate states of the computation in a basis of shiftable-twistable functions. The spatial components of these functions resemble the receptive fields of simple cells in primary visual cortex. Shiftable-twistable functions on the space of positions and directions are a generalization of shiftable-steerable functions on the plane."'),
('"Euclidean Structure from N \\u2265 2 Parallel Circles: Theory and Algorithms"', '"ECCV 2006"', '["Camera Calibration", "Absolute Signature", "Euclidean Structure", "Orthogonal Line", "Radical Axis', '"https://doi.org/10.1007/11744023_19"', '"Our problem is that of recovering, in one view, the 2D Euclidean structure, induced by the projections of N parallel circles. This structure is a prerequisite for camera calibration and pose computation. Until now, no general method has been described for N > 2. The main contribution of this work is to state the problem in terms of a system of linear equations to solve. We give a closed-form solution as well as bundle adjustment-like refinements, increasing the technical applicability and numerical stability. Our theoretical approach generalizes and extends all those described in existing works for N = 2 in several respects, as we can treat simultaneously pairs of orthogonal lines and pairs of circles within a unified framework. The proposed algorithm may be easily implemented, using well-known numerical algorithms. Its performance is illustrated by simulations and experiments with real images."'),
('"Euclidean Structure Recovery from Motion in Perspective Image Sequences via Hankel Rank Minimizatio', '"ECCV 2010"', '["Ground Truth Data", "Perspective Projection", "Bundle Adjustment", "Hankel Matrix", "Linear Time I', '"https://doi.org/10.1007/978-3-642-15552-9_6"', '"In this paper we consider the problem of recovering 3D Euclidean structure from multi-frame point correspondence data in image sequences under perspective projection. Existing approaches rely either only on geometrical constraints reflecting the rigid nature of the object, or exploit temporal information by recasting the problem into a nonlinear filtering form. In contrast, here we introduce a new constraint that implicitly exploits the temporal ordering of the frames, leading to a provably correct algorithm to find Euclidean structure (up to a single scaling factor) without the need to alternate between projective depth and motion estimation, estimate the Fundamental matrices or assume a camera motion model. Finally, the proposed approach does not require an accurate calibration of the camera. The accuracy of the algorithm is illustrated using several examples involving both synthetic and real data."'),
('"Evaluating Image Segmentation Algorithms Using the Pareto Front"', '"ECCV 2002"', '["Vision systems engineering & evaluation", "Image segmentation", "Multiobjective evaluation", "Pare', '"https://doi.org/10.1007/3-540-47979-1_3"', '"Image segmentation is the first stage of processing in many practical computer vision systems. While development of particular segmentation algorithms has attracted considerable research interest, relatively little work has been published on the subject of their evaluation. In this paper we propose the use of the Pareto front to allow evaluation and comparison of image segmentation algorithms in multi-dimensional fitness spaces, in a manner somewhat analogous to the use of receiver operating characteristic curves in binary classification problems. The principle advantage of this approach is that it avoids the need to aggregate metrics capturing multiple objectives into a single metric, and thus allows trade-offs between multiple aspects of algorithm behavior to be assessed. This is in contrast to previous approaches which have tended to use a single measure of \\u201cgoodness\\u201d, or discrepancy to ground truth data. We define the Pareto front in the context of algorithm evaluation, propose several fitness measures for image segmentation, and use a genetic algorithm for multi-objective optimization to explore the set of algorithms, parameters, and corresponding points in fitness space which lie on the front. Experimental results are presented for six general-purpose image segmentation algorithms, including several which may be considered state-of-the-art."'),
('"Evaluation and Selection of Models for Motion Segmentation"', '"ECCV 2002"', '["Greedy Algorithm", "Real Image", "Space Constraint", "Space Separation", "World Coordinate System"', '"https://doi.org/10.1007/3-540-47977-5_22"', '"We first present an improvement of the subspace separation for motion segmentation by newly introducing the affine space constraint. We point out that this improvement does not always fare well due to the effective noise it introduces. In order to judge which solution to adopt if different segmentations are obtained, we test two measures using real images: the standard F test, and the geometric model selection criteria."'),
('"Evaluation of Digital Inpainting Quality in the Context of Artwork Restoration"', '"ECCV 2012"', '["Human Visual System", "Mean Opinion Score", "Psychophysical Experiment", "Image Inpainting", "Perc', '"https://doi.org/10.1007/978-3-642-33863-2_58"', '"Improved digital image inpainting algorithms could provide substantial support for future artwork restoration. However, currently, there is an acknowledged lack of quantitative metrics for image inpainting evaluation. In this paper the performance of eight inpainting algorithms is first evaluated by means of a psychophysical experiment. The ranking of the algorithms thus obtained confirms that exemplar based methods generally outperform PDE based methods. Two novel inpainting quality metrics, proposed in this paper, eight general image quality metrics and four inpainting-specific metrics are then evaluated by validation against the perceptual data. Results show that no metric can adequately predict inpainting quality over the entire image database, and that the performance of the metrics is image-dependent."'),
('"Evaluation of Image Fusion Performance with Visible Differences"', '"ECCV 2004"', '["Input Image", "Image Fusion", "Fusion Evaluation", "Visible Difference", "Fusion Algorithm"]', '"https://doi.org/10.1007/978-3-540-24672-5_30"', '"Multisensor signal-level image fusion has attracted considerable research attention recently. Whereas it is relatively straightforward to obtain a fused image, e.g. a simple but crude method is to average the input signals, assessing the performance of fusion algorithms is much harder in practice. This is particularly true in widespread \\u201cfusion for display\\u201d applications where multisensor images are fused and the resulting image is presented to a human operator. As recent studies have shown, the most direct and reliable image fusion evaluation method, subjective tests with a representative sample of potential users are expensive in terms of both time/effort and equipment required. This paper presents an investigation into the application of the Visible signal Differences Prediction modelling, to the objective evaluation of the performance of fusion algorithms. Thus given a pair of input images and a resulting fused image, the Visual Difference Prediction process evaluates the probability that a signal difference between each of the inputs and the fused image can be detected by the human visual system. The resulting probability maps are used to form objective fusion performance metrics and are also integrated with more complex fusion performance measures. Experimental results indicate that the inclusion of visible differences information in fusion assessment yields metrics whose accuracy, with reference to subjective results, is superior to that obtained from the state of the art objective fusion performance measures."'),
('"Evaluation of Image Segmentation Quality by Adaptive Ground Truth Composition"', '"ECCV 2012"', '["Image segmentation evaluation", "ground truths", "image segmentation"]', '"https://doi.org/10.1007/978-3-642-33712-3_21"', '"Segmenting an image is an important step in many computer vision applications. However, image segmentation evaluation is far from being well-studied in contrast to the extensive studies on image segmentation algorithms. In this paper, we propose a framework to quantitatively evaluate the quality of a given segmentation with multiple ground truth segmentations. Instead of comparing directly the given segmentation to the ground truths, we assume that if a segmentation is \\u201cgood\\u201d, it can be constructed by pieces of the ground truth segmentations. Then for a given segmentation, we construct adaptively a new ground truth which can be locally matched to the segmentation as much as possible and preserve the structural consistency in the ground truths. The quality of the segmentation can then be evaluated by measuring its distance to the adaptively composite ground truth. To the best of our knowledge, this is the first work that provides a framework to adaptively combine multiple ground truths for quantitative segmentation evaluation. Experiments are conducted on the benchmark Berkeley segmentation database, and the results show that the proposed method can faithfully reflect the perceptual qualities of segmentations."'),
('"Evaluation of Robust Fitting Based Detection"', '"ECCV 2004"', '["Covariance Matrix", "Noise Model", "Noise Distribution", "Asymptotic Covariance Matrix", "Referenc', '"https://doi.org/10.1007/978-3-540-24671-8_27"', '"Low-level image processing algorithms generally provide noisy features that are far from being Gaussian. Medium-level tasks such as object detection must therefore be robust to outliers. This can be achieved by means of the well-known M-estimators. However, higher-level systems do not only need robust detection, but also a confidence value associated to the detection. When the detection is cast into the fitting framework, the inverse of the covariance matrix of the fit provides a valuable confidence matrix."'),
('"Evaluation of Texture Descriptors for Automated Gender Estimation from Fingerprints"', '"ECCV 2014"', '["Soft biometrics", "Fingerprints", "Gender estimation", "LBP", "LPQ", "BSIF", "LTP"]', '"https://doi.org/10.1007/978-3-319-16181-5_58"', '"Gender is an important demographic attribute. In the context of biometrics, gender information can be used to index databases or enhance the recognition accuracy of primary biometric traits. A number of studies have demonstrated that gender can be automatically deduced from face images. However, few studies have explored the possibility of automatically estimating gender information from fingerprint images. Consequently, there is a limited understanding in this topic. Fingerprint being a widely adopted biometrics, gender cues from the fingerprint image will significantly aid in commercial applications and forensic investigations. This study explores the use of classical texture descriptors - Local Binary Pattern (LBP), Local Phase Quantization (LPQ), Binarized Statistical Image Features (BSIF) and Local Ternary Pattern (LTP) - to estimate gender from fingerprint images. The robustness of these descriptors to various types of image degradations is evaluated. Experiments conducted on the WVU fingerprint dataset suggest the efficacy of LBP descriptor in encoding gender information from good quality fingerprints. The BSIF descriptor is observed to be robust to partial fingerprints, while LPQ is observed to work well on blurred fingerprints. However, the gender estimation accuracy in the case of fingerprints is much lower than that of face, thereby suggesting that more work is necessary on this topic."'),
('"Event Modeling and Recognition Using Markov Logic Networks"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_45"', '"We address the problem of visual event recognition in surveillance where noise and missing observations are serious problems. Common sense domain knowledge is exploited to overcome them. The knowledge is represented as first-order logic production rules with associated weights to indicate their confidence. These rules are used in combination with a relaxed deduction algorithm to construct a network of grounded atoms, the Markov Logic Network. The network is used to perform probabilistic inference for input queries about events of interest. The system\\u2019s performance is demonstrated on a number of videos from a parking lot domain that contains complex interactions of people and vehicles."'),
('"Events Detection Using a Video-Surveillance Ontology and a Rule-Based Approach"', '"ECCV 2014"', '["Ontology", "Video surveillance", "Blobs", "Rules"]', '"https://doi.org/10.1007/978-3-319-16181-5_21"', '"In this paper, we propose the use of a Video-surveillance Ontology and a rule-based approach to detect an event. The scene is described using the concepts presented in the ontology. Then, the blobs are extracted from the video stream and are represented using the bounding boxes that enclose them. Finally, a set of rules have been proposed and have been applied to videos selected from PETS 2012 challenge that contain multiple objects events (e.g. Group walking, Group splitting, etc.)."'),
('"Every Picture Tells a Story: Generating Sentences from Images"', '"ECCV 2010"', '["Machine Translation", "Head Noun", "Node Feature", "Generate Sentence", "Edge Potential"]', '"https://doi.org/10.1007/978-3-642-15561-1_2"', '"Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche."'),
('"Exact Acceleration of Linear Object Detectors"', '"ECCV 2012"', '["linear object detection", "part-based models"]', '"https://doi.org/10.1007/978-3-642-33712-3_22"', '"We describe a general and exact method to considerably speed up linear object detection systems operating in a sliding, multi-scale window fashion, such as the individual part detectors of part-based models. The main bottleneck of many of those systems is the computational cost of the convolutions between the multiple rescalings of the image to process, and the linear filters. We make use of properties of the Fourier transform and of clever implementation strategies to obtain a speedup factor proportional to the filters\\u2019 sizes. The gain in performance is demonstrated on the well known Pascal VOC benchmark, where we accelerate the speed of said convolutions by an order of magnitude."'),
('"Example Based Non-rigid Shape Detection"', '"ECCV 2006"', '["Convolutional Neural Network", "Shape Space", "Active Contour Model", "Weak Learner", "Target Shap', '"https://doi.org/10.1007/11744085_33"', '"Since it is hard to handcraft the prior knowledge in a shape detection framework, machine learning methods are preferred to exploit the expert annotation of the target shape in a database. In the previous approaches [1,2] , an optimal similarity transformation is exhaustively searched for to maximize the response of a trained classification model. At best, these approaches only give a rough estimate of the position of a non-rigid shape. In this paper, we propose a novel machine learning based approach to achieve a refined shape detection result. We train a model that has the largest response on a reference shape and a smaller response on other shapes. During shape detection, we search for an optimal non-rigid deformation to maximize the response of the trained model on the deformed image block. Since exhaustive searching is inapplicable for a non-rigid deformation space with a high dimension, currently, example based searching is used instead. Experiments on two applications, left ventricle endocardial border detection and facial feature detection, demonstrate the robustness of our approach. It outperforms the well-known ASM and AAM approaches on challenging samples."'),
('"Example-Based Stereo with General BRDFs"', '"ECCV 2004"', '["Target Object", "Reference Object", "Observation Vector", "Cast Shadow", "Visual Hull"]', '"https://doi.org/10.1007/978-3-540-24671-8_36"', '"This paper presents an algorithm for voxel-based reconstruction of objects with general reflectance properties from multiple calibrated views. It is assumed that one or more reference objects with known geometry are imaged under the same lighting and camera conditions as the object being reconstructed. The unknown object is reconstructed using a radiance basis inferred from the reference objects. Each view may have arbitrary, unknown distant lighting. If the lighting is calibrated, our model also takes into account shadows that the object casts upon itself. To our knowledge, this is the first stereo method to handle general, unknown, spatially-varying BRDFs under possibly varying, distant lighting, and shadows. We demonstrate our algorithm by recovering geometry and surface normals for objects with both uniform and spatially-varying BRDFs. The normals reveal fine-scale surface detail, allowing much richer renderings than the voxel geometry alone."'),
('"Exemplar-Based Face Recognition from Video"', '"ECCV 2002"', '["Surveillance", "Video-based Face Recognition", "Exemplar-based Learning"]', '"https://doi.org/10.1007/3-540-47979-1_49"', '"A new exemplar-based probabilistic approach for face recognition in video sequences is presented. The approach has two stages: First, Exemplars, which are selected representatives from the raw video, are automatically extracted from gallery videos. The exemplars are used to summarize the gallery video information. In the second part, exemplars are then used as centers for probabilistic mixture distributions for the tracking and recognition process. A particle method is used to compute the posteriori probabilities. Probabilistic methods are attractive in this context as they allow a systematic handling of uncertainty and an elegant way for fusing temporal information."'),
('"Expanding the Family of Grassmannian Kernels: An Embedding Perspective"', '"ECCV 2014"', '["Grassmann manifolds", "kernel methods", "Pl\\u00fccker embedding"]', '"https://doi.org/10.1007/978-3-319-10584-0_27"', '"Modeling videos and image-sets as linear subspaces has proven beneficial for many visual recognition tasks. However, it also incurs challenges arising from the fact that linear subspaces do not obey Euclidean geometry, but lie on a special type of Riemannian manifolds known as Grassmannian. To leverage the techniques developed for Euclidean spaces (e.g., support vector machines) with subspaces, several recent studies have proposed to embed the Grassmannian into a Hilbert space by making use of a positive definite kernel. Unfortunately, only two Grassmannian kernels are known, none of which -as we will show- is universal, which limits their ability to approximate a target function arbitrarily well. Here, we introduce several positive definite Grassmannian kernels, including universal ones, and demonstrate their superiority over previously-known kernels in various tasks, such as classification, clustering, sparse coding and hashing."'),
('"Explicit Performance Metric Optimization for Fusion-Based Video Retrieval"', '"ECCV 2012"', '["Performance Metrics", "Video Retrieval", "Audio Feature", "Multiple Kernel Learning", "Multimedia ', '"https://doi.org/10.1007/978-3-642-33885-4_40"', '"We present a learning framework for fusion-based video retrieval system, which explicitly optimizes given performance metrics. Real-world computer vision systems serve sophisticated user needs, and domain-specific performance metrics are used to monitor the success of such systems. However, the conventional approach for learning under such circumstances is to blindly minimize standard error rates and hope the targeted performance metrics improve, which is clearly suboptimal. In this work, a novel scheme to directly optimize such targeted performance metrics during learning is developed and presented. Our experimental results on two large consumer video archives are promising and showcase the benefits of the proposed approach."'),
('"Exploiting Contextual Motion Cues for Visual Object Tracking"', '"ECCV 2014"', '["Object tracking", "Adaptive particle filter", "Motion cues"]', '"https://doi.org/10.1007/978-3-319-16181-5_16"', '"In this paper, we propose an algorithm for on-line, real-time tracking of arbitrary objects in videos from unconstrained environments. The method is based on a particle filter framework using different visual features and motion prediction models. We effectively integrate a discriminative on-line learning classifier into the model and propose a new method to collect negative training examples for updating the classifier at each video frame. Instead of taking negative examples only from the surroundings of the object region, or from specific distracting objects, our algorithm samples the negatives from a contextual motion density function. We experimentally show that this type of learning improves the overall performance of the tracking algorithm. Finally, we present quantitative and qualitative results on four challenging public datasets that show the robustness of the tracking algorithm with respect to appearance and view changes, lighting variations, partial occlusions as well as object deformations."'),
('"Exploiting Loops in the Graph of Trifocal Tensors for Calibrating a Network of Cameras"', '"ECCV 2010"', '["Fundamental Matrix", "Less Square Estimator", "Projection Matrice", "Bundle Adjustment", "Fundamen', '"https://doi.org/10.1007/978-3-642-15552-9_7"', '"A technique for calibrating a network of perspective cameras based on their graph of trifocal tensors is presented. After estimating a set of reliable epipolar geometries, a parameterization of the graph of trifocal tensors is proposed in which each trifocal tensor is encoded by a 4-vector. The strength of this parameterization is that the homographies relating two adjacent trifocal tensors, as well as the projection matrices depend linearly on the parameters. A method for estimating these parameters in a global way benefiting from loops in the graph is developed. Experiments carried out on several real datasets demonstrate the efficiency of the proposed approach in distributing errors over the whole set of cameras."'),
('"Exploiting Low-Rank Structure from Latent Domains for Domain Generalization"', '"ECCV 2014"', '["Latent domains", "domain generalization", "domain adaptation", "exemplar-SVMs"]', '"https://doi.org/10.1007/978-3-319-10578-9_41"', '"In this paper, we propose a new approach for domain generalization by exploiting the low-rank structure from multiple latent source domains. Motivated by the recent work on exemplar-SVMs, we aim to train a set of exemplar classifiers with each classifier learnt by using only one positive training sample and all negative training samples. While positive samples may come from multiple latent domains, for the positive samples within the same latent domain, their likelihoods from each exemplar classifier are expected to be similar to each other. Based on this assumption, we formulate a new optimization problem by introducing the nuclear-norm based regularizer on the likelihood matrix to the objective function of exemplar-SVMs. We further extend Domain Adaptation Machine (DAM) to learn an optimal target classifier for domain adaptation. The comprehensive experiments for object recognition and action recognition demonstrate the effectiveness of our approach for domain generalization and domain adaptation."'),
('"Exploiting Model Similarity for Indexing and Matching to a Large Model Database"', '"ECCV 2006"', '["Target Model", "Iterative Close Point", "Shape Signature", "Spin Image", "Model Database"]', '"https://doi.org/10.1007/11744047_41"', '"This paper proposes a novel method to exploit model similarity in model-based 3D object recognition. The scenario consists of a large 3D model database of vehicles, and rapid indexing and matching needs to be done without sequential model alignment. In this scenario, the competition amongst shape features from similar models may pose serious challenge to recognition. To solve the problem, we propose to use a metric to quantitatively measure model similarities. For each model, we use similarity measures to define a model-centric class (MCC), which contains a group of similar models and the pose transformations between the model and its class members. Similarity information embedded in a MCC is used to boost matching hypotheses generation so that the correct model gains more opportunities to be hypothesized and identified. The algorithm is implemented and extensively tested on 1100 real LADAR scans of vehicles with a model database containing over 360 models."'),
('"Exploiting Perception for Face Analysis: Image Abstraction for Head Pose Estimation"', '"ECCV 2012"', '["Face", "Head Pose", "Non Photorealistic Rendering", "Abstraction"]', '"https://doi.org/10.1007/978-3-642-33868-7_32"', '"We present an algorithm to estimate the pose of a human head from a single, low resolution image in real time. It builds on the fundamentals of human perception i.e. abstracting the relevant details from visual cues. Most images contain far too many cues than what are required for estimating human head pose. Thus, we use non-photorealistic rendering to eliminate irrelevant details like expressions from the picture and accentuate facial features critical to estimating head pose. The maximum likelihood pose range is then estimated by training a classifier on scaled down abstracted images. The results are extremely encouraging especially when compared with other recent methods.Moreover the algorithm is robust to illumination, expression, identity and resolution."'),
('"Exploiting Pose Information for Gait Recognition from Depth Streams"', '"ECCV 2014"', '["Gait recognition", "Depth camera", "Key pose", "Incomplete cycle sequences", "Variance image"]', '"https://doi.org/10.1007/978-3-319-16178-5_24"', '"A key-pose based gait recognition approach is proposed that utilizes the depth streams from Kinect. Narrow corridor-like places, such as the entry/ exit points of a security zone, are best suited for its application. Alignment of frontal silhouette sequences is done using coordinate system transformation, followed by a three dimensional voxel volume construction, from which an equivalent fronto-parallel silhouette is generated. A set of fronto-parallel view silhouettes is, henceforth, utilized in deriving a number of key poses. Next, correspondences between the frames of an input sequence and the set of derived key poses are determined using a sequence alignment algorithm. Finally, a gait feature is constructed from each key pose taking into account only those pixels that undergo significant position variation with respect to the silhouette center. Extensive evaluation on a test dataset demonstrates the potential applicability of the proposed method in real-life scenarios."'),
('"Exploiting Privileged Information from Web Data for Image Categorization"', '"ECCV 2014"', '["learning using privileged information", "multi-instance learning", "domain adaptation"]', '"https://doi.org/10.1007/978-3-319-10602-1_29"', '"Relevant and irrelevant web images collected by tag-based image retrieval have been employed as loosely labeled training data for learning SVM classifiers for image categorization by only using the visual features. In this work, we propose a new image categorization method by incorporating the textual features extracted from the surrounding textual descriptions (tags, captions, categories, etc.) as privileged information and simultaneously coping with noise in the loose labels of training web images. When the training and test samples come from different datasets, our proposed method can be further extended to reduce the data distribution mismatch by adding a regularizer based on the Maximum Mean Discrepancy (MMD) criterion. Our comprehensive experiments on three benchmark datasets demonstrate the effectiveness of our proposed methods for image categorization and image retrieval by exploiting privileged information from web data."'),
('"Exploiting Repetitive Object Patterns for Model Compression and Completion"', '"ECCV 2010"', '["Repetitive Element", "Conditional Random Field", "Closed Contour", "Repetitive Pattern", "Object I', '"https://doi.org/10.1007/978-3-642-15555-0_22"', '"Many man-made and natural structures consist of similar elements arranged in regular patterns. In this paper we present an unsupervised approach for discovering and reasoning on repetitive patterns of objects in a single image. We propose an unsupervised detection technique based on a voting scheme of image descriptors. We then introduce the concept of latticelets: minimal sets of arcs that generalize the connectivity of repetitive patterns. Latticelets are used for building polygonal cycles where the smallest cycles define the sought groups of repetitive elements. The proposed method can be used for pattern prediction and completion and high-level image compression. Conditional Random Fields are used as a formalism to predict the location of elements at places where they are partially occluded or detected with very low confidence. Model compression is achieved by extracting and efficiently representing the repetitive structures in the image. Our method has been tested on simulated and real data and the quantitative and qualitative result show the effectiveness of the approach."'),
('"Exploiting Sparse Representations for Robust Analysis of Noisy Complex Video Scenes"', '"ECCV 2012"', '["Sparse Representation", "Sparse Code", "Nonnegative Matrix Factorization", "Complex Scene", "Proba', '"https://doi.org/10.1007/978-3-642-33783-3_15"', '"Recent works have shown that, even with simple low level visual cues, complex behaviors can be extracted automatically from crowded scenes, e.g. those depicting public spaces recorded from video surveillance cameras. However, low level features as optical flow or foreground pixels are inherently noisy. In this paper we propose a novel unsupervised learning approach for the analysis of complex scenes which is specifically tailored to cope directly with features\\u2019 noise and uncertainty. We formalize the task of extracting activity patterns as a matrix factorization problem, considering as reconstruction function the robust Earth Mover\\u2019s Distance. A constraint of sparsity on the computed basis matrix is imposed, filtering out noise and leading to the identification of the most relevant elementary activities in a typical high level behavior. We further derive an alternate optimization approach to solve the proposed problem efficiently and we show that it is reduced to a sequence of linear programs. Finally, we propose to use short trajectory snippets to account for object motion information, in alternative to the noisy optical flow vectors used in previous works. Experimental results demonstrate that our method yields similar or superior performance to state-of-the arts approaches."'),
('"Exploiting the Circulant Structure of Tracking-by-Detection with Kernels"', '"ECCV 2012"', '["Support Vector Machine", "Fast Fourier Transform", "Kernel Matrix", "Polynomial Kernel", "Fourier ', '"https://doi.org/10.1007/978-3-642-33765-9_50"', '"Recent years have seen greater interest in the use of discriminative classifiers in tracking systems, owing to their success in object detection. They are trained online with samples collected during tracking. Unfortunately, the potentially large number of samples becomes a computational burden, which directly conflicts with real-time requirements. On the other hand, limiting the samples may sacrifice performance."'),
('"Exploring Ambiguities for Monocular Non-rigid Shape Estimation"', '"ECCV 2010"', '["Reconstruction Error", "Reprojection Error", "Mesh Edge", "Wave Sequence", "Candidate Shape"]', '"https://doi.org/10.1007/978-3-642-15558-1_27"', '"Recovering the 3D shape of deformable surfaces from single images is difficult because many different shapes have very similar projections. This is commonly addressed by restricting the set of possible shapes to linear combinations of deformation modes and by imposing additional geometric constraints. Unfortunately, because image measurements are noisy, such constraints do not always guarantee that the correct shape will be recovered. To overcome this limitation, we introduce an efficient approach to exploring the set of solutions of an objective function based on point-correspondences and to proposing a small set of candidate 3D shapes. This allows the use of additional image information to choose the best one. As a proof of concept, we use either motion or shading cues to this end and show that we can handle a complex objective function without having to solve a difficult non-linear minimization problem."'),
('"Exploring Bag of Words Architectures in the Facial Expression Domain"', '"ECCV 2012"', '["Facial Expression", "Local Binary Pattern", "Gabor Wavelet", "Scene Recognition", "Spatial Pyramid', '"https://doi.org/10.1007/978-3-642-33868-7_25"', '"Automatic facial expression recognition (AFER) has undergone substantial advancement over the past two decades. This work explores the application of bag of words (BoW), a highly matured approach for object and scene recognition to AFER. We proceed by first highlighting the reasons that makes the task for BoW differ for AFER compared to object and scene recognition. We propose suitable extensions to BoW architecture for the AFER\\u2019s task. These extensions are able to address some of the limitations of current state of the art appearance-based approaches to AFER. Our BoW architecture is based on the spatial pyramid framework, augmented by multiscale dense SIFT features, and a recently proposed approach for object classification: locality-constrained linear coding and max-pooling. Combining these, we are able to achieve a powerful facial representation that works well even with linear classifiers. We show that a well designed BoW architecture can provide a performance benefit for AFER, and elements of the proposed BoW architecture are empirically evaluated. The proposed BoW approach supersedes previous state of the art results by achieving an average recognition rate of 96% on AFER for two public datasets."'),
('"Exploring Interactions Specific to Mixed Reality 3D Modeling Systems"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_12"', '"This paper proposes an interface especially tailored to create a design oriented realistic Mixed Reality (MR) workspace. An MR environment implies a natural mixture between virtual and real objects. We eplore the extension of real objects into the virtual world by having virtual objects attached to them. Also, in order to create a realistic environment, the problems of the occlusions between real and virtual objects are addresed. While providing more intuitive input methods than the conventional desktop modelers or Virtual Reality (VR) immersive systems, our prototype system enables the user to have an accurate perception of the shapes modeled. Thus, the user can recognize correctly the spacial location and real sizes of the models."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Exploring the Facial Expression Perception-Production Link Using Real-Time Automated Facial Express', '"ECCV 2012"', '["Autism Spectrum Disorder", "Autism Spectrum Disorder", "Facial Expression", "Emotion Recognition",', '"https://doi.org/10.1007/978-3-642-33868-7_27"', '"Motor production may play an important role in learning to recognize facial expressions. The present study explores the influence of facial production training on the perception of facial expressions by employing a novel production training intervention built on feedback from automated facial expression recognition. We hypothesized that production training using the automated feedback system would improve an individual\\u2019s ability to identify dynamic emotional faces. Thirty-four participants were administered a dynamic expression recognition task before and after either interacting with a production training video game called the Emotion Mirror or playing a control video game. Consistent with the prediction that perceptual benefits are tied to expression production, individuals with high engagement in production training improved more than individuals with low engagement or individuals who did not receive production training. These results suggest that the visual-motor associations involved in expression production training are related to perceptual abilities. Additionally, this study demonstrates a novel application of computer vision for real-time facial expression intervention training."'),
('"Exploring the Identity Manifold: Constrained Operations in Face Space"', '"ECCV 2010"', '["Vector Length", "Angular Error", "Principal Component Analysis Model", "Target Face", "Active Appe', '"https://doi.org/10.1007/978-3-642-15567-3_9"', '"In this paper, we constrain faces to points on a manifold within the parameter space of a linear statistical model. The manifold is the subspace of faces which have maximally likely distinctiveness and different points correspond to unique identities. We show how the tools of differential geometry can be used to replace linear operations such as warping and averaging with operations on the surface of this manifold. We use the manifold to develop a new method for fitting a statistical face shape model to data, which is both robust (avoids overfitting) and overcomes model dominance (is not susceptible to local minima close to the mean face). Our method outperforms a generic non-linear optimiser when fitting a dense 3D morphable face model to data."'),
('"Exploring the Magnitude of Human Sexual Dimorphism in 3D Face Gender Classification"', '"ECCV 2014"', '["3D face", "Gender classification", "Sexual dimorphism", "Random forest"]', '"https://doi.org/10.1007/978-3-319-16181-5_53"', '"Human faces demonstrate clear Sexual Dimorphism (SD) for recognizing the gender. Different faces, even of the same gender, convey different magnitude of sexual dimorphism. However, in gender classification, gender has been interpreted discretely as either male or female. The exact magnitude of the sexual dimorphism in each gender is ignored. In this paper, we propose to evaluate the SD magnitude, using the ratio of votes from the Random Forest algorithm performed on 3D geometric features related to the face morphology. Then, faces are separated into a Low-SD group and a High-SD group. In the Intra-group experiments, when the training is performed with scans of similar SD magnitude than the testing scan, the classification accuracy improves. In Inter-group experiments, the scans with low magnitude of SD demonstrate higher gender discrimination power than the ones with high SD magnitude. With a decision-level fusion method, our method achieves 97.46 % gender classification rate on the 466 earliest 3D scans of FRGCv2 (mainly neutral), and 97.18 % on the whole FRGCv2 dataset (with expressions)."'),
('"Exploring the Spatial Hierarchy of Mixture Models for Human Pose Estimation"', '"ECCV 2012"', '["Mixture Model", "Leaf Node", "Hierarchical Model", "Training Image", "Hide Node"]', '"https://doi.org/10.1007/978-3-642-33715-4_19"', '"Human pose estimation requires a versatile yet well-constrained spatial model for grouping locally ambiguous parts together to produce a globally consistent hypothesis. Previous works either use local deformable models deviating from a certain template, or use a global mixture representation in the pose space. In this paper, we propose a new hierarchical spatial model that can capture an exponential number of poses with a compact mixture representation on each part. Using latent nodes, it can represent high-order spatial relationship among parts with exact inference. Different from recent hierarchical models that associate each latent node to a mixture of appearance templates (like HoG), we use the hierarchical structure as a pure spatial prior avoiding the large and often confounding appearance space. We verify the effectiveness of this model in three ways. First, samples representing human-like poses can be drawn from our model, showing its ability to capture high-order dependencies of parts. Second, our model achieves accurate reconstruction of unseen poses compared to a nearest neighbor pose representation. Finally, our model achieves state-of-art performance on three challenging datasets, and substantially outperforms recent hierarchical models."'),
('"Exposure Stacks of Live Scenes with Hand-Held Cameras"', '"ECCV 2012"', '["Reference Image", "Camera Motion", "High Dynamic Range", "Camera Phone", "High Dynamic Range Image', '"https://doi.org/10.1007/978-3-642-33718-5_36"', '"Many computational photography applications require the user to take multiple pictures of the same scene with different camera settings. While this allows to capture more information about the scene than what is possible with a single image, the approach is limited by the requirement that the images be perfectly registered. In a typical scenario the camera is hand-held and is therefore prone to moving during the capture of an image burst, while the scene is likely to contain moving objects. Combining such images without careful registration introduces annoying artifacts in the final image. This paper presents a method to register exposure stacks in the presence of both camera motion and scene changes. Our approach warps and modifies the content of the images in the stack to match that of a reference image. Even in the presence of large, highly non-rigid displacements we show that the images are correctly registered to the reference."'),
('"Extended Lucas-Kanade Tracking"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-10602-1_10"', '"The Lucas-Kanade (LK) method is a classic tracking algorithm exploiting target structural constraints thorough template matching. Extended Lucas Kanade or ELK casts the original LK algorithm as a maximum likelihood optimization and then extends it by considering pixel object / background likelihoods in the optimization. Template matching and pixel-based object / background segregation are tied together by a unified Bayesian framework. In this framework two log-likelihood terms related to pixel object / background affiliation are introduced in addition to the standard LK template matching term. Tracking is performed using an EM algorithm, in which the E-step corresponds to pixel object/background inference, and the M-step to parameter optimization. The final algorithm, implemented using a classifier for object / background modeling and equipped with simple template update and occlusion handling logic, is evaluated on two challenging data-sets containing 50 sequences each. The first is a recently published benchmark where ELK ranks 3rd among 30 tracking methods evaluated. On the second data-set of vehicles undergoing severe view point changes ELK ranks in 1st place outperforming state-of-the-art methods."'),
('"Extending Interrupted Feature Point Tracking for 3-D Affine Reconstruction"', '"ECCV 2004"', '["Camera Model", "Motion Segmentation", "Outlier Removal", "Subspace Separation", "Complete Trajecto', '"https://doi.org/10.1007/978-3-540-24670-1_24"', '"Feature point tracking over a video sequence fails when the points go out of the field of view or behind other objects. In this paper, we extend such interrupted tracking by imposing the constraint that under the affine camera model all feature trajectories should be in an affine space. Our method consists of iterations for optimally extending the trajectories and for optimally estimating the affine space, coupled with an outlier removal process. Using real video images, we demonstrate that our method can restore a sufficient number of trajectories for detailed 3-D reconstruction."'),
('"Extending Kernel Fisher Discriminant Analysis with the Weighted Pairwise Chernoff Criterion"', '"ECCV 2006"', '["Face Recognition", "Kernel Principal Component Analysis", "Discriminatory Information", "Discrimin', '"https://doi.org/10.1007/11744085_24"', '"Many linear discriminant analysis (LDA) and kernel Fisher discriminant analysis (KFD) methods are based on the restrictive assumption that the data are homoscedastic. In this paper, we propose a new KFD method called heteroscedastic kernel weighted discriminant analysis (HKWDA) which has several appealing characteristics. First, like all kernel methods, it can handle nonlinearity efficiently in a disciplined manner. Second, by incorporating a weighting function that can capture heteroscedastic data distributions into the discriminant criterion, it can work under more realistic situations and hence can further enhance the classification accuracy in many real-world applications. Moreover, it can effectively deal with the small sample size problem. We have performed some face recognition experiments to compare HKWDA with several linear and nonlinear dimensionality reduction methods, showing that HKWDA consistently gives the best results."'),
('"Extracting 3D Scene-Consistent Object Proposals and Depth from Stereo Images"', '"ECCV 2012"', '["Accuracy Score", "Stereo Image", "Stereo Match", "Stereo Pair", "Object Extraction"]', '"https://doi.org/10.1007/978-3-642-33715-4_34"', '"This work combines two active areas of research in computer vision: unsupervised object extraction from a single image, and depth estimation from a stereo image pair. A recent, successful trend in unsupervised object extraction is to exploit so-called \\u201c3D scene-consistency\\u201d, that is enforcing that objects obey underlying physical constraints of the 3D scene, such as occupancy of 3D space and gravity of objects. Our main contribution is to introduce the concept of 3D scene-consistency into stereo matching. We show that this concept is beneficial for both tasks, object extraction and depth estimation. In particular, we demonstrate that our approach is able to create a large set of 3D scene-consistent object proposals, by varying e.g. the prior on the number of objects. After automatically ranking the proposals we show experimentally that our results are considerably closer to ground truth than state-of-the-art techniques which either use stereo or monocular images. We envision that our method will build the front-end of a future object recognition system for stereo images."'),
('"Extracting Moving People from Internet Videos"', '"ECCV 2008"', '["Human Motion", "Kernel Density Approximation", "Pedestrian Detection", "Pictorial Structure", "Int', '"https://doi.org/10.1007/978-3-540-88693-8_39"', '"We propose a fully automatic framework to detect and extract arbitrary human motion volumes from real-world videos collected from YouTube. Our system is composed of two stages. A person detector is first applied to provide crude information about the possible locations of humans. Then a constrained clustering algorithm groups the detections and rejects false positives based on the appearance similarity and spatio-temporal coherence. In the second stage, we apply a top-down pictorial structure model to complete the extraction of the humans in arbitrary motion. During this procedure, a density propagation technique based on a mixture of Gaussians is employed to propagate temporal information in a principled way. This method reduces greatly the search space for the measurement in the inference stage. We demonstrate the initial success of this framework both quantitatively and qualitatively by using a number of YouTube videos."'),
('"Extracting Structures in Image Collections for Object Recognition"', '"ECCV 2010"', '["Local Structure", "Object Class", "Object Representation", "Image Representation", "Unlabeled Data', '"https://doi.org/10.1007/978-3-642-15549-9_52"', '"Many computer vision methods rely on annotated image sets without taking advantage of the increasing number of unlabeled images available. This paper explores an alternative approach involving unsupervised structure discovery and semi-supervised learning (SSL) in image collections. Focusing on object classes, the first part of the paper contributes with an extensive evaluation of state-of-the-art image representations. Thus, it underlines the decisive influence of the local neighborhood structure and its direct consequences on SSL results and the importance of developing powerful object representations. In a second part, we propose and explore promising directions to improve results by looking at the local topology between images and feature combination strategies."'),
('"Extraction of Myocardial Contractility Patterns from Short-Axes MR Images Using Independent Compone', '"MMBIA 2004"', '["Principal Component Analysis", "Independent Component", "Shape Variation", "Independent Component ', '"https://doi.org/10.1007/978-3-540-27816-0_7"', '"Regional myocardial wall motion analysis has been used in clinical routine to assess myocardial disease, such as infarction and hypertrophy. These diseases can be distinguished from normals by looking at the local abnormality of cardiac motion. In this paper, we present a first result of a feature extraction experiment using the Independent Component Analysis (ICA), where abnormal patterns of myocardial contraction from patients are recognizable and distinguishable from normal subjects."'),
('"Extraction of Semantic Dynamic Content from Videos with Probabilistic Motion Models"', '"ECCV 2004"', '["Motion Model", "Camera Motion", "Video Segment", "Residual Motion", "Sport Video"]', '"https://doi.org/10.1007/978-3-540-24672-5_12"', '"The exploitation of video data requires to extract information at a rather semantic level, and then, methods able to infer \\u201cconcepts\\u201d from low-level video features. We adopt a statistical approach and we focus on motion information. Because of the diversity of dynamic video content (even for a given type of events), we have to design appropriate motion models and learn them from videos. We have defined original and parsimonious probabilistic motion models, both for the dominant image motion (camera motion) and the residual image motion (scene motion). These models are learnt off-line. Motion measurements include affine motion models to capture the camera motion, and local motion features for scene motion. The two-step event detection scheme consists in pre-selecting the video segments of potential interest, and then in recognizing the specified events among the pre-selected segments, the recognition being stated as a classification problem. We report accurate results on several sports videos."'),
('"Extrinsic Camera Calibration Using Multiple Reflections"', '"ECCV 2010"', '["Mobile Robot", "Maximum Likelihood Estimator", "Base Frame", "Camera Frame", "Pixel Noise"]', '"https://doi.org/10.1007/978-3-642-15561-1_23"', '"This paper presents a method for determining the six-degree-of-freedom (DOF) transformation between a camera and a base frame of interest, while concurrently estimating the 3D base-frame coordinates of unknown point features in the scene. The camera observes the reflections of fiducial points, whose base-frame coordinates are known, and reconstruction points, whose base-frame coordinates are unknown. In this paper, we examine the case in which, due to visibility constraints, none of the points are directly viewed by the camera, but instead are seen via reflection in multiple planar mirrors. Exploiting these measurements, we analytically compute the camera-to-base transformation and the 3D base-frame coordinates of the unknown reconstruction points, without a priori knowledge of the mirror sizes, motions, or placements with respect to the camera. Subsequently, we refine the analytical solution using a maximum-likelihood estimator (MLE), to obtain high-accuracy estimates of the camera-to-base transformation, the mirror configurations for each image, and the 3D coordinates of the reconstruction points in the base frame. We validate the accuracy and correctness of our method with simulations and real-world experiments."'),
('"Extrinsic Camera Parameter Recovery from Multiple Image Sequences Captured by an Omni-Directional M', '"ECCV 2004"', '["Natural Feature", "Camera Parameter", "Angle Error", "Real Scene", "Projection Error"]', '"https://doi.org/10.1007/978-3-540-24671-8_26"', '"Recently, many types of omni-directional cameras have been developed and attracted much attention in a number of different fields. Especially, the multi-camera type of omni-directional camera has advantages of high-resolution and almost uniform resolution for any direction of view. In this paper, an extrinsic camera parameter recovery method for a moving omni-directional multi-camera system (OMS) is proposed. First, we discuss a perspective n-point (PnP) problem for an OMS, and then describe a practical method for estimating extrinsic camera parameters from multiple image sequences obtained by an OMS. The proposed method is based on using the shape-from-motion and the PnP techniques."'),
('"Eye Blink Detection Using Variance of Motion Vectors"', '"ECCV 2014"', '["Eye blink detection", "Statistical variance", "Motion vectors", "Outlier detection", "Global movem', '"https://doi.org/10.1007/978-3-319-16199-0_31"', '"A new eye blink detection algorithm is proposed. It is based on analyzing the variance of the vertical motions in the eye region. The face and eyes are detected with a Viola\\u2013Jones type algorithm. Next, a flock of KLT trackers is placed over the eye region. For each eye, region is divided into \\\\(3\\\\times 3\\\\) cells. For each cell an average \\u201ccell\\u201d motion is calculated. Simple state machines analyse the variances for each eye. The proposed method has lower false positive rate compared to other methods based on tracking. We introduce a new challenging dataset Eyeblink8. Our method achieves the best reported mean accuracy 99 % on the Talking dataset and state-of-the-art results on the ZJU dataset."'),
('"Eye Gaze Correction with Stereovision for Video-Teleconferencing"', '"ECCV 2002"', '["Stereoscopic vision", "Eye-gaze correction", "Structure from motion"]', '"https://doi.org/10.1007/3-540-47967-8_32"', '"The lack of eye contact in desktop video teleconferencing substantially reduces the effectiveness of video contents. While expensive and bulky hardware is available on the market to correct eye gaze, researchers have been trying to provide a practical software-based solution to bring video-teleconferencing one step closer to the mass market. This paper presents a novel approach that is based on stereo analysis combined with rich domain knowledge (a personalized face model). This marriage is mutually beneficial. The personalized face model greatly improved the accuracy and robustness of the stereo analysis by substantially reducing the search range; the stereo techniques, using both feature matching and template matching, allow us to extract 3D information of objects other than the face and to determine the head pose in a much more reliable way than if only one camera is used. Thus we enjoy the versatility of stereo techniques without suffering from their vulnerability. By emphasizing a 3D description of the scene on the face part, we synthesize virtual views that maintain eye contact using graphics hardware. Our current system is able to generate an eye-gaze corrected video stream at about 5 frames per second on a commodity PC."'),
('"Eye Movements in Biometrics"', '"BioAW 2004"', '["False Acceptance Rate", "Human Identification", "False Rejection Rate", "Iris Recognition", "Audio', '"https://doi.org/10.1007/978-3-540-25976-3_23"', '"The paper presents a brand new technique of performing human identification which is based on eye movements characteristic. Using this method, the system measures human\\u2019s eyes reaction for visual stimulation. The eyes of the person who is being identified follow the point on the computer screen and eye movement tracking system is used to collect information about eye movements during the experiment. The first experiments showed that it was possible to identify people by means of that method. The method scrutinized here has several significant advantages. It compiles behavioral and physiological aspects and therefore it is difficult to counterfeit and at the same time it is easy to perform. Moreover, it is possible to combine it with other camera-based techniques like iris or face recognition."'),
('"Face Alignment Via Component-Based Discriminative Search"', '"ECCV 2008"', '["Facial Image", "Image Patch", "Initial Shape", "Local Patch", "Face Shape"]', '"https://doi.org/10.1007/978-3-540-88688-4_6"', '"In this paper, we propose a component-based discriminative approach for face alignment without requiring initialization. Unlike many approaches which locally optimize in a small range, our approach searches the face shape in a large range at the component level by a discriminative search algorithm. Specifically, a set of direction classifiers guide the search of the configurations of facial components among multiple detected modes of facial components. The direction classifiers are learned using a large number of aligned local patches and misaligned local patches from the training data. The discriminative search is extremely effective and able to find very good alignment results only in a few (2~3) search iterations. As the new approach gives excellent alignment results on the commonly used datasets (e.g., AR [18], FERET [21]) created under-controlled conditions, we evaluate our approach on a more challenging dataset containing over 1,700 well-labeled facial images with a large range of variations in pose, lighting, expression, and background. The experimental results show the superiority of our approach on both accuracy and efficiency."'),
('"Face Association across Unconstrained Video Frames Using Conditional Random Fields"', '"ECCV 2012"', '["False Detection", "Conditional Random Field", "Laplace Distribution", "Face Appearance", "Null Sta', '"https://doi.org/10.1007/978-3-642-33786-4_13"', '"Automatic face association across unconstrained video frames has many practical applications. Recent advances in the area of object detection have made it possible to replace the traditional tracking-based association approaches with the more robust detection-based ones. However, it is still a very challenging task for real-world unconstrained videos, especially if the subjects are in a moving platform and at distances exceeding several tens of meters. In this paper, we present a novel solution based on a Conditional Random Field (CRF) framework. The CRF approach not only gives a probabilistic and systematic treatment of the problem, but also elegantly combines global and local features. When ambiguities in labels cannot be solved by using the face appearance alone, our method relies on multiple contextual features to provide further evidence for association. Our algorithm works in an on-line mode and is able to reliably handle real-world videos. Results of experiments using challenging video data and comparisons with other methods are provided to demonstrate the effectiveness of our method."'),
('"Face Authentication Using Adapted Local Binary Pattern Histograms"', '"ECCV 2006"', '["Face Image", "Local Binary Pattern", "Local Binary Pattern Feature", "Local Binary Pattern Operato', '"https://doi.org/10.1007/11744085_25"', '"In this paper, we propose a novel generative approach for face authentication, based on a Local Binary Pattern (LBP) description of the face. A generic face model is considered as a collection of LBP-histograms. Then, a client-specific model is obtained by an adaptation technique from this generic model under a probabilistic framework. We compare the proposed approach to standard state-of-the-art face authentication methods on two benchmark databases, namely XM2VTS and BANCA, associated to their experimental protocol. We also compare our approach to two state-of-the-art LBP-based face recognition techniques, that we have adapted to the verification task."'),
('"Face Detection without Bells and Whistles"', '"ECCV 2014"', '["Object Detection", "Average Precision", "Face Detection", "Weak Learner", "Evaluation Protocol"]', '"https://doi.org/10.1007/978-3-319-10593-2_47"', '"Face detection is a mature problem in computer vision. While diverse high performing face detectors have been proposed in the past, we present two surprising new top performance results. First, we show that a properly trained vanilla DPM reaches top performance, improving over commercial and research systems. Second, we show that a detector based on rigid templates - similar in structure to the Viola&Jones detector - can reach similar top performance on this task. Importantly, we discuss issues with existing evaluation benchmark and propose an improved procedure."'),
('"Face Identification by Fitting a 3D Morphable Model Using Linear Shape and Texture Error Functions"', '"ECCV 2002"', '["Input Image", "Face Image", "Texture Parameter", "Stochastic Gradient Descent", "Active Appearance', '"https://doi.org/10.1007/3-540-47979-1_1"', '"This paper presents a novel algorithm aiming at analysis and identification of faces viewed from different poses and illumination conditions. Face analysis from a single image is performed by recovering the shape and textures parameters of a 3D Morphable Model in an analysis-by-synthesis fashion. The shape parameters are computed from a shape error estimated by optical flow and the texture parameters are obtained from a texture error. The algorithm uses linear equations to recover the shape and texture parameters irrespective of pose and lighting conditions of the face image. Identification experiments are reported on more than 5000 images from the publicly available CMU-PIE database which includes faces viewed from 13 different poses and under 22 different illuminations. Extensive identification results are available on our web page for future comparison with novel algorithms."'),
('"Face Image Relighting using Locally Constrained Global Optimization"', '"ECCV 2010"', '["Face Image", "Target Face", "Ratio Image", "Local Window", "Active Appearance Model"]', '"https://doi.org/10.1007/978-3-642-15561-1_4"', '"A face image relighting method using locally constrained global optimization is presented in this paper. Based on the empirical fact that common radiance environments are locally homogeneous, we propose to use an optimization based solution in which local linear adjustments are performed on overlapping windows throughout the input image. As such, local textures and global smoothness of the input image can be preserved simultaneously when applying the illumination transformation. Experimental results demonstrate the effectiveness of the proposed method comparing to some previous approaches."'),
('"Face Liveness Detection from a Single Image with Sparse Low Rank Bilinear Discriminative Model"', '"ECCV 2010"', '["Face Recognition", "Face Image", "Relevance Vector Machine", "Face Recognition System", "Real Face', '"https://doi.org/10.1007/978-3-642-15567-3_37"', '"Spoofing with photograph or video is one of the most common manner to circumvent a face recognition system. In this paper, we present a real-time and non-intrusive method to address this based on individual images from a generic webcamera. The task is formulated as a binary classification problem, in which, however, the distribution of positive and negative are largely overlapping in the input space, and a suitable representation space is hence of importance. Using the Lambertian model, we propose two strategies to extract the essential information about different surface properties of a live human face or a photograph, in terms of latent samples. Based on these, we develop two new extensions to the sparse logistic regression model which allow quick and accurate spoof detection. Primary experiments on a large photo imposter database show that the proposed method gives preferable detection performance compared to others."'),
('"Face Recognition Based on ICA Combined with FLD"', '"BioAW 2002"', '["Facial Expression", "Face Recognition", "Linear Discriminant Analysis", "Independent Component Ana', '"https://doi.org/10.1007/3-540-47917-1_2"', '"Recently in face recognition, as opposed to our expectation, the performance of an ICA (Independent Component Analysis) method combined with LDA (Linear Discriminant Analysis) was reported as lower than an ICA only based method. This research points out that (ICA+LDA) methods have not got a fair comparison for evaluating its recognition performance. In order to incorporate class specific information into ICA, we have employed FLD (Fisher Linear Discriminant) and have proposed our (ICA+FLD) method. In the experimental results, we report that our (ICA+FLD) method has better performance than ICA only based methods as well as other representative methods such as Eigenface and Fisherface methods."'),
('"Face Recognition Based on Locally Salient ICA Information"', '"BioAW 2004"', '["Face Recognition", "Independent Component Analysis", "Training Image", "Recognition Performance", ', '"https://doi.org/10.1007/978-3-540-25976-3_1"', '"ICA (Independent Component Analysis) is contrasted with PCA (Principal Component Analysis) in that ICA basis images are spatially localized, highlighting salient feature regions corresponding to eyes, eye brows, nose and lips. However, ICA basis images do not display perfectly local characteristic in the sense that pixels that do not belong to locally salient feature regions still have some weight values. These pixels in the non-salient regions contribute to the degradation of the recognition performance. We have proposed a novel method based on ICA that only employ locally salient information. The new method effectively implements the idea of \\u201drecognition by parts\\u201d for the problem of face recognition. Experimental results using AT&T, Harvard, FERET and AR databases show that the recognition performance of the proposed method outperforms that of PCA and ICA methods especially in the cases of facial images that have partial occlusions and local distortions such as changes in facial expression and at low dimensions."'),
('"Face Recognition by 3D Registration for the Visually Impaired Using a RGB-D Sensor"', '"ECCV 2014"', '["Face recognition", "Assistive computer vision", "3D registration", "RGB-D sensor"]', '"https://doi.org/10.1007/978-3-319-16199-0_53"', '"To help visually impaired people recognize people in their daily life, a 3D face feature registration approach is proposed with a RGB-D sensor. Compared to 2D face recognition methods, 3D data based approaches are more robust to the influence of face orientations and illumination changes. Different from most 3D data based methods, we employ a one-step ICP registration approach that is much less time consuming. The error tolerance of the 3D registration approach is analyzed with various error levels in 3D measurements. The method is tested with a Kinect sensor, by analyzing both the angular and distance errors to recognition performance. A number of other potential benefits in using 3D face data are also discussed, such as RGB image rectification, multiple-view face integration, and facial expression modeling, all useful for social interactions of visually impaired people with others."'),
('"Face Recognition from Facial Surface Metric"', '"ECCV 2004"', '["Face Recognition", "Geodesic Distance", "Facial Surface", "Surface Gradient", "Photometric Stereo"', '"https://doi.org/10.1007/978-3-540-24671-8_18"', '"Recently, a 3D face recognition approach based on geometric invariant signatures, has been proposed. The key idea is a representation of the facial surface, invariant to isometric deformations, such as those resulting from facial expressions. One important stage in the construction of the geometric invariants involves in measuring geodesic distances on triangulated surfaces, which is carried out by the fast marching on triangulated domains algorithm."'),
('"Face Recognition from Long-Term Observations"', '"ECCV 2002"', '["Face Recognition", "Face Image", "Combination Rule", "Virtual View", "Active Appearance Model"]', '"https://doi.org/10.1007/3-540-47977-5_56"', '"We address the problem of face recognition from a large set of images obtained over time - a task arising in many surveillance and authentication applications. A set or a sequence of images provides information about the variability in the appearance of the face which can be used for more robust recognition. We discuss different approaches to the use of this information, and show that when cast as a statistical hypothesis testing problem, the classification task leads naturally to an information-theoretic algorithm that classifies sets of images using the relative entropy (Kullback-Leibler divergence) between the estimated density of the input set and that of stored collections of images for each class. We demonstrate the performance of the proposed algorithm on two medium-sized data sets of approximately frontal face images, and describe an application of the method as part of a view-independent recognition system."'),
('"Face Recognition from Video Using the Generic Shape-Illumination Manifold"', '"ECCV 2006"', '["Face Recognition", "Video Sequence", "Recognition Rate", "Face Image", "Gaussian Mixture Model"]', '"https://doi.org/10.1007/11744085_3"', '"In spite of over two decades of intense research, illumination and pose invariance remain prohibitively challenging aspects of face recognition for most practical applications. The objective of this work is to recognize faces using video sequences both for training and recognition input, in a realistic, unconstrained setup in which lighting, pose and user motion pattern have a wide variability and face images are of low resolution. In particular there are three areas of novelty: (i) we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation, learnt offline, to generalize in the presence of extreme illumination changes; (ii) we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses; and (iii) we introduce an accurate video sequence \\u201creillumination\\u201d algorithm to achieve robustness to face motion patterns in video. We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination, pose and head motion variation. On this challenging data set our system consistently demonstrated a nearly perfect recognition rate (over 99.7%), significantly outperforming state-of-the-art commercial software and methods from the literature."'),
('"Face Recognition with Local Binary Patterns"', '"ECCV 2004"', '["Face Recognition", "Linear Discriminant Analysis", "Recognition Rate", "Face Image", "Local Binary', '"https://doi.org/10.1007/978-3-540-24670-1_36"', '"In this work, we present a novel approach to face recognition which considers both shape and texture information to represent face images. The face area is first divided into small regions from which Local Binary Pattern (LBP) histograms are extracted and concatenated into a single, spatially enhanced feature histogram efficiently representing the face image. The recognition is performed using a nearest neighbour classifier in the computed feature space with Chi square as a dissimilarity measure. Extensive experiments clearly show the superiority of the proposed scheme over all considered methods (PCA, Bayesian Intra/extrapersonal Classifier and Elastic Bunch Graph Matching) on FERET tests which include testing the robustness of the method against different facial expressions, lighting and aging of the subjects. In addition to its efficiency, the simplicity of the proposed method allows for very fast feature extraction."'),
('"Face Recognition with Patterns of Oriented Edge Magnitudes"', '"ECCV 2010"', '["Face Recognition", "Recognition Rate", "Face Image", "Local Binary Pattern", "Gabor Wavelet"]', '"https://doi.org/10.1007/978-3-642-15549-9_23"', '"This paper addresses the question of computationally inexpensive yet discriminative and robust feature sets for real-world face recognition. The proposed descriptor named Patterns of Oriented Edge Magnitudes (POEM) has desirable properties: POEM (1) is an oriented, spatial multi-resolution descriptor capturing rich information about the original image; (2) is a multi-scale self-similarity based structure that results in robustness to exterior variations; and (3) is of low complexity and is therefore practical for real-time applications. Briefly speaking, for every pixel, the POEM feature is built by applying a self-similarity based structure on oriented magnitudes, calculated by accumulating a local histogram of gradient orientations over all pixels of image cells, centered on the considered pixel. The robustness and discriminative power of the POEM descriptor is evaluated for face recognition on both constrained (FERET) and unconstrained (LFW) datasets. Experimental results show that our algorithm achieves better performance than the state-of-the-art representations. More impressively, the computational cost of extracting the POEM descriptor is so low that it runs around 20 times faster than just the first step of the methods based upon Gabor filters. Moreover, its data storage requirements are 13 and 27 times smaller than those of the LGBP (Local Gabor Binary Patterns) and HGPP (Histogram of Gabor Phase Patterns) descriptors respectively."'),
('"Face Representation Method Using Pixel-to-Vertex Map (PVM) for 3D Model Based Face Recognition"', '"ECCV 2006"', '["Face Recognition", "Singular Value Decomposition", "Iterative Close Point", "Texture Coefficient",', '"https://doi.org/10.1007/11754336_3"', '"3D model based approach for face recognition has been spotlighted as a robust solution under variant conditions of pose and illumination. Since a generative 3D face model consists of a large number of vertices, a 3D model based face recognition system is generally inefficient in computation time. In this paper, we propose a novel 3D face representation algorithm to reduce the number of vertices and optimize its computation time. Finally, we evaluate the performance of proposed algorithm with the Korean face database collected using a stereo-camera based 3D face capturing device."'),
('"Face-Based Illuminant Estimation"', '"ECCV 2012"', '["Input Image", "Skin Color", "Color Space", "Face Detector", "Color Constancy"]', '"https://doi.org/10.1007/978-3-642-33885-4_67"', '"In this work we show that it is possible to use skin tones to estimate the illuminant color. We use a face detector to find faces in the scene, and the corresponding skin colors to estimate the chromaticity of the illuminant. The method, that has been presented at CVPR 2012 [1] is based on two observations: first, skin colors tend to form a cluster in the color space, making it a cue to estimate the illuminant in the scene; second, many photographic images are portraits or contain people. If no faces are detected, the input image is processed with a low-level illuminant estimation algorithm automatically selected according to [2]. The method will be demonstrated on a public dataset of images in RAW format [3], and on images acquired live during the demo."'),
('"FaceHugger: The ALIEN Tracker Applied to Faces"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_61"', '"This paper proposes an online tracking method which has been inspired by studying the effects of Scale Invariant Feature Transform (SIFT) when applied to objects assumed to be flat even though they are not. The consequent deviation from flatness induces nuisance factors that act on the feature representation in a manner for which no general local invariants can be computed, such as in the case of occlusion, sensor quantization and casting shadows. However, if features are over-represented, they can provide the necessary information to build online, a robust object/context discriminative classifier. This is achieved based on weakly aligned multiple instance local features in a sense that will be made clear in the rest of this paper. According to this observation, we present a non parametric online tracking by detection approach that yields state of the art performance."'),
('"FaceMouse: A Human-Computer Interface for Tetraplegic People"', '"ECCV 2006"', '["Disable People", "Template Match", "Face Tracking", "Mouse Pointer", "Computer Vision Technique"]', '"https://doi.org/10.1007/11754336_10"', '"This paper proposes a new human-machine interface particularly conceived for people with severe disabilities (specifically tetraplegic people), that allows them to interact with the computer for their everyday life by means of mouse pointer. In this system, called FaceMouse, instead of classical \\"pointer paradigm\\" that requires the user to look at the point where to move, we propose to use a paradigm called \\"derivative paradigm\\", where the user does not indicate the precise position, but the direction along which the mouse pointer must be moved. The proposed system is composed of a common, low-cost webcam, and by a set of computer vision techniques developed to identify the parts of the user\\u2019s face (the only body part that a tetraplegic person can move) and exploit them for moving the pointer. Specifically, the implemented algorithm is based on template matching to track the nose of the user and on cross-correlation to calculate the best match. Finally, several real applications of the system are described and experimental results carried out by disabled people are reported."'),
('"FaceTracer: A Search Engine for Large Collections of Images with Faces"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88693-8_25"', '"We have created the first image search engine based entirely on faces. Using simple text queries such as \\u201csmiling men with blond hair and mustaches,\\u201d users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and computes all labels off-line, leading to fast on-line search performance. In addition, we describe how our system can be used for a number of applications, including law enforcement, social networks, and personal photo management. Our search engine will soon be made publicly available."'),
('"Facial Action Transfer with Personalized Bilinear Regression"', '"ECCV 2012"', '["Facial action transfer", "Bilinear regression"]', '"https://doi.org/10.1007/978-3-642-33709-3_11"', '"Facial Action Transfer (FAT) has recently attracted much attention in computer vision due to its diverse applications in the movie industry, computer games, and privacy protection. The goal of FAT is to \\u201cclone\\u201d the facial actions from the videos of one person (source) to another person (target). In this paper, we will assume that we have a video of the source person but only one frontal image of the target person. Most successful methods for FAT require a training set with annotated correspondence between expressions of different subjects, sometimes including many images of the target subject. However, labeling expressions is time consuming and error prone (i.e., it is difficult to capture the same intensity of the expression across people). Moreover, in many applications it is not realistic to have many labeled images of the target. This paper proposes a method to learn a personalized facial model, that can produce photo-realistic person-specific facial actions (e.g., synthesize wrinkles for smiling), from only a neutral image of the target person. More importantly, our learning method does not need an explicit correspondence of expressions across subjects. Experiments on the Cohn-Kanade and the RU-FACS databases show the effectiveness of our approach to generate video-realistic images of the target person driven by spontaneous facial actions of the source. Moreover, we illustrate applications of FAT to face de-identification."'),
('"Facial Age Estimation Through the Fusion of Texture and Local Appearance Descriptors"', '"ECCV 2014"', '["Age estimation", "CCA", "HOG", "LBP", "SURF"]', '"https://doi.org/10.1007/978-3-319-16181-5_51"', '"Automatic extraction of soft biometric characteristics from face images is a very prolific field of research. Among these soft biometrics, age estimation can be very useful for several applications, such as advanced video surveillance [5, 12], demographic statistics collection, business intelligence and customer profiling, and search optimization in large databases. However, estimating age from uncontrollable environments, with insufficient and incomplete training data, dealing with strong person-specificity, and high within-range variance, can be very challenging. These difficulties have been addressed in the past with complex and strongly hand-crafted descriptors, which make it difficult to replicate and compare the validity of posterior classification schemes. This paper presents a simple yet effective approach which fuses and exploits texture- and local appearance-based descriptors to achieve faster and more accurate results. A series of local descriptors and their combinations have been evaluated under a diversity of settings, and the extensive experiments carried out on two large databases (MORPH and FRGC) demonstrate state-of-the-art results over previous work."'),
('"Facial Contour Labeling via Congealing"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15549-9_26"', '"It is a challenging vision problem to discover non-rigid shape deformation for an image ensemble belonging to a single object class, in an automatic or semi-supervised fashion. The conventional semi- supervised approach [1] uses a congealing-like process to propagate manual landmark labels from a few images to a large ensemble. Although effective on an inter-person database with a large population, there is potential for increased labeling accuracy. With the goal of providing highly accurate labels, in this paper we present a parametric curve representation for each of the seven major facial contours. The appearance information along the curve, named curve descriptor, is extracted and used for congealing. Furthermore, we demonstrate that advanced features such as Histogram of Oriented Gradient (HOG) can be utilized in the proposed congealing framework, which operates in a dual-curve congealing manner for the case of a closed contour. With extensive experiments on a 300-image ensemble that exhibits moderate variation in facial pose and shape, we show that substantial progress has been achieved in the labeling accuracy compared to the previous state-of-the-art approach."'),
('"Facial Ethnic Appearance Synthesis"', '"ECCV 2014"', '["Soft biometrics", "Ethnicity", "Face synthesis", "Fukunaga Koontz transform"]', '"https://doi.org/10.1007/978-3-319-16181-5_62"', '"In this work, we have explored several subspace reconstruction methods for facial ethnic appearance synthesis (FEAS). In our experiments, our proposed dual subspace modeling using the Fukunaga Koontz transform (FKT) yields much better facial ethnic synthesis results than the \\\\(\\\\ell _1\\\\) minimization, the \\\\(\\\\ell _2\\\\) minimization and the principal component analysis (PCA) reconstruction method. With that, we are able to automatically and efficiently synthesize different facial ethnic appearance and alter the facial ethnic appearance of the query image to any other ethnic appearance as desired. Our technique well preserves the facial structure of the query image and simultaneously synthesize the skin tone and ethnic features that best matches target ethnicity group. Facial ethnic appearance synthesis can be applied to synthesizing facial images of a particular ethnicity group for unbalanced database, and can be used to train ethnicity invariant classifiers by generating multiple ethnic appearances of the same subject in the training stage."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Facial Expression Analysis Based on High Dimensional Binary Features"', '"ECCV 2014"', '["Facial expression recognition", "Smile detection", "High-dimensional feature", "Census transformat', '"https://doi.org/10.1007/978-3-319-16181-5_10"', '"High dimensional engineered features have yielded high performance results on a variety of visual recognition tasks and attracted significant recent attention. Here, we examine the problem of expression recognition in static facial images. We first present a technique to build high dimensional, \\\\(\\\\sim 60\\\\mathrm{k}\\\\) features composed of dense Census transformed vectors based on locations defined by facial keypoint predictions. The approach yields state of the art performance at 96.8% accuracy for detecting facial expressions on the well known Cohn-Kanade plus (CK+) evaluation and 93.2% for smile detection on the GENKI dataset. We also find that the subsequent application of a linear discriminative dimensionality reduction technique can make the approach more robust when keypoint locations are less precise. We go on to explore the recognition of expressions captured under more challenging pose and illumination conditions. Specifically, we test this representation on the GENKI smile detection dataset. Our high dimensional feature technique yields state of the art performance on both of these well known evaluations."'),
('"Facial Expression Recognition Based on 3D Dynamic Range Model Sequences"', '"ECCV 2008"', '["Facial Expression", "Linear Discriminative Analysis", "Recognition Rate", "Facial Expression Recog', '"https://doi.org/10.1007/978-3-540-88688-4_5"', '"Traditionally, facial expression recognition (FER) issues have been studied mostly based on modalities of 2D images, 2D videos, and 3D static models. In this paper, we propose a spatio-temporal expression analysis approach based on a new modality, 3D dynamic geometric facial model sequences, to tackle the FER problems. Our approach integrates a 3D facial surface descriptor and Hidden Markov Models (HMM) to recognize facial expressions. To study the dynamics of 3D dynamic models for FER, we investigated three types of HMMs: temporal 1D-HMM, pseudo 2D-HMM (a combination of a spatial HMM and a temporal HMM), and real 2D-HMM. We also created a new dynamic 3D facial expression database for the research community. The results show that our approach achieves a 90.44% person-independent recognition rate for distinguishing six prototypic facial expressions. The advantage of our method is demonstrated as compared to methods based on 2D texture images, 2D/3D Motion Units, and 3D static range models. Further experimental evaluations also verify the benefits of our approach with respect to partial facial surface occlusion, expression intensity changes, and 3D model resolution variations."'),
('"Facial Landmark Detection by Deep Multi-task Learning"', '"ECCV 2014"', '["Face Image", "Convolutional Neural Network", "Related Task", "Deep Neural Network", "Facial Landma', '"https://doi.org/10.1007/978-3-319-10599-4_7"', '"Facial landmark detection has long been impeded by the problems of occlusion and pose variation. Instead of treating the detection task as a single and independent problem, we investigate the possibility of improving detection robustness through multi-task learning. Specifically, we wish to optimize facial landmark detection together with heterogeneous but subtly correlated tasks, e.g. head pose estimation and facial attribute inference. This is non-trivial since different tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, with task-wise early stopping to facilitate learning convergence. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art method based on cascaded deep model [21]."'),
('"Facial Landmarking: Comparing Automatic Landmarking Methods with Applications in Soft Biometrics"', '"ECCV 2012"', '["facial landmarking", "auto-landmarking methods", "gender classification", "race determination"]', '"https://doi.org/10.1007/978-3-642-33868-7_29"', '"Registration is a critical step in computer-based image analysis. In this work we examine the effects of registration in face-based soft-biometrics. This form of soft-biometrics, better termed as facial analytics, takes an image containing a face and returns attributes of that face. In this work, the attributes of focus are gender and race. Automatic generation of facial analytics relies on accurate registration. Hence, this work evaluates three techniques for dense registration, namely AAM, Stacked ASM and CLM. Further, we evaluate the influence of facial landmark mis-localization, resulting from these techniques, on gender classification and race determination. To the best of our knowledge, such an evaluation of landmark mis-localization on soft biometrics, has not been conducted. We further demonstrate an effective system for gender and race classification based on dense landmarking and multi-factored principle components analysis. The system performs well against a multi-age face dataset for both gender and race classification."'),
('"Facial Model Fitting Based on Perturbation Learning and It\\u2019s Evaluation on Challenging Real-Wo', '"ECCV 2012"', '["Facial Expression", "Feature Point", "Canonical Correlation Analysis", "Shape Model", "Relevance V', '"https://doi.org/10.1007/978-3-642-33863-2_16"', '"We present a robust and efficient framework for facial shape model fitting. Traditional model fitting approaches are sensitive to noise resulting from scene variations due to lighting, facial expressions, poses, etc., and tend to spend substantial computational effort due to heuristic searching algorithms. Our work distinguishes itself from conventional approaches by employing (a) non-uniform sampling features unified by the shape model that affords robustness, and (b) regression analysis between observed features and underlying shape parameters that allow for efficient model update. We demonstrate the effectiveness of our framework by evaluating its performance on several new and existing datasets including challenging real-world diversities. Significantly higher localization accuracy and speedup factors of 15 have been observed comparing with the traditional approach."'),
('"Factorial Markov Random Fields"', '"ECCV 2002"', '["Grouping and segmentation", "Layer representation", "Graphical model", "Bayesian inference", "Mark', '"https://doi.org/10.1007/3-540-47977-5_21"', '"In this paper we propose an extension to the standard Markov Random Field (MRF) model in order to handle layers. Our extension, which we call a Factorial MRF (FMRF), is analogous to the extension from Hidden Markov Models (HMM\\u2019s) to Factorial HMM\\u2019s. We present an efficient EM-based algorithm for inference on Factorial MRF\\u2019s. Our algorithm makes use of the fact that layers are a priori independent, and that layers only interact through the observable image. The algorithm iterates between wide inference, i.e., inference within each layer for the entire set of pixels, and deep inference, i.e., inference through the layers for each single pixel. The efficiency of our method is partly due to the use of graph cuts for binary segmentation, which is part of the wide inference step. We show experimental results for both real and synthetic images."'),
('"Factorization of Natural 4 \\u00d7 4 Patch Distributions"', '"SMVP 2004"', '["Mutual Information", "Natural Image", "Domain Space", "Linked List", "Natural Patch"]', '"https://doi.org/10.1007/978-3-540-30212-4_15"', '"The lack of sufficient machine readable images makes impossible the direct computation of natural image 4 \\u00d7 4 block statistics and one has to resort to indirect approximated methods to reduce their domain space. A natural approach to this is to collect statistics over compressed images; if the reconstruction quality is good enough, these statistics will be sufficiently representative. However, a requirement for easier statistics collection is that the method used provides a uniform representation of the compression information across all patches, something for which codebook techniques are well suited. We shall follow this approach here, using a fractal compression\\u2013inspired quantization scheme to approximate a given patch B by a triplet (D B , \\u03bc B , \\u03c3 B ) with \\u03c3 B the patch\\u2019s contrast, \\u03bc B its brightness and D B a codebook approximation to the mean\\u2013variance normalization (B \\u2013 \\u03bc B )/\\u03c3 B of B. The resulting reduction of the domain space makes feasible the computation of entropy and mutual information estimates that, in turn, suggest a factorization of the approximation of p(B) \\u2243 p(D B , \\u03bc B , \\u03c3 B ) as p(D B , \\u03bc B , \\u03c3 B ) \\u2243 p(D B )p(\\u03bc)p(\\u03c3)\\u03a6(|| \\u2207 ||), with \\u03a6 being a high contrast correction."'),
('"Factorization with Uncertainty"', '"ECCV 2000"', '["Singular Value Decomposition", "Mahalanobis Distance", "Feature Position", "Factorization Algorith', '"https://doi.org/10.1007/3-540-45054-8_35"', '"Factorization using Singular Value Decomposition (SVD) is often used for recovering 3D shape and motion from feature correspondences across multiple views. SVD is powerful at finding the global solution to the associated least-square-error minimization problem. However, this is the correct error to minimize only when the x and y positional errors in the features are uncorrelated and identically distributed. But this is rarely the case in real data. Uncertainty in feature position depends on the underlying spatial intensity structure in the image, which has strong directionality to it. Hence, the proper measure to minimize is covariance-weighted squared-error (or the Mahalanobis distance). In this paper, we describe a new approach to covariance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty into structure and motion. Our approach is based on transforming the raw-data into a covariance-weighted data space, where the components of noise in the different directions are uncorrelated and identically distributed. Applying SVD to the transformed data now minimizes a meaningful objective function. We empirically show that our new algorithm gives good results for varying degrees of directional uncertainty. In particular, we show that unlike other SVD-based factorization algorithms, our method does not degrade with increase in directionality of uncertainty, even in the extreme when only normal-flow data is available. It thus provides a unified approach for treating corner-like points together with points along linear structures in the image."'),
('"Fast 3-D Urban Object Detection on Streaming Point Clouds"', '"ECCV 2014"', '["LIDAR", "Urban object detection", "3-D point clouds", "Dynamic processing"]', '"https://doi.org/10.1007/978-3-319-16181-5_48"', '"Efficient and fast object detection from continuously streamed 3-D point clouds has a major impact in many related research tasks, such as autonomous driving, self localization and mapping and understanding large scale environment. This paper presents a LIDAR-based framework, which provides fast detection of 3-D urban objects from point cloud sequences of a Velodyne HDL-64E terrestrial LIDAR scanner installed on a moving platform. The pipeline of our framework receives raw streams of 3-D data, and produces distinct groups of points which belong to different urban objects. In the proposed framework we present a simple, yet efficient hierarchical grid data structure and corresponding algorithms that significantly improve the processing speed of the object detection task. Furthermore, we show that this approach confidently handles streaming data, and provides a speedup of two orders of magnitude, with increased detection accuracy compared to a baseline connected component analysis algorithm."'),
('"Fast and Accurate Rotation Estimation on the 2-Sphere without Correspondences"', '"ECCV 2008"', '["Rotation Parameter", "Shape Retrieval", "Spherical Image", "Rotation Estimation", "Sinc Interpolat', '"https://doi.org/10.1007/978-3-540-88688-4_18"', '"We present a refined method for rotation estimation of signals on the 2-Sphere. Our approach utilizes a fast correlation in the harmonic domain to estimate rotation angles of arbitrary size and resolution. The method is able to achieve great accuracy even for very low spherical harmonic expansions of the input signals without using correspondences or any other kind of a priori information. The rotation parameters are computed analytically without additional iterative post-processing or \\u201cfine tuning\\u201d."'),
('"Fast and Accurate Texture Recognition with Multilayer Convolution and Multifractal Analysis"', '"ECCV 2014"', '["Invariant Representation", "Multifractal Analysis", "Multifractal Spectrum", "Viewpoint Change", "', '"https://doi.org/10.1007/978-3-319-10590-1_33"', '"A fast and accurate texture recognition system is presented. The new approach consists in extracting locally and globally invariant representations. The locally invariant representation is built on a multi-resolution convolutional network with a local pooling operator to improve robustness to local orientation and scale changes. This representation is mapped into a globally invariant descriptor using multifractal analysis. We propose a new multifractal descriptor that captures rich texture information and is mathematically invariant to various complex transformations. In addition, two more techniques are presented to further improve the robustness of our system. The first technique consists in combining the generative PCA classifier with multiclass SVMs. The second technique consists of two simple strategies to boost classification results by synthetically augmenting the training set. Experiments show that the proposed solution outperforms existing methods on three challenging public benchmark datasets, while being computationally efficient."'),
('"Fast and Adaptive Deep Fusion Learning for Detecting Visual Objects"', '"ECCV 2012"', '["Deep Learning", "Unlabelled Data", "Actual Probability Vector", "Stable Tracking", "Adaptive Metho', '"https://doi.org/10.1007/978-3-642-33885-4_35"', '"Currently, object tracking/detection is based on a \\u201cshallow learning\\u201d paradigm; they locally process features to build an object model and then they apply adaptive methodologies to estimate model parameters. However, such an approach presents the drawback of losing the \\u201cwhole picture information\\u201d required to maintain a stable tracking for long time and high visual changes. To overcome these obstacles, we need a \\u201cdeep\\u201d information fusion framework. Deep learning is a new emerging research area that simulates the efficiency and robustness by which the humans\\u2019 brain represents information; it deeply propagates data into complex hierarchies. However, implementing a deep fusion learning paradigm in a machine presents research challenges mainly due to the highly non-linear structures involved and the \\u201ccurse of dimensionality\\u201d. Another difficulty which is critical in computer vision applications is that learning should be self adapted to guarantee stable object detection over long time spans. In this paper, we propose a novel fast (in real-time) and adaptive information fusion strategy that exploits the deep learning paradigm. The proposed framework integrates optimization strategies able to update in real-time the non-linear model parameters according in a way to trust, as much as possible, the current changes of the environment, while providing a minimal degradation of the previous gained experience."'),
('"Fast and Exact Primal-Dual Iterations for Variational Problems in Computer Vision"', '"ECCV 2010"', '["Saddle Point", "Variational Problem", "Input Image", "Saddle Point Problem", "Segmentation Problem', '"https://doi.org/10.1007/978-3-642-15552-9_36"', '"The saddle point framework provides a convenient way to formulate many convex variational problems that occur in computer vision. The framework unifies a broad range of data and regularization terms, and is particularly suited for nonsmooth problems such as Total Variation-based approaches to image labeling. However, for many interesting problems the constraint sets involved are difficult to handle numerically. State-of-the-art methods rely on using nested iterative projections, which induces both theoretical and practical convergence issues. We present a dual multiple-constraint Douglas-Rachford splitting approach that is globally convergent, avoids inner iterative loops, enforces the constraints exactly, and requires only basic operations that can be easily parallelized. The method outperforms existing methods by a factor of 4\\u2009\\u2212\\u200920 while considerably increasing the numerical robustness."'),
('"Fast and Precise Template Matching Based on Oriented Gradients"', '"ECCV 2012"', '["template matching", "oriented gradients", "ICP"]', '"https://doi.org/10.1007/978-3-642-33885-4_63"', '"In this paper we propose a fast template matching method which can handle various types of objects. In our method the discretized orientations of image gradients which are robust to illumination changes and clutterd backgrounds are used as features. The features are binary represented and they can be matched very fast using bitwise operations. Furthermore, the rotated and resized templates those have similar feature vectors are clustered to one template and the total number of templates are greatly reduced, which boosts the detection speed. The experimental results show that our method can detect target objects (the search space includes translation, \\u00b1180 deg rotation, and \\u00b150% scale change) with sub-pixel accracy in real-time."'),
('"Fast Anisotropic Gauss Filtering"', '"ECCV 2002"', '["Bilinear Interpolation", "Line Detection", "Tracking Application", "Recursive Approximation", "Com', '"https://doi.org/10.1007/3-540-47969-4_7"', '"We derive the decomposition of the anisotropic Gaussian in a one dimensional Gauss filter in the x-direction followed by a one dimensional filter in a non-orthogonal direction \\u03d5. So also the anisotropic Gaussian can be decomposed by dimension. This appears to be extremely efficient from a computing perspective. An implementation scheme for normal convolution and for recursive filtering is proposed. Also directed derivative filters are demonstrated."'),
('"Fast Approximate Nearest Neighbor Methods for Non-Euclidean Manifolds with Applications to Human Ac', '"ECCV 2010"', '["Cluster Center", "Binary Code", "Geodesic Distance", "Grassmann Manifold", "Human Activity Recogni', '"https://doi.org/10.1007/978-3-642-15552-9_53"', '"Approximate Nearest Neighbor (ANN) methods such as Locality Sensitive Hashing, Semantic Hashing, and Spectral Hashing, provide computationally efficient procedures for finding objects similar to a query object in large datasets. These methods have been successfully applied to search web-scale datasets that can contain millions of images. Unfortunately, the key assumption in these procedures is that objects in the dataset lie in a Euclidean space. This assumption is not always valid and poses a challenge for several computer vision applications where data commonly lies in complex non-Euclidean manifolds. In particular, dynamic data such as human activities are commonly represented as distributions over bags of video words or as dynamical systems. In this paper, we propose two new algorithms that extend Spectral Hashing to non-Euclidean spaces. The first method considers the Riemannian geometry of the manifold and performs Spectral Hashing in the tangent space of the manifold at several points. The second method divides the data into subsets and takes advantage of the kernel trick to perform non-Euclidean Spectral Hashing. For a data set of N samples the proposed methods are able to retrieve similar objects in as low as O(K) time complexity, where K is the number of clusters in the data. Since K\\u2009\\u226a\\u2009N, our methods are extremely efficient. We test and evaluate our methods on synthetic data generated from the Unit Hypersphere and the Grassmann manifold. Finally, we show promising results on a human action database."'),
('"Fast Approximations to Structured Sparse Coding and Applications to Object Classification"', '"ECCV 2012"', '["Sparse Code", "Dictionary Learn", "Sift Descriptor", "Fast Approximation", "Group Lasso"]', '"https://doi.org/10.1007/978-3-642-33715-4_15"', '"We describe a method for fast approximation of sparse coding. A given input vector is passed through a binary tree. Each leaf of the tree contains a subset of dictionary elements. The coefficients corresponding to these dictionary elements are allowed to be nonzero and their values are calculated quickly by multiplication with a precomputed pseudoinverse. The tree parameters, the dictionary, and the subsets of the dictionary corresponding to each leaf are learned. In the process of describing this algorithm, we discuss the more general problem of learning the groups in group structured sparse modeling. We show that our method creates good sparse representations by using it in the object recognition framework of [1,2]. Implementing our own fast version of the SIFT descriptor the whole system runs at 20 frames per second on 321 \\u00d7481 sized images on a laptop with a quad-core cpu, while sacrificing very little accuracy on the Caltech 101, Caltech 256, and 15 scenes benchmarks."'),
('"Fast Automatic Single-View 3-d Reconstruction of Urban Scenes"', '"ECCV 2008"', '["Ground Plane", "Vertical Wall", "Graph Node", "Virtual View", "Building Wall"]', '"https://doi.org/10.1007/978-3-540-88688-4_8"', '"We consider the problem of estimating 3-d structure from a single still image of an outdoor urban scene. Our goal is to efficiently create 3-d models which are visually pleasant. We chose an appropriate 3-d model structure and formulate the task of 3-d reconstruction as model fitting problem. Our 3-d models are composed of a number of vertical walls and a ground plane, where ground-vertical boundary is a continuous polyline. We achieve computational efficiency by special preprocessing together with stepwise search of 3-d model parameters dividing the problem into two smaller sub-problems on chain graphs. The use of Conditional Random Field models for both problems allows to various cues. We infer orientation of vertical walls of 3-d model vanishing points."'),
('"Fast Covariance Computation and Dimensionality Reduction for Sub-window Features in Images"', '"ECCV 2010"', '["Feature Vector", "Linear Discriminant Analysis", "Image Patch", "Texture Synthesis", "Covariance C', '"https://doi.org/10.1007/978-3-642-15552-9_12"', '"This paper presents algorithms for efficiently computing the covariance matrix for features that form sub-windows in a large multi-dimensional image. For example, several image processing applications, e.g. texture analysis/synthesis, image retrieval, and compression, operate upon patches within an image. These patches are usually projected onto a low-dimensional feature space using dimensionality reduction techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), which in-turn requires computation of the covariance matrix from a set of features. Covariance computation is usually the bottleneck during PCA or LDA (O(nd 2) where n is the number of pixels in the image and d is the dimensionality of the vector). Our approach reduces the complexity of covariance computation by exploiting the redundancy between feature vectors corresponding to overlapping patches. Specifically, we show that the covariance between two feature components can be reduced to a function of the relative displacement between those components in patch space. One can then employ a lookup table to store covariance values by relative displacement. By operating in the frequency domain, this lookup table can be computed in O(n logn) time. We allow the patches to sub-sample the image, which is useful for hierarchical processing and also enables working with filtered responses over these patches, such as local gist features. We also propose a method for fast projection of sub-window patches onto the low-dimensional space."'),
('"Fast Difference Schemes for Edge Enhancing Beltrami Flow"', '"ECCV 2002"', '["Beltrami Flow", "Unconditionally Stable Schemes", "Color Images", "Segmentation"]', '"https://doi.org/10.1007/3-540-47969-4_23"', '"The Beltrami flow [13,14] is one of the most effective denoising algorithms in image processing. For gray-level images, we show that the Beltrami flow equation can be arranged in a reaction-diffusion form. This reveals the edge-enhancing properties of the equation and suggests the application of additive operator split (AOS) methods [4,5] for faster convergence. As we show with numerical simulations, the AOS method results in an unconditionally stable semi-implicit linearized difference scheme in 2D and 3D. The values of the edge indicator function are used from the previous step in scale, while the pixel values of the next step are used to approximate the flow. The optimum ratio between the reaction and diffusion counterparts of the governing PDE is studied, in order to achieve a better quality of segmentation. The computational time decreases by a factor of ten, as compared to the explicit scheme. For 2D color images, the Beltrami flow equations are coupled, and do not yield readily to the AOS technique. However, in the proximity of an edge, the cross-products of color gradients nearly vanish, and the coupling becomes weak. The principal directions of the edge indicator matrix are normal and tangent to the edge. Replacing the action of the matrix on the gradient vector by an action of its eigenvalue, we reduce the color problem to the gray level case with a reasonable accuracy. The scalar edge indicator function for the color case becomes essentially the same as that for the gray level image, and the fast implicit technique is implemented."'),
('"Fast Dynamic Texture Detection"', '"ECCV 2010"', '["Local Binary Pattern", "Dynamic Texture", "Smoke Detection", "Background Oriented Schlieren", "Glo', '"https://doi.org/10.1007/978-3-642-15561-1_49"', '"Dynamic textures can be considered to be spatio-temporally varying visual patterns in image sequences with certain temporal regularity. We propose a novel and efficient approach to explore the violation of the brightness constancy assumption, as an indication of presence of dynamic texture, using simple optical flow techniques. We assume that dynamic texture regions are those that have poor spatio-temporal optical flow coherence. Further, we propose a second approach that uses robust global parametric motion estimators that effectively and efficiently detect motion outliers, and which we exploit as powerful cues to localize dynamic textures. Experimental and comparative studies on a range of synthetic and real-world dynamic texture sequences show the feasibility of the proposed approaches, with results which are competitive to or better than recent state-of-art approaches and significantly faster."'),
('"Fast Features Invariant to Rotation and Scale of Texture"', '"ECCV 2014"', '["Texture", "Classification", "LBP", "LBP-HF", "Histogram", "SVM", "Feature maps", "Ffirst"]', '"https://doi.org/10.1007/978-3-319-16181-5_4"', '"A family of novel texture representations called Ffirst, the Fast Features Invariant to Rotation and Scale of Texture, is introduced. New rotation invariants are proposed, extending the LBP-HF features, improving the recognition accuracy. Using the full set of LBP features, as opposed to uniform only, leads to further improvement. Linear Support Vector Machines with an approximate \\\\(\\\\chi ^2\\\\)-kernel map are used for fast and precise classification."'),
('"Fast Fusion Moves for Multi-model Estimation"', '"ECCV 2012"', '["Facility Location", "Facility Location Problem", "Model Penalty", "Fusion Move", "Biclique Problem', '"https://doi.org/10.1007/978-3-642-33718-5_27"', '"We develop a fast, effective algorithm for minimizing a well-known objective function for robust multi-model estimation. Our work introduces a combinatorial step belonging to a family of powerful move-making methods like \\u03b1-expansion and fusion. We also show that our subproblem can be quickly transformed into a comparatively small instance of minimum-weighted vertex-cover. In practice, these vertex-cover subproblems are almost always bipartite and can be solved exactly by specialized network flow algorithms. Experiments indicate that our approach achieves the robustness of methods like affinity propagation, whilst providing the speed of fast greedy heuristics."'),
('"Fast Marching 3D Reconstruction of Interphase Chromosomes"', '"MMBIA 2004"', '["fast marching method", "deformable models", "3D object reconstruction", "biomedical application", ', '"https://doi.org/10.1007/978-3-540-27816-0_33"', '"Reliable 3D reconstruction of interphase chromosomes imaged using confocal microscopy is an important task in cell biology. Computer model of chromosome territories enables performing necessary measurements and consequently making morphological studies. A large number of processed objects is necessary to ensure statistical significance of the results. Therefore an automated procedure is needed. We have developed a successful algorithm for 3D reconstruction of chromosome territories on the basis of well-known fast marching algorithm. The fast marching algorithm solves front evolution problem similarly to deformable models but in an effective way with the time complexity \\\\({\\\\cal O}(n\\\\log n)\\\\)."'),
('"Fast Memory-Efficient Generalized Belief Propagation"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_35"', '"Generalized Belief Propagation (gbp) has proven to be a promising technique for performing inference on Markov random fields (mrfs). However, its heavy computational cost and large memory requirements have restricted its application to problems with small state spaces. We present methods for reducing both run time and storage needed by gbp for a large class of pairwise potentials of the mrf. Further, we show how the problem of subgraph matching can be formulated using this class of mrfs and thus, solved efficiently using our approach. Our results significantly outperform the state-of-the-art method. We also obtain excellent results for the related problem of matching pictorial structures for object recognition."'),
('"Fast Multi-aspect 2D Human Detection"', '"ECCV 2010"', '["Body Part", "Detection Result", "Inference Algorithm", "Compatibility Function", "Viewpoint Change', '"https://doi.org/10.1007/978-3-642-15558-1_33"', '"We address the problem of detecting human figures in images, taking into account that the image of the human figure may be taken from a range of viewpoints. We capture the geometric deformations of the 2D human figure using an extension of the Common Factor Model (CFM) of Lan and Huttenlocher. The key contribution of the paper is an improved iterative message passing inference algorithm that runs faster than the original CFM algorithm. This is based on the insight that messages created using the distance transform are shift invariant and therefore messages can be created once and then shifted for subsequent iterations. Since shifting (O(1) complexity) is faster than computing a distance transform (O(n) complexity), a significant speedup is observed in the experiments. We demonstrate the effectiveness of the new model for the human parsing problem using the Iterative Parsing data set and results are competitive with the state of the art detection algorithm of Andriluka, et al."'),
('"Fast Multi-labelling for Stereo Matching"', '"ECCV 2010"', '["Tuning Range", "Stereo Match", "Stereo Pair", "Minimum Cycle", "Exterior Edge"]', '"https://doi.org/10.1007/978-3-642-15558-1_38"', '"We describe a new fast algorithm for multi-labelling problems. In general, a multi-labelling problem is NP-hard. Widely used algorithms like \\u03b1-expansion can reach a suboptimal result in a time linear in the number of the labels. In this paper, we propose an algorithm which can obtain results of comparable quality polynomially faster. We use the Divide and Conquer paradigm to separate the complexities induced by the label set and the variable set, and deal with each of them respectively. Such a mechanism improves the solution speed without depleting the memory resource, hence it is particularly valuable for applications where the variable set and the label set are both huge. Another merit of the proposed method is that the trade-off between quality and time efficiency can be varied through using different parameters. The advantage of our method is validated by experiments."'),
('"Fast Object Detection with Occlusions"', '"ECCV 2004"', '["Face Detection", "Training Error", "Soft Margin", "Cascade Structure", "Occlude Face"]', '"https://doi.org/10.1007/978-3-540-24670-1_31"', '"We describe a new framework, based on boosting algorithms and cascade structures, to efficiently detect objects/faces with occlusions. While our approach is motivated by the work of Viola and Jones, several techniques have been developed for establishing a more general system, including (i) a robust boosting scheme, to select useful weak learners and to avoid overfitting; (ii) reinforcement training, to reduce false-positive rates via a more effective training procedure for boosted cascades; and (iii) cascading with evidence, to extend the system to handle occlusions, without compromising in detection speed. Experimental results on detecting faces under various situations are provided to demonstrate the performances of the proposed method."'),
('"Fast Optimization for Mixture Prior Models"', '"ECCV 2010"', '["Compressive Sensing", "Alternate Direction Method", "Regularization Problem", "Continuous Convex F', '"https://doi.org/10.1007/978-3-642-15558-1_44"', '"We consider the minimization of a smooth convex function regularized by the mixture of prior models. This problem is generally difficult to solve even each simpler regularization problem is easy. In this paper, we present two algorithms to effectively solve it. First, the original problem is decomposed into multiple simpler subproblems. Then, these subproblems are efficiently solved by existing techniques in parallel. Finally, the result of the original problem is obtained from the weighted average of solutions of subproblems in an iterative framework. We successfully applied the proposed algorithms to compressed MR image reconstruction and low-rank tensor completion. Numerous experiments demonstrate the superior performance of the proposed algorithm in terms of both the accuracy and computational complexity."'),
('"Fast Organization of Large Photo Collections Using CUDA"', '"ECCV 2010"', '["Binary Code", "Fundamental Matrix", "Query Expansion", "Thread Block", "Virtual Image"]', '"https://doi.org/10.1007/978-3-642-35740-4_36"', '"In this paper, we introduce a system for the automatic organization of photo collections consisting of millions of images downloaded from the Internet. To our knowledge, this is the first approach that tackles this problem exclusively through the use of general-purpose GPU computing techniques. By leveraging the inherent parallelism of the problem and through the use of efficient GPU-based algorithms, our system is able to effectively summarize datasets containing up to three million images in approximately 16 hours on a single PC, which is orders of magnitude faster compared to current state of the art techniques. In this paper, we present the various algorithmic considerations and design aspects of our system, and describe in detail the various steps of the processing pipeline. Additionally, we demonstrate the effectiveness of the system by showing results for a variety of real-world datasets, ranging from the scale of a single landmark, to that of an entire city."'),
('"Fast Parameter Sensitivity Analysis of PDE-Based Image Processing Methods"', '"ECCV 2012"', '["Polynomial Chaos", "Stochastic Parameter", "Polynomial Chaos Expansion", "Parameter Sensitivity An', '"https://doi.org/10.1007/978-3-642-33786-4_11"', '"We present a fast parameter sensitivity analysis by combining recent developments from uncertainty quantification with image processing operators. The approach is not based on a sampling strategy, instead we combine the polynomial chaos expansion and stochastic finite elements with PDE-based image processing operators. With our approach and a moderate number of parameters in the models the full sensitivity analysis is obtained at the cost of a few Monte Carlo runs. To demonstrate the efficiency and simplicity of the approach we show a parameter sensitivity analysis for Perona-Malik diffusion, random walker and Ambrosio-Tortorelli segmentation, and discontinuity-preserving optical flow computation."'),
('"Fast Planar Correlation Clustering for Image Segmentation"', '"ECCV 2012"', '["Image Segmentation", "Planar Graph", "Integer Linear Programming", "Markov Random Field", "Linear ', '"https://doi.org/10.1007/978-3-642-33783-3_41"', '"We describe a new optimization scheme for finding high-quality clusterings in planar graphs that uses weighted perfect matching as a subroutine. Our method provides lower-bounds on the energy of the optimal correlation clustering that are typically fast to compute and tight in practice. We demonstrate our algorithm on the problem of image segmentation where this approach outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations."'),
('"Fast Regularization of Matrix-Valued Images"', '"ECCV 2012"', '["Matrix-valued", "Regularization", "Total-variation", "Optimization", "Motion understanding", "DT-M', '"https://doi.org/10.1007/978-3-642-33712-3_13"', '"Regularization of images with matrix-valued data is important in medical imaging, motion analysis and scene understanding. We propose a novel method for fast regularization of matrix group-valued images."'),
('"Fast Segmentation of the Mitral Valve Leaflet in Echocardiography"', '"CVAMIA 2006"', '["Medical Image Analysis", "Tracking and Motion", "Active Contours", "Ultrasound Imaging"]', '"https://doi.org/10.1007/11889762_20"', '"This paper presents a semi-automatic method for tracking the mitral valve leaflet in transesophageal echocardiography. The algorithm requires a manual initialization and then segments an image sequence. The use of two constrained active contours and curve fitting techniques results in a fast segmentation algorithm. The active contours successfully track the inner cardiac muscle and the mitral valve leaflet axis. Three sequences have been processed and the generated muscle outline and leaflet axis have been visually assessed by an expert. This work is a part of a more general project which aims at providing real-time detection of the mitral valve leaflet in transesophageal echocardiography images."'),
('"Fast Selective Detection of Rotational Symmetries Using Normalized Inhibition"', '"ECCV 2000"', '["Rotational Symmetry", "Local Orientation", "Orientation Information", "Orientation Image", "Aerial', '"https://doi.org/10.1007/3-540-45054-8_57"', '"Perceptual experiments indicate that corners and curvature are very important features in the process of recognition. This paper presents a new method to efficiently detect rotational symmetries, which describe complex curvature such as corners, circles, star- and spiral patterns. The method is designed to give selective and sparse responses. It works in three steps; first extract local orientation from a gray-scale or color image, second correlate the orientation image with rotational symmetry filters and third let the filter responses inhibit each other in order to get more selective responses. The correlations can be made efficient by separating the 2D-filters into a small number of 1D-filters."'),
('"Fast Stixel Computation for Fast Pedestrian Detection"', '"ECCV 2012"', '["Object Detection", "Ground Plane", "Horizontal Gradient", "Stereo Match", "Obstacle Detection"]', '"https://doi.org/10.1007/978-3-642-33885-4_2"', '"Applications using pedestrian detection in street scene require both high speed and quality. Maximal speed is reached when exploiting the geometric information provided by stereo cameras. Yet, extracting useful information at speeds higher than 100 Hz is a non-trivial task. We propose a method to estimate the ground-obstacles boundary (and its distance), without computing a depth map. By properly parametrizing the search space in the image plane we improve the algorithmic performance, and reach speeds of \\\\(200\\\\ \\\\mbox{Hz}\\\\) on a desktop CPU. When connected with a state of the art GPU objects detector, we reach high quality detections at the record speed of \\\\(165\\\\ \\\\mbox{Hz}\\\\)."'),
('"Fast Tiered Labeling with Topological Priors"', '"ECCV 2012"', '["Markov Random Field", "Local Extremum", "Relation Graph", "Label Problem", "Bottom Tier"]', '"https://doi.org/10.1007/978-3-642-33765-9_42"', '"We consider labeling an image with multiple tiers. Tiers, one on top of another, enforce a strict vertical order among objects (e.g. sky is above the ground). Two new ideas are explored: First, under a simplification of the general tiered labeling framework proposed by Felzenszwalb and Veksler [1], we design an efficient O(KN) algorithm for the approximate optimal labeling of an image of N pixels with K tiers. Our algorithm runs in over 100 frames per second on images of VGA resolutions when K is less than 6. When K\\u2009=\\u20093, our solution overlaps with the globally optimal one by Felzenszwalb and Veksler in over 99% of all pixels but runs 1000 times faster. Second, we define a topological prior that specifies the number of local extrema in the tier boundaries, and give an O(NM) algorithm to find a single, optimal tier boundary with exactly M local maxima and minima. These two extensions enrich the general tiered labeling framework and enable fast computation. The proposed topological prior further improves the accuracy in labeling details."'),
('"Fast Visual Tracking via Dense Spatio-temporal Context Learning"', '"ECCV 2014"', '["Target Object", "Object Location", "Context Model", "Visual Tracking", "Multiple Instance Learn"]', '"https://doi.org/10.1007/978-3-319-10602-1_9"', '"In this paper, we present a simple yet fast and robust algorithm which exploits the dense spatio-temporal context for visual tracking. Our approach formulates the spatio-temporal relationships between the object of interest and its locally dense contexts in a Bayesian framework, which models the statistical correlation between the simple low-level features (i.e., image intensity and position) from the target and its surrounding regions. The tracking problem is then posed by computing a confidence map which takes into account the prior information of the target location and thereby alleviates target location ambiguity effectively. We further propose a novel explicit scale adaptation scheme, which is able to deal with target scale variations efficiently and effectively. The Fast Fourier Transform (FFT) is adopted for fast learning and detection in this work, which only needs 4 FFT operations. Implemented in MATLAB without code optimization, the proposed tracker runs at 350 frames per second on an i7 machine. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods in terms of efficiency, accuracy and robustness."'),
('"Fast, Quality, Segmentation of Large Volumes \\u2013 Isoperimetric Distance Trees"', '"ECCV 2006"', '["Gaussian Elimination", "Laplacian Matrix", "Graph Partitioning", "Cholesky Factor", "Distance Tree', '"https://doi.org/10.1007/11744078_35"', '"For many medical segmentation tasks, the contrast along most of the boundary of the target object is high, allowing simple thresholding or region growing approaches to provide nearly sufficient solutions for the task. However, the regions recovered by these techniques frequently leak through bottlenecks in which the contrast is low or non-existent. We propose a new approach based on a novel speed-up of the isoperimetric algorithm [1] that can solve the problem of leaks through a bottleneck. The speed enhancement converts the isoperimetric segmentation algorithm to a fast, linear-time computation by using a tree representation as the underlying graph instead of a standard lattice structure. In this paper, we show how to create an appropriate tree substrate for the segmentation problem and how to use this structure to perform a linear-time computation of the isoperimetric algorithm. This approach is shown to overcome common problems with watershed-based techniques and to provide fast, high-quality results on large datasets."'),
('"Feature Correspondence Via Graph Matching: Models and Global Optimization"', '"ECCV 2008"', '["Global Minimum", "Uniqueness Constraint", "Graph Match", "Feature Correspondence", "Dual Decomposi', '"https://doi.org/10.1007/978-3-540-88688-4_44"', '"In this paper we present a new approach for establishing correspondences between sparse image features related by an unknown non-rigid mapping and corrupted by clutter and occlusion, such as points extracted from a pair of images containing a human figure in distinct poses. We formulate this matching task as an energy minimization problem by defining a complex objective function of the appearance and the spatial arrangement of the features. Optimization of this energy is an instance of graph matching, which is in general a NP-hard problem. We describe a novel graph matching optimization technique, which we refer to as dual decomposition (DD), and demonstrate on a variety of examples that this method outperforms existing graph matching algorithms. In the majority of our examples DD is able to find the global minimum within a minute. The ability to globally optimize the objective allows us to accurately learn the parameters of our matching model from training examples. We show on several matching tasks that our learned model yields results superior to those of state-of-the-art methods."'),
('"Feature Disentangling Machine - A Novel Approach of Feature Selection and Disentangling in Facial E', '"ECCV 2014"', '["Feature Selection", "Facial Expression", "Local Binary Pattern", "Facial Expression Recognition", ', '"https://doi.org/10.1007/978-3-319-10593-2_11"', '"Studies in psychology show that not all facial regions are of importance in recognizing facial expressions and different facial regions make different contributions in various facial expressions. Motivated by this, a novel framework, named Feature Disentangling Machine (FDM), is proposed to effectively select active features characterizing facial expressions. More importantly, the FDM aims to disentangle these selected features into non-overlapped groups, in particular, common features that are shared across different expressions and expression-specific features that are discriminative only for a target expression. Specifically, the FDM integrates sparse support vector machine and multi-task learning in a unified framework, where a novel loss function and a set of constraints are formulated to precisely control the sparsity and naturally disentangle active features. Extensive experiments on two well-known facial expression databases have demonstrated that the FDM outperforms the state-of-the-art methods for facial expression analysis. More importantly, the FDM achieves an impressive performance in a cross-database validation, which demonstrates the generalization capability of the selected features."'),
('"Feature Harvesting for Tracking-by-Detection"', '"ECCV 2006"', '["Image Feature", "Feature Point", "Target Object", "Image Patch", "Training Sequence"]', '"https://doi.org/10.1007/11744078_46"', '"We propose a fast approach to 3\\u2013D object detection and pose estimation that owes its robustness to a training phase during which the target object slowly moves with respect to the camera. No additional information is provided to the system, save a very rough initialization in the first frame of the training sequence. It can be used to detect the target object in each video frame independently."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Feature Points Tracking: Robustness to Specular Highlights and Lighting Changes"', '"ECCV 2006"', '["Tracking Method", "Small Window", "Illumination Change", "Feature Tracking", "Lighting Variation"]', '"https://doi.org/10.1007/11744085_7"', '"Since the precise modeling of reflection is a difficult task, most feature points trackers assume that objects are lambertian and that no lighting change occurs. To some extent, a few approaches answer these issues by computing an affine photometric model or by achieving a photometric normalization. Through a study based on specular reflection models, we explain explicitly the assumptions on which these techniques are based. Then we propose a tracker that compensates for specular highlights and lighting variations more efficiently when small windows of interest are considered. Experimental results on image sequences prove the robustness and the accuracy of this technique in comparison with the existing trackers. Moreover, the computation time of the tracking is not significantly increased."'),
('"Feature Tracking for Wide-Baseline Image Retrieval"', '"ECCV 2010"', '["Image Retrieval", "Visual Word", "Query Image", "Feature Tracking", "Graph Construction"]', '"https://doi.org/10.1007/978-3-642-15555-0_23"', '"We address the problem of large scale image retrieval in a wide-baseline setting, where for any query image all the matching database images will come from very different viewpoints. In such settings traditional bag-of-visual-words approaches are not equipped to handle the significant feature descriptor transformations that occur under large camera motions. In this paper we present a novel approach that includes an offline step of feature matching which allows us to observe how local descriptors transform under large camera motions. These observations are encoded in a graph in the quantized feature space. This graph can be used directly within a soft-assignment feature quantization scheme for image retrieval."'),
('"Feature Vector Definition for a Decision Tree Based Craquelure Identification in Old Paintings"', '"ECCV 2012"', '["craquelure identification", "image segmentation", "feature detection"]', '"https://doi.org/10.1007/978-3-642-33863-2_56"', '"In the paper a new proposal of semi-automatic method of craquelure detection in old paintings is presented. It is well known, that craquelure pattern is a unique feature and its character gives a significant information about the overall condition of the work, progress and cause of its degradation and helps in dating as well as confirming the authentication of the work. There exist methods, mostly deriving from other ridge and valley recognition problems, like geodesic or medical image feature segmentation based on watershed transform, morphological operations and region growing algorithm but they sometimes fail because of a complex nature of a craquelure pattern or large scale of an analyzed area. In this work a method is presented continuing a known semi-automatic technique based on a region growing algorithm. The novel approach is to apply a decision tree based pixel segmentation method to indicate the start points of craquelure pattern. The main difficulty in this mathod is defining an adequate set of descriptors forming a feature vector for the mining model."'),
('"Feature-Preserving Medial Axis Noise Removal"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_45"', '"This paper presents a novel technique for medial axis noise removal. The method introduced removes the branches generated by noise on an object\\u2019s boundary without losing the fine features that are often altered or destroyed by current pruning methods. The algorithm consists of an intuitive threshold-based pruning process, followed by an automatic feature reconstruction phase that effectively recovers lost details without reintroducing noise. The result is a technique that is robust and easy to use. Tests show that the method works well on a variety of objects with significant differences in shape complexity, topology and noise characteristics."'),
('"Feedback Loop Between High Level Semantics and Low Level Vision"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-16181-5_38"', '"High level semantic analysis typically involves constructing a Markov network over detections from low level detectors to encode context and model relationships between them. In complex higher order networks (e.g. Markov Logic Networks), each detection can be part of many factors and the network size grows rapidly as a function of the number of detections. Hence to keep the network size small, a threshold is applied on the confidence measures of the detections to discard the less likely detections. A practical challenge is to decide what thresholds to use to discard noisy detections. A high threshold will lead to a high false dismissal rate. A low threshold can result in many detections including mostly noisy ones which leads to a large network size and increased computational requirements. We propose a feedback based incremental technique to keep the network size small. We initialize the network with detections above a high confidence threshold and then based on the high level semantics in the initial network, we incrementally select the relevant detections from the remaining ones that are below the threshold. We show three different ways of selecting detections which are based on three scoring functions that bound the increase in the optimal value of the objective function of network, with varying degrees of accuracy and computational cost. We perform experiments with an event recognition task in one-on-one basketball videos that uses Markov Logic Networks."'),
('"Feedback Retargeting"', '"ECCV 2010"', '["Input Image", "Source Image", "Output Image", "Salient Object", "Data Term"]', '"https://doi.org/10.1007/978-3-642-35740-4_12"', '"Feedback retargeting combines the benefits of two previous retargeting methods: Bidirectional similarity [1] and Shift-Map [2]. The first method may have blurry areas due to patch averaging and the latter can remove entire objects. Feedback retargeting has the sharpness of shift-map and the completeness of bidirectional similarity, avoiding the removal of salient objects."'),
('"Figure-Ground Image Segmentation Helps Weakly-Supervised Learning of Objects"', '"ECCV 2010"', '["Topic Model", "Common Object", "Image Collection", "Image Saliency", "Aware Model"]', '"https://doi.org/10.1007/978-3-642-15567-3_41"', '"Given a collection of images containing a common object, we seek to learn a model for the object without the use of bounding boxes or segmentation masks. In linguistics, a single document provides no information about location of the topics it contains. On the contrary, an image has a lot to tell us about where foreground and background topics lie. Extensive literature on modelling bottom-up saliency and pop-out aims at predicting eye fixations and allocation of visual attention in a single image, prior to any recognition of content. Most salient image parts are likely to capture image foreground. We propose a novel probabilistic model, shape and figure-ground aware model (sFG model) that exploits bottom-up image saliency to compute an informative prior on segment topic assignments. xtitsegmented objects into visually object classes. (ii) bottom up saliency combined with co-occurrence give us strong hints about figure/ground that can help guide the topic discovery from partially segmented data. Our model exploits both figure-ground organization in each image separately, as well as feature re-occurrence across the image collection. Since we use image dependent topic prior, during model learning we optimize a conditional likelihood of the image collection given the image bottom-up saliency information. Our discriminative framework can tolerate larger intraclass variability of objects with fewer training data. We iterate between bottom-up figure-ground image organization and model parameter learning by accumulating image statistics from the entire image collection. The model learned influences later image figure-ground labelling. We present results of our approach on diverse datasets showing great improvement over generative probabilistic models that do not exploit image saliency, indicating the suitability of our model for weakly-supervised visual organization."'),
('"Figure/Ground Assignment in Natural Images"', '"ECCV 2006"', '["Natural Image", "Conditional Random Field", "Ground Organization", "Conditional Random Field Model', '"https://doi.org/10.1007/11744047_47"', '"Figure/ground assignment is a key step in perceptual organization which assigns contours to one of the two abutting regions, providing information about occlusion and allowing high-level processing to focus on non-accidental shapes of figural regions. In this paper, we develop a computational model for figure/ground assignment in complex natural scenes. We utilize a large dataset of images annotated with human-marked segmentations and figure/ground labels for training and quantitative evaluation."'),
('"Filter-Based Mean-Field Inference for Random Fields with Higher-Order Terms and Product Label-Space', '"ECCV 2012"', '["Unary Potential", "Object Label", "Identical Kernel", "Pairwise Term", "IEEE PAMI"]', '"https://doi.org/10.1007/978-3-642-33715-4_3"', '"Recently, a number of cross bilateral filtering methods have been proposed for solving multi-label problems in computer vision, such as stereo, optical flow and object class segmentation that show an order of magnitude improvement in speed over previous methods. These methods have achieved good results despite using models with only unary and/or pairwise terms. However, previous work has shown the value of using models with higher-order terms e.g. to represent label consistency over large regions, or global co-occurrence relations. We show how these higher-order terms can be formulated such that filter-based inference remains possible. We demonstrate our techniques on joint stereo and object labeling problems, as well as object class segmentation, showing in addition for joint object-stereo labeling how our method provides an efficient approach to inference in product label-spaces. We show that we are able to speed up inference in these models around 10-30 times with respect to competing graph-cut/move-making methods, as well as maintaining or improving accuracy in all cases. We show results on PascalVOC-10 for object class segmentation, and Leuven for joint object-stereo labeling."'),
('"Finding Actions Using Shape Flows"', '"ECCV 2008"', '["Action Recognition", "Delaunay Triangulation", "Camera Motion", "Background Clutter", "Joint Traje', '"https://doi.org/10.1007/978-3-540-88688-4_21"', '"We propose a novel method for action detection based on a new action descriptor called a shape flow that represents both the shape and movement of an object in a holistic and parsimonious manner. We find actions by finding shape flows in a target video that are similar to a template shape flow. Shape flows are largely independent of appearance, and the match cost function that we propose is invariant to scale changes and smooth nonlinear deformation in space and time. The problem of matching shape flows is difficult, however, yielding a large, non-convex, integer program. We propose a novel relaxation method based on successive convexification that converts this hard program into a vastly smaller linear program: By using only those variables that appear on the 4D lower convex hull of the matching cost volume, most of the variables in the linear program may be eliminated. Experiments confirm that the proposed shape flow method can successfully detect complex actions in cluttered video, even with self-occlusion, camera motion, and intra-class variation."'),
('"Finding Approximate Convex Shapes in RGBD Images"', '"ECCV 2014"', '["Point Cloud", "Convex Hull", "Convex Shape", "Convex Part", "Indoor Scene"]', '"https://doi.org/10.1007/978-3-319-10578-9_38"', '"We propose a novel method to find approximate convex 3D shapes from single RGBD images. Convex shapes are more general than cuboids, cylinders, cones and spheres. Many real-world objects are near-convex and every non-convex object can be represented using convex parts. By finding approximate convex shapes in RGBD images, we extract important structures of a scene. From a large set of candidates generated from over-segmented superpixels we globally optimize the selection of these candidates so that they are mostly convex, have small intersection, have a small number and mostly cover the scene. The optimization is formulated as a two-stage linear optimization and efficiently solved using a branch and bound method which is guaranteed to give the global optimal solution. Our experiments on thousands of RGBD images show that our method is fast, robust against clutter and is more accurate than competing methods."'),
('"Finding Coherent Motions and Semantic Regions in Crowd Scenes: A Diffusion and Clustering Approach"', '"ECCV 2014"', '["Thermal Energy", "Motion Vector", "Input Motion", "Coherent Motion", "Cluster Label"]', '"https://doi.org/10.1007/978-3-319-10590-1_49"', '"This paper addresses the problem of detecting coherent motions in crowd scenes and subsequently constructing semantic regions for activity recognition. We first introduce a coarse-to-fine thermal-diffusion-based approach. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. Finally, these semantic regions are used to recognize activities in crowded scenes. Experiments on various videos demonstrate the effectiveness of our approach."'),
('"Finding Correspondence from Multiple Images via Sparse and Low-Rank Decomposition"', '"ECCV 2012"', '["Feature correspondence", "partial permutation", "low rank and sparse matrix decomposition"]', '"https://doi.org/10.1007/978-3-642-33715-4_24"', '"We investigate the problem of finding the correspondence from multiple images, which is a challenging combinatorial problem. In this work, we propose a robust solution by exploiting the priors that the rank of the ordered patterns from a set of linearly correlated images should be lower than that of the disordered patterns, and the errors among the reordered patterns are sparse. This problem is equivalent to find a set of optimal partial permutation matrices for the disordered patterns such that the rearranged patterns can be factorized as a sum of a low rank matrix and a sparse error matrix. A scalable algorithm is proposed to approximate the solution by solving two sub-problems sequentially: minimization of the sum of nuclear norm and l 1 norm for solving relaxed partial permutation matrices, followed by a binary integer programming to project each relaxed partial permutation matrix to the feasible solution. We verify the efficacy and robustness of the proposed method with extensive experiments with both images and videos."'),
('"Finding Deformable Shapes Using Loopy Belief Propagation"', '"ECCV 2002"', '["Grayscale Image", "Background Clutter", "Hand Shape", "Reference Shape", "Edge Strength"]', '"https://doi.org/10.1007/3-540-47977-5_30"', '"A novel deformable template is presented which detects and localizes shapes in grayscale images. The template is formulated as a Bayesian graphical model of a two-dimensional shape contour, and it is matched to the image using a variant of the belief propagation (BP) algorithm used for inference on graphical models. The algorithm can localize a target shape contour in a cluttered image and can accommodate arbitrary global translation and rotation of the target as well as significant shape deformations, without requiring the template to be initialized in any special way (e.g. near the target)."'),
('"Finding People Using Scale, Rotation and Articulation Invariant Matching"', '"ECCV 2012"', '["Human pose", "scale and rotation invariant matching", "global optimization"]', '"https://doi.org/10.1007/978-3-642-33765-9_28"', '"A scale, rotation and articulation invariant method is proposed to match human subjects in images. Different from the widely used pictorial structure scheme, the proposed method directly matches body parts to image regions which are obtained from object independent proposals and successively merged superpixels. Body part region matching is formulated as a graph matching problem. We globally assign a body part candidate to each node on the model graph so that the overall configuration satisfies the spatial layout of a human body plan, part regions have small overlap, and the part coverage follows proper area ratios. The proposed graph model is non-tree and contains high order hyper-edges. We propose an efficient method that finds global optimal solution to the matching problem with a sequence of branch and bound procedures. The experiments show that the proposed method is able to handle arbitrary scale, rotation, articulation and match human subjects in cluttered images."'),
('"Finding Semantic Structures in Image Hierarchies Using Laplacian Graph Energy"', '"ECCV 2010"', '["Regular Graph", "Graph Complexity", "Maximally Stable Extremal Region", "Graph Energy", "Benchmark', '"https://doi.org/10.1007/978-3-642-15561-1_50"', '"Many segmentation algorithms describe images in terms of a hierarchy of regions. Although such hierarchies can produce state of the art segmentations and have many applications, they often contain more data than is required for an efficient description. This paper shows Laplacian graph energy is a generic measure that can be used to identify semantic structures within hierarchies, independently of the algorithm that produces them. Quantitative experimental validation using hierarchies from two state of art algorithms show we can reduce the number of levels and regions in a hierarchy by an order of magnitude with little or no loss in performance when compared against human produced ground truth. We provide a tracking application that illustrates the value of reduced hierarchies."'),
('"Finding the Exact Rotation between Two Images Independently of the Translation"', '"ECCV 2012"', '["Rotation Matrix", "Rotation Matrice", "Visual Odometry", "Pure Rotation", "Epipolar Constraint"]', '"https://doi.org/10.1007/978-3-642-33783-3_50"', '"In this paper, we present a new epipolar constraint for computing the rotation between two images independently of the translation. Against the common belief in the field of geometric vision that it is not possible to find one independently of the other, we show how this can be achieved by relatively simple two-view constraints. We use the fact that translation and rotation cause fundamentally different flow fields on the unit sphere centered around the camera. This allows to establish independent constraints on translation and rotation, and the latter is solved using the Gr\\u00f6bner basis method. The rotation computation is completed by a solution to the cheiriality problem that depends neither on translation, nor on feature triangulations. Notably, we show for the first time how the constraint on the rotation has the advantage of remaining exact even in the case of translations converging to zero. We use this fact in order to remove the error caused by model selection via a non-linear optimization of rotation hypotheses. We show that our method operates in real-time and compare it to a standard existing approach in terms of both speed and accuracy."'),
('"Finding the Largest Unambiguous Component of Stereo Matching"', '"ECCV 2002"', '["Inhibition Zone", "Match Problem", "Stable Match", "Stereo Match", "Maximum Cardinality"]', '"https://doi.org/10.1007/3-540-47977-5_59"', '"Stereo matching is an ill-posed problem for at least two principal reasons: (1) because of the random nature of match similarity measure and (2) because of structural ambiguity due to repetitive patterns. Both ambiguities require the problem to be posed in the regularization framework. Continuity is a natural choice for a prior model. But this model may fail in low signal-to-noise ratio regions. The resulting artefacts may then completely spoil the subsequent visual task."'),
('"Fingerprint Distortion Measurement"', '"BioAW 2004"', '[]', '"https://doi.org/10.1007/978-3-540-25976-3_11"', '"A method for measuring deformations inside single fingerprint images is presented. State-of-the-art fingerprint recognition systems still use affine, to scale represented images. Therefore, image data from so-called sweep sensors is reconstructed to obtain conventional images. Measuring the deformation of fingerprint images facilitates other matching approaches not requiring the finger speed information, e.g. using directly concatenated slice images. Furthermore, fingerprint image compression can be realized by deleting similar content since the images can be reconstructed using deformation measurement."'),
('"Fingerprint Matching Using Feature Space Correlation"', '"BioAW 2002"', '["Query Image", "Dynamic Time Warping", "Fingerprint Image", "Core Point", "Ridge Structure"]', '"https://doi.org/10.1007/3-540-47917-1_6"', '"We present a novel fingerprint alignment and matching scheme that utilizes ridge feature maps to represent, align and match fingerprint images. The technique described here obviates the need for extracting minutiae points or the core point to either align or match fingerprint images. The proposed scheme examines the ridge strength (in local neighborhoods of the fingerprint image) at various orientations, using a set of 8 Gabor filters, whose spatial frequencies correspond to the average inter-ridge spacing in fingerprints. A standard deviation map corresponding to the variation in local pixel intensities in each of the 8 filtered images, is generated. The standard deviation map is sampled at regular intervals in both the horizontal and vertical directions, to construct the ridge feature map. The ridge feature map provides a compact fixed-length representation for a fingerprint image. When a query print is presented to the system, the standard deviation map of the query image and the ridge feature map of the template are correlated, in order to determine the translation offsets necessary to align them. Based on the translation offsets, a matching score is generated by computing the Euclidean distance between the aligned feature maps. Feature extraction and matching takes ~ 1 second in a Pentium III, 800 MHz processor. Combining the matching score generated by the proposed technique with that obtained from a minutiae-based matcher results in an overall improvement in performance of a fingerprint matching system."'),
('"Fingerprint Minutiae: A Constructive Definition"', '"BioAW 2002"', '[]', '"https://doi.org/10.1007/3-540-47917-1_7"', '"The flow pattern of ridges in a fingerprint is unique to the person in that no two people with the same fingerprints have yet been found. Fingerprints have been in use in forensic applications for many years and, more recently, in computer-automated identification and authentication. For automated fingerprint image matching, a machine representation of a fingerprint image is often a set of minutiae in the print; a minimal, but fundamental, representation is just a set of ridge endings and bifurcations. Oddly, however, after all the years of using minutiae, a precise definition of minutiae has never been formulated. We provide a formal definition of a minutia based on the gray scale image. This definition is constructive, in that, given a minutia image, the minutia location and orientation can be uniquely determined."'),
('"Fingerprint Verification by Decision-Level Fusion of Optical and Capacitive Sensors"', '"BioAW 2004"', '[]', '"https://doi.org/10.1007/978-3-540-25976-3_28"', '"Although some papers argued that multi-sensor fusion could improve performances and robustness of fingerprint verification systems, no previous work explicitly dealt with such topic, and no experimental evidence has been reported. In this paper, we show by experiments that a significant performance improvement can be obtained by decision-level fusion of two well-known fingerprint capture devices. As, to the best of our knowledge, this is the first work on multi-sensor fingerprint fusion, we believe that it can contribute to stimulate further researches on this promising topic."'),
('"First International Workshop on Video Segmentation - Panel Discussion"', '"ECCV 2014"', '["Video segmentation", "Computer vision"]', '"https://doi.org/10.1007/978-3-319-16220-1_27"', '"Interest in video segmentation has grown significantly in recent years, resulting in a large body of works along with advances in both methods and datasets. Progress in video segmentation would enable new approaches to building 3D object models from video, understanding dynamic scenes, robot-object interaction and several other high-level vision tasks. The workshop brought together a broad and representative group of video segmentation researchers working on a wide range of topics. This paper summarizes the panel discussion at the workshop, which focused on three questions: (1) Why does video segmentation currently not meet the performance of image segmentation and what difficulties prevent it from leveraging motion? (2) Is video segmentation a stand-alone problem or should it rather be addressed in combination with recognition and reconstruction? (3) Which are the right video segmentation subtasks the field should focus on, and how can we measure progress?"'),
('"Flexible Depth of Field Photography"', '"ECCV 2008"', '["Point Spread Function", "Detector Motion", "Image Integration", "Image Detector", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-540-88693-8_5"', '"The range of scene depths that appear focused in an image is known as the depth of field (DOF). Conventional cameras are limited by a fundamental trade-off between depth of field and signal-to-noise ratio (SNR). For a dark scene, the aperture of the lens must be opened up to maintain SNR, which causes the DOF to reduce. Also, today\\u2019s cameras have DOFs that correspond to a single slab that is perpendicular to the optical axis. In this paper, we present an imaging system that enables one to control the DOF in new and powerful ways. Our approach is to vary the position and/or orientation of the image detector, during the integration time of a single photograph. Even when the detector motion is very small (tens of microns), a large range of scene depths (several meters) is captured both in and out of focus."'),
('"Flexible Voxels for Motion-Aware Videography"', '"ECCV 2010"', '["Motion Information", "Successive Frame", "Motion Blur", "Global Illumination", "High Dynamic Range', '"https://doi.org/10.1007/978-3-642-15549-9_8"', '"The goal of this work is to build video cameras whose spatial and temporal resolutions can be changed post-capture depending on the scene. Building such cameras is difficult due to two reasons. First, current video cameras allow the same spatial resolution and frame rate for the entire captured spatio-temporal volume. Second, both these parameters are fixed before the scene is captured. We propose different components of video camera design: a sampling scheme, processing of captured data and hardware that offer post-capture variable spatial and temporal resolutions, independently at each image location. Using the motion information in the captured data, the correct resolution for each location is decided automatically. Our techniques make it possible to capture fast moving objects without motion blur, while simultaneously preserving high-spatial resolution for static scene parts within the same video sequence. Our sampling scheme requires a fast per-pixel shutter on the sensor-array, which we have implemented using a co-located camera-projector system."'),
('"Floor Fields for Tracking in High Density Crowd Scenes"', '"ECCV 2008"', '["Particle Image Velocimetry", "Tracking Error", "Tracking Algorithm", "Track Length", "Tracking Fai', '"https://doi.org/10.1007/978-3-540-88688-4_1"', '"This paper presents an algorithm for tracking individual targets in high density crowd scenes containing hundreds of people. Tracking in such a scene is extremely challenging due to the small number of pixels on the target, appearance ambiguity resulting from the dense packing, and severe inter-object occlusions. The novel tracking algorithm, which is outlined in this paper, will overcome these challenges using a scene structure based force model. In this force model an individual, when moving in a particular scene, is subjected to global and local forces that are functions of the layout of that scene and the locomotive behavior of other individuals in the scene. The key ingredients of the force model are three floor fields, which are inspired by the research in the field of evacuation dynamics, namely Static Floor Field (SFF), Dynamic Floor Field (DFF), and Boundary Floor Field (BFF). These fields determine the probability of move from one location to another by converting the long-range forces into local ones. The SFF specifies regions of the scene which are attractive in nature (e.g. an exit location). The DFF specifies the immediate behavior of the crowd in the vicinity of the individual being tracked. The BFF specifies influences exhibited by the barriers in the scene (e.g. walls, no-go areas). By combining cues from all three fields with the available appearance information, we track individual targets in high density crowds."'),
('"Flow Counting Using Realboosted Multi-sized Window Detectors"', '"ECCV 2012"', '["Object Detection", "Local Binary Pattern", "Lookup Table", "Face Detection", "Bilinear Interpolati', '"https://doi.org/10.1007/978-3-642-33885-4_20"', '"One classic approach to real-time object detection is to use adaboost to a train a set of look up tables of discrete features. By utilizing a discrete feature set, from features such as local binary patterns, efficient classifiers can be designed. However, these classifiers include interpolation operations while scaling the images over various scales. In this work, we propose the use of real valued weak classifiers which are designed on different scales in order to avoid costly interpolations. The use of real valued weak classifiers in combination with the proposed method avoiding interpolation leads to substantially faster detectors compared to baseline detectors. Furthermore, we investigate the speed and detection performance of such classifiers and their impact on tracking performance. Results indicate that the realboost framework combined with the proposed scaling framework achieves an 80% speed up over adaboost with bilinear interpolation."'),
('"Food-101 \\u2013 Mining Discriminative Components with Random Forests"', '"ECCV 2014"', '["Image classification", "Discriminative part mining", "Random Forest", "Food recognition"]', '"https://doi.org/10.1007/978-3-319-10599-4_29"', '"In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101\\u2019000 images. With an average accuracy of 50.76%, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88% and 8.13%, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods."'),
('"Force/Vision Based Active Damping Control of Contact Transition in Dynamic Environments"', '"WDV 2006"', '["Contact Force", "Force Control", "British Machine Vision Conf", "Rigid Manipulator", "Rigid Robot"', '"https://doi.org/10.1007/978-3-540-70932-9_23"', '"When a manipulator interacts with objects with poorly damped oscillatory modes, undesired oscillations and bouncing may result. In this paper, we present a method for observer-based control of a rigid manipulator interacting with an environment with linear dynamics. The controller injects a desired damping into the environment dynamics, using both visual- and force sensing for stable control of the contact transition. Stability of the system is shown using an observer-based backstepping design method, and simulations and experiments are performed in order to validate the chosen approach."'),
('"Foreground Consistent Human Pose Estimation Using Branch and Bound"', '"ECCV 2014"', '["Priority Queue", "Foreground Region", "Pairwise Term", "Global Branch", "Pose Estimation Problem"]', '"https://doi.org/10.1007/978-3-319-10602-1_21"', '"We propose a method for human pose estimation which extends common unary and pairwise terms of graphical models with a global foreground term. Given knowledge of per pixel foreground, a pose should not only be plausible according to the graphical model but also explain the foreground well."'),
('"Fourier Analysis of the 2D Screened Poisson Equation for Gradient Domain Problems"', '"ECCV 2008"', '["Discrete Cosine Transform", "Discrete Fourier Transform", "Data Term", "Fourier Domain", "Photomet', '"https://doi.org/10.1007/978-3-540-88688-4_9"', '"We analyze the problem of reconstructing a 2D function that approximates a set of desired gradients and a data term. The combined data and gradient terms enable operations like modifying the gradients of an image while staying close to the original image. Starting with a variational formulation, we arrive at the \\u201cscreened Poisson equation\\u201d known in physics. Analysis of this equation in the Fourier domain leads to a direct, exact, and efficient solution to the problem. Further analysis reveals the structure of the spatial filters that solve the 2D screened Poisson equation and shows gradient scaling to be a well-defined sharpen filter that generalizes Laplacian sharpening, which itself can be mapped to gradient domain filtering. Results using a DCT-based screened Poisson solver are demonstrated on several applications including image blending for panoramas, image sharpening, and de-blocking of compressed images."'),
('"Fourier Kernel Learning"', '"ECCV 2012"', '["Fourier Domain", "Kernel Parameter", "Feature Channel", "Multiple Kernel", "Multiple Kernel Learni', '"https://doi.org/10.1007/978-3-642-33709-3_33"', '"Approximations based on random Fourier embeddings have recently emerged as an efficient and formally consistent methodology to design large-scale kernel machines [23]. By expressing the kernel as a Fourier expansion, features are generated based on a finite set of random basis projections, sampled from the Fourier transform of the kernel, with inner products that are Monte Carlo approximations of the original non-linear model. Based on the observation that different kernel-induced Fourier sampling distributions correspond to different kernel parameters, we show that a scalable optimization process in the Fourier domain can be used to identify the different frequency bands that are useful for prediction on training data. This approach allows us to design a family of linear prediction models where we can learn the hyper-parameters of the kernel together with the weights of the feature vectors jointly. Under this methodology, we recover efficient and scalable linear reformulations for both single and multiple kernel learning. Experiments show that our linear models produce fast and accurate predictors for complex datasets such as the Visual Object Challenge 2011 and ImageNet ILSVRC 2011."'),
('"FPM: Fine Pose Parts-Based Model with 3D CAD Models"', '"ECCV 2014"', '["Object Detection", "Real Image", "Neural Information Processing System", "Objectness Score", "Defo', '"https://doi.org/10.1007/978-3-319-10599-4_31"', '"We introduce a novel approach to the problem of localizing objects in an image and estimating their fine-pose. Given exact CAD models, and a few real training images with aligned models, we propose to leverage the geometric information from CAD models and appearance information from real images to learn a model that can accurately estimate fine pose in real images. Specifically, we propose FPM, a fine pose parts-based model, that combines geometric information in the form of shared 3D parts in deformable part based models, and appearance information in the form of objectness to achieve both fast and accurate fine pose estimation. Our method significantly outperforms current state-of-the-art algorithms in both accuracy and speed."'),
('"Free Hand-Drawn Sketch Segmentation"', '"ECCV 2012"', '["Segmentation Algorithm", "Perceptual Grouping", "Meaningful Object", "Word Distribution", "Short S', '"https://doi.org/10.1007/978-3-642-33718-5_45"', '"In this paper, we study the problem of how to segment a freehand sketch at the object level. By carefully considering the basic principles of human perceptual organization, a real-time solution is presented to automatically segment a user\\u2019s sketch during his/her drawing. First, a graph-based sketch segmentation algorithm is proposed to segment a cluttered sketch into multiple parts based on the factor of proximity. Then, to improve the ability of detecting semantically meaningful objects, a semantic-based approach is introduced to simulate the past experience in the perceptual system by leveraging a web-scale clipart database. Finally, other important factors learnt from past experience, such as similarity, symmetry, direction, and closure, are also taken into account to make the approach more robust and practical. The proposed sketch segmentation framework has ability to handle complex sketches with overlapped objects. Extensive experimental results show the effectiveness of the proposed framework and algorithms."'),
('"Free-Shape Polygonal Object Localization"', '"ECCV 2014"', '["Elementary Circuit", "Aerial Image", "Object Contour", "Graph Cycle", "Selective Search"]', '"https://doi.org/10.1007/978-3-319-10599-4_21"', '"Polygonal objects are prevalent in man-made scenes. Early approaches to detecting them relied mainly on geometry while subsequent ones also incorporated appearance-based cues. It has recently been shown that this could be done fast by searching for cycles in graphs of line-fragments, provided that the cycle scoring function can be expressed as additive terms attached to individual fragments. In this paper, we propose an approach that eliminates this restriction. Given a weighted line-fragment graph, we use its cyclomatic number to partition the graph into managebly-sized sub-graphs that preserve nodes and edges with a high weight and are most likely to contain object contours. Object contours are then detected as maximally scoring elementary circuits enumerated in each sub-graph. Our approach can be used with any cycle scoring function and multiple candidates that share line fragments can be found. This is unlike in other approaches that rely on a greedy approach to finding candidates. We demonstrate that our approach significantly outperforms the state-of-the-art for the detection of building rooftops in aerial images and polygonal object categories from ImageNet."'),
('"Frequency Analysis of Transient Light Transport with Applications in Bare Sensor Imaging"', '"ECCV 2012"', '["Light transport", "Fourier analysis", "Time of flight", "Lensless imaging"]', '"https://doi.org/10.1007/978-3-642-33718-5_39"', '"Light transport has been analyzed extensively, in both the primal domain and the frequency domain; the latter provides intuition of effects introduced by free space propagation and by optical elements, and allows for optimal designs of computational cameras for tailored, efficient information capture. Here, we relax the common assumption that the speed of light is infinite and analyze free space propagation in the frequency domain considering spatial, temporal, and angular light variation. Using this analysis, we derive analytic expressions for cross-dimensional information transfer and show how this can be exploited for designing a new, time-resolved bare sensor imaging system."'),
('"Frequency-Space Decomposition and Acquisition of Light Transport under Spatially Varying Illuminati', '"ECCV 2012"', '["Discrete Fourier Transform", "Direct Transport", "Component Transport", "Global Illumination", "Li', '"https://doi.org/10.1007/978-3-642-33783-3_43"', '"We show that, under spatially varying illumination, the light transport of diffuse scenes can be decomposed into direct, near-range (subsurface scattering and local inter-reflections) and far-range transports (diffuse inter-reflections). We show that these three component transports are redundant either in the spatial or the frequency domain and can be separated using appropriate illumination patterns. We propose a novel, efficient method to sequentially separate and acquire the component transports. First, we acquire the direct transport by extending the direct-global separation technique from floodlit images to full transport matrices. Next, we separate and acquire the near-range transport by illuminating patterns sampled uniformly in the frequency domain. Finally, we acquire the far-range transport by illuminating low-frequency patterns. We show that theoretically, our acquisition method achieves the lower bound our model places on the required number of patterns. We quantify the savings in number of patterns over the brute force approach. We validate our observations and acquisition method with rendered and real examples throughout."'),
('"From a 2D Shape to a String Structure Using the Symmetry Set"', '"ECCV 2004"', '["String Representation", "Graph Structure", "Medial Axis", "Cusp Point", "Circle Tangent"]', '"https://doi.org/10.1007/978-3-540-24671-8_25"', '"Many attempts have been made to represent families of 2D shapes in a simpler way. These approaches lead to so-called structures as the Symmetry Set (\\\\(\\\\mathcal{SS}\\\\)) and a subset of it, the Medial Axis (\\\\(\\\\mathcal{MA}\\\\))."'),
('"From a Set of Shapes to Object Discovery"', '"ECCV 2010"', '["Dynamic Time Warping", "Object Discovery", "Graph Edge", "Negative Edge", "Image Contour"]', '"https://doi.org/10.1007/978-3-642-15555-0_5"', '"This paper presents an approach to object discovery in a given unlabeled image set, based on mining repetitive spatial configurations of image contours. Contours that similarly deform from one image to another are viewed as collaborating, or, otherwise, conflicting. This is captured by a graph over all pairs of matching contours, whose maximum a posteriori multicoloring assignment is taken to represent the shapes of discovered objects. Multicoloring is conducted by our new Coordinate Ascent Swendsen-Wang cut (CASW). CASW uses the Metropolis-Hastings (MH) reversible jumps to probabilistically sample graph edges, and color nodes. CASW extends SW cut by introducing a regularization in the posterior of multicoloring assignments that prevents the MH jumps to arrive at trivial solutions. Also, CASW seeks to learn parameters of the posterior via maximizing a lower bound of the MH acceptance rate. This speeds up multicoloring iterations, and facilitates MH jumps from local minima. On benchmark datasets, we outperform all existing approaches to unsupervised object discovery."'),
('"From Low-Cost Depth Sensors to CAD: Cross-Domain 3D Shape Retrieval via Regression Tree Fields"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-10590-1_32"', '"The recent advances of low-cost and mobile depth sensors dramatically extend the potential of 3D shape retrieval and analysis. While the traditional research of 3D retrieval mainly focused on searching by a rough 2D sketch or with a high-quality CAD model, we tackle a novel and challenging problem of cross-domain 3D shape retrieval, in which users can use 3D scans from low-cost depth sensors like Kinect as queries to search CAD models in the database. To cope with the imperfection of user-captured models such as model noise and occlusion, we propose a cross-domain shape retrieval framework, which minimizes the potential function of a Conditional Random Field to efficiently generate the retrieval scores. In particular, the potential function consists of two critical components: one unary potential term provides robust cross-domain partial matching and the other pairwise potential term embeds spatial structures to alleviate the instability from model noise. Both potential components are efficiently estimated using random forests with 3D local features, forming a Regression Tree Field framework. We conduct extensive experiments on two recently released user-captured 3D shape datasets and compare with several state-of-the-art approaches on the cross-domain shape retrieval task. The experimental results demonstrate that our proposed method outperforms the competing methods with a significant performance gain."'),
('"From Manifold to Manifold: Geometry-Aware Dimensionality Reduction for SPD Matrices"', '"ECCV 2014"', '["Riemannian geometry", "SPD manifold", "Grassmann manifold", "dimensionality reduction", "visual re', '"https://doi.org/10.1007/978-3-319-10605-2_2"', '"Representing images and videos with Symmetric Positive Definite (SPD) matrices and considering the Riemannian geometry of the resulting space has proven beneficial for many recognition tasks. Unfortunately, computation on the Riemannian manifold of SPD matrices \\u2013especially of high-dimensional ones\\u2013 comes at a high cost that limits the applicability of existing techniques. In this paper we introduce an approach that lets us handle high-dimensional SPD matrices by constructing a lower-dimensional, more discriminative SPD manifold. To this end, we model the mapping from the high-dimensional SPD manifold to the low-dimensional one with an orthonormal projection. In particular, we search for a projection that yields a low-dimensional manifold with maximum discriminative power encoded via an affinity-weighted similarity measure based on metrics on the manifold. Learning can then be expressed as an optimization problem on a Grassmann manifold. Our evaluation on several classification tasks shows that our approach leads to a significant accuracy gain over state-of-the-art methods."'),
('"From Meaningful Contours to Discriminative Object Shape"', '"ECCV 2012"', '["Training Image", "Query Image", "Multiple Instance Learning", "Cluttered Scene", "Object Hypothesi', '"https://doi.org/10.1007/978-3-642-33718-5_55"', '"Shape is a natural, highly prominent characteristic of objects that human vision utilizes everyday. But despite its expressiveness, shape poses significant challenges for category-level object detection in cluttered scenes: Object form is an emergent property that cannot be perceived locally but becomes only available once the whole object has been detected and segregated from the background. Thus we address the detection of objects and the assembling of their shape simultaneously. A dictionary of meaningful contours is obtained by clustering based on contour co-activation in all training images. We seek a joint, consistent placement of all contours in an image, since placing them independently from another is not reliable due to the emergence of shape. Therefore, the characteristic object shape is learned by discovering spatially consistent configurations of all dictionary contours using maximum margin multiple instance learning. During recognition, objects are detected and their shape is explained simultaneously by optimizing a single cost function. We demonstrate the benefit of our approach on standard shape benchmarks."'),
('"From Multiple Views to Textured 3D Meshes: A GPU-Powered Approach"', '"ECCV 2010"', '["Texture Mapping", "Graphic Hardware", "Visual Hull", "Foreground Detection", "Foreground Segmentat', '"https://doi.org/10.1007/978-3-642-35740-4_30"', '"We present work on exploiting modern graphics hardware towards the real-time production of a textured 3D mesh representation of a scene observed by a multicamera system. The employed computational infrastructure consists of a network of four PC workstations each of which is connected to a pair of cameras. One of the PCs is equipped with a GPU that is used for parallel computations. The result of the processing is a list of texture mapped triangles representing the reconstructed surfaces. In contrast to previous works, the entire processing pipeline (foreground segmentation, 3D reconstruction, 3D mesh computation, 3D mesh smoothing and texture mapping) has been implemented on the GPU. Experimental results demonstrate that an accurate, high resolution, texture-mapped 3D reconstruction of a scene observed by eight cameras is achievable in real time."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"From Tensor-Driven Diffusion to Anisotropic Wavelet Shrinkage"', '"ECCV 2006"', '["Diffusion Tensor", "Anisotropic Diffusion", "Structure Tensor", "Rotation Invariance", "Haar Wavel', '"https://doi.org/10.1007/11744023_31"', '"Diffusion processes driven by anisotropic diffusion tensors are known to be well-suited for structure-preserving denoising. However, numerical implementations based on finite differences introduce unwanted blurring artifacts that deteriorate these favourable filtering properties. In this paper we introduce a novel discretisation of a fairly general class of anisotropic diffusion processes on a 2-D grid. It leads to a locally semi-analytic scheme (LSAS) that is absolutely stable, simple to implement and offers an outstanding sharpness of filtered images. By showing that this scheme can be translated into a 2-D Haar wavelet shrinkage procedure, we establish a connection between tensor-driven diffusion and anisotropic wavelet shrinkage for the first time. This result leads to coupled shrinkage rules that allow to perform highly anisotropic filtering even with the simplest wavelets."'),
('"Full Body Performance Capture under Uncontrolled and Varying Illumination: A Shading-Based Approach', '"ECCV 2012"', '["Spherical Harmonic", "Surface Albedo", "Motion Capture", "Photometric Stereo", "Dual Quaternion"]', '"https://doi.org/10.1007/978-3-642-33765-9_54"', '"This paper presents a marker-less method for full body human performance capture by analyzing shading information from a sequence of multi-view images, which are recorded under uncontrolled and changing lighting conditions. Both the articulated motion of the limbs and then the fine-scale surface detail are estimated in a temporally coherent manner. In a temporal framework, differential 3D human pose-changes from the previous time-step are expressed in terms of constraints on the visible image displacements derived from shading cues, estimated albedo and estimated scene illumination. The incident illumination at each frame are estimated jointly with pose, by assuming the Lambertian model of reflectance. The proposed method is independent of image silhouettes and training data, and is thus applicable in cases where background segmentation cannot be performed or a set of training poses is unavailable. We show results on challenging cases for pose-tracking such as changing backgrounds, occlusions and changing lighting conditions."'),
('"Fully Isotropic Fast Marching Methods on Cartesian Grids"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15549-9_6"', '"The existing Fast Marching methods which are used to solve the Eikonal equation use a locally continuous model to estimate the accumulated cost, but a discontinuous (discretized) model for the traveling cost around each grid point. Because the accumulated cost and the traveling (local) cost are treated differently, the estimate of the accumulated cost at any point will vary based on the direction of the arriving front. Instead we propose to estimate the traveling cost at each grid point based on a locally continuous model, where we will interpolate the traveling cost along the direction of the propagating front. We further choose an interpolation scheme that is not biased by the direction of the front. Thus making the fast marching process truly isotropic. We show the significance of removing the directional bias in the computation of the cost in certain applications of fast marching method. We also compare the accuracy and computation times of our proposed methods with the existing state of the art fast marching techniques to demonstrate the superiority of our method."'),
('"Fun with Asynchronous Vision Sensors and Processing"', '"ECCV 2012"', '["Vision Sensor", "Voltage Gain", "CMOS Image Sensor", "Active Pixel Sensor", "Capacitive Divider"]', '"https://doi.org/10.1007/978-3-642-33863-2_52"', '"This paper provides a personal perspective on our group\\u2019s efforts in building event-based vision sensors, algorithms, and applications over the period 2002-2012. Some recent advances from other groups are also briefly described."'),
('"Fusion of Feature- and Area-Based Information for Urban Buildings Modeling from Aerial Imagery"', '"ECCV 2008"', '["Aerial Image", "Aerial Imagery", "Geometric Primitive", "Smoothness Term", "Ground Sampling Distan', '"https://doi.org/10.1007/978-3-540-88693-8_64"', '"Accurate and realistic building models of urban environments are increasingly important for applications, like virtual tourism or city planning. Initiatives like Virtual Earth or Google Earth are aiming at offering virtual models of all major cities world wide. The prohibitively high costs of manual generation of such models explain the need for an automatic workflow."'),
('"Fusion of HMM\\u2019s Likelihood and Viterbi Path for On-line Signature Verification"', '"BioAW 2004"', '["Score Fusion", "Hidden Markov Models", "Viterbi path", "Hamming distance", "on-line signature veri', '"https://doi.org/10.1007/978-3-540-25976-3_29"', '"We describe a method fusing two complementary scores descended from a Hidden Markov Model (HMM) for on-line signature verification. The signatures are acquired using a digitizer that captures pen-position, pen-pressure, and pen-inclination. A writer is considered as being authentic when the arithmetic mean of two similarity scores obtained on an input signature is higher than a threshold. The first score is related to the likelihood given by a HMM modeling the signatures of the claimed identity; the second score is related to the most likely path given by such HMM (Viterbi algorithm) on the input signature. Our approach was evaluated on the BIOMET database (1266 genuine signatures from 87 individuals), as well as on the Philips on-line signature database (1530 signatures from 51 individuals). On the Philips database, we study the influence of the amount of training data, and on the BIOMET database, that of time variability. Several Cross-Validation trials are performed to report robust results. We first compare our system on the Philips database to Dolfing\\u2019s system, on one of his protocols (15 signatures to train the HMM). We reach in these conditions an Equal Error Rate (EER) of 0.95%, compared to an EER of 2.2% previously obtained by Dolfing. When considering only 5 signatures to train the HMM, the best results relying only on the likelihood yield an EER of 6.45% on the BIOMET database, and of 4.18% on the Philips database. The error rates drop to 2.84% on the BIOMET database, and to 3.54% on the Philips database, when fusing both scores by a simple arithmetic mean."'),
('"Fusion of Human Posture Features for Continuous Action Recognition"', '"ECCV 2010"', '["Contiuous Action Recognition", "HMMs", "Fusion of Features"]', '"https://doi.org/10.1007/978-3-642-35749-7_19"', '"This paper presents a real-time or online system for continuous recognition of human actions. The system recognizes actions such as walking, bending, jumping, waving, and falling and relies on spatial features computed to characterize human posture. The paper evaluates the utility of these features based on its joint or independent treatment within the context of the Hidden Markov Model (HMM) framework. A baseline approach wherein disparate spatial features are treated as an input vector to trained HMMs is used to compare three different independent feature models. In addition, an action transition constraints is introduced to stabilize the developed models and allow for continuity in recognized actions. The system is evaluated across a dataset of videos and results reported in terms of frame error rate, the frame delay in recognizing an action, action recognition rate, and the missed and false recognition rates. Experimental results shows the effectiveness of the proposed treatment of input features and the corresponding HMM formulations."'),
('"Fusion of Infrared and Visible Images for Face Recognition"', '"ECCV 2004"', '["Facial Expression", "Face Recognition", "Visible Image", "Face Image", "Recognition Performance"]', '"https://doi.org/10.1007/978-3-540-24673-2_37"', '"A number of studies have demonstrated that infrared (IR) imagery offers a promising alternative to visible imagery due to it\\u2019s insensitive to variations in face appearance caused by illumination changes. IR, however, has other limitations including that it is opaque to glass. The emphasis in this study is on examining the sensitivity of IR imagery to facial occlusion caused by eyeglasses. Our experiments indicate that IR-based recognition performance degrades seriously when eyeglasses are present in the probe image but not in the gallery image and vice versa. To address this serious limitation of IR, we propose fusing the two modalities, exploiting the fact that visible-based recognition is less sensitive to the presence or absence of eyeglasses. Our fusion scheme is pixel-based, operates in the wavelet domain, and employs genetic algorithms (GAs) to decide how to combine IR with visible information. Although our fusion approach was not able to fully discount illumination effects present in the visible images, our experimental results show substantial improvements recognition performance overall, and it deserves further consideration."'),
('"Fusion of LDA and PCA for Face Verification"', '"BioAW 2002"', '["Principal Component Analysis", "Face Recognition", "Linear Discriminant Analysis", "Verification S', '"https://doi.org/10.1007/3-540-47917-1_4"', '"Although face verification systems have proven to be reliable in ideal environments, they can be very sensitive to real environmental conditions. The system robustness can be increased by the fusion of different face verification algorithms. To the best of our knowledge, no face verification system tried exploiting the fusion of LDA and PCA. In our opinion, the apparent strong correlation of LDA and PCA, especially when frontal views are used and PCA is applied before LDA, discouraged the fusion of such algorithms. In this paper, we show that PCA and LDA can be fused with some simple strategies, and such fusion allows outperforming the best individual verification algorithm based on PCA or LDA."'),
('"Fusion of Multiple Tracking Algorithms for Robust People Tracking"', '"ECCV 2002"', '["Visual Surveillance", "People Tracking", "Data Fusion", "PCA"]', '"https://doi.org/10.1007/3-540-47979-1_25"', '"This paper shows how the output of a number of detection and tracking algorithms can be fused to achieve robust tracking of people in an indoor environment. The new tracking system contains three co-operating parts: i) an Active Shape Tracker using a PCA-generated model of pedestrian outline shapes, ii) a Region Tracker, featuring region splitting and merging for multiple hypothesis matching, and iii) a Head Detector to aid in the initialisation of tracks. Data from the three parts are fused together to select the best tracking hypotheses."'),
('"Fusion of Multiple Visual Cues for Visual Saliency Extraction from Wearable Camera Settings with St', '"ECCV 2012"', '["Camera Motion", "Visual Saliency", "Saliency Model", "Wearable Camera", "Global Motion Estimation"', '"https://doi.org/10.1007/978-3-642-33885-4_44"', '"In this paper we are interested in the saliency of visual content from wearable cameras. The subjective saliency in wearable video is studied first due to the psycho-visual experience on this content. Then the method for objective saliency map computation with a specific contribution based on geometrical saliency is proposed. Fusion of spatial, temporal and geometric cues in an objective saliency map is realized by the multiplicative operator. Resulting objective saliency maps are evaluated against the subjective maps with promising results, highlighting interesting performance of proposed geometric saliency model."'),
('"Fusion of Speech, Faces and Text for Person Identification in TV Broadcast"', '"ECCV 2012"', '["Face Recognition", "Speaker Recognition", "Face Track", "Speaker Model", "Person Recognition"]', '"https://doi.org/10.1007/978-3-642-33885-4_39"', '"The Repere challenge is a project aiming at the evaluation of systems for supervised and unsupervised multimodal recognition of people in TV broadcast. In this paper, we describe, evaluate and discuss QCompere consortium submissions to the 2012 Repere evaluation campaign dry-run. Speaker identification (and face recognition) can be greatly improved when combined with name detection through video optical character recognition. Moreover, we show that unsupervised multimodal person recognition systems can achieve performance nearly as good as supervised monomodal ones (with several hundreds of identity models)."'),
('"G3Di: A Gaming Interaction Dataset with a Real Time Detection and Evaluation Framework"', '"ECCV 2014"', '["Human interaction recognition", "Multimodal dataset", "Multiplayer gaming", "Interaction evaluatio', '"https://doi.org/10.1007/978-3-319-16178-5_49"', '"This paper presents a new, realistic and challenging human interaction dataset for multiplayer gaming, containing synchronised colour, depth and skeleton data. In contrast to existing datasets where the interactions are scripted, G3Di was captured using a novel gamesourcing method so the movements are more realistic. Our detection framework decomposes interactions into the actions of each person to infer the interaction in real time. This modular approach is applicable to a virtual environment where the interaction between people occurs through a computer interface. We also propose an evaluation metric for real time applications, which assesses both the accuracy and latency of the interactions. Experimental results indicate higher complexity of the new dataset in comparison to existing gaming datasets."'),
('"Gabor Feature Based Sparse Representation for Face Recognition with Gabor Occlusion Dictionary"', '"ECCV 2010"', '["Face Recognition", "Recognition Rate", "Face Image", "Sparse Representation", "Sparse Code"]', '"https://doi.org/10.1007/978-3-642-15567-3_33"', '"By coding the input testing image as a sparse linear combination of the training samples via l 1-norm minimization, sparse representation based classification (SRC) has been recently successfully used for face recognition (FR). Particularly, by introducing an identity occlusion dictionary to sparsely code the occluded portions in face images, SRC can lead to robust FR results against occlusion. However, the large amount of atoms in the occlusion dictionary makes the sparse coding computationally very expensive. In this paper, the image Gabor-features are used for SRC. The use of Gabor kernels makes the occlusion dictionary compressible, and a Gabor occlusion dictionary computing algorithm is then presented. The number of atoms is significantly reduced in the computed Gabor occlusion dictionary, which greatly reduces the computational cost in coding the occluded face images while improving greatly the SRC accuracy. Experiments on representative face databases with variations of lighting, expression, pose and occlusion demonstrated the effectiveness of the proposed Gabor-feature based SRC (GSRC) scheme."'),
('"Gait Appearance for Recognition"', '"BioAW 2002"', '["Feature Vector", "Gait Feature", "Visual Hull", "Gait Recognition", "Person Recognition"]', '"https://doi.org/10.1007/3-540-47917-1_15"', '"This paper describes a set of representations of gait appearance features for the purpose of person identification. Our gait representation has two stages: the first stage computes a set of image features that are based on moments extracted from orthogonal view video silhouettes of human walking motion; the second stage applies three methods of aggregating these image features over time to create the gait sequence features. Despite their simplicity, the resulting gait sequence feature vectors contain enough information to perform well on human identification. We demonstrate the accuracy of recognition using gait video sequences collected over different days and times, under varying lighting environments and explore the differences in the three time-aggregation methods for the purpose of recognition."'),
('"Gait Recognition by Ranking"', '"ECCV 2012"', '["Gait recognition", "Learning to rank", "Transfer learning"]', '"https://doi.org/10.1007/978-3-642-33718-5_24"', '"The advantage of gait over other biometrics such as face or fingerprint is that it can operate from a distance and without subject cooperation. However, this also makes gait subject to changes in various covariate conditions including carrying, clothing, surface and view angle. Existing approaches attempt to address these condition changes by feature selection, feature transformation or discriminant subspace learning. However, they suffer from lack of training samples from each subject, can only cope with changes in a subset of conditions with limited success, and are based on the invalid assumption that the covariate conditions are known a priori. They are thus unable to perform gait recognition under a genuine uncooperative setting. We propose a novel approach which casts gait recognition as a bipartite ranking problem and leverages training samples from different classes/people and even from different datasets. This makes our approach suitable for recognition under a genuine uncooperative setting and robust against any covariate types, as demonstrated by our extensive experiments."'),
('"Gait Recognition Using a View Transformation Model in the Frequency Domain"', '"ECCV 2006"', '["Gait Analysis", "Gesture Recognition", "View Direction", "Perspective Projection", "Gait Feature"]', '"https://doi.org/10.1007/11744078_12"', '"Gait analyses have recently gained attention as methods of identification of individuals at a distance from a camera. However, appearance changes due to view direction changes cause difficulties for gait recognition systems. Here, we propose a method of gait recognition from various view directions using frequency-domain features and a view transformation model. We first construct a spatio-temporal silhouette volume of a walking person and then extract frequency-domain features of the volume by Fourier analysis based on gait periodicity. Next, our view transformation model is obtained with a training set of multiple persons from multiple view directions. In a recognition phase, the model transforms gallery features into the same view direction as that of an input feature, and so the features match each other. Experiments involving gait recognition from 24 view directions demonstrate the effectiveness of the proposed method."'),
('"Gait Sequence Analysis Using Frieze Patterns"', '"ECCV 2002"', '["Gait Pattern", "Dynamic Time Warping", "Stride Frequency", "Silhouette Image", "Gait Sequence"]', '"https://doi.org/10.1007/3-540-47967-8_44"', '"We analyze walking people using a gait sequence representation that bypasses the need for frame-to-frame tracking of body parts. The gait representation maps a video sequence of silhouettes into a pair of two-dimensional spatio-temporal patterns that are near-periodic along the time axis. Mathematically, such patterns are called \\u201cfrieze\\u201d patterns and associated symmetry groups \\u201cfrieze groups\\u201d. With the help of a walking humanoid avatar, we explore variation in gait frieze patterns with respect to viewing angle, and find that the frieze groups of the gait patterns and their canonical tiles enable us to estimate viewing direction of human walking videos. In addition, analysis of periodic patterns allows us to determine the dynamic time warping and affine scaling that aligns two gait sequences from similar viewpoints. We also show how gait alignment can be used to perform human identification and model-based body part segmentation."'),
('"Gait-Based Person Identification Using Motion Interchange Patterns"', '"ECCV 2014"', '["MIP", "LBP", "Gait recognition", "CASIA", "TUMGAID"]', '"https://doi.org/10.1007/978-3-319-16181-5_7"', '"Understanding human motion in unconstrained 2D videos has been a central theme in Computer Vision research, and over the years many attempts have been made to design effective representations of video content. In this paper, we apply to gait recognition the Motion Interchange Patterns (MIP) framework, a 3D extension of the LBP descriptors to videos that was successfully employed in action recognition. This effective framework encodes motion by capturing local changes in motion directions. Our scheme does not rely on silhouettes commonly used in gait recognition, and benefits from the capability of MIP encoding to model real world videos. We empirically demonstrate the effectiveness of this modeling of human motion on several challenging gait recognition datasets."'),
('"Galilean Differential Geometry of Moving Images"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_40"', '"In this paper we develop a systematic theory about local structure of moving images in terms of Galilean differential invariants. We argue that Galilean invariants are useful for studying moving images as they disregard constant motion that typically depends on the motion of the observer or the observed object, and only describe relative motion that might capture surface shape and motion boundaries. The set of Galilean invariants for moving images also contains the Euclidean invariants for (still) images. Complete sets of Galilean invariants are derived for two main cases: when the spatio-temporal gradient cuts the image plane and when it is tangent to the image plane. The former case correspond to isophote curve motion and the later to creation and disappearance of image structure, a case that is not well captured by the theory of optical flow. The derived invariants are shown to be describable in terms of acceleration, divergence, rotation and deformation of image structure. The described theory is completely based on bottom up computation from local spatio-temporal image information."'),
('"Gaussian-Like Spatial Priors for Articulated Tracking"', '"ECCV 2010"', '["Tangent Space", "Joint Angle", "Kinematic Chain", "Average Standard Deviation", "Likelihood Model"', '"https://doi.org/10.1007/978-3-642-15549-9_31"', '"We present an analysis of the spatial covariance structure of an articulated motion prior in which joint angles have a known covariance structure. From this, a well-known, but often ignored, deficiency of the kinematic skeleton representation becomes clear: spatial variance not only depends on limb lengths, but also increases as the kinematic chains are traversed. We then present two similar Gaussian-like motion priors that are explicitly expressed spatially and as such avoids any variance coming from the representation. The resulting priors are both simple and easy to implement, yet they provide superior predictions."'),
('"gDLS: A Scalable Solution to the Generalized Pose and Scale Problem"', '"ECCV 2014"', '["Image Noise", "Similarity Transformation", "Polynomial System", "Multiple Camera", "Scalable Solut', '"https://doi.org/10.1007/978-3-319-10593-2_2"', '"In this work, we present a scalable least-squares solution for computing a seven degree-of-freedom similarity transform. Our method utilizes the generalized camera model to compute relative rotation, translation, and scale from four or more 2D-3D correspondences. In particular, structure and motion estimations from monocular cameras lack scale without specific calibration. As such, our methods have applications in loop closure in visual odometry and registering multiple structure from motion reconstructions where scale must be recovered. We formulate the generalized pose and scale problem as a minimization of a least squares cost function and solve this minimization without iterations or initialization. Additionally, we obtain all minima of the cost function. The order of the polynomial system that we solve is independent of the number of points, allowing our overall approach to scale favorably. We evaluate our method experimentally on synthetic and real datasets and demonstrate that our methods produce higher accuracy similarity transform solutions than existing methods."'),
('"Gender Classification from Iris Images Using Fusion of Uniform Local Binary Patterns"', '"ECCV 2014"', '["Biometrics", "Iris", "LBP", "Gender classification"]', '"https://doi.org/10.1007/978-3-319-16181-5_57"', '"This paper is concerned in analyzing iris texture in order to determine \\u201csoft biometric\\u201d, attributes of a person, rather than identity. In particular, this paper is concerned with predicting the gender of a person based on analysis of features of the iris texture. Previous researchers have explored various approaches for predicting the gender of a person based on iris texture. We explore using different implementations of Local Binary Patterns from the iris image using the masked information. Uniform LBP with concatenated histograms significantly improves accuracy of gender prediction relative to using the whole iris image. Using a subject-disjoint test set, we are able to achieve over 91 % correct gender prediction using the texture of the iris. To our knowledge, this is the highest accuracy yet achieved for predicting gender from iris texture."'),
('"Gender Recognition Using Cognitive Modeling"', '"ECCV 2012"', '["Gender recognition", "Linear Discriminant Analysis", "Support Vector Machines", "Cognitive Modelin', '"https://doi.org/10.1007/978-3-642-33868-7_30"', '"In this work, we use cognitive modeling to estimate the \\u201dgender strength\\u201d of frontal faces, a continuous class variable, superseding the traditional binary class labeling. To incorporate this continuous variable we suggest a novel linear gender classification algorithm, the Gender Strength Regression. In addition, we use the gender strength to construct a smaller but refined training set, by identifying and removing ill-defined training examples. We use this refined training set to improve the performance of known classification algorithms. Also the human performance of known data sets is reported, and surprisingly it seems to be quite a hard task for humans. Finally our results are reproduced on a data set of above 40,000 public Danish LinkedIN profile pictures."'),
('"General and Nested Wiberg Minimization: L2 and Maximum Likelihood"', '"ECCV 2012"', '["Matrix Factorization", "Bundle Adjustment", "Projective Depth", "Simultaneous Minimization", "Glob', '"https://doi.org/10.1007/978-3-642-33786-4_15"', '"Wiberg matrix factorization breaks a matrix Y into low-rank factors U and V by solving for V in closed form given U, linearizing V(U) about U, and iteratively minimizing ||Y\\u2009\\u2212\\u2009UV(U)||2 with respect to U only. This approach factors the matrix while effectively removing V from the minimization. We generalize the Wiberg approach beyond factorization to minimize an arbitrary function that is nonlinear in each of two sets of variables. In this paper we focus on the case of L 2 minimization and maximum likelihood estimation (MLE), presenting an L 2 Wiberg bundle adjustment algorithm and a Wiberg MLE algorithm for Poisson matrix factorization. We also show that one Wiberg minimization can be nested inside another, effectively removing two of three sets of variables from a minimization. We demonstrate this idea with a nested Wiberg algorithm for L 2 projective bundle adjustment, solving for camera matrices, points, and projective depths."'),
('"General Imaging Geometry for Central Catadioptric Cameras"', '"ECCV 2008"', '["Image Point", "Fundamental Matrix", "Line Complex", "Epipolar Geometry", "Perspective Camera"]', '"https://doi.org/10.1007/978-3-540-88693-8_45"', '"Catadioptric cameras are a popular type of omnidirectional imaging system. Their imaging and multi-view geometry has been extensively studied; epipolar geometry for instance, is geometrically speaking, well understood. However, the existence of a bilinear matching constraint and an associated fundamental matrix, has so far only been shown for the special case of para-catadioptric cameras (consisting of a paraboloidal mirror and an orthographic camera). The main goal of this work is to obtain such results for all central catadioptric cameras. Our main result is to show the existence of a general 15\\u00d715 fundamental matrix. This is based on and completed by a number of other results, e.g. the formulation of general catadioptric projection matrices and plane homographies."'),
('"General Linear Cameras"', '"ECCV 2004"', '["Characteristic Equation", "Camera Model", "Pinhole Camera", "Congruent Triangle", "Uniform Directi', '"https://doi.org/10.1007/978-3-540-24671-8_2"', '"We present a General Linear Camera (GLC) model that unifies many previous camera models into a single representation. The GLC model is capable of describing all perspective (pinhole), orthographic, and many multiperspective (including pushbroom and two-slit) cameras, as well as epipolar plane images. It also includes three new and previously unexplored multiperspective linear cameras. Our GLC model is both general and linear in the sense that, given any vector space where rays are represented as points, it describes all 2D affine subspaces (planes) that can be formed by affine combinations of 3 rays. The incident radiance seen along the rays found on subregions of these 2D affine subspaces are a precise definition of a projected image of a 3D scene. The GLC model also provides an intuitive physical interpretation, which can be used to characterize real imaging systems. Finally, since the GLC model provides a complete description of all 2D affine subspaces, it can be used as a tool for first-order differential analysis of arbitrary (higher-order) multiperspective imaging systems."'),
('"General Trajectory Triangulation"', '"ECCV 2002"', '["Structure from motion"]', '"https://doi.org/10.1007/3-540-47967-8_55"', '"The multiple view geometry of static scenes is now well understood. Recently attention was turned to dynamic scenes where scene points may move while the cameras move. The triangulation of linear trajectories is now well handled. The case of quadratic trajectories also received some attention."'),
('"Generalised Pose Estimation Using Depth"', '"ECCV 2010"', '["pose", "depth", "stereo", "head", "hand", "classification", "particle filter", "gesture", "lbp", "', '"https://doi.org/10.1007/978-3-642-35749-7_24"', '"Estimating the pose of an object, be it articulated, deformable or rigid, is an important task, with applications ranging from Human-Computer Interaction to environmental understanding. The idea of a general pose estimation framework, capable of being rapidly retrained to suit a variety of tasks, is appealing. In this paper a solution is proposed requiring only a set of labelled training images in order to be applied to many pose estimation tasks. This is achieved by treating pose estimation as a classification problem, with particle filtering used to provide non-discretised estimates. Depth information extracted from a calibrated stereo sequence, is used for background suppression and object scale estimation. The appearance and shape channels are then transformed to Local Binary Pattern histograms, and pose classification is performed via a randomised decision forest. To demonstrate flexibility, the approach is applied to two different situations, articulated hand pose and rigid head orientation, achieving 97% and 84% accurate estimation rates, respectively."'),
('"Generalized Background Subtraction Using Superpixels with Label Integrated Motion Estimation"', '"ECCV 2014"', '["generalized background subtraction", "superpixel segmentation", "density propagation", "layered op', '"https://doi.org/10.1007/978-3-319-10602-1_12"', '"We propose an online background subtraction algorithm with superpixel-based density estimation for videos captured by moving camera. Our algorithm maintains appearance and motion models of foreground and background for each superpixel, computes foreground and background likelihoods for each pixel based on the models, and determines pixelwise labels using binary belief propagation. The estimated labels trigger the update of appearance and motion models, and the above steps are performed iteratively in each frame. After convergence, appearance models are propagated through a sequential Bayesian filtering, where predictions rely on motion fields of both labels whose computation exploits the segmentation mask. Superpixel-based modeling and label integrated motion estimation make propagated appearance models more accurate compared to existing methods since the models are constructed on visually coherent regions and the quality of estimated motion is improved by avoiding motion smoothing across regions with different labels. We evaluate our algorithm with challenging video sequences and present significant performance improvement over the state-of-the-art techniques quantitatively and qualitatively."'),
('"Generalized Connectivity Constraints for Spatio-temporal 3D Reconstruction"', '"ECCV 2014"', '["connectivity constraints", "spatio-temporal 3D reconstruction"]', '"https://doi.org/10.1007/978-3-319-10593-2_3"', '"This paper introduces connectivity preserving constraints into spatio-temporal multi-view reconstruction. We efficiently model connectivity constraints by precomputing a geodesic shortest path tree on the occupancy likelihood. Connectivity of the final occupancy labeling is ensured with a set of linear constraints on the labeling function. In order to generalize the connectivity constraints from objects with genus 0 to an arbitrary genus, we detect loops by analyzing the visual hull of the scene. A modification of the constraints ensures connectivity in the presence of loops. The proposed efficient implementation adds little runtime and memory overhead to the reconstruction method. Several experiments show significant improvement over state-of-the-art methods and validate the practical use of this approach in scenes with fine structured details."'),
('"Generalized Histogram: Empirical Optimization of Low Dimensional Features for Image Matching"', '"ECCV 2004"', '["Video Segment", "Normalize Cross Correlation", "Adaptive Weighting", "Sample Expansion", "Video Ar', '"https://doi.org/10.1007/978-3-540-24672-5_17"', '"We propose Generalized Histogram as low-dimensional representation of an image for efficient and precise image matching. Multiplicity detection of videos in broadcast video archives is getting important for many video-based applications including commercial film identification, unsupervised video parsing and structuring, and robust highlight shot detection. This inherently requires efficient and precise image matching among extremely huge number of images. Histogram-based image similarity search and matching is known to be effective, and its enhancement techniques such as adaptive binning, subregion histogram, and adaptive weighting have been studied. We show that these techniques can be represented as linear conversion of high-dimensional primitive histograms and can be integrated into generalized histograms. A linear learning method to obtain generalized histograms from sample sets is presented with a sample expansion technique to circumvent the overfitting problem due to high-dimensionality and insufficient sample size. The generalized histogram takes advantage of these techniques, and achieves more than 90% precision and recall with 16-D generalized histogram compared to the ground truth computed by normalized cross correlation. The practical importance of the work is revealed by successful matching performance with 20,000 frame images obtained from actual broadcast videos."'),
('"Generalized Multi-sensor Planning"', '"ECCV 2006"', '["Static Constraint", "Quality Function", "Face Detection", "Soft Constraint", "Stereo Match"]', '"https://doi.org/10.1007/11744023_41"', '"Vision systems for various tasks are increasingly being deployed. Although significant effort has gone into improving the algorithms for such tasks, there has been relatively little work on determining optimal sensor configurations. This paper addresses this need. We specifically address and enhance the state-of-the-art in the analysis of scenarios where there are dynamically occuring objects capable of occluding each other. The visibility constraints for such scenarios are analyzed in a multi-camera setting. Also analyzed are other static constraints such as image resolution and field-of-view, and algorithmic requirements such as stereo reconstruction, face detection and background appearance. Theoretical analysis with the proper integration of such visibility and static constraints leads to a generic framework for sensor planning, which can then be customized for a particular task. Our analysis can be applied to a variety of applications, especially those involving randomly occuring objects, and include surveillance and industrial automation. Several examples illustrate the wide applicability of the approach."'),
('"Generalized Rank Conditions in Multiple View Geometry with Applications to Dynamical Scenes"', '"ECCV 2002"', '["multiple view geometry", "rank condition", "multiple view matrix", "dynamical scenes", "segmentati', '"https://doi.org/10.1007/3-540-47967-8_14"', '"In this paper, the geometry of a general class of projections from \\u211dn to \\u211dk (k < n) is examined, as a generalization of classic multiple view geometry in computer vision. It is shown that geometric constraints that govern multiple images of hyperplanes in \\u211dn, as well as any incidence conditions among these hyperplanes (such as inclusion, intersection, and restriction), can be systematically captured through certain rank conditions on the so-called multiple view matrix. All constraints known or unknown in computer vision for the projection from \\u211d3 to \\u211d2 are simply instances of this result. It certainly simplifies current efforts to extending classic multiple view geometry to dynamical scenes. It also reveals that since most new constraints in spaces of higher dimension are nonlinear, the rank conditions are a natural replacement for the traditional multilinear analysis. We also demonstrate that the rank conditions encode extremely rich information about dynamical scenes and they give rise to fundamental criteria for purposes such as stereopsis in n-dimensional space, segmentation of dynamical features, detection of spatial and temporal formations, and rejection of occluding T-junctions."'),
('"Generalized Roof Duality for Multi-Label Optimization: Optimal Lower Bounds and Persistency"', '"ECCV 2012"', '["multi-label", "higher-order", "roof duality", "MRF", "computer vision"]', '"https://doi.org/10.1007/978-3-642-33783-3_29"', '"We extend the concept of generalized roof duality from pseudo-boolean functions to real-valued functions over multi-label variables. In particular, we prove that an analogue of the persistency property holds for energies of any order with any number of linearly ordered labels. Moreover, we show how the optimal submodular relaxation can be constructed in the first-order case."'),
('"Generation and Application of Hyperspectral 3D Plant Models"', '"ECCV 2014"', '["Hyperspectral", "3D scanning", "Close range", "Phenotyping", "Modeling", "Sensor fusion"]', '"https://doi.org/10.1007/978-3-319-16220-1_9"', '"Hyperspectral imaging sensors have been introduced for measuring the health status of plants. Recently, they have been also used for close-range sensing of plant canopies with a more complex architecture. The complex geometry of plants and their interaction with the illumination scenario severely affect the spectral information obtained. The combination of hyperspectral images and 3D point clouds are a promising approach to face this problem. Based on such hyperspectral 3D models the effects of plant geometry and sensor configuration can be quantified an modeled. Reflectance models can be used to remove or weaken the geometry-related effects in hyperspectral images and, therefore, have the potential potential to improve automated phenotyping significantly."'),
('"Generative Image Segmentation Using Random Walks with Restart"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_20"', '"We consider the problem of multi-label, supervised image segmentation when an initial labeling of some pixels is given. In this paper, we propose a new generative image segmentation algorithm for reliable multi-label segmentations in natural images. In contrast to most existing algorithms which focus on the inter-label discrimination, we address the problem of finding the generative model for each label. The primary advantage of our algorithm is that it produces very good segmentation results under two difficult problems: the weak boundary problem and the texture problem. Moreover, single-label image segmentation is possible. These are achieved by designing the generative model with the Random Walks with Restart (RWR). Experimental results with synthetic and natural images demonstrate the relevance and accuracy of our algorithm."'),
('"Generic Cuts: An Efficient Algorithm for Optimal Inference in Higher Order MRF-MAP"', '"ECCV 2012"', '["Higher Order MRF-MAP", "Submodular Function Minimization", "Optimal Algorithm"]', '"https://doi.org/10.1007/978-3-642-33715-4_2"', '"We propose a new algorithm called Generic Cuts for computing optimal solutions to 2 label MRF-MAP problems with higher order clique potentials satisfying submodularity. The algorithm runs in time O(2 k n 3) in the worst case (k is clique order and n is the number of pixels). A special gadget is introduced to model flows in a high order clique and a technique for building a flow graph is specified. Based on the primal dual structure of the optimization problem the notions of capacity of an edge and cut are generalized to define a flow problem. We show that in this flow graph max flow is equal to min cut which also is the optimal solution to the problem when potentials are submodular. This is in contrast to all prevalent techniques of optimizing Boolean energy functions involving higher order potentials including those based on reductions to quadratic potential functions which provide only approximate solutions even for submodular functions. We show experimentally that our implementation of the Generic Cuts algorithm is more than an order of magnitude faster than all algorithms including reduction based whose outputs on submodular potentials are near optimal."'),
('"Genetic Model Optimization for Hausdorff Distance-Based Face Localization"', '"BioAW 2002"', '[]', '"https://doi.org/10.1007/3-540-47917-1_11"', '"In our previous work we presented a model-based approach to perform robust, high-speed face localization based on the Hausdorff distance. A crucial step during the design of the system is the choice of an appropriate edge model that fits for a wide range of different human faces. In this paper we present an optimization approach that creates and successively improves such a model by means of genetic algorithms. To speed up the process and to prevent early saturation we use a special bootstrapping method on the sample set. Several initialization functions are tested and compared."'),
('"Geodesic Object Proposals"', '"ECCV 2014"', '["perceptual organization", "grouping"]', '"https://doi.org/10.1007/978-3-319-10602-1_47"', '"We present an approach for identifying a set of candidate objects in a given image. This set of candidates can be used for object recognition, segmentation, and other object-based image parsing tasks. To generate the proposals, we identify critical level sets in geodesic distance transforms computed for seeds placed in the image. The seeds are placed by specially trained classifiers that are optimized to discover objects. Experiments demonstrate that the presented approach achieves significantly higher accuracy than alternative approaches, at a fraction of the computational cost."'),
('"Geodesic Regression on the Grassmannian"', '"ECCV 2014"', '["Geodesic regression", "Grassmann manifold", "Traffic speed prediction", "Crowd counting", "Shape r', '"https://doi.org/10.1007/978-3-319-10605-2_41"', '"This paper considers the problem of regressing data points on the Grassmann manifold over a scalar-valued variable. The Grassmannian has recently gained considerable attention in the vision community with applications in domain adaptation, face recognition, shape analysis, or the classification of linear dynamical systems. Motivated by the success of these approaches, we introduce a principled formulation for regression tasks on that manifold. We propose an intrinsic geodesic regression model generalizing classical linear least-squares regression. Since geodesics are parametrized by a starting point and a velocity vector, the model enables the synthesis of new observations on the manifold. To exemplify our approach, we demonstrate its applicability on three vision problems where data objects can be represented as points on the Grassmannian: the prediction of traffic speed and crowd counts from dynamical system models of surveillance videos and the modeling of aging trends in human brain structures using an affine-invariant shape representation."'),
('"Geodesic Saliency Using Background Priors"', '"ECCV 2012"', '["Image Patch", "Salient Object", "Saliency Detection", "Image Boundary", "Salient Object Detection"', '"https://doi.org/10.1007/978-3-642-33712-3_3"', '"Generic object level saliency detection is important for many vision tasks. Previous approaches are mostly built on the prior that \\u201cappearance contrast between objects and backgrounds is high\\u201d. Although various computational models have been developed, the problem remains challenging and huge behavioral discrepancies between previous approaches can be observed. This suggest that the problem may still be highly ill-posed by using this prior only."'),
('"Geodesic Shape Retrieval via Optimal Mass Transport"', '"ECCV 2010"', '["Point Cloud", "Geodesic Distance", "Local Descriptor", "Global Descriptor", "Shape Retrieval"]', '"https://doi.org/10.1007/978-3-642-15555-0_56"', '"This paper presents a new method for 2-D and 3-D shape retrieval based on geodesic signatures. These signatures are high dimensional statistical distributions computed by extracting several features from the set of geodesic distance maps to each point. The resulting high dimensional distributions are matched to perform retrieval using a fast approximate Wasserstein metric. This allows to propose a unifying framework for the compact description of planar shapes and 3-D surfaces."'),
('"Geodesics Between 3D Closed Curves Using Path-Straightening"', '"ECCV 2006"', '["Tangent Vector", "Closed Curve", "Parallel Transport", "Closed Curf", "Shape Space"]', '"https://doi.org/10.1007/11744023_8"', '"In order to analyze shapes of continuous curves in \\u211d3, we parameterize them by arc-length and represent them as curves on a unit two-sphere. We identify the subset denoting the closed curves, and study its differential geometry. To compute geodesics between any two such curves, we connect them with an arbitrary path, and then iteratively straighten this path using the gradient of an energy associated with this path. The limiting path of this path-straightening approach is a geodesic. Next, we consider the shape space of these curves by removing shape-preserving transformations such as rotation and re-parametrization. To construct a geodesic in this shape space, we construct the shortest geodesic between the all possible transformations of the two end shapes; this is accomplished using an iterative procedure. We provide step-by-step descriptions of all the procedures, and demonstrate them with simple examples."'),
('"Geometric Calibration of Micro-Lens-Based Light-Field Cameras Using Line Features"', '"ECCV 2014"', '["Calibration", "plenoptic", "light-field cameras"]', '"https://doi.org/10.1007/978-3-319-10599-4_4"', '"We present a novel method of geometric calibration of micro-lens-based light-field cameras. Accurate geometric calibration is a basis of various applications. Instead of using sub-aperture images, we utilize raw images directly for calibration. We select proper regions in raw images and extract line features from micro-lens images in those regions. For the whole process, we formulate a new projection model of micro-lens-based light-field cameras. It is transformed into a linear form using line features. We compute an initial solution of both intrinsic and extrinsic parameters by a linear computation, and refine it via a non-linear optimization. Experimental results show the accuracy of the correspondences between rays and pixels in raw images, estimated by the proposed method."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Geometric Constraints for Human Detection in Aerial Imagery"', '"ECCV 2010"', '["Support Vector Machine", "Ground Plane", "Geometric Constraint", "Object Candidate", "Human Detect', '"https://doi.org/10.1007/978-3-642-15567-3_19"', '"In this paper, we propose a method for detecting humans in imagery taken from a UAV. This is a challenging problem due to small number of pixels on target, which makes it more difficult to distinguish people from background clutter, and results in much larger searchspace. We propose a method for human detection based on a number of geometric constraints obtained from the metadata. Specifically, we obtain the orientation of groundplane normal, the orientation of shadows cast by humans in the scene, and the relationship between human heights and the size of their corresponding shadows. In cases when metadata is not available we propose a method for automatically estimating shadow orientation from image data. We utilize the above information in a geometry based shadow, and human blob detector, which provides an initial estimation for locations of humans in the scene. These candidate locations are then classified as either human or clutter using a combination of wavelet features, and a Support Vector Machine. Our method works on a single frame, and unlike motion detection based methods, it bypasses the global motion compensation process, and allows for detection of stationary and slow moving humans, while avoiding the search across the entire image, which makes it more accurate and very fast. We show impressive results on sequences from the VIVID dataset and our own data, and provide comparative analysis."'),
('"Geometric Driven Optical Flow Estimation and Segmentation for 3D Reconstruction"', '"ECCV 2000"', '["Fundamental Matrix", "Epipolar Line", "Epipolar Geometry", "View Synthesis", "Epipolar Constraint"', '"https://doi.org/10.1007/3-540-45053-X_54"', '"We present a method for fully automatic 3D reconstruction from a pair of uncalibrated images in order to deal with the modeling of complex rigid scenes. A 2D triangular mesh model of the scene is calculated using a two-step algorithm mixing sparse matching and dense motion estimation approaches. The 2D mesh is iteratively refined to fit any arbitrary 3D surface. At convergence, each triangular patch corresponds to the projection of a 3D plane. The algorithm proposed here relies first on a dense disparity field. The dense field estimation modelized within a robust framework is constrained by the epipolar geometry. The resulting field is then segmented according to homographic models using iterative Delaunay triangulation. In association with a simplified self-calibration algorithm, this 2D planar model is used to obtain a VRML-compatible 3D model of the scene."'),
('"Geometric Image Parsing in Man-Made Environments"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_5"', '"We present a new parsing framework for the line-based geometric analysis of a single image coming from a man-made environment. This parsing framework models the scene as a composition of geometric primitives spanning different layers from low level (edges) through mid-level (lines and vanishing points) to high level (the zenith and the horizon). The inference in such a model thus jointly and simultaneously estimates a) the grouping of edges into the straight lines, b) the grouping of lines into parallel families, and c) the positioning of the horizon and the zenith in the image. Such a unified treatment means that the uncertainty information propagates between the layers of the model. This is in contrast to most previous approaches to the same problem, which either ignore the middle levels (lines) all together, or use the bottom-up step-by-step pipeline."'),
('"Geometric Properties of Central Catadioptric Line Images"', '"ECCV 2002"', '["Projective Transformation", "Perspective Camera", "Direction Point", "Absolute Conic", "Conic Curv', '"https://doi.org/10.1007/3-540-47979-1_16"', '"It is highly desirable that an imaging system has a single effective viewpoint. Central catadioptric systems are imaging systems that use mirrors to enhance the field of view while keeping a unique center of projection. A general model for central catadioptric image formation has already been established. The present paper exploits this model to study the catadioptric projection of lines. The equations and geometric properties of general catadioptric line imaging are derived. We show that it is possible to determine the position of both the effective viewpoint and the absolute conic in the catadioptric image plane from the images of three lines. It is also proved that it is possible to identify the type of catadioptric system and the position of the line at infinity without further information. A methodology for central catadioptric system calibration is proposed. Reconstruction aspects are discussed. Experimental results are presented. All the results presented are original and completely new."'),
('"Geometric Structure of Degeneracy for Multi-body Motion Segmentation"', '"SMVP 2004"', '["Video Sequence", "Geometric Structure", "Motion Model", "Unsupervised Learning", "Moment Matrix"]', '"https://doi.org/10.1007/978-3-540-30212-4_2"', '"Many techniques have been proposed for segmenting feature point trajectories tracked through a video sequence into independent motions. It has been found, however, that methods that perform very well in simulations perform very poorly for real video sequences. This paper resolves this mystery by analyzing the geometric structure of the degeneracy of the motion model. This leads to a new segmentation algorithm: a multi-stage unsupervised learning scheme first using the degenerate motion model and then using the general 3-D motion model. We demonstrate by simulated and real video experiments that our method is superior to all existing methods in practical situations."'),
('"Geometry and Kinematics with Uncertain Data"', '"ECCV 2006"', '["Covariance Matrix", "Conformal Space", "Uncertain Data", "Geometric Algebra", "Jacobi Matrice"]', '"https://doi.org/10.1007/11744023_18"', '"In Computer Vision applications, one usually has to work with uncertain data. It is therefore important to be able to deal with uncertain geometry and uncertain transformations in a uniform way. The Geometric Algebra of conformal space offers a unifying framework to treat not only geometric entities like points, lines, planes, circles and spheres, but also transformations like reflection, inversion, rotation and translation. In this text we show how the uncertainty of all elements of the Geometric Algebra of conformal space can be appropriately described by covariance matrices. In particular, it will be shown that it is advantageous to represent uncertain transformations in Geometric Algebra as compared to matrices. Other important results are a novel pose estimation approach, a uniform framework for geometric entity fitting and triangulation, the testing of uncertain tangentiality relations and the treatment of catadioptric cameras with parabolic mirrors within this framework. This extends previous work by F\\u00f6rstner and Heuel from points, lines and planes to non-linear geometric entities and transformations, while keeping the linearity of the estimation method. We give a theoretical description of our approach and show exemplary applications."'),
('"Geometry Construction from Caustic Images"', '"ECCV 2010"', '["Mean Square Error", "Stochastic Approximation", "Target Distribution", "Objective Function Evaluat', '"https://doi.org/10.1007/978-3-642-15555-0_34"', '"In this work we investigate an inverse geometry problem. Given a light source, a diffuse plane and a caustic image, how must a geometric object look like (transmissive or reflective) in oder to project the desired caustic onto the diffuse plane when lit by the light source? In order to construct the geometry we apply an analysis-by-synthesis approach, exploiting the GPU to accelerate caustic rendering based on the current geometry estimate. The optimization is driven by simultaneous perturbation stochastic approximation (SPSA). We confirm that this algorithm converges to the global minimum with high probability even in this ill-posed setting. We demonstrate results for precise geometry reconstruction given a caustic image and for reflector design producing an intended light distribution."'),
('"Geometry Driven Semantic Labeling of Indoor Scenes"', '"ECCV 2014"', '["Scale Invariant Feature Transform", "Conditional Random Field", "Kinect Sensor", "Semantic Label",', '"https://doi.org/10.1007/978-3-319-10590-1_44"', '"We present a discriminative graphical model which integrates geometrical information from RGBD images in its unary, pairwise and higher order components. We propose an improved geometry estimation scheme which is robust to erroneous sensor inputs. At the unary level, we combine appearance based beliefs defined on pixels and planes using a hybrid decision fusion scheme. Our proposed location potential gives an improved representation of the planar classes. At the pairwise level, we learn a balanced combination of various boundaries to consider the spatial discontinuity. Finally, we treat planar regions as higher order cliques and use graphcuts to make efficient inference. In our model based formulation, we use structured learning to fine tune the model parameters. We test our approach on two RGBD datasets and demonstrate significant improvements over the state-of-the-art scene labeling techniques."'),
('"GeoS: Geodesic Image Segmentation"', '"ECCV 2008"', '["Geodesic Distance", "Conditional Random Field", "Global Constraint", "Video Segmentation", "Stereo', '"https://doi.org/10.1007/978-3-540-88682-2_9"', '"This paper presents GeoS, a new algorithm for the efficient segmentation of n-dimensional image and video data."'),
('"Geotensity Constraint for 3D Surface Reconstruction under Multiple Light Sources"', '"ECCV 2000"', '["Light Source", "Point Light Source", "Lighting Parameter", "Light Source Direction", "Single Light', '"https://doi.org/10.1007/3-540-45054-8_47"', '"We tackle the problem of 3D surface reconstruction by a single static camera, extracting the maximum amount of information from gray level changes caused by object motion under illumination by a fixed set of light sources. We basically search for the depth at each point on the surface of the object while exploiting the recently proposed Geotensity constraint [11] that accurately governs the relationship between four or more images of a moving object in spite of the illumination variance due to object motion. The thrust of this paper is then to extend the availability of the Geotensity constraint to the case of multiple point light sources instead of a single light source. We first show that it is mathematically possible to identify multiple illumination subspaces for an arbitrary unknown number of light sources. We then propose a new technique to effectively carry out the separation of the subspaces by introducing the surface interaction matrix. Finally, we construct a framework for surface recovery, taking the multiple illumination subspaces into account. The theoretical propositions are investigated through experiments and shown to be practically useful."'),
('"Gesture Recognition Using Template Based Random Forest Classifiers"', '"ECCV 2014"', '["Template based learning", "Random Decision Forest", "Gesture recognition"]', '"https://doi.org/10.1007/978-3-319-16178-5_41"', '"This paper presents a framework for spotting and recognizing continuous human gestures. Skeleton based features are extracted from normalized human body coordinates to represent gestures. These features are then used to construct spatio-temporal template based Random Decision Forest models. Finally, predictions from different models are fused at decision-level to improve overall recognition performance. Our method has shown competitive results on the ChaLearn 2014 Looking at People: Gesture Recognition dataset. Trained on a dataset of 20 gesture vocabulary and 7754 gesture samples, our method achieved a Jaccard Index of \\\\(0.74663\\\\) on the test set, reaching 7th place among contenders. Among methods that exclusively used skeleton based features, our method obtained the highest recognition performance."'),
('"GIS-Assisted Object Detection and Geospatial Localization"', '"ECCV 2014"', '["Geographical Information System", "Object Detection", "Query Image", "Graph Match", "Geographical ', '"https://doi.org/10.1007/978-3-319-10599-4_39"', '"Geographical Information System (GIS) databases contain information about many objects, such as traffic signals, road signs, fire hydrants, etc. in urban areas. This wealth of information can be utilized for assisting various computer vision tasks. In this paper, we propose a method for improving object detection using a set of priors acquired from GIS databases. Given a database of object locations from GIS and a query image with metadata, we compute the expected spatial location of the visible objects in the image. We also perform object detection in the query image (e.g., using DPM) and obtain a set of candidate bounding boxes for the objects. Then, we fuse the GIS priors with the potential detections to find the final object bounding boxes. To cope with various inaccuracies and practical complications, such as noisy metadata, occlusion, inaccuracies in GIS, and poor candidate detections, we formulate our fusion as a higher-order graph matching problem which we robustly solve using RANSAC. We demonstrate that this approach outperforms well established object detectors, such as DPM, with a large margin."'),
('"Global Optimization of Object Pose and Motion from a Single Rolling Shutter Image with Automatic 2D', '"ECCV 2012"', '["rolling shutter", "motion estimation", "matching"]', '"https://doi.org/10.1007/978-3-642-33718-5_33"', '"Low cost CMOS cameras can have an acquisition mode called rolling shutter which sequentially exposes the scan-lines. When a single object moves with respect to the camera, this creates image distortions. Assuming 2D-3D correspondences known, previous work showed that the object pose and kinematics can be estimated from a single rolling shutter image. This was achieved using a suboptimal initialization followed by local iterative optimization."'),
('"Globally Optimal Active Contours, Sequential Monte Carlo and On-Line Learning for Vessel Segmentati', '"ECCV 2006"', '["Gaussian Mixture Model", "Particle Filter", "Active Contour", "Deformable Model", "Minimal Path"]', '"https://doi.org/10.1007/11744078_37"', '"In this paper we propose a Particle Filter-based propagation approach for the segmentation of vascular structures in 3D volumes. Because of pathologies and inhomogeneities, many deterministic methods fail to segment certain types of vessel. Statistical methods represent the solution using a probability density function (pdf). This pdf does not only indicate the best possible solution, but also valuable information about the solution\\u2019s variance. Particle Filters are used to learn the variations of direction and appearance of the vessel as the segmentation goes. These variations are used in turn in the particle filters framework to control the perturbations introduced in the Sampling Importance Resampling step (SIR). For the segmentation itself, successive planes of the vessel are modeled as states of a Particle Filter. Such states consist of the orientation, position and appearance (in statistical terms) of the vessel. The shape of the vessel and subsequently the particles pdf are recovered using globally active contours, implemented using circular shortest paths by branch and bound [1] that guarantees the global optimal solution. Promising results on the segmentation of coronary arteries demonstrate the potential of the proposed approach."'),
('"Globally Optimal Closed-Surface Segmentation for Connectomics"', '"ECCV 2012"', '["Volume Image", "Rand Index", "Video Segmentation", "Integer Linear Program Problem", "Correlation ', '"https://doi.org/10.1007/978-3-642-33712-3_56"', '"We address the problem of partitioning a volume image into a previously unknown number of segments, based on a likelihood of merging adjacent supervoxels. Towards this goal, we adapt a higher-order probabilistic graphical model that makes the duality between supervoxels and their joint faces explicit and ensures that merging decisions are consistent and surfaces of final segments are closed. First, we propose a practical cutting-plane approach to solve the MAP inference problem to global optimality despite its NP-hardness. Second, we apply this approach to challenging large-scale 3D segmentation problems for neural circuit reconstruction (Connectomics), demonstrating the advantage of this higher-order model over independent decisions and finite-order approximations."'),
('"Globally Optimal Inlier Set Maximization with Unknown Rotation and Focal Length"', '"ECCV 2014"', '["Consensus set maximization", "branch-and-bound", "inlier detection", "RANSAC"]', '"https://doi.org/10.1007/978-3-319-10605-2_52"', '"Identifying inliers and outliers among data is a fundamental problem for model estimation. This paper considers models composed of rotation and focal length, which typically occurs in the context of panoramic imaging. An efficient approach consists in computing the underlying model such that the number of inliers is maximized. The most popular tool for inlier set maximization must be RANSAC and its numerous variants. While they can provide interesting results, they are not guaranteed to return the globally optimal solution, i.e. the model leading to the highest number of inliers. We propose a novel globally optimal approach based on branch-and-bound. It computes the rotation and the focal length maximizing the number of inlier correspondences and considers the reprojection error in the image space. Our approach has been successfully applied on synthesized data and real images."'),
('"Globally Optimal Multi-target Tracking on a Hexagonal Lattice"', '"ECCV 2010"', '["Target Location", "Integer Linear Program", "Hexagonal Lattice", "Observation Model", "Integer Lin', '"https://doi.org/10.1007/978-3-642-15549-9_34"', '"We propose a global optimisation approach to multi-target tracking. The method extends recent work which casts tracking as an integer linear program, by discretising the space of target locations. Our main contribution is to show how dynamic models can be integrated in such an approach. The dynamic model, which encodes prior expectations about object motion, has been an important component of tracking systems for a long time, but has recently been dropped to achieve globally optimisable objective functions. We re-introduce it by formulating the optimisation problem such that deviations from the prior can be measured independently for each variable. Furthermore, we propose to sample the location space on a hexagonal lattice to achieve smoother, more accurate trajectories in spite of the discrete setting. Finally, we argue that non-maxima suppression in the measured evidence should be performed during tracking, when the temporal context and the motion prior are available, rather than as a preprocessing step on a per-frame basis. Experiments on five different recent benchmark sequences demonstrate the validity of our approach."'),
('"GMCP-Tracker: Global Multi-object Tracking Using Generalized Minimum Clique Graphs"', '"ECCV 2012"', '["Data Association", "Human Tracking", "Generalized Graphs", "GMCP", "Generalized Minimum Clique Pro', '"https://doi.org/10.1007/978-3-642-33709-3_25"', '"Data association is an essential component of any human tracking system. The majority of current methods, such as bipartite matching, incorporate a limited-temporal-locality of the sequence into the data association problem, which makes them inherently prone to IDswitches and difficulties caused by long-term occlusion, cluttered background, and crowded scenes.We propose an approach to data association which incorporates both motion and appearance in a global manner. Unlike limited-temporal-locality methods which incorporate a few frames into the data association problem, we incorporate the whole temporal span and solve the data association problem for one object at a time, while implicitly incorporating the rest of the objects. In order to achieve this, we utilize Generalized Minimum Clique Graphs to solve the optimization problem of our data association method. Our proposed method yields a better formulated approach to data association which is supported by our superior results. Experiments show the proposed method makes significant improvements in tracking in the diverse sequences of Town Center [1], TUD-crossing [2], TUD-Stadtmitte [2], PETS2009 [3], and a new sequence called Parking Lot compared to the state of the art methods."'),
('"Going with the Flow: Pedestrian Efficiency in Crowded Scenes"', '"ECCV 2012"', '["Anomaly Detection", "Emergent Behavior", "Intended Motion", "Social Force Model", "Crowded Scene"]', '"https://doi.org/10.1007/978-3-642-33765-9_40"', '"Video analysis of crowded scenes is challenging due to the complex motion of individual people in the scene. The collective motion of pedestrians form a crowd flow, but individuals often largely deviate from it as they anticipate and react to each other. Deviations from the crowd decreases the pedestrian\\u2019s efficiency: a sociological concept that measures the difference of actual motion from the intended speed and direction. In this paper, we derive a novel method for estimating pedestrian efficiency from videos. We first introduce a novel crowd motion model that encodes the temporal evolution of local motion patterns represented with directional statistics distributions. This model is then used to estimate the intended motion of pedestrians at every space-time location, which enables visual measurement of the pedestrian efficiency. We demonstrate the use of this pedestrian efficiency to detect unusual events and to track individuals in crowded scenes. Experimental results show that the use of pedestrian efficiency leads to state-of-the-art accuracy in these critical applications."'),
('"Good Edgels to Track: Beating the Aperture Problem with Epipolar Geometry"', '"ECCV 2014"', '["Densification", "Tracking", "Epipolar geometry", "Lucas-Kanade", "Feature extraction", "Edgels", "', '"https://doi.org/10.1007/978-3-319-16181-5_50"', '"An open issue in multiple view geometry and structure from motion, applied to real life scenarios, is the sparsity of the matched key-points and of the reconstructed point cloud. We present an approach that can significantly improve the density of measured displacement vectors in a sparse matching or tracking setting, exploiting the partial information of the motion field provided by linear oriented image patches (edgels). Our approach assumes that the epipolar geometry of an image pair already has been computed, either in an earlier feature-based matching step, or by a robustified differential tracker. We exploit key-points of a lower order, edgels, which cannot provide a unique 2D matching, but can be employed if a constraint on the motion is already given. We present a method to extract edgels, which can be effectively tracked given a known camera motion scenario, and show how a constrained version of the Lucas-Kanade tracking procedure can efficiently exploit epipolar geometry to reduce the classical KLT optimization to a 1D search problem. The potential of the proposed methods is shown by experiments performed on real driving sequences."'),
('"Good Image Priors for Non-blind Deconvolution"', '"ECCV 2014"', '["deblur", "non-blind deconvolution", "gaussian mixtures", "image pyramid", "image priors", "camera ', '"https://doi.org/10.1007/978-3-319-10593-2_16"', '"Most image restoration techniques build \\u201cuniversal\\u201d image priors, trained on a variety of scenes, which can guide the restoration of any image. But what if we have more specific training examples, e.g. sharp images of similar scenes? Surprisingly, state-of-the-art image priors don\\u2019t seem to benefit from from context-specific training examples. Re-training generic image priors using ideal sharp example images provides minimal improvement in non-blind deconvolution. To help understand this phenomenon we explore non-blind deblurring performance over a broad spectrum of training image scenarios. We discover two strategies that become beneficial as example images become more context-appropriate: (1) locally adapted priors trained from region level correspondence significantly outperform globally trained priors, and (2) a novel multi-scale patch-pyramid formulation is more successful at transferring mid and high frequency details from example scenes. Combining these two key strategies we can qualitatively and quantitatively outperform leading generic non-blind deconvolution methods when context-appropriate example images are available. We also compare to recent work which, like ours, tries to make use of context-specific examples."'),
('"Good Regions to Deblur"', '"ECCV 2012"', '["Kernel Estimation", "Good Region", "Blind Deconvolution", "Recovered Image", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-642-33715-4_5"', '"The goal of single image deblurring is to recover both a latent clear image and an underlying blur kernel from one input blurred image. Recent works focus on exploiting natural image priors or additional image observations for deblurring, but pay less attention to the influence of image structures on estimating blur kernels. What is the useful image structure and how can one select good regions for deblurring? We formulate the problem of learning good regions for deblurring within the Conditional Random Field framework. To better compare blur kernels, we develop an effective similarity metric for labeling training samples. The learned model is able to predict good regions from an input blurred image for deblurring without user guidance. Qualitative and quantitative evaluations demonstrate that good regions can be selected by the proposed algorithms for effective image deblurring."'),
('"GPS-Based Multi-viewpoint Integration for Anticipative Scene Analysis"', '"ECCV 2012"', '["Multi-viewpoint Image", "Scene Analysis", "Chromatic Complexity", "GPS Signal Processing", "Image ', '"https://doi.org/10.1007/978-3-642-33885-4_38"', '"A multi-viewpoint integration scheme is introduced to recognize scene features prior to physical access. In this schematics, chromatic complexity of vehicle\\u2019s- and bird\\u2019s-eye-views of roadway scenes are matched to extend GPS tracks towards possible destinations. Saliency patterns arising in destination images are anticipatively extracted to control the focus of inherent and machine vision to what to be analyzed."'),
('"GPU Accelerated Likelihoods for Stereo-Based Articulated Tracking"', '"ECCV 2010"', '["CUDA", "GPU Computing", "Articulated Tracking", "Particle Filtering"]', '"https://doi.org/10.1007/978-3-642-35740-4_28"', '"For many years articulated tracking has been an active research topic in the computer vision community. While working solutions have been suggested, computational time is still problematic. We present a GPU implementation of a ray-casting based likelihood model that is orders of magnitude faster than a traditional CPU implementation. We explain the non-intuitive steps required to attain an optimized GPU implementation, where the dominant part is to hide the memory latency effectively. Benchmarks show that computations which previously required several minutes, are now performed in few seconds."'),
('"Grading Tai Chi Performance in Competition with RGBD Sensors"', '"ECCV 2014"', '["Tai Chi", "RGBD sensor", "Kinect"]', '"https://doi.org/10.1007/978-3-319-16181-5_1"', '"In order to grade objectively, referees of Tai Chi practices always have to be very concentrated on every posture of the performer. This makes the referees easy to be fatigue and thus grade with occasional mistakes. In this paper, we propose using Kinect sensors to grade automatically. Firstly, we record the joint movement of the performer skeleton. Then we adopt the joint differences both temporally and spatially to model the joint dynamics and configuration. We apply Principal Component Analysis (PCA) to the joint differences in order to reduce redundancy and noise. We then employ non-parametric Nave-Bayes-Nearest-Neighbor (NBNN) as a classifier to recognize the multiple categories of Tai Chi forms. To give grade of each form, we study the grading criteria and convert them into decision on angles or distances between vectors. Experiments on several Tai Chi forms show the feasibility of our method."'),
('"Graduated Consistency-Regularized Optimization for Multi-graph Matching"', '"ECCV 2014"', '["Permutation Matrix", "Graph Match", "Quadratic Assignment Problem", "Consistency Constraint", "Ass', '"https://doi.org/10.1007/978-3-319-10590-1_27"', '"Graph matching has a wide spectrum of computer vision applications such as finding feature point correspondences across images. The problem of graph matching is generally NP-hard, so most existing work pursues suboptimal solutions between two graphs. This paper investigates a more general problem of matching N attributed graphs to each other, i.e. labeling their common node correspondences such that a certain compatibility/affinity objective is optimized. This multi-graph matching problem involves two key ingredients affecting the overall accuracy: a) the pairwise affinity matching score between two local graphs, and b) global matching consistency that measures the uniqueness and consistency of the pairwise matching results by different sequential matching orders. Previous work typically either enforces the matching consistency constraints in the beginning of iterative optimization, which may propagate matching error both over iterations and across different graph pairs; or separates score optimizing and consistency synchronization in two steps. This paper is motivated by the observation that affinity score and consistency are mutually affected and shall be tackled jointly to capture their correlation behavior. As such, we propose a novel multi-graph matching algorithm to incorporate the two aspects by iteratively approximating the global-optimal affinity score, meanwhile gradually infusing the consistency as a regularizer, which improves the performance of the initial solutions obtained by existing pairwise graph matching solvers. The proposed algorithm with a theoretically proven convergence shows notable efficacy on both synthetic and public image datasets."'),
('"Grain Segmentation of 3D Superalloy Images Using Multichannel EWCVT under Human Annotation Constrai', '"ECCV 2012"', '["Image Segmentation", "Image Slice", "Voronoi Tessellation", "Segmentation Accuracy", "Constrain Mi', '"https://doi.org/10.1007/978-3-642-33712-3_18"', '"Grain segmentation on 3D superalloy images provides superalloy\\u2019s micro-structures, based on which many physical and mechanical properties can be evaluated. This is a challenging problem in senses of (1) the number of grains in a superalloy sample could be thousands or even more; (2) the intensity within a grain may not be homogeneous; and (3) superalloy images usually contains carbides and noises. Recently, the Multichannel Edge-Weighted Centroid Voronoi Tessellation (MCEWCVT) algorithm [1] was developed for grain segmentation and showed better performance than many widely used image segmentation algorithms. However, as a general-purpose clustering algorithm, MCEWCVT does not consider possible prior knowledge from material scientists in the process of grain segmentation. In this paper, we address this issue by defining an energy minimization problem which subject to certain constraints. Then we develop a Constrained Multichannel Edge-Weighted Centroid Voronoi Tessellation (CMEWCVT) algorithm to effectively solve this constrained minimization problem. In particular, manually annotated segmentation on a very small set of 2D slices are taken as constraints and incorporated into the whole clustering process. Experimental results demonstrate that the proposed CMEWCVT algorithm significantly improve the previous grain-segmentation performance."'),
('"Graph Cut Based Inference with Co-occurrence Statistics"', '"ECCV 2010"', '["Object Class", "Graph Construction", "Move Energy", "Pairwise Potential", "Swap Move"]', '"https://doi.org/10.1007/978-3-642-15555-0_18"', '"Markov and Conditional random fields (crfs) used in computer vision typically model only local interactions between variables, as this is computationally tractable. In this paper we consider a class of global potentials defined over all variables in the crf. We show how they can be readily optimised using standard graph cut algorithms at little extra expense compared to a standard pairwise field."'),
('"Graph Cuts for Supervised Binary Coding"', '"ECCV 2014"', '["Binary Code", "Stereo Match", "Supervise Method", "Continuous Relaxation", "Unary Term"]', '"https://doi.org/10.1007/978-3-319-10584-0_17"', '"Learning short binary codes is challenged by the inherent discrete nature of the problem. The graph cuts algorithm is a well-studied discrete label assignment solution in computer vision, but has not yet been applied to solve the binary coding problems. This is partially because it was unclear how to use it to learn the encoding (hashing) functions for out-of-sample generalization. In this paper, we formulate supervised binary coding as a single optimization problem that involves both the encoding functions and the binary label assignment. Then we apply the graph cuts algorithm to address the discrete optimization problem involved, with no continuous relaxation. This method, named as Graph Cuts Coding (GCC), shows competitive results in various datasets."'),
('"Graph Degree Linkage: Agglomerative Clustering on a Directed Graph"', '"ECCV 2012"', '["Pairwise Distance", "Spectral Cluster", "Normalize Mutual Information", "Agglomerative Cluster", "', '"https://doi.org/10.1007/978-3-642-33718-5_31"', '"This paper proposes a simple but effective graph-based agglomerative algorithm, for clustering high-dimensional data. We explore the different roles of two fundamental concepts in graph theory, indegree and outdegree, in the context of clustering. The average indegree reflects the density near a sample, and the average outdegree characterizes the local geometry around a sample. Based on such insights, we define the affinity measure of clusters via the product of average indegree and average outdegree. The product-based affinity makes our algorithm robust to noise. The algorithm has three main advantages: good performance, easy implementation, and high computational efficiency. We test the algorithm on two fundamental computer vision problems: image clustering and object matching. Extensive experiments demonstrate that it outperforms the state-of-the-arts in both applications."'),
('"Graph Matching via Sequential Monte Carlo"', '"ECCV 2012"', '["graph matching", "sequential Monte Carlo", "feature correspondence", "image matching", "object rec', '"https://doi.org/10.1007/978-3-642-33712-3_45"', '"Graph matching is a powerful tool for computer vision and machine learning. In this paper, a novel approach to graph matching is developed based on the sequential Monte Carlo framework. By constructing a sequence of intermediate target distributions, the proposed algorithm sequentially performs a sampling and importance resampling to maximize the graph matching objective. Through the sequential sampling procedure, the algorithm effectively collects potential matches under one-to-one matching constraints to avoid the adverse effect of outliers and deformation. Experimental evaluations on synthetic graphs and real images demonstrate its higher robustness to deformation and outliers."'),
('"Graph-Based Shape Similarity of Petroglyphs"', '"ECCV 2014"', '["Petroglyph similarity", "Shape similarity", "Graph matching", "Graph edit distance", "Graph embedd', '"https://doi.org/10.1007/978-3-319-16178-5_9"', '"Petroglyphs can be found on rock panels all over the world. The possibilities of digital photography and more recently various 3D scanning methods opened a new stage for the documentation and analysis of petroglyphs. The existing work on petroglyph shape similarity has largely avoided the questions of articulation, merged petroglyphs and potentially missing parts of petroglyphs. We aim at contributing to close this gap by applying a novel petroglyph shape descriptor based on the skeletal graph. Our contribution is twofold: First, we provide a real-world dataset of petroglyph shapes. Second, we propose a graph-based shape descriptor for petroglyphs. Comprehensive evaluations show, that the combination of the proposed descriptor with existing ones improves the performance in petroglyph shape similarity modeling."'),
('"Grassmann Registration Manifolds for Face Recognition"', '"ECCV 2008"', '["Face Recognition", "Tangent Space", "Recognition Rate", "Local Binary Pattern", "Geodesic Distance', '"https://doi.org/10.1007/978-3-540-88688-4_4"', '"Motivated by image perturbation and the geometry of manifolds, we present a novel method combining these two elements. First, we form a tangent space from a set of perturbed images and observe that the tangent space admits a vector space structure. Second, we embed the approximated tangent spaces on a Grassmann manifold and employ a chordal distance as the means for comparing subspaces. The matching process is accelerated using a coarse to fine strategy. Experiments on the FERET database suggest that the proposed method yields excellent results using both holistic and local features. Specifically, on the FERET Dup2 data set, our proposed method achieves 83.8% rank 1 recognition: to our knowledge the currently the best result among all non-trained methods. Evidence is also presented that peak recognition performance is achieved using roughly 100 distinct perturbed images."'),
('"Gray Scale and Rotation Invariant Texture Classification with Local Binary Patterns"', '"ECCV 2000"', '["Rotation Angle", "Gray Scale", "Local Binary Pattern", "Average Error Rate", "Angular Space"]', '"https://doi.org/10.1007/3-540-45054-8_27"', '"This paper presents a theoretically very simple yet efficient approach for gray scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The proposed approach is very robust in terms of gray scale variations, since the operators are by definition invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity, as the operators can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in two true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis."'),
('"Group Dynamics and Multimodal Interaction Modeling Using a Smart Digital Signage"', '"ECCV 2012"', '["Gaussian Mixture Model", "Joint Attention", "Linear Dynamical System", "Visual Information Process', '"https://doi.org/10.1007/978-3-642-33863-2_36"', '"This paper presents a new multimodal system for group dynamics and interaction analysis. The framework is composed of a mic array and multiview video cameras placed on a digital signage display which serves as a support for interaction. We show that visual information processing can be used to localize nonverbal communication events and synchronized with audio information. Our contribution is twofold: 1) we present a scalable portable system for multiple people multimodal interaction sensing, and 2) we propose a general framework to model A/V multimodal interaction that employs speaker diarization for audio processing and hybrid dynamical systems (HDS) for video processing. HDS are used to represent communication dynamics between multiple people by capturing the characteristics of temporal structures in head motions. Experimental results show real-world situations of group communication processing for joint attention estimation. We believe the proposed framework is very promising for further research."'),
('"Group Tracking: Exploring Mutual Relations for Multiple Object Tracking"', '"ECCV 2012"', '["Relational Weight", "Object Tracking", "Online Algorithm", "Related Object", "Visual Tracking"]', '"https://doi.org/10.1007/978-3-642-33712-3_10"', '"In this paper, we propose to track multiple previously unseen objects in unconstrained scenes. Instead of considering objects individually, we model objects in mutual context with each other to benefit robust and accurate tracking. We introduce a unified framework to combine both Individual Object Models (IOMs) and Mutual Relation Models (MRMs). The MRMs consist of three components, the relational graph to indicate related objects, the mutual relation vectors calculated within related objects to show the interactions, and the relational weights to balance all interactions and IOMs. As MRMs are varying along temporal sequences, we propose online algorithms to make MRMs adapt to current situations. We update relational graphs through analyzing object trajectories and cast the relational weight learning task as an online latent SVM problem. Extensive experiments on challenging real world video sequences demonstrate the efficiency and effectiveness of our framework."'),
('"Group-Valued Regularization for Analysis of Articulated Motion"', '"ECCV 2012"', '["Parameteric Surfaces", "Motion Segmentation", "Articulated Motion"]', '"https://doi.org/10.1007/978-3-642-33863-2_6"', '"We present a novel method for estimation of articulated motion in depth scans. The method is based on a framework for regularization of vector- and matrix- valued functions on parametric surfaces."'),
('"Groupwise Diffeomorphic Non-rigid Registration for Automatic Model Building"', '"ECCV 2004"', '["Face Image", "Appearance Model", "Large Mode", "Active Appearance Model", "Image Registration Meth', '"https://doi.org/10.1007/978-3-540-24673-2_26"', '"We describe a framework for registering a group of images together using a set of non-linear diffeomorphic warps. The result of the groupwise registration is an implicit definition of dense correspondences between all of the images in a set, which can be used to construct statistical models of shape change across the set, avoiding the need for manual annotation of training images. We give examples on two datasets (brains and faces) and show the resulting models of shape and appearance variation. We show results of experiments demonstrating that the groupwise approach gives a more reliable correspondence than pairwise matching alone."'),
('"Growing Regression Forests by Classification: Applications to Object Pose Estimation"', '"ECCV 2014"', '["Pose Estimation", "Direction Estimation", "Regression Tree", "Random Forest"]', '"https://doi.org/10.1007/978-3-319-10605-2_36"', '"In this work, we propose a novel node splitting method for regression trees and incorporate it into the regression forest framework. Unlike traditional binary splitting, where the splitting rule is selected from a predefined set of binary splitting rules via trial-and-error, the proposed node splitting method first finds clusters of the training data which at least locally minimize the empirical loss without considering the input space. Then splitting rules which preserve the found clusters as much as possible are determined by casting the problem into a classification problem. Consequently, our new node splitting method enjoys more freedom in choosing the splitting rules, resulting in more efficient tree structures. In addition to the Euclidean target space, we present a variant which can naturally deal with a circular target space by the proper use of circular statistics. We apply the regression forest employing our node splitting to head pose estimation (Euclidean target space) and car direction estimation (circular target space) and demonstrate that the proposed method significantly outperforms state-of-the-art methods (38.5% and 22.5% error reduction respectively)."'),
('"Guaranteed Ellipse Fitting with the Sampson Distance"', '"ECCV 2012"', '["Feasible Point", "Geometric Error", "Merit Function", "Orthogonal Distance", "Approximate Maximum ', '"https://doi.org/10.1007/978-3-642-33715-4_7"', '"When faced with an ellipse fitting problem, practitioners frequently resort to algebraic ellipse fitting methods due to their simplicity and efficiency. Currently, practitioners must choose between algebraic methods that guarantee an ellipse fit but exhibit high bias, and geometric methods that are less biased but may no longer guarantee an ellipse solution. We address this limitation by proposing a method that strikes a balance between these two objectives. Specifically, we propose a fast stable algorithm for fitting a guaranteed ellipse to data using the Sampson distance as a data-parameter discrepancy measure. We validate the stability, accuracy, and efficiency of our method on both real and synthetic data. Experimental results show that our algorithm is a fast and accurate approximation of the computationally more expensive orthogonal-distance-based ellipse fitting method. In view of these qualities, our method may be of interest to practitioners who require accurate and guaranteed ellipse estimates."'),
('"Guided Image Filtering"', '"ECCV 2010"', '["Bilateral Filter", "Local Linear Model", "Dark Channel", "Alpha Matte", "Detail Layer"]', '"https://doi.org/10.1007/978-3-642-15549-9_1"', '"In this paper, we propose a novel type of explicit image filter - guided filter. Derived from a local linear model, the guided filter generates the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can perform as an edge-preserving smoothing operator like the popular bilateral filter [1], but has better behavior near the edges. It also has a theoretical connection with the matting Laplacian matrix [2], so is a more generic concept than a smoothing operator and can better utilize the structures in the guidance image. Moreover, the guided filter has a fast and non-approximate linear-time algorithm, whose computational complexity is independent of the filtering kernel size. We demonstrate that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications including noise reduction, detail smoothing/enhancement, HDR compression, image matting/feathering, haze removal, and joint upsampling."'),
('"Guided Sampling and Consensus for Motion Estimation"', '"ECCV 2002"', '["Simultaneous Estimation", "Mismatch Distribution", "Correct Match", "Correlation Score", "Epipolar', '"https://doi.org/10.1007/3-540-47969-4_6"', '"We present techniques for improving the speed of robust motion estimation based on random sampling of image features. Starting from Torr and Zisserman\\u2019s MLESAC algorithm, we address some of the problems posed from both practical and theoretical standpoints and in doing so allow the random search to be replaced by a guided search. Guidance of the search is based on readily-available information which is usually discarded, but can significantly reduce the search time. This guided-sampling algorithm is further specialised for tracking of multiple motions, for which results are presented."'),
('"Hallucination-Free Multi-View Stereo"', '"ECCV 2010"', '["multi-view stereo", "stereo", "3D reconstruction"]', '"https://doi.org/10.1007/978-3-642-35740-4_15"', '"We present a multi-view stereo method that avoids producing hallucinated surfaces which do not correspond to real surfaces. Our approach to 3D reconstruction is based on the minimal s-t cut of the graph derived from the Delaunay tetrahedralization of a dense 3D point cloud, which produces water-tight meshes. This is often a desirable property but it hallucinates surfaces in complicated scenes with multiple objects and free open space. For example, a sequence of images obtained from a moving vehicle often produces meshes where the sky is hallucinated because there are no images looking from the above to the ground plane. We present a method for detecting and removing such surfaces. The method is based on removing perturbation sensitive parts of the reconstruction using multiple reconstructions of perturbed input data. We demonstrate our method on several standard datasets often used to benchmark multi-view stereo and show that it outperforms the state-of-the-art techniques ."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_24"', '"This paper improves recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We, first, analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy."'),
('"Hand Gesture Recognition in Camera-Projector System*"', '"CVHCI 2004"', '["Augmented Reality", "Feature Extraction Method", "Fourier Descriptor", "Recognition Probability", ', '"https://doi.org/10.1007/978-3-540-24837-8_9"', '"Our paper proposes a vision-based hand gesture recognition system. It is implemented in a camera-projector system to achieve an augmented reality tool. In this configuration the main problem is that the hand surface reflects the projected background, thus we apply a robust hand segmentation method. Hand localizing is based on a background subtraction method, which adapts to the changes of the projected background. Hand poses are described by a method based on modified Fourier descriptors, which involves distance metric for the nearest neighbor classification. The proposed classification method is compared to other feature extraction methods. We also conducted tests on several users. Finally, the recognition efficiency is improved by the recognition probabilities of the consecutive detected gestures by maximum likelihood approach."'),
('"Hand Gesture Recognition within a Linguistics-Based Framework"', '"ECCV 2004"', '["Apparent Motion", "Gesture Recognition", "American Sign Language", "Kinematic Feature", "Hand Shap', '"https://doi.org/10.1007/978-3-540-24670-1_22"', '"An approach to recognizing hand gestures from a monocular temporal sequence of images is presented. Of particular concern is the representation and recognition of hand movements that are used in single handed American Sign Language (ASL). The approach exploits previous linguistic analysis of manual languages that decompose dynamic gestures into their static and dynamic components. The first level of decomposition is in terms of three sets of primitives, hand shape, location and movement. Further levels of decomposition involve the lexical and sentence levels and are part of our plan for future work. We propose and demonstrate that given a monocular gesture sequence, kinematic features can be recovered from the apparent motion that provide distinctive signatures for 14 primitive movements of ASL. The approach has been implemented in software and evaluated on a database of 592 gesture sequences with an overall recognition rate of 86.00% for fully automated processing and 97.13% for manually initialized processing."'),
('"Hand Modeling and Tracking for Video-Based Sign Language Recognition by Robust Principal Component ', '"ECCV 2010"', '["hand modeling and tracking", "sign language recognition", "robust PCA", "L1 norm"]', '"https://doi.org/10.1007/978-3-642-35749-7_21"', '"Hand modeling and tracking are essential in video-based sign language recognition. The high reformability and the large number of degrees of freedom of hands render the problem difficult. To tackle these challenges, a novel approach based on robust principal component analysis (PCA) is proposed. The robust PCA incorporates an L 1 norm objective function to deal with background clutter, and a projection pursuit strategy to deal with the lack of alignment due to the deformation of hands. The learning algorithm of the robust PCA is very simple, involving only a search for the solutions in a finite set constructed from the training data, which leads to the learning of much more representative and interpretable bases. The incorporation of the L 1 regularization in the fitting of the learned robust PCA models results in cleaner reconstructions and more stable fitting. Based on the robust PCA, a hand tracking system is developed that contains a skin-color region segmentation based on graph cuts and template matching in the framework of particle filtering. Experiments on a publicly available sign-language video database demonstrates the strength of the method."'),
('"Hand Motion from 3D Point Trajectories and a Smooth Surface Model"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_38"', '"A method is proposed to track the full hand motion from 3D points reconstructed using a stereoscopic set of cameras. This approach combines the advantages of methods that use 2D motion (e.g. optical flow), and those that use a 3D reconstruction at each time frame to capture the hand motion. Matching either contours or a 3D reconstruction against a 3D hand model is usually very difficult due to self-occlusions and the locally-cylindrical structure of each phalanx in the model, but our use of 3D point trajectories constrains the motion and overcomes these problems."'),
('"Hand Pose Estimation and Hand Shape Classification Using Multi-layered Randomized Decision Forests"', '"ECCV 2012"', '["Posterior Probability", "Leaf Node", "Depth Image", "Spectral Cluster", "Depth Sensor"]', '"https://doi.org/10.1007/978-3-642-33783-3_61"', '"Vision based articulated hand pose estimation and hand shape classification are challenging problems. This paper proposes novel algorithms to perform these tasks using depth sensors. In particular, we introduce a novel randomized decision forest (RDF) based hand shape classifier, and use it in a novel multi\\u2013layered RDF framework for articulated hand pose estimation. This classifier assigns the input depth pixels to hand shape classes, and directs them to the corresponding hand pose estimators trained specifically for that hand shape. We introduce two novel types of multi\\u2013layered RDFs: Global Expert Network (GEN) and Local Expert Network (LEN), which achieve significantly better hand pose estimates than a single\\u2013layered skeleton estimator and generalize better to previously unseen hand poses. The novel hand shape classifier is also shown to be accurate and fast. The methods run in real\\u2013time on the CPU, and can be ported to the GPU for further increase in speed."'),
('"Hand Pose Estimation Using Hierarchical Detection"', '"CVHCI 2004"', '["False Positive Rate", "Edge Image", "Oriented Edge", "Articulated Object", "Chamfer Distance"]', '"https://doi.org/10.1007/978-3-540-24837-8_11"', '"This paper presents an analysis of the design of classifiers for use in a hierarchical object recognition approach. In this approach, a cascade of classifiers is arranged in a tree in order to recognize multiple object classes. We are interested in the problem of recognizing multiple patterns as it is closely related to the problem of locating an articulated object. Each different pattern class corresponds to the hand in a different pose, or set of poses. For this problem obtaining labelled training data of the hand in a given pose can be problematic. Given a parametric 3D model, generating training data in the form of example images is cheap, and we demonstate that it can be used to design classifiers almost as good as those trained using non-synthetic data. We compare a variety of different template-based classifiers and discuss their merits."'),
('"Hand Tracking and Affine Shape-Appearance Handshape Sub-units in Continuous Sign Language Recogniti', '"ECCV 2010"', '["Sign Language", "American Sign", "Active Appearance Model", "Vocabulary Size", "Hand Gesture Recog', '"https://doi.org/10.1007/978-3-642-35749-7_20"', '"We propose and investigate a framework that utilizes novel aspects concerning probabilistic and morphological visual processing for the segmentation, tracking and handshape modeling of the hands, which is used as front-end for sign language video analysis. Our ultimate goal is to explore the automatic Handshape Sub-Unit (HSU) construction and moreover the exploitation of the overall system in automatic sign language recognition (ASLR). We employ probabilistic skin color detection followed by the proposed morphological algorithms and related shape filtering for fast and reliable segmentation of hands and head. This is then fed to our hand tracking system which emphasizes robust handling of occlusions based on forward-backward prediction and incorporation of probabilistic constraints. The tracking is exploited by an Affine-invariant Modeling of hand Shape-Appearance images, offering a compact and descriptive representation of the hand configurations. We further propose that the handshape features extracted via the fitting of this model are utilized to construct in an unsupervised way basic HSUs. We first provide intuitive results on the HSU to sign mapping and further quantitatively evaluate the integrated system and the constructed HSUs on ASLR experiments at the sub-unit and sign level. These are conducted on continuous SL data from the BU400 corpus and investigate the effect of the involved parameters. The experiments indicate the effectiveness of the overall approach and especially for the modeling of handshapes when incorporated in the HSU-based framework showing promising results."'),
('"Hand Waving Away Scale"', '"ECCV 2014"', '["Smart devices", "IMU", "metric", "3D reconstruction"]', '"https://doi.org/10.1007/978-3-319-10593-2_19"', '"This paper presents a novel solution to the metric reconstruction of objects using any smart device equipped with a camera and an inertial measurement unit (IMU). We propose a batch, vision centric approach which only uses the IMU to estimate the metric scale of a scene reconstructed by any algorithm with Structure from Motion like (SfM) output. IMUs have a rich history of being combined with monocular vision for robotic navigation and odometry applications. These IMUs require sophisticated and quite expensive hardware rigs to perform well. IMUs in smart devices, however, are chosen for enhancing interactivity - a task which is more forgiving to noise in the measurements. We anticipate, however, that the ubiquity of these \\u201cnoisy\\u201d IMUs makes them increasingly useful in modern computer vision algorithms. Indeed, we show in this work how an IMU from a smart device can help a face tracker to measure pupil distance, and an SfM algorithm to measure the metric size of objects. We also identify motions that produce better results, and develop a heuristic for estimating, in real-time, when enough data has been collected for an accurate scale estimation."'),
('"Hand-Eye Calibration from Image Derivatives"', '"ECCV 2000"', '["Motion Estimation", "Translational Motion", "Intrinsic Parameter", "Rotational Part", "Robot Hand"', '"https://doi.org/10.1007/3-540-45053-X_32"', '"In this paper it is shown how to perform hand-eye calibration using only the normal flow field and knowledge about the motion of the hand. The proposed method comprise a simple way to calculate the hand-eye calibration when a camera is mounted on a robot. Firstly, it is shown how the orientation of the optical axis can be estimated from at least two different translational motions of the robot. Secondly, it is shown how the other parameters can be obtained using at least two different motions containing also a rotational part. In both stages, only image gradients are used, i.e. no point matches are needed. As a by-product, both the motion field and the depth of the scene can be obtained. The proposed method is illustrated in experiments using both simulated and real data."'),
('"Handling Urban Location Recognition as a 2D Homothetic Problem"', '"ECCV 2010"', '["Query Image", "Scale Ratio", "Panoramic Image", "Sift Descriptor", "Urban Scenario"]', '"https://doi.org/10.1007/978-3-642-15567-3_20"', '"We address the problem of large scale place-of-interest recognition in cell phone images of urban scenarios. Here, we go beyond what has been shown in earlier approaches by exploiting the nowadays often available 3D building information (e.g. from extruded floor plans) and massive street-view like image data for database creation. Exploiting vanishing points in query images and thus fully removing 3D rotation from the recognition problem allows then to simplify the feature invariance to a pure homothetic problem, which we show leaves more discriminative power in feature descriptors than classical SIFT. We rerank visual word based document queries using a fast stratified homothetic verification that is tailored for repetitive patterns like window grids on facades and in most cases boosts the correct document to top positions if it was in the short list. Since we exploit 3D building information, the approach finally outputs the camera pose in real world coordinates ready for augmenting the cell phone image with virtual 3D information. The whole system is demonstrated to outperform traditional approaches on city scale experiments for different sources of street-view like image data and a challenging set of cell phone images."'),
('"Has My Algorithm Succeeded? An Evaluator for Human Pose Estimators"', '"ECCV 2012"', '["Ground Truth", "Marginal Distribution", "IEEE Conf", "Detection Window", "Vision Algorithm"]', '"https://doi.org/10.1007/978-3-642-33712-3_9"', '"Most current vision algorithms deliver their output \\u2018as is\\u2019, without indicating whether it is correct or not. In this paper we propose evaluator algorithms that predict if a vision algorithm has succeeded. We illustrate this idea for the case of Human Pose Estimation (HPE)."'),
('"Hausdorff Distance Constraint for Multi-surface Segmentation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_43"', '"It is well known that multi-surface segmentation can be cast as a multi-labeling problem. Different segments may belong to the same semantic object which may impose various inter-segment constraints [1]. In medical applications, there are a lot of scenarios where upper bounds on the Hausdorff distances between subsequent surfaces are known. We show that incorporating these priors into multi-surface segmentation is potentially NP-hard. To cope with this problem we develop a submodular-supermodular procedure that converges to a locally optimal solution well-approximating the problem. While we cannot guarantee global optimality, only feasible solutions are considered during the optimization process. Empirically, we get useful solutions for many challenging medical applications including MRI and ultrasound images."'),
('"Hausdorff Kernel for 3D Object Acquisition and Detection"', '"ECCV 2002"', '["Support Vector Machine", "Object Detection", "Kernel Method", "Linear Kernel", "Polynomial Kernel"', '"https://doi.org/10.1007/3-540-47979-1_2"', '"Learning one class at a time can be seen as an effective solution to classification problems in which only the positive examples are easily identifiable. A kernel method to accomplish this goal consists of a representation stage - which computes the smallest sphere in feature space enclosing the positive examples - and a classification stage - which uses the obtained sphere as a decision surface to determine the positivity of new examples. In this paper we describe a kernel well suited to represent, identify, and recognize 3D objects from unconstrained images. The kernel we introduce, based on Hausdorff distance, is tailored to deal with grey-level image matching. The effectiveness of the proposed method is demonstrated on several data sets of faces and objects of artistic relevance, like statues."'),
('"HDR Imaging under Non-uniform Blurring"', '"ECCV 2012"', '["High Dynamic Range", "Blur Kernel", "High Dynamic Range Image", "Total Variation Regularization", ', '"https://doi.org/10.1007/978-3-642-33868-7_45"', '"Knowledge of scene irradiance is necessary in many computer vision algorithms. In this paper, we develop a technique to obtain the high dynamic range (HDR) irradiance of a scene from a set of differently exposed images captured using a hand-held camera. Any incidental motion induced by camera-shake can result in non-uniform motion blur. This is particularly true for frames captured with high exposure durations. We model the motion blur using a transformation spread function (TSF) that represents space-variant blurring as a weighted average of differently transformed versions of the latent image. We initially estimate the TSF of the blurred frames and then estimate the latent irradiance of the scene."'),
('"Head Tracking and Hand Segmentation during Hand over Face Occlusion in Sign Language"', '"ECCV 2010"', '["Hand segmentation", "sign language", "head registration"]', '"https://doi.org/10.1007/978-3-642-35749-7_18"', '"This paper presents a method to accurately segment the hand over the face. The similarity of colours and the important variability of the hand shape make it challenging. We propose a method based on the combination of two features: pixel colour and edges orientation. First, a specific skin model is used to find, before occlusion, the face position and the face template. Then, during occlusion the face template is registered using local gradient orientations to track the face position. Colour information is extracted from changes on pixel colours and edges are classified as belonging to the hand or to the face by mapping edges orientation to the face template. Finally by merging both features and by using an hysteresis threshold, which considers connectivity, a robust hand segmentation is reached. Experiments were performed using the Dicta-Sign corpus and showed the versatility of the proposed approach."'),
('"Heliometric Stereo: Shape from Sun Position"', '"ECCV 2012"', '["Photometric stereo", "webcams", "camera response"]', '"https://doi.org/10.1007/978-3-642-33709-3_26"', '"In this work, we present a method to uncover shape from webcams \\u201cin the wild.\\u201d We present a variant of photometric stereo which uses the sun as a distant light source, so that lighting direction can be computed from known GPS and timestamps. We propose an iterative, non-linear optimization process that optimizes the error in reproducing all images from an extended time-lapse with an image formation model that accounts for ambient lighting, shadows, changing light color, dense surface normal maps, radiometric calibration, and exposure. Unlike many approaches to uncalibrated outdoor image analysis, this procedure is automatic, and we report quantitative results by comparing extracted surface normals to Google Earth 3D models. We evaluate this procedure on data from a varied set of scenes and emphasize the advantages of including imagery from many months."'),
('"Helmholtz Stereopsis: Exploiting Reciprocity for Surface Reconstruction"', '"ECCV 2002"', '["Surface Reconstruction", "Bidirectional Reflectance Distribution Function", "Photometric Stereo", ', '"https://doi.org/10.1007/3-540-47977-5_57"', '"We present a method \\u2014 termed Helmholtz stereopsis \\u2014 for reconstructing the geometry of objects from a collection of images. Unlike most existing methods for surface reconstruction (e.g., stereo vision, structure from motion, photometric stereo), Helmholtz stereopsis makes no assumptions about the nature of the bidirectional reflectance distribution functions (BRDFs) of objects. This new method of multinocular stereopsis exploits Helmholtz reciprocity by choosing pairs of light source and camera positions that guarantee that the ratio of the emitted radiance to the incident irradiance is the same for corresponding points in the two images. The method provides direct estimates of both depth and field of surface normals, and consequently weds the advantages of both conventional and photometric stereopsis. Results from our implementations lend empirical support to our technique."'),
('"Hierarchical Analysis of Low-Contrast Temporal Images with Linear Scale Space"', '"MMBIA 2004"', '["Saddle Point", "Stationary Point", "Hessian Matrix", "Topological Change", "Scale Space"]', '"https://doi.org/10.1007/978-3-540-27816-0_13"', '"This paper focuses on the spatio-temporal analysis to the topology of topography of temporal gray-value images. We extract a sequence of trees which expresses the hierarchical structure of a temporal gray-value image using the linear scale space analysis. This hierarchical features of temporal images provide topological and geometrical information for the global understanding of temporal images."'),
('"Hierarchical Implicit Surface Joint Limits to Constrain Video-Based Motion Capture"', '"ECCV 2004"', '["Euler Angle", "Motion Capture", "Elbow Joint", "Implicit Surface", "Joint Limit"]', '"https://doi.org/10.1007/978-3-540-24671-8_32"', '"To increase the reliability of existing human motion tracking algorithms, we propose a method for imposing limits on the underlying hierarchical joint structures in a way that is true to life. Unlike most existing approaches, we explicitly represent dependencies between the various degrees of freedom and derive these limits from actual experimental data."'),
('"Hierarchical Late Fusion for Concept Detection in Videos"', '"ECCV 2012"', '["late fusion", "hierarchical", "semantic concepts", "video", "semantic indexing"]', '"https://doi.org/10.1007/978-3-642-33885-4_34"', '"We deal with the issue of combining dozens of classifiers into a better one, for concept detection in videos. We compare three fusion approaches that share a common structure: they all start with a classifier clustering stage, continue with an intra-cluster fusion and end with an inter-cluster fusion. The main difference between them comes from the first stage. The first approach relies on a priori knowledge about the internals of each classifier (low-level descriptors and classification algorithm) to group the set of available classifiers by similarity. The second and third approaches obtain classifier similarity measures directly from their output and group them using agglomerative clustering for the second approach and community detection for the third one."'),
('"Hierarchical Organization of Shapes for Efficient Retrieval"', '"ECCV 2004"', '["Simulated Annealing", "Hierarchical Organization", "Shape Space", "Multiple Hypothesis Test", "Geo', '"https://doi.org/10.1007/978-3-540-24672-5_45"', '"This paper presents a geometric approach to perform: (i) hierarchical clustering of imaged objects according to the shapes of their boundaries, and (ii) testing of observed shapes for classification. An intrinsic metric on nonlinear, infinite-dimensional shape space, obtained using geodesic lengths, is used for clustering. This analysis is landmark free, does not require embedding shapes in \\u211d2, and uses ordinary differential equations for flows (as opposed to partial differential equations). Intrinsic analysis also leads to well defined shape statistics such as means and covariances, and is computationally efficient. Clustering is performed in a hierarchical fashion. At any level of hierarchy clusters are generated using a minimum dispersion criterion and an MCMC-type search algorithm. Cluster means become elements to be clustered at the next level. Gaussian models on tangent spaces are used to pose binary or multiple hypothesis tests for classifying observed shapes. Hierarchical clustering and shape testing combine to form an efficient tool for shape retrieval from a large database of shapes. For databases with n shapes, the searches are performed using log(n) tests on average. Examples are presented for demonstrating these tools using shapes from Kimia shape database and the Surrey fish database."'),
('"Hierarchical Properties of Multi-resolution Optical Flow Computation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33868-7_57"', '"Most of the methods to compute optical flows are variational-technique-based methods, which assume that image functions have spatiotemporal continuities and appearance motions are small. In the viewpoint of the discrete errors of spatial- and time-differentials, the appropriate resolution for optical flow depends on both the resolution and the frame rate of images since there is a problem with the accuracy of the discrete approximations of derivatives. Therefore, for low frame-rate images, the appropriate resolution for optical flow should be lower than the resolution of the images. However, many traditional methods estimate optical flow with the same resolution as the images. Therefore, if the resolution of images is too high, down-sampling the images is effective for the variational-technique-based methods. In this paper, we analyze the appropriate resolutions for optical flows estimated by variational optical-flow computations from the viewpoint of the error analysis of optical flows. To analyze the appropriate resolutions, we use hierarchical structures constructed from the multi-resolutions of images. Numerical results show that decreasing image resolutions is effective for computing optical flows by variational optical-flow computations in low frame-rate sequences."'),
('"Hierarchical Shape Modeling for Automatic Face Localization"', '"ECCV 2002"', '["Markov Chain Monte Carlo", "Feature Point", "Face Image", "Gaussian Mixture Model", "Shape Model"]', '"https://doi.org/10.1007/3-540-47967-8_46"', '"Many approaches have been proposed to locate faces in an image. There are, however, two problems in previous facial shape models using feature points. First, the dimension of the solution space is too big since a large number of key points are needed to model a face. Second, the local features associated with the key points are assumed to be independent. Therefore, previous approaches require good initialization (which is often done manually), and may generate inaccurate localization. To automatically locate faces, we propose a novel hierarchical shape model (HSM) or multi-resolution shape models corresponding to a Gaussian pyramid of the face image. The coarsest shape model can be quickly located in the lowest resolution image. The located coarse model is then used to guide the search for a finer face model in the higher resolution image. Moreover, we devise a Global and Local (GL) distribution to learn the likelihood of the joint distribution of facial features. A novel hierarchical data-driven Markov chain Monte Carlo (HDDMCMC) approach is proposed to achieve the global optimum of face localization. Experimental results demonstrate that our algorithm produces accurate localization results quickly, bypassing the need for good initialization."'),
('"Hierarchical Support Vector Random Fields: Joint Training to Combine Local and Global Features"', '"ECCV 2008"', '["Part Assignment", "Evidence Aggregation", "Loopy Belief Propagation", "Joint Training", "Newton Op', '"https://doi.org/10.1007/978-3-540-88688-4_39"', '"Recently, impressive results have been reported for the detection of objects in challenging real-world scenes. Interestingly however, the underlying models vary greatly even between the most successful approaches. Methods using a global feature descriptor (e.g. ) paired with discriminative classifiers such as SVMs enable high levels of performance, but require large amounts of training data and typically degrade in the presence of partial occlusions. Local feature-based approaches (e.g. ) are more robust in the presence of partial occlusions but often produce a significant number of false positives. This paper proposes a novel approach called hierarchical support vector random field that allows 1) to combine the power of global feature-based approaches with the flexibility of local feature-based methods in one consistent multi-layer framework and 2) to automatically learn the tradeoff and the optimal interplay between local, semi-local and global feature contributions. Experiments show that both the combination of local and global features as well as the joint training result in improved detection performance on challenging datasets."'),
('"High Accuracy Optical Flow Estimation Based on a Theory for Warping"', '"ECCV 2004"', '["IEEE Computer Society", "Motion Estimation", "Angular Error", "Point Iteration", "Smoothness Const', '"https://doi.org/10.1007/978-3-540-24673-2_3"', '"We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise."'),
('"High Accuracy Optical Flow Serves 3-D Pose Tracking: Exploiting Contour and Flow Based Constraints"', '"ECCV 2006"', '["Block Match", "Point Correspondence", "Contour Extraction", "Constancy Assumption", "Pose Estimati', '"https://doi.org/10.1007/11744047_8"', '"Tracking the 3-D pose of an object needs correspondences between 2-D features in the image and their 3-D counterparts in the object model. A large variety of such features has been suggested in the literature. All of them have drawbacks in one situation or the other since their extraction in the image and/or the matching is prone to errors. In this paper, we propose to use two complementary types of features for pose tracking, such that one type makes up for the shortcomings of the other. Aside from the object contour, which is matched to a free-form object surface, we suggest to employ the optic flow in order to compute additional point correspondences. Optic flow estimation is a mature research field with sophisticated algorithms available. Using here a high quality method ensures a reliable matching. In our experiments we demonstrate the performance of our method and in particular the improvements due to the optic flow."'),
('"High Accuracy TOF and Stereo Sensor Fusion at Interactive Rates"', '"ECCV 2012"', '["Interactive Rate", "Sensor Fusion", "Stereo Match", "Total Variation Regularization", "Ground Trut', '"https://doi.org/10.1007/978-3-642-33868-7_1"', '"We propose two new GPU-based sensor fusion approaches for time of flight (TOF) and stereo depth data. Data fidelity measures are defined to deal with the fundamental limitations of both techniques alone. Our algorithms combine TOF and stereo, yielding megapixel depth maps, enabling our approach to be used in a movie production scenario. Our local model works at interactive rates but yields noisier results, whereas our variational technique is more robust at a higher computational cost. The results show an improvement over each individual method with TOF interreflection remaining an open challenge. To encourage quantitative evaluations, a ground truth dataset is made publicly available."'),
('"High Dynamic Range Imaging System for the Visually Impaired"', '"ECCV 2014"', '["High Dynamic Range (HDR)", "Contrast enhancement", "Assistive devices", "Visually impaired", "Low ', '"https://doi.org/10.1007/978-3-319-16199-0_44"', '"This paper describes a portable High Dynamic Range (HDR) imaging system for visually impaired people, intended to display contrast enhanced images of real world environment. The device is composed of a digital camera and head mounted display (HMD) equipped with high resolution screens. The camera is mounted on the HMD to acquire the ambient scene, the acquired images are processed to generate HDR images through the control of local luminance information. The contrast enhancement method adopted in our system is based on pyramidal image contrast structure representation that relies on the local band-limited contrast definition. The imaging system we propose aims at displaying images that meet the visual capabilities related to contrast sensitivity of people with low vision. It also provides a solution to alleviate discomfort problem expressed by these people when they are facing real-world changing light conditions."'),
('"High Information Rate and Efficient Color Barcode Decoding"', '"ECCV 2012"', '["Information Rate", "Color Constancy", "Color Patch", "Color Palette", "Reference Color"]', '"https://doi.org/10.1007/978-3-642-33868-7_48"', '"The necessity of increasing information density in a given space motivates the use of more colors in color barcodes. A popular system, Microsoft\\u2019s HCCB technology, uses four or eight colors per patch. This system displays a color palette of four or eight colors in the color barcode to solve the problem with the dependency of the surface color on the illuminant spectrum, viewing parameters, and other sources. Since the displayed colors cannot be used to encode information, this solution comes at the cost of reduced information rate. In this contribution, we introduce a new approach to color barcode decoding that uses 24 colors per patch and requires a small number of reference colors to display in a barcode. Our algorithm builds groups of colors from each color patch and a small number of reference color patches, and models their evolution due to changing illuminant using a linear subspace. Therefore, each group of colors is represented by one such subspace. Our experimental results show that our barcode decoding algorithm achieves higher information rate with a very low probability of decoding error compared to systems that do display a color palette. The computational complexity of our algorithm is relatively low due to searching for the nearest subspace among 24 subspaces only."'),
('"High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging"', '"ECCV 2004"', '["Video Frame", "Color Channel", "Range Image", "Stripe Pattern", "Light Pattern"]', '"https://doi.org/10.1007/978-3-540-24670-1_8"', '"For structured-light range imaging, color stripes can be used for increasing the number of distinguishable light patterns compared to binary BW stripes. Therefore, an appropriate use of color patterns can reduce the number of light projections and range imaging is achievable in single video frame or in \\u201cone shot\\u201d. On the other hand, the reliability and range resolution attainable from color stripes is generally lower than those from multiply projected binary BW patterns since color contrast is affected by object color reflectance and ambient light. This paper presents new methods for selecting stripe colors and designing multiple-stripe patterns for \\u201cone-shot\\u201d and \\u201ctwo-shot\\u201d imaging. We show that maximizing color contrast between the stripes in one-shot imaging reduces the ambiguities resulting from colored object surfaces and limitations in sensor/projector resolution. Two-shot imaging adds an extra video frame and maximizes the color contrast between the first and second video frames to diminish the ambiguities even further. Experimental results demonstrate the effectiveness of the presented one-shot and two-shot color-stripe imaging schemes."'),
('"High-Resolution Plant Shape Measurements from Multi-view Stereo Reconstruction"', '"ECCV 2014"', '["Stereo Reconstruction", "Convex Optimization", "Plant Phenotyping", "Octrees"]', '"https://doi.org/10.1007/978-3-319-16220-1_13"', '"Accurate high-resolution 3D models are essential for a non-invasive analysis of phenotypic characteristics of plants. Leaf surface areas, fruit volumes and leaf inclination angles are typically of interest. This work presents a globally optimal 3D geometry reconstruction method that is specialized to high-resolutions and is thus suitable to reconstruct thin structures typically occuring in the geometry of plants. Volumetric 3D models are computed in a convex optimization framework from a set of RGB input images depicting the plant from different view points. The method uses the memory and run-time efficient octree data structure for fast computations of high-resolution 3D models. Results show accurate 3D reconstructions of barley, while an increase in resolution of a factor of up to 2000 is achieved in comparison to the use of a uniform voxel based data structure, making the choice of data structure crucial for feasible resolutions."'),
('"Higher Dimensional Affine Registration and Vision Applications"', '"ECCV 2008"', '["Registration Algorithm", "Vision Application", "Iterative Close Point", "Iterative Close Point Alg', '"https://doi.org/10.1007/978-3-540-88693-8_19"', '"Affine registration has a long and venerable history in computer vision literature, and extensive work have been done for affine registrations in \\u211d2 and \\u211d3. In this paper, we study affine registrations in \\u211d m for m\\u2009>\\u20093, and to justify breaking this dimension barrier, we show two interesting types of matching problems that can be formulated and solved as affine registration problems in dimensions higher than three: stereo correspondence under motion and image set matching. More specifically, for an object undergoing non-rigid motion that can be linearly modelled using a small number of shape basis vectors, the stereo correspondence problem can be solved by affine registering points in \\u211d3n . And given two collections of images related by an unknown linear transformation of the image space, the correspondences between images in the two collections can be recovered by solving an affine registration problem in \\u211d m , where m is the dimension of a PCA subspace. The algorithm proposed in this paper estimates the affine transformation between two point sets in \\u211d m . It does not require continuous optimization, and our analysis shows that, in the absence of data noise, the algorithm will recover the exact affine transformation for almost all point sets with the worst-case time complexity of O(mk 2), k the size of the point set. We validate the proposed algorithm on a variety of synthetic point sets in different dimensions with varying degrees of deformation and noise, and we also show experimentally that the two types of matching problems can indeed be solved satisfactorily using the proposed affine registration algorithm."'),
('"Higher Order Image Pyramids"', '"ECCV 2006"', '["Face Recognition", "Scale Invariance", "Natural Image", "Illumination Change", "Step Edge"]', '"https://doi.org/10.1007/11744047_24"', '"The scale invariant property of an ensemble of natural images is examined which motivates a new early visual representation termed the higher order pyramid. The representation is a non-linear generalization of the Laplacian pyramid and is tuned to the type of scale invariance exhibited by natural imagery as opposed to other scale invariant images such as 1/f correlated noise and the step edge. The transformation of an image to a higher order pyramid is simple to compute and straightforward to invert. Because the representation is invertible it is shown that the higher order pyramid can be truncated and quantized with little loss of visual quality. Images coded in this representation have much less redundancy than the raw image pixels and decorrelating transformations such as the Laplacian pyramid. This is demonstrated by showing statistical independence between pairs of coefficients. Because the representation is tuned to the ensemble redundancies the coefficients of the higher order pyramid are more efficient at capturing the variation within the ensemble which leads too improved matching results. This is demonstrated on two recognition tasks, face recognition with illumination changes and object recognition which viewpoint changes."'),
('"Highlight Removal Using Shape-from-Shading"', '"ECCV 2002"', '["Matte Image", "Photometric Stereo", "Iterate Conditional Mode", "Light Source Direction", "Lambert', '"https://doi.org/10.1007/3-540-47967-8_42"', '"One of the problems that hinders the application of conventional methods for shape-from-shading to the analysis of shiny objects is the presence of local highlights. The first of these are specularities which appear at locations on the viewed object where the local surface normal is the bisector of the light source and viewing directions. Highlights also occur at the occluding limb of the object where roughness results in backscattering from microfacets which protrude above the surface. In this paper, we consider how to subtract both types of highlight from shiny surfaces in order to improve the quality of surface normal information recoverable using shape-from-shading."'),
('"Highly Overparameterized Optical Flow Using PatchMatch Belief Propagation"', '"ECCV 2014"', '["Optical flow", "large displacement", "9 DoF", "PatchMatch", "PMBP"]', '"https://doi.org/10.1007/978-3-319-10578-9_15"', '"Motion in the image plane is ultimately a function of 3D motion in space. We propose to compute optical flow using what is ostensibly an extreme overparameterization: depth, surface normal, and frame-to-frame 3D rigid body motion at every pixel, giving a total of 9 DoF. The advantages of such an overparameterization are twofold: first, geometrically meaningful reasoning can be called upon in the optimization, reflecting possible 3D motion in the underlying scene; second, the \\u2018fronto-parallel\\u2019 assumption implicit in the use of traditional matching pixel windows is ameliorated because the parameterization determines a plane-induced homography at every pixel. We show that optimization over this high-dimensional, continuous state space can be carried out using an adaptation of the recently introduced PatchMatch Belief Propagation (PMBP) energy minimization algorithm, and that the resulting flow fields compare favorably to the state of the art on a number of small- and large-displacement datasets."'),
('"Hipster Wars: Discovering Elements of Fashion Styles"', '"ECCV 2014"', '["Skill Level", "Online Shopping", "Personal Style", "Computer Vision Community", "Style Category"]', '"https://doi.org/10.1007/978-3-319-10590-1_31"', '"The clothing we wear and our identities are closely tied, revealing to the world clues about our wealth, occupation, and socio-identity. In this paper we examine questions related to what our clothing reveals about our personal style. We first design an online competitive Style Rating Game called Hipster Wars to crowd source reliable human judgments of style. We use this game to collect a new dataset of clothing outfits with associated style ratings for 5 style categories: hipster, bohemian, pinup, preppy, and goth. Next, we train models for between-class and within-class classification of styles. Finally, we explore methods to identify clothing elements that are generally discriminative for a style, and methods for identifying items in a particular outfit that may indicate a style."'),
('"HiRF: Hierarchical Random Field for Collective Activity Recognition in Videos"', '"ECCV 2014"', '["Activity recognition", "hierarchical graphical models"]', '"https://doi.org/10.1007/978-3-319-10599-4_37"', '"This paper addresses the problem of recognizing and localizing coherent activities of a group of people, called collective activities, in video. Related work has argued the benefits of capturing long-range and higher-order dependencies among video features for robust recognition. To this end, we formulate a new deep model, called Hierarchical Random Field (HiRF). HiRF models only hierarchical dependencies between model variables. This effectively amounts to modeling higher-order temporal dependencies of video features. We specify an efficient inference of HiRF that iterates in each step linear programming for estimating latent variables. Learning of HiRF parameters is specified within the max-margin framework. Our evaluation on the benchmark New Collective Activity and Collective Activity datasets, demonstrates that HiRF yields superior recognition and localization as compared to the state of the art."'),
('"Homeomorphic Manifold Analysis: Learning Decomposable Generative Models for Human Motion Analysis"', '"WDV 2006"', '["Facial Expression", "Unit Circle", "Orthogonal Factor", "Manifold Representation", "Nonlinear Dime', '"https://doi.org/10.1007/978-3-540-70932-9_8"', '"If we consider the appearance of human motion such as gait, facial expression and gesturing, most of such activities result in nonlinear manifolds in the image space. Although the intrinsic body configuration manifolds might be very low in dimensionality, the resulting appearance manifold is challenging to model given various aspects that affects the appearance such as the view point, the person shape and appearance, etc. In this paper we learn decomposable generative models that explicitly decompose the intrinsic body configuration as a function of time from other conceptually orthogonal aspects that affects the appearance such as the view point, the person performing the action, etc. The frameworks is based on learning nonlinear mappings from a conceptual representation of the motion manifold that is homeomorphic to the actual manifold and decompose other sources of variation in the mapping coefficient space."'),
('"Homography Tensors: On Algebraic Entities That Represent Three Views of Static or Moving Planar Poi', '"ECCV 2000"', '["Static Point", "Matching Point", "Moving Point", "Covariant Vector", "Independent Constraint"]', '"https://doi.org/10.1007/3-540-45054-8_33"', '"We introduce a 3 \\u00d7 3 \\u00d7 3 tensor H ijk and its dual H ijk which represent the 2D projective mapping of points across three projections (views). The tensor H ijk is a generalization of the well known 2D collineation matrix (homography matrix) and it concatenates two homography matrices to represent the joint mapping across three views. The dual tensor H ijk concatenates two dual homography matrices (mappings of line space) and is responsible for representing the mapping associated with moving points along straight-line paths, i.e., H ijk can be recovered from line-of-sight measurements only."'),
('"HOPC: Histogram of Oriented Principal Components of 3D Pointclouds for Action Recognition"', '"ECCV 2014"', '["Spatio-temporal keypoints", "multiview action dataset"]', '"https://doi.org/10.1007/978-3-319-10605-2_48"', '"Existing techniques for 3D action recognition are sensitive to viewpoint variations because they extract features from depth images which change significantly with viewpoint. In contrast, we directly process the pointclouds and propose a new technique for action recognition which is more robust to noise, action speed and viewpoint variations. Our technique consists of a novel descriptor and keypoint detection algorithm. The proposed descriptor is extracted at a point by encoding the Histogram of Oriented Principal Components (HOPC) within an adaptive spatio-temporal support volume around that point. Based on this descriptor, we present a novel method to detect Spatio-Temporal Key-Points (STKPs) in 3D pointcloud sequences. Experimental results show that the proposed descriptor and STKP detector outperform state-of-the-art algorithms on three benchmark human activity datasets. We also introduce a new multiview public dataset and show the robustness of our proposed method to viewpoint variations."'),
('"Hough Forest-Based Facial Expression Recognition from Video Sequences"', '"ECCV 2010"', '["Facial expression recognition", "generalised Hough transform"]', '"https://doi.org/10.1007/978-3-642-35749-7_15"', '"Automatic recognition of facial expression is a necessary step toward the design of more natural human-computer interaction systems. This work presents a user-independent approach for the recognition of facial expressions from image sequences. The faces are normalized in scale and rotation based on the eye centers\\u2019 locations into tracks from which we extract features representing shape and motion. Classification and localization of the center of the expression in the video sequences are performed using a Hough transform voting method based on randomized forests. We tested our approach on two publicly available databases and achieved encouraging results comparable to the state of the art."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Hough Regions for Joining Instance Localization and Segmentation"', '"ECCV 2012"', '["Object Detection", "Seed Region", "Conditional Random Field", "Object Instance", "Category Instanc', '"https://doi.org/10.1007/978-3-642-33712-3_19"', '"Object detection and segmentation are two challenging tasks in computer vision, which are usually considered as independent steps. In this paper, we propose a framework which jointly optimizes for both tasks and implicitly provides detection hypotheses and corresponding segmentations. Our novel approach is attachable to any of the available generalized Hough voting methods. We introduce Hough Regions by formulating the problem of Hough space analysis as Bayesian labeling of a random field. This exploits provided classifier responses, object center votes and low-level cues like color consistency, which are combined into a global energy term. We further propose a greedy approach to solve this energy minimization problem providing a pixel-wise assignment to background or to a specific category instance. This way we bypass the parameter sensitive non-maximum suppression that is required in related methods. The experimental evaluation demonstrates that state-of-the-art detection and segmentation results are achieved and that our method is inherently able to handle overlapping instances and an increased range of articulations, aspect ratios and scales."'),
('"Hough Transform and 3D SURF for Robust Three Dimensional Classification"', '"ECCV 2010"', '["Visual Word", "Interest Point", "Hough Transform", "Shape Retrieval", "Visual Vocabulary"]', '"https://doi.org/10.1007/978-3-642-15567-3_43"', '"Most methods for the recognition of shape classes from 3D datasets focus on classifying clean, often manually generated models. However, 3D shapes obtained through acquisition techniques such as Structure-from-Motion or LIDAR scanning are noisy, clutter and holes. In that case global shape features\\u2014still dominating the 3D shape class recognition literature\\u2014are less appropriate. Inspired by 2D methods, recently researchers have started to work with local features. In keeping with this strand, we propose a new robust 3D shape classification method. It contains two main contributions. First, we extend a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes. Second, we show how 3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D. Through our experiments on partial shape retrieval, we show the power of the proposed 3D features. Their combination with the Hough transform yields superior results for class recognition on standard datasets. The potential for the applicability of such a method in classifying 3D obtained from Structure-from-Motion methods is promising, as we show in some initial experiments."'),
('"How Does Aging Affect Facial Components?"', '"ECCV 2012"', '["face recognition", "aging", "facial components", "demographics"]', '"https://doi.org/10.1007/978-3-642-33868-7_19"', '"There is growing interest in achieving age invariant face recognition due to its wide applications in law enforcement. The challenge lies in that face aging is quite a complicated process, which involves both intrinsic and extrinsic factors. Face aging also influences individual facial components (such as the mouth, eyes, and nose) differently. We propose a component based method for age invariant face recognition. Facial components are automatically localized based on landmarks detected using an Active Shape Model. Multi-scale local binary pattern and scale-invariant feature transform features are then extracted from each component, followed by random subspace linear discriminant analysis for classification. With a component based representation, we study how aging influences individual facial components on two large aging databases (MORPH Album2 and PCSO). Per component performance analysis shows that the nose is the most stable component during face aging. Age invariant recognition exploiting demographics shows that face aging has more influence on females than males. Overall, recognition performance on the two databases shows that the proposed component based approach is more robust to large time lapses than FaceVACS, a leading commercial face matcher."'),
('"How Does CONDENSATION Behave with a Finite Number of Samples?"', '"ECCV 2000"', '["Markov Chain", "State Transition Matrix", "Prior Density", "Stochastic Simulation Algorithm", "Hom', '"https://doi.org/10.1007/3-540-45054-8_45"', '"Condensationis a popular algorithm for sequential inference that resamples a sampled representation of the posterior. The algorithm is known to be asymptotically correct as the number of samples tends to infinity. However, the resampling phase involves a loss of information. The sequence of representations produced by the algorithm is a Markov chain, which is usually inhomogeneous. We show simple discrete examples where this chain is homogeneous and has absorbing states. In these examples, the representation moves to one of these states in time apparently linear in the number of samples and remains there. This phenomenon appears in the continuous case as well, where the algorithm tends to produce \\u201cclumpy\\u201d representations. In practice, this means that different runs of a tracker on the same data can give very different answers, while a particular run of the tracker will look stable. Furthermore, the state of the tracker can collapse to a single peak \\u2014 which has non-zero probability of being the wrong peak \\u2014 within time linear in the number of samples, and the tracker can appear to be following tight peaks in the posterior even in the absence of any meaningful measurement. This means that, if theoretical lower bounds on the number of samples are not available, experiments must be very carefully designed to avoid these effects."'),
('"How Important Are \\u201cDeformable Parts\\u201d in the Deformable Parts Model?"', '"ECCV 2012"', '["Object Detection", "Good Initialization", "Part Deformation", "Deformable Part", "Camera Viewpoint', '"https://doi.org/10.1007/978-3-642-33885-4_4"', '"The Deformable Parts Model (DPM) has recently emerged as a very useful and popular tool for tackling the intra-category diversity problem in object detection. In this paper, we summarize the key insights from our empirical analysis of the important elements constituting this detector. More specifically, we study the relationship between the role of deformable parts and the mixture model components within this detector, and understand their relative importance. First, we find that by increasing the number of components, and switching the initialization step from their aspect-ratio, left-right flipping heuristics to appearance-based clustering, considerable improvement in performance is obtained. But more intriguingly, we observed that with these new components, the part deformations can now be turned off, yet obtaining results that are almost on par with the original DPM detector."'),
('"How Industrial Robots Benefit from Affordances"', '"ECCV 2014"', '["Bayesian Network", "Mobile Manipulator", "Industrial Robot", "Robot Control", "European Regional D', '"https://doi.org/10.1007/978-3-319-16181-5_35"', '"In this paper we discuss the potential of Gibson\\u2019s affordance concept in industrial robotics. Recent advances in robotics introduce more and more robots to collaborate with human co-workers in industrial environments, more ambitious development of using mobile manipulators in industrial environments has also received widespread attentions. We investigate how the conventional robotic affordance concept fits the pragmatic industrial robotic applications with the focuses on flexibility, re-purposing and safety."'),
('"How Much Information Kinect Facial Depth Data Can Reveal About Identity, Gender and Ethnicity?"', '"ECCV 2014"', '["Face Recognition", "Local Binary Pattern", "Depth Image", "Iterative Close Point", "Kinect Sensor"', '"https://doi.org/10.1007/978-3-319-16181-5_55"', '"Human face images acquired using conventional 2D cameras may have inherent restrictions that hinder the inference of some specific information in the face. The low-cost depth sensors such as Microsoft Kinect introduced in late 2010 allow extracting directly 3D information, together with RGB color images. This provides new opportunities for computer vision and face analysis research. Although more accurate sensors for detailed facial image analysis are expected to be available soon (e.g. Kinect 2), this paper investigates the usefulness of the depth images provided by the current Microsoft Kinect sensors in different face analysis tasks. We conduct an in-depth study comparing the performance of the depth images provided by Microsoft Kinect sensors against RGB counterpart images in three face analysis tasks, namely identity, gender and ethnicity. Four local feature extraction methods are investigated for both face texture and shape description. Moreover, the two modalities (i.e. depth and RGB) are fused to gain insight into their complementarity. The experimental analysis conducted on two publicly available kinect face databases, EurecomKinect and Curtinfaces, yields into interesting results."'),
('"How to Measure the Relevance of a Retargeting Approach?"', '"ECCV 2010"', '["Video Sequence", "Visual Attention", "Video Content", "Coverage Ratio", "Temporal Consistency"]', '"https://doi.org/10.1007/978-3-642-35740-4_13"', '"Most cell phones today can receive and display video content. Nonetheless, we are still significantly behind the point where premium made for mobile content is mainstream, largely available, and affordable. Significant issues must be overcome. The small screen size is one of them. Indeed, the direct transfer of conventional contents (i.e. not specifically shot for mobile devices) will provide a video in which the main characters or objects of interest may become indistinguishable from the rest of the scene. Therefore, it is required to retarget the content. Different solutions exist, either based on distortion of the image, on removal of redundant areas, or cropping. The most efficient ones are based on dynamic adaptation of the cropping window. They significantly improve the viewing experience by zooming in the regions of interest. Currently, there is no common agreement on how to compare different solutions. A retargeting metric is proposed in order to gauge its quality. Eye-tracking experiments, zooming effect through coverage ratio and temporal consistency are introduced and discussed."'),
('"How to Supervise Topic Models"', '"ECCV 2014"', '["Topic modeling", "SLDA", "LDA", "Factorized supervised topic models"]', '"https://doi.org/10.1007/978-3-319-16181-5_39"', '"Supervised topic models are important machine learning tools which have been widely used in computer vision as well as in other domains. However, there is a gap in the understanding of the supervision impact on the model. In this paper, we present a thorough analysis on the behaviour of supervised topic models using Supervised Latent Dirichlet Allocation (SLDA) and propose two factorized supervised topic models, which factorize the topics into signal and noise. Experimental results on both synthetic data and real-world data for computer vision tasks show that supervision need to be boosted to be effective and factorized topic models are able to enhance the performance."'),
('"Human Action Recognition by Random Features and Hand-Crafted Features: A Comparative Study"', '"ECCV 2014"', '["Action recognition", "Hand-crafted feature", "Random representation"]', '"https://doi.org/10.1007/978-3-319-16181-5_2"', '"One popular approach for human action recognition is to extract features from videos as representations, subsequently followed by a classification procedure of the representations. In this paper, we investigate and compare hand-crafted and random feature representation for human action recognition on YouTube dataset. The former is built on 3D HoG/HoF and SIFT descriptors while the latter bases on random projection. Three encoding methods: Bag of Feature(BoF), Sparse Coding(SC) and VLAD are adopted. Spatial temporal pyramid and a two-layer SVM classifier are employed for classification. Our experiments demonstrate that: 1) Sparse Coding is confirmed to outperform Bag of Feature; 2) Using a model of hybrid features incorporating frame-static can significantly improve the overall recognition accuracy; 3) The frame-static features works surprisingly better than motion features only; 4) Compared with the success of hand-crafted feature representation, the random feature representation does not perform well in this dataset."'),
('"Human Activities as Stochastic Kronecker Graphs"', '"ECCV 2012"', '["Kronecker Product", "Equal Error Rate", "Training Video", "Activity Primitive", "Video Feature"]', '"https://doi.org/10.1007/978-3-642-33709-3_10"', '"A human activity can be viewed as a space-time repetition of activity primitives. Both instances of the primitives, and their repetition are stochastic. They can be modeled by a generative model-graph, where nodes correspond to the primitives, and the graph\\u2019s adjacency matrix encodes their affinities for probabilistic grouping into observable video features. When a video of the activity is represented by a graph capturing the space-time layout of video features, such a video graph can be viewed as probabilistically sampled from the activity\\u2019s model-graph. This sampling is formulated as a successive Kronecker multiplication of the model\\u2019s affinity matrix. The resulting Kronecker-power matrix is taken as a noisy permutation of the adjacency matrix of the video graph. The paper presents our: 1) model-graph; 2) memory- and time-efficient, weakly supervised learning of activity primitives and their affinities; and 3) inference aimed at finding the best expected correspondences between the primitives and observed video features. Our results demonstrate good scalability on UCF50, and superior performance to that of the state of the art on individual, structured, and collective activities of UCF YouTube, Olympic, and Collective datasets."'),
('"Human Activity Recognition with Metric Learning"', '"ECCV 2008"', '["Activity Recognition", "Random Projection", "Human Activity Recognition", "Motion Capture Data", "', '"https://doi.org/10.1007/978-3-540-88682-2_42"', '"This paper proposes a metric learning based approach for human activity recognition with two main objectives: (1) reject unfamiliar activities and (2) learn with few examples. We show that our approach outperforms all state-of-the-art methods on numerous standard datasets for traditional action classification problem. Furthermore, we demonstrate that our method not only can accurately label activities but also can reject unseen activities and can learn from few examples with high accuracy. We finally show that our approach works well on noisy YouTube videos."'),
('"Human Attributes from 3D Pose Tracking"', '"ECCV 2010"', '["Domain Adaptation", "Biological Motion", "Human Attribute", "Transfer Learning", "Gait Recognition', '"https://doi.org/10.1007/978-3-642-15558-1_18"', '"We show that, from the output of a simple 3D human pose tracker one can infer physical attributes (e.g., gender and weight) and aspects of mental state (e.g., happiness or sadness). This task is useful for man-machine communication, and it provides a natural benchmark for evaluating the performance of 3D pose tracking methods (vs. conventional Euclidean joint error metrics). Based on an extensive corpus of motion capture data, with physical and perceptual ground truth, we analyze the inference of subtle biologically-inspired attributes from cyclic gait data. It is shown that inference is also possible with partial observations of the body, and with motions as short as a single gait cycle. Learning models from small amounts of noisy video pose data is, however, prone to over-fitting. To mitigate this we formulate learning in terms of domain adaptation, for which mocap data is uses to regularize models for inference from video-based data."'),
('"Human Daily Action Analysis with Multi-view and Color-Depth Data"', '"ECCV 2012"', '["Daily action", "Multi-View", "RGB-D", "Depth descriptor"]', '"https://doi.org/10.1007/978-3-642-33868-7_6"', '"Improving human action recognition in videos is restricted by the inherent limitations of the visual data. In this paper, we take the depth information into consideration and construct a novel dataset of human daily actions. The proposed ACT42 dataset provides synchronized data from 4 views and 2 sources, aiming to facilitate the research of action analysis across multiple views and multiple sources. We also propose a new descriptor of depth information for action representation, which depicts the structural relations of spatiotemporal points within action volume using the distance information in depth data. In experimental validation, our descriptor obtains superior performance to the state-of-the-art action descriptors designed for color information, and more robust to viewpoint variations. The fusion of features from different sources is also discussed, and a simple but efficient method is presented to provide a baseline performance on the proposed dataset."'),
('"Human Detection Based on a Probabilistic Assembly of Robust Part Detectors"', '"ECCV 2004"', '["Body Part", "Face Detection", "Human Detection", "Dominant Orientation", "Part Detector"]', '"https://doi.org/10.1007/978-3-540-24670-1_6"', '"We describe a novel method for human detection in single images which can detect full bodies as well as close-up views in the presence of clutter and occlusion. Humans are modeled as flexible assemblies of parts, and robust part detection is the key to the approach. The parts are represented by co-occurrences of local features which captures the spatial layout of the part\\u2019s appearance. Feature selection and the part detectors are learnt from training images using AdaBoost."'),
('"Human Detection Using Learned Part Alphabet and Pose Dictionary"', '"ECCV 2014"', '["Human detection", "mid-level elements", "part alphabet", "pose dictionary", "matching"]', '"https://doi.org/10.1007/978-3-319-10602-1_17"', '"As structured data, human body and text are similar in many aspects. In this paper, we make use of the analogy between human body and text to build a compositional model for human detection in natural scenes. Basic concepts and mature techniques in text recognition are introduced into this model. A discriminative alphabet, each grapheme of which is a mid-level element representing a body part, is automatically learned from bounding box labels. Based on this alphabet, the flexible structure of human body is expressed by means of symbolic sequences, which correspond to various human poses and allow for robust, efficient matching. A pose dictionary is constructed from training examples, which is used to verify hypotheses at runtime. Experiments on standard benchmarks demonstrate that the proposed algorithm achieves state-of-the-art or competitive performance."'),
('"Human Detection Using Oriented Histograms of Flow and Appearance"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744047_33"', '"Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8% miss rate on our Test Set 1."'),
('"Human Focused Action Localization in Video"', '"ECCV 2010"', '["Action recognition", "localization", "human tracking", "HOG"]', '"https://doi.org/10.1007/978-3-642-35749-7_17"', '"We propose a novel human-centric approach to detect and localize human actions in challenging video data, such as Hollywood movies. Our goal is to localize actions in time through the video and spatially in each frame. We achieve this by first obtaining generic spatio-temporal human tracks and then detecting specific actions within these using a sliding window classifier."'),
('"Human Pose Estimation Using Learnt Probabilistic Region Similarities and Partial Configurations"', '"ECCV 2004"', '["Body Part", "Probabilistic Region", "Part Likelihood", "Single Part", "Object Occlusion"]', '"https://doi.org/10.1007/978-3-540-24673-2_24"', '"A model of human appearance is presented for efficient pose estimation from real-world images. In common with related approaches, a high-level model defines a space of configurations which can be associated with image measurements and thus scored. A search is performed to identify good configuration(s). Such an approach is challenging because the configuration space is high dimensional, the search is global, and the appearance of humans in images is complex due to background clutter, shape uncertainty and texture."'),
('"Human Pose Estimation with Fields of Parts"', '"ECCV 2014"', '["Human Pose Estimation", "Efficient Inference"]', '"https://doi.org/10.1007/978-3-319-10602-1_22"', '"This paper proposes a new formulation of the human pose estimation problem. We present the Fields of Parts model, a binary Conditional Random Field model designed to detect human body parts of articulated people in single images."'),
('"Human Pose Tracking Using Multi-level Structured Models"', '"ECCV 2006"', '["Belief Propagation", "State Candidate", "Observation Function", "Motion Capture Data", "Body Joint', '"https://doi.org/10.1007/11744078_29"', '"Tracking body poses of multiple persons in monocular video is a challenging problem due to the high dimensionality of the state space and issues such as inter-occlusion of the persons\\u2019 bodies. We proposed a three-stage approach with a multi-level state representation that enables a hierarchical estimation of 3D body poses. At the first stage, humans are tracked as blobs. In the second stage, parts such as face, shoulders and limbs are estimated and estimates are combined by grid-based belief propagation to infer 2D joint positions. The derived belief maps are used as proposal functions in the third stage to infer the 3D pose using data-driven Markov chain Monte Carlo. Experimental results on realistic indoor video sequences show that the method is able to track multiple persons during complex movement such as turning movement with inter-occlusion."'),
('"Human Upper Body Pose Estimation in Static Images"', '"ECCV 2004"', '["Prior Distribution", "Proposal Distribution", "Image Position", "Image Observation", "Body Joint"]', '"https://doi.org/10.1007/978-3-540-24671-8_10"', '"Estimating human pose in static images is challenging due to the high dimensional state space, presence of image clutter and ambiguities of image observations. We present an MCMC framework for estimating 3D human upper body pose. A generative model, comprising of the human articulated structure, shape and clothing models, is used to formulate likelihood measures for evaluating solution candidates. We adopt a data-driven proposal mechanism for searching the solution space efficiently. We introduce the use of proposal maps, which is an efficient way of implementing inference proposals derived from multiple types of image cues. Qualitative and quantitative results show that the technique is effective in estimating 3D body pose over a variety of images."'),
('"Human-Centric Indoor Environment Modeling from Depth Videos"', '"ECCV 2012"', '["Scene understanding", "environment modeling", "human-centric", "depth videos"]', '"https://doi.org/10.1007/978-3-642-33868-7_5"', '"We propose an approach to model indoor environments from depth videos (the camera is stationary when recording the videos), which includes extracting the 3-D spatial layout of the rooms and modeling objects as 3-D cuboids. Different from previous work which purely relies on image appearance, we argue that indoor environment modeling should be human-centric: not only because humans are an important part of the indoor environments, but also because the interaction between humans and environments can convey much useful information about the environments. In this paper, we develop an approach to extract physical constraints from human poses and motion to better recover the spatial layout and model objects inside. We observe that the cues provided by human-environment intersection are very powerful: we don\\u2019t have a lot of training data but our method can still achieve promising performance. Our approach is built on depth videos, which makes it more user friendly."'),
('"Hybrid Classifiers for Object Classification with a Rich Background"', '"ECCV 2012"', '["Natural Image", "Kernel Matrix", "Probability Constraint", "Negative Class", "Random Projection"]', '"https://doi.org/10.1007/978-3-642-33715-4_21"', '"The majority of current methods in object classification use the one-against-rest training scheme. We argue that when applied to a large number of classes, this strategy is problematic: as the number of classes increases, the negative class becomes a very large and complicated collection of images. The resulting classification problem then becomes extremely unbalanced, and kernel SVM classifiers trained on such sets require long training time and are slow in prediction. To address these problems, we propose to consider the negative class as a background and characterize it by a prior distribution. Further, we propose to construct \\u201dhybrid\\u201d classifiers, which are trained to separate this distribution from the samples of the positive class. A typical classifier first projects (by a function which may be non-linear) the inputs to a one-dimensional space, and then thresholds this projection. Theoretical results and empirical evaluation suggest that, after projection, the background has a relatively simple distribution, which is much easier to parameterize and work with. Our results show that hybrid classifiers offer an advantage over SVM classifiers, both in performance and complexity, especially when the negative (background) class is large."'),
('"Hybrid Compressive Sampling via a New Total Variation TVL1"', '"ECCV 2010"', '["Gradient Magnitude", "Compressive Sampling", "Hybrid Sampling", "Diagonal Edge", "Partial Gradient', '"https://doi.org/10.1007/978-3-642-15567-3_29"', '"Compressive sampling (CS) is aimed at acquiring a signal or image from data which is deemed insufficient by Nyquist/Shannon sampling theorem. Its main idea is to recover a signal from limited measurements by exploring the prior knowledge that the signal is sparse or compressible in some domain. In this paper, we propose a CS approach using a new total-variation measure TVL1, or equivalently TV\\\\(_{\\\\ell_1}\\\\), which enforces the sparsity and the directional continuity in the gradient domain. Our TV\\\\(_{\\\\ell_1}\\\\) based CS is characterized by the following attributes. First, by minimizing the \\u21131-norm of partial gradients, it can achieve greater accuracy than the widely-used TV\\\\(_{\\\\ell_1\\\\ell_2}\\\\) based CS. Second, it, named hybrid CS, combines low-resolution sampling (LRS) and random sampling (RS), which is motivated by our induction that these two sampling methods are complementary. Finally, our theoretical and experimental results demonstrate that our hybrid CS using TV\\\\(_{\\\\ell_1}\\\\) yields sharper and more accurate images."'),
('"Hybrid Consensus Learning for Legume Species and Cultivars Classification"', '"ECCV 2014"', '["Legume and variety classification", "Venation images", "Consensus learning"]', '"https://doi.org/10.1007/978-3-319-16220-1_15"', '"In this work we propose an automatic method aimed at classifying five legume species and varieties using leaf venation features. Firstly, we segment the leaf veins and measure several multiscale morphological features on the vein segments and the areoles. Next, we build a hybrid consensus of experts formed by five different automatic classifiers to perform the classification using the extracted features. We propose to use two strategies in order to assign the importance to the votes of the algorithms in the consensus. The first one is considering all the algorithms equally important. The second one is based on the accuracy of the standalone classifiers. The performance of both consensus classifiers show to outperform the standalone classification algorithms in the five class recognition task."'),
('"Hybrid Image Deblurring by Fusing Edge and Power Spectrum Information"', '"ECCV 2014"', '["Power Spectrum", "Input Image", "Latent Image", "Kernel Estimation", "Hybrid Image"]', '"https://doi.org/10.1007/978-3-319-10584-0_6"', '"Recent blind deconvolution methods rely on either salient edges or the power spectrum of the input image for estimating the blur kernel, but not both. In this work we show that the two methods are inherently complimentary to each other. Edge-based methods work well for images containing large salient structures, but fail on small-scale textures. Power-spectrum-based methods, on the contrary, are efficient on textural regions but not on structural edges. This observation inspires us to propose a hybrid approach that combines edge-based and power-spectrum-based priors for more robust deblurring. Given an input image, our method first derives a structure prediction that coincides with the edge-based priors, and then extracts dominant edges from it to eliminate the errors in computing the power-spectrum-based priors. These two priors are then integrated in a combined cost function for blur kernel estimation. Experimental results show that the proposed approach is more robust and achieves higher quality results than previous methods on both real world and synthetic examples."'),
('"Hybrid Pooling Fusion in the BoW Pipeline"', '"ECCV 2012"', '["Local Descriptor", "Sparse Code", "Sift Descriptor", "Early Fusion", "Codebook Size"]', '"https://doi.org/10.1007/978-3-642-33885-4_36"', '"In the context of object and scene recognition, state-of-the-art performances are obtained with Bag of Words (BoW) models of mid-level representations computed from dense sampled local descriptors (e.g. SIFT). Several methods to combine low-level features and to set mid-level parameters have been evaluated recently for image classification."'),
('"Hybrid Stochastic / Deterministic Optimization for Tracking Sports Players and Pedestrians"', '"ECCV 2014"', '["Markov Chain Monte Carlo", "Ground Plane", "Data Association", "Markov Chain Monte Carlo Sampler",', '"https://doi.org/10.1007/978-3-319-10605-2_20"', '"Although \\u2018tracking-by-detection\\u2019 is a popular approach when reliable object detectors are available, missed detections remain a difficult hurdle to overcome. We present a hybrid stochastic/deterministic optimization scheme that uses RJMCMC to perform stochastic search over the space of detection configurations, interleaved with deterministic computation of the optimal multi-frame data association for each proposed detection hypothesis. Since object trajectories do not need to be estimated directly by the sampler, our approach is more efficient than traditional MCMCDA techniques. Moreover, our holistic formulation is able to generate longer, more reliable trajectories than baseline tracking-by-detection approaches in challenging multi-target scenarios."'),
('"Hyperdynamics Importance Sampling"', '"ECCV 2002"', '["Hyperdynamics", "Markov-chain Monte Carlo", "importance sampling", "global optimization", "human t', '"https://doi.org/10.1007/3-540-47969-4_51"', '"Sequential random sampling (\\u2018Markov Chain Monte-Carlo\\u2019) is a popular strategy for many vision problems involving multimodal distributions over high-dimensional parameter spaces. It applies both to importance sampling (where one wants to sample points according to their \\u2018importance\\u2019 for some calculation, but otherwise fairly) and to global optimization (where one wants to find good minima, or at least good starting points for local minimization, regardless of fairness). Unfortunately, most sequential samplers are very prone to becoming \\u2018trapped\\u2019 for long periods in unrepresentative local minima, which leads to biased or highly variable estimates. We present a general strategy for reducing MCMC trapping that generalizes Voter\\u2019s \\u2018hyperdynamic sampling\\u2019 from computational chemistry. The local gradient and curvature of the input distribution are used to construct an adaptive importance sampler that focuses samples on low cost negative curvature regions likely to contain \\u2018transition states\\u2019 \\u2014 codimension-1 saddle points representing \\u2018mountain passes\\u2019 connecting adjacent cost basins. This substantially accelerates inter-basin transition rates while still preserving correct relative transition probabilities. Experimental tests on the difficult problem of 3D articulated human pose estimation from monocular images show significantly enhanced minimum exploration."'),
('"Hyperfeatures \\u2013 Multilevel Local Coding for Visual Recognition"', '"ECCV 2006"', '["Vector Quantization", "Latent Dirichlet Allocation", "Image Patch", "Convolutional Neural Network"', '"https://doi.org/10.1007/11744023_3"', '"Histograms of local appearance descriptors are a popular representation for visual recognition. They are highly discriminant and have good resistance to local occlusions and to geometric and photometric variations, but they are not able to exploit spatial co-occurrence statistics at scales larger than their local input patches. We present a new multilevel visual representation, \\u2018hyperfeatures\\u2019, that is designed to remedy this. The starting point is the familiar notion that to detect object parts, in practice it often suffices to detect co-occurrences of more local object fragments \\u2013 a process that can be formalized as comparison (e.g. vector quantization) of image patches against a codebook of known fragments, followed by local aggregation of the resulting codebook membership vectors to detect co-occurrences. This process converts local collections of image descriptor vectors into somewhat less local histogram vectors \\u2013 higher-level but spatially coarser descriptors. We observe that as the output is again a local descriptor vector, the process can be iterated, and that doing so captures and codes ever larger assemblies of object parts and increasingly abstract or \\u2018semantic\\u2019 image properties. We formulate the hyperfeatures model and study its performance under several different image coding methods including clustering based Vector Quantization, Gaussian Mixtures, and combinations of these with Latent Dirichlet Allocation. We find that the resulting high-level features provide improved performance in several object image and texture image classification tasks."'),
('"Identification of Highly Similar 3D Objects Using Model Saliency"', '"ECCV 2006"', '["Point Cloud", "Salient Region", "Range Sensor", "Model Saliency", "Iterate Close Point Algorithm"]', '"https://doi.org/10.1007/11744085_37"', '"We present a novel approach for identifying 3D objects from a database of models, highly similar in shape, using range data acquired in unconstrained settings from a limited number of viewing directions. We are addressing also the challenging case of identifying targets not present in the database. The method is based on learning offline saliency tests for each object in the database, by maximizing an objective measure of discriminability with respect to other similar models. Our notion of model saliency differs from traditionally used structural saliency that characterizes weakly the uniqueness of a region by the amount of 3D texture available, by directly linking discriminability with the Bhattacharyya distance between the distribution of errors between the target and its corresponding ground truth, respectively other similar models. Our approach was evaluated on thousands of queries obtained by different sensors and acquired in various operating conditions and using a database of hundreds of models. The results presented show a significant improvement in the recognition performance when using saliency compared to global point-to-point mismatch errors, traditionally used in matching and verification algorithms."'),
('"Identification of Illustrators"', '"ECCV 2012"', '["Color Histogram", "Sift Feature", "Computer Vision Technique", "Artistic Style", "Dense Sift"]', '"https://doi.org/10.1007/978-3-642-33863-2_61"', '"This paper is motivated by a book in which artists and illustrators from all over the world offer their personal interpretations of the declaration of human rights in pictures [1]. It was enthusiastic for a young reader to see an illustration of an artist that he already knows from his books . The characters were different, the topic was irrelevant, but still it was easy to identify the illustrators based on the style of the illustration. Inspired by the human\\u2019s ability to identify illustrators, in this study we propose a method that can automatically learn to distinguish illustrations of different illustrators using computer vision techniques."'),
('"Identity Inference: Generalizing Person Re-identification Scenarios"', '"ECCV 2012"', '["Test Image", "Spatial Pyramid", "Label Problem", "Gallery Image", "Unlabeled Image"]', '"https://doi.org/10.1007/978-3-642-33863-2_44"', '"In this article we introduce the problem of identity inference as a generalization of the re-identification problem. Identity inference is applicable in situations where a large number of unknown persons must be identified without knowing a priori that groups of test images represent the same individual. Standard single- and multi-shot person re-identification are special cases of our formulation. We present an approach to solving identity inference problems using a Conditional Random Field (CRF) to model identity inference as a labeling problem in the CRF. The CRF model ensures that the final labeling gives similar labels to detections that are similar in feature space, and is flexible enough to incorporate constraints in the temporal and spatial domains. Experimental results are given on the ETHZ dataset. Our approach yields state-of-the-art performance for the multi-shot re-identification task and promising results for more general identity inference problems."'),
('"Illuminant Estimation from Projections on the Planckian Locus"', '"ECCV 2012"', '["Color Constancy", "Vote Procedure", "Correlate Color Temperature", "Chromaticity Diagram", "Grey P', '"https://doi.org/10.1007/978-3-642-33868-7_37"', '"This paper deals with the automatic evaluation of the illuminant from a color photography. While many methods have been developed over the last years, this problem is still open since no method builds on hypotheses that are universal enough to deal with all possible situations. The proposed approach relies on a physical assumption about the possible set of illuminants and on the selection of grey pixels. Namely, a subset of pixels is automatically selected, which is then projected on the Planckian locus. Then, a simple voting procedure yields a robust estimation of the illuminant. As shown by experiments on two classical databases, the method offers state of the art performances among learning-free methods, at a reasonable computational cost."'),
('"Illumination and Person-Insensitive Head Pose Estimation Using Distance Metric Learning"', '"ECCV 2008"', '["Face Image", "Near Neighbor", "Illumination Change", "Generalize Regression Regression Neural Netw', '"https://doi.org/10.1007/978-3-540-88688-4_46"', '"Head pose estimation is an important task for many face analysis applications, such as face recognition systems and human computer interactions. In this paper we aim to address the pose estimation problem under some challenging conditions, e.g., from a single image, large pose variation, and un-even illumination conditions. The approach we developed combines non-linear dimension reduction techniques with a learned distance metric transformation. The learned distance metric provides better intra-class clustering, therefore preserving a smooth low-dimensional manifold in the presence of large variation in the input images due to illumination changes. Experiments show that our method improves the performance, achieving accuracy within 2-3 degrees for face images with varying poses and within 3-4 degrees error for face images with varying pose and illumination changes."'),
('"Illumination Normalization Using Self-lighting Ratios for 3D2D Face Recognition"', '"ECCV 2012"', '["Lighting ratio", "illumination suppression", "3D2D face recognition"]', '"https://doi.org/10.1007/978-3-642-33868-7_22"', '"3D2D face recognition is beginning to gain attention from the research community. It takes advantage of 3D facial geometry to normalize the head pose and registers it into a canonical 2D space. In this paper, we present a novel illumination normalization approach for 3D2D face recognition which does not require any training or prior knowledge on the type, number, and direction of the lighting sources. Estimated using an image-specific filtering technique in the frequency domain, a self-lighting ratio is employed to suppress illumination differences. Experimental results on the UHDB11 and FRGC databases indicate that the proposed approach improves the performance significantly for face images with large illumination variations."'),
('"Image and Video Segmentation by Anisotropic Kernel Mean Shift"', '"ECCV 2004"', '["Image Segmentation", "Video Data", "Shift Point", "Video Segmentation", "Color Domain"]', '"https://doi.org/10.1007/978-3-540-24671-8_19"', '"Mean shift is a nonparametric estimator of density which has been applied to image and video segmentation. Traditional mean shift based segmentation uses a radially symmetric kernel to estimate local density, which is not optimal in view of the often structured nature of image and more particularly video data. In this paper we present an anisotropic kernel mean shift in which the shape, scale, and orientation of the kernels adapt to the local structure of the image or video. We decompose the anisotropic kernel to provide handles for modifying the segmentation based on simple heuristics. Experimental results show that the anisotropic kernel mean shift outperforms the original mean shift on image and video segmentation in the following aspects: 1) it gets better results on general images and video in a smoothness sense; 2) the segmented results are more consistent with human visual saliency; 3) the algorithm is robust to initial parameters."'),
('"Image Anisotropic Diffusion Based on Gradient Vector Flow Fields"', '"ECCV 2004"', '["Curvature Flow", "Anisotropic Diffusion", "Gradient Vector Flow", "Fairing Process", "Error Diagra', '"https://doi.org/10.1007/978-3-540-24672-5_23"', '"In this paper, the gradient vector flow fields are introduced in the image anisotropic diffusion, and the shock filter, mean curvature flow and Perona-Malik equation are reformulated respectively in the context of this flow fields. Many advantages over the original models can be obtained, such as numerical stability, a large capture range, and computational simplification etc. In addition, the fairing process is introduced in the anisotropic diffusion, which contains the fourth order derivative and is reformulated as the intrinsic Laplacian of curvature under the level set framework. By this fairing process, the boundaries of shape will become more outstanding. In order to overcome numerical errors, the intrinsic Laplacian of curvature is computed from the gradient vector flow fields, but not directly from the observed images."'),
('"Image Annotation Using Metric Learning in Semantic Neighbourhoods"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33712-3_60"', '"Automatic image annotation aims at predicting a set of textual labels for an image that describe its semantics. These are usually taken from an annotation vocabulary of few hundred labels. Because of the large vocabulary, there is a high variance in the number of images corresponding to different labels (\\u201cclass-imbalance\\u201d). Additionally, due to the limitations of manual annotation, a significant number of available images are not annotated with all the relevant labels (\\u201cweak-labelling\\u201d). These two issues badly affect the performance of most of the existing image annotation models. In this work, we propose 2PKNN, a two-step variant of the classical K-nearest neighbour algorithm, that addresses these two issues in the image annotation task. The first step of 2PKNN uses \\u201cimage-to-label\\u201d similarities, while the second step uses \\u201cimage-to-image\\u201d similarities; thus combining the benefits of both. Since the performance of nearest-neighbour based methods greatly depends on how features are compared, we also propose a metric learning framework over 2PKNN that learns weights for multiple features as well as distances together. This is done in a large margin set-up by generalizing a well-known (single-label) classification metric learning algorithm for multi-label prediction. For scalability, we implement it by alternating between stochastic sub-gradient descent and projection steps."'),
('"Image Categorization Using Directed Graphs"', '"ECCV 2010"', '["Undirected Graph", "Recognition Accuracy", "Sparse Representation", "Image Categorization", "Image', '"https://doi.org/10.1007/978-3-642-15558-1_55"', '"Most existing graph-based semi-supervised classification methods use pairwise similarities as edge weights of an undirected graph with images as the nodes of the graph. Recently several new graph construction methods produce, however, directed graph (asymmetric similarity between nodes). A simple symmetrization is often used to convert a directed graph to an undirected one. This, however, loses important structural information conveyed by asymmetric similarities. In this paper, we propose a novel symmetric co-linkage similarity which captures the essential relationship among the nodes in the directed graph. We apply this new co-linkage similarity in two important computer vision tasks for image categorization: object recognition and image annotation. Extensive empirical studies demonstrate the effectiveness of our method."'),
('"Image Classification Using Super-Vector Coding of Local Image Descriptors"', '"ECCV 2010"', '["Vector Quantization", "Local Descriptor", "Kullback Leibler", "Codebook Size", "Spatial Pyramid Ma', '"https://doi.org/10.1007/978-3-642-15555-0_11"', '"This paper introduces a new framework for image classification using local visual descriptors. The pipeline first performs a nonlinear feature transformation on descriptors, then aggregates the results together to form image-level representations, and finally applies a classification model. For all the three steps we suggest novel solutions which make our approach appealing in theory, more scalable in computation, and transparent in classification. Our experiments demonstrate that the proposed classification method achieves state-of-the-art accuracy on the well-known PASCAL benchmarks."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Image Clustering with Metric, Local Linear Structure, and Affine Symmetry"', '"ECCV 2004"', '["Cluster Algorithm", "Cluster Result", "Image Space", "Quotient Space", "Spectral Cluster"]', '"https://doi.org/10.1007/978-3-540-24670-1_35"', '"This paper addresses the problem of clustering images of objects seen from different viewpoints. That is, given an unlabelled set of images of n objects, we seek an unsupervised algorithm that can group the images into n disjoint subsets such that each subset only contains images of a single object. We formulate this clustering problem under a very broad geometric framework. The theme is the interplay between the geometry of appearance manifolds and the symmetry of the 2D affine group. Specifically, we identify three important notions for image clustering: the L 2 distance metric of the image space, the local linear structure of the appearance manifolds, and the action of the 2D affine group in the image space. Based on these notions, we propose a new image clustering algorithm. In a broad outline, the algorithm uses the metric to determine a neighborhood structure in the image space for each input image. Using local linear structure, comparisons (affinities) between images are computed only among the neighbors. These local comparisons are agglomerated into an affinity matrix, and a spectral clustering algorithm is used to yield the final clustering result. The technical part of the algorithm is to make all of these compatible with the action of the 2D affine group. Using human face images and images from the COIL database, we demonstrate experimentally that our algorithm is effective in clustering images (according to ojbect identity) where there is a large range of pose variation."'),
('"Image Deconvolution Ringing Artifact Detection and Removal via PSF Frequency Analysis"', '"ECCV 2014"', '["deconvolution", "image deblurring", "point spread function", "ringing artifacts", "zero-magnitude ', '"https://doi.org/10.1007/978-3-319-10593-2_17"', '"We present a new method to detect and remove ringing artifacts produced by the deconvolution process in image deblurring techniques. The method takes into account non-invertible frequency components of the blur kernel used in the deconvolution. Efficient Gabor wavelets are produced for each non-invertible frequency and applied on the deblurred image to generate a set of filter responses that reveal existing ringing artifacts. The set of Gabor filters is then employed in a regularization scheme to remove the corresponding artifacts from the deblurred image. The regularization scheme minimizes the responses of the reconstructed image to these Gabor filters through an alternating algorithm in order to suppress the artifacts. As a result of these steps we are able to significantly enhance the quality of the deblurred images produced by deconvolution algorithms. Our numerical evaluations using a ringing artifact metric indicate the effectiveness of the proposed deringing method."'),
('"Image Enhancement Using Calibrated Lens Simulations"', '"ECCV 2012"', '["Point Spread Function", "Image Enhancement", "Chromatic Aberration", "Dispersion Function", "Lens ', '"https://doi.org/10.1007/978-3-642-33765-9_4"', '"All lenses have optical aberrations which reduce image sharpness. These aberrations can be reduced by deconvolving an image using the lens point spread function (PSF). However, fully measuring a PSF is laborious and prohibitive. Alternatively, one can simulate the PSF if the lens model is known. However, due to manufacturing tolerances lenses differ subtly from their models, so often a simulated PSF is a poor match to measured data. We present an algorithm that uses a PSF measurement at a single depth to calibrate the nominal lens model to the measured PSF. The calibrated model can then be used to compute the PSF for any desired setting of lens parameters for any scene depth, without additional measurements or calibration. The calibrated model gives deconvolution results comparable to measurement but is much more compact and require hundreds of times fewer calibration images."'),
('"Image Feature Extraction Using Gradient Local Auto-Correlations"', '"ECCV 2008"', '["Image Patch", "Image Gradient", "Mask Pattern", "Human Detection", "Torus Manifold"]', '"https://doi.org/10.1007/978-3-540-88682-2_27"', '"In this paper, we propose a method for extracting image features which utilizes 2nd order statistics, i.e., spatial and orientational auto-correlations of local gradients. It enables us to extract richer information from images and to obtain more discriminative power than standard histogram based methods. The image gradients are sparsely described in terms of magnitude and orientation. In addition, normal vectors on the image surface are derived from the gradients and these could also be utilized instead of the gradients. From a geometrical viewpoint, the method extracts information about not only the gradients but also the curvatures of the image surface. Experimental results for pedestrian detection and image patch matching demonstrate the effectiveness of the proposed method compared with other methods, such as HOG and SIFT."'),
('"Image Features Based on a New Approach to 2D Rotation Invariant Quadrature Filters"', '"ECCV 2002"', '["image features", "quadrature filters", "analytic signal", "structure tensor", "point-of-interest o', '"https://doi.org/10.1007/3-540-47969-4_25"', '"Quadrature filters are a well known method of low-level computer vision for estimating certain properties of the signal, as there are local amplitude and local phase. However, 2D quadrature filters suffer from being not rotation invariant. Furthermore, they do not allow to detect truly 2D features as corners and junctions unless they are combined to form the structure tensor. The present paper deals with a new 2D generalization of quadrature filters which is rotation invariant and allows to analyze intrinsically 2D signals. Hence, the new approach can be considered as the union of properties of quadrature filters and of the structure tensor. The proposed method first estimates the local orientation of the signal which is then used for steering some basis filter responses. Certain linear combination of these filter responses are derived which allow to estimate the local isotropy and two perpendicular phases of the signal. The phase model is based on the assumption of an angular band-limitation in the signal. As an application, a simple and efficient point-of-interest operator is presented and it is compared to the Plessey detector."'),
('"Image Guided Tone Mapping with Locally Nonlinear Model"', '"ECCV 2012"', '["high dynamic range", "tone mapping", "locally nonlinear model", "guided image"]', '"https://doi.org/10.1007/978-3-642-33765-9_56"', '"In this paper, we propose an effective locally nonlinear tone mapping algorithm for compressing the High Dynamic Range (HDR) images. Instead of linearly scaling the luminance of pixels, our core idea is to introduce local gamma correction with adaptive parameters on small overlapping patches over the entire input image. A framework for HDR image compression is then introduced, in which the global optimization problem is deduced and two guided images are adopted to induct the optimum solution. The optimal compression can finally be achieved by solving the optimization problem which can be transformed to a sparse linear equation. Extensive experimental results on a variety of HDR images and a carefully designed perceptually evaluation have demonstrated that our approach can achieve better performances than the state-of-the-art approaches."'),
('"Image Invariants for Smooth Reflective Surfaces"', '"ECCV 2010"', '["Iterative Close Point", "Image Gradient", "Parabolic Curvature", "Perspective Camera", "Parabolic ', '"https://doi.org/10.1007/978-3-642-15552-9_18"', '"Image invariants are those properties of the images of an object that remain unchanged with changes in camera parameters, illumination etc. In this paper, we derive an image invariant for smooth surfaces with mirror-like reflectance. Since, such surfaces do not have an appearance of their own but rather distort the appearance of the surrounding environment, the applicability of geometric invariants is limited. We show that for such smooth mirror-like surfaces, the image gradients exhibit degeneracy at the surface points that are parabolic. We leverage this result in order to derive a photometric invariant that is associated with parabolic curvature points. Further, we show that these invariant curves can be effectively extracted from just a few images of the object in uncontrolled, uncalibrated environments without the need for any a priori information about the surface shape. Since these parabolic curves are a geometric property of the surface, they can then be used as features for a variety of machine vision tasks. This is especially powerful, since there are very few vision algorithms that can handle such mirror-like surfaces. We show the potential of the proposed invariant using experiments on two related applications - object recognition and pose estimation for smooth mirror surfaces."'),
('"Image Labeling on a Network: Using Social-Network Metadata for Image Classification"', '"ECCV 2012"', '["Image Classification", "Social Networks", "Structured Learning"]', '"https://doi.org/10.1007/978-3-642-33765-9_59"', '"Large-scale image retrieval benchmarks invariably consist of images from the Web. Many of these benchmarks are derived from online photo sharing networks, like Flickr, which in addition to hosting images also provide a highly interactive social community. Such communities generate rich metadata that can naturally be harnessed for image classification and retrieval. Here we study four popular benchmark datasets, extending them with social-network metadata, such as the groups to which each image belongs, the comment thread associated with the image, who uploaded it, their location, and their network of friends. Since these types of data are inherently relational, we propose a model that explicitly accounts for the interdependencies between images sharing common properties. We model the task as a binary labeling problem on a network, and use structured learning techniques to learn model parameters. We find that social-network metadata are useful in a variety of classification tasks, in many cases outperforming methods based on image content."'),
('"Image Processing Done Right"', '"ECCV 2002"', '["Image features", "texture", "image indexing", "scale-space", "image transformations", "image space', '"https://doi.org/10.1007/3-540-47969-4_11"', '"A large part of \\u201cimage processing\\u201d involves the computation of significant points, curves and areas (\\u201cfeatures\\u201d). These can be defined as loci where absolute differential invariants of the image assume fiducial values, taking spatial scale and intensity (in a generic sense) scale into account. \\u201cDifferential invariance\\u201d implies a group of \\u201csimilarities\\u201d or \\u201ccongruences\\u201d. These \\u201cmotions\\u201d define the geometrical structure of image space. Classical Euclidian invariants don\\u2019t apply to images because image space is non-Euclidian. We analyze image structure from first principles and construct the fundamental group of image space motions. Image space is a Cayley-Klein geometry with one isotropic dimension. The analysis leads to a principled definition of \\u201cfeatures\\u201d and the operators that define them."'),
('"Image Registration Accuracy Estimation Without Ground Truth Using Bootstrap"', '"CVAMIA 2006"', '["Image Registration", "Geometrical Error", "Registration Algorithm", "Ground Truth Data", "Block Ma', '"https://doi.org/10.1007/11889762_6"', '"We consider the problem of estimating the local accuracy of image registration when no ground truth data is available. The technique is based on a statistical resampling technique called bootstrap. Only the two input images are used, no other data are needed. The general bootstrap uncertainty estimation framework described here is in principle applicable to most of the existing pixel based registration techniques. In practice, a large computing power is required. We present experimental results for a block matching method on an ultrasound image sequence for elastography with both known and unknown deformation field."'),
('"Image Registration for Foveated Omnidirectional Sensing"', '"ECCV 2002"', '["Vision systems", "omnidirectional sensing", "foveated sensing", "attention", "parametric template ', '"https://doi.org/10.1007/3-540-47979-1_41"', '"This paper addresses the problem of registering high-resolution, small FOV images with low-resolution panoramic images provided by an omnidirectional catadioptric video sensor. Such systems may find application in surveillance and telepresence systems that require a large FOV and high resolution at selected locations. Although image registration has been studied in more conventional applications, the problem of registering omnidirectional and conventional video has not previously been addressed, and this problem presents unique challenges due to (i) the extreme differences in resolution between the sensors (more than a 16:1 linear resolution ratio in our application), and (ii) the resolution inhomogeneity of omnidirectional images. In this paper we show how a coarse registration can be computed from raw images using parametric template matching techniques. Further, we develop and evaluate robust feature-based and featureless methods for computing the full 2D projective transforms between the two images. We find that our novel featureless approach yields superior performance for this application."'),
('"Image Registration Neural System for the Analysis of Fundus Topology"', '"MMBIA 2004"', '["High Resolution Image", "Angular Aperture", "Angular Location", "Content Addressable Memory", "Hig', '"https://doi.org/10.1007/978-3-540-27816-0_36"', '"The developed system is a tool for high aperture imaging of the fundus. The obtained high resolution images preserve the topology of the blood vessels. The system is based on mosaicking a series of distinct low aperture fragments in order to obtain a high aperture image. Mosaicking is implemented by a neural network with stubborn learning taking into account the importance of the information of particular features. In mosaicking, the aberrations of the third order are partly compensated."'),
('"Image Retrieval and Ranking via Consistently Reconstructing Multi-attribute Queries"', '"ECCV 2014"', '["Multi-Attribute Image", "Image Retrieval & Ranking", "Group Sparsity"]', '"https://doi.org/10.1007/978-3-319-10590-1_37"', '"Image retrieval and ranking based on the multi-attribute queries is beneficial to various real world applications. Traditional methods on this problem often utilize intermediate representations generated by attribute classifiers to describe the images, and then the images in the database are sorted according to their similarities to the query. However, such a scheme has two main challenges: 1) how to exploit the correlation between query attributes and non-query attributes, and 2) how to handle noisy representations since the pre-defined attribute classifiers are probably unreliable. To overcome these challenges, we discover the correlation among attributes via expanding the query representation, and imposing the group sparsity on representations to reduce the disturbance of noisy data. Specifically, given a multi-attribute query matrix with each row corresponding to a query attribute and each column the pre-defined attribute, we firstly expand the query based on the correlation of the attributes learned from the training data. Then, the expanded query matrix is reconstructed by the images in the dataset with the \\u21132,1 regularization. Furthermore, we introduce the ranking SVM into the objective function to guarantee the ranking consistency. Finally, we adopt a graph regularization to preserve the local visual similarity among images. Extensive experiments on LFW, CUB-200-2011, and Shoes datasets are conducted to demonstrate the effectiveness of our proposed method."'),
('"Image Retrieval with Structured Object Queries Using Latent Ranking SVM"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_10"', '"We consider image retrieval with structured object queries \\u2013 queries that specify the objects that should be present in the scene, and their spatial relations. An example of such queries is \\u201ccar on the road\\u201d. Existing image retrieval systems typically consider queries consisting of object classes (i.e. keywords). They train a separate classifier for each object class and combine the output heuristically. In contrast, we develop a learning framework to jointly consider object classes and their relations. Our method considers not only the objects in the query (\\u201ccar\\u201d and \\u201croad\\u201d in the above example), but also related object categories can be useful for retrieval. Since we do not have ground-truth labeling of object bounding boxes on the test image, we represent them as latent variables in our model. Our learning method is an extension of the ranking SVM with latent variables, which we call latent ranking SVM. We demonstrate image retrieval and ranking results on a dataset with more than a hundred of object classes."'),
('"Image Segmentation by Branch-and-Mincut"', '"ECCV 2008"', '["Image Segmentation", "Active Front", "Optimal Segmentation", "Shape Prior", "Image Segmentation Pr', '"https://doi.org/10.1007/978-3-540-88693-8_2"', '"Efficient global optimization techniques such as graph cut exist for energies corresponding to binary image segmentation from low-level cues. However, introducing a high-level prior such as a shape prior or a color-distribution prior into the segmentation process typically results in an energy that is much harder to optimize. The main contribution of the paper is a new global optimization framework for a wide class of such energies. The framework is built upon two powerful techniques: graph cut and branch-and-bound. These techniques are unified through the derivation of lower bounds on the energies. Being computable via graph cut, these bounds are used to prune branches within a branch-and-bound search."'),
('"Image Segmentation by Flexible Models Based on Robust Regularized Networks"', '"ECCV 2002"', '["Segmentation", "Restoration", "Edge-preserving Regularization"]', '"https://doi.org/10.1007/3-540-47977-5_41"', '"The object of this paper is to present a formulation for the segmentation and restoration problem using flexible models with a robust regularized network (RRN). A two-steps iterative algorithm is presented. In the first step an approximation of the classification is computed by using a local minimization algorithm, and in the second step the parameters of the RRN are updated. The use of robust potentials is motivated by (a) classification errors that can result from the use of local minimizer algorithms in the implementation, and (b) the need to adapt the RN using local image gradient information to improve fidelity of the model to the data."'),
('"Image Segmentation by Nonparametric Clustering Based on the Kolmogorov-Smirnov Distance"', '"ECCV 2000"', '["Image Segmentation", "Gaussian Mixture Model", "Independent Component Analysis", "Salient Region",', '"https://doi.org/10.1007/3-540-45053-X_6"', '"In this paper we introduce a non-parametric clustering algorithm for 1-dimensional data. The procedure looks for the simplest (i.e. smoothest) density that is still compatible with the data. Compatibility is given a precise meaning in terms of the Kolmogorov-Smirnov statistic. After discussing experimental results for colour segmentation, we outline how this proposed algorithm can be extended to higher dimensions."'),
('"Image Segmentation in the Presence of Shadows and Highlights"', '"ECCV 2008"', '["Image Segmentation", "Segmentation Method", "Dominant Colour", "Dominant Structure", "Mean Shift"]', '"https://doi.org/10.1007/978-3-540-88693-8_1"', '"The segmentation method proposed in this paper is based on the observation that a single physical reflectance can have many different image values. We call the set of all these values a dominant colour. These variations are caused by shadows, shading and highlights and due to varying object geometry. The main idea is that dominant colours trace connected ridges in the chromatic histogram. To capture them, we propose a new Ridge based Distribution Analysis (RAD) to find the set of ridges representative of the dominant colour. First, a multilocal creaseness technique followed by a ridge extraction algorithm is proposed. Afterwards, a flooding procedure is performed to find the dominant colours in the histogram. Qualitative results illustrate the ability of our method to obtain excellent results in the presence of shadow and highlight edges. Quantitative results obtained on the Berkeley data set show that our method outperforms state-of-the-art segmentation methods at low computational cost."'),
('"Image Segmentation with Topic Random Field"', '"ECCV 2010"', '["Image Segmentation", "Visual Word", "Segmentation Result", "Markov Random Field", "Latent Dirichle', '"https://doi.org/10.1007/978-3-642-15555-0_57"', '"Recently, there has been increasing interests in applying aspect models (e.g., PLSA and LDA) in image segmentation. However, these models ignore spatial relationships among local topic labels in an image and suffers from information loss by representing image feature using the index of its closest match in the codebook. In this paper, we propose Topic Random Field (TRF) to tackle these two problems. Specifically, TRF defines a Markov Random Field over hidden labels of an image, to enforce the spatial coherence between topic labels for neighboring regions. Moreover, TRF utilizes a noise channel to model the generation of local image features, and avoids the off-line process of building visual codebook. We provide details of variational inference and parameter learning for TRF. Experimental evaluations on three image data sets show that TRF achieves better segmentation performance."'),
('"Image Similarity Using Mutual Information of Regions"', '"ECCV 2004"', '["Mutual Information", "Independent Component Analysis", "Central Limit Theorem", "Image Registratio', '"https://doi.org/10.1007/978-3-540-24672-5_47"', '"Mutual information (MI) has emerged in recent years as an effective similarity measure for comparing images. One drawback of MI, however, is that it is calculated on a pixel by pixel basis, meaning that it takes into account only the relationships between corresponding individual pixels and not those of each pixel\\u2019s respective neighborhood. As a result, much of the spatial information inherent in images is not utilized. In this paper, we propose a novel extension to MI called regional mutual information (RMI). This extension efficiently takes neighborhood regions of corresponding pixels into account. We demonstrate the usefulness of RMI by applying it to a real-world problem in the medical domain\\u2014intensity-based 2D-3D registration of X-ray projection images (2D) to a CT image (3D). Using a gold-standard spine image data set, we show that RMI is a more robust similarity meaure for image registration than MI."'),
('"Image Specific Feature Similarities"', '"ECCV 2006"', '["Feature Space", "Feature Point", "Image Segmentation", "Texture Feature", "Segmentation Result"]', '"https://doi.org/10.1007/11744047_25"', '"Calculating a reliable similarity measure between pixel features is essential for many computer vision and image processing applications. We propose a similarity measure (affinity) between pixel features, which depends on the feature space histogram of the image. We use the observation that clusters in the feature space histogram are typically smooth and roughly convex. Given two feature points we adjust their similarity according to the bottleneck in the histogram values on the straight line between them. We call our new similarities Bottleneck Affinities. These measures are computed efficiently, we demonstrate superior segmentation results compared to the use of the Euclidean metric."'),
('"Image Tag Completion by Noisy Matrix Recovery"', '"ECCV 2014"', '["Tag completion", "noisy tag matrix recovery", "matrix completion", "missing/noisy tags", "image ta', '"https://doi.org/10.1007/978-3-319-10584-0_28"', '"It is now generally recognized that user-provided image tags are incomplete and noisy. In this study, we focus on the problem of tag completion that aims to simultaneously enrich the missing tags and remove noisy tags. The novel component of the proposed framework is a noisy matrix recovery algorithm. It assumes that the observed tags are independently sampled from an unknown tag matrix and our goal is to recover the tag matrix based on the sampled tags. We show theoretically that the proposed noisy tag matrix recovery algorithm is able to simultaneously recover the missing tags and de-emphasize the noisy tags even with a limited number of observations. In addition, a graph Laplacian based component is introduced to combine the noisy matrix recovery component with visual features. Our empirical study with multiple benchmark datasets for image tagging shows that the proposed algorithm outperforms state-of-the-art approaches in terms of both effectiveness and efficiency when handling missing and noisy tags."'),
('"Image-Based 4-d Reconstruction Using 3-d Change Detection"', '"ECCV 2014"', '["Change Detection", "Illumination Condition", "Entire Scene", "Urban Scene", "Initial Time Step"]', '"https://doi.org/10.1007/978-3-319-10578-9_3"', '"This paper describes an approach to reconstruct the complete history of a 3-d scene over time from imagery. The proposed approach avoids rebuilding 3-d models of the scene at each time instant. Instead, the approach employs an initial 3-d model which is continuously updated with changes in the environment to form a full 4-d representation. This updating scheme is enabled by a novel algorithm that infers 3-d changes with respect to the model at one time step from images taken at a subsequent time step. This algorithm can effectively detect changes even when the illumination conditions between image collections are significantly different. The performance of the proposed framework is demonstrated on four challenging datasets in terms of 4-d modeling accuracy as well as quantitative evaluation of 3-d change detection."'),
('"Image-Based Phenotyping of the Mature Arabidopsis Shoot System"', '"ECCV 2014"', '["Image-based phenotyping", "Geometrical/topological traits", "Tracing", "Hierarchical reconstructio', '"https://doi.org/10.1007/978-3-319-16220-1_17"', '"The image-based phenotyping of mature plants faces several challenges from the image acquisition to the determination of quantitative characteristics describing their appearance. In this work a framework to extract geometrical and topological traits of 2D images of mature Arabidopsis thaliana is proposed. The phenotyping pipeline recovers the realistic branching architecture of dried and flattened plants in two steps. In the first step, a tracing approach is used for the extraction of centerline segments of the plant. In the second step, a hierarchical reconstruction is done to group the segments according to continuity principles. This paper covers an overview of the relevant processing steps along the proposed pipeline and provides an insight into the image acquisition as well as into the most relevant results from the evaluation process."'),
('"Image-to-Class Distance Metric Learning for Image Classification"', '"ECCV 2010"', '["Recognition Accuracy", "Spatial Restriction", "Spatial Pyramid", "Candidate Class", "Spatial Divis', '"https://doi.org/10.1007/978-3-642-15549-9_51"', '"Image-To-Class (I2C) distance is first used in Naive-Bayes Nearest-Neighbor (NBNN) classifier for image classification and has successfully handled datasets with large intra-class variances. However, the performance of this distance relies heavily on the large number of local features in the training set and test image, which need heavy computation cost for nearest-neighbor (NN) search in the testing phase. If using small number of local features for accelerating the NN search, the performance will be poor."'),
('"iModel: Interactive Co-segmentation for Object of Interest 3D Modeling"', '"ECCV 2010"', '["Multiple View", "Camera Parameter", "Control Setup", "Photo Collection", "Interactive Algorithm"]', '"https://doi.org/10.1007/978-3-642-35740-4_17"', '"We present an interactive system to create 3D models of objects of interest in their natural cluttered environments."'),
('"Impact of Topology-Related Attributes from Local Binary Patterns on Texture Classification"', '"ECCV 2014"', '["Local binary pattern", "Local descriptor", "Texture classification"]', '"https://doi.org/10.1007/978-3-319-16181-5_6"', '"A general texture description model is proposed, using topology related attributes calculated from Local Binary Patterns (LBP). The proposed framework extends and generalises existing LBP-based descriptors like LBP-rotation invariant uniform patterns (\\\\(\\\\mathrm {LBP}^{riu2}\\\\)), and Local Binary Count (LBC). Like them, it allows contrast and rotation invariant image description using more compact descriptors than classic LBP. However, its expressiveness, and then its discrimination capability, is higher, since it includes additional information, including the number of connected components. The impact of the different attributes on texture classification performance is assessed through a systematic comparative evaluation, performed on three texture datasets. The results validate the interest of the proposed approach, by showing that some combinations of attributes outperform state-of-the-art LBP-based texture descriptors."'),
('"Implementing Decision Trees and Forests on a GPU"', '"ECCV 2008"', '["Random Forest", "Leaf Node", "Integral Image", "Candidate Feature", "Leaf Image"]', '"https://doi.org/10.1007/978-3-540-88693-8_44"', '"We describe a method for implementing the evaluation and training of decision trees and forests entirely on a GPU, and show how this method can be used in the context of object recognition."'),
('"Implicit Probabilistic Models of Human Motion for Synthesis and Tracking"', '"ECCV 2002"', '["Joint Angle", "Leaf Node", "Motion Model", "Human Motion", "Texture Synthesis"]', '"https://doi.org/10.1007/3-540-47969-4_52"', '"This paper addresses the problem of probabilistically modeling 3D human motion for synthesis and tracking. Given the high dimensional nature of human motion, learning an explicit probabilistic model from available training data is currently impractical. Instead we exploit methods from texture synthesis that treat images as representing an implicit empirical distribution. These methods replace the problem of representing the probability of a texture pattern with that of searching the training data for similar instances of that pattern. We extend this idea to temporal data representing 3D human motion with a large database of example motions. To make the method useful in practice, we must address the problem of efficient search in a large training set; efficiency is particularly important for tracking. Towards that end, we learn a low dimensional linear model of human motion that is used to structure the example motion database into a binary tree. An approximate probabilistic tree search method exploits the coefficients of this low-dimensional representation and runs in sub-linear time. This probabilistic tree search returns a particular sample human motion with probability approximating the true distribution of human motions in the database. This sampling method is suitable for use with particle filtering techniques and is applied to articulated 3D tracking of humans within a Bayesian framework. Successful tracking results are presented, along with examples of synthesizing human motion using the model."'),
('"Improved Human Parsing with a Full Relational Model"', '"ECCV 2010"', '["Tree Model", "Appearance Model", "Equal Error Rate", "Part Detector", "Approximate Inference"]', '"https://doi.org/10.1007/978-3-642-15561-1_17"', '"We show quantitative evidence that a full relational model of the body performs better at upper body parsing than the standard tree model, despite the need to adopt approximate inference and learning procedures. Our method uses an approximate search for inference, and an approximate structure learning method to learn. We compare our method to state of the art methods on our dataset (which depicts a wide range of poses), on the standard Buffy dataset, and on the reduced PASCAL dataset published recently. Our results suggest that the Buffy dataset over emphasizes poses where the arms hang down, and that leads to generalization problems."'),
('"Improved Motion Invariant Deblurring through Motion Estimation"', '"ECCV 2014"', '["Root Mean Square Error", "Motion Estimation", "Latent Image", "Motion Blur", "Blind Deconvolution"', '"https://doi.org/10.1007/978-3-319-10593-2_6"', '"We address the capture of sharp images of fast-moving objects, and build on the Motion Invariant photographic technique. The key advantage of motion invariance is that, unlike other computational photographic techniques, it does not require pre-exposure velocity estimation in order to ensure numerically stable deblurring. Its disadvantage is that the invariance is only approximate - objects moving with non-zero velocity will exhibit artifacts in the deblurred image related to tail clipping in the motion Point Spread Function (PSF). We model these artifacts as a convolution of the desired latent image with an error PSF, and demonstrate that the spatial scale of these artifacts corresponds to the object velocity. Surprisingly, despite the use of parabolic motion to capture an image in which blur is invariant to motion, we demonstrate that the motion invariant image can be used to estimate object motion post-capture. With real camera images, we demonstrate significant reductions in the artifacts by using the estimated motion for deblurring. We also quantify a 96% reduction in reconstruction error, relative to a floor established by exact PSF deconvolution, via simulation with a large test set of photographic images."'),
('"Improved Reconstruction of Deforming Surfaces by Cancelling Ambient Occlusion"', '"ECCV 2012"', '["Optical Flow", "Input Shape", "Uniform Illumination", "Photometric Stereo", "Brightness Change"]', '"https://doi.org/10.1007/978-3-642-33718-5_3"', '"We present a general technique for improving space-time reconstructions of deforming surfaces, which are captured in an video-based reconstruction scenario under uniform illumination. Our approach simultaneously improves both the acquired shape as well as the tracked motion of the deforming surface. The method is based on factoring out surface shading, computed by a fast approximation to global illumination called ambient occlusion. This allows us to improve the performance of optical flow tracking that mainly relies on constancy of image features, such as intensity. While cancelling the local shading, we also optimize the surface shape to minimize the residual between the ambient occlusion of the 3D geometry and that of the image, yielding more accurate surface details in the reconstruction. Our enhancement is independent of the actual space-time reconstruction algorithm. We experimentally measure the quantitative improvements produced by our algorithm using a synthetic example of deforming skin, where ground truth shape and motion is available. We further demonstrate our enhancement on a real-world sequence of human face reconstruction."'),
('"Improvement of On-line Signature Verification System Robust to Intersession Variability"', '"BioAW 2002"', '["Reference Data", "Signature Shape", "Verification System", "Matching Error", "Signature Verificati', '"https://doi.org/10.1007/3-540-47917-1_17"', '"In signature verification using on-line hand written data, we have shown that the pen inclination data improves the verification rate[1]. For the data of 9 weeks, we applied some reference renewal method to reduce the influence of intersession variability and obtained some improvement[2]. In this paper, we propose a threshold adaptation method, in which a reference renewal using the matching error of last five authenticated data is done. And, we show that the proposed method is effective to maintain verification rate with 91.2% using genuine and forgery signatures over 9 weeks."'),
('"Improvements to Gamut Mapping Colour Constancy Algorithms"', '"ECCV 2000"', '["Illumination Change", "Colour Constancy", "Camera Sensor", "Solution Selection", "Diagonal Model"]', '"https://doi.org/10.1007/3-540-45054-8_26"', '"In his paper we introduce two improvements to the threedimensional gamut mapping approach to computational colour constancy. This approach consist of two separate parts. First the possible solutions are constrained. This part is dependent on the diagonal model of illumination change, which in turn, is a function of the camera sensors. In this work we propose a robust method for relaxing this reliance on the diagonal model. The second part of the gamut mapping paradigm is to choose a solution from the feasible set. Currently there are two general approaches for doing so. We propose a hybrid method which embodies the benefits of both, and generally performs better than either. We provide results using both generated data and a carefully calibrated set of 321 images. In the case of the modification for diagonal model failure, we provide synthetic results using two cameras with a distinctly different degree of support for the diagonal model. Here we verify that the new method does indeed reduce error due to the diagonal model. We also verify that the new method for choosing the solution offers significant improvement, both in the case of synthetic data and with real images."'),
('"Improving Ancient Roman Coin Recognition with Alignment and Spatial Encoding"', '"ECCV 2014"', '["Recognition", "Detection", "Coin recognition"]', '"https://doi.org/10.1007/978-3-319-16178-5_10"', '"Roman coins play an important role to understand the Roman empire because they convey rich information about key historical events of the time. Moreover, as large amounts of coins are daily traded over the Internet, it becomes necessary to develop automatic coin recognition systems to prevent illegal trades. In this paper, we describe a new large annotated database of over 2800 Roman coin images and propose an effective automated system for recognition of coins that leverages this new coin image set. As the use of succinct spatial-appearance relationships is critical for accurate coin recognition, we suggest two competing methods, adapted for the coin domain, to accomplish this task."'),
('"Improving Data Association by Joint Modeling of Pedestrian Trajectories and Groupings"', '"ECCV 2010"', '["Data Association", "Short Time Window", "Space Syntax", "Social Force Model", "Crowded Scene"]', '"https://doi.org/10.1007/978-3-642-15549-9_33"', '"We consider the problem of data association in a multi-person tracking context. In semi-crowded environments, people are still discernible as individually moving entities, that undergo many interactions with other people in their direct surrounding. Finding the correct association is therefore difficult, but higher-order social factors, such as group membership, are expected to ease the problem. However, estimating group membership is a chicken-and-egg problem: knowing pedestrian trajectories, it is rather easy to find out possible groupings in the data, but in crowded scenes, it is often difficult to estimate closely interacting trajectories without further knowledge about groups. To this end, we propose a third-order graphical model that is able to jointly estimate correct trajectories and group memberships over a short time window. A set of experiments on challenging data underline the importance of joint reasoning for data association in crowded scenarios."'),
('"Improving Global Multi-target Tracking with Local Updates"', '"ECCV 2014"', '["Multi-target tracking", "Data association"]', '"https://doi.org/10.1007/978-3-319-16199-0_13"', '"We propose a scheme to explicitly detect and resolve ambiguous situations in multiple target tracking. During periods of uncertainty, our method applies multiple local single target trackers to hypothesise short term tracks. These tracks are combined with the tracks obtained by a global multi-target tracker, if they result in a reduction in the global cost function. Since tracking failures typically arise when targets become occluded, we propose a local data association scheme to maintain the target identities in these situations. We demonstrate a reduction of up to \\\\(50\\\\,\\\\%\\\\) in the global cost function, which in turn leads to superior performance on several challenging benchmark sequences. Additionally, we show tracking results in sports videos where poor video quality and frequent and severe occlusions between multiple players pose difficulties for state-of-the-art trackers."'),
('"Improving Image-Based Localization by Active Correspondence Search"', '"ECCV 2012"', '["Visual Word", "Query Image", "Search Cost", "Active Search", "Registration Time"]', '"https://doi.org/10.1007/978-3-642-33718-5_54"', '"We propose a powerful pipeline for determining the pose of a query image relative to a point cloud reconstruction of a large scene consisting of more than one million 3D points. The key component of our approach is an efficient and effective search method to establish matches between image features and scene points needed for pose estimation. Our main contribution is a framework for actively searching for additional matches, based on both 2D-to-3D and 3D-to-2D search. A unified formulation of search in both directions allows us to exploit the distinct advantages of both strategies, while avoiding their weaknesses. Due to active search, the resulting pipeline is able to close the gap in registration performance observed between efficient search methods and approaches that are allowed to run for multiple seconds, without sacrificing run-time efficiency. Our method achieves the best registration performance published so far on three standard benchmark datasets, with run-times comparable or superior to the fastest state-of-the-art methods."'),
('"Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections"', '"ECCV 2014"', '["Canonical Correlation Analysis", "Query Image", "Ridge Regression", "Cosine Similarity", "Transfer', '"https://doi.org/10.1007/978-3-319-10593-2_35"', '"This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of images that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description."'),
('"Improving Local Descriptors by Embedding Global and Local Spatial Information"', '"ECCV 2010"', '["Spatial Information", "Region Feature", "Local Descriptor", "Multiple Kernel Learn", "Spatial Pyra', '"https://doi.org/10.1007/978-3-642-15561-1_53"', '"In this paper, we present a novel problem: \\u201cGiven local descriptors, how can we incorporate both local and global spatial information into the descriptors, and obtain compact and discriminative features?\\u201d To address this problem, we proposed a general framework to improve any local descriptors by embedding both local and global spatial information. In addition, we proposed a simple and powerful combination method for different types of features. We evaluated the proposed method for the most standard scene and object recognition dataset, and confirm the effectiveness of the proposed method from the viewpoint of speed and accuracy."'),
('"Improving NCC-Based Direct Visual Tracking"', '"ECCV 2012"', '["Mutual Information", "Augmented Reality", "Reference Image", "Visual Tracking", "Illumination Chan', '"https://doi.org/10.1007/978-3-642-33783-3_32"', '"Direct visual tracking can be impaired by changes in illumination if the right choice of similarity function and photometric model is not made. Tracking using the sum of squared differences, for instance, often needs to be coupled with a photometric model to mitigate illumination changes. More sophisticated similarities, e.g. mutual information and cross cumulative residual entropy, however, can cope with complex illumination variations at the cost of a reduction of the convergence radius, and an increase of the computational effort. In this context, the normalized cross correlation (NCC) represents an interesting alternative. The NCC is intrinsically invariant to affine illumination changes, and also presents low computational cost. This article proposes a new direct visual tracking method based on the NCC. Two techniques have been developed to improve the robustness to complex illumination variations and partial occlusions. These techniques are based on subregion clusterization, and weighting by a residue invariant to affine illumination changes. The last contribution is an efficient Newton-style optimization procedure that does not require the explicit computation of the Hessian. The proposed method is compared against the state of the art using a benchmark database with ground-truth, as well as real-world sequences."'),
('"Improving People Search Using Query Expansions"', '"ECCV 2008"', '["Gaussian Mixture Model", "Face Detector", "Query Expansion", "Discriminative Model", "Discriminati', '"https://doi.org/10.1007/978-3-540-88688-4_7"', '"In this paper we are interested in finding images of people on the web, and more specifically within large databases of captioned news images. It has recently been shown that visual analysis of the faces in images returned on a text-based query over captions can significantly improve search results. The underlying idea to improve the text-based results is that although this initial result is imperfect, it will render the queried person to be relatively frequent as compared to other people, so we can search for a large group of highly similar faces. The performance of such methods depends strongly on this assumption: for people whose face appears in less than about 40% of the initial text-based result, the performance may be very poor. The contribution of this paper is to improve search results by exploiting faces of other people that co-occur frequently with the queried person. We refer to this process as \\u2018query expansion\\u2019. In the face analysis we use the query expansion to provide a query-specific relevant set of \\u2018negative\\u2019 examples which should be separated from the potentially positive examples in the text-based result set. We apply this idea to a recently-proposed method which filters the initial result set using a Gaussian mixture model, and apply the same idea using a logistic discriminant model. We experimentally evaluate the methods using a set of 23 queries on a database of 15.000 captioned news stories from Yahoo! News. The results show that (i) query expansion improves both methods, (ii) that our discriminative models outperform the generative ones, and (iii) our best results surpass the state-of-the-art results by 10% precision on average."'),
('"Improving Shape Retrieval by Learning Graph Transduction"', '"ECCV 2008"', '["Dynamic Time Warping", "Retrieval Result", "Label Propagation", "Retrieval Rate", "Shape Retrieval', '"https://doi.org/10.1007/978-3-540-88693-8_58"', '"Shape retrieval/matching is a very important topic in computer vision. The recent progress in this domain has been mostly driven by designing smart features for providing better similarity measure between pairs of shapes. In this paper, we provide a new perspective to this problem by considering the existing shapes as a group, and study their similarity measures to the query shape in a graph structure. Our method is general and can be built on top of any existing shape matching algorithms. It learns a better metric through graph transduction by propagating the model through existing shapes, in a way similar to computing geodesics in shape manifold. However, the proposed method does not require learning the shape manifold explicitly and it does not require knowing any class labels of existing shapes. The presented experimental results demonstrate that the proposed approach yields significant improvements over the state-of-art shape matching algorithms. We obtained a retrieval rate of 91% on the MPEG-7 data set, which is the highest ever reported in the literature."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Improving the Agility of Keyframe-Based SLAM"', '"ECCV 2008"', '["Point Feature", "Augmented Reality", "Coordinate Frame", "Motion Blur", "Edge Feature"]', '"https://doi.org/10.1007/978-3-540-88688-4_59"', '"The ability to localise a camera moving in a previously unknown environment is desirable for a wide range of applications. In computer vision this problem is studied as monocular SLAM. Recent years have seen improvements to the usability and scalability of monocular SLAM systems to the point that they may soon find uses outside of laboratory conditions. However, the robustness of these systems to rapid camera motions (we refer to this quality as agility) still lags behind that of tracking systems which use known object models. In this paper we attempt to remedy this. We present two approaches to improving the agility of a keyframe-based SLAM system: Firstly, we add edge features to the map and exploit their resilience to motion blur to improve tracking under fast motion. Secondly, we implement a very simple inter-frame rotation estimator to aid tracking when the camera is rapidly panning \\u2013 and demonstrate that this method also enables a trivially simple yet effective relocalisation method. Results show that a SLAM system combining points, edge features and motion initialisation allows highly agile tracking at a moderate increase in processing time."'),
('"Improving the Fisher Kernel for Large-Scale Image Classification"', '"ECCV 2010"', '["Gaussian Mixture Model", "Training Image", "Average Precision", "Sparse Code", "Sift Feature"]', '"https://doi.org/10.1007/978-3-642-15561-1_11"', '"The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets."'),
('"IMPSAC: Synthesis of Importance Sampling and Random Sample Consensus"', '"ECCV 2000"', '["Structure from motion", "Stereoscopic vision"]', '"https://doi.org/10.1007/3-540-45053-X_52"', '"This paper proposes a new method for effecting feature correspondence between images. The method operates from coarse to fine and is superior to previous methods in that it can solve the wide baseline stereo problem, even when the image has been deformed or rotated. At the coarsest level a RANSAC-style estimator is used to estimate the two view image constraint R which is then used to guide matching. The two view relation is an augmented fundamental matrix, being a fundamental matrix plus a homography consistent with that fundamental matrix. This is akin to the plane plus parallax representation with the homography being used to help guide matching and to mitigate the effects of image deformation. In order to propagate the information from coarse to fine images, the distribution of the parameters \\u0398 of R is encoded using a set of particles and an importance sampling function. It is not known in general how to choose the importance sampling function, but a new method \\u201cIMPSAC\\u201d is presented that automatically generates such a function. It is shown that the method is superior to previous single resolution RANSAC-style feature matchers."'),
('"In Defence of Negative Mining for Annotating Weakly Labelled Data"', '"ECCV 2012"', '["Weakly Supervised Learning", "Multiple-Instance Learning", "Negative Mining", "Automatic Annotatio', '"https://doi.org/10.1007/978-3-642-33712-3_43"', '"We propose a novel approach to annotating weakly labelled data. In contrast to many existing approaches that perform annotation by seeking clusters of self-similar exemplars (minimising intra-class variance), we perform image annotation by selecting exemplars that have never occurred before in the much larger, and strongly annotated, negative training set (maximising inter-class variance). Compared to existing methods, our approach is fast, robust, and obtains state of the art results on two challenging data-sets \\u2013 voc2007 (all poses), and the msr2 action data-set, where we obtain a 10% increase. Moreover, this use of negative mining complements existing methods, that seek to minimize the intra-class variance, and can be readily integrated with many of them."'),
('"In Defence of RANSAC for Outlier Rejection in Deformable Registration"', '"ECCV 2012"', '["Thin Plate Spline", "Deformable Surface", "Deformable Registration", "Outlier Rejection", "Local S', '"https://doi.org/10.1007/978-3-642-33765-9_20"', '"This paper concerns the robust estimation of non-rigid deformations from feature correspondences. We advance the surprising view that for many realistic physical deformations, the error of the mismatches (outliers) usually dwarfs the effects of the curvature of the manifold on which the correct matches (inliers) lie, to the extent that one can tightly enclose the manifold within the error bounds of a low-dimensional hyperplane for accurate outlier rejection. This justifies a simple RANSAC-driven deformable registration technique that is at least as accurate as other methods based on the optimisation of fully deformable models. We support our ideas with comprehensive experiments on synthetic and real data typical of the deformations examined in the literature."'),
('"In Search of Art"', '"ECCV 2014"', '["Domain Adaptation", "Object Classification", "Computer Vision in Art"]', '"https://doi.org/10.1007/978-3-319-16178-5_4"', '"The objective of this work is to find objects in paintings by learning object-category classifiers from available sources of natural images. Finding such objects is of much benefit to the art history community as well as being a challenging problem in large-scale retrieval and domain adaptation."'),
('"Incorporating Non-motion Cues into 3D Motion Segmentation"', '"ECCV 2006"', '["Measurement Matrix", "Spatial Coherence", "Motion Segmentation", "Inverse Covariance Matrix", "Loo', '"https://doi.org/10.1007/11744078_7"', '"We address the problem of segmenting an image sequence into rigidly moving 3D objects. An elegant solution to this problem is the multibody factorization approach in which the measurement matrix is factored into lower rank matrices. Despite progress in factorization algorithms, the performance is still far from satisfactory and in scenes with missing data and noise, most existing algorithms fail."'),
('"Increasing 3D Resolution of Kinect Faces"', '"ECCV 2014"', '["Kinect camera", "3D super-resolution", "2D box-splines"]', '"https://doi.org/10.1007/978-3-319-16178-5_45"', '"Performing face recognition across 3D scans of different resolution is now attracting an increasing interest thanks to the introduction of a new generation of depth cameras, capable of acquiring color/depth images over time. However, these devices have still a much lower resolution than the 3D high-resolution scanners typically used for face recognition applications. Due to this, comparing low- and high-resolution scans can be misleading. Based on these considerations, in this paper we define an approach for reconstructing a higher-resolution 3D face model from a sequence of low-resolution 3D scans. The proposed solution uses the scaled ICP algorithm to align the low-resolution scans with each other, and estimates the value of the high-resolution 3D model through a 2D Box-spline approximation. The approach is evaluated on the The Florence face dataset that collects high- and low-resolution data for about 50 subjects. Measures of the quality of the reconstructed models with respect to high-resolution scans and in comparison with two alternative techniques, demonstrate the viability of the proposed solution."'),
('"Increasing Space-Time Resolution in Video"', '"ECCV 2002"', '["Super-resolution", "space-time analysis"]', '"https://doi.org/10.1007/3-540-47969-4_50"', '"We propose a method for constructing a video sequence of high space-time resolution by combining information from multiple low-resolution video sequences of the same dynamic scene. Super-resolution is performed simultaneously in time and in space. By \\u201ctemporal super-resolution\\u201d we mean recovering rapid dynamic events that occur faster than regular frame-rate. Such dynamic events are not visible (or else observed incorrectly) in any of the input sequences, even if these are played in \\u201cslow-motion\\u201d."'),
('"Incremental Singular Value Decomposition of Uncertain Data with Missing Values"', '"ECCV 2002"', '["Singular Vector", "Uncertain Data", "Minimal Rank", "Lanczos Method", "User Prior"]', '"https://doi.org/10.1007/3-540-47969-4_47"', '"We introduce an incremental singular value decomposition (svd) of incomplete data. The svd is developed as data arrives, and can handle arbitrary missing/untrusted values, correlated uncertainty across rows or columns of the measurement matrix, and user priors. Since incomplete data does not uniquely specify an svd, the procedure selects one having minimal rank. For a dense p \\u00d7 q matrix of low rank r, the incremental method has time complexity O(pqr) and space complexity O((p + q)r)\\u2014better than highly optimized batch algorithms such as matlab\\u2019s svd(). In cases of missing data, it produces factorings of lower rank and residual than batch svd algorithms applied to standard missing-data imputations. We show applications in computer vision and audio feature extraction. In computer vision, we use the incremental svd to develop an efficient and unusually robust subspace-estimating flow-based tracker, and to handle occlusions/missing points in structure-from-motion factorizations."'),
('"Indoor Segmentation and Support Inference from RGBD Images"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33715-4_54"', '"We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation."'),
('"Inferring 3D Shapes and Deformations from Single Views"', '"ECCV 2010"', '["Joint Angle", "Shape Variation", "Latent Variable Model", "Single View", "Gaussian Process Latent ', '"https://doi.org/10.1007/978-3-642-15558-1_22"', '"In this paper we propose a probabilistic framework that models shape variations and infers dense and detailed 3D shapes from a single silhouette. We model two types of shape variations, the object phenotype variation and its pose variation using two independent Gaussian Process Latent Variable Models (GPLVMs) respectively. The proposed shape variation models are learnt from 3D samples without prior knowledge about object class, e.g. object parts and skeletons, and are combined to fully span the 3D shape space. A novel probabilistic inference algorithm for 3D shape estimation is proposed by maximum likelihood estimates of the GPLVM latent variables and the camera parameters that best fit generated 3D shapes to given silhouettes. The proposed inference involves a small number of latent variables and it is computationally efficient. Experiments on both human body and shark data demonstrate the efficacy of our new approach."'),
('"Inferring Gene Interaction Networks from ISH Images via Kernelized Graphical Models"', '"ECCV 2012"', '["Annotation Term", "Gaussian Graphical Model", "Gene Interaction Network", "Bregman Divergence", "M', '"https://doi.org/10.1007/978-3-642-33783-3_6"', '"New bio-technologies are being developed that allow high-throughput imaging of gene expressions, where each image captures the spatial gene expression pattern of a single gene in the tissue of interest. This paper addresses the problem of automatically inferring a gene interaction network from such images. We propose a novel kernel-based graphical model learning algorithm, that is both convex and consistent. The algorithm uses multi-instance kernels to compute similarity between the expression patterns of different genes, and then minimizes the L 1 regularized Bregman divergence to estimate a sparse gene interaction network. We apply our algorithm on a large, publicly available data set of gene expression images of Drosophila embryos, where our algorithm makes novel and interesting predictions."'),
('"Inferring White Matter Geometry from Diffusion Tensor MRI: Application to Connectivity Mapping"', '"ECCV 2004"', '["White Matter", "Riemannian Manifold", "Distance Function", "Connectivity Mapping", "WENO5 Scheme"]', '"https://doi.org/10.1007/978-3-540-24673-2_11"', '"We introduce a novel approach to the cerebral white matter connectivity mapping from diffusion tensor MRI. DT-MRI is the unique non-invasive technique capable of probing and quantifying the anisotropic diffusion of water molecules in biological tissues. We address the problem of consistent neural fibers reconstruction in areas of complex diffusion profiles with potentially multiple fibers orientations. Our method relies on a global modelization of the acquired MRI volume as a Riemannian manifold M and proceeds in 4 majors steps: First, we establish the link between Brownian motion and diffusion MRI by using the Laplace-Beltrami operator on M. We then expose how the sole knowledge of the diffusion properties of water molecules on M is sufficient to infer its geometry. There exists a direct mapping between the diffusion tensor and the metric of M. Next, having access to that metric, we propose a novel level set formulation scheme to approximate the distance function related to a radial Brownian motion on M. Finally, a rigorous numerical scheme using the exponential map is derived to estimate the geodesics of M, seen as the diffusion paths of water molecules. Numerical experimentations conducted on synthetic and real diffusion MRI datasets illustrate the potentialities of this global approach."'),
('"Information Theoretic Learning for Pixel-Based Visual Agents"', '"ECCV 2012"', '["Input Image", "Feature Detector", "Scale Invariant Feature Transform", "Convolutional Neural Netwo', '"https://doi.org/10.1007/978-3-642-33783-3_62"', '"In this paper we promote the idea of using pixel-based models not only for low level vision, but also to extract high level symbolic representations. We use a deep architecture which has the distinctive property of relying on computational units that incorporate classic computer vision invariances and, especially, the scale invariance. The learning algorithm that is proposed, which is based on information theory principles, develops the parameters of the computational units and, at the same time, makes it possible to detect the optimal scale for each pixel. We give experimental evidence of the mechanism of feature extraction at the first level of the hierarchy, which is very much related to SIFT-like features. The comparison shows clearly that, whenever we can rely on the massive availability of training data, the proposed model leads to better performances with respect to SIFT."'),
('"Instance Segmentation of Indoor Scenes Using a Coverage Loss"', '"ECCV 2014"', '["Semantic Segmentation", "Deep Learning"]', '"https://doi.org/10.1007/978-3-319-10590-1_40"', '"A major limitation of existing models for semantic segmentation is the inability to identify individual instances of the same class: when labeling pixels with only semantic classes, a set of pixels with the same label could represent a single object or ten. In this work, we introduce a model to perform both semantic and instance segmentation simultaneously. We introduce a new higher-order loss function that directly minimizes the coverage metric and evaluate a variety of region features, including those from a convolutional network. We apply our model to the NYU Depth V2 dataset, obtaining state of the art results."'),
('"Instant Scene Recognition on Mobile Platform"', '"ECCV 2012"', '["Scene Recognition", "DCT Features", "FCam", "Mobile Platform"]', '"https://doi.org/10.1007/978-3-642-33885-4_75"', '"Scene recognition is extremely useful to improve different tasks involved in the Image Generation Pipeline of single sensor mobile devices (e.g., white balancing, autoexposure, etc). This demo showcases our scene recognition engine implemented on a Nokia N900 smartphone. The engine exploits an image representation directly obtainable in the IGP of mobile devices. The demo works in realtime and it is able to discriminate among different classes of scenes. The framework is built by employing the FCam API to have an easy and precise control of the mobile digital camera. Each acquired image (or frame of a video) is holistically represented starting from the statistics collected on DCT domain. This allow instant and \\u201cfree of charge\\u201d feature extraction process since the DCT is always computed into the IGP of a mobile for storage purposes (i.e., JPEG or MPEG format). A SVM classifier is used to perform the final inference about the context of the scene."'),
('"INTAIRACT: Joint Hand Gesture and Fingertip Classification for Touchless Interaction"', '"ECCV 2012"', '["Random Forest", "Gesture Recognition", "Hand Gesture", "Depth Data", "Structure Inference"]', '"https://doi.org/10.1007/978-3-642-33885-4_62"', '"In this demo we present intAIRact, an online hand-based touchless interaction system. Interactions are based on easy-to-learn hand gestures, that combined with translations and rotations render a user friendly and highly configurable system. The main advantage with respect to existing approaches is that we are able to robustly locate and identify fingertips. Hence, we are able to employ a simple but powerful alphabet of gestures not only by determining the number of visible fingers in a gesture, but also which fingers are being observed. To achieve such a system we propose a novel method that jointly infers hand gestures and fingertip locations using a single depth image from a consumer depth camera. Our approach is based on a novel descriptor for depth data, the Oriented Radial Distribution (ORD) [1]. On the one hand, we exploit the ORD for robust classification of hand gestures by means of efficient k-NN retrieval. On the other hand, maxima of the ORD are used to perform structured inference of fingertip locations. The proposed method outperforms other state-of-the-art approaches both in gesture recognition and fingertip localization. An implementation of the ORD extraction on a GPU yields a real-time demo running at approximately 17fps on a single laptop."'),
('"Integral Invariant Signatures"', '"ECCV 2004"', '["Object Recognition", "Kernel Size", "Shape Match", "Moment Invariant", "Shape Representation"]', '"https://doi.org/10.1007/978-3-540-24673-2_8"', '"For shapes represented as closed planar contours, we introduce a class of functionals that are invariant with respect to the Euclidean and similarity group, obtained by performing integral operations. While such integral invariants enjoy some of the desirable properties of their differential cousins, such as locality of computation (which allows matching under occlusions) and uniqueness of representation (in the limit), they are not as sensitive to noise in the data. We exploit the integral invariants to define a unique signature, from which the original shape can be reconstructed uniquely up to the symmetry group, and a notion of scale-space that allows analysis at multiple levels of resolution. The invariant signature can be used as a basis to define various notions of distance between shapes, and we illustrate the potential of the integral invariant representation for shape matching on real and synthetic data."'),
('"Integrating Context and Occlusion for Car Detection by Hierarchical And-Or Model"', '"ECCV 2014"', '["Car Detection", "Context", "Occlusion", "And-Or Graph"]', '"https://doi.org/10.1007/978-3-319-10599-4_42"', '"This paper presents a method of learning reconfigurable hierarchical And-Or models to integrate context and occlusion for car detection. The And-Or model represents the regularities of car-to-car context and occlusion patterns at three levels: (i) layouts of spatially-coupled N cars, (ii) single cars with different viewpoint-occlusion configurations, and (iii) a small number of parts. The learning process consists of two stages. We first learn the structure of the And-Or model with three components: (a) mining N-car contextual patterns based on layouts of annotated single car bounding boxes, (b) mining the occlusion configurations based on the overlapping statistics between single cars, and (c) learning visible parts based on car 3D CAD simulation or heuristically mining latent car parts. The And-Or model is organized into a directed and acyclic graph which leads to the Dynamic Programming algorithm in inference. In the second stage, we jointly train the model parameters (for appearance, deformation and bias) using Weak-Label Structural SVM. In experiments, we test our model on four car datasets: the KITTI dataset [11], the street parking dataset [19], the PASCAL VOC2007 car dataset [7], and a self-collected parking lot dataset. We compare with state-of-the-art variants of deformable part-based models and other methods. Our model obtains significant improvement consistently on the four datasets."'),
('"Integrating Faces, Fingerprints, and Soft Biometric Traits for User Recognition"', '"BioAW 2004"', '["Linear Discriminant Analysis", "Facial Image", "Recognition Performance", "Face Database", "Biomet', '"https://doi.org/10.1007/978-3-540-25976-3_24"', '"Soft biometric traits like gender, age, height, weight, ethnicity, and eye color cannot provide reliable user recognition because they are not distinctive and permanent. However, such ancillary information can complement the identity information provided by the primary biometric traits (face, fingerprint, hand-geometry, iris, etc.). This paper describes a hybrid biometric system that uses face and fingerprint as the primary characteristics and gender, ethnicity, and height as the soft characteristics. We have studied the effect of the soft biometric traits on the recognition performance of unimodal face and fingerprint recognition systems and a multimodal system that uses both the primary traits. Experiments conducted on a database of 263 users show that the recognition performance of the primary biometric system can be improved significantly by making use of soft biometric information. The results also indicate that such a performance improvement can be achieved only if the soft biometric traits are complementary to the primary biometric traits."'),
('"Integrating Local Affine into Global Projective Images in the Joint Image Space"', '"ECCV 2000"', '["Fundamental Matrix", "Match Point", "Epipolar Line", "Epipolar Geometry", "Tangent Hyperplane"]', '"https://doi.org/10.1007/3-540-45054-8_59"', '"The fundamental matrix defines a nonlinear 3D variety in the joint image space of multiple projective (or \\u201cuncalibrated perspective\\u201d) images. We show that, in the case of two images, this variety is a 4D cone whose vertex is the joint epipole (namely the 4D point obtained by stacking the two epipoles in the two images). Affine (or \\u201cpara-perspective\\u201d) projection approximates this nonlinear variety with a linear subspace, both in two views and in multiple views. We also show that the tangent to the projective joint image at any point on that image is obtained by using local affine projection approximations around the corresponding 3D point. We use these observations to develop a new approach for recovering multiview geometry by integrating multiple local affine joint images into the global projective joint image. Given multiple projective images, the tangents to the projective joint image are computed using local affine approximations for multiple image patches. The affine parameters from different patches are combined to obtain the epipolar geometry of pairs of projective images. We describe two algorithms for this purpose, including one that directly recovers the image epipoles without recovering the fundamental matrix as an intermediate step."'),
('"Integrating Object Affordances with Artificial Visual Attention"', '"ECCV 2014"', '["Attention", "Saliency", "Affordance"]', '"https://doi.org/10.1007/978-3-319-16181-5_32"', '"Affordances, e.g., grasping possibilities, play a role in the guidance of human attention. We report experiments on the integration of affordance estimation with artificial visual attention in a prototypical model. Furthermore, Growing Neural Gas is discussed as a potential framework for future attention models that deeply integrate affordance, saliency and further attentional mechanisms."'),
('"Integrating Surface Normal Vectors Using Fast Marching Method"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744078_19"', '"Integration of surface normal vectors is a vital component in many shape reconstruction algorithms that require integrating surface normals to produce their final outputs, the depth values. In this paper, we introduce a fast and efficient method for computing the depth values from surface normal vectors. The method is based on solving the Eikonal equation using Fast Marching Method. We introduce two ideas. First, while it is not possible to solve for the depths Z directly using Fast Marching Method, we solve the Eikonal equation for a function W of the form W = Z + \\u03bbf. With appropriately chosen values for \\u03bb, we can ensure that the Eikonal equation for W can be solved using Fast Marching Method. Second, we solve for W in two stages with two different \\u03bb values, first in a small neighborhood of the given initial point with large \\u03bb, and then for the rest of the domain with a smaller \\u03bb. This step is needed because of the finite machine precision and rounding-off errors. The proposed method is very easy to implement, and we demonstrate experimentally that, with insignificant loss in precision, our method is considerably faster than the usual optimization method that uses conjugate gradient to minimize an error function."'),
('"Integration of Multiview Stereo and Silhouettes Via Convex Functionals on Convex Domains"', '"ECCV 2008"', '["Admissible Function", "Visual Hull", "Reprojection Error", "Stereo Information", "Multiview Stereo', '"https://doi.org/10.1007/978-3-540-88682-2_57"', '"We propose a convex framework for silhouette and stereo fusion in 3D reconstruction from multiple images. The key idea is to show that the reconstruction problem can be cast as one of minimizing a convex functional where the exact silhouette consistency is imposed as a convex constraint that restricts the domain of admissible functions. As a consequence, we can retain the original stereo-weighted surface area as a cost functional without heuristic modifications by balloon terms or other strategies, yet still obtain meaningful (nonempty) global minimizers. Compared to previous methods, the introduced approach does not depend on initialization and leads to a more robust numerical scheme by removing the bias near the visual hull boundary. We propose an efficient parallel implementation of this convex optimization problem on a graphics card. Based on a photoconsistency map and a set of image silhouettes we are therefore able to compute highly-accurate and silhouette-consistent reconstructions for challenging real-world data sets in less than one minute."'),
('"Inter-camera Association of Multi-target Tracks by On-Line Learned Appearance Affinity Models"', '"ECCV 2010"', '["Training Sample", "Color Histogram", "Appearance Model", "Equal Error Rate", "Multiple Instance Le', '"https://doi.org/10.1007/978-3-642-15549-9_28"', '"We propose a novel system for associating multi-target tracks across multiple non-overlapping cameras by an on-line learned discriminative appearance affinity model. Collecting reliable training samples is a major challenge in on-line learning since supervised correspondence is not available at runtime. To alleviate the inevitable ambiguities in these samples, Multiple Instance Learning (MIL) is applied to learn an appearance affinity model which effectively combines three complementary image descriptors and their corresponding similarity measurements. Based on the spatial-temporal information and the proposed appearance affinity model, we present an improved inter-camera track association framework to solve the \\u201ctarget handover\\u201d problem across cameras. Our evaluations indicate that our method have higher discrimination between different targets than previous methods."'),
('"Inter-modality Face Recognition"', '"ECCV 2006"', '["Face Recognition", "Linear Discriminant Analysis", "Face Image", "Learning Objective", "Query Imag', '"https://doi.org/10.1007/11744085_2"', '"Recently, the wide deployment of practical face recognition systems gives rise to the emergence of the inter-modality face recognition problem. In this problem, the face images in the database and the query images captured on spot are acquired under quite different conditions or even using different equipments. Conventional approaches either treat the samples in a uniform model or introduce an intermediate conversion stage, both of which would lead to severe performance degradation due to the great discrepancies between different modalities. In this paper, we propose a novel algorithm called Common Discriminant Feature Extraction specially tailored to the inter-modality problem. In the algorithm, two transforms are simultaneously learned to transform the samples in both modalities respectively to the common feature space. We formulate the learning objective by incorporating both the empirical discriminative power and the local smoothness of the feature transformation. By explicitly controlling the model complexity through the smoothness constraint, we can effectively reduce the risk of overfitting and enhance the generalization capability. Furthermore, to cope with the nongaussian distribution and diverse variations in the sample space, we develop two nonlinear extensions of the algorithm: one is based on kernelization, while the other is a multi-mode framework. These extensions substantially improve the recognition performance in complex situation. Extensive experiments are conducted to test our algorithms in two application scenarios: optical image-infrared image recognition and photo-sketch recognition. Our algorithms show excellent performance in the experiments."'),
('"Interactive Facial Feature Localization"', '"ECCV 2012"', '["Face Image", "Facial Feature", "Shape Model", "Candidate Location", "Viterbi Algorithm"]', '"https://doi.org/10.1007/978-3-642-33712-3_49"', '"We address the problem of interactive facial feature localization from a single image. Our goal is to obtain an accurate segmentation of facial features on high-resolution images under a variety of pose, expression, and lighting conditions. Although there has been significant work in facial feature localization, we are addressing a new application area, namely to facilitate intelligent high-quality editing of portraits, that brings requirements not met by existing methods. We propose an improvement to the Active Shape Model that allows for greater independence among the facial components and improves on the appearance fitting step by introducing a Viterbi optimization process that operates along the facial contours. Despite the improvements, we do not expect perfect results in all cases. We therefore introduce an interaction model whereby a user can efficiently guide the algorithm towards a precise solution. We introduce the Helen Facial Feature Dataset consisting of annotated portrait images gathered from Flickr that are more diverse and challenging than currently existing datasets. We present experiments that compare our automatic method to published results, and also a quantitative evaluation of the effectiveness of our interactive method."'),
('"Interactive Image Segmentation Using an Adaptive GMMRF Model"', '"ECCV 2004"', '["Partition Function", "Ising Model", "Spatial Interaction", "Foreground Object", "Parameter Learnin', '"https://doi.org/10.1007/978-3-540-24670-1_33"', '"The problem of interactive foreground/background segmentation in still images is of great practical importance in image editing. The state of the art in interactive segmentation is probably represented by the graph cut algorithm of Boykov and Jolly (ICCV 2001). Its underlying model uses both colour and contrast information, together with a strong prior for region coherence. Estimation is performed by solving a graph cut problem for which very efficient algorithms have recently been developed. However the model depends on parameters which must be set by hand and the aim of this work is for those constants to be learned from image data."'),
('"Interactive Object Counting"', '"ECCV 2014"', '["Interactive vision systems", "object counting", "relevance feedback", "visual recognition", "biome', '"https://doi.org/10.1007/978-3-319-10578-9_33"', '"Our objective is to count (and localize) object instances in an image interactively. We target the regime where individual object detectors do not work reliably due to crowding, or overlap, or size of the instances, and take the approach of estimating an object density."'),
('"Interactive Tracking of 2D Generic Objects with Spacetime Optimization"', '"ECCV 2008"', '["Target Object", "Object Tracking", "Illumination Change", "Candidate Object", "Interpolation Weigh', '"https://doi.org/10.1007/978-3-540-88682-2_50"', '"We present a continuous optimization framework for interactive tracking of 2D generic objects in a single video stream. The user begins with specifying the locations of a target object in a small set of keyframes; the system then automatically tracks locations of the objects by combining user constraints with visual measurements across the entire sequence. We formulate the problem in a spacetime optimization framework that optimizes over the whole sequence simultaneously. The resulting solution is consistent with visual measurements across the entire sequence while satisfying user constraints. We also introduce prior terms to reduce tracking ambiguity. We demonstrate the power of our algorithm on tracking objects with significant occlusions, scale and orientation changes, illumination changes, sudden movement of objects, and also simultaneous tracking of multiple objects. We compare the performance of our algorithm with alternative methods."'),
('"Interactively Guiding Semi-Supervised Clustering via Attribute-Based Explanations"', '"ECCV 2014"', '["Cluster Algorithm", "Spectral Cluster", "Soft Constraint", "Neural Information Processing System",', '"https://doi.org/10.1007/978-3-319-10599-4_22"', '"Unsupervised image clustering is a challenging and often ill-posed problem. Existing image descriptors fail to capture the clustering criterion well, and more importantly, the criterion itself may depend on (unknown) user preferences. Semi-supervised approaches such as distance metric learning and constrained clustering thus leverage user-provided annotations indicating which pairs of images belong to the same cluster (must-link) and which ones do not (cannot-link). These approaches require many such constraints before achieving good clustering performance because each constraint only provides weak cues about the desired clustering. In this paper, we propose to use image attributes as a modality for the user to provide more informative cues. In particular, the clustering algorithm iteratively and actively queries a user with an image pair. Instead of the user simply providing a must-link/cannot-link constraint for the pair, the user also provides an attribute-based reasoning e.g. \\u201cthese two images are similar because both are natural and have still water\\u201d or \\u201cthese two people are dissimilar because one is way older than the other\\u201d. Under the guidance of this explanation, and equipped with attribute predictors, many additional constraints are automatically generated. We demonstrate the effectiveness of our approach by incorporating the proposed attribute-based explanations in three standard semi-supervised clustering algorithms: Constrained K-Means, MPCK-Means, and Spectral Clustering, on three domains: scenes, shoes, and faces, using both binary and relative attributes."'),
('"Interestingness Prediction by Robust Learning to Rank"', '"ECCV 2014"', '["Majority Vote", "Outlier Detection", "Interestingness Prediction", "Robust Learn", "Regularisation', '"https://doi.org/10.1007/978-3-319-10605-2_32"', '"The problem of predicting image or video interestingness from their low-level feature representations has received increasing interest. As a highly subjective visual attribute, annotating the interestingness value of training data for learning a prediction model is challenging. To make the annotation less subjective and more reliable, recent studies employ crowdsourcing tools to collect pairwise comparisons \\u2013 relying on majority voting to prune the annotation outliers/errors. In this paper, we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem, tackling both the outlier detection and interestingness prediction tasks jointly. Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives."'),
('"Interpolating Novel Views from Image Sequences by Probabilistic Depth Carving"', '"ECCV 2004"', '["Reference Frame", "Texture Region", "View Synthesis", "Reference View", "Gaussian Pyramid"]', '"https://doi.org/10.1007/978-3-540-24671-8_30"', '"We describe a novel approach to view interpolation from image sequences based on probabilistic depth carving. This builds a multivalued representation of depth for novel views consisting of likelihoods of depth samples corresponding to either opaque or free space points. The likelihoods are obtained from iterative probabilistic combination of local disparity estimates about a subset of reference frames. This avoids the difficult problem of correspondence matching across distant views and leads to an explicit representation of occlusion. Novel views are generated by combining pixel values from the reference frames based on estimates of surface points within the likelihood representation. Efficient implementation is achieved using a multiresolution framework. Results of experiments on real image sequences show that the technique is effective."'),
('"Interpolating Orientation Fields: An Axiomatic Approach"', '"ECCV 2006"', '["Curvature Operator", "Level Line", "Scalar Case", "Extension Operator", "Illusory Contour"]', '"https://doi.org/10.1007/11744085_19"', '"We develop an axiomatic approach of vector field interpolation, which is useful as a feature extraction preprocessing step. Two operators will be singled out: the curvature operator, appearing in the total variation minimisation for image restoration and inpainting/disocclusion, and the Absolutely Minimizing Lipschitz Extension (AMLE), already known as a robust and coherent scalar image interpolation technique if we relax slightly the axioms. Numerical results, using a multiresolution scheme, show that they produce fields in accordance with the human perception of edges."'),
('"Interpolating Sporadic Data"', '"ECCV 2002"', '["shape", "image analysis and features", "curve interpolation"]', '"https://doi.org/10.1007/3-540-47967-8_41"', '"We report here on the problem of estimating a smooth planar curve \\u03b3: [0, T] \\u2192 \\u211d2 and its derivatives from an ordered sample of interpolation points \\u03b3(t 0), \\u03b3(t 1),...,\\u03b3(t i -1),\\u03b3(t i ),...,\\u03b3(t m -1),\\u03b3(t m ), where 0 = t 0 < t 1 <... < t i - 1 < t i <...< t m - 1 < t m = T, and the t i are not known precisely for 0 < i < m. Such situtation may appear while searching for the boundaries of planar objects or tracking the mass center of a rigid body with no times available. In this paper we assume that the distribution of t i coincides with more-or-less uniform sampling. A fast algorithm, yielding quartic convergence rate based on 4-point piecewise-quadratic interpolation is analysed and tested. Our algorithm forms a substantial improvement (with respect to the speed of convergence) of piecewise 3-point quadratic Lagrange intepolation [19] and [20]. Some related work can be found in [7]. Our results may be of interest in computer vision and digital image processing [5], [8], [13], [14], [17] or [24], computer graphics [1], [4], [9], [10], [21] or [23], approximation and complexity theory [3], [6], [16], [22], [26] or [27], and digital and computational geometry [2] and [15]."'),
('"Interreflection Removal Using Fluorescence"', '"ECCV 2014"', '["Fluorescence", "bispectral model", "bispectral interreflection model", "and interreflection remova', '"https://doi.org/10.1007/978-3-319-10602-1_14"', '"Interreflections exhibit a number of challenges for existing shape-from-intensity methods that only assume a direct lighting model. Removing the interreflections from scene observations is of broad interest since it enhances the accuracy of those methods. In this paper, we propose a method for removing interreflections from a single image using fluorescence. From a bispectral observation of reflective and fluorescent components recorded in distinct color channels, our method separates direct lighting from interreflections. Experimental results demonstrate the effectiveness of the proposed method on complex and dynamic scenes. In addition, we show how our method improves an existing photometric stereo method in shape recovery."'),
('"Intrinsic Face Image Decomposition with Human Face Priors"', '"ECCV 2014"', '["intrinsic image decomposition", "reflectance models", "human face priors"]', '"https://doi.org/10.1007/978-3-319-10602-1_15"', '"We present a method for decomposing a single face photograph into its intrinsic image components. Intrinsic image decomposition has commonly been used to facilitate image editing operations such as relighting and re-texturing. Although current single-image intrinsic image methods are able to obtain an approximate decomposition, image operations involving the human face require greater accuracy since slight errors can lead to visually disturbing results. To improve decomposition for faces, we propose to utilize human face priors as constraints for intrinsic image estimation. These priors include statistics on skin reflectance and facial geometry. We also make use of a physically-based model of skin translucency to heighten accuracy, as well as to further decompose the reflectance image into a diffuse and a specular component. With the use of priors and a skin reflectance model for human faces, our method is able to achieve appreciable improvements in intrinsic image decomposition over more generic techniques."'),
('"Intrinsic Image Decomposition Using Structure-Texture Separation and Surface Normals"', '"ECCV 2014"', '["intrinsic image decomposition", "structure-texture separation", "RGB-D image"]', '"https://doi.org/10.1007/978-3-319-10584-0_15"', '"While intrinsic image decomposition has been studied extensively during the past a few decades, it is still a challenging problem. This is partly because commonly used constraints on shading and reflectance are often too restrictive to capture an important property of natural images, i.e., rich textures. In this paper, we propose a novel image model for handling textures in intrinsic image decomposition, which enables us to produce high quality results even with simple constraints. We also propose a novel constraint based on surface normals obtained from an RGB-D image. Assuming Lambertian surfaces, we formulate the constraint based on a locally linear embedding framework to promote local and global consistency on the shading layer. We demonstrate that combining the novel texture-aware image model and the novel surface normal based constraint can produce superior results to existing approaches."'),
('"Intrinsic Images by Entropy Minimization"', '"ECCV 2004"', '["Colour Target", "Colour Patch", "Entropy Minimization", "Projection Angle", "Lighting Change"]', '"https://doi.org/10.1007/978-3-540-24672-5_46"', '"A method was recently devised for the recovery of an invariant image from a 3-band colour image. The invariant image, originally 1D greyscale but here derived as a 2D chromaticity, is independent of lighting, and also has shading removed: it forms an intrinsic image that may be used as a guide in recovering colour images that are independent of illumination conditions. Invariance to illuminant colour and intensity means that such images are free of shadows, as well, to a good degree. The method devised finds an intrinsic reflectivity image based on assumptions of Lambertian reflectance, approximately Planckian lighting, and fairly narrowband camera sensors. Nevertheless, the method works well when these assumptions do not hold. A crucial piece of information is the angle for an \\u201cinvariant direction\\u201d in a log-chromaticity space. To date, we have gleaned this information via a preliminary calibration routine, using the camera involved to capture images of a colour target under different lights. In this paper, we show that we can in fact dispense with the calibration step, by recognizing a simple but important fact: the correct projection is that which minimizes entropy in the resulting invariant image. To show that this must be the case we first consider synthetic images, and then apply the method to real images. We show that not only does a correct shadow-free image emerge, but also that the angle found agrees with that recovered from a calibration. As a result, we can find shadow-free images for images with unknown camera, and the method is applied successfully to remove shadows from unsourced imagery."'),
('"Intrinsic Images for Dense Stereo Matching with Occlusions"', '"ECCV 2000"', '[]', '"https://doi.org/10.1007/3-540-45054-8_7"', '"Stereo correspondence is a central issue in computer vision. The traditional approach involves extracting image features, establishing correspondences based on photometric and geometric criteria and finally, determine a dense disparity field by interpolation. In this context, occlusions are considered as undesirable artifacts and often ignored."'),
('"Intrinsic Regularity Detection in 3D Geometry"', '"ECCV 2010"', '["Isometric Embedding", "Descriptor Image", "Intrinsic Structure", "Landmark Point", "Geometry Proce', '"https://doi.org/10.1007/978-3-642-15558-1_29"', '"Automatic detection of symmetries, regularity, and repetitive structures in 3D geometry is a fundamental problem in shape analysis and pattern recognition with applications in computer vision and graphics. Especially challenging is to detect intrinsic regularity, where the repetitions are on an intrinsic grid, without any apparent Euclidean pattern to describe the shape, but rising out of (near) isometric deformation of the underlying surface. In this paper, we employ multidimensional scaling to reduce the problem of intrinsic structure detection to a simpler problem of 2D grid detection. Potential 2D grids are then identified using an autocorrelation analysis, refined using local fitting, validated, and finally projected back to the spatial domain. We test the detection algorithm on a variety of scanned plaster models in presence of imperfections like missing data, noise and outliers. We also present a range of applications including scan completion, shape editing, super-resolution, and structural correspondence."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Intrinsic Textures for Relightable Free-Viewpoint Video"', '"ECCV 2014"', '["Free-Viewpoint Video Rendering", "Image-Based Rendering", "Relighting", "Intrinsic Images"]', '"https://doi.org/10.1007/978-3-319-10605-2_26"', '"This paper presents an approach to estimate the intrinsic texture properties (albedo, shading, normal) of scenes from multiple view acquisition under unknown illumination conditions. We introduce the concept of intrinsic textures, which are pixel-resolution surface textures representing the intrinsic appearance parameters of a scene. Unlike previous video relighting methods, the approach does not assume regions of uniform albedo, which makes it applicable to richly textured scenes. We show that intrinsic image methods can be used to refine an initial, low-frequency shading estimate based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading. The method is applied to relighting of free-viewpoint rendering from multiple view video capture. This demonstrates relighting with reproduction of fine surface detail. Quantitative evaluation on synthetic models with textured appearance shows accurate estimation of intrinsic surface reflectance properties."'),
('"Intrinsic Video"', '"ECCV 2014"', '["intrinsic images", "video", "temporal coherence", "optical flow"]', '"https://doi.org/10.1007/978-3-319-10605-2_24"', '"Intrinsic images such as albedo and shading are valuable for later stages of visual processing. Previous methods for extracting albedo and shading use either single images or images together with depth data. Instead, we define intrinsic video estimation as the problem of extracting temporally coherent albedo and shading from video alone. Our approach exploits the assumption that albedo is constant over time while shading changes slowly. Optical flow aids in the accurate estimation of intrinsic video by providing temporal continuity as well as putative surface boundaries. Additionally, we find that the estimated albedo sequence can be used to improve optical flow accuracy in sequences with changing illumination. The approach makes only weak assumptions about the scene and we show that it substantially outperforms existing single-frame intrinsic image methods. We evaluate this quantitatively on synthetic sequences as well on challenging natural sequences with complex geometry, motion, and illumination."'),
('"Inverse Kernels for Fast Spatial Deconvolution"', '"ECCV 2014"', '["deconvolution", "inverse kernels", "numerical analysis", "optimization"]', '"https://doi.org/10.1007/978-3-319-10602-1_3"', '"Deconvolution is an indispensable tool in image processing and computer vision. It commonly employs fast Fourier transform (FFT) to simplify computation. This operator, however, needs to transform from and to the frequency domain and loses spatial information when processing irregular regions. We propose an efficient spatial deconvolution method that can incorporate sparse priors to suppress noise and visual artifacts. It is based on estimating inverse kernels that are decomposed into a series of 1D kernels. An augmented Lagrangian method is adopted, making inverse kernel be estimated only once for each optimization process. Our method is fully parallelizable and its speed is comparable to or even faster than other strategies employing FFTs."'),
('"Inverse Rendering of Faces on a Cloudy Day"', '"ECCV 2012"', '["Principal Component Analysis Model", "Global Illumination", "Morphable Model", "Ambient Occlusion"', '"https://doi.org/10.1007/978-3-642-33712-3_15"', '"In this paper we consider the problem of inverse rendering faces under unknown environment illumination using a morphable model. In contrast to previous approaches, we account for global illumination effects by incorporating statistical models for ambient occlusion and bent normals into our image formation model. We show that solving for ambient occlusion and bent normal parameters as part of the fitting process improves the accuracy of the estimated texture map and illumination environment. We present results on challenging data, rendered under complex natural illumination with both specular reflectance and occlusion of the illumination environment."'),
('"Investigating Open-World Person Re-identification Using a Drone"', '"ECCV 2014"', '["Computer Vision", "Benchmark Dataset", "Domain Adaptation", "Discriminative Model", "CCTV Camera"]', '"https://doi.org/10.1007/978-3-319-16199-0_16"', '"Person re-identification is now one of the most topical and intensively studied problems in computer vision due to its challenging nature and its critical role in underpinning many multi-camera surveillance tasks. A fundamental assumption in almost all existing re-identification research is that cameras are in fixed emplacements, allowing the explicit modelling of camera and inter-camera properties in order to improve re-identification. In this paper, we present an introductory study pushing re-identification in a different direction: re-identification on a mobile platform, such as a drone. We formalise some variants of the standard formulation for re-identification that are more relevant for mobile re-identification. We introduce the first dataset for mobile re-identification, and we use this to elucidate the unique challenges of mobile re-identification. Finally, we re-evaluate some conventional wisdom about re-id models in the light of these challenges and suggest future avenues for research in this area."'),
('"Invited Talk: Coupling Deformable Models and Learning Methods for Nonverbal Behavior Analysis: Appl', '"ECCV 2010"', '["American Sign Language", "Average Recognition Rate", "Facial Landmark", "Mock Crime", "Reduce Feat', '"https://doi.org/10.1007/978-3-642-35749-7_25"', '"Based on recent advances in deformable model tracking theory, we have developed a novel system for real-time facial and gesture tracking and action recognition. In particular, our face tracker by using deformable statistical models that encode facial shape variation and local texture distribution, it robustly tracks 79 facial landmarks, which correspond to facial components such as the eyes, eyebrows, nose, and mouth. The model initializes automatically, tolerates partial occlusions, detects and recovers from lost track. Moreover, it handles head rotations of -90\\u2019 to 90\\u2019 in any direction by using manifold embedding methods. During online tracking, the model dynamically adapts to the facial shape of the current subject and temporal filters stochastically smooth the target\\u2019s position. Tracked landmarks are then used by our learning modules for feature extraction and event recognition. In order to speed up convergence to the optimal landmark configuration, the system employs multi-resolution model fitting. To further reduce computational complexity, we track landmarks in successive frames using a Sum of Squared Differences point tracker and running the relatively \\u201dexpensive\\u201d step of face search only periodically to prevent any error accumulation. This scheme allows us to have a measure of tracking success (confidence) for each landmark, so that we can detect early on if we are beginning to drift from the target, in which case we immediately invoke the deformable fitting algorithm to self-correct the result. Similarly, we have developed a skin blob tracker for tracking the orientation, position, velocity and area of head and hand blobs, which is automatically initialized with a generic skin color model, dynamically learning the specific subject\\u2019s color distribution online for adaptive tracking. Detected blobs are filtered online, both in terms of shape and motion, using eigenspace analysis and temporal dynamical models to prune false detections."'),
('"Is Light Blue (azzurro) Color Name Universal in the Italian Language?"', '"ECCV 2010"', '["Color perception", "Color categories", "Italian color terms", "Cultural influence"]', '"https://doi.org/10.1007/978-3-642-35740-4_8"', '"In the study of 1969 Berlin and Kay have argued that there are a limited number of universal \\u201dbasic color terms\\u201d which are the same for each culture [1]. They postulate the existence of 11 basic color terms, including a single blue term. After Berlin and Kay\\u2019s work, several researcher have tried to confirm or refuse this theory. Those successive studies led to two principal theories: universalistic [2] (confirming the Berlin and Key theory) and relativistic (refusing the Berlin and Key hypothesis) theories [3-6]. This papers brings a new argument in favor of the relativistic theory and provides some evidence on the existence of a twelfth color class in the Italian language. In particular, results support the hypothesis of the existence of an additional monoleximic color name for the class corresponding to light blue (azzurro in Italian language). This hypothesis is proved by using the Stroop effect, introduced in 1953 by John Ridley Stroop [7]. The Stroop effect is based on the analysis of the reaction time in a given task. Our claim is that when the name of a color (e.g., \\u201dblue,\\u201d \\u201dgreen,\\u201d or \\u201dred\\u201d) is printed in a color which is not denoted by the name (e.g., the word \\u201dred\\u201d printed in blue ink instead of red ink), naming the color of the word takes longer and is more prone to errors than when the color of the ink matches the name of the color. Accordingly, we investigated the reaction time of Italian mother language speakers performing a Stroop task with both dark blue and light blue color. Results show that the reaction time is statistically different when the light blue is associated to the monoleximic color name azzurro than to monoleximic color name blue (blu in Italian language)."'),
('"Is Super-Resolution with Optical Flow Feasible?"', '"ECCV 2002"', '["Optical Flow", "Motion Estimation", "Motion Error", "Image Alignment", "Reprojection Error"]', '"https://doi.org/10.1007/3-540-47969-4_40"', '"Reconstruction-based super-resolution from motion video has been an active area of study in computer vision and video analysis. Image alignment is a key component of super-resolution algorithms. Almost all previous super-resolution algorithms have assumed that standard methods of image alignment can provide accurate enough alignment for creating super-resolution images. However, a systematic study of the demands on accuracy of multi-image alignment and its effects on super-resolution has been lacking. Furthermore, implicitly or explicitly most algorithms have assumed that the multiple video frames or specific regions of interest are related through global parametric transformations. From previous works, it is not at all clear how super-resolution performs under alignment with piecewise parametric or local optical flow based methods. This paper is an attempt at understanding the influence of image alignment and warping errors on super-resolution. Requirements on the consistency of optical flow across multiple images are studied and it is shown that errors resulting from traditional flow algorithms may render super-resolution infeasible."'),
('"Iso-disparity Surfaces for General Stereo Configurations"', '"ECCV 2004"', '["Stereo Vision", "Active Vision", "Stereo Match", "Stereo Pair", "Warping Function"]', '"https://doi.org/10.1007/978-3-540-24672-5_40"', '"This paper discusses the iso-disparity surfaces for general stereo configurations. These are the surfaces that are observed at the same resolution along the epipolar lines in both images of a stereo pair. For stereo algorithms that include smoothness terms either implicitly through area-based correlation or explicitly by using penalty terms for neighboring pixels with dissimilar disparities these surfaces also represent the implicit hypothesis made during stereo matching. Although the shape of these surfaces is well known for the standard stereo case (i.e. fronto-parallel planes), surprisingly enough for two cameras in a general configuration to our knowledge their shape has not been studied. This is, however, very important since it represents the discretisation of stereo sampling in 3D space and represents absolute bounds on performance independent of later resampling. We prove that the intersections of these surfaces with an epipolar plane consists of a family of conics with three fixed points. There is an interesting relation to the human horopter and we show that for stereo the retinas act as if they were flat. Further we discuss the relevance of iso-disparity surfaces to image-pair rectification and active vision. In experiments we show how one can configure an active stereo head to align iso-disparity surfaces to scene structures of interest such as a vertical wall, allowing better and faster stereo results."'),
('"Iterative Extensions of the Sturm/Triggs Algorithm: Convergence and Nonconvergence"', '"ECCV 2006"', '["Stationary Point", "Quadratic Convergence", "Bundle Adjustment", "Projective Depth", "Real Image S', '"https://doi.org/10.1007/11744085_17"', '"We show that SIESTA, the simplest iterative extension of the Sturm/Triggs algorithm, descends an error function. However, we prove that SIESTA does not converge to usable results. The iterative extension of Mahamud et al. has similar problems, and experiments with \\u201cbalanced\\u201d iterations show that they can fail to converge. We present CIESTA, an algorithm which avoids these problems. It is identical to SIESTA except for one extra, simple stage of computation. We prove that CIESTA descends an error and approaches fixed points. Under weak assumptions, it converges. The CIESTA error can be minimized using a standard descent method such as Gauss\\u2013Newton, combining quadratic convergence with the advantage of minimizing in the projective depths."'),
('"JenAesthetics Subjective Dataset: Analyzing Paintings by Subjective Scores"', '"ECCV 2014"', '["Computational aesthetics", "Aesthetic", "Beauty", "Color", "Content", "Composition", "Paintings", ', '"https://doi.org/10.1007/978-3-319-16178-5_1"', '"Over the last few years, researchers from the computer vision and image processing community have joined other research groups in searching for the bases of aesthetic judgment of paintings and photographs. One of the most important issues, which has hampered research in the case of paintings compared to photographs, is the lack of subjective datasets available for public use. This issue has not only been mentioned in different publications, but was also widely discussed at different conferences and workshops. In the current work, we perform a subjective test on a recently released dataset of aesthetic paintings. The subjective test not only collects scores based on the subjective aesthetic quality, but also on other properties that have been linked to aesthetic judgment."'),
('"Jet-Based Local Image Descriptors"', '"ECCV 2012"', '["Interest Point", "Image Patch", "View Angle", "Linear Path", "Interest Point Detector"]', '"https://doi.org/10.1007/978-3-642-33712-3_46"', '"We present a general novel image descriptor based on higherorder differential geometry and investigate the effect of common descriptor choices. Our investigation is twofold in that we develop a jet-based descriptor and perform a comparative evaluation with current state-of-the-art descriptors on the recently released DTU Robot dataset. We demonstrate how the use of higher-order image structures enables us to reduce the descriptor dimensionality while still achieving very good performance. The descriptors are tested in a variety of scenarios including large changes in scale, viewing angle and lighting. We show that the proposed jet-based descriptor is superior to state-of-the-art for DoG interest points and show competitive performance for the other tested interest points."'),
('"Joint Bayes Filter: A Hybrid Tracker for Non-rigid Hand Motion Recognition"', '"ECCV 2004"', '["Hide Markov Model", "Hand Motion", "Particle Filter", "Gesture Recognition", "Dynamic Bayesian Net', '"https://doi.org/10.1007/978-3-540-24672-5_39"', '"In sign-language or gesture recognition, articulated hand motion tracking is usually a prerequisite to behaviour understanding. However the difficulties such as non-rigidity of the hand, complex background scenes, and occlusion etc make tracking a challenging task. In this paper we present a hybrid HMM/Particle filter tracker for simultaneously tracking and recognition of non-rigid hand motion. By utilising separate image cues, we decompose complex motion into two independent (non-rigid/rigid) components. A generative model is used to explore the intrinsic patterns of the hand articulation. Non-linear dynamics of the articulation such as fast appearance deformation can therefore be tracked without resorting to a complex kinematic model. The rigid motion component is approximated as the motion of a planar region, where a standard particle filter method suffice. The novel contribution of the paper is that we unify the independent treatments of non-rigid motion and rigid motion into a robust Bayesian framework. The efficacy of this method is demonstrated by performing successful tracking in the presence of significant occlusion clutter."'),
('"Joint Cascade Face Detection and Alignment"', '"ECCV 2014"', '["Face Detection", "Face Shape", "Sift Descriptor", "Split Test", "Facial Point"]', '"https://doi.org/10.1007/978-3-319-10599-4_8"', '"We present a new state-of-the-art approach for face detection. The key idea is to combine face alignment with detection, observing that aligned face shapes provide better features for face classification. To make this combination more effective, our approach learns the two tasks jointly in the same cascade framework, by exploiting recent advances in face alignment. Such joint learning greatly enhances the capability of cascade detection and still retains its realtime performance. Extensive experiments show that our approach achieves the best accuracy on challenging datasets, where all existing solutions are either inaccurate or too slow."'),
('"Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation"', '"ECCV 2012"', '["Leaf Node", "Class Label", "Tree Depth", "Split Node", "Spatial Regularization"]', '"https://doi.org/10.1007/978-3-642-33765-9_62"', '"In many segmentation scenarios, labeled images contain rich structural information about spatial arrangement and shapes of the objects. Integrating this rich information into supervised learning techniques is promising as it generates models which go beyond learning class association, only. This paper proposes a new supervised forest model for joint classification-regression which exploits both class and structural information. Training our model is achieved by optimizing a joint objective function of pixel classification and shape regression. Shapes are represented implicitly via signed distance maps obtained directly from ground truth label maps. Thus, we can associate each image point not only with its class label, but also with its distances to object boundaries, and this at no additional cost regarding annotations. The regression component acts as spatial regularization learned from data and yields a predictor with both class and spatial consistency. In the challenging context of simultaneous multi-organ segmentation, we demonstrate the potential of our approach through experimental validation on a large dataset of 80 three-dimensional CT scans."'),
('"Joint Estimation of Motion, Structure and Geometry from Stereo Sequences"', '"ECCV 2010"', '["Fundamental Matrix", "Joint Estimation", "Epipolar Line", "Smoothness Term", "Scene Structure"]', '"https://doi.org/10.1007/978-3-642-15561-1_41"', '"We present a novel variational method for the simultaneous estimation of dense scene flow and structure from stereo sequences. In contrast to existing approaches that rely on a fully calibrated camera setup, we assume that only the intrinsic camera parameters are known. To couple the estimation of motion, structure and geometry, we propose a joint energy functional that integrates spatial and temporal information from two subsequent image pairs subject to an unknown stereo setup. We further introduce a normalisation of image and stereo constraints such that deviations from model assumptions can be interpreted in a geometrical way. Finally, we suggest a separate discontinuity-preserving regularisation to improve the accuracy. Experiments on calibrated and uncalibrated data demonstrate the excellent performance of our approach. We even outperform recent techniques for the rectified case that make explicit use of the simplified geometry."'),
('"Joint Face Alignment with Non-parametric Shape Models"', '"ECCV 2012"', '["Ground Truth", "Video Sequence", "Input Image", "Face Shape", "Local Detector"]', '"https://doi.org/10.1007/978-3-642-33712-3_4"', '"We present a joint face alignment technique that takes a set of images as input and produces a set of shape- and appearance-consistent face alignments as output. Our method is an extension of the recent localization method of Belhumeur et al. [1], which combines the output of local detectors with a non-parametric set of face shape models. We are inspired by the recent joint alignment method of Zhao et al. [20], which employs a modified Active Appearance Model (AAM) approach to align a batch of images. We introduce an approach for simultaneously optimizing both a local appearance constraint, which couples the local estimates between multiple images, and a global shape constraint, which couples landmarks and images across the image set. In video sequences, our method greatly improves the temporal stability of landmark estimates without compromising accuracy relative to ground truth."'),
('"Joint Face Alignment: Rescue Bad Alignments with Good Ones by Regularized Re-fitting"', '"ECCV 2012"', '["Facial Image", "Appearance Model", "Alignment Result", "Active Appearance Model", "Active Shape Mo', '"https://doi.org/10.1007/978-3-642-33709-3_44"', '"Nowadays, more and more applications need to jointly align a set of facial images from one specific person, which forms the so-called joint face alignment problem. To address this problem, in this paper, starting from an initial face alignment results, we propose to enhance the alignments by a fundamentally novel idea: rescuing the bad alignments with their well-aligned neighbors. In our method, a discriminative alignment evaluator is well designed to assess the initial face alignments and separate the well-aligned images from the badly-aligned ones. To correct the bad ones, a robust regularized re-fitting algorithm is proposed by exploiting the appearance consistency between the badly-aligned image and its k well-aligned nearest neighbors. Experiments conducted on faces in the wild demonstrate that our method greatly improves the initial face alignment results of an off-the-shelf facial landmark locator. In addition, the effectiveness of our method is validated through comparing with other state-of-the-art methods in joint face alignment under complex conditions."'),
('"Joint Image and Word Sense Discrimination for Image Retrieval"', '"ECCV 2012"', '["Image Retrieval", "Ranking Function", "Image Annotation", "Word Sense", "Word Sense Disambiguation', '"https://doi.org/10.1007/978-3-642-33718-5_10"', '"We study the task of learning to rank images given a text query, a problem that is complicated by the issue of multiple senses. That is, the senses of interest are typically the visually distinct concepts that a user wishes to retrieve. In this paper, we propose to learn a ranking function that optimizes the ranking cost of interest and simultaneously discovers the disambiguated senses of the query that are optimal for the supervised task. Note that no supervised information is given about the senses. Experiments performed on web images and the ImageNet dataset show that using our approach leads to a clear gain in performance."'),
('"Joint Learning for Attribute-Consistent Person Re-Identification"', '"ECCV 2014"', '["Joint Model", "Correct Match", "Pairwise Constraint", "Semantic Aspect", "Hinge Loss"]', '"https://doi.org/10.1007/978-3-319-16199-0_10"', '"Person re-identification has recently attracted a lot of attention in the computer vision community. This is in part due to the challenging nature of matching people across cameras with different viewpoints and lighting conditions, as well as across human pose variations. The literature has since devised several approaches to tackle these challenges, but the vast majority of the work has been concerned with appearance-based methods. We propose an approach that goes beyond appearance by integrating a semantic aspect into the model. We jointly learn a discriminative projection to a joint appearance-attribute subspace, effectively leveraging the interaction between attributes and appearance for matching. Our experimental results support our model and demonstrate the performance gain yielded by coupling both tasks. Our results outperform several state-of-the-art methods on VIPeR, a standard re-identification dataset. Finally, we report similar results on a new large-scale dataset we collected and labeled for our task."'),
('"Joint Object Class Sequencing and Trajectory Triangulation (JOST)"', '"ECCV 2014"', '["Discrete Cosine Transform", "Object Detection", "Object Class", "Dynamic Object", "Object Instance', '"https://doi.org/10.1007/978-3-319-10584-0_39"', '"We introduce the problem of joint object class sequencing and trajectory triangulation (JOST), which is defined as the reconstruction of the motion path of a class of dynamic objects through a scene from an unordered set of images. We leverage standard object detection techinques to identify object instances within a set of registered images. Each of these object detections defines a single 2D point with a corresponding viewing ray. The set of viewing rays attained from the aggregation of all detections belonging to a common object class is then used to estimate a motion path denoted as the object class trajectory. Our method jointly determines the topology of the objects motion path and reconstructs the 3D object points corresponding to our object detections. We pose the problem as an optimization over both the unknown 3D points and the topology of the path, which is approximated by a Generalized Minimum Spanning Tree (GMST) on a multipartite graph and then refined through a continuous optimization over the 3D object points. Experiments on synthetic and real datasets demonstrate the effectiveness of our method and the feasibility to solve a previously intractable problem."'),
('"Joint Parametric and Non-parametric Curve Evolution for Medical Image Segmentation"', '"ECCV 2008"', '["Principle Component Analysis", "Active Contour", "Curve Evolution", "Object Shape", "Statistical S', '"https://doi.org/10.1007/978-3-540-88682-2_14"', '"This paper proposes a new joint parametric and nonparametric curve evolution algorithm of the level set functions for medical image segmentation. Traditional level set algorithms employ non-parametric curve evolution for object matching. Although matching image boundaries accurately, they often suffer from local minima and generate incorrect segmentation of object shapes, especially for images with noise, occlusion and low contrast. On the other hand, statistical model-based segmentation methods allow parametric object shape variations subject to some shape prior constraints, and they are more robust in dealing with noise and low contrast. In this paper, we combine the advantages of both of these methods and jointly use parametric and non-parametric curve evolution in object matching. Our new joint curve evolution algorithm is as robust as and at the same time, yields more accurate segmentation results than the parametric methods using shape prior information. Comparative results on segmenting ventricle frontal horn and putamen shapes in MR brain images confirm both robustness and accuracy of the proposed joint curve evolution algorithm."'),
('"Joint People, Event, and Location Recognition in Personal Photo Collections Using Cross-Domain Cont', '"ECCV 2010"', '["Face Recognition", "Personal Photo", "Photo Collection", "Event Label", "Location Recognition"]', '"https://doi.org/10.1007/978-3-642-15549-9_18"', '"We present a framework for vision-assisted tagging of personal photo collections using context. Whereas previous efforts mainly focus on tagging people, we develop a unified approach to jointly tag across multiple domains (specifically people, events, and locations). The heart of our approach is a generic probabilistic model of context that couples the domains through a set of cross-domain relations. Each relation models how likely the instances in two domains are to co-occur. Based on this model, we derive an algorithm that simultaneously estimates the cross-domain relations and infers the unknown tags in a semi-supervised manner. We conducted experiments on two well-known datasets and obtained significant performance improvements in both people and location recognition. We also demonstrated the ability to infer event labels with missing timestamps (i.e. with no event features)."'),
('"Joint Semantic Segmentation and 3D Reconstruction from Monocular Video"', '"ECCV 2014"', '["Conditional Random Field", "High Order Factor", "Structure From Motion", "Semantic Label", "Condit', '"https://doi.org/10.1007/978-3-319-10599-4_45"', '"We present an approach for joint inference of 3D scene structure and semantic labeling for monocular video. Starting with monocular image stream, our framework produces a 3D volumetric semantic + occupancy map, which is much more useful than a series of 2D semantic label images or a sparse point cloud produced by traditional semantic segmentation and Structure from Motion(SfM) pipelines respectively. We derive a Conditional Random Field (CRF) model defined in the 3D space, that jointly infers the semantic category and occupancy for each voxel. Such a joint inference in the 3D CRF paves the way for more informed priors and constraints, which is otherwise not possible if solved separately in their traditional frameworks. We make use of class specific semantic cues that constrain the 3D structure in areas, where multiview constraints are weak. Our model comprises of higher order factors, which helps when the depth is unobservable.We also make use of class specific semantic cues to reduce either the degree of such higher order factors, or to approximately model them with unaries if possible. We demonstrate improved 3D structure and temporally consistent semantic segmentation for difficult, large scale, forward moving monocular image sequences."'),
('"Joint Sparsity-Based Robust Multimodal Biometrics Recognition"', '"ECCV 2012"', '["Sparse Representation", "Augmented Lagrangian Method", "Alternative Direction Method", "Biometric ', '"https://doi.org/10.1007/978-3-642-33885-4_37"', '"Traditional biometric recognition systems rely on a single biometric signature for authentication. While the advantage of using multiple sources of information for establishing the identity has been widely recognized, computational models for multimodal biometrics recognition have only recently received attention. We propose a novel multimodal multivariate sparse representation method for multimodal biometrics recognition, which represents the test data by a sparse linear combination of training data, while constraining the observations from different modalities of the test subject to share their sparse representations. Thus, we simultaneously take into account correlations as well as coupling information between biometric modalities. Furthermore, the model is modified to make it robust to noise and occlusion. The resulting optimization problem is solved using an efficient alternative direction method. Experiments on a challenging public dataset show that our method compares favorably with competing fusion-based methods."'),
('"Joint Spatio-temporal Depth Features Fusion Framework for 3D Structure Estimation in Urban Environm', '"ECCV 2012"', '["Markov Random Field", "Camera Motion", "Depth Estimation", "Bundle Adjustment", "Structure From Mo', '"https://doi.org/10.1007/978-3-642-33885-4_53"', '"We present a novel approach to improve 3D structure estimation from an image stream in urban scenes. We consider a particular setup where the camera is installed on a moving vehicle. Applying traditional structure from motion (SfM) technique in this case generates poor estimation of the 3d structure due to several reasons such as texture-less images, small baseline variations and dominant forward camera motion. Our idea is to introduce the monocular depth cues that exist in a single image, and add time constraints on the estimated 3D structure. We assume that our scene is made up of small planar patches which are obtained using over-segmentation method, and our goal is to estimate the 3D positioning for each of these planes. We propose a fusion framework that employs Markov Random Field (MRF) model to integrate both spatial and temporal depth information. An advantage of our model is that it performs well even in the absence of some depth information. Spatial depth information is obtained through a global and local feature extraction method inspired by Saxena et al. [1]. Temporal depth information is obtained via sparse optical flow based structure from motion approach. That allows decreasing the estimation ambiguity by forcing some constraints on camera motion. Finally, we apply a fusion scheme to create unique 3D structure estimation."'),
('"Joint Unsupervised Face Alignment and Behaviour Analysis"', '"ECCV 2014"', '["Face alignment", "time series alignment", "slow feature analysis"]', '"https://doi.org/10.1007/978-3-319-10593-2_12"', '"The predominant strategy for facial expressions analysis and temporal analysis of facial events is the following: a generic facial landmarks tracker, usually trained on thousands of carefully annotated examples, is applied to track the landmark points, and then analysis is performed using mostly the shape and more rarely the facial texture. This paper challenges the above framework by showing that it is feasible to perform joint landmarks localization (i.e. spatial alignment) and temporal analysis of behavioural sequence with the use of a simple face detector and a simple shape model. To do so, we propose a new component analysis technique, which we call Autoregressive Component Analysis (ARCA), and we show how the parameters of a motion model can be jointly retrieved. The method does not require the use of any sophisticated landmark tracking methodology and simply employs pixel intensities for the texture representation."'),
('"Jointly Optimizing 3D Model Fitting and Fine-Grained Classification"', '"ECCV 2014"', '["Active Shape Model", "Landmark Location", "Fisher Vector", "Pickup Truck", "Deformable Part Model"', '"https://doi.org/10.1007/978-3-319-10593-2_31"', '"3D object modeling and fine-grained classification are often treated as separate tasks. We propose to optimize 3D model fitting and fine-grained classification jointly. Detailed 3D object representations encode more information (e.g., precise part locations and viewpoint) than traditional 2D-based approaches, and can therefore improve fine-grained classification performance. Meanwhile, the predicted class label can also improve 3D model fitting accuracy, e.g., by providing more detailed class-specific shape models. We evaluate our method on a new fine-grained 3D car dataset (FG3DCar), demonstrating our method outperforms several state-of-the-art approaches. Furthermore, we also conduct a series of analyses to explore the dependence between fine-grained classification performance and 3D models."'),
('"KAZE Features"', '"ECCV 2012"', '["Feature Detection", "Scale Space", "Scale Level", "Scale Invariant Feature Transform", "Nonlinear ', '"https://doi.org/10.1007/978-3-642-33783-3_16"', '"In this paper, we introduce KAZE features, a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces. Previous approaches detect and describe features at different scale levels by building or approximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In this way, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining superior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable conductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods."'),
('"Kernel Codebooks for Scene Categorization"', '"ECCV 2008"', '["Kernel Density Estimation", "Latent Dirichlet Allocation", "Word Ambiguity", "Kernel Size", "Scene', '"https://doi.org/10.1007/978-3-540-88690-7_52"', '"This paper introduces a method for scene categorization by modeling ambiguity in the popular codebook approach. The codebook approach describes an image as a bag of discrete visual codewords, where the frequency distributions of these words are used for image categorization. There are two drawbacks to the traditional codebook model: codeword uncertainty and codeword plausibility. Both of these drawbacks stem from the hard assignment of visual features to a single codeword. We show that allowing a degree of ambiguity in assigning codewords improves categorization performance for three state-of-the-art datasets."'),
('"Kernel Conditional Ordinal Random Fields for Temporal Segmentation of Facial Action Units"', '"ECCV 2012"', '["Action units", "histogram intersection kernel", "ordinal regression", "conditional random field", ', '"https://doi.org/10.1007/978-3-642-33868-7_26"', '"We consider the problem of automated recognition of temporal segments (neutral, onset, apex and offset) of Facial Action Units. To this end, we propose the Laplacian-regularized Kernel Conditional Ordinal Random Field model. In contrast to standard modeling approaches to recognition of AUs\\u2019 temporal segments, which treat each segment as an independent class, the proposed model takes into account ordinal relations between the segments. The experimental results evidence the effectiveness of such an approach."'),
('"Kernel Feature Selection with Side Data Using a Spectral Approach"', '"ECCV 2004"', '["Feature Selection", "Facial Expression", "Weight Vector", "Face Image", "Relevant Feature"]', '"https://doi.org/10.1007/978-3-540-24672-5_4"', '"We address the problem of selecting a subset of the most relevant features from a set of sample data in cases where there are multiple (equally reasonable) solutions. In particular, this topic includes on one hand the introduction of hand-crafted kernels which emphasize certain desirable aspects of the data and, on the other hand, the suppression of one of the solutions given \\u201cside\\u201d data, i.e., when one is given information about undesired aspects of the data. Such situations often arise when there are several, even conflicting, dimensions to the data. For example, documents can be clustered based on topic, authorship or writing style; images of human faces can be clustered based on illumination conditions, facial expressions or by person identity, and so forth."'),
('"Kernel Sparse Representation for Image Classification and Face Recognition"', '"ECCV 2010"', '["Face Recognition", "Visual Word", "Reconstruction Error", "Sparse Code", "High Dimensional Feature', '"https://doi.org/10.1007/978-3-642-15561-1_1"', '"Recent research has shown the effectiveness of using sparse coding(Sc) to solve many computer vision problems. Motivated by the fact that kernel trick can capture the nonlinear similarity of features, which may reduce the feature quantization error and boost the sparse coding performance, we propose Kernel Sparse Representation(KSR). KSR is essentially the sparse coding technique in a high dimensional feature space mapped by implicit mapping function. We apply KSR to both image classification and face recognition. By incorporating KSR into Spatial Pyramid Matching(SPM), we propose KSRSPM for image classification. KSRSPM can further reduce the information loss in feature quantization step compared with Spatial Pyramid Matching using Sparse Coding(ScSPM). KSRSPM can be both regarded as the generalization of Efficient Match Kernel(EMK) and an extension of ScSPM. Compared with sparse coding, KSR can learn more discriminative sparse codes for face recognition. Extensive experimental results show that KSR outperforms sparse coding and EMK, and achieves state-of-the-art performance for image classification and face recognition on publicly available datasets."'),
('"Kernel-Predictability: A New Information Measure and Its Application to Image Registration"', '"ECCV 2006"', '["Mutual Information", "Image Registration", "Information Measure", "Registration Method", "Normaliz', '"https://doi.org/10.1007/11744078_39"', '"A new information measure for probability distributions is presented; based on it, a similarity measure between images is derived, which is used for constructing a robust image registration algorithm based on random sampling, similar to classical approaches like mutual information. It is shown that the registration method obtained with the new similarity measure shows a significantly better performance for small sampling sets; this makes it specially suited for the estimation of non-parametric deformation fields, where the estimation of the local transformation is done on small windows. This is confirmed by extensive comparisons using synthetic deformations of real images."'),
('"Kernelized Temporal Cut for Online Temporal Segmentation and Recognition"', '"ECCV 2012"', '["Action Recognition", "Spectral Cluster", "Reproduce Kernel Hilbert Space", "Rand Index", "Depth Se', '"https://doi.org/10.1007/978-3-642-33712-3_17"', '"We address the problem of unsupervised online segmenting human motion sequences into different actions. Kernelized Temporal Cut (KTC), is proposed to sequentially cut the structured sequential data into different regimes. KTC extends previous works on online change-point detection by incorporating Hilbert space embedding of distributions to handle the nonparametric and high dimensionality issues. Based on KTC, a realtime online algorithm and a hierarchical extension are proposed for detecting both action transitions and cyclic motions at the same time. We evaluate and compare the approach to state-of-the-art methods on motion capture data, depth sensor data and videos. Experimental results demonstrate the effectiveness of our approach, which yields realtime segmentation, and produces higher action segmentation accuracy. Furthermore, by combining with sequence matching algorithms, we can online recognize actions of an arbitrary person from an arbitrary viewpoint, given realtime depth sensor input."'),
('"Key Object Driven Multi-category Object Recognition, Localization and Tracking Using Spatio-tempora', '"ECCV 2008"', '["Object Recognition", "False Alarm Rate", "Interest Point", "Object Category", "Object Segmentation', '"https://doi.org/10.1007/978-3-540-88693-8_30"', '"In this paper we address the problem of recognizing, localizing and tracking multiple objects of different categories in meeting room videos. Difficulties such as lack of detail and multi-object co-occurrence make it hard to directly apply traditional object recognition methods. Under such circumstances, we show that incorporating object-level spatio-temporal relationships can lead to significant improvements in inference of object category and state. Contextual relationships are modeled by a dynamic Markov random field, in which recognition, localization and tracking are done simultaneously. Further, we define human as the key object of the scene, which can be detected relatively robustly and therefore is used to guide the inference of other objects. Experiments are done on the CHIL meeting video corpus. Performance is evaluated in terms of object detection and false alarm rates, object recognition confusion matrix and pixel-level accuracy of object segmentation."'),
('"Keyframe Selection for Camera Motion and Structure Estimation from Multiple Views"', '"ECCV 2004"', '["Feature Point", "Augmented Reality", "Camera Motion", "Camera Parameter", "Rigid Object"]', '"https://doi.org/10.1007/978-3-540-24670-1_40"', '"Estimation of camera motion and structure of rigid objects in the 3D world from multiple camera images by bundle adjustment is often performed by iterative minimization methods due to their low computational effort. These methods need a robust initialization in order to converge to the global minimum. In this paper a new criterion for keyframe selection is presented. While state of the art criteria just avoid degenerated camera motion configurations, the proposed criterion selects the keyframe pairing with the lowest expected estimation error of initial camera motion and object structure. The presented results show, that the convergence probability of bundle adjustment is significantly improved with the new criterion compared to the state of the art approaches."'),
('"Keypoint Signatures for Fast Learning and Recognition"', '"ECCV 2008"', '["Feature Point", "Recognition Rate", "Randomize Tree", "Signature Length", "Fast Learning"]', '"https://doi.org/10.1007/978-3-540-88682-2_6"', '"Statistical learning techniques have been used to dramatically speed-up keypoint matching by training a classifier to recognize a specific set of keypoints. However, the training itself is usually relatively slow and performed offline. Although methods have recently been proposed to train the classifier online, they can only learn a very limited number of new keypoints. This represents a handicap for real-time applications, such as Simultaneous Localization and Mapping (SLAM), which require incremental addition of arbitrary numbers of keypoints as they become visible."'),
('"Know Your Limits: Accuracy of Long Range Stereoscopic Object Measurements in Practice"', '"ECCV 2014"', '["Stereo Vision", "Stereo Match", "Advanced Driver Assistance System", "Scene Flow", "Inverse Compos', '"https://doi.org/10.1007/978-3-319-10605-2_7"', '"Modern applications of stereo vision, such as advanced driver assistance systems and autonomous vehicles, require highest precision when determining the location and velocity of potential obstacles. Subpixel disparity accuracy in selected image regions is therefore essential. Evaluation benchmarks for stereo correspondence algorithms, such as the popular Middlebury and KITTI frameworks, provide important reference values regarding dense matching performance, but do not sufficiently treat local sub-pixel matching accuracy. In this paper, we explore this important aspect in detail. We present a comprehensive statistical evaluation of selected state-of-the-art stereo matching approaches on an extensive dataset and establish reference values for the precision limits actually achievable in practice. For a carefully calibrated camera setup under real-world imaging conditions, a consistent error limit of 1/10 pixel is determined. We present guidelines on algorithmic choices derived from theory which turn out to be relevant to achieving this limit in practice."'),
('"Knowing a Good HOG Filter When You See It: Efficient Selection of Filters for Detection"', '"ECCV 2014"', '["Linear Discriminant Analysis", "Object Detection", "Training Time", "Average Precision", "Mean Ave', '"https://doi.org/10.1007/978-3-319-10590-1_6"', '"Collections of filters based on histograms of oriented gradients (HOG) are common for several detection methods, notably, poselets and exemplar SVMs. The main bottleneck in training such systems is the selection of a subset of good filters from a large number of possible choices. We show that one can learn a universal model of part \\u201cgoodness\\u201d based on properties that can be computed from the filter itself. The intuition is that good filters across categories exhibit common traits such as, low clutter and gradients that are spatially correlated. This allows us to quickly discard filters that are not promising thereby speeding up the training procedure. Applied to training the poselet model, our automated selection procedure allows us to improve its detection performance on the PASCAL VOC data sets, while speeding up training by an order of magnitude. Similar results are reported for exemplar SVMs."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Knowledge Based Activity Recognition with Dynamic Bayesian Network"', '"ECCV 2010"', '["Domain Knowledge", "Training Sequence", "Dynamic Bayesian Network", "Candidate Structure", "Defens', '"https://doi.org/10.1007/978-3-642-15567-3_39"', '"In this paper, we propose solutions on learning dynamic Bayesian network (DBN) with domain knowledge for human activity recognition. Different types of domain knowledge, in terms of first order probabilistic logics (FOPLs), are exploited to guide the DBN learning process. The FOPLs are transformed into two types of model priors: structure prior and parameter constraints. We present a structure learning algorithm, constrained structural EM (CSEM), on learning the model structures combining the training data with these priors. Our method successfully alleviates the common problem of lack of sufficient training data in activity recognition. The experimental results demonstrate simple logic knowledge can compensate effectively for the shortage of the training data and therefore reduce our dependencies on training data."'),
('"Kruppa Equation Revisited: Its Renormalization and Degeneracy"', '"ECCV 2000"', '["Camera self-calibration", "Kruppa equations", "renormalization", "dege-neracy", "chirality"]', '"https://doi.org/10.1007/3-540-45053-X_36"', '"In this paper, we study general questions about the solvability of the Kruppa equations and show that, in several special cases, the Kruppa equations can be renormalized and become linear. In particular, for cases when the camera motion is such that its rotation axis is parallel or perpendicular to translation, we can obtain linear algorithms for self-calibration. A further study of these cases not only reveals generic difficulties with degeneracy in conventional self-calibration methods based on the nonlinear Kruppa equations, but also clarifies some incomplete discussion in the literature about the solutions of the Kruppa equations. We demonstrate that Kruppa equations do not provide sufficient constraints on camera calibration and give a complete account of exactly what is missing in Kruppa equations. In particular, a clear relationship between the Kruppa equations and chirality is revealed. The results then resolve the discrepancy between the Kruppa equations and the necessary and sufficient condition for a unique calibration. Simulation results are presented for evaluation of the sensitivity and robustness of the proposed linear algorithms."'),
('"Labeling Images by Integrating Sparse Multiple Distance Learning and Semantic Context Modeling"', '"ECCV 2012"', '["Automatic Image Annotation", "multiple distance learning", "semantic context", "alternating optimi', '"https://doi.org/10.1007/978-3-642-33765-9_49"', '"Recent progress on Automatic Image Annotation (AIA) is achieved by either exploiting low level visual features or high level semantic context. Integrating these two paradigms to further leverage the performance of AIA is promising. However, very few previous works have studied this issue in a unified framework. In this paper, we propose a unified model based on Conditional Random Fields (CRF), which establishes tight interaction between visual features and semantic context. In particular, Kernelized Logistic Regression (KLR) with multiple visual distance learning is embedded into the CRF framework. We introduce L 1 and L 2 regularization terms into the unified learning process for the distance learning and the parameters penalty respectively. The experiments are conducted on two benchmarks: Corel and TRECVID-2005 data sets for evaluation. The experimental results show that, compared with the state-of-the-art methods, the unified model achieves significant improvement on annotation performance and shows more robustness with increasing number of various visual features."'),
('"LACBoost and FisherBoost: Optimally Building Cascade Classifiers"', '"ECCV 2010"', '["Linear Discriminant Analysis", "Quadratic Programming", "Object Detection", "Column Generation", "', '"https://doi.org/10.1007/978-3-642-15552-9_44"', '"Object detection is one of the key tasks in computer vision. The cascade framework of Viola and Jones has become the de facto standard. A classifier in each node of the cascade is required to achieve extremely high detection rates, instead of low overall classification error. Although there are a few reported methods addressing this requirement in the context of object detection, there is no a principled feature selection method that explicitly takes into account this asymmetric node learning objective. We provide such a boosting algorithm in this work. It is inspired by the linear asymmetric classifier (LAC) of [1] in that our boosting algorithm optimizes a similar cost function. The new totally-corrective boosting algorithm is implemented by the column generation technique in convex optimization. Experimental results on face detection suggest that our proposed boosting algorithms can improve the state-of-the-art methods in detection performance."'),
('"Laplacian Meshes for Monocular 3D Shape Recovery"', '"ECCV 2012"', '["Control Point", "Input Image", "Reference Image", "Regularization Term", "Reprojection Error"]', '"https://doi.org/10.1007/978-3-642-33712-3_30"', '"We show that by extending the Laplacian formalism, which was first introduced in the Graphics community to regularize 3D meshes, we can turn the monocular 3D shape reconstruction of a deformable surface given correspondences with a reference image into a well-posed problem. Furthermore, this does not require any training data and eliminates the need to pre-align the reference shape with the one to be reconstructed, as was done in earlier methods."'),
('"Large Margin Local Metric Learning"', '"ECCV 2014"', '["Dissimilarity learning", "local metric learning", "face recognition", "nearest neighbor classifica', '"https://doi.org/10.1007/978-3-319-10605-2_44"', '"Linear metric learning is a widely used methodology to learn a dissimilarity function from a set of similar/dissimilar example pairs. Using a single metric may be a too restrictive assumption when handling heterogeneous datasets. Recently, local metric learning methods have been introduced to overcome this limitation. However, they are subjects to constraints preventing their usage in many applications. For example, they require knowledge of the class label of the training points. In this paper, we present a novel local metric learning method, which overcomes some limitations of previous approaches. The method first computes a Gaussian Mixture Model from a low dimensional embedding of training data. Then it estimates a set of local metrics by solving a convex optimization problem; finally, a dissimilarity function is obtained by aggregating the local metrics. Our experiments show that the proposed method achieves state-of-the-art results on four datasets."'),
('"Large Scale Visual Geo-Localization of Images in Mountainous Terrain"', '"ECCV 2012"', '["Digital Elevation Model", "Visual Word", "Visible Horizon", "Query Image", "Iterative Close Point"', '"https://doi.org/10.1007/978-3-642-33709-3_37"', '"Given a picture taken somewhere in the world, automatic geo-localization of that image is a task that would be extremely useful e.g. for historical and forensic sciences, documentation purposes, organization of the world\\u2019s photo material and also intelligence applications. While tremendous progress has been made over the last years in visual location recognition within a single city, localization in natural environments is much more difficult, since vegetation, illumination, seasonal changes make appearance-only approaches impractical. In this work, we target mountainous terrain and use digital elevation models to extract representations for fast visual database lookup. We propose an automated approach for very large scale visual localization that can efficiently exploit visual information (contours) and geometric constraints (consistent orientation) at the same time. We validate the system on the scale of a whole country (Switzerland, 40 000km2) using a new dataset of more than 200 landscape query pictures with ground truth."'),
('"Large Vocabularies for Keypoint-Based Representation and Matching of Image Patches"', '"ECCV 2012"', '["keypoint description", "keypoint correspondences", "visual vocabulary", "near-duplicate patches", ', '"https://doi.org/10.1007/978-3-642-33863-2_23"', '"In large visual databases, detection of prospectively similar contents requires simple and robust methods. Keypoint correspondences are a popular approach which, nevertheless, cannot detect (using typical descriptions) similarities in a wider image context, e.g. detection of similar fragments. For such capabilities, the analysis of configuration constraints is needed. We propose keypoint descriptions which (by using sets of words from large vocabularies) represent semi-local characteristics of images. Thus, similar image patches (including similarly looking objects) can be preliminarily retrieved by straightforward keypoint matching. A limited-scale experimental verification is provided. The approach can be prospectively used as a simple mid-level feature matching in large and unpredictable visual databases."'),
('"Large-Lexicon Attribute-Consistent Text Recognition in Natural Images"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_54"', '"This paper proposes a new model for the task of word recognition in natural images that simultaneously models visual and lexicon consistency of words in a single probabilistic model. Our approach combines local likelihood and pairwise positional consistency priors with higher order priors that enforce consistency of characters (lexicon) and their attributes (font and colour). Unlike traditional stage-based methods, word recognition in our framework is performed by estimating the maximum a posteriori (MAP) solution under the joint posterior distribution of the model. MAP inference in our model is performed through the use of weighted finite-state transducers (WFSTs). We show how the efficiency of certain operations on WFSTs can be utilized to find the most likely word under the model in an efficient manner. We evaluate our method on a range of challenging datasets (ICDAR\\u201903, SVT, ICDAR\\u201911). Experimental results demonstrate that our method outperforms state-of-the-art methods for cropped word recognition."'),
('"Large-Scale Gaussian Process Classification with Flexible Adaptive Histogram Kernels"', '"ECCV 2012"', '["Large-scale Gaussian Processes", "Histogram Intersection Kernels", "Hyperparameter Optimization", ', '"https://doi.org/10.1007/978-3-642-33765-9_7"', '"We present how to perform exact large-scale multi-class Gaussian process classification with parameterized histogram intersection kernels. In contrast to previous approaches, we use a full Bayesian model without any sparse approximation techniques, which allows for learning in sub-quadratic and classification in constant time. To handle the additional model flexibility induced by parameterized kernels, our approach is able to optimize the parameters with large-scale training data. A key ingredient of this optimization is a new efficient upper bound of the negative Gaussian process log-likelihood. Experiments with image categorization tasks exhibit high performance gains with flexible kernels as well as learning within a few minutes and classification in microseconds for databases, where exact Gaussian process inference was not possible before."'),
('"Large-Scale Object Classification Using Label Relation Graphs"', '"ECCV 2014"', '["Object Recognition", "Categorization"]', '"https://doi.org/10.1007/978-3-319-10590-1_4"', '"In this paper we study how to perform object classification in a principled way that exploits the rich structure of real world labels. We develop a new model that allows encoding of flexible relations between labels. We introduce Hierarchy and Exclusion (HEX) graphs, a new formalism that captures semantic relations between any two labels applied to the same object: mutual exclusion, overlap and subsumption. We then provide rigorous theoretical analysis that illustrates properties of HEX graphs such as consistency, equivalence, and computational implications of the graph structure. Next, we propose a probabilistic classification model based on HEX graphs and show that it enjoys a number of desirable properties. Finally, we evaluate our method using a large-scale benchmark. Empirical results demonstrate that our model can significantly improve object classification by exploiting the label relations."'),
('"Latent Hough Transform for Object Detection"', '"ECCV 2012"', '["Training Data", "Training Image", "Object Detection", "Training Instance", "Latent Variable Model"', '"https://doi.org/10.1007/978-3-642-33712-3_23"', '"Hough transform based methods for object detection work by allowing image features to vote for the location of the object. While this representation allows for parts observed in different training instances to support a single object hypothesis, it also produces false positives by accumulating votes that are consistent in location but inconsistent in other properties like pose, color, shape or type. In this work, we propose to augment the Hough transform with latent variables in order to enforce consistency among votes. To this end, only votes that agree on the assignment of the latent variable are allowed to support a single hypothesis. For training a Latent Hough Transform (LHT) model, we propose a learning scheme that exploits the linearity of the Hough transform based methods. Our experiments on two datasets including the challenging PASCAL VOC 2007 benchmark show that our method outperforms traditional Hough transform based methods leading to state-of-the-art performance on some categories."'),
('"Latent Pose Estimator for Continuous Action Recognition"', '"ECCV 2008"', '["Action Recognition", "Hide State", "Conditional Random Field", "Informative Representation", "Huma', '"https://doi.org/10.1007/978-3-540-88688-4_31"', '"Recently, models based on conditional random fields (CRF) have produced promising results on labeling sequential data in several scientific fields. However, in the vision task of continuous action recognition, the observations of visual features have dimensions as high as hundreds or even thousands. This might pose severe difficulties on parameter estimation and even degrade the performance. To bridge the gap between the high dimensional observations and the random fields, we propose a novel model that replace the observation layer of a traditional random fields model with a latent pose estimator. In training stage, the human pose is not observed in the action data, and the latent pose estimator is learned under the supervision of the labeled action data, instead of image-to-pose data. The advantage of this model is twofold. First, it learns to convert the high dimensional observations into more compact and informative representations. Second, it enables transfer learning to fully utilize the existing knowledge and data on image-to-pose relationship. The parameters of the latent pose estimator and the random fields are jointly optimized through a gradient ascent algorithm. Our approach is tested on HumanEva [1] \\u2013 a publicly available dataset. The experiments show that our approach can improve recognition accuracy over standard CRF model and its variations. The performance can be further significantly improved by using additional image-to-pose data for training. Our experiments also show that the model trained on HumanEva can generalize to different environment and human subjects."'),
('"Latent Pyramidal Regions for Recognizing Scenes"', '"ECCV 2012"', '["Feature Vector", "Sparse Code", "Region Detector", "Image Descriptor", "Scene Recognition"]', '"https://doi.org/10.1007/978-3-642-33715-4_17"', '"In this paper we propose a simple but efficient image representation for solving the scene classification problem. Our new representation combines the benefits of spatial pyramid representation using nonlinear feature coding and latent Support Vector Machine (LSVM) to train a set of Latent Pyramidal Regions (LPR). Each of our LPRs captures a discriminative characteristic of the scenes and is trained by searching over all possible sub-windows of the images in a latent SVM training procedure. Each LPR is represented in a spatial pyramid and uses non-linear locality constraint coding for learning both shape and texture patterns of the scene. The final response of the LPRs form a single feature vector which we call the LPR representation and can be used for the classification task. We tested our proposed scene representation model in three datasets which contain a variety of scene categories (15-Scenes, UIUC-Sports and MIT-indoor). Our LPR representation obtains state-of-the-art results on all these datasets which shows that it can simultaneously model the global and local scene characteristics in a single framework and is general enough to be used for both indoor and outdoor scene classification."'),
('"Latent-Class Hough Forests for 3D Object Detection and Pose Estimation"', '"ECCV 2014"', '["Leaf Node", "Split Function", "Background Clutter", "Segmentation Mask", "Template Feature"]', '"https://doi.org/10.1007/978-3-319-10599-4_30"', '"In this paper we propose a novel framework, Latent-Class Hough Forests, for 3D object detection and pose estimation in heavily cluttered and occluded scenes. Firstly, we adapt the state-of-the-art template matching feature, LINEMOD [14], into a scale-invariant patch descriptor and integrate it into a regression forest using a novel template-based split function. In training, rather than explicitly collecting representative negative samples, our method is trained on positive samples only and we treat the class distributions at the leaf nodes as latent variables. During the inference process we iteratively update these distributions, providing accurate estimation of background clutter and foreground occlusions and thus a better detection rate. Furthermore, as a by-product, the latent class distributions can provide accurate occlusion aware segmentation masks, even in the multi-instance scenario. In addition to an existing public dataset, which contains only single-instance sequences with large amounts of clutter, we have collected a new, more challenging, dataset for multiple-instance detection containing heavy 2D and 3D clutter as well as foreground occlusions. We evaluate the Latent-Class Hough Forest on both of these datasets where we outperform state-of-the art methods."'),
('"Layer Extraction with a Bayesian Model of Shapes"', '"ECCV 2000"', '["Structure from motion", "Grouping and segmentation"]', '"https://doi.org/10.1007/3-540-45053-X_18"', '"This paper describes an automatic 3D surface modelling system that extracts dense 3D surfaces from uncalibrated video sequences. In order to extract this 3D model the scene is represented as a collection of layers and a new method for layer extraction is described. The new segmentation method differs from previous methods in that it uses a specific prior model for layer shape. A probabilistic hierarchical model of layer shape is constructed, which assigns a density function to the shape and spatial relationships between layers. This allows accurate and efficient algorithms to be used when finding the best segmentation. Here this framework is applied to architectural scenes, in which layers commonly correspond to windows or doors and hence belong to a tightly constrained family of shapes."'),
('"Leafsnap: A Computer Vision System for Automatic Plant Species Identification"', '"ECCV 2012"', '["Input Image", "Contour Point", "Leaf Image", "Computer Vision System", "Histogram Intersection"]', '"https://doi.org/10.1007/978-3-642-33709-3_36"', '"We describe the first mobile app for identifying plant species using automatic visual recognition. The system \\u2013 called Leafsnap \\u2013 identifies tree species from photographs of their leaves. Key to this system are computer vision components for discarding non-leaf images, segmenting the leaf from an untextured background, extracting features representing the curvature of the leaf\\u2019s contour over multiple scales, and identifying the species from a dataset of the 184 trees in the Northeastern United States. Our system obtains state-of-the-art performance on the real-world images from the new Leafsnap Dataset \\u2013 the largest of its kind. Throughout the paper, we document many of the practical steps needed to produce a computer vision system such as ours, which currently has nearly a million users."'),
('"Learn to Move: Activity Specific Motion Models for Tracking by Detection"', '"ECCV 2012"', '["Random Forest", "Motion Model", "Action Recognition", "Canonical Correlation Analysis", "Human Act', '"https://doi.org/10.1007/978-3-642-33885-4_19"', '"In this paper, we focus on human activity detection, which solves detection, tracking, and recognition jointly. Existing approaches typically use off-the-shelf approaches for detection and tracking, ignoring naturally given prior knowledge. Hence, in this work we present a novel strategy for learning activity specific motion models by feature-to-temporal-displacement relationships. We propose a method based on an augmented version of canonical correlation analysis (AuCCA) for linking high-dimensional features to activity-specific spatial displacements over time. We compare this continuous and discriminative approach to other well established methods in the field of activity recognition and detection. In particular, we first improve activity detections by incorporating temporal forward and backward mappings for regularization of detections. Second, we extend a particle filter framework by using activity-specific motion proposals, allowing for drastically reducing the search space. To show these improvements, we run detailed evaluations on several benchmark data sets, clearly showing the advantages of our activity-specific motion models."'),
('"Learning 2D Hand Shapes Using the Topology Preservation Model GNG"', '"ECCV 2006"', '["Input Space", "Input Pattern", "Minimum Description Length", "Active Contour Model", "Statistical ', '"https://doi.org/10.1007/11744023_25"', '"Recovering the shape of a class of objects requires establishing correct correspondences between manually or automatically annotated landmark points. In this study, we utilise a novel approach to automatically recover the shape of hand outlines from a series of 2D training images. Automated landmark extraction is accomplished through the use of the self-organising model the growing neural gas (GNG) network which is able to learn and preserve the topological relations of a given set of input patterns without requiring a priori knowledge of the structure of the input space. To measure the quality of the mapping throughout the adaptation process we use the topographic product. Results are given for the training set of hand outlines."'),
('"Learning 6D Object Pose Estimation Using 3D Object Coordinates"', '"ECCV 2014"', '["Training Image", "Object Detection", "Background Model", "Vary Lighting Condition", "Decision Fore', '"https://doi.org/10.1007/978-3-319-10605-2_35"', '"This work addresses the problem of estimating the 6D Pose of specific objects from a single RGB-D image. We present a flexible approach that can deal with generic objects, both textured and texture-less. The key new concept is a learned, intermediate representation in form of a dense 3D object coordinate labelling paired with a dense class labelling. We are able to show that for a common dataset with texture-less objects, where template-based techniques are suitable and state of the art, our approach is slightly superior in terms of accuracy. We also demonstrate the benefits of our approach, compared to template-based techniques, in terms of robustness with respect to varying lighting conditions. Towards this end, we contribute a new ground truth dataset with 10k images of 20 objects captured each under three different lighting conditions. We demonstrate that our approach scales well with the number of objects and has capabilities to run fast."'),
('"Learning a Deep Convolutional Network for Image Super-Resolution"', '"ECCV 2014"', '["Super-resolution", "deep convolutional neural networks"]', '"https://doi.org/10.1007/978-3-319-10593-2_13"', '"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage."'),
('"Learning a Fine Vocabulary"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_1"', '"A novel similarity measure for bag-of-words type large scale image retrieval is presented. The similarity function is learned in an unsupervised manner, requires no extra space over the standard bag-of-words method and is more discriminative than both L2-based soft assignment and Hamming embedding."'),
('"Learning a Sparse Representation for Object Detection"', '"ECCV 2002"', '["Training Image", "Object Detection", "Sparse Representation", "Interest Point", "Object Class"]', '"https://doi.org/10.1007/3-540-47979-1_8"', '"We present an approach for learning to detect objects in still gray images, that is based on a sparse, part-based representation of objects. A vocabulary of information-rich object parts is automatically constructed from a set of sample images of the object class of interest. Images are then represented using parts from this vocabulary, along with spatial relations observed among them. Based on this representation, a feature-efficient learning algorithm is used to learn to detect instances of the object class. The framework developed can be applied to any object with distinguishable parts in a relatively fixed spatial configuration. We report experiments on images of side views of cars. Our experiments show that the method achieves high detection accuracy on a difficult test set of real-world images, and is highly robust to partial occlusion and background variation."'),
('"Learning Action Primitives for Multi-level Video Event Understanding"', '"ECCV 2014"', '["Gaussian Mixture Model", "Video Frame", "Action Recognition", "Video Event", "Temporal Segment"]', '"https://doi.org/10.1007/978-3-319-16199-0_7"', '"Human action categories exhibit significant intra-class variation. Changes in viewpoint, human appearance, and the temporal evolution of an action confound recognition algorithms. In order to address this, we present an approach to discover action primitives, sub-categories of action classes, that allow us to model this intra-class variation. We learn action primitives and their interrelations in a multi-level spatio-temporal model for action recognition. Action primitives are discovered via a data-driven clustering approach that focuses on repeatable, discriminative sub-categories. Higher-level interactions between action primitives and the actions of a set of people present in a scene are learned. Empirical results demonstrate that these action primitives can be effectively localized, and using them to model action classes improves action recognition performance on challenging datasets."'),
('"Learning and Bayesian Shape Extraction for Object Recognition"', '"ECCV 2004"', '["Object Recognition", "Active Contour", "Image Model", "Shape Space", "Angle Function"]', '"https://doi.org/10.1007/978-3-540-24673-2_6"', '"We present a novel algorithm for extracting shapes of contours of (possibly partially occluded) objects from noisy or low-contrast images. The approach taken is Bayesian: we adopt a region-based model that incorporates prior knowledge of specific shapes of interest. To quantify this prior knowledge, we address the problem of learning probability models for collections of observed shapes. Our method is based on the geometric representation and algorithmic analysis of planar shapes introduced and developed in [15]. In contrast with the commonly used approach to active contours using partial differential equation methods [12,20,1], we model the dynamics of contours on vector fields on shape manifolds."'),
('"Learning and Incorporating Top-Down Cues in Image Segmentation"', '"ECCV 2006"', '["Image Segmentation", "Object Category", "Conditional Random Field", "Label Distribution", "Snow Fe', '"https://doi.org/10.1007/11744023_27"', '"Bottom-up approaches, which rely mainly on continuity principles, are often insufficient to form accurate segments in natural images. In order to improve performance, recent methods have begun to incorporate top-down cues, or object information, into segmentation. In this paper, we propose an approach to utilizing category-based information in segmentation, through a formulation as an image labelling problem. Our approach exploits bottom-up image cues to create an over-segmented representation of an image. The segments are then merged by assigning labels that correspond to the object category. The model is trained on a database of images, and is designed to be modular: it learns a number of image contexts, which simplify training and extend the range of object classes and image database size that the system can handle. The learning method estimates model parameters by maximizing a lower bound of the data likelihood. We examine performance on three real-world image databases, and compare our system to a standard classifier and other conditional random field approaches, as well as a bottom-up segmentation method."'),
('"Learning Artistic Lighting Template from Portrait Photographs"', '"ECCV 2010"', '["Quantile Regression", "Equal Error Rate", "Local Contrast", "Aesthetic Quality", "Lighting Usage"]', '"https://doi.org/10.1007/978-3-642-15561-1_8"', '"This paper presents a method for learning artistic portrait lighting template from a dataset of artistic and daily portrait photographs. The learned template can be used for (1) classification of artistic and daily portrait photographs, and (2) numerical aesthetic quality assessment of these photographs in lighting usage. For learning the template, we adopt Haar-like local lighting contrast features, which are then extracted from pre-defined areas on frontal faces, and selected to form a log-linear model using a stepwise feature pursuit algorithm. Our learned template corresponds well to some typical studio styles of portrait photography. With the template, the classification and assessment tasks are achieved under probability ratio test formulations. On our dataset composed of 350 artistic and 500 daily photographs, we achieve a 89.5% classification accuracy in cross-validated tests, and the assessment model assigns reasonable numerical scores based on portraits\\u2019 aesthetic quality in lighting."'),
('"Learning Brightness Transfer Functions for the Joint Recovery of Illumination Changes and Optical F', '"ECCV 2014"', '["Optical Flow", "Illumination Change", "Smoothness Term", "Constancy Assumption", "Camera Response ', '"https://doi.org/10.1007/978-3-319-10590-1_30"', '"The increasing importance of outdoor applications such as driver assistance systems or video surveillance tasks has recently triggered the development of optical flow methods that aim at performing robustly under uncontrolled illumination. Most of these methods are based on patch-based features such as the normalized cross correlation, the census transform or the rank transform. They achieve their robustness by locally discarding both absolute brightness and contrast. In this paper, we follow an alternative strategy: Instead of discarding potentially important image information, we propose a novel variational model that jointly estimates both illumination changes and optical flow. The key idea is to parametrize the illumination changes in terms of basis functions that are learned from training data. While such basis functions allow for a meaningful representation of illumination effects, they also help to distinguish real illumination changes from motion-induced brightness variations if supplemented by additional smoothness constraints. Experiments on the KITTI benchmark show the clear benefits of our approach. They do not only demonstrate that it is possible to obtain meaningful basis functions, they also show state-of-the-art results for robust optical flow estimation."'),
('"Learning Class-to-Image Distance via Large Margin and L1-Norm Regularization"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33709-3_17"', '"Image-to-Class (I2C) distance has demonstrated its effectiveness for object recognition in several single-label datasets. However, for the multi-label problem, where an image may contain several regions belonging to different classes, this distance may not work well since it cannot discriminate local features from different regions in the test image and all local features have to be counted in the I2C distance calculation. In this paper, we propose to use Class-to-Image (C2I) distance and show that this distance performs better than I2C distance for multi-label image classification. However, since the number of local features in a class is huge compared to that in an image, the calculation of C2I distance is much more expensive than I2C distance. Moreover, the label information of training images can be used to help select relevant local features for each class and further improve the recognition performance. Therefore, to make C2I distance faster and perform better, we propose an optimization algorithm using L1-norm regularization and large margin constraint to learn the C2I distance, which will not only reduce the number of local features in the class feature set, but also improve the performance of C2I distance due to the use of label information. Experiments on MSRC, Pascal VOC and MirFlickr datasets show that our method can significantly speed up the C2I distance calculation, while achieves better recognition performance than the original C2I distance and other related methods for multi-labeled datasets."'),
('"Learning Compact Visual Attributes for Large-Scale Image Classification"', '"ECCV 2012"', '["Gaussian Mixture Model", "Training Image", "Spectral Cluster", "Image Signature", "Visual Attribut', '"https://doi.org/10.1007/978-3-642-33885-4_6"', '"Attributes based image classification has received a lot of attention recently, as an interesting tool to share knowledge across different categories or to produce compact signature of images. However, when high classification performance is expected, state-of-the-art results are typically obtained by combining Fisher Vectors (FV) and Spatial Pyramid Matching (SPM), leading to image signatures with dimensionality up to 262,144 [1]. This is a hindrance to large-scale image classification tasks, for which the attribute based approaches would be more efficient. This paper proposes a new compact way to represent images, based on attributes, which allows to obtain image signatures that are typically 103 times smaller than the FV+SPM combination without significant loss of performance. The main idea lies in the definition of intermediate level representation built by learning both image and region level visual attributes. Experiments on three challenging image databases (PASCAL VOC 2007, CalTech256 and SUN-397) validate our method."'),
('"Learning Compositional Categorization Models"', '"ECCV 2006"', '["Training Image", "Local Descriptor", "Category Label", "Salient Region", "Retrieval Rate"]', '"https://doi.org/10.1007/11744078_25"', '"This contribution proposes a compositional approach to visual object categorization of scenes. Compositions are learned from the Caltech 101 database and form intermediate abstractions of images that are semantically situated between low-level representations and the high-level categorization. Salient regions, which are described by localized feature histograms, are detected as image parts. Subsequently compositions are formed as bags of parts with a locality constraint. After performing a spatial binding of compositions by means of a shape model, coupled probabilistic kernel classifiers are applied thereupon to establish the final image categorization. In contrast to the discriminative training of the categorizer, intermediate compositions are learned in a generative manner yielding relevant part agglomerations, i.e. groupings which are frequently appearing in the dataset while simultaneously supporting the discrimination between sets of categories. Consequently, compositionality simplifies the learning of a complex categorization model for complete scenes by splitting it up into simpler, sharable compositions. The architecture is evaluated on the highly challenging Caltech 101 database which exhibits large intra-category variations. Our compositional approach shows competitive retrieval rates in the range of 53.6 \\u00b1 0.88% or, with a multi-scale feature set, rates of 57.8 \\u00b1 0.79%."'),
('"Learning CRFs Using Graph Cuts"', '"ECCV 2008"', '["Ground Truth", "Loss Function", "Parameter Learning", "Submodular Function", "Foreground Region"]', '"https://doi.org/10.1007/978-3-540-88688-4_43"', '"Many computer vision problems are naturally formulated as random fields, specifically MRFs or CRFs. The introduction of graph cuts has enabled efficient and optimal inference in associative random fields, greatly advancing applications such as segmentation, stereo reconstruction and many others. However, while fast inference is now widespread, parameter learning in random fields has remained an intractable problem. This paper shows how to apply fast inference algorithms, in particular graph cuts, to learn parameters of random fields with similar efficiency. We find optimal parameter values under standard regularized objective functions that ensure good generalization. Our algorithm enables learning of many parameters in reasonable time, and we explore further speedup techniques. We also discuss extensions to non-associative and multi-class problems. We evaluate the method on image segmentation and geometry recognition."'),
('"Learning Deformations with Parallel Transport"', '"ECCV 2012"', '["Tangent Space", "Parallel Transport", "Deformation Model", "Active Appearance Model", "Standard Cl', '"https://doi.org/10.1007/978-3-642-33709-3_21"', '"Many vision problems, such as object recognition and image synthesis, are greatly impacted by deformation of objects. In this paper, we develop a deformation model based on Lie algebraic analysis. This work aims to provide a generative model that explicitly decouples deformation from appearance, which is fundamentally different from the prior work that focuses on deformation-resilient features or metrics. Specifically, the deformation group for each object can be characterized by a set of Lie algebraic basis. Such basis for different objects are related via parallel transport. Exploiting the parallel transport relations, we formulate an optimization problem, and derive an algorithm that jointly estimates the deformation basis for a class of objects, given a set of images resulted from the action of the deformations. We test the proposed model empirically on both character recognition and face synthesis."'),
('"Learning Discriminative and Shareable Features for Scene Classification"', '"ECCV 2014"', '["Feature learning", "Discriminant analysis", "Information sharing", "Scene Classificsion"]', '"https://doi.org/10.1007/978-3-319-10590-1_36"', '"In this paper, we propose to learn a discriminative and shareable feature transformation filter bank to transform local image patches (represented as raw pixel values) into features for scene image classification. The learned filters are expected to: (1) encode common visual patterns of a flexible number of categories; (2) encode discriminative and class-specific information. For each category, a subset of the filters are activated in a data-adaptive manner, meanwhile sharing of filters among different categories is also allowed. Discriminative power of the filter bank is further enhanced by enforcing the features from the same category to be close to each other in the feature space, while features from different categories to be far away from each other. The experimental results on three challenging scene image classification datasets indicate that our features can achieve very promising performance. Furthermore, our features also show great complementary effect to the state-of-the-art ConvNets feature."'),
('"Learning Discriminative Canonical Correlations for Object Recognition with Image Sets"', '"ECCV 2006"', '["Face Recognition", "Singular Value Decomposition", "Object Recognition", "Linear Discriminant Anal', '"https://doi.org/10.1007/11744078_20"', '"We address the problem of comparing sets of images for object recognition, where the sets may represent arbitrary variations in an object\\u2019s appearance due to changing camera pose and lighting conditions. The concept of Canonical Correlations (also known as principal angles) can be viewed as the angles between two subspaces. As a way of comparing sets of vectors or images, canonical correlations offer many benefits in accuracy, efficiency, and robustness compared to the classical parametric distribution-based and non-parametric sample-based methods. Here, this is demonstrated experimentally for reasonably sized data sets using existing methods exploiting canonical correlations. Motivated by their proven effectiveness, a novel discriminative learning over sets is proposed for object recognition. Specifically, inspired by classical Linear Discriminant Analysis (LDA), we develop a linear discriminant function that maximizes the canonical correlations of within-class sets and minimizes the canonical correlations of between-class sets. The proposed method significantly outperforms the state-of-the-art methods on two different object recognition problems using face image sets with arbitrary motion captured under different illuminations and image sets of five hundred general object categories taken at different views."'),
('"Learning Discriminative Spatial Relations for Detector Dictionaries: An Application to Pedestrian D', '"ECCV 2012"', '["Random Forest", "Object Detector", "Spatial Relation", "Pedestrian Detection", "Pictorial Structur', '"https://doi.org/10.1007/978-3-642-33709-3_20"', '"The recent availability of large scale training sets in conjunction with accurate classifiers (e.g., SVMs) makes it possible to build large sets of \\u201csimple\\u201d object detectors and to develop new classification approaches in which dictionaries of visual features are substituted by dictionaries of object detectors. The responses of this collection of detectors can then be used as a high-level image representation. In this work, we propose to go a step further in this direction by modeling spatial relations among different detector responses. We use Random Forests in order to discriminatively select spatial relations which represent frequent co-occurrences of detector responses. We demonstrate our idea in the specific people detection framework, which is a challenging classification task due to the variability of the human body articulations and appearance, and we use the recently proposed poselets as our basic object dictionary. The use of poselets is not the only possible, actually the proposed method can be applied more in general since few assumptions are made on the basic object detector. The results obtained show sharp improvements with respect to both the original poselet-based people detection method and to other state-of-the-art approaches on two difficult benchmark datasets."'),
('"Learning Domain Knowledge for Fa\\u00e7ade Labelling"', '"ECCV 2012"', '["Segmentation Method", "Visual Recognition", "Content Support", "Shape Grammar", "Optimal Labelling', '"https://doi.org/10.1007/978-3-642-33718-5_51"', '"This paper presents an approach to address the problem of image fa\\u00e7ade labelling. In the architectural literature, domain knowledge is usually expressed geometrically in the final design, so fa\\u00e7ade labelling should on the one hand conform to visual evidence, and on the other hand to the architectural principles \\u2013 how individual assets (e.g. doors, windows) interact with each other to form a fa\\u00e7ade as a whole. To this end, we first propose a recursive splitting method to segment fa\\u00e7ades into a bunch of tiles for semantic recognition. The segmentation improves the processing speed, guides visual recognition on suitable scales and renders the extraction of architectural principles easy. Given a set of segmented training fa\\u00e7ades with their label maps, we then identify a set of meta-features to capture both the visual evidence and the architectural principles. The features are used to train our fa\\u00e7ade labelling model. In the test stage, the features are extracted from segmented fa\\u00e7ades and the inferred label maps. The following three steps are iterated until the optimal labelling is reached: 1) proposing modifications to the current labelling; 2) extracting new features for the proposed labelling; 3) feeding the new features to the labelling model to decide whether to accept the modifications. In experiments, we evaluated our method on the ECP fa\\u00e7ade dataset and achieved higher precision than the state-of-the-art at both the pixel level and the structural level."'),
('"Learning Effective Intrinsic Features to Boost 3D-Based Face Recognition"', '"ECCV 2006"', '["Face Recognition", "Intrinsic Feature", "Range Image", "Iterative Close Point", "Mesh Node"]', '"https://doi.org/10.1007/11744047_32"', '"3D image data provide several advantages than 2D data for face recognition and overcome many problems with 2D intensity images based methods. In this paper, we propose a novel approach to 3D-based face recognition. First, a novel representation, called intrinsic features, is presented to encode local 3D shapes. It describes complementary non-relational features to provide an intrinsic representation of faces. This representation is extracted after alignment, and is invariant to translation, rotation and scale. Without reduction, tens of thousands of intrinsic features can be produced for a face, but not all of them are useful and equally important. Therefore, in the second part of the work, we introduce a learning method for learning most effective local features and combining them into a strong classifier using an AdaBoost learning procedure. Experimental results are performed on a large 3D face database obtained with complex illumination, pose and expression variations. The results demonstrate that the proposed approach produces consistently better results than existing methods."'),
('"Learning for Optical Flow Using Stochastic Optimization"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_28"', '"We present a technique for learning the parameters of a continuous-state Markov random field (MRF) model of optical flow, by minimizing the training loss for a set of ground-truth images using simultaneous perturbation stochastic approximation (SPSA). The use of SPSA to directly minimize the training loss offers several advantages over most previous work on learning MRF models for low-level vision, which instead seek to maximize the likelihood of the data given the model parameters. In particular, our approach explicitly optimizes the error criterion used to evaluate the quality of the flow field, naturally handles missing data values in the ground truth, and does not require the kinds of approximations that current methods use to address the intractable nature of maximum-likelihood estimation for such problems. We show that our method achieves state-of-the-art results and requires only a very small number of training images. We also find that our method generalizes well to unseen data, including data with quite different characteristics than the training set."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Learning from Real Images to Model Lighting Variations for Face Images"', '"ECCV 2008"', '["Face Recognition", "Face Image", "Real Image", "Basis Image", "Cast Shadow"]', '"https://doi.org/10.1007/978-3-540-88693-8_21"', '"For robust face recognition, the problem of lighting variation is considered as one of the greatest challenges. Since the nine points of light (9PL) subspace is an appropriate low-dimensional approximation to the illumination cone, it yielded good face recognition results under a wide range of difficult lighting conditions. However building the 9PL subspace for a subject requires 9 gallery images under specific lighting conditions, which are not always possible in practice. Instead, we propose a statistical model for performing face recognition under variable illumination. Through this model, the nine basis images of a face can be recovered via maximum-a-posteriori (MAP) estimation with only one gallery image of that face. Furthermore, the training procedure requires only some real images and avoids tedious processing like SVD decomposition or the use of geometric (3D) or albedo information of a surface. With the recovered nine dimensional lighting subspace, recognition experiments were performed extensively on three publicly available databases which include images under single and multiple distant point light sources. Our approach yields better results than current ones. Even under extreme lighting conditions, the estimated subspace can still represent lighting variation well. The recovered subspace retains the main characteristics of 9PL subspace. Thus, the proposed algorithm can be applied to recognition under variable lighting conditions."'),
('"Learning Graphs to Model Visual Objects across Different Depictive Styles"', '"ECCV 2014"', '["Object Recognition", "Deformable Models", "Multi-labeled Graph", "Graph Matching"]', '"https://doi.org/10.1007/978-3-319-10584-0_21"', '"Visual object classification and detection are major problems in contemporary computer vision. State-of-art algorithms allow thousands of visual objects to be learned and recognized, under a wide range of variations including lighting changes, occlusion, point of view and different object instances. Only a small fraction of the literature addresses the problem of variation in depictive styles (photographs, drawings, paintings etc.). This is a challenging gap but the ability to process images of all depictive styles and not just photographs has potential value across many applications. In this paper we model visual classes using a graph with multiple labels on each node; weights on arcs and nodes indicate relative importance (salience) to the object description. Visual class models can be learned from examples from a database that contains photographs, drawings, paintings etc. Experiments show that our representation is able to improve upon Deformable Part Models for detection and Bag of Words models for classification."'),
('"Learning High-Level Judgments of Urban Perception"', '"ECCV 2014"', '["Perceptual Characteristic", "Perceptual Judgment", "Scene Recognition", "Perceptual Score", "Fishe', '"https://doi.org/10.1007/978-3-319-10599-4_32"', '"Human observers make a variety of perceptual inferences about pictures of places based on prior knowledge and experience. In this paper we apply computational vision techniques to the task of predicting the perceptual characteristics of places by leveraging recent work on visual features along with a geo-tagged dataset of images associated with crowd-sourced urban perception judgments for wealth, uniqueness, and safety. We perform extensive evaluations of our models, training and testing on images of the same city as well as training and testing on images of different cities to demonstrate generalizability. In addition, we collect a new densely sampled dataset of streetview images for 4 cities and explore joint models to collectively predict perceptual judgments at city scale. Finally, we show that our predictions correlate well with ground truth statistics of wealth and crime."'),
('"Learning Human Interaction by Interactive Phrases"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_22"', '"In this paper, we present a novel approach for human interaction recognition from videos. We introduce high-level descriptions called interactive phrases to express binary semantic motion relationships between interacting people. Interactive phrases naturally exploit human knowledge to describe interactions and allow us to construct a more descriptive model for recognizing human interactions. We propose a novel hierarchical model to encode interactive phrases based on the latent SVM framework where interactive phrases are treated as latent variables. The interdependencies between interactive phrases are explicitly captured in the model to deal with motion ambiguity and partial occlusion in interactions. We evaluate our method on a newly collected BIT-Interaction dataset and UT-Interaction dataset. Promising results demonstrate the effectiveness of the proposed method."'),
('"Learning Hybrid Part Filters for Scene Recognition"', '"ECCV 2012"', '["Object Class", "Object Part", "Scene Recognition", "Hybrid Filter", "Spatial Pyramid"]', '"https://doi.org/10.1007/978-3-642-33715-4_13"', '"This paper introduces a new image representation for scene recognition, where an image is described based on the response maps of object part filters. The part filters are learned from existing datasets with object location annotations, using deformable part-based models trained by latent SVM [1]. Since different objects may contain similar parts, we describe a method that uses a semantic hierarchy to automatically determine and merge filters shared by multiple objects. The merged hybrid filters are then applied to new images. Our proposed representation, called Hybrid-Parts, is generated by pooling the response maps of the hybrid filters. Contrast to previous scene recognition approaches that adopted object-level detections as feature inputs, we harness filter responses of object parts, which enable a richer and finer-grained representation. The use of the hybrid filters is important towards a more compact representation, compared to directly using all the original part filters. Through extensive experiments on several scene recognition benchmarks, we demonstrate that Hybrid-Parts outperforms recent state-of-the-arts, and combining it with standard low-level features such as the GIST descriptor can lead to further improvements."'),
('"Learning Implicit Transfer for Person Re-identification"', '"ECCV 2012"', '["Binary Relation", "True Match", "Positive Pair", "Negative Pair", "Cumulative Match Characteristic', '"https://doi.org/10.1007/978-3-642-33863-2_38"', '"This paper proposes a novel approach for pedestrian re-identification. Previous re-identification methods use one of 3 approaches: invariant features; designing metrics that aim to bring instances of shared identities close to one another and instances of different identities far from one another; or learning a transformation from the appearance in one domain to the other. Our implicit approach models camera transfer by a binary relation R\\u2009=\\u2009{(x,y)|x and y describe the same person seen from cameras A and B respectively}. This solution implies that the camera transfer function is a multi-valued mapping and not a single-valued transformation, and does not assume the existence of a metric with desirable properties. We present an algorithm that follows this approach and achieves new state-of-the-art performance."'),
('"Learning Intrinsic Video Content Using Levenshtein Distance in Graph Partitioning"', '"ECCV 2002"', '["Automatic Model Order Selection", "Dynamic Time Warping", "Graph-Partitioning", "Modelling Video C', '"https://doi.org/10.1007/3-540-47979-1_45"', '"We present a novel approach for automatically learning models of temporal trajectories extracted from video data. Instead of using a representation of linearly time-normalised vectors of fixed-length, our approach makes use of Dynamic Time Warp distance as a similarity measure to capture the underlying ordered structure of variable-length temporal data while removing the non-linear warping of the time scale. We reformulate the structure learning problem as an optimal graph-partitioning of the dataset to solely exploit Dynamic Time Warp similarity weights without the need for intermediate cluster centroid representations. We extend the graph partitioning method and in particular, the Normalised Cut model originally introduced for static image segmentation to unsupervised clustering of temporal trajectories with fully automated model order selection. By computing hierarchical average Dynamic Time Warp for each cluster, we learn warp-free trajectory models and recover the time warp profiles and structural variance in the data. We demonstrate the approach on modelling trajectories of continuous hand-gestures and moving objects in an indoor environment."'),
('"Learning Invariant Feature Hierarchies"', '"ECCV 2012"', '["Visual Cortex", "Sparse Code", "Neural Information Processing System", "Restricted Boltzmann Machi', '"https://doi.org/10.1007/978-3-642-33863-2_51"', '"Fast visual recognition in the mammalian cortex seems to be a hierarchical process by which the representation of the visual world is transformed in multiple stages from low-level retinotopic features to high-level, global and invariant features, and to object categories. Every single step in this hierarchy seems to be subject to learning. How does the visual cortex learn such hierarchical representations by just looking at the world? How could computers learn such representations from data? Computer vision models that are weakly inspired by the visual cortex will be described. A number of unsupervised learning algorithms to train these models will be presented, which are based on the sparse auto-encoder concept. The effectiveness of these algorithms for learning invariant feature hierarchies will be demonstrated with a number of practical tasks such as scene parsing, pedestrian detection, and object classification."'),
('"Learning Latent Constituents for Recognition of Group Activities in Video"', '"ECCV 2014"', '["Group Activity Recognition", "Latent Parts", "Multiple-Instance Learning", "Functional Grouping", ', '"https://doi.org/10.1007/978-3-319-10590-1_3"', '"The collective activity of a group of persons is more than a mere sum of individual person actions, since interactions and the context of the overall group behavior have crucial influence. Consequently, the current standard paradigm for group activity recognition is to model the spatiotemporal pattern of individual person bounding boxes and their interactions. Despite this trend towards increasingly global representations, activities are often defined by semi-local characteristics and their interrelation between different persons. For capturing the large visual variability with small semi-local parts, a large number of them are required, thus rendering manual annotation infeasible. To automatically learn activity constituents that are meaningful for the collective activity, we sample local parts and group related ones not merely based on visual similarity but based on the function they fulfill on a set of validation images. Then max-margin multiple instance learning is employed to jointly i) remove clutter from these groups and focus on only the relevant samples, ii) learn the activity constituents, and iii) train the multi-class activity classifier. Experiments on standard activity benchmark sets show the advantage of this joint procedure and demonstrate the benefit of functionally grouped latent activity constituents for group activity recognition."'),
('"Learning Mixtures of Weighted Tree-Unions by Minimizing Description Length"', '"ECCV 2004"', '["Sample Tree", "Tree Union", "Edit Distance", "Shape Class", "Node Probability"]', '"https://doi.org/10.1007/978-3-540-24672-5_2"', '"This paper focuses on how to perform the unsupervised clustering of tree structures in an information theoretic setting. We pose the problem of clustering as that of locating a series of archetypes that can be used to represent the variations in tree structure present in the training sample. The archetypes are tree-unions that are formed by merging sets of sample trees, and are attributed with probabilities that measure the node frequency or weight in the training sample. The approach is designed to operate when the correspondences between nodes are unknown and must be inferred as part of the learning process. We show how the tree merging process can be posed as the minimisation of an information theoretic minimum descriptor length criterion. We illustrate the utility of the resulting algorithm on the problem of classifying 2D shapes using a shock graph representation."'),
('"Learning Montages of Transformed Latent Images as Representations of Objects That Change in Appeara', '"ECCV 2002"', '["Bayesian Network", "Latent Image", "Expectation Maximization Algorithm", "Coarse Scale", "Montage ', '"https://doi.org/10.1007/3-540-47979-1_48"', '"This paper introduces a novel probabilistic model for representing objects that change in appearance as a result of changes in pose, due to small deformations of their sub-parts and the relative spatial transformation of sub-parts of the object. We call the model a probabilistic montage. The model is based upon the idea that an image can be represented as a montage using many, small transformed and cropped patches from a collection of latent images. The approach is similar to that which might be employed by a police artist who might represent an image of a criminal suspect\\u2019s face using a montage of face parts cut out of a \\u201dlibrary\\u201d of face parts. In contrast, for our model, we learn the library of small latent images from a set of examples of objects that are changing in shape. In our approach, first the image is divided into a grid of sub-images. Each sub-image in the grid acts as window that crops a piece out of one of a collection of slightly larger images possible for that location in the image. We illustrate various probability models that can be used to encode the appropriate relationships for latent images and cropping transformations among the different patches. In this paper we present the complete algorithm for a tree-structured model. We show how the approach and model are able to find representations of the appearance of full body images of people in motion. We show how our approach can be used to learn representations of objects in an \\u201dunsupervised\\u201d manner and present results using our model for recognition and tracking purposes in a \\u201dsupervised\\u201d manner."'),
('"Learning Nonlinear Manifolds from Time Series"', '"ECCV 2006"', '["Time Series Data", "Object Tracking", "Inference Algorithm", "Dynamic Bayesian Network", "Dynamic ', '"https://doi.org/10.1007/11744047_19"', '"There has been growing interest in developing nonlinear dimensionality reduction algorithms for vision applications. Although progress has been made in recent years, conventional nonlinear dimensionality reduction algorithms have been designed to deal with stationary, or independent and identically distributed data. In this paper, we present a novel method that learns nonlinear mapping from time series data to their intrinsic coordinates on the underlying manifold. Our work extends the recent advances in learning nonlinear manifolds within a global coordinate system to account for temporal correlation inherent in sequential data. We formulate the problem with a dynamic Bayesian network and propose an approximate algorithm to tackle the learning and inference problems. Numerous experiments demonstrate the proposed method is able to learn nonlinear manifolds from time series data, and as a result of exploiting the temporal correlation, achieve superior results."'),
('"Learning Optical Flow"', '"ECCV 2008"', '["Ground Truth", "Optical Flow", "Data Term", "Spatial Term", "Brightness Constancy"]', '"https://doi.org/10.1007/978-3-540-88690-7_7"', '"Assumptions of brightness constancy and spatial smoothness underlie most optical flow estimation methods. In contrast to standard heuristic formulations, we learn a statistical model of both brightness constancy error and the spatial properties of optical flow using image sequences with associated ground truth flow fields. The result is a complete probabilistic model of optical flow. Specifically, the ground truth enables us to model how the assumption of brightness constancy is violated in naturalistic sequences, resulting in a probabilistic model of \\u201cbrightness inconstancy\\u201d. We also generalize previous high-order constancy assumptions, such as gradient constancy, by modeling the constancy of responses to various linear filters in a high-order random field framework. These filters are free variables that can be learned from training data. Additionally we study the spatial structure of the optical flow and how motion boundaries are related to image intensity boundaries. Spatial smoothness is modeled using a Steerable Random Field, where spatial derivatives of the optical flow are steered by the image brightness structure. These models provide a statistical motivation for previous methods and enable the learning of all parameters from training data. All proposed models are quantitatively compared on the Middlebury flow dataset."'),
('"Learning Outdoor Color Classification from Just One Training Image"', '"ECCV 2004"', '["Expectation Maximization", "Training Image", "Correct Match", "Conditional Likelihood", "Outdoor S', '"https://doi.org/10.1007/978-3-540-24673-2_33"', '"We present an algorithm for color classification with explicit illuminant estimation and compensation. A Gaussian classifier is trained with color samples from just one training image. Then, using a simple diagonal illumination model, the illuminants in a new scene that contains some of the same surface classes are estimated in a Maximum Likelihood framework using the Expectation Maximization algorithm. We also show how to impose priors on the illuminants, effectively computing a Maximum-A-Posteriori estimation. Experimental results show the excellent performances of our classification algorithm for outdoor images."'),
('"Learning over Multiple Temporal Scales in Image Databases"', '"ECCV 2000"', '["Discrete Cosine Transform", "Retrieval System", "Query Image", "Relevance Feedback", "Image Class"', '"https://doi.org/10.1007/3-540-45054-8_3"', '"The ability to learn from user interaction is an important asset for content-based image retrieval (CBIR) systems. Over short times scales, it enables the integration of information from successive queries assuring faster convergence to the desired target images. Over long time scales (retrieval sessions) it allows the retrieval system to tailor itself to the preferences of particular users. We address the issue of learning by formulating retrieval as a problem of Bayesian inference. The new formulation is shown to have various advantages over previous approaches: it leads to the minimization of the probability of retrieval error, enables region-based queries without prior image segmentation, and suggests elegant procedures for combining multiple user specifications. As a consequence of all this, it enables the design of short and long-term learning mechanisms that are simple, intuitive, and extremely efficient in terms of computational and storage requirements. We introduce two such algorithms and present experimental evidence illustrating the clear advantages of learning for CBIR."'),
('"Learning Pain from Emotion: Transferred HoT Data Representation for Pain Intensity Estimation"', '"ECCV 2014"', '["Histograms of Topographical features (HoT)", "Spectral regression", "Transfer learning", "Pain int', '"https://doi.org/10.1007/978-3-319-16199-0_54"', '"Automatic monitoring for the assessment of pain can significantly improve the psychological comfort of patients. Recently introduced databases with expert annotation opened the way for pain intensity estimation from facial analysis. In this contribution, pivotal face elements are identified using the Histograms of Topographical features (HoT) which are a generalization of the topographical primal sketch. In order to improve the discrimination between different pain intensity values and respectively the generalization with respect to the monitored persons, we transfer data representation from the emotion oriented Cohn-Kanade database to the UNBC McMaster Shoulder Pain database."'),
('"Learning PDEs for Image Restoration via Optimal Control"', '"ECCV 2010"', '["Optimal Control Problem", "Image Restoration", "Noisy Image", "Image Denoising", "Total Variation ', '"https://doi.org/10.1007/978-3-642-15549-9_9"', '"Partial differential equations (PDEs) have been successfully applied to many computer vision and image processing problems. However, designing PDEs requires high mathematical skills and good insight into the problems. In this paper, we show that the design of PDEs could be made easier by borrowing the learning strategy from machine learning. In our learning-based PDE (L-PDE) framework for image restoration, there are two terms in our PDE model: (i) a regularizer which encodes the prior knowledge of the image model and (ii) a linear combination of differential invariants, which is data-driven and can effectively adapt to different problems and complex conditions. The L-PDE is learnt from some input/output pairs of training samples via an optimal control technique. The effectiveness of our L-PDE framework for image restoration is demonstrated with two exemplary applications: image denoising and inpainting, where the PDEs are obtained easily and the produced results are comparable to or better than those of traditional PDEs, which were elaborately designed."'),
('"Learning Pre-attentive Driving Behaviour from Holistic Visual Features"', '"ECCV 2010"', '["Visual Scene", "Weak Learner", "Action Prediction", "Visual Context", "Human Driver"]', '"https://doi.org/10.1007/978-3-642-15567-3_12"', '"The aim of this paper is to learn driving behaviour by associating the actions recorded from a human driver with pre-attentive visual input, implemented using holistic image features (GIST). All images are labelled according to a number of driving\\u2013relevant contextual classes (eg, road type, junction) and the driver\\u2019s actions (eg, braking, accelerating, steering) are recorded. The association between visual context and the driving data is learnt by Boosting decision stumps, that serve as input dimension selectors. Moreover, we propose a novel formulation of GIST features that lead to an improved performance for action prediction. The areas of the visual scenes that contribute to activation or inhibition of the predictors is shown by drawing activation maps for all learnt actions. We show good performance not only for detecting driving\\u2013relevant contextual labels, but also for predicting the driver\\u2019s actions. The classifier\\u2019s false positives and the associated activation maps can be used to focus attention and further learning on the uncommon and difficult situations."'),
('"Learning Relations among Movie Characters: A Social Network Perspective"', '"ECCV 2010"', '["Social Network", "Support Vector Regression", "Social Network Analysis", "Learn Relation", "Eigenv', '"https://doi.org/10.1007/978-3-642-15561-1_30"', '"If you have ever watched movies or television shows, you know how easy it is to tell the good characters from the bad ones. Little, however, is known \\u201cwhether\\u201d or \\u201chow\\u201d computers can achieve such high-level understanding of movies. In this paper, we take the first step towards learning the relations among movie characters using visual and auditory cues. Specifically, we use support vector regression to estimate local characterization of adverseness at the scene level. Such local properties are then synthesized via statistical learning based on Gaussian processes to derive the affinity between the movie characters. Once the affinity is learned, we perform social network analysis to find communities of characters and identify the leader of each community. We experimentally demonstrate that the relations among characters can be determined with reasonable accuracy from the movie content."'),
('"Learning Rich Features from RGB-D Images for Object Detection and Segmentation"', '"ECCV 2014"', '["RGB-D perception", "object detection", "object segmentation"]', '"https://doi.org/10.1007/978-3-319-10584-0_23"', '"In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3%, which is a 56% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics."'),
('"Learning Semantic Scene Models by Trajectory Analysis"', '"ECCV 2006"', '["Spectral Cluster", "Trajectory Analysis", "Local Path", "Scene Model", "Trajectory Cluster"]', '"https://doi.org/10.1007/11744078_9"', '"In this paper, we describe an unsupervised learning framework to segment a scene into semantic regions and to build semantic scene models from long-term observations of moving objects in the scene. First, we introduce two novel similarity measures for comparing trajectories in far-field visual surveillance. The measures simultaneously compare the spatial distribution of trajectories and other attributes, such as velocity and object size, along the trajectories. They also provide a comparison confidence measure which indicates how well the measured image-based similarity approximates true physical similarity. We also introduce novel clustering algorithms which use both similarity and comparison confidence. Based on the proposed similarity measures and clustering methods, a framework to learn semantic scene models by trajectory analysis is developed. Trajectories are first clustered into vehicles and pedestrians, and then further grouped based on spatial and velocity distributions. Different trajectory clusters represent different activities. The geometric and statistical models of structures in the scene, such as roads, walk paths, sources and sinks, are automatically learned from the trajectory clusters. Abnormal activities are detected using the semantic scene models. The system is robust to low-level tracking errors."'),
('"Learning Shape Detector by Quantizing Curve Segments with Multiple Distance Metrics"', '"ECCV 2010"', '["Object Detection", "Shape Model", "Geodesic Distance", "Shape Descriptor", "Distance Metrics"]', '"https://doi.org/10.1007/978-3-642-15558-1_25"', '"In this paper, we propose a very efficient method to learn shape models using local curve segments with multiple types of distance metrics. Our learning approach includes two key steps: feature generation and model pursuit. In the first step, for each category, we first extract a massive number of local \\u201cprototype\\u201d curve segments from a few roughly aligned shape instances. Then we quantize these curve segments with three types of distance metrics corresponding to different shape deformations. In each metric space, the quantized curve segments are further grown (spanned) into a large number of ball-like manifolds, and each of them represents a equivalence class of shape variance. In the second step of shape model pursuit, using these manifolds as features, we propose a fast greedy learning algorithm based on the information projection principle. The algorithm is guided by a generative model, and stepwise selects the features that have maximum information gain. The advantage of the proposed method is identified on several public datasets and summarized as follows. (1) Our models consisting of local curve segments with multiple distance metrics are robust to the various shape deformations, and thus enable us to perform robust shape classification and detect shapes against background clutter. (2) The auto-generated curve-based features are very general and convenient, rather than designing specific features for each category."'),
('"Learning Shape from Defocus"', '"ECCV 2002"', '["Input Image", "Singular Value Decomposition", "Real Image", "Depth Level", "Blind Deconvolution"]', '"https://doi.org/10.1007/3-540-47967-8_49"', '"We present a novel method for inferring three-dimensional shape from a collection of defocused images. It is based on the observation that defocused images are the null-space of certain linear operators that depend on the three-dimensional shape of the scene as well as on the optics of the camera. Unlike most current work based on inverting the imaging model to recover the \\u201cdeblurred\\u201d image and the shape of the scene, we approach the problem from a new angle by collecting a number of deblurred images, and estimating the operator that spans their left null space directly. This is done using a singular value decomposition. Since the operator depends on the depth of the scene, we repeat the procedure for a number of different depths. Once this is done, depth can be recovered in real time: the new image is projected onto each null-space, and the depth that results in the smallest residual is chosen. The most salient feature of this algorithm is its robustness: not only can one learn the operators with one camera and then use them to successfully retrieve depth from images taken with another camera, but one can even learn the operators from simulated images, and use them to retrieve depth from real images. Thus we train the system with synthetic patterns, and then use it on real data without knowledge of the optics of the camera. Another attractive feature is that the algorithm does not rely on a discretization or an approximation of the radiance of the scene (the \\u201cdeblurred\\u201d image). In fact, the operator we recover is finite-dimensional, but it arises as the orthogonal projector of a semi-infinite operator that maps square-integrable radiance distributions onto images. Thus, the radiance is never approximated or represented via a finite set of filters. Instead, the rank of the operator learned from real data provides an estimate of the intrinsic dimensionality of the radiance distribution of real images. The algorithm is optimal in the sense of \\\\( \\\\mathcal{L}^2 \\\\) and can be implemented in real time."'),
('"Learning Shape Segmentation Using Constrained Spectral Clustering and Probabilistic Label Transfer"', '"ECCV 2010"', '["Spectral Cluster", "Cluster Label", "Pairwise Constraint", "Spectral Cluster Algorithm", "Weighted', '"https://doi.org/10.1007/978-3-642-15555-0_54"', '"We propose a spectral learning approach to shape segmentation. The method is composed of a constrained spectral clustering algorithm that is used to supervise the segmentation of a shape from a training data set, followed by a probabilistic label transfer algorithm that is used to match two shapes and to transfer cluster labels from a training-shape to a test-shape. The novelty resides both in the use of the Laplacian embedding to propagate must-link and cannot-link constraints, and in the segmentation algorithm which is based on a learn, align, transfer, and classify paradigm. We compare the results obtained with our method with other constrained spectral clustering methods and we assess its performance based on ground-truth data."'),
('"Learning Similarity for Texture Image Retrieval"', '"ECCV 2000"', '["Image indexing", "learning pattern similarity", "boundary distance metric", "support vector machin', '"https://doi.org/10.1007/3-540-45054-8_12"', '"A novel algorithm is proposed to learn pattern similarities for texture image retrieval. Similar patterns in different texture classes are grouped into a cluster in the feature space. Each cluster is isolated from others by an enclosed boundary, which is represented by several support vectors and their weights obtained from a statistical learning algorithm called support vector machine (SVM). The signed distance of a pattern to the boundary is used to measure its similarity. Furthermore, the patterns of different classes within each cluster are separated by several sub-boundaries, which are also learned by the SVMs. The signed distances of the similar patterns to a particular sub-boundary associated with the query image are used for ranking these patterns. Experimental results on the Brodatz texture database indicate that the new method performs significantly better than the traditional Euclidean distance based approach."'),
('"Learning Skeleton Stream Patterns with Slow Feature Analysis for Action Recognition"', '"ECCV 2014"', '["Action recognition", "Skeleton", "Joint stream", "Multi-order streams", "Slow feature analysis"]', '"https://doi.org/10.1007/978-3-319-16199-0_8"', '"Previous studies on MoCap (Motion Capturing (MoCap) System tracks the key points which are marked with conspicuous color or other materials (such as LED lights). The motion sequences are collected into MoCap action datasets, e.g., 1973 [3] and CMU [4] MoCap action datasets.) action data suggest that skeleton joint streams contain sufficient intrinsic information for understanding human body actions. With the advancement in depth sensors, e.g., Kinect, pose estimation with depth image provides more available realistic skeleton stream data. However, the locations of joints are always unstable due to noises. Moreover, as the estimated skeletons of different persons are not the same, the variance of intra-class is large. In this paper, we first expand the coordinate stream of each joint into multi-order streams by fusing hierarchical global information to improve the stability of joint streams. Then, Slow Feature Analysis is applied to learn the visual pattern of each joint, and the high-level information in the learnt general patterns is encoded into each skeleton to reduce the intra-variance of the skeletons. Temporal pyramid of posture word histograms is used to describe the global temporal information of action sequence. Our approach is verified with Support Vector Machine (SVM) classifier on MSR Action3D dataset, and the experimental results demonstrate that our approach achieves the state-of-the-art level."'),
('"Learning Spatial Context: Using Stuff to Find Things"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_4"', '"The sliding window approach of detecting rigid objects (such as cars) is predicated on the belief that the object can be identified from the appearance in a small region around the object. Other types of objects of amorphous spatial extent (e.g., trees, sky), however, are more naturally classified based on texture or color. In this paper, we seek to combine recognition of these two types of objects into a system that leverages \\u201ccontext\\u201d toward improving detection. In particular, we cluster image regions based on their ability to serve as context for the detection of objects. Rather than providing an explicit training set with region labels, our method automatically groups regions based on both their appearance and their relationships to the detections in the image. We show that our things and stuff (TAS) context model produces meaningful clusters that are readily interpretable, and helps improve our detection ability over state-of-the-art detectors. We also present a method for learning the active set of relationships for a particular dataset. We present results on object detection in images from the PASCAL VOC 2005/2006 datasets and on the task of overhead car detection in satellite images, demonstrating significant improvements over state-of-the-art detectors."'),
('"Learning Spatially-Smooth Mappings in Non-Rigid Structure From Motion"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33765-9_19"', '"Non-rigid structure from motion (NRSFM) is a classical underconstrained problem in computer vision. A common approach to make NRSFM more tractable is to constrain 3D shape deformation to be smooth over time. This constraint has been used to compress the deformation model and reduce the number of unknowns that are estimated. However, temporal smoothness cannot be enforced when the data lacks temporal ordering and its benefits are less evident when objects undergo abrupt deformations. This paper proposes a new NRSFM method that addresses these problems by considering deformations as spatial variations in shape space and then enforcing spatial, rather than temporal, smoothness. This is done by modeling each 3D shape coefficient as a function of its input 2D shape. This mapping is learned in the feature space of a rotation invariant kernel, where spatial smoothness is intrinsically defined by the mapping function. As a result, our model represents shape variations compactly using custom-built coefficient bases learned from the input data, rather than a pre-specified set such as the Discrete Cosine Transform. The resulting kernel-based mapping is a by-product of the NRSFM solution and leads to another fundamental advantage of our approach: for a newly observed 2D shape, its 3D shape is recovered by simply evaluating the learned function."'),
('"Learning Spatio-Temporal Features for Action Recognition with Modified Hidden Conditional Random Fi', '"ECCV 2014"', '["Human action recognition", "Spatio-temporal features", "HCRF", "Changeable spatial-temporal constr', '"https://doi.org/10.1007/978-3-319-16178-5_55"', '"Previous work on human action analysis mainly focuses on designing hand-crafted local features and combining their context information. In this paper, we propose using supervised feature learning as a way to learn spatio-temporal features. More specifically, a modified hidden conditional random field is applied to learn two high-level features conditioned on a certain action label. Among them, the individual features can describe the appearance of local parts and the interaction features can capture their spatial constraints. In order to make the best of what have been learned, a new categorization model is proposed for action matching. It is inspired by the Deformable Part Model and the intuition is that actions can be modeled by local features in a changeable spatial and temporal dependency. Experimental result shows that our algorithm can successfully recognize human actions with high accuracies both on the simple atomic action database (KTH and Weizmann) and complex interaction activity database (CASIA)."'),
('"Learning the Face Prior for Bayesian Face Recognition"', '"ECCV 2014"', '["Face Recognition", "Face Image", "Observation Space", "Fisher Vector", "Probabilistic Linear Discr', '"https://doi.org/10.1007/978-3-319-10593-2_9"', '"For the traditional Bayesian face recognition methods, a simple prior on face representation cannot cover large variations in facial poses, illuminations, expressions, aging, and occlusions in the wild. In this paper, we propose a new approach to learn the face prior for Bayesian face recognition. First, we extend Manifold Relevance Determination to learn the identity subspace for each individual automatically. Based on the structure of the learned identity subspaces, we then propose to estimate Gaussian mixture densities in the observation space with Gaussian process regression. During the training of our approach, the leave-set-out algorithm is also developed for overfitting avoidance. On extensive experimental evaluations, the learned face prior can improve the performance of the traditional Bayesian face and other related methods significantly. It is also proved that the simple Bayesian face method with the learned face prior can handle the complex intra-personal variations such as large poses and large occlusions. Experiments on the challenging LFW benchmark shows that our algorithm outperforms most of the state-of-art methods."'),
('"Learning the Topology of Object Views"', '"ECCV 2002"', '["Object recognition", "pose estimation", "view sphere", "correspondence maps", "learning"]', '"https://doi.org/10.1007/3-540-47979-1_50"', '"A visual representation of an object must meet at least three basic requirements. First, it must allow identification of the object in the presence of slight but unpredictable changes in its visual appearance. Second, it must account for larger changes in appearance due to variations in the object\\u2019s fundamental degrees of freedom, such as, e.g., changes in pose. And last, any object representation must be derivable from visual input alone, i.e., it must be learnable."'),
('"Learning to Combine Bottom-Up and Top-Down Segmentation"', '"ECCV 2006"', '["Conditional Random Field", "Image Fragment", "Ground Truth Segmentation", "Marginal Vector", "Gene', '"https://doi.org/10.1007/11744085_45"', '"Bottom-up segmentation based only on low-level cues is a notoriously difficult problem. This difficulty has lead to recent top-down segmentation algorithms that are based on class-specific image information. Despite the success of top-down algorithms, they often give coarse segmentations that can be significantly refined using low-level cues. This raises the question of how to combine both top-down and bottom-up cues in a principled manner."'),
('"Learning to Detect Objects of Many Classes Using Binary Classifiers"', '"ECCV 2006"', '["Test Image", "Class Label", "Training Image", "Face Detection", "Positive Instance"]', '"https://doi.org/10.1007/11744023_28"', '"Viola and Jones [VJ] demonstrate that cascade classification methods can successfully detect objects belonging to a single class, such as faces. Detecting and identifying objects that belong to any of a set of \\u201cclasses\\u201d, many class detection, is a much more challenging problem. We show that objects from each class can form a \\u201ccluster\\u201d in a \\u201cclassifier space\\u201d and illustrate examples of such clusters using images of real world objects. Our detection algorithm uses a \\u201cdecision tree classifier\\u201d (whose internal nodes each correspond to a VJ classifier) to propose a class label for every sub-image W of a test image (or reject it as a negative instance). If this W reaches a leaf of this tree, we then pass W through a subsequent VJ cascade of classifiers, specific to the identified class, to determine whether W is truly an instance of the proposed class. We perform several empirical studies to compare our system for detecting objects of any of M classes, to the obvious approach of running a set of M learned VJ cascade classifiers, one for each class of objects, on the same image. We found that the detection rates are comparable, and our many-class detection system is about as fast as running a single VJ cascade, and scales up well as the number of classes increases."'),
('"Learning to Detect Roads in High-Resolution Aerial Images"', '"ECCV 2010"', '["Road Network", "Hide Unit", "Aerial Image", "Stochastic Gradient Descent", "Aerial Imagery"]', '"https://doi.org/10.1007/978-3-642-15567-3_16"', '"Reliably extracting information from aerial imagery is a difficult problem with many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detection system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The network is trained on massive amounts of data using a consumer GPU. We demonstrate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels. We show that our method works reliably on two challenging urban datasets that are an order of magnitude larger than what was used to evaluate previous approaches."'),
('"Learning to Efficiently Detect Repeatable Interest Points in Depth Data"', '"ECCV 2012"', '["Random Forest", "Regression Tree", "Interest Point", "Depth Image", "Depth Data"]', '"https://doi.org/10.1007/978-3-642-33718-5_15"', '"Interest point (IP) detection is an important component of many computer vision methods. While there are a number of methods for detecting IPs in RGB images, modalities such as depth images and range scans have seen relatively little work. In this paper, we approach the IP detection problem from a machine learning viewpoint and formulate it as a regression problem. We learn a regression forest (RF) model that, given an image patch, tells us if there is an IP in the center of the patch. Our RF based method for IP detection allows an easy trade-off between speed and repeatability by adapting the depth and number of trees used for approximating the interest point response maps. The data used for training the RF model is obtained by running state-of-the-art IP detection methods on the depth images. We show further how the IP response map used for training the RF can be specifically designed to increase repeatability by employing 3D models of scenes generated by reconstruction systems such as KinectFusion [1]. Our experiments demonstrate that the use of such data leads to considerably improved IP detection."'),
('"Learning to Hash with Partial Tags: Exploring Correlation between Tags and Hashing Bits for Large S', '"ECCV 2014"', '["Hashing", "Tags", "Similarity Search", "Image Retrieval"]', '"https://doi.org/10.1007/978-3-319-10578-9_25"', '"Similarity search is an important technique in many large scale vision applications. Hashing approach becomes popular for similarity search due to its computational and memory efficiency. Recently, it has been shown that the hashing quality could be improved by combining supervised information, e.g. semantic tags/labels, into hashing function learning. However, tag information is not fully exploited in existing unsupervised and supervised hashing methods especially when only partial tags are available. This paper proposes a novel semi-supervised tag hashing (SSTH) approach that fully incorporates tag information into learning effective hashing function by exploring the correlation between tags and hashing bits. The hashing function is learned in a unified learning framework by simultaneously ensuring the tag consistency and preserving the similarities between image examples. An iterative coordinate descent algorithm is designed as the optimization procedure. Furthermore, we improve the effectiveness of hashing function through orthogonal transformation by minimizing the quantization error. Extensive experiments on two large scale image datasets demonstrate the superior performance of the proposed approach over several state-of-the-art hashing methods."'),
('"Learning to Localize Objects with Structured Output Regression"', '"ECCV 2008"', '["Support Vector Machine", "Ground Truth", "Object Detection", "Quality Function", "Structure Learni', '"https://doi.org/10.1007/978-3-540-88682-2_2"', '"Sliding window classifiers are among the most successful and widely applied techniques for object localization. However, training is typically done in a way that is not specific to the localization task. First a binary classifier is trained using a sample of positive and negative examples, and this classifier is subsequently applied to multiple regions within test images. We propose instead to treat object localization in a principled way by posing it as a problem of predicting structured data: we model the problem not as binary classification, but as the prediction of the bounding box of objects located in images. The use of a joint-kernel framework allows us to formulate the training procedure as a generalization of an SVM, which can be solved efficiently. We further improve computational efficiency by using a branch-and-bound strategy for localization during both training and testing. Experimental evaluation on the PASCAL VOC and TU Darmstadt datasets show that the structured training procedure improves performance over binary training as well as the best previously published scores."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Learning to Match Appearances by Correlations in a Covariance Metric Space"', '"ECCV 2012"', '["covariance matrix", "re-identification", "appearance matching"]', '"https://doi.org/10.1007/978-3-642-33712-3_58"', '"This paper addresses the problem of appearance matching across disjoint camera views. Significant appearance changes, caused by variations in view angle, illumination and object pose, make the problem challenging. We propose to formulate the appearance matching problem as the task of learning a model that selects the most descriptive features for a specific class of objects. Learning is performed in a covariance metric space using an entropy-driven criterion. Our main idea is that different regions of the object appearance ought to be matched using various strategies to obtain a distinctive representation. The proposed technique has been successfully applied to the person re-identification problem, in which a human appearance has to be matched across non-overlapping cameras. We demonstrate that our approach improves state of the art performance in the context of pedestrian recognition."'),
('"Learning to Match Images in Large-Scale Collections"', '"ECCV 2012"', '["Image Retrieval", "Visual Word", "Image Pair", "Training Pair", "Large Scale Image"]', '"https://doi.org/10.1007/978-3-642-33863-2_26"', '"Many computer vision applications require computing structure and feature correspondence across a large, unorganized image collection. This is a computationally expensive process, because the graph of matching image pairs is unknown in advance, and so methods for quickly and accurately predicting which of the O(n 2) pairs of images match are critical. Image comparison methods such as bag-of-words models or global features are often used to predict similar pairs, but can be very noisy. In this paper, we propose a new image matching method that uses discriminative learning techniques\\u2014applied to training data gathered automatically during the image matching process\\u2014to gradually compute a better similarity measure for predicting whether two images in a given collection overlap. By using such a learned similarity measure, our algorithm can select image pairs that are more likely to match for performing further feature matching and geometric verification, improving the overall efficiency of the matching process. Our approach processes a set of images in an iterative manner, alternately performing pairwise feature matching and learning an improved similarity measure. Our experiments show that our learned measures can significantly improve match prediction over the standard tf-idf-weighted similarity and more recent unsupervised techniques even with small amounts of training data, and can improve the overall speed of the image matching process by more than a factor of two."'),
('"Learning to Parse Pictures of People"', '"ECCV 2002"', '["Object recognition", "image and video indexing", "grouping and segmentation", "statistical pattern', '"https://doi.org/10.1007/3-540-47979-1_47"', '"Detecting people in images is a key problem for video indexing, browsing and retrieval. The main difficulties are the large appearance variations caused by action, clothing, illumination, viewpoint and scale. Our goal is to find people in static video frames using learned models of both the appearance of body parts (head, limbs, hands), and of the geometry of their assemblies. We build on Forsyth & Fleck\\u2019s general \\u2018body plan\\u2019 methodology and Felzenszwalb & Huttenlocher\\u2019s dynamic programming approach for efficiently assembling candidate parts into \\u2018pictorial structures\\u2019. However we replace the rather simple part detectors used in these works with dedicated detectors learned for each body part using Support Vector Machines (SVMs) or Relevance Vector Machines (RVMs). We are not aware of any previous work using SVMs to learn articulated body plans, however they have been used to detect both whole pedestrians and combinations of rigidly positioned subimages (typically, upper body, arms, and legs) in street scenes, under a wide range of illumination, pose and clothing variations. RVMs are SVM-like classifiers that offer a well-founded probabilistic interpretation and improved sparsity for reduced computation. We demonstrate their benefits experimentally in a series of results showing great promise for learning detectors in more general situations."'),
('"Learning to Rank 3D Features"', '"ECCV 2014"', '["3D pose estimation", "feature selection", "max-margin learning"]', '"https://doi.org/10.1007/978-3-319-10590-1_34"', '"Representation of three dimensional objects using a set of oriented point pair features has been shown to be effective for object recognition and pose estimation. Combined with an efficient voting scheme on a generalized Hough space, existing approaches achieve good recognition accuracy and fast operation. However, the performance of these approaches degrades when the objects are (self-)similar or exhibit degeneracies, such as large planar surfaces which are very common in both man made and natural shapes, or due to heavy object and background clutter. We propose a max-margin learning framework to identify discriminative features on the surface of three dimensional objects. Our algorithm selects and ranks features according to their importance for the specified task, which leads to improved accuracy and reduced computational cost. In addition, we analyze various grouping and optimization strategies to learn the discriminative pair features. We present extensive synthetic and real experiments demonstrating the improved results."'),
('"Learning to Rank Using High-Order Information"', '"ECCV 2014"', '["Support Vector Machine", "Feature Vector", "Loss Function", "Action Class", "Negative Class"]', '"https://doi.org/10.1007/978-3-319-10593-2_40"', '"The problem of ranking a set of visual samples according to their relevance to a query plays an important role in computer vision. The traditional approach for ranking is to train a binary classifier such as a support vector machine (svm). Binary classifiers suffer from two main deficiencies: (i) they do not optimize a ranking-based loss function, for example, the average precision (ap) loss; and (ii) they cannot incorporate high-order information such as the a priori correlation between the relevance of two visual samples (for example, two persons in the same image tend to perform the same action). We propose two novel learning formulations that allow us to incorporate high-order information for ranking. The first framework, called high-order binary svm (hob-svm), allows for a structured input. The parameters of hob-svm are learned by minimizing a convex upper bound on a surrogate 0-1 loss function. In order to obtain the ranking of the samples that form the structured input, hob-svm sorts the samples according to their max-marginals. The second framework, called high-order average precision svm (hoap-svm), also allows for a structured input and uses the same ranking criterion. However, in contrast to hob-svm, the parameters of hoap-svm are learned by minimizing a difference-of-convex upper bound on the ap loss. Using a standard, publicly available dataset for the challenging problem of action classification, we show that both hob-svm and hoap-svm outperform the baselines that ignore high-order information."'),
('"Learning to Recognize 3D Objects with SNoW"', '"ECCV 2000"', '["Support Vector Machine", "Object Recognition", "Recognition Rate", "Target Node", "Linear Support ', '"https://doi.org/10.1007/3-540-45054-8_29"', '"This paper describes a novel view-based learning algorithm for 3D object recognition from 2D images using a network of linear units. The SNoW learning architecture is a sparse network of linear functions over a pre-defined or incrementally learned feature space and is specifically tailored for learning in the presence of a very large number of features. We use pixel-based and edge-based representations in large scale object recognition experiments in which the performance of SNoW is compared with that of Support Vector Machines (SVMs) and nearest neighbor using the 100 objects in the Columbia Image Object Database (COIL-100). Experimental results show that the SNoW-based method outperforms the SVM-based system in terms of recognition rate and the computational cost involved in learning. Most importantly, SNoW\\u2019s performance degrades more gracefully when the training data contains fewer views. The empirical results also provide insight into practical and theoretical considerations on view-based methods for 3D object recognition."'),
('"Learning to Recognize Activities from the Wrong View Point"', '"ECCV 2008"', '["Descriptive Feature", "Transfer Learning", "Random Projection", "Appearance Feature", "Human Activ', '"https://doi.org/10.1007/978-3-540-88682-2_13"', '"Appearance features are good at discriminating activities in a fixed view, but behave poorly when aspect is changed. We describe a method to build features that are highly stable under change of aspect. It is not necessary to have multiple views to extract our features. Our features make it possible to learn a discriminative model of activity in one view, and spot that activity in another view, for which one might poses no labeled examples at all. Our construction uses labeled examples to build activity models, and unlabeled, but corresponding, examples to build an implicit model of how appearance changes with aspect. We demonstrate our method with challenging sequences of real human motion, where discriminative methods built on appearance alone fail badly."'),
('"Learning to Recognize Daily Actions Using Gaze"', '"ECCV 2012"', '["Action Recognition", "Humanoid Robot", "Training Sequence", "Action Label", "Foreground Region"]', '"https://doi.org/10.1007/978-3-642-33718-5_23"', '"We present a probabilistic generative model for simultaneously recognizing daily actions and predicting gaze locations in videos recorded from an egocentric camera. We focus on activities requiring eye-hand coordination and model the spatio-temporal relationship between the gaze point, the scene objects, and the action label. Our model captures the fact that the distribution of both visual features and object occurrences in the vicinity of the gaze point is correlated with the verb-object pair describing the action. It explicitly incorporates known properties of gaze behavior from the psychology literature, such as the temporal delay between fixation and manipulation events. We present an inference method that can predict the best sequence of gaze locations and the associated action label from an input sequence of images. We demonstrate improvements in action recognition rates and gaze prediction accuracy relative to state-of-the-art methods, on two new datasets that contain egocentric videos of daily activities and gaze."'),
('"Learning to Recognize Objects from Unseen Modalities"', '"ECCV 2010"', '["Object Recognition", "Unlabeled Data", "Correct Classification Rate", "Kernel Principal Component ', '"https://doi.org/10.1007/978-3-642-15549-9_49"', '"In this paper we investigate the problem of exploiting multiple sources of information for object recognition tasks when additional modalities that are not present in the labeled training set are available for inference. This scenario is common to many robotics sensing applications and is in contrast with the assumption made by existing approaches that require at least some labeled examples for each modality. To leverage the previously unseen features, we make use of the unlabeled data to learn a mapping from the existing modalities to the new ones. This allows us to predict the missing data for the labeled examples and exploit all modalities using multiple kernel learning. We demonstrate the effectiveness of our approach on several multi-modal tasks including object recognition from multi-resolution imagery, grayscale and color images, as well as images and text. Our approach outperforms multiple kernel learning on the original modalities, as well as nearest-neighbor and bootstrapping schemes."'),
('"Learning to Recognize Unsuccessful Activities Using a Two-Layer Latent Structural Model"', '"ECCV 2012"', '["Class Label", "Activity Recognition", "Temporal Position", "Temporal Stage", "Activity Video"]', '"https://doi.org/10.1007/978-3-642-33712-3_54"', '"In this paper, we propose to recognize unsuccessful activities (e.g., one tries to dress himself but fails), which have much more complex temporal structures, as we don\\u2019t know when the activity performer fails (which is called the point of failure in this paper). We develop a two-layer latent structural SVM model to tackle this problem: the first layer specifies the point of failure, and the second layer specifies the temporal positions of a number of key stages accordingly. The stages before the point of failure are successful stages, while the stages after the point of failure are background stages. Given weakly labeled training data, our training algorithm alternates between inferring the two-layer latent structure and updating the structural SVM parameters. In recognition, our method can not only recognize unsuccessful activities, but also infer the latent structure. We demonstrate the effectiveness of our proposed method on several newly collected datasets."'),
('"Learning to Segment a Video to Clips Based on Scene and Camera Motion"', '"ECCV 2012"', '["video temporal segmentation", "film study"]', '"https://doi.org/10.1007/978-3-642-33712-3_20"', '"In this paper, we present a novel learning-based algorithm for temporal segmentation of a video into clips based on both camera and scene motion, in particular, based on combinations of static vs. dynamic camera and static vs. dynamic scene. Given a video, we first perform shot boundary detection to segment the video to shots. We enforce temporal continuity by constructing a Markov Random Field (MRF) over the frames of each video shot with edges between consecutive frames and cast the segmentation problem as a frame level discrete labeling problem. Using manually labeled data we learn classifiers exploiting cues from optical flow to provide evidence for the different labels, and infer the best labeling over the frames. We show the effectiveness of the approach using user videos and full-length movies. Using sixty full-length movies spanning 50 years, we show that the proposed algorithm of grouping frames purely based on motion cues can aid computational applications such as recovering depth from a video and also reveal interesting trends in movies, which finds itself interesting novel applications in video analysis (time-stamping archive movies) and film studies."'),
('"Learning to Segment Humans by Stacking Their Body Parts"', '"ECCV 2014"', '["Human body segmentation", "Stacked Sequential Learning"]', '"https://doi.org/10.1007/978-3-319-16178-5_48"', '"Human segmentation in still images is a complex task due to the wide range of body poses and drastic changes in environmental conditions. Usually, human body segmentation is treated in a two-stage fashion. First, a human body part detection step is performed, and then, human part detections are used as prior knowledge to be optimized by segmentation strategies. In this paper, we present a two-stage scheme based on Multi-Scale Stacked Sequential Learning (MSSL). We define an extended feature set by stacking a multi-scale decomposition of body part likelihood maps. These likelihood maps are obtained in a first stage by means of a ECOC ensemble of soft body part detectors. In a second stage, contextual relations of part predictions are learnt by a binary classifier, obtaining an accurate body confidence map. The obtained confidence map is fed to a graph cut optimization procedure to obtain the final segmentation. Results show improved segmentation when MSSL is included in the human segmentation pipeline."'),
('"Learning to Segment"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24672-5_25"', '"We describe a new approach for learning to perform class-based segmentation using only unsegmented training examples. As in previous methods, we first use training images to extract fragments that contain common object parts. We then show how these parts can be segmented into their figure and ground regions in an automatic learning process. This is in contrast with previous approaches, which required complete manual segmentation of the objects in the training examples. The figure-ground learning combines top-down and bottom-up processes and proceeds in two stages, an initial approximation followed by iterative refinement. The initial approximation produces figure-ground labeling of individual image fragments using the unsegmented training images. It is based on the fact that on average, points inside the object are covered by more fragments than points outside it. The initial labeling is then improved by an iterative refinement process, which converges in up to three steps. At each step, the figure-ground labeling of individual fragments produces a segmentation of complete objects in the training images, which in turn induce a refined figure-ground labeling of the individual fragments. In this manner, we obtain a scheme that starts from unsegmented training images, learns the figure-ground labeling of image fragments, and then uses this labeling to segment novel images. Our experiments demonstrate that the learned segmentation achieves the same level of accuracy as methods using manual segmentation of training images, producing an automatic and robust top-down segmentation."'),
('"Learning Two-View Stereo Matching"', '"ECCV 2008"', '["Graphic Processing Unit", "Image Space", "Stereo Match", "Locally Linear Embedding", "Graphic Proc', '"https://doi.org/10.1007/978-3-540-88690-7_2"', '"We propose a graph-based semi-supervised symmetric matching framework that performs dense matching between two uncalibrated wide-baseline images by exploiting the results of sparse matching as labeled data. Our method utilizes multiple sources of information including the underlying manifold structure, matching preference, shapes of the surfaces in the scene, and global epipolar geometric constraints for occlusion handling. It can give inherent sub-pixel accuracy and can be implemented in a parallel fashion on a graphics processing unit (GPU). Since the graphs are directly learned from the input images without relying on extra training data, its performance is very stable and hence the method is applicable under general settings. Our algorithm is robust against outliers in the initial sparse matching due to our consideration of all matching costs simultaneously, and the provision of iterative restarts to reject outliers from the previous estimate. Some challenging experiments have been conducted to evaluate the robustness of our method."'),
('"Learning Visual Shape Lexicon for Document Image Content Recognition"', '"ECCV 2008"', '["Local Binary Pattern", "Pattern Anal", "Template Match", "Document Image", "Text Line"]', '"https://doi.org/10.1007/978-3-540-88688-4_55"', '"Developing effective content recognition methods for diverse imagery continues to challenge computer vision researchers. We present a new approach for document image content categorization using a lexicon of shape features. Each lexical word corresponds to a scale and rotation invariant shape feature that is generic enough to be detected repeatably and segmentation free. We learn a concise, structurally indexed shape lexicon from training by clustering and partitioning feature types through graph cuts. We demonstrate our approach on two challenging document image content recognition problems: 1) The classification of 4,500 Web images crawled from Google Image Search into three content categories \\u2014 pure image, image with text, and document image, and 2) Language identification of 8 languages (Arabic, Chinese, English, Hindi, Japanese, Korean, Russian, and Thai) on a 1,512 complex document image database composed of mixed machine printed text and handwriting. Our approach is capable to handle high intra-class variability and shows results that exceed other state-of-the-art approaches, allowing it to be used as a content recognizer in image indexing and retrieval systems."'),
('"Learning What and How of Contextual Models for Scene Labeling"', '"ECCV 2010"', '["Training Dataset", "Training Image", "Contextual Model", "Feature Weight", "Contextual Relationshi', '"https://doi.org/10.1007/978-3-642-15561-1_15"', '"We present a data-driven approach to predict the importance of edges and construct a Markov network for image analysis based on statistical models of global and local image features. We also address the coupled problem of predicting the feature weights associated with each edge of a Markov network for evaluation of context. Experimental results indicate that this scene dependent structure construction model eliminates spurious edges and improves performance over fully-connected and neighborhood connected Markov network."'),
('"Learning Where to Classify in Multi-view Semantic Segmentation"', '"ECCV 2014"', '["semantic segmentation", "multi-view", "efficiency", "view selection", "redundancy", "ranking", "im', '"https://doi.org/10.1007/978-3-319-10602-1_34"', '"There is an increasing interest in semantically annotated 3D models, e.g. of cities. The typical approaches start with the semantic labelling of all the images used for the 3D model. Such labelling tends to be very time consuming though. The inherent redundancy among the overlapping images calls for more efficient solutions. This paper proposes an alternative approach that exploits the geometry of a 3D mesh model obtained from multi-view reconstruction. Instead of clustering similar views, we predict the best view before the actual labelling. For this we find the single image part that bests supports the correct semantic labelling of each face of the underlying 3D mesh. Moreover, our single-image approach may surprise because it tends to increase the accuracy of the model labelling when compared to approaches that fuse the labels from multiple images. As a matter of fact, we even go a step further, and only explicitly label a subset of faces (e.g. 10%), to subsequently fill in the labels of the remaining faces. This leads to a further reduction of computation time, again combined with a gain in accuracy. Compared to a process that starts from the semantic labelling of the images, our method to semantically label 3D models yields accelerations of about 2 orders of magnitude. We tested our multi-view semantic labelling on a variety of street scenes."'),
('"Learning-Based Symmetry Detection in Natural Images"', '"ECCV 2012"', '["Natural Image", "Spectral Cluster", "Integral Image", "Multiple Instance Learn", "Symmetry Detecti', '"https://doi.org/10.1007/978-3-642-33786-4_4"', '"In this work we propose a learning-based approach to symmetry detection in natural images. We focus on ribbon-like structures, i.e. contours marking local and approximate reflection symmetry and make three contributions to improve their detection. First, we create and make publicly available a ground-truth dataset for this task by building on the Berkeley Segmentation Dataset. Second, we extract features representing multiple complementary cues, such as grayscale structure, color, texture, and spectral clustering information. Third, we use supervised learning to learn how to combine these cues, and employ MIL to accommodate the unknown scale and orientation of the symmetric structures. We systematically evaluate the performance contribution of each individual component in our pipeline, and demonstrate that overall we consistently improve upon results obtained using existing alternatives."'),
('"Least Committment Graph Matching by Evolutionary Optimisation"', '"ECCV 2000"', '["Genetic Algorithm", "Machine Intelligence", "Evolutionary Optimisation", "Hybrid Genetic Algorithm', '"https://doi.org/10.1007/3-540-45054-8_14"', '"This paper presents a method of matching ambiguous feature sets extracted from images. The method is based on Wilson and Hancock\\u2019s Bayesian matching framework [1], which is extended to handle the case where the feature measurements are ambiguous. A multimodal evolutionary optimisation framework is proposed, which is capable of simultaneously producing several good alternative solutions. Unlike other multimodal genetic algorithms, the one reported here requires no extra parameters: solution yields are maximised by removing bias in the selection step, while optimisation performance is maintained by a local search step. An experimental study demonstrates the effectiveness of the new approach on synthetic and real data. The framework is in principle applicable to any multimodal optimisation problem where local search performs well."'),
('"Lens Distortion Recovery for Accurate Sequential Structure and Motion Recovery"', '"ECCV 2002"', '["Structure from motion", "calibration", "lens distortion recovery", "high accuracy", "sequential"]', '"https://doi.org/10.1007/3-540-47967-8_13"', '"Lens distortions in off-the-shelf or wide-angle cameras block the road to high accuracy Structure and Motion Recovery (SMR) from video sequences. Neglecting lens distortions introduces a systematic error buildup which causes recovered structure and motion to bend and inhibits turntable or other loop sequences to close perfectly. Locking back onto previously reconstructed structure can become impossible due to the large drift caused by the error buildup. Bundle adjustments are widely used to perform an ultimate post-minimization of the total reprojection error. However, the initial recovered structure and motion needs to be close to optimal to avoid local minima. We found that bundle adjustments cannot remedy the error buildup caused by ignoring lens distortions. The classical approach to distortion removal involves a preliminary distortion estimation using a calibration pattern, known geometric properties of perspective projections or only 2D feature correspondences. Often the distortion is assumed constant during camera usage and removed from the images before applying SMR algorithms. However, lens distortions can change by zooming, focusing and temperature variations. Moreover, when only the video sequence is available preliminary calibration is often not an option. This paper addresses all fore-mentioned problems by sequentially recovering lens distortions together with structure and motion from video sequences without tedious pre-calibrations and allowing lens distortions to change over time. The devised algorithms are fairly simple as they only use linear least squares techniques. The unprocessed video sequence forms the only input and no severe restrictions are placed on viewed scene geometry. Therefore, the accurate recovery of structure and motion is fully automated and widely applicable. The experiments demonstrate the necessity of modeling lens distortions to achieve high accuracy in recovered structure and motion."'),
('"Lessons and Insights from Creating a Synthetic Optical Flow Benchmark"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33868-7_17"', '"With the MPI-Sintel Flow dataset, we introduce a naturalistic dataset for optical flow evaluation derived from the open source CGI movie Sintel. In contrast to the well-known Middlebury dataset, the MPI-Sintel Flow dataset contains longer and more varied sequences with image degradations such as motion blur, defocus blur, and atmospheric effects. Animators use a variety of techniques that produce pleasing images but make the raw animation data inappropriate for computer vision applications if used \\u201cout of the box\\u201d. Several changes to the rendering software and animation files were necessary in order to produce data for flow evaluation and similar changes are likely for future efforts to construct a scientific dataset from an animated film. Here we distill our experience with Sintel into a set of best practices for using computer animation to generate scientific data for vision research."'),
('"Lessons from the Primate Visual System"', '"ECCV 2012"', '["Posterior Parietal Cortex", "Biological Motion", "Superior Temporal Sulcus", "Visual Cortical Area', '"https://doi.org/10.1007/978-3-642-33863-2_46"', '"The primate visual system can perform an astonishing array of tasks as reflected by the correspondingly large portion of the cerebral cortex devoted to analyzing retinal signals. Although a potential source of inspiration for computer vision, with a few exceptions, progress has been slow in this field. Principal obstacles are the lack of any exhaustive list of what vision achieves in humans and the restricting of areas of investigation to a few topics such as motion, object categories and the control of a few actions such as reaching or saccades. Here I will review how we integrated several experimental techniques to address a question that arose from interactions with computer vision scientists more than fifteen years ago: the extraction of 3D surfaces. This goal is achieved by a new type of higher-order visual neuron: the gradient-selective neurons. Neurons selective for speed gradients were initially discovered in motion processing areas, such as MT/V5, MSTd and FST, located in the monkey superior temporal sulcus (STS). Subsequently, neurons selective for disparity gradients were discovered in shape processing areas, such as TEs and AIP. By combining these single-cell studies with fMRI in human and awake monkey, we were able to localize similar neurons to human cortical areas. In the second part I address my present interest in understanding the visual signals related to the actions of conspecifics, which is perhaps the ultimate challenge of motion processing, but which receives surprisingly little attention in vision. The understanding of observed actions exemplifies my statement that to be useful visual signals have to leave the visual system, as signals related to biological motion in the STS are indeed relayed to parietal regions involved in the control of diverse actions to be understood as actions."'),
('"Let There Be Color! Large-Scale Texturing of 3D Reconstructions"', '"ECCV 2014"', '["Input Image", "Data Term", "Support Region", "Photo Collection", "Smoothness Term"]', '"https://doi.org/10.1007/978-3-319-10602-1_54"', '"3D reconstruction pipelines using structure-from-motion and multi-view stereo techniques are today able to reconstruct impressive, large-scale geometry models from images but do not yield textured results. Current texture creation methods are unable to handle the complexity and scale of these models. We therefore present the first comprehensive texturing framework for large-scale, real-world 3D reconstructions. Our method addresses most challenges occurring in such reconstructions: the large number of input images, their drastically varying properties such as image scale, (out-of-focus) blur, exposure variation, and occluders (e.g., moving plants or pedestrians). Using the proposed technique, we are able to texture datasets that are several orders of magnitude larger and far more challenging than shown in related work."'),
('"Level Lines as Global Minimizers of Energy Functionals in Image Segmentation"', '"ECCV 2000"', '["Global Minimizer", "Image Segmentation", "Energy Model", "Segmentation Result", "Object Boundary"]', '"https://doi.org/10.1007/3-540-45053-X_16"', '"We propose a variational framework for determining global minimizers of rough energy functionals used in image segmentation. Segmentation is achieved by minimizing an energy model, which is comprised of two parts: the first part is the interaction between the observed data and the model, the second is a regularity term. The optimal boundaries are the set of curves that globally minimize the energy functional. Our motivation comes from the observation that energy functionals are traditionally complex, for which it is usually difficult to precise global minimizers corresponding to \\u201cbest\\u201d segmentations. Therefore, we focus on basic energy models, which global minimizers can be explicitly determined. In this paper, we prove that the set of curves that minimizes the image moment-based energy functionals is a family of level lines, i.e. the boundaries of level sets (connected components) of the image. For the completeness of the paper, we present a non-iterative algorithm for computing partitions with connected components. It leads to a sound initialization-free algorithm without any hidden parameter to be tuned."'),
('"Level Set and Region Based Surface Propagation for Diffusion Tensor MRI Segmentation"', '"MMBIA 2004"', '["Corpus Callosum", "Surface Evolution", "Human Brain Imaging", "Base Group Mapping", "Major White M', '"https://doi.org/10.1007/978-3-540-27816-0_11"', '"Diffusion Tensor Imaging (DTI) is a relatively new modality for human brain imaging. During the last years, this modality has become widely used in medical studies. Tractography is currently the favorite technique to characterize and analyse the structure of the brain white matter. Only a few studies have been proposed to group data of particular interest. Rather than working on extracted fibers or on an estimated scalar value accounting for anisotropy as done in other approaches, we propose to extend classical segmentation techniques based on surface evolution by considering region statistics defined on the full diffusion tensor field itself. A multivariate Gaussian is used to approximate the density of the components of diffusion tensor for each sub-region of the volume. We validate our approach on synthetical data and we show promising results on the extraction of the corpus callosum from a real dataset."'),
('"Level Sets and Distance Functions"', '"ECCV 2000"', '["Distance Function", "Coupling Term", "Implicit Representation", "Active Contour Model", "Signed Di', '"https://doi.org/10.1007/3-540-45054-8_38"', '"This paper is concerned with the simulation of the Partial Differential Equation (PDE) driven evolution of a closed surface by means of an implicit representation. In most applications, the natural choice for the implicit representation is the signed distance function to the closed surface. Osher and Sethian propose to evolve the distance function with a Hamilton-Jacobi equation. Unfortunately the solution to this equation is not a distance function. As a consequence, the practical application of the level set method is plagued with such questions as when do we have to \\u201creinitialize\\u201d the distance function? How do we \\u201creinitialize\\u201d the distance function? Etc... which reveal a disagreement between the theory and its implementation. This paper proposes an alternative to the use of Hamilton-Jacobi equations which eliminates this contradiction: in our method the implicit representation always remains a distance function by construction, and the implementation does not differ from the theory anymore. This is achieved through the introduction of a new equation. Besides its theoretical advantages, the proposed method also has several practical advantages which we demonstrate in three applications: (i) the segmentation of the human cortex surfaces from MRI images using two coupled surfaces [27], (ii) the construction of a hierarchy of Euclidean skeletons of a 3D surface, (iii) the reconstruction of the surface of 3D objects through stereo [13]."'),
('"Level-Set Curve Particles"', '"ECCV 2006"', '["Active Contour", "Boundary Component", "Plane Curf", "Signed Distance Function", "Geodesic Active ', '"https://doi.org/10.1007/11744078_49"', '"In many applications it is necessary to track a moving and deforming boundary on the plane from infrequent, sparse measurements. For instance, each of a set of mobile observers may be able to tell the position of a point on the boundary. Often boundary components split, merge, appear, and disappear over time. Data are typically sparse and noisy and the underlying dynamics is uncertain. To address these issues, we use a particle filter to represent a distribution in the large space of all plane curves and propose a full-fledged combination of level sets and particle filters. Our main contribution is in controlling the potentially high expense of multiplying the cost of a level set representation of boundaries by the number of particles needed. Experiments on tracking the boundary of a colon in tomographic imagery from sparse edge measurements show the promise of the approach."'),
('"Lie Bodies: A Manifold Representation of 3D Human Shape"', '"ECCV 2012"', '["Shape deformation", "Lie group", "Statistics on manifolds"]', '"https://doi.org/10.1007/978-3-642-33718-5_1"', '"Three-dimensional object shape is commonly represented in terms of deformations of a triangular mesh from an exemplar shape. Existing models, however, are based on a Euclidean representation of shape deformations. In contrast, we argue that shape has a manifold structure: For example, summing the shape deformations for two people does not necessarily yield a deformation corresponding to a valid human shape, nor does the Euclidean difference of these two deformations provide a meaningful measure of shape dissimilarity. Consequently, we define a novel manifold for shape representation, with emphasis on body shapes, using a new Lie group of deformations. This has several advantages. First we define triangle deformations exactly, removing non-physical deformations and redundant degrees of freedom common to previous methods. Second, the Riemannian structure of Lie Bodies enables a more meaningful definition of body shape similarity by measuring distance between bodies on the manifold of body shape deformations. Third, the group structure allows the valid composition of deformations. This is important for models that factor body shape deformations into multiple causes or represent shape as a linear combination of basis shapes. Finally, body shape variation is modeled using statistics on manifolds. Instead of modeling Euclidean shape variation with Principal Component Analysis we capture shape variation on the manifold using Principal Geodesic Analysis. Our experiments show consistent visual and quantitative advantages of Lie Bodies over traditional Euclidean models of shape deformation and our representation can be easily incorporated into existing methods."'),
('"Light Field Appearance Manifolds"', '"ECCV 2004"', '["Appearance Model", "Polygonal Mesh", "Active Appearance Model", "Global Illumination", "Reference ', '"https://doi.org/10.1007/978-3-540-24673-2_39"', '"Statistical shape and texture appearance models are powerful image representations, but previously had been restricted to 2D or 3D shapes with smooth surfaces and lambertian reflectance. In this paper we present a novel 3D appearance model using image-based rendering techniques, which can represent complex lighting conditions, structures, and surfaces. We construct a light field manifold capturing the multi-view appearance of an object class and extend the direct search algorithm of Cootes and Taylor to match new light fields or 2D images of an object to a point on this manifold. When matching to a 2D image the reconstructed light field can be used to render unseen views of the object. Our technique differs from previous view-based active appearance models in that model coefficients between views are explicitly linked, and that we do not model any pose variation within the shape model at a single view. It overcomes the limitations of polygonal based appearance models and uses light fields that are acquired in real-time."'),
('"Light Field from Smartphone-Based Dual Video"', '"ECCV 2014"', '["Computer vision", "Light field imaging", "Video processing"]', '"https://doi.org/10.1007/978-3-319-16181-5_46"', '"In this work, we introduce a light field acquisition approach for standard smartphones. The smartphone is manually translated along a horizontal rail, while recording synchronized video with front and rear camera. The front camera captures a control pattern, mounted parallel to the direction of translation to determine the smartphones current position. This information is used during a postprocessing step to identify an equally spaced subset of recorded frames from the rear camera, which captures the actual scene. From this data we assemble a light field representation of the scene. For subsequent disparity estimation, we apply a structure tensor approach in the epipolar plane images."'),
('"Light-Efficient Photography"', '"ECCV 2008"', '["Exposure Level", "Optimal Sequence", "Aperture Diameter", "Left Endpoint", "Capture Sequence"]', '"https://doi.org/10.1007/978-3-540-88693-8_4"', '"We consider the problem of imaging a scene with a given depth of field at a given exposure level in the shortest amount of time possible. We show that by (1) collecting a sequence of photos and (2) controlling the aperture, focus and exposure time of each photo individually, we can span the given depth of field in less total time than it takes to expose a single narrower-aperture photo. Using this as a starting point, we obtain two key results. First, for lenses with continuously-variable apertures, we derive a closed-form solution for the globally optimal capture sequence, i.e., that collects light from the specified depth of field in the most efficient way possible. Second, for lenses with discrete apertures, we derive an integer programming problem whose solution is the optimal sequence. Our results are applicable to off-the-shelf cameras and typical photography conditions, and advocate the use of dense, wide-aperture photo sequences as a light-efficient alternative to single-shot, narrow-aperture photography."'),
('"Lighting and Pose Robust Face Sketch Synthesis"', '"ECCV 2010"', '["Local Binary Pattern", "Markov Random Field", "Test Photo", "Lighting Variation", "Face Photo"]', '"https://doi.org/10.1007/978-3-642-15567-3_31"', '"Automatic face sketch synthesis has important applications in law enforcement and digital entertainment. Although great progress has been made in recent years, previous methods only work under well controlled conditions and often fail when there are variations of lighting and pose. In this paper, we propose a robust algorithm for synthesizing a face sketch from a face photo taken under a different lighting condition and in a different pose than the training set. It synthesizes local sketch patches using a multiscale Markov Random Field (MRF) model. The robustness to lighting and pose variations is achieved in three steps. Firstly, shape priors specific to facial components are introduced to reduce artifacts and distortions caused by variations of lighting and pose. Secondly, new patch descriptors and metrics which are more robust to lighting variations are used to find candidates of sketch patches given a photo patch. Lastly, a smoothing term measuring both intensity compatibility and gradient compatibility is used to match neighboring sketch patches on the MRF network more effectively. The proposed approach significantly improves the performance of the state-of-the-art method. Its effectiveness is shown through experiments on the CUHK face sketch database and celebrity photos collected from the web."'),
('"Lighting Aware Preprocessing for Face Recognition across Varying Illumination"', '"ECCV 2010"', '["Face Recognition", "Face Image", "Face Database", "Normal Lighting", "Illumination Normalization"]', '"https://doi.org/10.1007/978-3-642-15552-9_23"', '"Illumination variation is one of intractable yet crucial problems in face recognition and many lighting normalization approaches have been proposed in the past decades. Nevertheless, most of them preprocess all the face images in the same way thus without considering the specific lighting in each face image. In this paper, we propose a lighting aware preprocessing (LAP) method, which performs adaptive preprocessing for each testing image according to its lighting attribute. Specifically, the lighting attribute of a testing face image is first estimated by using spherical harmonic model. Then, a von Mises-Fisher (vMF) distribution learnt from a training set is exploited to model the probability that the estimated lighting belongs to normal lighting. Based on this probability, adaptive preprocessing is performed to normalize the lighting variation in the input image. Extensive experiments on Extended YaleB and Multi-PIE face databases show the effectiveness of our proposed method."'),
('"Lighting Estimation in Indoor Environments from Low-Quality Images"', '"ECCV 2012"', '["light estimation", "depth sensor", "color constancy"]', '"https://doi.org/10.1007/978-3-642-33868-7_38"', '"Lighting conditions estimation is a crucial point in many applications. In this paper, we show that combining color images with corresponding depth maps (provided by modern depth sensors) allows to improve estimation of positions and colors of multiple lights in a scene. Since usually such devices provide low-quality images, for many steps of our framework we propose alternatives to classical algorithms that fail when the image quality is low. Our approach consists in decomposing an original image into specular shading, diffuse shading and albedo. The two shading images are used to render different versions of the original image by changing the light configuration. Then, using an optimization process, we find the lighting conditions allowing to minimize the difference between the original image and the rendered one."'),
('"Line Geometry for 3D Shape Understanding and Reconstruction"', '"ECCV 2004"', '["Point Cloud", "Spine Curve", "Moulding Surface", "Helical Gear", "Rotational Surface"]', '"https://doi.org/10.1007/978-3-540-24670-1_23"', '"We understand and reconstruct special surfaces from 3D data with line geometry methods. Based on estimated surface normals we use approximation techniques in line space to recognize and reconstruct rotational, helical, developable and other surfaces, which are characterized by the configuration of locally intersecting surface normals. For the computational solution we use a modified version of the Klein model of line space. Obvious applications of these methods lie in Reverse Engineering. We have tested our algorithms on real world data obtained from objects as antique pottery, gear wheels, and a surface of the ankle joint."'),
('"Linear Multi View Reconstruction with Missing Data"', '"ECCV 2002"', '["Structure from Motion", "Linear Multiple View Reconstruction", "Missing Data", "Affine and Project', '"https://doi.org/10.1007/3-540-47967-8_21"', '"General multi view reconstruction from affine or projective cameras has so far been solved most efficiently using methods of factorizing image data matrices into camera and scene parameters. This can be done directly for affine cameras[18] and after computing epipolar geometry for projective cameras [17]. A notorious problem has been the fact that these factorization methods require all points to be visible in all views. This paper presents alternative algorithms for general affine and projective views of multiple points where a) points and camera centers are computed as the nullspace of one linear system constructed from all the image data b) only three points have to be visible in all views. The latter requirement increases the flexibility and usefulness of 3D reconstruction from multiple views. In the case of projective views and unknown epipolar geometry, an additional algorithm is presented which initially assumes affine views and compensates iteratively for the perspective effects. In this paper affine cameras are represented in a projective framework which is novel and leads to a unified treatment of parallel and perspective projection in a single framework. The experiments cover a wide range of different camera motions and compare the presented algorithms to factorization methods, including approaches which handle missing data."'),
('"Linear Pose Estimation from Points or Lines"', '"ECCV 2002"', '["Ground Truth", "Augmented Reality", "Machine Intelligence", "Rotation Error", "Linear Algorithm"]', '"https://doi.org/10.1007/3-540-47979-1_19"', '"Estimation of camera pose from an image of n points or lines with known correspondence is a thoroughly studied problem in computer vision. Most solutions are iterative and depend on nonlinear optimization of some geometric constraint, either on the world coordinates or on the projections to the image plane. For real-time applications we are interested in linear or closed-form solutions free of initialization. We present a general framework which allows for a novel set of linear solutions to the pose estimation problem for both n points and n lines. We present a number of simulations which compare our results to two other recent linear algorithm as well as to iterative approaches. We conclude with tests on real imagery in an augmented reality setup. We also present an analysis of the sensitivity of our algorithms to image noise."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Linear Time Maximally Stable Extremal Regions"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_14"', '"In this paper we present a new algorithm for computing Maximally Stable Extremal Regions (MSER), as invented by Matas et al. The standard algorithm makes use of a union-find data structure and takes quasi-linear time in the number of pixels. The new algorithm provides exactly identical results in true worst-case linear time. Moreover, the new algorithm uses significantly less memory and has better cache-locality, resulting in faster execution. Our CPU implementation performs twice as fast as a state-of-the-art FPGA implementation based on the standard algorithm."'),
('"Linearized Smooth Additive Classifiers"', '"ECCV 2012"', '["Training Time", "Computer Vision Application", "Memory Overhead", "Empirical Risk Minimization", "', '"https://doi.org/10.1007/978-3-642-33863-2_24"', '"We consider a framework for learning additive classifiers based on regularized empirical risk minimization, where the regularization favors \\u201csmooth\\u201d functions. We present representations of classifiers for which the optimization problem can be efficiently solved. The first family of such classifiers are derived from a penalized spline formulation due to Eilers and Marx, which is modified to enabled linearization. The second is a novel family of classifiers that are based on classes of orthogonal basis functions with othogonal derivatives. Both these families lead to explicit feature embeddings that can be used with off-the-shelf linear solvers such as LIBLINEAR to obtain additive classifiers. The proposed family of classifiers offer better trade-offs between training time, memory overhead and classifier accuracy, compared to the state-of-the-art in additive classifier training."'),
('"Linking People in Videos with \\u201cTheir\\u201d Names Using Coreference Resolution"', '"ECCV 2014"', '["Person naming", "coreference resolution", "text-video alignment"]', '"https://doi.org/10.1007/978-3-319-10590-1_7"', '"Natural language descriptions of videos provide a potentially rich and vast source of supervision. However, the highly-varied nature of language presents a major barrier to its effective use. What is needed are models that can reason over uncertainty over both videos and text. In this paper, we tackle the core task of person naming: assigning names of people in the cast to human tracks in TV videos. Screenplay scripts accompanying the video provide some crude supervision about who\\u2019s in the video. However, even the basic problem of knowing who is mentioned in the script is often difficult, since language often refers to people using pronouns (e.g., \\u201che\\u201d) and nominals (e.g., \\u201cman\\u201d) rather than actual names (e.g., \\u201cSusan\\u201d). Resolving the identity of these mentions is the task of coreference resolution, which is an active area of research in natural language processing. We develop a joint model for person naming and coreference resolution, and in the process, infer a latent alignment between tracks and mentions. We evaluate our model on both vision and NLP tasks on a new dataset of 19 TV episodes. On both tasks, we significantly outperform the independent baselines."'),
('"Linking Pose and Motion"', '"ECCV 2008"', '["Video Sequence", "Ground Plane", "Color Histogram", "Angle Error", "Temporal Consistency"]', '"https://doi.org/10.1007/978-3-540-88693-8_15"', '"Algorithms designed to estimate 3D pose in video sequences enforce temporal consistency but typically overlook an important source of information: The 3D pose of an object, be it rigid or articulated, has a direct influence on its direction of travel."'),
('"Local Analysis for 3D Reconstruction of Specular Surfaces \\u2014 Part II"', '"ECCV 2002"', '["Shape recovery", "geometry", "mirror surfaces"]', '"https://doi.org/10.1007/3-540-47967-8_51"', '"We analyze the problem of recovering the shape of a mirror surface. We generalize the results of [1], where the special case of planar and spherical mirror surfaces was considered, extending that analysis to any smooth surface. A calibrated scene composed of lines passing through a point is assumed. The lines are reflected by the mirror surface onto the image plane of a calibrated camera, where the intersection and orientation of such reflections are measured. The relationship between the local geometry of the surface around the point of reflection and the measurements is analyzed. We give necessary and sufficient conditions, as well as a practical algorithm, for recovering first order local information (positions and normals) when three intersecting lines are visible. A small number of \\u2018ghost solutions\\u2019 may arise. Second order surface geometry may also be obtained up to one unknown parameter. Experimental results with real mirror surfaces are presented."'),
('"Local Binary Patterns to Evaluate Trabecular Bone Structure from Micro-CT Data: Application to Stud', '"ECCV 2014"', '["Bone structural analysis", "Micro-CT", "Osteoarthritis", "Multiscale LBP"]', '"https://doi.org/10.1007/978-3-319-16181-5_5"', '"Osteoarthritis (OA) causes progressive degeneration of articular cartilage and pathological changes in subchondral bone. These changes can be assessed volumetrically using micro-computed tomography (\\\\(\\\\mu \\\\)CT) imaging. The local descriptor, i.e. local binary pattern (LBP), is a new alternative solution to perform analysis of local bone structures from \\\\(\\\\mu \\\\)CT scans. In this study, different trabecular bone samples were prepared from patients diagnosed with OA and treated with total knee arthroplasty. The LBP descriptor was applied to correlate the distribution of local patterns with the severity of the disease. The results obtained suggest the appearance and disappearance of specific oriented patterns with OA, as an adaptation of the bone to the decrease of cartilage thickness. The experimental results suggest that the LBP descriptor can be used to assess the changes in the trabecular bone due to OA."'),
('"Local Descriptors Encoded by Fisher Vectors for Person Re-identification"', '"ECCV 2012"', '["Gaussian Mixture Model", "Local Binary Pattern", "IEEE Conf", "Local Descriptor", "Pairwise Constr', '"https://doi.org/10.1007/978-3-642-33863-2_41"', '"This paper proposes a new descriptor for person re-identification building on the recent advances of Fisher Vectors. Specifically, a simple vector of attributes consisting in the pixel coordinates, its intensity as well as the first and second-order derivatives is computed for each pixel of the image. These local descriptors are turned into Fisher Vectors before being pooled to produce a global representation of the image. The so-obtained Local Descriptors encoded by Fisher Vector (LDFV) have been validated through experiments on two person re-identification benchmarks (VIPeR and ETHZ), achieving state-of-the-art performance on both datasets."'),
('"Local Estimation of High Velocity Optical Flow with Correlation Image Sensor"', '"ECCV 2014"', '["optical flow", "high velocity flow", "real time estimation", "time correlation image sensor"]', '"https://doi.org/10.1007/978-3-319-10578-9_16"', '"In this article, the authors address a problem of the estimation of high velocity optical flow. When images are captured by conventional image sensors, the problem of the optical flow estimation is ill-posed if only the temporal constancy of the image brightness is the valid assumption. When given images are captured by the correlation image sensors, though, you can make the problem of the optical flow estimation well-posed under some condition and can locally estimate the unique optical flow at each pixel in each single frame. The condition though would not be satisfied when the flow velocity is high. In this article, we propose a method that can estimate the normal component of high velocity optical flow using only the local image values in each single frame. The equation used for estimating the normal velocity is theoretically derived and the condition the equation holds is also revealed."'),
('"Local Expert Forest of Score Fusion for Video Event Classification"', '"ECCV 2012"', '["Event Category", "Local Expert", "Video Event", "Binary Partition", "Score Fusion"]', '"https://doi.org/10.1007/978-3-642-33715-4_29"', '"We address the problem of complicated event categorization from a large dataset of videos \\u201cin the wild\\u201d, where multiple classifiers are applied independently to evaluate each video with a \\u2018likelihood\\u2019 score. The core contribution of this paper is a local expert forest model for meta-level score fusion for event detection under heavily imbalanced class distributions. Our motivation is to adapt to performance variations of the classifiers in different regions of the score space, using a divide-and-conquer technique. We propose a novel method to partition the likelihood-space, being sensitive to local label distributions in imbalanced data, and train a pair of locally optimized experts each time. Multiple pairs of experts based on different partitions (\\u2018trees\\u2019) form a \\u2018forest\\u2019, balancing local adaptivity and over-fitting of the model. As a result, our model disregards classifiers in regions of the score space where their performance is bad, achieving both local source selection and fusion. We experiment with the TRECVID Multimedia Event Detection (MED) dataset, detecting 15 complicated events from around 34k video clips comprising more than 1000 hours, and demonstrate superior performance compared to other score-level fusion methods."'),
('"Local Higher-Order Statistics (LHS) for Texture Categorization and Facial Analysis"', '"ECCV 2012"', '["Face Image", "Gaussian Mixture Model", "Local Binary Pattern", "Facial Expression Recognition", "L', '"https://doi.org/10.1007/978-3-642-33786-4_1"', '"This paper proposes a new image representation for texture categorization and facial analysis, relying on the use of higher-order local differential statistics as features. In contrast with models based on the global structure of textures and faces, it has been shown recently that small local pixel pattern distributions can be highly discriminative. Motivated by such works, the proposed model employs higher-order statistics of local non-binarized pixel patterns for the image description. Hence, in addition to being remarkably simple, it requires neither any user specified quantization of the space (of pixel patterns) nor any heuristics for discarding low occupancy volumes of the space. This leads to a more expressive representation which, when combined with discriminative SVM classifier, consistently achieves state-of-the-art performance on challenging texture and facial analysis datasets outperforming contemporary methods (with similar powerful classifiers)."'),
('"Local Label Descriptor for Example Based Semantic Image Labeling"', '"ECCV 2012"', '["Random Forest", "Training Image", "Feature Descriptor", "Conditional Random Field", "Baseline Meth', '"https://doi.org/10.1007/978-3-642-33786-4_27"', '"In this paper we introduce the concept of local label descriptor, which is a concatenation of label histograms for each cell in a patch. Local label descriptors alleviate the label patch misalignment issue in combining structured label predictions for semantic image labeling. Given an input image, we solve for a label map whose local label descriptors can be approximated as a sparse convex combination of exemplar label descriptors in the training data, where the sparsity is regularized by the similarity measure between the local feature descriptor of the input image and that of the exemplars in the training data set. Low-level image over-segmentation can be incorporated into our formulation to improve efficiency. Our formulation and algorithm compare favorably with the baseline method on the CamVid and Barcelona datasets."'),
('"Local Log-Euclidean Covariance Matrix (L2ECM) for Image Representation and Its Applications"', '"ECCV 2012"', '["Covariance Matrice", "Scale Invariant Feature Transform", "Human Detection", "Scale Invariant Feat', '"https://doi.org/10.1007/978-3-642-33712-3_34"', '"This paper presents Local Log-Euclidean Covariance Matrix (L2ECM) to represent neighboring image properties by capturing correlation of various image cues. Our work is inspired by the structure tensor which computes the second-order moment of image gradients for representing local image properties, and the Diffusion Tensor Imaging which produces tensor-valued image characterizing the local tissue structure. Our approach begins with extraction of raw features consisting of multiple image cues. For each pixel we compute a covariance matrix in its neighboring region, producing a tensor-valued image. The covariance matrices are symmetric and positive-definite (SPD) which forms a Riemannian manifold. In the Log-Euclidean framework, the SPD matrices form a Lie group equipped with Euclidean space structure, which enables common Euclidean operations in the logarithm domain. Hence, we compute the covariance matrix logarithm, obtaining the pixel-wise symmetric matrix. After half-vectorization we obtain the vector-valued L2ECM image, which can be flexibly handled with Euclidean operations while preserving the geometric structure of SPD matrices. The L2ECM features can be used in diverse image or vision tasks. We demonstrate some applications of its statistical modeling by simple second-order central moment and achieve promising performance."'),
('"Local Occlusion Detection under Deformations Using Topological Invariants"', '"ECCV 2010"', '["Image Sequence", "Motion Estimate", "Color Variation", "IEEE Conf", "Topological Invariant"]', '"https://doi.org/10.1007/978-3-642-15558-1_8"', '"Occlusions provide critical cues about the 3D structure of man-made and natural scenes. We present a mathematical framework and algorithm to detect and localize occlusions in image sequences of scenes that include deforming objects. Our occlusion detector works under far weaker assumptions than other detectors. We prove that occlusions in deforming scenes occur when certain well-defined local topological invariants are not preserved. Our framework employs these invariants to detect occlusions with a zero false positive rate under assumptions of bounded deformations and color variation. The novelty and strength of this methodology is that it does not rely on spatio-temporal derivatives or matching, which can be problematic in scenes including deforming objects, but is instead based on a mathematical representation of the underlying cause of occlusions in a deforming 3D scene. We demonstrate the effectiveness of the occlusion detector using image sequences of natural scenes, including deforming cloth and hand motions."'),
('"Local Orientation Smoothness Prior for Vascular Segmentation of Angiography"', '"ECCV 2004"', '["Magnetic Resonance Angiography", "Local Orientation", "Manual Segmentation", "Orientation Tensor",', '"https://doi.org/10.1007/978-3-540-24671-8_28"', '"We present a new generic method for vascular segmentation of angiography. Angiography is used for the medical diagnosis of arterial diseases. To facilitate an effective and efficient review of the vascular information in the angiograms, segmentation is a first stage for other post-processing routines. The method we propose uses a novel a priori \\u2014 local orientation smoothness prior \\u2014 to enforce an adaptive regularization constraint for the vascular segmentation within the Bayes\\u2019 framework. It aspires to segment a variety of angiographies and is aimed at improving the quality of segmentation in low blood flow regions. Our algorithm is tested on numerical phantoms and clinical datasets. The experimental results show that our method produces better segmentations than the maximum likelihood estimation and the estimation with a multi-level logistic Markov random field model. Furthermore, the novel algorithm produces aneurysm segmentations comparable to the manual segmentations obtained from an experienced consultant radiologist."'),
('"Local Regularization for Multiclass Classification Facing Significant Intraclass Variations"', '"ECCV 2008"', '["Training Image", "Canonical Correlation Analysis", "Local Regularization", "Local Learning", "Clas', '"https://doi.org/10.1007/978-3-540-88693-8_55"', '"We propose a new local learning scheme that is based on the principle of decisiveness: the learned classifier is expected to exhibit large variability in the direction of the test example. We show how this principle leads to optimization functions in which the regularization term is modified, rather than the empirical loss term as in most local learning schemes. We combine this local learning method with a Canonical Correlation Analysis based classification method, which is shown to be similar to multiclass LDA. Finally, we show that the classification function can be computed efficiently by reusing the results of previous computations. In a variety of experiments on new and existing data sets, we demonstrate the effectiveness of the CCA based classification method compared to SVM and Nearest Neighbor classifiers, and show that the newly proposed local learning method improves it even further, and outperforms conventional local learning schemes."'),
('"Local Scale Selection for Gaussian Based Description Techniques"', '"ECCV 2000"', '["Recognition Rate", "Gradient Norm", "Orientation Error", "Point Correspondence", "Dominant Orienta', '"https://doi.org/10.1007/3-540-45054-8_8"', '"This paper addresses the problem of the local scale parameter selection for recognition techniques based on Gaussian derivatives. Patterns are described in a feature space of which each dimension is a scale and orientation normalized receptive field (a unit composed of normalized Gaussian-based filters)."'),
('"Local Statistic Based Region Segmentation with Automatic Scale Selection"', '"ECCV 2008"', '["Image Segmentation", "Active Contour", "Image Edge", "Active Contour Model", "Homogeneous Area"]', '"https://doi.org/10.1007/978-3-540-88688-4_36"', '"Recently, new segmentation models based on local information have emerged. They combine local statistics of the regions along the contour (inside and outside) to drive the segmentation procedure. Since they are based on local decisions, these models are more robust to local variations of the regions of interest (contrast, noise, blur, ...). They nonetheless also introduce some new difficulties which are inherent to the fact of basing a global property (the segmentation) on pure local decisions. This papers explores some of those difficulties and proposes some possible corrections. Results on both 2D and 3D data are compared to those obtained without these corrections."'),
('"Localizing Objects While Learning Their Appearance"', '"ECCV 2010"', '["Training Image", "Object Class", "Appearance Model", "Target Class", "Learning Stage"]', '"https://doi.org/10.1007/978-3-642-15561-1_33"', '"Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown. Previous works generally require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on the challenging Pascal VOC 2007 dataset. Furthermore, our method enables to train any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations."'),
('"Localizing Objects with Smart Dictionaries"', '"ECCV 2008"', '["Visual Word", "Integral Image", "Sift Descriptor", "Vocabulary Tree", "Visual Dictionary"]', '"https://doi.org/10.1007/978-3-540-88682-2_15"', '"We present an approach to determine the category and location of objects in images. It performs very fast categorization of each pixel in an image, a brute-force approach made feasible by three key developments: First, our method reduces the size of a large generic dictionary (on the order of ten thousand words) to the low hundreds while increasing classification performance compared to k-means. This is achieved by creating a discriminative dictionary tailored to the task by following the information bottleneck principle. Second, we perform feature-based categorization efficiently on a dense grid by extending the concept of integral images to the computation of local histograms. Third, we compute SIFT descriptors densely in linear time. We compare our method to the state of the art and find that it excels in accuracy and simplicity, performing better while assuming less."'),
('"Locally Consistent ToF and Stereo Data Fusion"', '"ECCV 2012"', '["Stereo Vision", "Stereo Pair", "Stereo Vision System", "Stereo System", "Stereo Algorithm"]', '"https://doi.org/10.1007/978-3-642-33863-2_62"', '"Depth estimation for dynamic scenes is a challenging and relevant problem in computer vision. Although this problem can be tackled by means of ToF cameras or stereo vision systems, each of the two systems alone has its own limitations. In this paper a framework for the fusion of 3D data produced by a ToF camera and a stereo vision system is proposed. Initially, depth data acquired by the ToF camera are up-sampled to the spatial resolution of the stereo vision images by a novel up-sampling algorithm based on image segmentation and bilateral filtering. In parallel a dense disparity field is obtained by a stereo vision algorithm. Finally, the up-sampled ToF depth data and the disparity field provided by stereo vision are synergically fused by enforcing the local consistency of depth data. The depth information obtained with the proposed framework is characterized by the high resolution of the stereo vision system and by an improved accuracy with respect to the one produced by both subsystems. Experimental results clearly show how the proposed method is able to outperform the compared fusion algorithms."'),
('"Located Hidden Random Fields: Learning Discriminative Parts for Object Detection"', '"ECCV 2006"', '["Object Detection", "Object Class", "Conditional Random Field", "Discriminative Model", "Segmentati', '"https://doi.org/10.1007/11744078_24"', '"This paper introduces the Located Hidden Random Field (LHRF), a conditional model for simultaneous part-based detection and segmentation of objects of a given class. Given a training set of images with segmentation masks for the object of interest, the LHRF automatically learns a set of parts that are both discriminative in terms of appearance and informative about the location of the object. By introducing the global position of the object as a latent variable, the LHRF models the long-range spatial configuration of these parts, as well as their local interactions. Experiments on benchmark datasets show that the use of discriminative parts leads to state-of-the-art detection and segmentation performance, with the additional benefit of obtaining a labeling of the object\\u2019s component parts."'),
('"Locating Facial Features with an Extended Active Shape Model"', '"ECCV 2008"', '["Facial Feature", "Mahalanobis Distance", "Shape Model", "Kernel Principal Component Analysis", "Ac', '"https://doi.org/10.1007/978-3-540-88693-8_37"', '"We make some simple extensions to the Active Shape Model of Cootes et al. [4], and use it to locate features in frontal views of upright faces. We show on independent test data that with the extensions the Active Shape Model compares favorably with more sophisticated methods. The extensions are (i) fitting more landmarks than are actually needed (ii) selectively using two- instead of one-dimensional landmark templates (iii) adding noise to the training set (iv) relaxing the shape model where advantageous (v) trimming covariance matrices by setting most entries to zero, and (vi) stacking two Active Shape Models in series."'),
('"Location Recognition Using Prioritized Feature Matching"', '"ECCV 2010"', '["Point Cloud", "Image Retrieval", "Query Image", "Visibility Graph", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-642-15552-9_57"', '"We present a fast, simple location recognition and image localization method that leverages feature correspondence and geometry estimated from large Internet photo collections. Such recovered structure contains a significant amount of useful information about images and image features that is not available when considering images in isolation. For instance, we can predict which views will be the most common, which feature points in a scene are most reliable, and which features in the scene tend to co-occur in the same image. Based on this information, we devise an adaptive, prioritized algorithm for matching a representative set of SIFT features covering a large scene to a query image for efficient localization. Our approach is based on considering features in the scene database, and matching them to query image features, as opposed to more conventional methods that match image features to visual words or database features. We find this approach results in improved performance, due to the richer knowledge of characteristics of the database features compared to query image features. We present experiments on two large city-scale photo collections, showing that our algorithm compares favorably to image retrieval-style approaches to location recognition."'),
('"Location-Based Information Support System Using Multiple Cameras and LED Light Sources with the Com', '"CVHCI 2004"', '["Information Support", "Optical Beam", "Multiple Camera", "Extrinsic Parameter", "World Coordinate ', '"https://doi.org/10.1007/978-3-540-24837-8_14"', '"To realize a location-based information support system, we propose an information support system using Compact Battery-less Information Terminals (CoBITs). This system consists of the information terminals CoBITs, and many environment system units. The environmental system unit plays an important role that to send particular information to respective users, and consists of multiple cameras for estimation of the position and orientation of the terminal, and LED light sources on the controllable pan-tilt heads for sending the optical beam information. For the application for information support in an event, a method for calibration of multiple cameras and light sources which is easy to use for inexperienced persons and to setup quickly is needed. To calibrate multiple cameras and LED light sources simultaneously, we employ a calibration technique for the multiple cameras based on the self-calibration method."'),
('"Log-Polar Stereo for Anthropomorphic Robots"', '"ECCV 2000"', '["Human Visual System", "Stereo Image", "Stereo Match", "Cortical Representation", "Stereoscopic Vis', '"https://doi.org/10.1007/3-540-45054-8_20"', '"Stereoscopic vision has a fundamental role both for animals and humans. Nonetheless, in the computer vision literature there is limited reference to biological models related to stereoscopic vision and, in particular, to the functional properties and the organization of binocular information within the visual cortex."'),
('"Long-Range Spatio-Temporal Modeling of Video with Application to Fire Detection"', '"ECCV 2012"', '["Ground Truth", "Video Sequence", "Detection Performance", "Anomaly Detection", "Equal Error Rate"]', '"https://doi.org/10.1007/978-3-642-33709-3_24"', '"We describe a methodology for modeling backgrounds subject to significant variability over time-scales ranging from days to years, where the events of interest exhibit subtle variability relative to the normal mode. The motivating application is fire monitoring from remote stations, where illumination changes spanning the day and the season, meteorological phenomena resembling smoke, and the absence of sufficient training data for the two classes make out-of-the-box classification algorithms ineffective. We exploit low-level descriptors, incorporate explicit modeling of nuisance variability, and learn the residual normal-model variability. Our algorithm achieves state-of-the-art performance not only compared to other anomaly detection schemes, but also compared to human performance, both for untrained and trained operators."'),
('"Loosely Distinctive Features for Robust Surface Alignment"', '"ECCV 2010"', '["Root Mean Square", "Feature Point", "Mixed Strategy", "Interest Point", "Evolutionary Stable Strat', '"https://doi.org/10.1007/978-3-642-15555-0_38"', '"Many successful feature detectors and descriptors exist for 2D intensity images. However, obtaining the same effectiveness in the domain of 3D objects has proven to be a more elusive goal. In fact, the smoothness often found in surfaces and the lack of texture information on the range images produced by conventional 3D scanners hinder both the localization of interesting points and the distinctiveness of their characterization in terms of descriptors. To overcome these limitations several approaches have been suggested, ranging from the simple enlargement of the area over which the descriptors are computed to the reliance on external texture information. In this paper we offer a change in perspective, where a game-theoretic matching technique that exploits global geometric consistency allows to obtain an extremely robust surface registration even when coupled with simple surface features exhibiting very low distinctiveness. In order to assess the performance of the whole approach we compare it with state-of-the-art alignment pipelines. Furthermore, we show that using the novel feature points with well-known alternative non-global matching techniques leads to poorer results."'),
('"Loss-Specific Training of Non-Parametric Image Restoration Models: A New State of the Art"', '"ECCV 2012"', '["Mean Square Error", "Loss Function", "Regression Tree", "Image Patch", "Conditional Random Field"]', '"https://doi.org/10.1007/978-3-642-33786-4_9"', '"After a decade of rapid progress in image denoising, recent methods seem to have reached a performance limit. Nonetheless, we find that state-of-the-art denoising methods are visually clearly distinguishable and possess complementary strengths and failure modes. Motivated by this observation, we introduce a powerful non-parametric image restoration framework based on Regression Tree Fields (RTF). Our restoration model is a densely-connected tractable conditional random field that leverages existing methods to produce an image-dependent, globally consistent prediction. We estimate the conditional structure and parameters of our model from training data so as to directly optimize for popular performance measures. In terms of peak signal-to-noise-ratio (PSNR), our model improves on the best published denoising method by at least 0.26dB across a range of noise levels. Our most practical variant still yields statistically significant improvements, yet is over 20\\u00d7 faster than the strongest competitor. Our approach is well-suited for many more image restoration and low-level vision problems, as evidenced by substantial gains in tasks such as removal of JPEG blocking artefacts."'),
('"Low-Rank Sparse Learning for Robust Visual Tracking"', '"ECCV 2012"', '["Sparse Representation", "Object Tracking", "Visual Tracking", "Template Size", "Background Templat', '"https://doi.org/10.1007/978-3-642-33783-3_34"', '"In this paper, we propose a new particle-filter based tracking algorithm that exploits the relationship between particles (candidate targets). By representing particles as sparse linear combinations of dictionary templates, this algorithm capitalizes on the inherent low-rank structure of particle representations that are learned jointly. As such, it casts the tracking problem as a low-rank matrix learning problem. This low-rank sparse tracker (LRST) has a number of attractive properties. (1) Since LRST adaptively updates dictionary templates, it can handle significant changes in appearance due to variations in illumination, pose, scale, etc. (2) The linear representation in LRST explicitly incorporates background templates in the dictionary and a sparse error term, which enables LRST to address the tracking drift problem and to be robust against occlusion respectively. (3) LRST is computationally attractive, since the low-rank learning problem can be efficiently solved as a sequence of closed form update operations, which yield a time complexity that is linear in the number of particles and the template size. We evaluate the performance of LRST by applying it to a set of challenging video sequences and comparing it to 6 popular tracking methods. Our experiments show that by representing particles jointly, LRST not only outperforms the state-of-the-art in tracking accuracy but also significantly improves the time complexity of methods that use a similar sparse linear representation model for particles [1]."'),
('"LSD-SLAM: Large-Scale Direct Monocular SLAM"', '"ECCV 2014"', '["Augmented Reality", "Image Alignment", "Visual Odometry", "Convergence Radius", "Inverse Depth"]', '"https://doi.org/10.1007/978-3-319-10605-2_54"', '"We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on \\\\(\\\\mathfrak{sim}(3)\\\\), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU."'),
('"LZM in Action: Realtime Face Recognition System"', '"ECCV 2012"', '["Face Recognition", "False Alarm Rate", "Face Detection", "Image Pyramid", "Face Tracking"]', '"https://doi.org/10.1007/978-3-642-33885-4_73"', '"In this technical demonstration, we introduce a real time face detection and recognition prototype. The proposed system can work with different image sources such as still images, videos from web cameras , and videos from ip cameras. The captured images are firstly processed by a cascaded classifier of Modified Census Transform (MCT) features to detect the faces. Then, facial features are detected inside the face region. These features are used to align and crop the face patches. Detection phase can be considerably improved by incorporating a tracking scheme to increase the hit rate while decreasing the false alarm rate. The registered faces are recognized using a novel method called Local Zernike Moments (LZM). A probabilistic decision step is employed in the final inference phase to provide a confidence margin. Introducing new identities via system\\u2019s user interface is considerably simple since the system does not require retraining after each new identity."'),
('"M2Tracker: A Multi-View Approach to Segmenting and Tracking People in a Cluttered Scene Using Regio', '"ECCV 2002"', '["Multi-camera Tracking", "Region-Based Stereo", "Grouping and Segmentation"]', '"https://doi.org/10.1007/3-540-47969-4_2"', '"We present a system that is capable of segmenting, detecting and tracking multiple people in a cluttered scene using multiple synchronized cameras located far from each other. The system improves upon existing systems in many ways including: (1) We do not assume that a foreground connected component belongs to only one object; rather, we segment the views taking into account color models for the objects and the background. This helps us to not only separate foreground regions belonging to different objects, but to also obtain better background regions than traditional background subtraction methods (as it uses foreground color models in the algorithm). (2) It is fully automatic and does not require any manual input or initializations of any kind. (3) Instead of taking decisions about object detection and tracking from a single view or camera pair, we collect evidences from each pair and combine the evidence to obtain a decision in the end. This helps us to obtain much better detection and tracking as opposed to traditional systems."'),
('"Machine Learning for High-Speed Corner Detection"', '"ECCV 2006"', '["Feature Detector", "Interest Point", "Corner Detection", "Segment Test", "Interest Point Detector"', '"https://doi.org/10.1007/11744023_34"', '"Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7% of the available processing time. By comparison neither the Harris detector (120%) nor the detection stage of SIFT (300%) can operate at full frame rate."'),
('"Making Action Recognition Robust to Occlusions and Viewpoint Changes"', '"ECCV 2010"', '["Recognition Rate", "Action Recognition", "Average Recognition Rate", "Viewpoint Change", "Local Pa', '"https://doi.org/10.1007/978-3-642-15558-1_46"', '"Most state-of-the-art approaches to action recognition rely on global representations either by concatenating local information in a long descriptor vector or by computing a single location independent histogram. This limits their performance in presence of occlusions and when running on multiple viewpoints. We propose a novel approach to providing robustness to both occlusions and viewpoint changes that yields significant improvements over existing techniques. At its heart is a local partitioning and hierarchical classification of the 3D Histogram of Oriented Gradients (HOG) descriptor to represent sequences of images that have been concatenated into a data volume. We achieve robustness to occlusions and viewpoint changes by combining training data from all viewpoints to train classifiers that estimate action labels independently over sets of HOG blocks. A top level classifier combines these local labels into a global action class decision."'),
('"Making Background Subtraction Robust to Sudden Illumination Changes"', '"ECCV 2008"', '["Input Image", "Augmented Reality", "Gaussian Mixture Model", "Background Subtraction", "Background', '"https://doi.org/10.1007/978-3-540-88693-8_42"', '"Modern background subtraction techniques can handle gradual illumination changes but can easily be confused by rapid ones. We propose a technique that overcomes this limitation by relying on a statistical model, not of the pixel intensities, but of the illumination effects. Because they tend to affect whole areas of the image as opposed to individual pixels, low-dimensional models are appropriate for this purpose and make our method extremely robust to illumination changes, whether slow or fast."'),
('"Manifold Learning for Object Tracking with Multiple Motion Dynamics"', '"ECCV 2010"', '["Gaussian Process", "Dynamic Texture", "Gaussian Process Regression", "Manifold Learn", "Nonlinear ', '"https://doi.org/10.1007/978-3-642-15558-1_13"', '"This paper presents a novel manifold learning approach for high dimensional data, with emphasis on the problem of motion tracking in video sequences. In this problem, the samples are time-ordered, providing additional information that most current methods do not take advantage of. Additionally, most methods assume that the manifold topology admits a single chart, which is overly restrictive. Instead, the algorithm can deal with arbitrary manifold topology by decomposing the manifold into multiple local models that are combined in a probabilistic fashion using Gaussian process regression. Thus, the algorithm is termed herein as Gaussian Process Multiple Local Models (GP\\u2013MLM)."'),
('"Manifold Statistics for Essential Matrices"', '"ECCV 2012"', '["Riemannian Manifold", "Tangent Vector", "Riemannian Geometry", "Riemannian Mapping", "Unit Quatern', '"https://doi.org/10.1007/978-3-642-33709-3_38"', '"Riemannian geometry allows for the generalization of statistics designed for Euclidean vector spaces to Riemannian manifolds. It has recently gained popularity within computer vision as many relevant parameter spaces have such a Riemannian manifold structure. Approaches which exploit this have been shown to exhibit improved efficiency and accuracy. The Riemannian logarithmic and exponential mappings are at the core of these approaches."'),
('"Manifold Valued Statistics, Exact Principal Geodesic Analysis and the Effect of Linear Approximatio', '"ECCV 2010"', '["Vertebral Fracture", "Tangent Space", "Hand Position", "Stereo Camera", "Human Skeleton"]', '"https://doi.org/10.1007/978-3-642-15567-3_4"', '"Manifolds are widely used to model non-linearity arising in a range of computer vision applications. This paper treats statistics on manifolds and the loss of accuracy occurring when linearizing the manifold prior to performing statistical operations. Using recent advances in manifold computations, we present a comparison between the non-linear analog of Principal Component Analysis, Principal Geodesic Analysis, in its linearized form and its exact counterpart that uses true intrinsic distances. We give examples of datasets for which the linearized version provides good approximations and for which it does not. Indicators for the differences between the two versions are then developed and applied to two examples of manifold valued data: outlines of vertebrae from a study of vertebral fractures and spacial coordinates of human skeleton end-effectors acquired using a stereo camera and tracking software."'),
('"Many-to-Many Feature Matching Using Spherical Coding of Directed Graphs"', '"ECCV 2004"', '["Feature Match", "Graph Match", "Normed Vector Space", "Graph Edit Distance", "Spherical Code"]', '"https://doi.org/10.1007/978-3-540-24670-1_25"', '"In recent work, we presented a framework for many-to-many matching of multi-scale feature hierarchies, in which features and their relations were captured in a vertex-labeled, edge-weighted directed graph. The algorithm was based on a metric-tree representation of labeled graphs and their metric embedding into normed vector spaces, using the embedding algorithm of Matousek [13]. However, the method was limited by the fact that two graphs to be matched were typically embedded into vector spaces with different dimensionality. Before the embeddings could be matched, a dimensionality reduction technique (PCA) was required, which was both costly and prone to error. In this paper, we introduce a more efficient embedding procedure based on a spherical coding of directed graphs. The advantage of this novel embedding technique is that it prescribes a single vector space into which both graphs are embedded. This reduces the problem of directed graph matching to the problem of geometric point matching, for which efficient many-to-many matching algorithms exist, such as the Earth Mover\\u2019s Distance. We apply the approach to the problem of multi-scale, view-based object recognition, in which an image is decomposed into a set of blobs and ridges with automatic scale selection."'),
('"MAP-Inference on Large Scale Higher-Order Discrete Graphical Models by Fusion Moves"', '"ECCV 2014"', '["Integer Linear Programming", "Fusion Algorithm", "Optimal Move", "Unary Term", "Proposal Generator', '"https://doi.org/10.1007/978-3-319-16181-5_37"', '"Many computer vision problems can be cast into optimization problems over discrete graphical models also known as Markov or conditional random fields. Standard methods are able to solve those problems quite efficiently. However, problems with huge label spaces and or higher-order structure remain challenging or intractable even for approximate methods."'),
('"Match Graph Construction for Large Image Databases"', '"ECCV 2012"', '["Image matching", "graph construction", "link prediction"]', '"https://doi.org/10.1007/978-3-642-33718-5_20"', '"How best to efficiently establish correspondence among a large set of images or video frames is an interesting unanswered question. For large databases, the high computational cost of performing pair-wise image matching is a major problem. However, for many applications, images are inherently sparsely connected, and so current techniques try to correctly estimate small potentially matching subsets of databases upon which to perform expensive pair-wise matching. Our contribution is to pose the identification of potential matches as a link prediction problem in an image correspondence graph, and to propose an effective algorithm to solve this problem. Our algorithm facilitates incremental image matching: initially, the match graph is very sparse, but it becomes dense as we alternate between link prediction and verification. We demonstrate the effectiveness of our algorithm by comparing it with several existing alternatives on large-scale databases. Our resulting match graph is useful for many different applications. As an example, we show the benefits of our graph construction method to a label propagation application which propagates user-provided sparse object labels to other instances of that object in large image collections."'),
('"Match Selection and Refinement for Highly Accurate Two-View Structure from Motion"', '"ECCV 2014"', '["Interest Point", "Fundamental Matrix", "Epipolar Line", "Structure From Motion", "Reprojection Err', '"https://doi.org/10.1007/978-3-319-10605-2_53"', '"We present an approach to enhance the accuracy of structure from motion (SfM) in the two-view case. We first answer the question: \\u201cfewer data with higher accuracy, or more data with less accuracy?\\u201d For this, we establish a relation between SfM errors and a function of the number of matches and their epipolar errors. Using an accuracy estimator of individual matches, we then propose a method to select a subset of matches that has a good quality vs. quantity compromise. We also propose a variant of least squares matching to refine match locations based on a focused grid and a multi-scale exploration. Experiments show that both selection and refinement contribute independently to a better accuracy. Their combination reduces errors by a factor of 1.1 to 2.0 for rotation, and 1.6 to 3.8 for translation."'),
('"Matching and Embedding through Edit-Union of Trees"', '"ECCV 2002"', '["Transitive Closure", "Edit Operation", "Isomorphism Problem", "Node Removal", "Graph Edit Distance', '"https://doi.org/10.1007/3-540-47977-5_54"', '"This paper investigates a technique to extend the tree edit distance framework to allow the simultaneous matching of multiple tree structures. This approach extends a previous result that showed the edit distance between two trees is completely determined by the maximum tree obtained from both tree with node removal operations only. In our approach we seek the minimum structure from which we can obtain the original trees with removal operations. This structure has the added advantage that it can be extended to more than two trees and it imposes consistency on node matches throughout the matched trees. Furthermore through this structure we can get a \\u201cnatural\\u201d embedding space of tree structures that can be used to analyze how tree representations vary in our problem domain."'),
('"Matching Distance Functions: A Shape-to-Area Variational Approach for Global-to-Local Registration"', '"ECCV 2002"', '["Local Deformation", "Active Contour", "Rigid Transformation", "Registration Result", "Input Shape"', '"https://doi.org/10.1007/3-540-47967-8_52"', '"This paper deals with the matching of geometric shapes. Our primary contribution is the use of a simple, robust, rich and efficient way to represent shapes, the level set representations according to singed distance transforms. Based on these representations we propose a variational framework for global as well as local shape registration that can be extended to deal with structures of higher dimension. The optimization criterion is invariant to rotation, translation and scale and combines efficiently a global motion model with local pixel-wise deformations. Promising results are obtained on examples showing small and large global deformations as well as arbitrary topological changes."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Matching Tensors for Automatic Correspondence and Registration"', '"ECCV 2004"', '["Iterate Close Point", "Mesh Resolution", "Model Tensor", "Fourth Order Tensor", "Triangular Facet"', '"https://doi.org/10.1007/978-3-540-24671-8_39"', '"Complete 3-D modeling of a free-form object requires acquisition from multiple view-points. These views are then required to be registered in a common coordinate system by establishing correspondence between them in their regions of overlap. In this paper, we present an automatic correspondence technique for pair-wise registration of different views of a free-form object. The technique is based upon a novel robust representation scheme reported in this paper. Our representation scheme defines local 3-D grids over the object\\u2019s surface and represents the surface inside each grid by a fourth order tensor. Multiple tensors are built for the views which are then matched, using a correlation and verification technique to establish correspondence between a model and a scene tensor. This correspondence is then used to derive a rigid transformation that aligns the two views. The transformation is verified and refined using a variant of ICP. Our correspondence technique is fully automatic and does not assume any knowledge of the viewpoints or regions of overlap of the data sets. Our results show that our technique is accurate, robust, efficient and independent of the resolution of the views."'),
('"MatchMiner: Efficient Spanning Structure Mining in Large Image Collections"', '"ECCV 2012"', '["Mutual Information", "Visual Word", "Image Pair", "Query Image", "Relevance Feedback"]', '"https://doi.org/10.1007/978-3-642-33709-3_4"', '"Many new computer vision applications are utilizing large-scale data- sets of places derived from the many billions of photos on the Web. Such applications often require knowledge of the visual connectivity structure of these image collections\\u2014describing which images overlap or are otherwise related\\u2014and an important step in understanding this structure is to identify connected components of this underlying image graph. As the structure of this graph is often initially unknown, this problem can be posed as one of exploring the connectivity between images as quickly as possible, by intelligently selecting a subset of image pairs for feature matching and geometric verification, without having to test all O(n2) possible pairs. We propose a novel, scalable algorithm called MatchMiner that efficiently explores visual relations between images, incorporating ideas from relevance feedback to improve decision making over time, as well as a simple yet effective rank distance measure for detecting outlier images. Using these ideas, our algorithm automatically prioritizes image pairs that can potentially connect or contribute to large connected components, using an information-theoretic algorithm to decide which image pairs to test next. Our experimental results show that MatchMiner can efficiently find connected components in large image collections, significantly outperforming state-of-the-art image matching methods."'),
('"Material Classification Based on Training Data Synthesized Using a BTF Database"', '"ECCV 2014"', '["Material classification", "material database", "reflectance", "texture synthesis"]', '"https://doi.org/10.1007/978-3-319-10578-9_11"', '"To cope with the richness in appearance variation found in real-world data under natural illumination, we propose to synthesize training data capturing these variations for material classification. Using synthetic training data created from separately acquired material and illumination characteristics allows to overcome the problems of existing material databases which only include a tiny fraction of the possible real-world conditions under controlled laboratory environments. However, it is essential to utilize a representation for material appearance which preserves fine details in the reflectance behavior of the digitized materials. As BRDFs are not sufficient for many materials due to the lack of modeling mesoscopic effects, we present a high-quality BTF database with 22,801 densely measured view-light configurations including surface geometry measurements for each of the 84 measured material samples. This representation is used to generate a database of synthesized images depicting the materials under different view-light conditions with their characteristic surface geometry using image-based lighting to simulate the complexity of real-world scenarios. We demonstrate that our synthesized data allows classifying materials under complex real-world scenarios."'),
('"Material Recognition for Efficient Acquisition of Geometry and Reflectance"', '"ECCV 2014"', '["Material recognition", "Reflectance", "Set-based classification"]', '"https://doi.org/10.1007/978-3-319-16199-0_23"', '"Typically, 3D geometry acquisition and reflectance acquisition techniques strongly rely on some basic assumptions about the surface reflectance behavior of the sample to be measured. Methods are tailored e.g. to Lambertian reflectance, mirroring reflectance, smooth and homogeneous surfaces or surfaces exhibiting mesoscopic effects. In this paper, we analyze whether multi-view material recognition can be performed robust enough to guide a subsequent acquisition process by reliably recognizing a certain material in a database with its respective annotation regarding the reconstruction methods to be chosen. This allows selecting the appropriate geometry/reflectance reconstruction approaches and, hence, increasing the efficiency of the acquisition process. In particular, we demonstrate that considering only a few view-light configurations is sufficient for obtaining high recognition scores."'),
('"Max-Margin Dictionary Learning for Multiclass Image Categorization"', '"ECCV 2010"', '["Visual Word", "Sparse Code", "Dictionary Learning", "Hinge Loss", "Binary Problem"]', '"https://doi.org/10.1007/978-3-642-15561-1_12"', '"Visual dictionary learning and base (binary) classifier training are two basic problems for the recently most popular image categorization framework, which is based on the bag-of-visual-terms (BOV) models and multiclass SVM classifiers. In this paper, we study new algorithms to improve performance of this framework from these two aspects. Typically SVM classifiers are trained with dictionaries fixed, and as a result the traditional loss function can only be minimized with respect to hyperplane parameters (w and b). We propose a novel loss function for a binary classifier, which links the hinge-loss term with dictionary learning. By doing so, we can further optimize the loss function with respect to the dictionary parameters. Thus, this framework is able to further increase margins of binary classifiers, and consequently decrease the error bound of the aggregated classifier. On two benchmark dataset, Graz [1] and the fifteen scene category dataset [2], our experiment results significantly outperformed state-of-the-art works."'),
('"Maximally Stable Local Description for Scale Selection"', '"ECCV 2006"', '["Scale Invariant Feature Transform", "Image Match", "Rotation Invariance", "Correct Match", "Moment', '"https://doi.org/10.1007/11744085_39"', '"Scale and affine-invariant local features have shown excellent performance in image matching, object and texture recognition. This paper optimizes keypoint detection to achieve stable local descriptors, and therefore, an improved image representation. The technique performs scale selection based on a region descriptor, here SIFT, and chooses regions for which this descriptor is maximally stable. Maximal stability is obtained, when the difference between descriptors extracted for consecutive scales reaches a minimum. This scale selection technique is applied to multi-scale Harris and Laplacian points. Affine invariance is achieved by an integrated affine adaptation process based on the second moment matrix. An experimental evaluation compares our detectors to Harris-Laplace and the Laplacian in the context of image matching as well as of category and texture classification. The comparison shows the improved performance of our detector."'),
('"Maximizing Rigidity: Optimal Matching under Scaled-Orthography"', '"ECCV 2002"', '["Optimal Match", "Point Correspondence", "Correspondence Problem", "Outlier Rejection", "Epipolar C', '"https://doi.org/10.1007/3-540-47967-8_16"', '"Establishing point correspondences between images is a key step for 3D-shape computation. Nevertheless, shape extraction and point correspondence are treated, usually, as two different computational processes. We propose a new method for solving the correspondence problem between points of a fully uncalibrated scaled-orthographic image sequence. Among all possible point selections and permutations, our method chooses the one that minimizes the fourth singular value of the observation matrix in the factorization method. This way, correspondences are set such that shape and motion computation are optimal. Furthermore, we show this is an optimal criterion under bounded noise conditions."'),
('"Maximum Margin Distance Learning for Dynamic Texture Recognition"', '"ECCV 2010"', '["Recognition Rate", "Local Binary Pattern", "Distance Constraint", "Linear Dynamical System", "Text', '"https://doi.org/10.1007/978-3-642-15552-9_17"', '"The range space of dynamic textures spans spatiotemporal phenomena that vary along three fundamental dimensions: spatial texture, spatial texture layout, and dynamics. By describing each dimension with appropriate spatial or temporal features and by equipping it with a suitable distance measure, elementary distances (one for each dimension) between dynamic texture sequences can be computed. In this paper, we address the problem of dynamic texture (DT) recognition by learning linear combinations of these elementary distances. By learning weights to these distances, we shed light on how \\u201csalient\\u201d (in a discriminative manner) each DT dimension is in representing classes of dynamic textures. To do this, we propose an efficient maximum margin distance learning (MMDL) method based on the Pegasos algorithm [1], for both class-independent and class-dependent weight learning. In contrast to popular MMDL methods, which enforce restrictive distance constraints and have a computational complexity that is cubic in the number of training samples, we show that our method, called DL-PEGASOS, can handle more general distance constraints with a computational complexity that can be made linear. When class dependent weights are learned, we show that, for certain classes of DTs , spatial texture features are dominantly \\u201csalient\\u201d, while for other classes, this \\u201csaliency\\u201d lies in their temporal features. Furthermore, DL-PEGASOS outperforms state-of-the-art recognition methods on the UCLA benchmark DT dataset. By learning class independent weights, we show that this benchmark does not offer much variety along the three DT dimensions, thus, motivating the proposal of a new DT dataset, called DynTex++."'),
('"MCMC-Based Multiview Reconstruction of Piecewise Smooth Subdivision Curves with a Variable Number o', '"ECCV 2004"', '["Control Point", "Markov Chain Monte Carlo", "Markov Chain Monte Carlo Algorithm", "Reversible Jump', '"https://doi.org/10.1007/978-3-540-24672-5_26"', '"We investigate the automated reconstruction of piecewise smooth 3D curves, using subdivision curves as a simple but flexible curve representation. This representation allows tagging corners to model non-smooth features along otherwise smooth curves. We present a reversible jump Markov chain Monte Carlo approach which obtains an approximate posterior distribution over the number of control points and tags. In a Rao-Blackwellization scheme, we integrate out the control point locations, reducing the variance of the resulting sampler. We apply this general methodology to the reconstruction of piecewise smooth curves from multiple calibrated views, in which the object is segmented from the background using a Markov random field approach. Results are shown for multiple images of two pot shards as would be encountered in archaeological applications."'),
('"Mean-Shift Blob Tracking with Kernel-Color Distribution Estimate and Adaptive Model Update Criterio', '"SMVP 2004"', '["Kalman Filter", "Current Frame", "Appearance Change", "Frame Index", "Object Appearance"]', '"https://doi.org/10.1007/978-3-540-30212-4_8"', '"We propose an adaptive model update mechanism for mean-shift blob tracking. It is novel for us to use self-tuning Kalman filters for estimating object kernel-color distribution, i.e. kernel-histogram. Filtering residuals are employed for hypothesis testing in order to obtain a robust criterion for model update. Therefore, tracker has the ability to keep up with the changes of object appearance as well as the changes in scale. Moreover, over-update is avoided in the cases of severe occlusion and dramatic appearance changes. Various tracking sequences demonstrate the superior behavior of our tracker which runs in real-time with non-parameter initialization and is robust to appearance changes."'),
('"Measuring Image Distances via Embedding in a Semantic Manifold"', '"ECCV 2012"', '["Support Vector Machine", "Random Walk", "Near Neighbor", "Target Node", "Semantic Distance"]', '"https://doi.org/10.1007/978-3-642-33765-9_29"', '"In this work we introduce novel image metrics that can be used with distance-based classifiers or directly to decide whether two input images belong to the same class. While most prior image distances rely purely on comparisons of low-level features extracted from the inputs, our metrics use a large database of labeled photos as auxiliary data to draw semantic relationships between the two images, beyond those computable from simple visual features. In a preprocessing stage our approach derives a semantic image graph from the labeled dataset, where the nodes are the labeled images and the edges connect pictures with related labels. The graph can be viewed as modeling a semantic image manifold, and it enables the use of graph distances to approximate semantic distances. Thus, we reformulate the task of measuring the semantic distance between two unlabeled pictures as the problem of embedding the two input images in the semantic graph. We propose and evaluate several embedding schemes and graph distance metrics. Our results on Caltech101, Caltech256 and ImageNet show that our distances consistently match or outperform the state-of-the-art in this field."'),
('"Measuring the Self-Consistency of Stereo Algorithms"', '"ECCV 2000"', '["Window Size", "Mahalanobis Distance", "Scatter Diagram", "Camera Parameter", "Stereo Match"]', '"https://doi.org/10.1007/3-540-45054-8_19"', '"A new approach to characterizing the performance of point-correspondence algorithms is presented. Instead of relying on any \\u201cground truth\\u2019, it uses the self-consistency of the outputs of an algorithm independently applied to different sets of views of a static scene. It allows one to evaluate algorithms for a given class of scenes, as well as to estimate the accuracy of every element of the output of the algorithm for a given set of views. Experiments to demonstrate the usefulness of the methodology are presented."'),
('"Measuring Uncertainty in Graph Cut Solutions \\u2013 Efficiently Computing Min-marginal Energies Usi', '"ECCV 2006"', '["Energy Function", "Image Segmentation", "Markov Random Field", "Graph Construction", "Multiple Lab', '"https://doi.org/10.1007/11744047_3"', '"In recent years the use of graph-cuts has become quite popular in computer vision. However, researchers have repeatedly asked the question whether it might be possible to compute a measure of uncertainty associated with the graph-cut solutions. In this paper we answer this particular question by showing how the min-marginals associated with the label assignments in a MRF can be efficiently computed using a new algorithm based on dynamic graph cuts. We start by reporting the discovery of a novel relationship between the min-marginal energy corresponding to a latent variable label assignment, and the flow potentials of the node representing that variable in the graph used in the energy minimization procedure. We then proceed to show how the min-marginal energy can be computed by minimizing a projection of the energy function defined by the MRF. We propose a fast and novel algorithm based on dynamic graph cuts to efficiently minimize these energy projections. The min-marginal energies obtained by our proposed algorithm are exact, as opposed to the ones obtained from other inference algorithms like loopy belief propagation and generalized belief propagation. We conclude by showing how min-marginals can be used to compute a confidence measure for label assignments in labelling problems such as image segmentation."'),
('"MEEM: Robust Tracking via Multiple Experts Using Entropy Minimization"', '"ECCV 2014"', '["Image Patch", "Entropy Minimization", "Robust Tracking", "Multiple Instance Learning", "Multiple E', '"https://doi.org/10.1007/978-3-319-10599-4_13"', '"We propose a multi-expert restoration scheme to address the model drift problem in online tracking. In the proposed scheme, a tracker and its historical snapshots constitute an expert ensemble, where the best expert is selected to restore the current tracker when needed based on a minimum entropy criterion, so as to correct undesirable model updates. The base tracker in our formulation exploits an online SVM on a budget algorithm and an explicit feature mapping method for efficient model update and inference. In experiments, our tracking method achieves substantially better overall performance than 32 trackers on a benchmark dataset of 50 video sequences under various evaluation settings. In addition, in experiments with a newly collected dataset of challenging sequences, we show that the proposed multi-expert restoration scheme significantly improves the robustness of our base tracker, especially in scenarios with frequent occlusions and repetitive appearance variations."'),
('"Melanoma Recognition Using Representative and Discriminative Kernel Classifiers"', '"CVAMIA 2006"', '["Support Vector Machine", "Recognition Rate", "Color Histogram", "Evidence Theory", "Dysplastic Les', '"https://doi.org/10.1007/11889762_1"', '"Malignant melanoma is the most deadly form of skin lesion. Early diagnosis is of critical importance to patient survival. Existent visual recognition algorithms for skin lesions classification focus mostly on segmentation and feature extraction. In this paper instead we put the emphasis on the learning process by using two kernel-based classifiers. We chose a discriminative approach using support vector machines, and a probabilistic approach using spin glass-Markov random fields. We benchmarked these algorithms against the (to our knowledge) state-of-the-art method on melanoma recognition, exploring how performance changes by using color or textural features, and how it is affected by the quality of the segmentation mask. We show with extensive experiments that the support vector machine approach outperforms the existing method and, on two classes out of three, it achieves performances comparable to those obtained by expert clinicians."'),
('"Membrane Nonrigid Image Registration"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15552-9_55"', '"We introduce a novel nonrigid 2D image registration method that establishes dense and accurate correspondences across images without the need of any manual intervention. Our key insight is to model the image as a membrane, i.e., a thin 3D surface, and to constrain its deformation based on its geometric properties. To do so, we derive a novel Bayesian formulation. We impose priors on the moving membrane which act to preserve its shape as it deforms to meet the target.We derive these as curvature weighted first and second order derivatives that correspond to the changes in stretching and bending potential energies of the membrane and estimate the registration as the maximum a posteriori. Experimental results on real data demonstrate the effectiveness of our method, in particular, its robustness to local minima and its ability to establish accurate correspondences across the entire image. The results clearly show that our method overcomes the shortcomings of previous intensity-based and feature-based approaches with conventional uniform smoothing or diffeomorphic constraints that suffer from large errors in textureless regions and in areas in-between specified features."'),
('"Memory-Based Particle Filter for Tracking Objects with Large Variation in Pose and Appearance"', '"ECCV 2010"', '["Posterior Distribution", "Prior Distribution", "Interest Point", "Appearance Model", "Robust Track', '"https://doi.org/10.1007/978-3-642-15558-1_16"', '"A novel memory-based particle filter is proposed to achieve robust visual tracking of a target\\u2019s pose even with large variations in target\\u2019s position and rotation, i.e. large appearance changes. The memory-based particle filter (M-PF) is a recent extension of the particle filter, and incorporates a memory-based mechanism to predict prior distribution using past memory of target state sequence; it offers robust target tracking against complex motion. This paper extends the M-PF to a unified probabilistic framework for joint estimation of the target\\u2019s pose and appearance based on memory-based joint prior prediction using stored past pose and appearance sequences. We call it the Memory-based Particle Filter with Appearance Prediction (M-PFAP). A memory-based approach enables generating the joint prior distribution of pose and appearance without explicit modeling of the complex relationship between them. M-PFAP can robustly handle the large changes in appearance caused by large pose variation, in addition to abrupt changes in moving direction; it allows robust tracking under self and mutual occlusion. Experiments confirm that M-PFAP successfully tracks human faces from frontal view to profile view; it greatly eases the limitations of M-PF."'),
('"Mental Characteristics of Person as Basic Biometrics"', '"BioAW 2002"', '["Human Resource Management", "Mental Characteristic", "Mental Ability", "Massive Experiment", "Biom', '"https://doi.org/10.1007/3-540-47917-1_9"', '"On basis of the appropriate, interdisciplinary, scientific and cognitive knowledge as well as the special massive experiment result, it has been confirmed an existence of a neuro-psychological mechanism concerning the appearance of certain positive/negative emotions while a person perceives a particular plan-geometrical figure. These emotions appear subconsciously, and certain types of reflexes/responses accompany them, that is, the particular figure from the special set is a stable mental trace of some features of one\\u2019s personality. To find some stable mental characteristics for person\\u2019s identification/authentication, it has been researched some advanced properties as well as modernized S. Dellinger\\u2019s psycho-geometrical method, and also created new method for testing of mental abilities with accent on a person\\u2019s manner of processing special knowledge (not data) called Kotelnikov-Shannon_Theorem_Test."'),
('"Metric Learning for Large Scale Image Classification: Generalizing to New Classes at Near-Zero Cost', '"ECCV 2012"', '["Training Image", "Query Image", "Reference Class", "Transfer Learning", "Large Scale Image"]', '"https://doi.org/10.1007/978-3-642-33709-3_35"', '"We are interested in large-scale image classification and especially in the setting where images corresponding to new or existing classes are continuously added to the training set. Our goal is to devise classifiers which can incorporate such images and classes on-the-fly at (near) zero cost. We cast this problem into one of learning a metric which is shared across all classes and explore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers. We learn metrics on the ImageNet 2010 challenge data set, which contains more than 1.2M training images of 1K classes. Surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier, and has comparable performance to linear SVMs. We also study the generalization performance, among others by using the learned metric on the ImageNet-10K dataset, and we obtain competitive performance. Finally, we explore zero-shot classification, and show how the zero-shot model can be combined very effectively with small training datasets."'),
('"Metric-Based Pairwise and Multiple Image Registration"', '"ECCV 2014"', '["metric-based registration", "elastic image deformation", "post-registration analysis", "mean image', '"https://doi.org/10.1007/978-3-319-10605-2_16"', '"Registering pairs or groups of images is a widely-studied problem that has seen a variety of solutions in recent years. Most of these solutions are variational, using objective functions that should satisfy several basic and desired properties. In this paper, we pursue two additional properties \\u2013 (1) invariance of objective function under identical warping of input images and (2) the objective function induces a proper metric on the set of equivalence classes of images \\u2013 and motivate their importance. Then, a registration framework that satisfies these properties, using the L 2-norm between a novel representation of images, is introduced. Additionally, for multiple images, the induced metric enables us to compute a mean image, or a template, and perform joint registration. We demonstrate this framework using examples from a variety of image types and compare performances with some recent methods."'),
('"Micro-Expression Recognition Using Robust Principal Component Analysis and Local Spatiotemporal Dir', '"ECCV 2014"', '["Micro-expression recognition", "Sparse representation", "Dynamic features", "Local binary pattern"', '"https://doi.org/10.1007/978-3-319-16178-5_23"', '"One of important cues of deception detection is micro-expression. It has three characteristics: short duration, low intensity and usually local movements. These characteristics imply that micro-expression is sparse. In this paper, we use the sparse part of Robust PCA (RPCA) to extract the subtle motion information of micro-expression. The local texture features of the information are extracted by Local Spatiotemporal Directional Features (LSTD). In order to extract more effective local features, 16 Regions of Interest (ROIs) are assigned based on the Facial Action Coding System (FACS). The experimental results on two micro-expression databases show the proposed method gain better performance. Moreover, the proposed method may further be used to extract other subtle motion information (such as lip-reading, the human pulse, and micro-gesture etc.) from video."'),
('"Micro-Facial Movements: An Investigation on Spatio-Temporal Descriptors"', '"ECCV 2014"', '["Micro-movement detection", "Facial analysis", "Random forests", "Support vector machines"]', '"https://doi.org/10.1007/978-3-319-16181-5_8"', '"This paper aims to investigate whether micro-facial movement sequences can be distinguished from neutral face sequences. As a micro-facial movement tends to be very quick and subtle, classifying when a movement occurs compared to the face without movement can be a challenging computer vision problem. Using local binary patterns on three orthogonal planes and Gaussian derivatives, local features, when interpreted by machine learning algorithms, can accurately describe when a movement and non-movement occurs. This method can then be applied to help aid humans in detecting when the small movements occur. This also differs from current literature as most only concentrate in emotional expression recognition. Using the CASME II dataset, the results from the investigation of different descriptors have shown a higher accuracy compared to state-of-the-art methods."'),
('"Microsoft COCO: Common Objects in Context"', '"ECCV 2014"', '["Object Detection", "Common Object", "Object Category", "Object Instance", "Scene Understanding"]', '"https://doi.org/10.1007/978-3-319-10602-1_48"', '"We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."'),
('"Middle-Level Representation for Human Activities Recognition: The Role of Spatio-Temporal Relations', '"ECCV 2010"', '["Computer Vision", "Video Sequence", "Activity Recognition", "Motion Descriptor", "Consistent Motio', '"https://doi.org/10.1007/978-3-642-35749-7_13"', '"We tackle the challenging problem of human activity recognition in realistic video sequences. Unlike local features-based methods or global template-based methods, we propose to represent a video sequence by a set of middle-level parts. A part, or component, has consistent spatial structure and consistent motion. We first segment the visual motion patterns and generate a set of middle-level components by clustering keypoints-based trajectories extracted from the video. To further exploit the interdependencies of the moving parts, we then define spatio-temporal relationships between pairwise components. The resulting descriptive middle-level components and pairwise-components thereby catch the essential motion characteristics of human activities. They also give a very compact representation of the video. We apply our framework on popular and challenging video datasets: Weizmann dataset and UT-Interaction dataset. We demonstrate experimentally that our middle-level representation combined with a \\u03c7 2-SVM classifier equals to or outperforms the state-of-the-art results on these dataset."'),
('"MIForests: Multiple-Instance Learning with Randomized Trees"', '"ECCV 2010"', '["Random Forest", "Multiple Instance Learning", "Instance Label", "Deterministic Annealing", "Label ', '"https://doi.org/10.1007/978-3-642-15567-3_3"', '"Multiple-instance learning (MIL) allows for training classifiers from ambiguously labeled data. In computer vision, this learning paradigm has been recently used in many applications such as object classification, detection and tracking. This paper presents a novel multiple-instance learning algorithm for randomized trees called MIForests. Randomized trees are fast, inherently parallel and multi-class and are thus increasingly popular in computer vision. MIForest combine the advantages of these classifiers with the flexibility of multiple instance learning. In order to leverage the randomized trees for MIL, we define the hidden class labels inside target bags as random variables. These random variables are optimized by training random forests and using a fast iterative homotopy method for solving the non-convex optimization problem. Additionally, most previously proposed MIL approaches operate in batch or off-line mode and thus assume access to the entire training set. This limits their applicability in scenarios where the data arrives sequentially and in dynamic environments. We show that MIForests are not limited to off-line problems and present an on-line extension of our approach. In the experiments, we evaluate MIForests on standard visual MIL benchmark datasets where we achieve state-of-the-art results while being faster than previous approaches and being able to inherently solve multi-class problems. The on-line version of MIForests is evaluated on visual object tracking where we outperform the state-of-the-art method based on boosting."'),
('"Min-Space Integral Histogram"', '"ECCV 2012"', '["Integral histogram", "multiple histogram computation"]', '"https://doi.org/10.1007/978-3-642-33709-3_14"', '"In this paper, we present a new approach for quickly computing the histograms of a set of unrotating rectangular regions. Although it is related to the well-known Integral Histogram (IH), our approach significantly outperforms it, both in terms of memory requirements and of response times. By preprocessing the region of interest (ROI) computing and storing a temporary histogram for each of its pixels, IH is effective only when a large amount of histograms located in a small ROI need be computed by the user. Unlike IH, our approach, called Min-Space Integral Histogram, only computes and stores those temporary histograms that are strictly necessary (less than 4 times the number of regions). Comparative tests highlight its efficiency, which can be up to 75 times faster than IH. In particular, we show that our approach is much less sensitive than IH to histogram quantization and to the size of the ROI."'),
('"Minimal Correlation Classification"', '"ECCV 2012"', '["Local Binary Pattern", "Ensemble Method", "Minimal Correlation", "Decision Stump", "Correlation En', '"https://doi.org/10.1007/978-3-642-33783-3_3"', '"When the description of the visual data is rich and consists of many features, a classification based on a single model can often be enhanced using an ensemble of models. We suggest a new ensemble learning method that encourages the base classifiers to learn different aspects of the data. Initially, a binary classification algorithm such as Support Vector Machine is applied and its confidence values on the training set are considered. Following the idea that ensemble methods work best when the classification errors of the base classifiers are not related, we serially learn additional classifiers whose output confidences on the training examples are minimally correlated. Finally, these uncorrelated classifiers are assembled using the GentleBoost algorithm. Presented experiments in various visual recognition domains demonstrate the effectiveness of the method."'),
('"Minimal Paths in 3D Images and Application to Virtual Endoscopy"', '"ECCV 2000"', '["Deformable Models", "Minimal paths", "Level Set methods", "Medical image understanding", "Eikonal ', '"https://doi.org/10.1007/3-540-45053-X_35"', '"This paper presents a new method to find minimal paths in 3D images, giving as initial data one or two endpoints. This is based on previous work [1] for extracting paths in 2D images using Fast Marching [4]. Our original contribution is to extend this technique to 3D, and give new improvements of the approach that are relevant in 2D as well as in 3D. We also introduce several methods to reduce the computation cost and the user interaction. This work finds its motivation in the particular case of 3D medical images. We show that this technique can be efficiently applied to the problem of finding a centered path in tubular anatomical structures with minimum interactivity, and we apply it to path construction for virtual endoscopy. Synthetic and real medical images are used to illustrate each contribution."'),
('"Minimal Surfaces for Stereo"', '"ECCV 2002"', '["Minimal Surface", "Uniqueness Constraint", "Dual Graph", "Primal Graph", "Spatial Complex"]', '"https://doi.org/10.1007/3-540-47977-5_58"', '"Determining shape from stereo has often been posed as a global minimization problem. Once formulated, the minimization problems are then solved with a variety of algorithmic approaches. These approaches include techniques such as dynamic programming min-cut and alpha-expansion. In this paper we show how an algorithmic technique that constructs a discrete spatial minimal cost surface can be brought to bear on stereo global minimization problems. This problem can then be reduced to a single min-cut problem. We use this approach to solve a new global minimization problem that naturally arises when solving for three-camera (trinocular) stereo. Our formulation treats the three cameras symmetrically, while imposing a natural occlusion cost and uniqueness constraint."'),
('"Mixed-Resolution Patch-Matching"', '"ECCV 2012"', '["Exhaustive Search", "Search Range", "Resolution Level", "Texture Synthesis", "Locality Sensitive H', '"https://doi.org/10.1007/978-3-642-33783-3_14"', '"Matching patches of a source image with patches of itself or a target image is a first step for many operations. Finding the optimum nearest-neighbors of each patch using a global search of the image is expensive. Optimality is often sacrificed for speed as a result. We present the Mixed-Resolution Patch-Matching (MRPM) algorithm that uses a pyramid representation to perform fast global search. We compare mixed-resolution patches at coarser pyramid levels to alleviate the effects of smoothing. We store more matches at coarser resolutions to ensure wider search ranges and better accuracy at finer levels. Our method achieves near optimality in terms of average error compared to exhaustive search. Our approach is simple compared to complex trees or hash tables used by others. This enables fast parallel implementations on the GPU, yielding upto 70\\u00d7 speedup compared to other iterative approaches. Our approach is best suited when multiple, global matches are needed."'),
('"Mixing Low-Level and Semantic Features for Image Interpretation"', '"ECCV 2014"', '["Computer vision", "Ontologies", "Semantic image interpretation"]', '"https://doi.org/10.1007/978-3-319-16181-5_20"', '"Semantic Content-Based Image Retrieval (SCBIR) allows users to retrieve images via complex expressions of some ontological language describing a domain of interest. SCBIR adds some flexibility to the state-of-the-art methods for image retrieval, which support query either by keywords or by image examples. The price for this additional flexibility is the generation of a semantically rich description of the image content reflecting the ontology constraints. Generating these semantic interpretations is an open research problem. This paper contributes to this research line by proposing an approach for SCBIR based on the somehow natural idea that the interpretation of a picture is an (onto) logical model of an ontology that describes the domain of the picture. We implement this idea in an unsupervised method that jointly exploits the ontological constraints and the low-level features of the image. The preliminary evaluation, presented in the paper, shows promising results."'),
('"Mixture Component Identification and Learning for Visual Recognition"', '"ECCV 2012"', '["Feature Selection", "Mixture Component", "Spectral Cluster", "Visual Similarity", "Visual Recognit', '"https://doi.org/10.1007/978-3-642-33783-3_9"', '"The non-linear decision boundary between object and background classes - due to large intra-class variations - needs to be modelled by any classifier wishing to achieve good results. While a mixture of linear classifiers is capable of modelling this non-linearity, learning this mixture from weakly annotated data is non-trivial and is the paper\\u2019s focus. Our approach is to identify the modes in the distribution of our positive examples by clustering, and to utilize this clustering in a latent SVM formulation to learn the mixture model. The clustering relies on a robust measure of visual similarity which suppresses uninformative clutter by using a novel representation based on the exemplar SVM. This subtle clustering of the data leads to learning better mixture models, as is demonstrated via extensive evaluations on Pascal VOC 2007. The final classifier, using a HOG representation of the global image patch, achieves performance comparable to the state-of-the-art while being more efficient at detection time."'),
('"Mixture of Heterogeneous Attribute Analyzers for Human Action Detection"', '"ECCV 2014"', '["Human trajectory", "Key pose", "Local dense trajectories", "Discriminative mining", "Latent struct', '"https://doi.org/10.1007/978-3-319-16178-5_37"', '"We propose a human action detection framework called \\u201cmixture of heterogeneous attribute analyzer\\u201d. This framework integrates heterogeneous attributes learned from static and dynamic, local and global video features, to boost the action detection performance. To this end, we first detect and track multiple people by SVM-HOG detector and tracklet generation. Multiple short human tracklets are then linked into long trajectories by spatio-temporal matching. Human key poses and local dense motion trajectories are then extracted within the tracked human bounding box sequences. Second, we propose a mining method to learn discriminative attributes from these three feature modalities: human bounding box trajectory, key pose and local dense motion trajectories. Finally, the learned discriminative attributes are integrated in a latent structural max-margin learning framework which also explores the spatio-temporal relationship between heterogeneous feature attributes. Experiments on the ChaLearn 2014 human action dataset demonstrate the superior detection performance of the proposed framework."'),
('"Mobile Panoramic Vision for Assisting the Blind via Indexing and Localization"', '"ECCV 2014"', '["Panoramic vision", "Mobile computing", "Cloud computing", "Blind navigation"]', '"https://doi.org/10.1007/978-3-319-16199-0_42"', '"In this paper, we propose a first-person localization and navigation system for helping blind and visually-impaired people navigate in indoor environments. The system consists of a mobile vision front end with a portable panoramic lens mounted on a smart phone, and a remote GPU-enabled server. Compact and effective omnidirectional video features are extracted and represented in the smart phone front end, and then transmitted to the server, where the features of an input image or a short video clip are used to search a database of an indoor environment via image-based indexing to find both the location and the orientation of the current view. To deal with the high computational cost in searching a large database for a realistic navigation application, data parallelism and task parallelism properties are identified in database indexing, and computation is accelerated by using multi-core CPUs and GPUs. Experiments on synthetic data and real data are carried out to demonstrate the capacity of the proposed system, with respect to real-time response and robustness."'),
('"Mobile Product Image Search by Automatic Query Object Extraction"', '"ECCV 2012"', '["Visual Word", "Query Image", "Query Expansion", "Retrieval Accuracy", "Background Clutter"]', '"https://doi.org/10.1007/978-3-642-33765-9_9"', '"Mobile product image search aims at identifying a product, or retrieving similar products from a database based on a photo captured from a mobile phone camera. Application of traditional image retrieval methods (e.g. bag-of-words) to mobile visual search has been shown to be effective in identifying duplicate/near-duplicate photos, near-planar and textured objects such as landmarks, books/cd covers. However, retrieving more general product categories is still a challenging research problem due to variations in viewpoint, illumination, scale, the existence of blur and background clutter in the query image, etc. In this paper, we propose a new approach that can simultaneously extract the product instance from the query, identify the instance, and retrieve visually similar product images. Based on the observation that good query segmentation helps improve retrieval accuracy and good search results provide good priors for segmentation, we formulate our approach in an iterative scheme to improve both query segmentation and retrieval accuracy. To this end, a weighted object mask voting algorithm is proposed based on a spatially-constrained model, which allows robust localization and segmentation of the query object, and achieves significantly better retrieval accuracy than previous methods. We show the effectiveness of our approach by applying it to a large, real-world product image dataset and a new object category dataset."'),
('"Mode Seeking with an Adaptive Distance Measure"', '"ECCV 2012"', '["Mean Shift Algorithm", "Metric Learning"]', '"https://doi.org/10.1007/978-3-642-33885-4_22"', '"The mean shift algorithm is a widely used non-parametric clustering algorithm. It has been extended to cluster a mixture of linear subspaces for solving problems in computer vision such as multi-body motion segmentation, etc. Existing methods only work with a set of subspaces, which are computed from samples of observations. However, noises from observations can distort these subspace estimates and influence clustering accuracy. We propose to use both subspaces and observations to improve performance. Furthermore, while these mean shift methods use fixed metrics for computing distances, we prefer an adaptive distance measure. The insight is, we can use temporary modes in a mode seeking process to improve this measure and obtain better performance. In this paper, an adaptive mode seeking algorithm is proposed for clustering linear subspaces. By experiments, the proposed algorithm compares favorably to the state-of-the-art algorithm in terms of clustering accuracy."'),
('"Mode-Driven Volume Analysis Based on Correlation of Time Series"', '"ECCV 2014"', '["Action Recognition", "Canonical Correlation Analysis", "Adjacent Frame", "Human Action Recognition', '"https://doi.org/10.1007/978-3-319-16178-5_57"', '"Tensor analysis is widely used for face recognition and action recognition. In this paper, a mode-driven discriminant analysis (MDA) in tensor subspace is proposed for visual recognition. For training, we treat each sample as an N-order tensor, of which the first N-1 modes capture the spatial information of images while the N-th mode captures the sequential patterns of images. We employ Fisher criteria on the first N-1 modes to extract discriminative features of the visual information. After that, considering the correlation of adjacent frames in the sequence, i.e., the current frame and its former and latter ones, we update the sequence by calculating the correlation of triple adjacent frames, then perform discriminant analysis on the N-th mode. The alternating projection procedure of MDA converges and is convex with different initial values of the transformation matrices. Such hybrid tensor subspace learning scheme may sufficiently preserve both discrete and continuous distributions information of action videos in lower dimensional spaces to boost discriminant power. Experiments on the MSR action 3D database, KTH database and ETH database showed that our algorithm MDA outperformed other tensor-based methods in terms of accuracy and is competitive considering the time efficiency. Besides, it is robust to deal with the damaged and self-occluded action silhouettes and RGB object images in various viewing angles."'),
('"Model Acquisition by Registration of Multiple Acoustic Range Views"', '"ECCV 2002"', '["Range Image", "Registration Error", "Median Absolute Deviation", "Rigid Transformation", "Iterate ', '"https://doi.org/10.1007/3-540-47967-8_54"', '"This paper deals with the three-dimensional reconstruction of an underwater environment from multiple acoustic range views acquired by a remotely operated vehicle. The problem is made challenging by the very noisy nature of the data, the low resolution and the narrow field of view of the sensor. Our contribution is twofold: first, we introduce a statistically sound thresholding (the X84 rejection rule) to improve ICP robustness against noise and non-overlapping data. Second, we propose a new global registration technique to distribute registration errors evenly across all views. Our approach does not use data points after the first pairwise registration, for it works only on the transformations. Therefore, it is fast and occupies only a small memory. Experimental results suggest that ICP with X84 performs better than Zhang\\u2019s ICP, and that the global registration technique is effective in reducing and equalizing the error."'),
('"Model Based Pose Estimator Using Linear-Programming"', '"ECCV 2000"', '["Maximum Likelihood Estimator", "Covariance Matrice", "Robust Estimator", "Stereo Pair", "Breakdown', '"https://doi.org/10.1007/3-540-45054-8_18"', '"Given a 3D object and some measurements for points in this object, it is desired to find the 3D location of the object. A new model based pose estimator from stereo pairs based on linear programming (lp) is presented. In the presence of outliers, the new lp estimator provides better results than maximum likelihood estimators such as weighted least squares, and is usually almost as good as robust estimators such as least-median-of-squares (lmeds). In the presence of noise the new lp estimator provides better results than robust estimators such as lmeds, and is slightly inferior to maximum likelihood estimators such as weighted least squares. In the presence of noise and outliers - especially for wide angle stereo - the new estimator provides the best results."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Model Selection by Linear Programming"', '"ECCV 2014"', '["test-time budget", "adaptive model selection", "cost-sensitive learning"]', '"https://doi.org/10.1007/978-3-319-10605-2_42"', '"Budget constraints arise in many computer vision problems. Computational costs limit many automated recognition systems while crowdsourced systems are hindered by monetary costs. We leverage wide variability in image complexity and learn adaptive model selection policies. Our learnt policy maximizes performance under average budget constraints by selecting \\u201ccheap\\u201d models for low complexity instances and utilizing descriptive models only for complex ones. During training, we assume access to a set of models that utilize features of different costs and types. We consider a binary tree architecture where each leaf corresponds to a different model. Internal decision nodes adaptively guide model-selection process along paths on a tree. The learning problem can be posed as an empirical risk minimization over training data with a non-convex objective function. Using hinge loss surrogates we show that adaptive model selection reduces to a linear program thus realizing substantial computational efficiencies and guaranteed convergence properties."'),
('"Model Selection for Range Segmentation of Curved Objects"', '"ECCV 2004"', '["Curve Surface", "Segmentation Algorithm", "Segmentation Result", "Machine Intelligence", "Range Da', '"https://doi.org/10.1007/978-3-540-24670-1_7"', '"In the present paper, we address the problem of recovering the true underlying model of a surface while performing the segmentation. A novel criterion for surface (model) selection is introduced and its performance for selecting the underlying model of various surfaces has been tested and compared with many other existing techniques. Using this criterion, we then present a range data segmentation algorithm capable of segmenting complex objects with planar and curved surfaces. The algorithm simultaneously identifies the type (order and geometric shape) of surface and separates all the points that are part of that surface from the rest in a range image. The paper includes the segmentation results of a large collection of range images obtained from objects with planar and curved surfaces."'),
('"Model-Based Approach to Tomographic Reconstruction Including Projection Deblurring. Sensitivity of ', '"ECCV 2004"', '["tomography", "flexible models", "regularization", "deblurring"]', '"https://doi.org/10.1007/978-3-540-24673-2_4"', '"Classical techniques for the reconstruction of axisymmetrical objects are all creating artefacts (smooth or unstable solutions). Moreover, the extraction of very precise features related to big density transitions remains quite delicate. In this paper, we develop a new approach -in one dimension for the moment- that allows us both to reconstruct and to extract characteristics: an a priori is provided thanks to a density model. We show the interest of this method in regard to noise effects quantification ; we also explain how to take into account some physical perturbations occuring with real data acquisition."'),
('"Model-Based Head and Facial Motion Tracking"', '"CVHCI 2004"', '["Temporal Coherence", "Facial Motion", "Active Appearance Model", "Facial Animation", "Head Trackin', '"https://doi.org/10.1007/978-3-540-24837-8_21"', '"This paper addresses the real-time tracking of head and facial motion in monocular image sequences using 3D deformable models. It introduces two methods. The first method only tracks the 3D head pose using a cascade of two stages: the first stage utilizes a robust feature-based pose estimator associated with two consecutive frames, the second stage relies on a Maximum a Posteriori inference scheme exploiting the temporal coherence in both the 3D head motions and facial textures. The facial texture is updated dynamically in order to obtain a simple on-line appearance model. The implementation of this method is kept simple and straightforward. In addition to the 3D head pose tracking, the second method tracks some facial animations using an Active Appearance Model search. Tracking experiments and performance evaluation demonstrate the robustness and usefulness of the developed methods that retain the advantages of both feature-based and appearance-based methods."'),
('"Model-Based Initialisation for Segmentation"', '"ECCV 2000"', '["Basis Vector", "Independent Component Analysis", "Goal Function", "Object Instance", "Active Appea', '"https://doi.org/10.1007/3-540-45053-X_19"', '"The initialisation of segmentation methods aiming at the localisation of biological structures in medical imagery is frequently regarded as a given precondition. In practice, however, initialisation is usually performed manually or by some heuristic preprocessing steps. Moreover, the same framework is often employed to recover from imperfect results of the subsequent segmentation. Therefore, it is of crucial importance for everyday application to have a simple and effective initialisation method at one\\u2019s disposal. This paper proposes a new model-based framework to synthesise sound initialisations by calculating the most probable shape given a minimal set of statistical landmarks and the applied shape model. Shape information coded by particular points is first iteratively removed from a statistical shape description that is based on the principal component analysis of a collection of shape instances. By using the inverse of the resulting operation, it is subsequently possible to construct initial outlines with minimal effort. The whole framework is demonstrated by means of a shape database consisting of a set of corpus callosum instances. Furthermore, both manual and fully automatic initialisation with the proposed approach is evaluated. The obtained results validate its suitability as a preprocessing step for semi-automatic as well as fully automatic segmentation. And last but not least, the iterative construction of increasingly point-invariant shape statistics provides a deeper insight into the nature of the shape under investigation."'),
('"Model-Based Motion Tracking of Infants"', '"ECCV 2014"', '["3D model fitting", "Infant pose estimation", "Markerless motion tracking", "Depth images"]', '"https://doi.org/10.1007/978-3-319-16199-0_47"', '"Even though motion tracking is a widely used technique to analyze and measure human movements, only a few studies focus on motion tracking of infants. In recent years, a number of studies have emerged focusing on analyzing the motion pattern of infants, using computer vision. Most of these studies are based on 2D images, but few are based on 3D information. In this paper, we present a model-based approach for tracking infants in 3D. The study extends a novel study on graph-based motion tracking of infants and we show that the extension improves the tracking results. A 3D model is constructed that resembles the body surface of an infant, where the model is based on simple geometric shapes and a hierarchical skeleton model."'),
('"Model-Based Silhouette Extraction for Accurate People Tracking"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_22"', '"In this work, we introduce a model-based approach to extracting the silhouette of people in motion from stereo video sequences. To this end, we extend a purely stereo-based approach to tracking people proposed in earlier work. This approach is based on an implicit surface model of the body. It lets us accurately predict the silhouette\\u2019s location and, therefore, detect them more robustly. In turn these silhouettes allow us to fit the model more precisely. This allows effective motion recovery, even when people are filmed against a cluttered unknown background. This is in contrast to many recent approaches that require silhouette contours to be readily obtainable using relatively simple methods, such as background subtraction, that typically require either engineering the scene or making strong assumptions."'),
('"Model-Free Segmentation and Grasp Selection of Unknown Stacked Objects"', '"ECCV 2014"', '["3D segmentation", "grasp selection"]', '"https://doi.org/10.1007/978-3-319-10602-1_43"', '"We present a novel grasping approach for unknown stacked objects using RGB-D images of highly complex real-world scenes. Specifically, we propose a novel 3D segmentation algorithm to generate an efficient representation of the scene into segmented surfaces (known as surfels) and objects. Based on this representation, we next propose a novel grasp selection algorithm which generates potential grasp hypotheses and automatically selects the most appropriate grasp without requiring any prior information of the objects or the scene. We tested our algorithms in real-world scenarios using live video streams from Kinect and publicly available RGB-D object datasets. Our experimental results show that both our proposed segmentation and grasp selection algorithms consistently perform superior compared to the state-of-the-art methods."'),
('"Modeling 3D Objects from Stereo Views and Recognizing Them in Photographs"', '"ECCV 2006"', '["Test Image", "Training Image", "Interest Point", "Partial Model", "Equal Error Rate"]', '"https://doi.org/10.1007/11744047_43"', '"Local appearance models in the neighborhood of salient image features, together with local and/or global geometric constraints, serve as the basis for several recent and effective approaches to 3D object recognition from photographs. However, these techniques typically either fail to explicitly account for the strong geometric constraints associated with multiple images of the same 3D object, or require a large set of training images with much overlap to construct relatively sparse object models. This paper proposes a simple new method for automatically constructing 3D object models consisting of dense assemblies of small surface patches and affine-invariant descriptions of the corresponding texture patterns from a few (7 to 12) stereo pairs. Similar constraints are used to effectively identify instances of these models in highly cluttered photographs taken from arbitrary and unknown viewpoints. Experiments with a dataset consisting of 80 test images of 9 objects, including comparisons with a number of baseline algorithms, demonstrate the promise of the proposed approach."'),
('"Modeling and Analysis of Dynamic Behaviors of Web Image Collections"', '"ECCV 2010"', '["Visual Word", "Cosine Similarity", "Temporal Context", "Similarity Network", "Sequential Monte Car', '"https://doi.org/10.1007/978-3-642-15555-0_7"', '"Can we model the temporal evolution of topics in Web image collections? If so, can we exploit the understanding of dynamics to solve novel visual problems or improve recognition performance? These two challenging questions are the motivation for this work. We propose a nonparametric approach to modeling and analysis of topical evolution in image sets. A scalable and parallelizable sequential Monte Carlo based method is developed to construct the similarity network of a large-scale dataset that provides a base representation for wide ranges of dynamics analysis. In this paper, we provide several experimental results to support the usefulness of image dynamics with the datasets of 47 topics gathered from Flickr. First, we produce some interesting observations such as tracking of subtopic evolution and outbreak detection, which cannot be achieved with conventional image sets. Second, we also present the complementary benefits that the images can introduce over the associated text analysis. Finally, we show that the training using the temporal association significantly improves the recognition performance."'),
('"Modeling and Detection of Wrinkles in Aging Human Faces Using Marked Point Processes"', '"ECCV 2012"', '["Modeling of wrinkles", "Markov Point Process", "Reversible Jump Markov Chain Monte Carlo", "stocha', '"https://doi.org/10.1007/978-3-642-33868-7_18"', '"In this paper we propose a new generative model for wrinkles on aging human faces using Marked Point Processes (MPP). Wrinkles are considered as stochastic spatial arrangements of sequences of line segments, and detected in an image by proper localization of line segments. The intensity gradients are used to detect more probable locations and a prior probability model is used to constrain properties of line segments. Wrinkles are localized by sampling MPP using the Reversible Jump Markov Chain Monte Carlo (RJMCMC) algorithm. We also present an evaluation setup to measure the performance of the proposed model. We present results on a variety of images obtained from the Internet to illustrate the performance of the proposed model."'),
('"Modeling and Recognition of Landmark Image Collections Using Iconic Scene Graphs"', '"ECCV 2008"', '["Image Collection", "Bundle Adjustment", "Scene Graph", "Vocabulary Tree", "Iconic Image"]', '"https://doi.org/10.1007/978-3-540-88682-2_33"', '"This paper presents an approach for modeling landmark sites such as the Statue of Liberty based on large-scale contaminated image collections gathered from the Internet. Our system combines 2D appearance and 3D geometric constraints to efficiently extract scene summaries, build 3D models, and recognize instances of the landmark in new test images. We start by clustering images using low-dimensional global \\u201cgist\\u201d descriptors. Next, we perform geometric verification to retain only the clusters whose images share a common 3D structure. Each valid cluster is then represented by a single iconic view, and geometric relationships between iconic views are captured by an iconic scene graph. In addition to serving as a compact scene summary, this graph is used to guide structure from motion to efficiently produce 3D models of the different aspects of the landmark. The set of iconic images is also used for recognition, i.e., determining whether new test images contain the landmark. Results on three data sets consisting of tens of thousands of images demonstrate the potential of the proposed approach."'),
('"Modeling and Synthesis of Facial Motion Driven by Speech"', '"ECCV 2004"', '["Independent Component Analysis", "Independent Component Analysis", "Facial Motion", "Speech Segmen', '"https://doi.org/10.1007/978-3-540-24672-5_36"', '"We introduce a novel approach to modeling the dynamics of human facial motion induced by the action of speech for the purpose of synthesis. We represent the trajectories of a number of salient features on the human face as the output of a dynamical system made up of two subsystems, one driven by the deterministic speech input, and a second driven by an unknown stochastic input. Inference of the model (learning) is performed automatically and involves an extension of independent component analysis to time-depentend data. Using a shape-texture decompositional representation for the face, we generate facial image sequences reconstructed from synthesized feature point positions."'),
('"Modeling Blurred Video with Layers"', '"ECCV 2014"', '["Optical Flow", "Layers", "Object Boundaries", "Motion Blur"]', '"https://doi.org/10.1007/978-3-319-10599-4_16"', '"Videos contain complex spatially-varying motion blur due to the combination of object motion, camera motion, and depth variation with finite shutter speeds. Existing methods to estimate optical flow, deblur the images, and segment the scene fail in such cases. In particular, boundaries between differently moving objects cause problems, because here the blurred images are a combination of the blurred appearances of multiple surfaces. We address this with a novel layered model of scenes in motion. From a motion-blurred video sequence, we jointly estimate the layer segmentation and each layer\\u2019s appearance and motion. Since the blur is a function of the layer motion and segmentation, it is completely determined by our generative model. Given a video, we formulate the optimization problem as minimizing the pixel error between the blurred frames and images synthesized from the model, and solve it using gradient descent. We demonstrate our approach on synthetic and real sequences."'),
('"Modeling Complex Temporal Composition of Actionlets for Activity Prediction"', '"ECCV 2012"', '["Action Unit", "Interest Point", "Rand Index", "Activity Prediction", "Predictable Characteristic"]', '"https://doi.org/10.1007/978-3-642-33718-5_21"', '"Early prediction of ongoing activity has been more and more valuable in a large variety of time-critical applications. To build an effective representation for prediction, human activities can be characterized by a complex temporal composition of constituent simple actions. Different from early recognition on short-duration simple activities, we propose a novel framework for long-duration complex activity prediction by discovering the causal relationships between constituent actions and the predictable characteristics of activities. The major contributions of our work include: (1) we propose a novel activity decomposition method by monitoring motion velocity which encodes a temporal decomposition of long activities into a sequence of meaningful action units; (2) Probabilistic Suffix Tree (PST) is introduced to represent both large and small order Markov dependencies between action units; (3) we present a Predictive Accumulative Function (PAF) to depict the predictability of each kind of activity. The effectiveness of the proposed method is evaluated on two experimental scenarios: activities with middle-level complexity and activities with high-level complexity. Our method achieves promising results and can predict global activity classes and local action units."'),
('"Modeling Perceptual Color Differences by Local Metric Learning"', '"ECCV 2014"', '["Color difference", "Metric learning", "Uniform color space"]', '"https://doi.org/10.1007/978-3-319-10602-1_7"', '"Having perceptual differences between scene colors is key in many computer vision applications such as image segmentation or visual salient region detection. Nevertheless, most of the times, we only have access to the rendered image colors, without any means to go back to the true scene colors. The main existing approaches propose either to compute a perceptual distance between the rendered image colors, or to estimate the scene colors from the rendered image colors and then to evaluate perceptual distances. However the first approach provides distances that can be far from the scene color differences while the second requires the knowledge of the acquisition conditions that are unavailable for most of the applications. In this paper, we design a new local Mahalanobis-like metric learning algorithm that aims at approximating a perceptual scene color difference that is invariant to the acquisition conditions and computed only from rendered image colors. Using the theoretical framework of uniform stability, we provide consistency guarantees on the learned model. Moreover, our experimental evaluation shows its great ability (i) to generalize to new colors and devices and (ii) to deal with segmentation tasks."'),
('"Modeling Supporting Regions for Close Human Interaction Recognition"', '"ECCV 2014"', '["Interest Point", "Interaction Class", "Global Interaction", "Patch Model", "Close Physical Contact', '"https://doi.org/10.1007/978-3-319-16181-5_3"', '"This paper addresses the problem of recognizing human interactions with close physical contact from videos. Different from conventional human interaction recognition, recognizing close interactions faces the problems of ambiguities in feature-to-person assignments and frequent occlusions. Therefore, it is infeasible to accurately extract the interacting people, and the recognition performance of an interaction model is degraded. We propose a patch-aware model to overcome the two problems in close interaction recognition. Our model learns discriminative supporting regions for each interacting individual. The learned supporting regions accurately extract individuals at patch level, and explicitly indicate feature assignments. In addition, our model encodes a set of body part configurations for one interaction class, which provide rich representations for frequent occlusions. Our approach is evaluated on the UT-Interaction dataset and the BIT-Interaction dataset, and achieves promising results."'),
('"Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification"', '"ECCV 2010"', '["Video Sequence", "Temporal Structure", "Motion Segment", "Anchor Point", "Interest Point"]', '"https://doi.org/10.1007/978-3-642-15552-9_29"', '"Much recent research in human activity recognition has focused on the problem of recognizing simple repetitive (walking, running, waving) and punctual actions (sitting up, opening a door, hugging). However, many interesting human activities are characterized by a complex temporal composition of simple actions. Automatic recognition of such complex actions can benefit from a good understanding of the temporal structures. We present in this paper a framework for modeling motion by exploiting the temporal structure of the human activities. In our framework, we represent activities as temporal compositions of motion segments. We train a discriminative model that encodes a temporal decomposition of video sequences, and appearance models for each motion segment. In recognition, a query video is matched to the model according to the learned appearances and motion segment decomposition. Classification is made based on the quality of matching between the motion segment classifiers and the temporal segments in the query sequence. To validate our approach, we introduce a new dataset of complex Olympic Sports activities. We show that our algorithm performs better than other state of the art methods."'),
('"Modeling the Activity Pattern of the Constellation of Cardiac Chambers in Echocardiogram Videos"', '"CVAMIA 2006"', '["Activity Pattern", "Cardiac Chamber", "Dynamic Bayesian Network", "Topological Transformation", "T', '"https://doi.org/10.1007/11889762_18"', '"A novel approach is presented for modeling the complex activity pattern of the heart in echocardiogram videos. In this approach, the heart is represented by the constellation of its chambers, where the constellation is modeled by pictorial structure at each instance in time. Pictorial structure is then extended to the temporal domain to simultaneously capture the evolution pattern of the appearance of each chamber, the evolving spatial relationships between them, and the topological transformations in their constellation due to phase transitions. Inference and learning algorithms are presented for the model. The problem of correspondence is solved at each stage of the inference process, by matching the evolving model of the complex activity pattern to the observed constellations. The model, which is trained using examples of normal echocardiogram videos is shown to be efficient in temporal segmentation of the content of echocardiogram videos into different phases during one cycle of heart activity."'),
('"Modeling the Temporal Extent of Actions"', '"ECCV 2010"', '["Action Recognition", "Temporal Extent", "Training Video", "Multiple Instance Learning", "Action Re', '"https://doi.org/10.1007/978-3-642-15549-9_39"', '"In this paper, we present a framework for estimating what portions of videos are most discriminative for the task of action recognition. We explore the impact of the temporal cropping of training videos on the overall accuracy of an action recognition system, and we formalize what makes a set of croppings optimal. In addition, we present an algorithm to determine the best set of croppings for a dataset, and experimentally show that our approach increases the accuracy of various state-of-the-art action recognition techniques."'),
('"Modeling Video Dynamics with Deep Dynencoder"', '"ECCV 2014"', '["Video Dynamics", "Deep Model", "Autoencoder", "Time Series", "Dynamic Textures"]', '"https://doi.org/10.1007/978-3-319-10593-2_15"', '"Videos always exhibit various pattern motions, which can be modeled according to dynamics between adjacent frames. Previous methods based on linear dynamic system can model dynamic textures but have limited capacity of representing sophisticated nonlinear dynamics. Inspired by the nonlinear expression power of deep autoencoders, we propose a novel model named dynencoder which has an autoencoder at the bottom and a variant of it at the top (named as dynpredictor). It generates hidden states from raw pixel inputs via the autoencoder and then encodes the dynamic of state transition over time via the dynpredictor. Deep dynencoder can be constructed by proper stacking strategy and trained by layer-wise pre-training and joint fine-tuning. Experiments verify that our model can describe sophisticated video dynamics and synthesize endless video texture sequences with high visual quality. We also design classification and clustering methods based on our model and demonstrate the efficacy of them on traffic scene classification and motion segmentation."'),
('"Modelling Primate Control of Grasping for Robotics Applications"', '"ECCV 2014"', '["Grasping", "Affordances", "Macaque", "Robotics", "AIP", "F5"]', '"https://doi.org/10.1007/978-3-319-16181-5_33"', '"The neural circuits that control grasping and perform related visual processing have been studied extensively in macaque monkeys. We are developing a computational model of this system, in order to better understand its function, and to explore applications to robotics. We recently modelled the neural representation of three-dimensional object shapes, and are currently extending the model to produce hand postures so that it can be tested on a robot. To train the extended model, we are developing a large database of object shapes and corresponding feasible grasps. Finally, further extensions are needed to account for the influence of higher-level goals on hand posture. This is essential because often the same object must be grasped in different ways for different purposes. The present paper focuses on a method of incorporating such higher-level goals. A proof-of-concept exhibits several important behaviours, such as choosing from multiple approaches to the same goal. Finally, we discuss a neural representation of objects that supports fast searching for analogous objects."'),
('"Molding Face Shapes by Example"', '"ECCV 2006"', '["Facial Expression", "Input Image", "Reference Model", "Regularization Term", "Face Database"]', '"https://doi.org/10.1007/11744023_22"', '"Human faces are remarkably similar in global properties, including size, aspect ratios, and locations of main features, but can vary considerably in details across individuals, gender, race, or due to facial expression. We propose a novel method for 3D shape recovery of a face from a single image using a single 3D reference model of a different person\\u2019s face. The method uses the input image as a guide to mold the reference model to reach a desired reconstruction. Assuming Lambertian reflectance and rough alignment of the input image and reference model, we seek shape, albedo, and lighting that best fit the image while preserving the rough structure of the model. We demonstrate our method by providing accurate reconstructions of novel faces overcoming significant differences in shape due to gender, race, and facial expressions."'),
('"Monocular 3D Reconstruction of Human Motion in Long Action Sequences"', '"ECCV 2004"', '["Human Motion", "Motion Blur", "Reference Camera", "Human Motion Capture", "Skeletal Joint"]', '"https://doi.org/10.1007/978-3-540-24673-2_36"', '"A novel algorithm is presented for the 3D reconstruction of human action in long (>30 second) monocular image sequences. A sequence is represented by a small set of automatically found representative keyframes. The skeletal joint positions are manually located in each keyframe and mapped to all other frames in the sequence. For each keyframe a 3D key pose is created, and interpolation between these 3D body poses, together with the incorporation of limb length and symmetry constraints, provides a smooth initial approximation of the 3D motion. This is then fitted to the image data to generate a realistic 3D reconstruction. The degree of manual input required is controlled by the diversity of the sequence\\u2019s content. Sports\\u2019 footage is ideally suited to this approach as it frequently contains a limited number of repeated actions. Our method is demonstrated on a long (36 second) sequence of a woman playing tennis filmed with a non-stationary camera. This sequence required manual initialisation on <1.5% of the frames, and demonstrates that the system can deal with very rapid motion, severe self-occlusions, motion blur and clutter occurring over several concurrent frames. The monocular 3D reconstruction is verified by synthesising a view from the perspective of a \\u2018ground truth\\u2019 reference camera, and the result is seen to provide a qualitatively accurate 3D reconstruction of the motion."'),
('"Monocular 3D Scene Modeling and Inference: Understanding Multi-Object Traffic Scenes"', '"ECCV 2010"', '["Hide Markov Model", "Object Detection", "False Detection", "Data Association", "Stereo Camera"]', '"https://doi.org/10.1007/978-3-642-15561-1_34"', '"Scene understanding has (again) become a focus of computer vision research, leveraging advances in detection, context modeling, and tracking. In this paper, we present a novel probabilistic 3D scene model that encompasses multi-class object detection, object tracking, scene labeling, and 3D geometric relations. This integrated 3D model is able to represent complex interactions like inter-object occlusion, physical exclusion between objects, and geometric context. Inference allows to recover 3D scene context and perform 3D multiobject tracking from a mobile observer, for objects of multiple categories, using only monocular video as input. In particular, we show that a joint scene tracklet model for the evidence collected over multiple frames substantially improves performance. The approach is evaluated for two different types of challenging onboard sequences. We first show a substantial improvement to the state-of-the-art in 3D multi-people tracking. Moreover, a similar performance gain is achieved for multi-class 3D tracking of cars and trucks on a new, challenging dataset."'),
('"Monocular Camera Fall Detection System Exploiting 3D Measures: A Semi-supervised Learning Approach"', '"ECCV 2012"', '["image motion analysis", "semisupervised learning", "self calibration", "fall detection"]', '"https://doi.org/10.1007/978-3-642-33885-4_9"', '"Falls have been reported as the leading cause of injury-related visits to emergency departments and the primary etiology of accidental deaths in elderly. The system presented in this article addresses the fall detection problem through visual cues. The proposed methodology utilize a fast, real-time background subtraction algorithm based on motion information in the scene and capable to operate properly in dynamically changing visual conditions, in order to detect the foreground object and, at the same time, it exploits 3D space\\u2019s measures, through automatic camera calibration, to increase the robustness of fall detection algorithm which is based on semi-supervised learning. The above system uses a single monocular camera and is characterized by minimal computational cost and memory requirements that make it suitable for real-time large scale implementations."'),
('"Monocular Multiview Object Tracking with 3D Aspect Parts"', '"ECCV 2014"', '["multiview object tracking", "3D aspect part representation"]', '"https://doi.org/10.1007/978-3-319-10599-4_15"', '"In this work, we focus on the problem of tracking objects under significant viewpoint variations, which poses a big challenge to traditional object tracking methods. We propose a novel method to track an object and estimate its continuous pose and part locations under severe viewpoint change. In order to handle the change in topological appearance introduced by viewpoint transformations, we represent objects with 3D aspect parts and model the relationship between viewpoint and 3D aspect parts in a part-based particle filtering framework. Moreover, we show that instance-level online-learned part appearance can be incorporated into our model, which makes it more robust in difficult scenarios with occlusions. Experiments are conducted on a new dataset of challenging YouTube videos and a subset of the KITTI dataset [14] that include significant viewpoint variations, as well as a standard sequence for car tracking. We demonstrate that our method is able to track the 3D aspect parts and the viewpoint of objects accurately despite significant changes in viewpoint."'),
('"Monocular Object Detection Using 3D Geometric Primitives"', '"ECCV 2012"', '["Object Detection", "Ground Plane", "Object Location", "Multiple Camera", "Ground Location"]', '"https://doi.org/10.1007/978-3-642-33718-5_62"', '"Multiview object detection methods achieve robustness in adverse imaging conditions by exploiting projective consistency across views. In this paper, we present an algorithm that achieves performance comparable to multiview methods from a single camera by employing geometric primitives as proxies for the true 3D shape of objects, such as pedestrians or vehicles. Our key insight is that for a calibrated camera, geometric primitives produce predetermined location-specific patterns in occupancy maps. We use these to define spatially-varying kernel functions of projected shape. This leads to an analytical formation model of occupancy maps as the convolution of locations and projected shape kernels. We estimate object locations by deconvolving the occupancy map using an efficient template similarity scheme. The number of objects and their positions are determined using the mean shift algorithm. The approach is highly parallel because the occupancy probability of a particular geometric primitive at each ground location is an independent computation. The algorithm extends to multiple cameras without requiring significant bandwidth. We demonstrate comparable performance to multiview methods and show robust, realtime object detection on full resolution HD video in a variety of challenging imaging conditions."'),
('"Monocular Perception of Biological Motion - Clutter and Partial Occlusion"', '"ECCV 2000"', '["Body Part", "False Alarm Rate", "Conditional Independence", "Signal Point", "Biological Motion"]', '"https://doi.org/10.1007/3-540-45053-X_46"', '"The problem of detecting and labeling a moving human body viewed monocularly in a cluttered scene is considered. The task is to decide whether or not one or more people are in the scene (detection), to count them, and to label their visible body parts (labeling)."'),
('"Monocular Rear-View Obstacle Detection Using Residual Flow"', '"ECCV 2012"', '["Ground Motion", "Optical Flow", "Residual Flow", "Obstacle Detection", "Visual Odometry"]', '"https://doi.org/10.1007/978-3-642-33868-7_50"', '"We present a system for automatically detecting obstacles from a moving vehicle using a monocular wide angle camera. Our system was developed in the context of finding obstacles and particularly children when backing up. Camera viewpoint is transformed to a virtual bird-eye view. We developed a novel image registration algorithm to obtain ego-motion that in combination with variational dense optical flow outputs a residual motion map with respect to the ground. The residual motion map is used to identify and segment 3D and moving objects. Our main contribution is the feature-based image registration algorithm that is able to separate and obtain ground layer ego-motion accurately even in cases of ground covering only 20% of the image, outperforming RANSAC."'),
('"Monocular Tracking of 3D Human Motion with a Coordinated Mixture of Factor Analyzers"', '"ECCV 2006"', '["Tracking Algorithm", "Neural Information Processing System", "Locally Linear Embedding", "Motion C', '"https://doi.org/10.1007/11744047_11"', '"Filtering based algorithms have become popular in tracking human body pose. Such algorithms can suffer the curse of dimensionality due to the high dimensionality of the pose state space; therefore, efforts have been dedicated to either smart sampling or reducing the dimensionality of the original pose state space. In this paper, a novel formulation that employs a dimensionality reduced state space for multi-hypothesis tracking is proposed. During off-line training, a mixture of factor analyzers is learned. Each factor analyzer can be thought of as a \\u201clocal dimensionality reducer\\u201d that locally approximates the pose manifold. Global coordination between local factor analyzers is achieved by learning a set of linear mixture functions that enforces agreement between local factor analyzers. The formulation allows easy bidirectional mapping between the original body pose space and the low-dimensional space. During online tracking, the clusters of factor analyzers are utilized in a multiple hypothesis tracking algorithm. Experiments demonstrate that the proposed algorithm tracks 3D body pose efficiently and accurately , even when self-occlusion, motion blur and large limb movements occur. Quantitative comparisons show that the formulation produces more accurate 3D pose estimates over time than those that can be obtained via a number of previously-proposed particle filtering based tracking algorithms."'),
('"Monocular Visual Odometry and Dense 3D Reconstruction for On-Road Vehicles"', '"ECCV 2012"', '["Visual odometry", "2-point motion estimation", "pose estimation", "plane sweeping"]', '"https://doi.org/10.1007/978-3-642-33868-7_59"', '"More and more on-road vehicles are equipped with cameras each day. This paper presents a novel method for estimating the relative motion of a vehicle from a sequence of images obtained using a single vehicle-mounted camera. Recently, several researchers in robotics and computer vision have studied the performance of motion estimation algorithms under non-holonomic constraints and planarity. The successful algorithms typically use the smallest number of feature correspondences with respect to the motion model. It has been strongly established that such minimal algorithms are efficient and robust to outliers when used in a hypothesize-and-test framework such as random sample consensus (RANSAC). In this paper, we show that the planar 2-point motion estimation can be solved analytically using a single quadratic equation, without the need of iterative techniques such as Newton-Raphson method used in existing work. Non-iterative methods are more efficient and do not suffer from local minima problems. Although 2-point motion estimation generates visually accurate on-road vehicle trajectory, the motion is not precise enough to perform dense 3D reconstruction due to the non-planarity of roads. Thus we use a 2-point relative motion algorithm for the initial images followed by 3-point 2D-to-3D camera pose estimation for the subsequent images. Using this hybrid approach, we generate accurate motion estimates for a plane-sweeping algorithm that produces dense depth maps for obstacle detection applications."'),
('"Morphable Displacement Field Based Image Matching for Face Recognition across Pose"', '"ECCV 2012"', '["Face Recognition", "Normalize Root Mean Square Error", "Virtual Image", "Virtual View", "Local Con', '"https://doi.org/10.1007/978-3-642-33718-5_8"', '"Fully automatic Face Recognition Across Pose (FRAP) is one of the most desirable techniques, however, also one of the most challenging tasks in face recognition field. Matching a pair of face images in different poses can be converted into matching their pixels corresponding to the same semantic facial point. Following this idea, given two images G and P in different poses, we propose a novel method, named Morphable Displacement Field (MDF), to match G with P\\u2019s virtual view under G\\u2019s pose. By formulating MDF as a convex combination of a number of template displacement fields generated from a 3D face database, our model satisfies both global conformity and local consistency. We further present an approximate but effective solution of the proposed MDF model, named implicit Morphable Displacement Field (iMDF), which synthesizes virtual view implicitly via an MDF by minimizing matching residual. This formulation not only avoids intractable optimization of the high-dimensional displacement field but also facilitates a constrained quadratic optimization. The proposed method can work well even when only 2 facial landmarks are labeled, which makes it especially suitable for fully automatic FRAP system. Extensive evaluations on FERET, PIE and Multi-PIE databases show considerable improvement over state-of-the-art FRAP algorithms in both semi-automatic and fully automatic evaluation protocols."'),
('"Morphological Operations on Matrix-Valued Images"', '"ECCV 2004"', '["mathematical morphology", "dilation", "erosion", "matrix-valued imaging", "DT-MRI"]', '"https://doi.org/10.1007/978-3-540-24673-2_13"', '"The output of modern imaging techniques such as diffusion tensor MRI or the physical measurement of anisotropic behaviour in materials such as the stress-tensor consists of tensor-valued data. Hence adequate image processing methods for shape analysis, skeletonisation, denoising and segmentation are in demand. The goal of this paper is to extend the morphological operations of dilation, erosion, opening and closing to the matrix-valued setting. We show that naive approaches such as componentwise application of scalar morphological operations are unsatisfactory, since they violate elementary requirements such as invariance under rotation. This lead us to study an analytic and a geometric alternative which are rotation invariant. Both methods introduce novel non-component-wise definitions of a supremum and an infimum of a finite set of matrices. The resulting morphological operations incorporate information from all matrix channels simultaneously and preserve positive definiteness of the matrix field. Their properties and their performance are illustrated by experiments on diffusion tensor MRI data."'),
('"Motion Capture of Hands in Action Using Discriminative Salient Points"', '"ECCV 2012"', '["Particle Swarm Optimization", "Motion Capture", "Salient Point", "Virtual Node", "Hand Tracking"]', '"https://doi.org/10.1007/978-3-642-33783-3_46"', '"Capturing the motion of two hands interacting with an object is a very challenging task due to the large number of degrees of freedom, self-occlusions, and similarity between the fingers, even in the case of multiple cameras observing the scene. In this paper we propose to use discriminatively learned salient points on the fingers and to estimate the finger-salient point associations simultaneously with the estimation of the hand pose. We introduce a differentiable objective function that also takes edges, optical flow and collisions into account. Our qualitative and quantitative evaluations show that the proposed approach achieves very accurate results for several challenging sequences containing hands and objects in action."'),
('"Motion Context: A New Representation for Human Action Recognition"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88693-8_60"', '"One of the key challenges in human action recognition from video sequences is how to model an action sufficiently. Therefore, in this paper we propose a novel motion-based representation called Motion Context (MC), which is insensitive to the scale and direction of an action, by employing image representation techniques. A MC captures the distribution of the motion words (MWs) over relative locations in a local region of the motion image (MI) around a reference point and thus summarizes the local motion information in a rich 3D MC descriptor. In this way, any human action can be represented as a 3D descriptor by summing up all the MC descriptors of this action. For action recognition, we propose 4 different recognition configurations: MW+pLSA, MW+SVM, MC+w 3-pLSA (a new direct graphical model by extending pLSA), and MC+SVM. We test our approach on two human action video datasets from KTH and Weizmann Institute of Science (WIS) and our performances are quite promising. For the KTH dataset, the proposed MC representation achieves the highest performance using the proposed w 3-pLSA. For the WIS dataset, the best performance of the proposed MC is comparable to the state of the art."'),
('"Motion Curves for Parametric Shape and Motion Estimation"', '"ECCV 2002"', '["Structure from motion", "camera modeling", "model selection", "motion curves", "model-based estima', '"https://doi.org/10.1007/3-540-47967-8_18"', '"This paper presents a novel approach to camera motion parametrization for the structure and motion problem. In a model-based framework, the hypothesis of (relatively) continuous and smooth sensor motion enables to reformulate the motion recovery problem as a global curve estimation problem on the camera path. Curves of incremental complexity are fitted using model selection to take into account incoming image data. No first estimate guess is needed. The use of modeling curves lead to a meaningful description of the camera trajectories, with a drastic reduction in the number of degrees of freedom. In order to characterize the behaviour and performances of the approach, experiments with various long video sequences, both synthetic and real, are undertaken. Several candidate curve models for motion estimation are presented and compared, and the results validate the work in terms of reconstruction accuracy, noise robustness and model compacity."'),
('"Motion Interchange Patterns for Action Recognition in Unconstrained Videos"', '"ECCV 2012"', '["Video Clip", "Action Recognition", "Local Binary Pattern", "Current Frame", "Camera Motion"]', '"https://doi.org/10.1007/978-3-642-33783-3_19"', '"Action Recognition in videos is an active research field that is fueled by an acute need, spanning several application domains. Still, existing systems fall short of the applications\\u2019 needs in real-world scenarios, where the quality of the video is less than optimal and the viewpoint is uncontrolled and often not static. In this paper, we consider the key elements of motion encoding and focus on capturing local changes in motion directions. In addition, we decouple image edges from motion edges using a suppression mechanism, and compensate for global camera motion by using an especially fitted registration scheme. Combined with a standard bag-of-words technique, our methods achieves state-of-the-art performance in the most recent and challenging benchmarks."'),
('"Motion Profiles for Deception Detection Using Visual Cues"', '"ECCV 2010"', '["Nonverbal Behavior", "Interview Response", "Active Shape Model", "Multiple Instance Learn", "Decep', '"https://doi.org/10.1007/978-3-642-15567-3_34"', '"We propose a data-driven, unobtrusive and covert method for automatic deception detection in interrogation interviews from visual cues only. Using skin blob analysis together with Active Shape Modeling, we continuously track and analyze the motion of the hands and head as a subject is responding to interview questions, as well as their facial micro expressions, thus extracting motion profiles, which we aggregate over each interview response. Our novelty lies in the representation of the motion profile distribution for each response. In particular, we use a kernel density estimator with uniform bins in log feature space. This scheme allows the representation of relatively over-controlled and relatively agitated behaviors of interviewed subjects, thus aiding in the discrimination of truthful and deceptive responses."'),
('"Motion Segmentation by Tracking Edge Information over Multiple Frames"', '"ECCV 2000"', '["Motion Estimation", "Markov Random Field", "Markov Chain Model", "Foreground Object", "Motion Segm', '"https://doi.org/10.1007/3-540-45053-X_26"', '"This paper presents a new Bayesian framework for layered motion segmentation, dividing the frames of an image sequence into foreground and background layers by tracking edges. The first frame in the sequence is segmented into regions using image edges, which are tracked to estimate two affine motions. The probability of the edges fitting each motion is calculated using 1st order statistics along the edge. The most likely region labelling is then resolved using these probabilities, together with a Markov Random Field prior. As part of this process one of the motions is also identified as the foreground motion."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Motion Segmentation Using an Occlusion Detector"', '"WDV 2006"', '["Video Sequence", "Coarse Scale", "Motion Boundary", "Stereo Pair", "Motion Segmentation"]', '"https://doi.org/10.1007/978-3-540-70932-9_3"', '"We present a novel method for the detection of motion boundaries in a video sequence based on differential properties of the spatio-temporal domain. Regarding the video sequence as a 3D spatio-temporal function, we consider the second moment matrix of its gradients (averaged over a local window), and show that the eigenvalues of this matrix can be used to detect occlusions and motion discontinuities. Since these cannot always be determined locally (due to false corners and the aperture problem), a scale-space approach is used for extracting the location of motion boundaries. A closed contour is then constructed from the most salient boundary fragments, to provide the final segmentation. The method is shown to give good results on pairs of real images taken in general motion. We use synthetic data to show its robustness to high levels of noise and illumination changes; we also include cases where no intensity edge exists at the location of the motion boundary, or when no parametric motion model can describe the data."'),
('"Motion Words for Videos"', '"ECCV 2014"', '["Video representations", "action classification"]', '"https://doi.org/10.1007/978-3-319-10590-1_47"', '"In the task of activity recognition in videos, computing the video representation often involves pooling feature vectors over spatially local neighborhoods. The pooling is done over the entire video, over coarse spatio-temporal pyramids, or over pre-determined rigid cuboids. Similarly to pooling image features over superpixels in images, it is natural to consider pooling spatio-temporal features over video segments, e.g., supervoxels. However, since the number of segments is variable, this produces a video representation of variable size. We propose Motion Words - a new, fixed size video representation, where we pool features over supervoxels. To segment the video into supervoxels, we explore two recent video segmentation algorithms. The proposed representation enables localization of common regions across videos in both space and time. Importantly, since the video segments are meaningful regions, we can interpret the proposed features and obtain a better understanding of why two videos are similar. Evaluation on classification and retrieval tasks on two datasets further shows that Motion Words achieves state-of-the-art performance."'),
('"Motion \\u2014 Stereo Integration for Depth Estimation"', '"ECCV 2002"', '["Depth Estimation", "Stereo Pair", "Epipolar Line", "Radial Distortion", "Depth Discontinuity"]', '"https://doi.org/10.1007/3-540-47967-8_12"', '"Depth extraction with a mobile stereo system is described. The stereo setup is precalibrated, but the system extracts its own motion. Emphasis lies on the integration of the motion and stereo cues. It is guided by the relative confidence that the system has in these cues. This weighing is fine-grained in that it is determined for every pixel at every iteration. Reliable information spreads fast at the expense of less reliable data, both in terms of spatial communication and in terms of exchange between cues. The resulting system can handle large displacements, depth discontinuities and occlusions. Experimental results corroborate the viability of the approach."'),
('"Motion-Aware Structured Light Using Spatio-Temporal Decodable Patterns"', '"ECCV 2012"', '["Structured light", "motion-aware 3D reconstruction", "spatio-temporal decoding", "adaptive window ', '"https://doi.org/10.1007/978-3-642-33715-4_60"', '"Single-shot structured light methods allow 3D reconstruction of dynamic scenes. However, such methods lose spatial resolution and perform poorly around depth discontinuities. Previous single-shot methods project the same pattern repeatedly; thereby spatial resolution is reduced even if the scene is static or has slowly moving parts. We present a structured light system using a sequence of shifted stripe patterns that is decodable both spatially and temporally. By default, our method allows single-shot 3D reconstruction with any of our projected patterns by using spatial windows. Moreover, the sequence is designed so as to progressively improve the reconstruction quality around depth discontinuities by using temporal windows."'),
('"Motivational System for Human-Robot Interaction"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_3"', '"Human-Robot Interaction (HRI) has recently drawn increased attention. Robots can not only passively receive information but also actively emit actions. We present a motivational system for human-robot interaction. The motivational system signals the occurrence of salient sensory inputs, modulates the mapping from sensory inputs to action outputs, and evaluates candidate actions. No salient feature is predefined in the motivational system but instead novelty based on experience, which is applicable to any task. Novelty is defined as an innate drive. Reinforcer is integrated with novelty. Thus, the motivational system of a robot can be developed through interactions with trainers. We treat vision-based neck action selection as a behavior guided by the motivational system. The experimental results are consistent with the attention mechanism in human infants."'),
('"Movement Pattern Histogram for Action Recognition and Retrieval"', '"ECCV 2014"', '["Action Recognition", "Transfer Learning", "Viewpoint Change", "Fisher Vector", "Background Motion"', '"https://doi.org/10.1007/978-3-319-10605-2_45"', '"We present a novel action representation based on encoding the global temporal movement of an action. We represent an action as a set of movement pattern histograms that encode the global temporal dynamics of an action. Our key observation is that temporal dynamics of an action are robust to variations in appearance and viewpoint changes, making it useful for action recognition and retrieval. We pose the problem of computing similarity between action representations as a maximum matching problem in a bipartite graph. We demonstrate the effectiveness of our method for cross-view action recognition on the IXMAS dataset. We also show how our representation complements existing bag-of-features representations on the UCF50 dataset. Finally we show the power of our representation for action retrieval on a new real-world dataset containing repetitive motor movements emitted by children with autism in an unconstrained classroom setting."'),
('"Movie/Script: Alignment and Parsing of Video and Text Transcription"', '"ECCV 2008"', '["Travel Salesman Problem", "Hamiltonian Path", "Face Track", "Scene Segmentation", "Action Retrieva', '"https://doi.org/10.1007/978-3-540-88693-8_12"', '"Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales \\u201cin the wild\\u201d. Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series."'),
('"Moving Object Segmentation Using Motor Signals"', '"ECCV 2012"', '["Motor Signal", "Fundamental Matrix", "Foreground Object", "Active Contour Model", "Object Segmenta', '"https://doi.org/10.1007/978-3-642-33715-4_49"', '"Moving object segmentation from an image sequence is essential for a robot to interact with its environment. Traditional vision approaches appeal to pure motion analysis on videos without exploiting the source of the background motion. We observe, however, that the background motion (from the robot\\u2019s egocentric view) has stronger correlation to the robot\\u2019s motor signals than the foreground motion. We propose a novel approach to detecting moving objects by clustering features into background and foreground according to their motion consistency with motor signals. Specifically, our approach learns homography and fundamental matrices as functions of motor signals, and predict sparse feature locations from the learned matrices. The errors between the predictions and their actual tracked locations are used to label them into background and foreground. The labels of the sparse features are then propagated to all pixels. Our approach does not require building a dense mosaic background or searching for affine, homography, or fundamental matrix parameters for foreground separation. In addition, it does not need to explicitly model the intrinsic and extrinsic calibration parameters hence requires much less prior geometry knowledge. It works completely in 2D image space, and does not involve any complex analysis or computation in 3D space."'),
('"MRF Inference by k-Fan Decomposition and Tight Lagrangian Relaxation"', '"ECCV 2010"', '["Markov Random Field", "Lagrangian Relaxation", "Linear Programming Relaxation", "Single View", "Pr', '"https://doi.org/10.1007/978-3-642-15558-1_53"', '"We present a novel dual decomposition approach to MAP inference with highly connected discrete graphical models. Decompositions into cyclic k-fan structured subproblems are shown to significantly tighten the Lagrangian relaxation relative to the standard local polytope relaxation, while enabling efficient integer programming for solving the subproblems. Additionally, we introduce modified update rules for maximizing the dual function that avoid oscillations and converge faster to an optimum of the relaxed problem, and never get stuck in non-optimal fixed points."'),
('"Multi Focus Structured Light for Recovering Scene Shape and Global Illumination"', '"ECCV 2014"', '["Structured Light", "Depth from Focus/Defocus", "Global Light Transport"]', '"https://doi.org/10.1007/978-3-319-10590-1_14"', '"Illumination defocus and global illumination effects are major challenges for active illumination scene recovery algorithms. Illumination defocus limits the working volume of projector-camera systems and global illumination can induce large errors in shape estimates. In this paper, we develop an algorithm for scene recovery in the presence of both defocus and global light transport effects such as interreflections and sub-surface scattering. Our method extends the working volume by using structured light patterns at multiple projector focus settings. A careful characterization of projector blur allows us to decode even partially out-of-focus patterns. This enables our algorithm to recover scene shape and the direct and global illumination components over a large depth of field while still using a relatively small number of images (typically 25-30). We demonstrate the effectiveness of our approach by recovering high quality depth maps of scenes containing objects made of optically challenging materials such as wax, marble, soap, colored glass and translucent plastic."'),
('"Multi-body Depth-Map Fusion with Non-intersection Constraints"', '"ECCV 2014"', '["Multi-view stereo", "multi-body structure-from-motion", "depthmap fusion", "convex optimization", ', '"https://doi.org/10.1007/978-3-319-10599-4_47"', '"Depthmap fusion is the problem of computing dense 3D reconstructions from a set of depthmaps. Whereas this problem has received a lot of attention for purely rigid scenes, there is remarkably little prior work for dense reconstructions of scenes consisting of several moving rigid bodies or parts. This paper therefore explores this multi-body depthmap fusion problem. A first observation in the multi-body setting is that when treated naively, ghosting artifacts will emerge, ie. the same part will be reconstructed multiple times at different positions. We therefore introduce non-intersection constraints which resolve these issues: at any point in time, a point in space can only be occupied by at most one part. Interestingly enough, these constraints can be expressed as linear inequalities and as such define a convex set. We therefore propose to phrase the multi-body depthmap fusion problem in a convex voxel labeling framework. Experimental evaluation shows that our approach succeeds in computing artifact-free dense reconstructions of the individual parts with a minimal overhead due to the non-intersection constraints."'),
('"Multi-camera Scene Reconstruction via Graph Cuts"', '"ECCV 2002"', '["Stereo Match", "Visual Hull", "Scene Point", "Smoothness Term", "Scene Reconstruction"]', '"https://doi.org/10.1007/3-540-47977-5_6"', '"We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known viewpoints. Multi-camera scene reconstruction is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility. In this paper, we take an approach that has yielded excellent results for stereo, namely energy minimization via graph cuts. We first give an energy minimization formulation of the multi-camera scene reconstruction problem. The energy that we minimize treats the input images symmetrically, handles visibility properly, and imposes spatial smoothness while preserving discontinuities. As the energy function is NP-hard to minimize exactly, we give a graph cut algorithm that computes a local minimum in a strong sense. We handle all camera configurations where voxel coloring can be used, which is a large and natural class. Experimental data demonstrates the effectiveness of our approach."'),
('"Multi-camera Tracking and Atypical Motion Detection with Behavioral Maps"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_9"', '"We introduce a novel behavioral model to describe pedestrians motions, which is able to capture sophisticated motion patterns resulting from the mixture of different categories of random trajectories. Due to its simplicity, this model can be learned from video sequences in a totally unsupervised manner through an Expectation-Maximization procedure."'),
('"Multi-camera Tracking and Segmentation of Occluded People on Ground Plane Using Search-Guided Parti', '"ECCV 2006"', '["Ground Plane", "Color Model", "Appearance Model", "Camera View", "Multiple Camera"]', '"https://doi.org/10.1007/11744078_8"', '"A multi-view multi-hypothesis approach to segmenting and tracking multiple (possibly occluded) persons on a ground plane is proposed. During tracking, several iterations of segmentation are performed using information from human appearance models and ground plane homography. To more precisely locate the ground location of a person, all center vertical axes of the person across views are mapped to the top-view plane and their intersection point on the ground is estimated. To tackle the explosive state space due to multiple targets and views, iterative segmentation-searching is incorporated into a particle filtering framework. By searching for people\\u2019s ground point locations from segmentations, a set of a few good particles can be identified, resulting in low computational cost. In addition, even if all the particles are away from the true ground point, some of them move towards the true one through the iterated process as long as they are located nearby. We demonstrate the performance of the approach on several video sequences."'),
('"Multi-channel Shape-Flow Kernel Descriptors for Robust Video Event Detection and Retrieval"', '"ECCV 2012"', '["Kernel Descriptor", "Multi-channel", "Low Level Feature", "Video Event Detection", "TRECVID"]', '"https://doi.org/10.1007/978-3-642-33709-3_22"', '"Despite the success of spatio-temporal visual features, they are hand-designed and aggregate image or flow gradients using a pre-specified, uniform set of orientation bins. Kernel descriptors [1] generalize such orientation histograms by defining match kernels over image patches, and have shown superior performance for visual object and scene recognition. In our work, we make two contributions: first, we extend kernel descriptors to the spatio-temporal domain to model salient flow, gradient and texture patterns in video. Further, we apply our kernel descriptors to extract features from different color channels. Second, we present a fast algorithm for kernel descriptor computation of O(1) complexity for each pixel in each video patch, producing two orders of magnitude speedup over conventional kernel descriptors and other popular motion features. Our evaluation results on TRECVID MED 2011 dataset indicate that the proposed multi-channel shape-flow kernel descriptors outperform several other features including SIFT, SURF, STIP and Color SIFT."'),
('"Multi-class Classification on Riemannian Manifolds for Video Surveillance"', '"ECCV 2010"', '["Riemannian Manifold", "Tangent Space", "Sectional Curvature", "Covariance Matrice", "Video Surveil', '"https://doi.org/10.1007/978-3-642-15552-9_28"', '"In video surveillance, classification of visual data can be very hard, due to the scarce resolution and the noise characterizing the sensors\\u2019 data. In this paper, we propose a novel feature, the ARray of COvariances (ARCO), and a multi-class classification framework operating on Riemannian manifolds. ARCO is composed by a structure of covariance matrices of image features, able to extract information from data at prohibitive low resolutions. The proposed classification framework consists in instantiating a new multi-class boosting method, working on the manifold \\\\(Sym^{+}_d\\\\) of symmetric positive definite d\\u00d7d (covariance) matrices. As practical applications, we consider different surveillance tasks, such as head pose classification and pedestrian detection, providing novel state-of-the-art performances on standard datasets."'),
('"Multi-class Open Set Recognition Using Probability of Inclusion"', '"ECCV 2014"', '["Support Vector Machine", "Positive Class", "Support Vector Data Description", "Extreme Value Theor', '"https://doi.org/10.1007/978-3-319-10578-9_26"', '"The perceived success of recent visual recognition approaches has largely been derived from their performance on classification tasks, where all possible classes are known at training time. But what about open set problems, where unknown classes appear at test time? Intuitively, if we could accurately model just the positive data for any known class without overfitting, we could reject the large set of unknown classes even under an assumption of incomplete class knowledge. In this paper, we formulate the problem as one of modeling positive training data at the decision boundary, where we can invoke the statistical extreme value theory. A new algorithm called the P I -SVM is introduced for estimating the unnormalized posterior probability of class inclusion."'),
('"Multi-component Models for Object Detection"', '"ECCV 2012"', '["Component Model", "Object Detection", "Spatial Pyramid", "Procrustes Distance", "Spatial Pyramid M', '"https://doi.org/10.1007/978-3-642-33765-9_32"', '"In this paper, we propose a multi-component approach for object detection. Rather than attempting to represent an object category with a monolithic model, or pre-defining a reduced set of aspects, we form visual clusters from the data that are tight in appearance and configuration spaces. We train individual classifiers for each component, and then learn a second classifier that operates at the category level by aggregating responses from multiple components. In order to reduce computation cost during detection, we adopt the idea of object window selection, and our segmentation-based selection mechanism produces fewer than 500 windows per image while preserving high object recall. When compared to the leading methods in the challenging VOC PASCAL 2010 dataset, our multi-component approach obtains highly competitive results. Furthermore, unlike monolithic detection methods, our approach allows the transfer of finer-grained semantic information from the components, such as keypoint location and segmentation masks."'),
('"Multi-Entity Bayesian Networks for Knowledge-Driven Analysis of ICH Content"', '"ECCV 2014"', '["Semantic analysis", "Intangible cultural heritage", "Multi-entity bayesian networks"]', '"https://doi.org/10.1007/978-3-319-16181-5_25"', '"In this paper we introduce Multi-Entity Bayesian Networks (MEBNs) as the means to combine first-order logic with probabilistic inference and facilitate the semantic analysis of Intangible Cultural Heritage (ICH) content. First, we mention the need to capture and maintain ICH manifestations for the safeguarding of cultural treasures. Second, we present the MEBN models and stress their key features that can be used as a powerful tool for the aforementioned cause. Third, we present the methodology followed to build a MEBN model for the analysis of a traditional dance. Finally, we compare the efficiency of our MEBN model with that of a simple Bayesian network and demonstrate its superiority in cases that demand for situation-specific treatment."'),
('"Multi-label Feature Transform for Image Classifications"', '"ECCV 2010"', '["Support Vector Machine", "Principle Component Analysis", "Image Annotation", "Pairwise Similarity"', '"https://doi.org/10.1007/978-3-642-15561-1_57"', '"Image and video annotations are challenging but important tasks to understand digital multimedia contents in computer vision, which by nature is a multi-label multi-class classification problem because every image is usually associated with more than one semantic keyword. As a result, label assignments are no longer confined to class membership indications as in traditional single-label multi-class classification, which also convey important characteristic information to assess object similarity from knowledge perspective. Therefore, besides implicitly making use of label assignments to formulate label correlations as in many existing multi-label classification algorithms, we propose a novel Multi-Label Feature Transform (MLFT) approach to also explicitly use them as part of data features. Through two transformations on attributes and label assignments respectively, MLFT approach uses kernel to implicitly construct a label-augmented feature vector to integrate attributes and labels of a data set in a balanced manner, such that the data discriminability is enhanced because of taking advantage of the information from both data and label perspectives. Promising experimental results on four standard multi-label data sets from image annotation and other applications demonstrate the effectiveness of our approach."'),
('"Multi-label Image Segmentation for Medical Applications Based on Graph-Theoretic Electrical Potenti', '"MMBIA 2004"', '["Random Walker", "Image Segmentation", "Dirichlet Problem", "Seed Point", "Weak Boundary"]', '"https://doi.org/10.1007/978-3-540-27816-0_20"', '"A novel method is proposed for performing multi-label, semi-automated image segmentation. Given a small number of pixels with user-defined labels, one can analytically (and quickly) determine the probability that a random walker starting at each unlabeled pixel will first reach one of the pre-labeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension."'),
('"Multi-label Linear Discriminant Analysis"', '"ECCV 2010"', '["Support Vector Machine", "Dimensionality Reduction", "Linear Discriminant Analysis", "Image Annota', '"https://doi.org/10.1007/978-3-642-15567-3_10"', '"Multi-label problems arise frequently in image and video annotations, and many other related applications such as multi-topic text categorization, music classification, etc. Like other computer vision tasks, multi-label image and video annotations also suffer from the difficulty of high dimensionality because images often have a large number of features. Linear discriminant analysis (LDA) is a well-known method for dimensionality reduction. However, the classical Linear Discriminant Analysis (LDA) only works for single-label multi-class classifications and cannot be directly applied to multi-label multi-class classifications. It is desirable to naturally generalize the classical LDA to multi-label formulations. At the same time, multi-label data present a new opportunity to improve classification accuracy through label correlations, which are absent in single-label data. In this work, we propose a novel Multi-label Linear Discriminant Analysis (MLDA) method to take advantage of label correlations and explore the powerful classification capability of the classical LDA to deal with multi-label multi-class problems. Extensive experimental evaluations on five public multi-label data sets demonstrate excellent performance of our method."'),
('"Multi-layered Decomposition of Recurrent Scenes"', '"ECCV 2008"', '["Fundamental Period", "Recurrence Plot", "Dynamic Background", "Motion History Image", "Scene Activ', '"https://doi.org/10.1007/978-3-540-88690-7_43"', '"There is considerable interest in techniques capable of identifying anomalies and unusual events in busy outdoor scenes, e.g. road junctions. Many approaches achieve this by exploiting deviations in spatial appearance from some expected norm accumulated by a model over time. In this work we show that much can be gained from explicitly modelling temporal aspects in detail. Specifically, many traffic junctions are regulated by lights controlled by a timing device of considerable precision, and it is in these situations that we advocate a model which learns periodic spatio-temporal patterns with a view to highlighting anomalous events such as broken-down vehicles, traffic accidents, or pedestrians jay-walking. More specifically, by estimating autocovariance of self-similarity, used previously in the context gait recognition, we characterize a scene by identifying a global fundamental period. As our model, we introduce a spatio-temporal grid of histograms built in accordance with some chosen feature. This model is then used to classify objects found in subsequent test data. In particular we demonstrate the effect of such characterization experimentally by monitoring the bounding box aspect ratio and optical flow field of objects detected on a road traffic junction, enabling our model to discriminate between people and cars sufficiently well to provide useful warnings of adverse behaviour in real time."'),
('"Multi-level Adaptive Active Learning for Scene Classification"', '"ECCV 2014"', '["Active Learning", "Scene Classification"]', '"https://doi.org/10.1007/978-3-319-10584-0_16"', '"Semantic scene classification is a challenging problem in computer vision. In this paper, we present a novel multi-level active learning approach to reduce the human annotation effort for training robust scene classification models. Different from most existing active learning methods that can only query labels for selected instances at the target categorization level, i.e., the scene class level, our approach establishes a semantic framework that predicts scene labels based on a latent object-based semantic representation of images, and is capable to query labels at two different levels, the target scene class level (abstractive high level) and the latent object class level (semantic middle level). Specifically, we develop an adaptive active learning strategy to perform multi-level label query, which maintains the default label query at the target scene class level, but switches to the latent object class level whenever an \\u201cunexpected\\u201d target class label is returned by the labeler. We conduct experiments on two standard scene classification datasets to investigate the efficacy of the proposed approach. Our empirical results show the proposed adaptive multi-level active learning approach can outperform both baseline active learning methods and a state-of-the-art multi-level active learning method."'),
('"Multi-modal and Multi-spectral Registration for Natural Images"', '"ECCV 2014"', '["multi-modal", "multi-spectral", "dense matching", "variational model"]', '"https://doi.org/10.1007/978-3-319-10593-2_21"', '"Images now come in different forms \\u2013 color, near-infrared, depth, etc. \\u2013 due to the development of special and powerful cameras in computer vision and computational photography. Their cross-modal correspondence establishment is however left behind. We address this challenging dense matching problem considering structure variation possibly existing in these image sets and introduce new model and solution. Our main contribution includes designing the descriptor named robust selective normalized cross correlation (RSNCC) to establish dense pixel correspondence in input images and proposing its mathematical parameterization to make optimization tractable. A computationally robust framework including global and local matching phases is also established. We build a multi-modal dataset including natural images with labeled sparse correspondence. Our method will benefit image and vision applications that require accurate image alignment."'),
('"Multi-Modal Distance Metric Learning: ABayesian Non-parametric Approach"', '"ECCV 2014"', '["Metric learning", "Multi-modal data", "Beta process", "Variational inference", "Gibbs sampling"]', '"https://doi.org/10.1007/978-3-319-16199-0_5"', '"In many real-world applications (e.g. social media application), data usually consists of diverse input modalities that originates from various heterogeneous sources. Learning a similarity measure for such data is of great importance for vast number of applications such as classification, clustering, retrieval, etc."'),
('"Multi-modal Gesture Recognition Using Skeletal Joints and Motion Trail Model"', '"ECCV 2014"', '["Gesture recognition", "Skeletal joints", "HMM", "SVM", "2DMTM", "PHOG"]', '"https://doi.org/10.1007/978-3-319-16178-5_44"', '"This paper proposes a novel approach to multi-modal gesture recognition by using skeletal joints and motion trail model. The approach includes two modules, i.e. spotting and recognition. In the spotting module, a continuous gesture sequence is segmented into individual gesture intervals based on hand joint positions within a sliding window. In the recognition module, three models are combined to classify each gesture interval into one gesture category. For skeletal model, Hidden Markov Models (HMM) and Support Vector Machines (SVM) are adopted for classifying skeleton features. For depth maps and user masks, we employ 2D Motion Trail Model (2DMTM) for gesture representation to capture motion region information. SVM is then used to classify Pyramid Histograms of Oriented Gradient (PHOG) features from 2DMTM. These three models are complementary to each other. Finally, a fusion scheme incorporates the probability weights of each classifier for gesture recognition. The proposed approach is evaluated on the 2014 ChaLearn Multi-modal Gesture Recognition Challenge dataset. Experimental results demonstrate that the proposed approach using combined models outperforms single-modal approaches, and the recognition module can perform effectively on user-independent gesture recognition."'),
('"Multi-modal Unsupervised Feature Learning for RGB-D Scene Labeling"', '"ECCV 2014"', '["RGB-D scene labeling", "unsupervised feature learning", "joint feature learning and encoding", "mu', '"https://doi.org/10.1007/978-3-319-10602-1_30"', '"Most of the existing approaches for RGB-D indoor scene labeling employ hand-crafted features for each modality independently and combine them in a heuristic manner. There has been some attempt on directly learning features from raw RGB-D data, but the performance is not satisfactory. In this paper, we adapt the unsupervised feature learning technique for RGB-D labeling as a multi-modality learning problem. Our learning framework performs feature learning and feature encoding simultaneously which significantly boosts the performance. By stacking basic learning structure, higher-level features are derived and combined with lower-level features for better representing RGB-D data. Experimental results on the benchmark NYU depth dataset show that our method achieves competitive performance, compared with state-of-the-art."'),
('"Multi-modality Gesture Detection and Recognition with Un-supervision, Randomization and Discriminat', '"ECCV 2014"', '["Multi-modality gesture", "Unsupervised learning", "Random forest", "Discriminative training"]', '"https://doi.org/10.1007/978-3-319-16178-5_43"', '"We describe in this paper our gesture detection and recognition system for the 2014 ChaLearn Looking at People (Track 3: Gesture Recognition) organized by ChaLearn in conjunction with the ECCV 2014 conference. The competition\\u2019s task was to learn a vacabulary of 20 types of Italian gestures and detect them in sequences. Our system adopts a multi-modality approach for detecting as well as recognizing the gestures. The goal of our approach is to identify semantically meaningful contents from dense sampling spatio-temporal feature space for gesture recognition. To achieve this, we develop three concepts under the random forest framework: un-supervision; discrimination; and randomization. Un-supervision learns spatio-temporal features from two channels (grayscale and depth) of RGB-D video in an unsupervised way. Discrimination extracts the information in dense sampling spatio-temporal space effectively. Randomization explores the dense sampling spatio-temporal feature space efficiently. An evaluation of our approach shows that we achieve a mean Jaccard Index of \\\\(0.6489\\\\), and a mean average accuracy of \\\\(90.3\\\\,\\\\%\\\\) over the test dataset."'),
('"Multi-Model Component-Based Tracking Using Robust Information Fusion"', '"SMVP 2004"', '["Motion Estimation", "Current Frame", "Appearance Model", "Component Location", "Model Template"]', '"https://doi.org/10.1007/978-3-540-30212-4_6"', '"One of the most difficult aspects of visual object tracking is the handling of occlusions and target appearance changes due to variations in illumination and viewing direction. To address these challenges we introduce a novel tracking technique that relies on component-based target representations and on robust fusion to integrate model information across frames. More specifically, we maintain a set of component-based models of the target, acquired at different time instances, and combine robustly the estimated motion suggested by each component to determine the next position of the target. In this paper we allow the target to undergo similarity transformations, although the framework is general enough to be applied to more complex ones. We pay particular attention to uncertainty handling and propagation, for component motion estimation, robust fusion across time and estimation of the similarity transform. The theory is tested on very difficult real tracking scenarios with promising results."'),
('"Multi-person Tracking with Sparse Detection and Continuous Segmentation"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15549-9_29"', '"This paper presents an integrated framework for mobile street-level tracking of multiple persons. In contrast to classic tracking-by-detection approaches, our framework employs an efficient level-set tracker in order to follow individual pedestrians over time. This low-level tracker is initialized and periodically updated by a pedestrian detector and is kept robust through a series of consistency checks. In order to cope with drift and to bridge occlusions, the resulting tracklet outputs are fed to a high-level multi-hypothesis tracker, which performs longer-term data association. This design has the advantage of simplifying short-term data association, resulting in higher-quality tracks that can be maintained even in situations where the pedestrian detector does no longer yield good detections. In addition, it requires the pedestrian detector to be active only part of the time, resulting in computational savings. We quantitatively evaluate our approach on several challenging sequences and show that it achieves state-of-the-art performance."'),
('"Multi-scale Clustering of Frame-to-Frame Correspondences for Motion Segmentation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33709-3_32"', '"We present an approach for motion segmentation using independently detected keypoints instead of commonly used tracklets or trajectories. This allows us to establish correspondences over non- consecutive frames, thus we are able to handle multiple object occlusions consistently. On a frame-to-frame level, we extend the classical split-and-merge algorithm for fast and precise motion segmentation. Globally, we cluster multiple of these segmentations of different time scales with an accurate estimation of the number of motions. On the standard benchmarks, our approach performs best in comparison to all algorithms which are able to handle unconstrained missing data. We further show that it works on benchmark data with more than 98% of the input data missing. Finally, the performance is evaluated on a mobile-phone-recorded sequence with multiple objects occluded at the same time."'),
('"Multi-scale Deep Learning for Gesture Detection and Localization"', '"ECCV 2014"', '["Gesture recognition", "Multi-modal systems", "Deep learning"]', '"https://doi.org/10.1007/978-3-319-16178-5_33"', '"We present a method for gesture detection and localization based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at two temporal scales. Key to our technique is a training strategy which exploits i) careful initialization of individual modalities; and ii) gradual fusion of modalities from strongest to weakest cross-modality structure. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams."'),
('"Multi-scale EM-ICP: A Fast and Robust Approach for Surface Registration"', '"ECCV 2002"', '["Surface registration", "ICP algorithm", "EM algorithm", "Multiscale"]', '"https://doi.org/10.1007/3-540-47979-1_28"', '"We investigate in this article the rigid registration of large sets of points, generally sampled from surfaces. We formulate this problem as a general Maximum-Likelihood (ML) estimation of the transformation and the matches. We show that, in the specific case of a Gaussian noise, it corresponds to the Iterative Closest Point algorithm (ICP) with the Mahalanobis distance."'),
('"Multi-scale Improves Boundary Detection in Natural Images"', '"ECCV 2008"', '["Average Precision", "Natural Image", "Boundary Detection", "Relative Contrast", "Berkeley Segmenta', '"https://doi.org/10.1007/978-3-540-88690-7_40"', '"In this work we empirically study the multi-scale boundary detection problem in natural images. We utilize local boundary cues including contrast, localization and relative contrast, and train a classifier to integrate them across scales. Our approach successfully combines strengths from both large-scale detection (robust but poor localization) and small-scale detection (detail-preserving but sensitive to clutter). We carry out quantitative evaluations on a variety of boundary and object datasets with human-marked groundtruth. We show that multi-scale boundary detection offers large improvements, ranging from 20% to 50%, over single-scale approaches. This is the first time that multi-scale is demonstrated to improve boundary detection on large datasets of natural images."'),
('"Multi-scale Orderless Pooling of Deep Convolutional Activation Features"', '"ECCV 2014"', '["Image Retrieval", "Convolutional Neural Network", "Bighorn Sheep", "Scene Recognition", "Fisher Ve', '"https://doi.org/10.1007/978-3-319-10584-0_26"', '"Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets."'),
('"Multi-scale Patch Based Collaborative Representation for Face Recognition with Margin Distribution ', '"ECCV 2012"', '["Training Sample", "Face Recognition", "Patch Size", "Ensemble Learning", "Query Sample"]', '"https://doi.org/10.1007/978-3-642-33718-5_59"', '"Small sample size is one of the most challenging problems in face recognition due to the difficulty of sample collection in many real-world applications. By representing the query sample as a linear combination of training samples from all classes, the so-called collaborative representation based classification (CRC) shows very effective face recognition performance with low computational cost. However, the recognition rate of CRC will drop dramatically when the available training samples per subject are very limited. One intuitive solution to this problem is operating CRC on patches and combining the recognition outputs of all patches. Nonetheless, the setting of patch size is a non-trivial task. Considering the fact that patches on different scales can have complementary information for classification, we propose a multi-scale patch based CRC method, while the ensemble of multi-scale outputs is achieved by regularized margin distribution optimization. Our extensive experiments validated that the proposed method outperforms many state-of-the-art patch based face recognition algorithms."'),
('"Multi-stage Contour Based Detection of Deformable Objects"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_37"', '"We present an efficient multi stage approach to detection of deformable objects in real, cluttered images given a single or few hand drawn examples as models. The method handles deformations of the object by first breaking the given model into segments at high curvature points. We allow bending at these points as it has been studied that deformation typically happens at high curvature points. The broken segments are then scaled, rotated, deformed and searched independently in the gradient image. Point maps are generated for each segment that represent the locations of the matches for that segment. We then group k points from the point maps of k adjacent segments using a cost function that takes into account local scale variations as well as inter-segment orientations. These matched groups yield plausible locations for the objects. In the fine matching stage, the entire object contour in the localized regions is built from the k-segment groups and given a comprehensive score in a method that uses dynamic programming. An evaluation of our algorithm on a standard dataset yielded results that are better than published work on the same dataset. At the same time, we also evaluate our algorithm on additional images with considerable object deformations to verify the robustness of our method."'),
('"Multi-stage Sampling with Boosting Cascades for Pedestrian Detection in Images and Videos"', '"ECCV 2010"', '["Object Detection", "Proposal Distribution", "Miss Rate", "Pedestrian Detection", "Slide Window"]', '"https://doi.org/10.1007/978-3-642-15567-3_15"', '"Many works address the problem of object detection by means of machine learning with boosted classifiers. They exploit sliding window search, spanning the whole image: the patches, at all possible positions and sizes, are sent to the classifier. Several methods have been proposed to speed up the search (adding complementary features or using specialized hardware). In this paper we propose a statistical-based search approach for object detection which uses a Monte Carlo sampling approach for estimating the likelihood density function with Gaussian kernels. The estimation relies on a multi-stage strategy where the proposal distribution is progressively refined by taking into account the feedback of the classifier (i.e. its response). For videos, this approach is plugged in a Bayesian-recursive framework which exploits the temporal coherency of the pedestrians. Several tests on both still images and videos on common datasets are provided in order to demonstrate the relevant speedup and the increased localization accuracy with respect to sliding window strategy using a pedestrian classifier based on covariance descriptors and a cascade of Logitboost classifiers."'),
('"Multi-Task Multi-Sample Learning"', '"ECCV 2014"', '["Multi-task learning", "Exemplar SVMs"]', '"https://doi.org/10.1007/978-3-319-16199-0_6"', '"In the exemplar SVM (E-SVM) approach of Malisiewicz et al., ICCV 2011, an ensemble of SVMs is learnt, with each SVM trained independently using only a single positive sample and all negative samples for the class. In this paper we develop a multi-sample learning (MSL) model which enables joint regularization of the E-SVMs without any additional cost over the original ensemble learning. The advantage of the MSL model is that the degree of sharing between positive samples can be controlled, such that the classification performance of either an ensemble of E-SVMs (sample independence) or a standard SVM (all positive samples used) is reproduced. However, between these two limits the model can exceed the performance of either. This MSL framework is inspired by multi-task learning approaches."'),
('"Multi-thread Parsing for Recognizing Complex Events in Videos"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_55"', '"This paper presents a probabilistic grammar approach to the recognition of complex events in videos. Firstly, based on the original motion features, a rule induction algorithm is adopted to learn the event rules. Then, a multi-thread parsing (MTP) algorithm is adopted to recognize the complex events involving parallel temporal relation in sub-events, whereas the commonly used parser can only handle the sequential relation. Additionally, a Viterbi-like error recovery strategy is embedded in the parsing process to correct the large time scale errors, such as insertion and deletion errors. Extensive experiments including indoor gymnastic exercises and outdoor traffic events are performed. As supported by experimental results, the MTP algorithm can effectively recognize the complex events due to the strong discriminative representation and the error recovery strategy."'),
('"Multi-User Egocentric Online System for Unsupervised Assistance on Object Usage"', '"ECCV 2014"', '["Video guidance", "Wearable computing", "Real-time computer vision", "Assistive computing", "Object', '"https://doi.org/10.1007/978-3-319-16199-0_34"', '"We present an online fully unsupervised approach for automatically extracting video guides of how objects are used from wearable gaze trackers worn by multiple users. Given egocentric video and eye gaze from multiple users performing tasks, the system discovers task-relevant objects and automatically extracts guidance videos on how these objects have been used. In the assistive mode, the paper proposes a method for selecting a suitable video guide to be displayed to a novice user indicating how to use an object, purely triggered by the user\\u2019s gaze. The approach is tested on a variety of daily tasks ranging from opening a door, to preparing coffee and operating a gym machine."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Multi-view Constraints between Collineations: Application to Self-Calibration from Unknown Planar S', '"ECCV 2000"', '["Self-calibration", "Homography"]', '"https://doi.org/10.1007/3-540-45053-X_39"', '"In this paper we describe an efficient method to impose the constraints existing between the collineations which can be computed from a sequence of views of a planar structure. These constraints are usually not taken into account by multi-view techniques in order not to increase the computational complexity of the algorithms. However, imposing the constraints is very useful since it allows a reduction in the geometric errors in the reprojected features and provides a consistent set of collineations which can be used for several applications such as mosaicing, reconstruction and self-calibration. In order to show the validity of our approach, this paper focus on self-calibration from unknown planar structures proposing a new method exploiting the consistent set of collineations. Our method can deal with an arbitrary number of views and an arbitrary number of planes and varying camera internal parameters. However, for simplicity this paper will only discuss the case with constant camera internal parameters. The results obtained with synthetic and real data are very accurate and stable even when using only a few images."'),
('"Multi-view Discriminant Analysis"', '"ECCV 2012"', '["Multi-view Discriminant Analysis", "Multi-view Face Recognition", "Common space for Multi-view"]', '"https://doi.org/10.1007/978-3-642-33718-5_58"', '"The same object can be observed at different viewpoints or even by different sensors, thus generating multiple distinct even heterogeneous samples. Nowadays, more and more applications need to recognize object from distinct views. Some seminal works have been proposed for object recognition across two views and applied to multiple views in some inefficient pairwise manner. In this paper, we propose a Multi-view Discriminant Analysis (MvDA) method, which seeks for a discriminant common space by jointly learning multiple view-specific linear transforms for robust object recognition from multiple views, in a non-pairwise manner. Specifically, our MvDA is formulated to jointly solve the multiple linear transforms by optimizing a generalized Rayleigh quotient, i.e., maximizing the between-class variations and minimizing the within-class variations of the low-dimensional embeddings from both intra-view and inter-view in the common space. By reformulating this problem as a ratio trace problem, an analytical solution can be achieved by using the generalized eigenvalue decomposition. The proposed method is applied to three multi-view face recognition problems: face recognition across poses, photo-sketch face recognition, and Visual (VIS) image vs. Near Infrared (NIR) image face recognition. Evaluations are conducted respectively on Multi-PIE, CUFSF and HFB databases. Intensive experiments show that MvDA can achieve a more discriminant common space, with up to 13% improvement compared with the best known results."'),
('"Multi-View Face Image Synthesis Using Factorization Model"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_19"', '"We present a sample-based method for synthesizing face images in a wide range of view. Here the \\u201dhuman identity\\u201d and \\u201dhead pose\\u201d are regarded as two influence factors of face appearance and a factorization model is used to learn their interaction with a face database. Our method extends original bilinear factorization model to nonlinear case so that global optimum solution can be found in solving \\u201dtranslation\\u201d task. Thus, some view of a new person\\u2019s face image is able to be \\u201dtranslated\\u201d into other views. Experimental results show that the synthesized faces are quite similar to the ground-truth. The proposed method can be applied to a broad area of human computer interaction, such as face recognition across view or face synthesis in virtual reality."'),
('"Multi-view Facial Expression Recognition Analysis with Generic Sparse Coding Feature"', '"ECCV 2012"', '["Tilt Angle", "Recognition Rate", "Emotion Recognition", "Recognition Performance", "Sparse Code"]', '"https://doi.org/10.1007/978-3-642-33885-4_58"', '"Expression recognition from non-frontal faces is a challenging research area with growing interest. This paper works with a generic sparse coding feature, inspired from object recognition, for multi-view facial expression recognition. Our extensive experiments on face images with seven pan angles and five tilt angles, rendered from the BU-3DFE database, achieve state-of-the-art results. We achieve a recognition rate of 69.1% on all images with four expression intensity levels, and a recognition performance of 76.1% on images with the strongest expression intensity. We then also present detailed analysis of the variations in expression recognition performance for various pose changes."'),
('"Multi-view Matching for Unordered Image Sets, or \\u201cHow Do I Organize My Holiday Snaps?\\u201d"', '"ECCV 2002"', '["Span Tree", "Image Patch", "Invariant Vector", "Invariant Space", "Maximally Stable Extremal Regio', '"https://doi.org/10.1007/3-540-47969-4_28"', '"There has been considerable success in automated reconstruction for image sequences where small baseline algorithms can be used to establish matches across a number of images. In contrast in the case of widely separated views, methods have generally been restricted to two or three views."'),
('"Multi-way Clustering Using Super-Symmetric Non-negative Tensor Factorization"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_46"', '"We consider the problem of clustering data into k \\u2265 2 clusters given complex relations \\u2014 going beyond pairwise \\u2014 between the data points. The complex n-wise relations are modeled by an n-way array where each entry corresponds to an affinity measure over an n-tuple of data points. We show that a probabilistic assignment of data points to clusters is equivalent, under mild conditional independence assumptions, to a super-symmetric non-negative factorization of the closest hyper-stochastic version of the input n-way affinity array. We derive an algorithm for finding a local minimum solution to the factorization problem whose computational complexity is proportional to the number of n-tuple samples drawn from the data. We apply the algorithm to a number of visual interpretation problems including 3D multi-body segmentation and illumination-based clustering of human faces."'),
('"Multibody Structure and Motion: 3-D Reconstruction of Independently Moving Objects"', '"ECCV 2000"', '["Focal Length", "Camera Calibration", "Fundamental Matrix", "Internal Parameter", "Foreground Objec', '"https://doi.org/10.1007/3-540-45054-8_58"', '"This paper extends the recovery of structure and motion to image sequences with several independently moving objects. The motion, structure, and camera calibration are all a-priori unknown. The fundamental constraint that we introduce is that multiple motions must share the same camera parameters. Existing work on independent motions has not employed this constraint, and therefore has not gained over independent static-scene reconstructions."'),
('"Multiclass Image Labeling with Semidefinite Programming"', '"ECCV 2006"', '["Lagrangian Relaxation", "Image Element", "Quadratic Assignment Problem", "Image Label", "Label Pro', '"https://doi.org/10.1007/11744047_35"', '"We propose a semidefinite relaxation technique for multiclass image labeling problems. In this context, we consider labeling as a special case of supervised classification with a predefined number of classes and known but arbitrary dissimilarities between each image element and each class. Using Markov random fields to model pairwise relationships, this leads to a global energy minimization problem. In order to handle its combinatorial complexity, we apply Lagrangian relaxation to derive a semidefinite program, which has several advantageous properties over alternative methods like graph cuts. In particular, there are no restrictions concerning the form of the pairwise interactions, which e.g. allows us to incorporate a basic shape concept into the energy function. Based on the solution matrix of our convex relaxation, a suboptimal solution of the original labeling problem can be easily computed. Statistical ground-truth experiments and several examples of multiclass image labeling and restoration problems show that high quality solutions are obtained with this technique."'),
('"Multidimensional Spectral Hashing"', '"ECCV 2012"', '["Binary Code", "Distance Threshold", "Optimal Code", "Factorization Problem", "Kernel Trick"]', '"https://doi.org/10.1007/978-3-642-33715-4_25"', '"With the growing availability of very large image databases, there has been a surge of interest in methods based on \\u201csemantic hashing\\u201d, i.e. compact binary codes of data-points so that the Hamming distance between codewords correlates with similarity. In reviewing and comparing existing methods, we show that their relative performance can change drastically depending on the definition of ground-truth neighbors. Motivated by this finding, we propose a new formulation for learning binary codes which seeks to reconstruct the affinity between datapoints, rather than their distances. We show that this criterion is intractable to solve exactly, but a spectral relaxation gives an algorithm where the bits correspond to thresholded eigenvectors of the affinity matrix, and as the number of datapoints goes to infinity these eigenvectors converge to eigenfunctions of Laplace-Beltrami operators, similar to the recently proposed Spectral Hashing (SH) method. Unlike SH whose performance may degrade as the number of bits increases, the optimal code using our formulation is guaranteed to faithfully reproduce the affinities as the number of bits increases. We show that the number of eigenfunctions needed may increase exponentially with dimension, but introduce a \\u201ckernel trick\\u201d to allow us to compute with an exponentially large number of bits but using only memory and computation that grows linearly with dimension. Experiments shows that MDSH outperforms the state-of-the art, especially in the challenging regime of small distance thresholds."'),
('"Multilinear Analysis of Image Ensembles: TensorFaces"', '"ECCV 2002"', '["Singular Value Decomposition", "Independent Component Analysis", "Facial Image", "Independent Comp', '"https://doi.org/10.1007/3-540-47969-4_30"', '"Natural images are the composite consequence of multiple factors related to scene structure, illumination, and imaging. Multilinear algebra, the algebra of higher-order tensors, offers a potent mathematical framework for analyzing the multifactor structure of image ensembles and for addressing the difficult problem of disentangling the constituent factors or modes. Our multilinear modeling technique employs a tensor extension of the conventional matrix singular value decomposition (SVD), known as the N-mode SVD. As a concrete example, we consider the multilinear analysis of ensembles of facial images that combine several modes, including different facial geometries (people), expressions, head poses, and lighting conditions. Our resulting \\u201cTensorFaces\\u201d representation has several advantages over conventional eigenfaces. More generally, multilinear analysis shows promise as a unifying framework for a variety of computer vision problems."'),
('"Multilinear Wavelets: A Statistical Shape Space for Human Faces"', '"ECCV 2014"', '["Statistical shape models", "human faces", "multilinear model", "wavelets"]', '"https://doi.org/10.1007/978-3-319-10590-1_20"', '"We present a statistical model for 3D human faces in varying expression, which decomposes the surface of the face using a wavelet transform, and learns many localized, decorrelated multilinear models on the resulting coefficients. Using this model we are able to reconstruct faces from noisy and occluded 3D face scans, and facial motion sequences. Accurate reconstruction of face shape is important for applications such as tele-presence and gaming. The localized and multi-scale nature of our model allows for recovery of fine-scale detail while retaining robustness to severe noise and occlusion, and is computationally efficient and scalable. We validate these properties experimentally on challenging data in the form of static scans and motion sequences. We show that in comparison to a global multilinear model, our model better preserves fine detail and is computationally faster, while in comparison to a localized PCA model, our model better handles variation in expression, is faster, and allows us to fix identity parameters for a given subject."'),
('"Multimodal Data Representations with Parameterized Local Structures"', '"ECCV 2002"', '["Function Association", "Initial Cluster", "Nonlinear Structure", "Local Component", "Multimodal Di', '"https://doi.org/10.1007/3-540-47969-4_12"', '"In many vision problems, the observed data lies in a nonlinear manifold in a high-dimensional space. This paper presents a generic modelling scheme to characterize the nonlinear structure of the manifold and to learn its multimodal distribution. Our approach represents the data as a linear combination of parameterized local components, where the statistics of the component parameterization describe the nonlinear structure of the manifold. The components are adaptively selected from the training data through a progressive density approximation procedure, which leads to the maximum likelihood estimate of the underlying density. We show results on both synthetic and real training sets, and demonstrate that the proposed scheme has the ability to reveal important structures of the data."'),
('"Multimodal Elastic Matching of Brain Images"', '"ECCV 2000"', '["Multimodality", "Elastic registration", "Intensity correction", "Robust estimation", "Medical imag', '"https://doi.org/10.1007/3-540-45053-X_33"', '"This paper presents an original method for three-dimensional elastic registration of multimodal images. We propose to make use of a scheme that iterates between correcting for intensity differences between images and performing standard monomodal registration. The core of our contribution resides in providing a method that finds the transformation that maps the intensities of one image to those of another. It makes the assumption that there are at most two functional dependences between the intensities of structures present in the images to register, and relies on robust estimation techniques to evaluate these functions. We provide results showing successful registration between several imaging modalities involving segmentations, T1 magnetic resonance (MR), T2 MR, proton density (PD) MR and computed tomography (CT)."'),
('"Multiphase Dynamic Labeling for Variational Recognition-Driven Image Segmentation"', '"ECCV 2004"', '["Active Contour", "Familiar Object", "Label Function", "Segmentation Scheme", "Variational Framewor', '"https://doi.org/10.1007/978-3-540-24673-2_7"', '"We propose a variational framework for the integration multiple competing shape priors into level set based segmentation schemes. By optimizing an appropriate cost functional with respect to both a level set function and a (vector-valued) labeling function, we jointly generate a segmentation (by the level set function) and a recognition-driven partition of the image domain (by the labeling function) which indicates where to enforce certain shape priors. Our framework fundamentally extends previous work on shape priors in level set segmentation by directly addressing the central question of where to apply which prior. It allows for the seamless integration of numerous shape priors such that \\u2013 while segmenting both multiple known and unknown objects \\u2013 the level set process may selectively use specific shape knowledge for simultaneously enhancing segmentation and recognizing shape."'),
('"Multiple Alignment of Spatiotemporal Deformable Objects for the Average-Organ Computation"', '"ECCV 2014"', '["Spatial Average", "Beating Heart", "Volumetric Data", "Frame Average", "Medical Image Analysis"]', '"https://doi.org/10.1007/978-3-319-16220-1_25"', '"We deal with multiple image warping, which computes deformation fields between an image and a collection of images, as an extension of variational image registration. Using multiple image warping, we develop a variational method for the computation of average images of biological organs in three-dimensional Euclidean space. The average shape of three-dimensional biological organs is an essential feature to discriminate abnormal organs from normal organs. There are two kinds of volumetric image sets in medical image analysis. The first one is a collection of static volumetric data of an organ and/or organs. The other is a collection of temporal volumetric data of an organ and/or organs. A collection of temporal volumetric beating hearts is an example of temporal volumetric data. For spatiotemporal volumetric data, we can compute (1) the temporal average, which is the average of a heart during a cycle, (2) the frame average, which is the average of hearts at a frame, and (3) the temporal average of frame averages."'),
('"Multiple Classifier System Approach to Model Pruning in Object Recognition"', '"ECCV 2004"', '["Object Recognition", "Recognition Rate", "Pruning Strategy", "Multiple Expert", "Candidate Label"]', '"https://doi.org/10.1007/978-3-540-24673-2_28"', '"We propose a multiple classifier system approach to object recognition in computer vision. The aim of the approach is to use multiple experts successively to prune the list of candidate hypotheses that have to be considered for object interpretation. The experts are organised in a serial architecture, with the later stages of the system dealing with a monotonically decreasing number of models. We develop a theoretical model which underpins this approach to object recognition and show how it relates to various heuristic design strategies advocated in the literature. The merits of the advocated approach are then demonstrated experimentally using the SOIL database. We show how the overall performance of a two stage object recognition system, designed using the proposed methodology, improves. The improvement is achieved in spite of using a weak recogniser for the first (pruning) stage. The effects of different pruning strategies are demonstrated."'),
('"Multiple Component Learning for Object Detection"', '"ECCV 2008"', '["Training Image", "Object Detection", "Pedestrian Detection", "Multiple Instance Learn", "Object La', '"https://doi.org/10.1007/978-3-540-88688-4_16"', '"Object detection is one of the key problems in computer vision. In the last decade, discriminative learning approaches have proven effective in detecting rigid objects, achieving very low false positives rates. The field has also seen a resurgence of part-based recognition methods, with impressive results on highly articulated, diverse object categories. In this paper we propose a discriminative learning approach for detection that is inspired by part-based recognition approaches. Our method, Multiple Component Learning (mcl), automatically learns individual component classifiers and combines these into an overall classifier. Unlike previous methods, which rely on either fairly restricted part models or labeled part data, mcl learns powerful component classifiers in a weakly supervised manner, where object labels are provided but part labels are not. The basis of mcl lies in learning a set classifier; we achieve this by combining boosting with weakly supervised learning, specifically the Multiple Instance Learning framework (mil). mcl is general, and we demonstrate results on a range of data from computer audition and computer vision. In particular, mcl outperforms all existing methods on the challenging INRIA pedestrian detection dataset, and unlike methods that are not part-based, mcl is quite robust to occlusions."'),
('"Multiple Human Pose Estimation with Temporally Consistent 3D Pictorial Structures"', '"ECCV 2014"', '["Human pose estimation", "3D pictorial structures", "Part-based pose estimation"]', '"https://doi.org/10.1007/978-3-319-16178-5_52"', '"Multiple human 3D pose estimation from multiple camera views is a challenging task in unconstrained environments. Each individual has to be matched across each view and then the body pose has to be estimated. Additionally, the body pose of every individual changes in a consistent manner over time. To address these challenges, we propose a temporally consistent 3D Pictorial Structures model (3DPS) for multiple human pose estimation from multiple camera views. Our model builds on the 3D Pictorial Structures to introduce the notion of temporal consistency between the inferred body poses. We derive this property by relying on multi-view human tracking. Identifying each individual before inference significantly reduces the size of the state space and positively influences the performance as well. To evaluate our method, we use two challenging multiple human datasets in unconstrained environments. We compare our method with the state-of-the-art approaches and achieve better results."'),
('"Multiple Hypothesis Tracking for Automatic Optical Motion Capture"', '"ECCV 2002"', '["Visual motion", "correspondence problem", "tracking", "optical motion capture"]', '"https://doi.org/10.1007/3-540-47969-4_35"', '"We present a technique for performing the tracking stage of optical motion capture which retains, at each time frame, multiple marker association hypotheses and estimates of the subject\\u2019s position. Central to this technique are the equations for calculating the likelihood of a sequence of association hypotheses, which we develop using a Bayesian approach. The system is able to perform motion capture using fewer cameras and a lower frame rate than has been used previously, and does not require the assistance of a human operator. We conclude by demonstrating the tracker on real data and provide an example in which our technique is able to correctly determine all marker associations and standard tracking techniques fail."'),
('"Multiple Hypothesis Video Segmentation from Superpixel Flows"', '"ECCV 2010"', '["Video Sequence", "Video Stream", "Processing Window", "Video Segmentation", "Label Disagreement"]', '"https://doi.org/10.1007/978-3-642-15555-0_20"', '"Multiple Hypothesis Video Segmentation (MHVS) is a method for the unsupervised photometric segmentation of video sequences. MHVS segments arbitrarily long video streams by considering only a few frames at a time, and handles the automatic creation, continuation and termination of labels with no user initialization or supervision. The process begins by generating several pre-segmentations per frame and enumerating multiple possible trajectories of pixel regions within a short time window. After assigning each trajectory a score, we let the trajectories compete with each other to segment the sequence. We determine the solution of this segmentation problem as the MAP labeling of a higher-order random field. This framework allows MHVS to achieve spatial and temporal long-range label consistency while operating in an on-line manner. We test MHVS on several videos of natural scenes with arbitrary camera and object motion."'),
('"Multiple Instance Boost Using Graph Embedding Based Decision Stump for Pedestrian Detection"', '"ECCV 2008"', '["Support Vector Machine", "Multiple Instance", "Linear Support Vector Machine", "Histogram Feature"', '"https://doi.org/10.1007/978-3-540-88693-8_40"', '"Pedestrian detection in still image should handle the large appearance and stance variations arising from the articulated structure, various clothing of human as well as viewpoints. In this paper, we address this problem from a view which utilizes multiple instances to represent the variations in multiple instance learning (MIL) framework. Specifically, logistic multiple instance boost (LMIBoost) is advocated to learn the pedestrian appearance model. To efficiently use the histogram feature, we propose the graph embedding based decision stump for the data with non-Gaussian distribution. First the topology structure of the examples are carefully designed to keep between-class far and within-class close. Second, K-means algorithm is adopted to fast locate the multiple decision planes for the weak classifier. Experiments show the improved accuracy of the proposed approach in comparison with existing pedestrian detection methods, on two public test sets: INRIA and VOC2006\\u2019s person detection subtask [1]."'),
('"Multiple Instance Metric Learning from Automatically Labeled Bags of Faces"', '"ECCV 2010"', '["Semantic Similarity", "Multiple Instance", "Semantic Distance", "Multiple Instance Learning", "Ins', '"https://doi.org/10.1007/978-3-642-15549-9_46"', '"Metric learning aims at finding a distance that approximates a task-specific notion of semantic similarity. Typically, a Mahalanobis distance is learned from pairs of data labeled as being semantically similar or not. In this paper, we learn such metrics in a weakly supervised setting where \\u201cbags\\u201d of instances are labeled with \\u201cbags\\u201d of labels. We formulate the problem as a multiple instance learning (MIL) problem over pairs of bags. If two bags share at least one label, we label the pair positive, and negative otherwise. We propose to learn a metric using those labeled pairs of bags, leading to MildML, for multiple instance logistic discriminant metric learning. MildML iterates between updates of the metric and selection of putative positive pairs of examples from positive pairs of bags. To evaluate our approach, we introduce a large and challenging data set, Labeled Yahoo! News, which we have manually annotated and contains 31147 detected faces of 5873 different people in 20071 images. We group the faces detected in an image into a bag, and group the names detected in the caption into a corresponding set of labels. When the labels come from manual annotation, we find that MildML using the bag-level annotation performs as well as fully supervised metric learning using instance-level annotation. We also consider performance in the case of automatically extracted labels for the bags, where some of the bag labels do not correspond to any example in the bag. In this case MildML works substantially better than relying on noisy instance-level annotations derived from the bag-level annotation by resolving face-name associations in images with their captions."'),
('"Multiple Object Tracking via Prediction and Filtering with a Sobolev-Type Metric on Curves"', '"ECCV 2012"', '["Active Contour", "Shape Space", "Multiple Object Tracking", "Dimensional Riemannian Manifold", "Se', '"https://doi.org/10.1007/978-3-642-33863-2_15"', '"The problem of multi-target tracking of deforming objects in video sequences arises in many situations in image processing and computer vision. Many algorithms based on finite dimensional particle filters have been proposed. Recently, particle filters for infinite dimensional Shape Spaces have been proposed although predictions are restricted to a low dimensional subspace. We try to extend this approach using predictions in the whole shape space based on a Sobolev-type metric for curves which allows unrestricted infinite dimensional deformations. For the measurement model, we utilize contours which locally minimize a segmentation energy function and focus on the multiple contour tracking framework when there are many local minima of the segmentation energy to be detected. The method detects figures moving without the need of initialization and without the need for prior shape knowledge of the objects tracked."'),
('"Multiple Target Tracking in World Coordinate with Single, Minimally Calibrated Camera"', '"ECCV 2010"', '["Camera Motion", "Camera Parameter", "Camera Model", "Single Camera", "Structure From Motion"]', '"https://doi.org/10.1007/978-3-642-15561-1_40"', '"Tracking multiple objects is important in many application domains. We propose a novel algorithm for multi-object tracking that is capable of working under very challenging conditions such as minimal hardware equipment, uncalibrated monocular camera, occlusions and severe background clutter. To address this problem we propose a new method that jointly estimates object tracks, estimates corresponding 2D/3D temporal trajectories in the camera reference system as well as estimates the model parameters (pose, focal length, etc) within a coherent probabilistic formulation. Since our goal is to estimate stable and robust tracks that can be univocally associated to the object IDs, we propose to include in our formulation an interaction (attraction and repulsion) model that is able to model multiple 2D/3D trajectories in space-time and handle situations where objects occlude each other. We use a MCMC particle filtering algorithm for parameter inference and propose a solution that enables accurate and efficient tracking and camera model estimation. Qualitative and quantitative experimental results obtained using our own dataset and the publicly available ETH dataset shows very promising tracking and camera estimation results."'),
('"Multiple Tree Models for Occlusion and Spatial Constraints in Human Pose Estimation"', '"ECCV 2008"', '["Tree Model", "Markov Random Fields", "Conditional Random Field", "Spatial Constraint", "Weak Learn', '"https://doi.org/10.1007/978-3-540-88690-7_53"', '"Tree-structured models have been widely used for human pose estimation, in either 2D or 3D. While such models allow efficient learning and inference, they fail to capture additional dependencies between body parts, other than kinematic constraints between connected parts. In this paper, we consider the use of multiple tree models, rather than a single tree model for human pose estimation. Our model can alleviate the limitations of a single tree-structured model by combining information provided across different tree models. The parameters of each individual tree model are trained via standard learning algorithms in a single tree-structured model. Different tree models can be combined in a discriminative fashion by a boosting procedure. We present experimental results showing the improvement of our approaches on two different datasets. On the first dataset, we use our multiple tree framework for occlusion reasoning. On the second dataset, we combine multiple deformable trees for capturing spatial constraints between non-connected body parts."'),
('"Multiple View Feature Descriptors from Image Sequences via Kernel Principal Component Analysis"', '"ECCV 2004"', '["Feature Descriptor", "Image Patch", "Training Sequence", "Scale Invariant Feature Transform", "Rob', '"https://doi.org/10.1007/978-3-540-24670-1_17"', '"We present a method for learning feature descriptors using multiple images, motivated by the problems of mobile robot navigation and localization. The technique uses the relative simplicity of small baseline tracking in image sequences to develop descriptors suitable for the more challenging task of wide baseline matching across significant viewpoint changes. The variations in the appearance of each feature are learned using kernel principal component analysis (KPCA) over the course of image sequences. An approximate version of KPCA is applied to reduce the computational complexity of the algorithms and yield a compact representation. Our experiments demonstrate robustness to wide appearance variations on non-planar surfaces, including changes in illumination, viewpoint, scale, and geometry of the scene."'),
('"Multiple View Object Cosegmentation Using Appearance and Stereo Cues"', '"ECCV 2012"', '["object cosegmentation", "multiview segmentation", "multiview stereo"]', '"https://doi.org/10.1007/978-3-642-33715-4_57"', '"We present an automatic approach to segment an object in calibrated images acquired from multiple viewpoints. Our system starts with a new piecewise planar layer-based stereo algorithm that estimates a dense depth map that consists of a set of 3D planar surfaces. The algorithm is formulated using an energy minimization framework that combines stereo and appearance cues, where for each surface, an appearance model is learnt using an unsupervised approach. By treating the planar surfaces as structural elements of the scene and reasoning about their visibility in multiple views, we segment the object in each image independently. Finally, these segmentations are refined by probabilistically fusing information across multiple views. We demonstrate that our approach can segment challenging objects with complex shapes and topologies, which may have thin structures and non-Lambertian surfaces. It can also handle scenarios where the object and background color distributions overlap significantly."'),
('"Multiresolution Models for Object Detection"', '"ECCV 2010"', '["Object Detection", "Contextual Feature", "Large Instance", "Small Instance", "Pedestrian Detection', '"https://doi.org/10.1007/978-3-642-15561-1_18"', '"Most current approaches to recognition aim to be scale-invariant. However, the cues available for recognizing a 300 pixel tall object are qualitatively different from those for recognizing a 3 pixel tall object. We argue that for sensors with finite resolution, one should instead use scale-variant, or multiresolution representations that adapt in complexity to the size of a putative detection window. We describe a multiresolution model that acts as a deformable part-based model when scoring large instances and a rigid template with scoring small instances. We also examine the interplay of resolution and context, and demonstrate that context is most helpful for detecting low-resolution instances when local models are limited in discriminative power. We demonstrate impressive results on the Caltech Pedestrian benchmark, which contains object instances at a wide range of scales. Whereas recent state-of-the-art methods demonstrate missed detection rates of 86%-37% at 1 false-positive-per-image, our multiresolution model reduces the rate to 29%."'),
('"Multiscale Inverse Compositional Alignment for Subdivision Surface Maps"', '"ECCV 2004"', '["Template Match", "Subdivision Surface", "Surface Template", "Control Vertex", "Template Match Algo', '"https://doi.org/10.1007/978-3-540-24670-1_11"', '"We propose an efficient alignment method for textured Doo-Sabin subdivision surface templates. A variation of the inverse compositional image alignment is derived by introducing smooth adjustments in the parametric space of the surface and relating them to the control point increments. The convergence properties of the proposed method are improved by a coarse-to-fine multiscale matching. The method is applied to real-time tracking of specially marked surfaces from a single camera view."'),
('"Multivalued Default Logic for Identity Maintenance in Visual Surveillance"', '"ECCV 2006"', '["Activity Recognition", "Belief Revision", "Belief State", "Priority Level", "Default Rule"]', '"https://doi.org/10.1007/11744085_10"', '"Recognition of complex activities from surveillance video requires detection and temporal ordering of its constituent \\u201catomic\\u201d events. It also requires the capacity to robustly track individuals and maintain their identities across single as well as multiple camera views. Identity maintenance is a primary source of uncertainty for activity recognition and has been traditionally addressed via different appearance matching approaches. However these approaches, by themselves, are inadequate. In this paper, we propose a prioritized, multivalued, default logic based framework that allows reasoning about the identities of individuals. This is achieved by augmenting traditional appearance matching with contextual information about the environment and self identifying traits of certain actions. This framework also encodes qualitative confidence measures for the identity decisions it takes and finally, uses this information to reason about the occurrence of certain predefined activities in video."'),
('"Multivariate Relevance Vector Machines for Tracking"', '"ECCV 2006"', '["Mapping Function", "Relevance Vector Machine", "Shape Template", "Human Body Motion", "Multivariat', '"https://doi.org/10.1007/11744078_10"', '"This paper presents a learning based approach to tracking articulated human body motion from a single camera. In order to address the problem of pose ambiguity, a one-to-many mapping from image features to state space is learned using a set of relevance vector machines, extended to handle multivariate outputs. The image features are Hausdorff matching scores obtained by matching different shape templates to the image, where the multivariate relevance vector machines (MVRVM) select a sparse set of these templates. We demonstrate that these Hausdorff features reduce the estimation error in clutter compared to shape-context histograms. The method is applied to the pose estimation problem from a single input frame, and is embedded within a probabilistic tracking framework to include temporal information. We apply the algorithm to 3D hand tracking and full human body tracking."'),
('"Multivariate Saddle Point Detection for Statistical Clustering"', '"ECCV 2002"', '["grouping and segmentation", "image features", "nonparametric clustering", "cluster significance"]', '"https://doi.org/10.1007/3-540-47977-5_37"', '"Decomposition methods based on nonparametric density estimation define a cluster as the basin of attraction of a local maximum (mode) of the density function, with the cluster borders being represented by valleys surrounding the mode. To measure the significance of each delineated cluster we propose a test statistics that compares the estimated density of the mode with the estimated maximum density on the cluster boundary. While for a given kernel bandwidth the modes can be safely obtained by using the mean shift procedure, the detection of maximum density points on the cluster boundary (i.e., the saddle points) is not straightforward for multivariate data. We therefore develop a gradient-based iterative algorithm for saddle point detection and show its effectiveness in various data decomposition tasks. After finding the largest density saddle point associated with each cluster, we compute significance measures that allow formal hypothesis testing of cluster existence. The new statistical framework is extended and tested for the task of image segmentation."'),
('"Multiview Registration of 3D Scenes by Minimizing Error between Coordinate Frames"', '"ECCV 2002"', '["Mobile Robot", "Span Tree", "Range Image", "Basis Cycle", "Partial Graph"]', '"https://doi.org/10.1007/3-540-47967-8_39"', '"This paper addresses the problem of large scale multiview registration of range images captured from unknown viewing directions. To reduce the computational burden, we decouple the local problem of pairwise registration on neighboring views from the global problem of distribution of accumulated errors. We define the global problem over the graph of neighboring views, and we show that this graph can be decomposed into a set of cycles such that the optimal transformation parameters for each cycle can be solved in closed form. We then describe an iterative procedure that can be used to integrate the solutions for the set of cycles across the graph. This method for error distribution does not require point correspondences between views, and therefore can be used together with robot odometry or any method of pairwise registration. Experimental results demonstrate the effectiveness of this technique on range images of an indoor facility."'),
('"N-tuple Color Segmentation for Multi-view Silhouette Extraction"', '"ECCV 2012"', '["Color Model", "Foreground Object", "Color Distribution", "Background Pixel", "Foreground Pixel"]', '"https://doi.org/10.1007/978-3-642-33715-4_59"', '"We present a new method to extract multiple segmentations of an object viewed by multiple cameras, given only the camera calibration. We introduce the n-tuple color model to express inter-view consistency when inferring in each view the foreground and background color models permitting the final segmentation. A color n-tuple is a set of pixel colors associated to the n projections of a 3D point. The first goal is set as finding the MAP estimate of background/foreground color models based on an arbitrary sample set of such n-tuples, such that samples are consistently classified, in a soft way, as \\u201dempty\\u201d if they project in the background of at least one view, or \\u201doccupied\\u201d if they project to foreground pixels in all views. An Expectation Maximization framework is then used to alternate between color models and soft classifications. In a final step, all views are segmented based on their attached color models. The approach is significantly simpler and faster than previous multi-view segmentation methods, while providing results of equivalent or better quality."'),
('"Natural Action Recognition Using Invariant 3D Motion Encoding"', '"ECCV 2014"', '["Action recognition", "in the wild", "3D motion", "scene flow", "invariant encoding", "stereo seque', '"https://doi.org/10.1007/978-3-319-10605-2_49"', '"We investigate the recognition of actions \\u201cin the wild\\u201d using 3D motion information. The lack of control over (and knowledge of) the camera configuration, exacerbates this already challenging task, by introducing systematic projective inconsistencies between 3D motion fields, hugely increasing intra-class variance. By introducing a robust, sequence based, stereo calibration technique, we reduce these inconsistencies from fully projective to a simple similarity transform. We then introduce motion encoding techniques which provide the necessary scale invariance, along with additional invariances to changes in camera viewpoint."'),
('"Nature Conservation Drones for Automatic Localization and Counting of Animals"', '"ECCV 2014"', '["Nature conservation", "Micro UAVs", "Object detection"]', '"https://doi.org/10.1007/978-3-319-16178-5_17"', '"This paper is concerned with nature conservation by automatically monitoring animal distribution and animal abundance. Typically, such conservation tasks are performed manually on foot or after an aerial recording from a manned aircraft. Such manual approaches are expensive, slow and labor intensive. In this paper, we investigate the combination of small unmanned aerial vehicles (UAVs or \\u201cdrones\\u201d) with automatic object recognition techniques as a viable solution to manual animal surveying. Since no controlled data is available, we record our own animal conservation dataset with a quadcopter drone. We evaluate two nature conservation tasks: (i) animal detection (ii) animal counting using three state-of-the-art generic object recognition methods that are particularly well-suited for on-board detection. Results show that object detection techniques for human-scale photographs do not directly translate to a drone perspective, but that light-weight automatic object detection techniques are promising for nature conservation tasks."'),
('"Nautical Scene Segmentation Using Variable Size Image Windows and Feature Space Reclustering"', '"ECCV 2000"', '["Feature Vector", "Feature Space", "Mahalanobis Distance", "Main Cluster", "Rigid Object"]', '"https://doi.org/10.1007/3-540-45053-X_21"', '"This paper describes the development of a system for the segmentation of small vessels and objects present in a maritime environment. The system assumes no a priori knowledge of the sea, but uses statistical analysis within variable size image windows to determine a characteristic vector that represents the current sea state. A space of characteristic vectors is searched and a main group of characteristic vectors and its centroid found automatically by using a new method of iterative reclustering. This method is an extension and improvement of the work described in [9]. A Mahalanobis distance measure from the centroid is calculated for each characteristic vector and is used to determine inhomogenities in the sea caused by the presence of a rigid object. The system has been tested using several input image sequences of static small objects such as buoys and small and large maritime vessels moving into and out of a harbour scene and the system successfully segmented these objects."'),
('"Negative Evidences and Co-occurences in Image Retrieval: The Benefit of PCA and Whitening"', '"ECCV 2012"', '["Dimensionality Reduction", "Image Retrieval", "Visual Word", "Query Image", "Local Descriptor"]', '"https://doi.org/10.1007/978-3-642-33709-3_55"', '"The paper addresses large scale image retrieval with short vector representations. We study dimensionality reduction by Principal Component Analysis (PCA) and propose improvements to its different phases. We show and explicitly exploit relations between i) mean subtraction and the negative evidence, i.e., a visual word that is mutually missing in two descriptions being compared, and ii) the axis de-correlation and the co-occurrences phenomenon. Finally, we propose an effective way to alleviate the quantization artifacts through a joint dimensionality reduction of multiple vocabularies. The proposed techniques are simple, yet significantly and consistently improve over the state of the art on compact image representations. Complementary experiments in image classification show that the methods are generally applicable."'),
('"Nested Pictorial Structures"', '"ECCV 2012"', '["Nest Pattern", "Pictorial Structure", "Maximal Part", "Nest Relation", "Nest Feature"]', '"https://doi.org/10.1007/978-3-642-33709-3_58"', '"We propose a theoretical construct coined nested pictorial structure to represent an object by parts that are recursively nested. Three innovative ideas are proposed: First, the nested pictorial structure finds a part configuration that is allowed to be deformed in geometric arrangement, while being confined to be topologically nested. Second, we define nested features which lend themselves to better, more detailed accounting of pixel data cost and describe occlusion in a principled way. Third, we develop the concept of constrained distance transform, a variation of the generalized distance transform, to guarantee the topological nesting relations and to further enforce that parts have no overlap with each other. We show that matching an optimal nested pictorial structure of K parts on an image of N pixels takes O(NK) time using dynamic programming and constrained distance transform. In our MATLAB/C++ implementation, it takes less than 0.1 seconds to do the global optimal matching when K\\u2009=\\u200910 and N\\u2009=\\u2009400 \\u00d7400. We demonstrate the usefulness of nested pictorial structures in the matching of objects of nested patterns, objects in occlusion, and objects that live in a context."'),
('"Nested Sparse Quantization for Efficient Feature Coding"', '"ECCV 2012"', '["Quantization Error", "Sparse Code", "Feature Code", "Codebook Size", "Feature Encode"]', '"https://doi.org/10.1007/978-3-642-33709-3_53"', '"Many state-of-the-art methods in object recognition extract features from an image and encode them, followed by a pooling step and classification. Within this processing pipeline, often the encoding step is the bottleneck, for both computational efficiency and performance. We present a novel assignment-based encoding formulation. It allows for the fusion of assignment-based encoding and sparse coding into one formulation. We also use this to design a new, very efficient, encoding. At the heart of our formulation lies a quantization into a set of k-sparse vectors, which we denote as sparse quantization. We design the new encoding as two nested, sparse quantizations. Its efficiency stems from leveraging bit-wise representations. In a series of experiments on standard recognition benchmarks, namely Caltech 101, PASCAL VOC 07 and ImageNet, we demonstrate that our method achieves results that are competitive with the state-of-the-art, and requires orders of magnitude less time and memory. Our method is able to encode one million images using 4 CPUs in a single day, while maintaining a good performance."'),
('"Neural Codes for Image Retrieval"', '"ECCV 2014"', '["image retrieval", "same-object image search", "deep learning", "convolutional neural networks", "f', '"https://doi.org/10.1007/978-3-319-10590-1_38"', '"It has been shown that the activations invoked by an image within the top layers of a large convolutional neural network provide a high-level descriptor of the visual content of the image. In this paper, we investigate the use of such descriptors (neural codes) within the image retrieval application. In the experiments with several standard retrieval benchmarks, we establish that neural codes perform competitively even when the convolutional neural network has been trained for an unrelated classification task (e.g. Image-Net). We also evaluate the improvement in the retrieval performance of neural codes, when the network is retrained on a dataset of images that are similar to images encountered at test time."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Neural Fields Models of Visual Areas: Principles, Successes, and Caveats"', '"ECCV 2012"', '["Visual Area", "Synaptic Weight", "Tuning Curve", "Binocular Rivalry", "Visual Orientation"]', '"https://doi.org/10.1007/978-3-642-33863-2_48"', '"I discuss how the notion of neural fields, a phenomenological averaged description of spatially distributed populations of neurons, can be used to build models of how visual information is represented and processed in the visual areas of primates. I describe one of the basic principles of operation of these neural fields equations which is closely connected to the idea of a bifurcation of their solutions. I then apply this concept to several visual features, edges, textures and motion and show that it can account very simply for a number of experimental facts as well as suggest new experiments."'),
('"Neural Mechanisms for Form and Motion Detection and Integration: Biology Meets Machine Vision"', '"ECCV 2012"', '["Motion Detection", "Motion Perception", "Automatic Gain Control", "Feed Forward Signal", "Feed For', '"https://doi.org/10.1007/978-3-642-33863-2_47"', '"General-purpose vision systems, either biological or technical, rely on the robust processing of visual data from the sensor array. Such systems need to adapt their processing capabilities to varying conditions, have to deal with noise, and also need to learn task-relevant representations. Here, we describe models of early and mid-level vision. These models are motivated by the layered and hierarchical processing of form and motion information in primate cortex. Core cortical processing principles are: (i) bottom-up processing to build representations of increasing feature specificity and spatial scale, (ii) selective amplification of bottom-up signals by feedback that utilizes spatial, temporal, or task-related context information, and (iii) automatic gain control via center-surround competitive interaction and activity normalization. We use these principles as a framework to design and develop bio-inspired models for form and motion processing. Our models replicate experimental findings and, furthermore, provide a functional explanation for psychophysical and physiological data. In addition, our models successfully process natural images or videos. We show mechanism that group items into boundary representations or estimate visual motions from opaque or transparent surfaces. Our framework suggests a basis for designing bio-inspired models that solve typical computer vision problems and enable the development of neural technology for vision."'),
('"Neural Network Fusion of Color, Depth and Location for Object Instance Recognition on a Mobile Robo', '"ECCV 2014"', '["Semantic mapping", "Indoor scene understanding", "Instance recognition", "Mobile robotics", "RGB-D', '"https://doi.org/10.1007/978-3-319-16199-0_55"', '"The development of mobile robots for domestic assistance requires solving problems integrating ideas from different fields of research like computer vision, robotic manipulation, localization and mapping. Semantic mapping, that is, the enrichment a map with high-level information like room and object identities, is an example of such a complex robotic task. Solving this task requires taking into account hard software and hardware constraints brought by the context of autonomous mobile robots, where short processing times and low energy consumption are mandatory. We present a light-weight scene segmentation and object instance recognition algorithm using an RGB-D camera and demonstrate it in a semantic mapping experiment. Our method uses a feed-forward neural network to fuse texture, color and depth information. Running at 3 Hz on a single laptop computer, our algorithm achieves a recognition rate of 97 % in a controlled environment, and 87 % in the adversarial conditions of a real robotic task. Our results demonstrate that state of the art recognition rates on a database does not guarantee performance in a real world experiment. We also show the benefit in these conditions of fusing several recognition decisions and data from different sources. The database we compiled for the purpose of this study is publicly available."'),
('"Neuro-Fuzzy Shadow Filter"', '"ECCV 2002"', '["Grouping and segmentation", "neuro-fuzzy classifier", "shadow removal"]', '"https://doi.org/10.1007/3-540-47977-5_25"', '"In video sequence processing, shadow remains a major source of error for object segmentation. Traditional methods of shadow removal are mainly based on colour difference thresholding between the background and current images. The application of colour filters on MPEG or MJPEG images, however, is often erroneous as the chrominance information is significantly reduced due to compression. In addition, as the colour attributes of shadows and objects arc often very similar, discrete thresholding cannot always provide reliable results. This paper presents a novel approach for adaptive shadow removal by incorporating four different filters in a neuro-fuzzy framework. The neuro-fuzzy classifier has the ability of real-time self-adaptation and training, and its performance has been quantitatively assessed with both indoor and outdoor video sequences."'),
('"New Algorithms for Controlling Active Contours Shape and Topology"', '"ECCV 2000"', '["Internal Force", "Active Contour", "Hash Table", "Active Contour Model", "Topology Constraint"]', '"https://doi.org/10.1007/3-540-45053-X_25"', '"In recent years, the field of active-contour based image segmentation have seen the emergence of two competing approaches. The first and oldest approach represents active contours in an explicit (or parametric) manner corresponding to the Lagrangian formulation. The second approach represent active contours in an implicit manner corresponding to the Eulerian framework. After comparing these two approaches, we describe several new topological and physical constraints applied on parametric active contours in order to combine the advantages of these two contour representations. We introduce three key algorithms for independently controlling active contour parameterization, shape and topology. We compare our result to the level-set method and show similar results with a significant speed-up."'),
('"New Techniques for Automated Architectural Reconstruction from Photographs"', '"ECCV 2002"', '["Ground Plane", "Principal Direction", "Wall Plane", "Coarse Model", "Polyhedral Model"]', '"https://doi.org/10.1007/3-540-47967-8_36"', '"We investigate a strategy for reconstructing of buildings from multiple (uncalibrated) images. In a similar manner to the Facade approach we first generate a coarse piecewise planar model of the principal scene planes and their delineations, and then use these facets to guide the search for indentations and protrusions such as windows and doors. However, unlike the Facade approach which involves manual selection and alignment of the geometric primitives, the strategy here is fully automatic."'),
('"New View Generation with a Bi-centric Camera"', '"ECCV 2002"', '["View Generation", "Input Sequence", "Projection Center", "Camera Model", "Panoramic Image"]', '"https://doi.org/10.1007/3-540-47969-4_41"', '"We propose a novel method for new view generation from a rectified sequence of images. Our new images correspond to a new camera model, which we call a bi-centric camera; in this model the centers of horizontal and vertical projections lie in different locations on the camera\\u2019s optical axis. This model reduces to the regular pinhole camera when the two projection centers coincide, and the pushbroom camera when one projection center lies at infinity. We first analyze the properties of this camera model. We then show how to generate new bi-centric views from vertical cuts in the epipolar volume of a rectified sequence. Every vertical cut generates a new bi-centric view, where the specific parameters of the cut determine the location of the projection centers. We discuss and demonstrate applications, including the generation of images where the virtual camera lies behind occluding surfaces (e.g., behind the back wall of a room), and in unreachable positions (e.g., in front of a glass window). Our final application is the generation of movies taken by a simulated forward moving camera, using as input a movie taken by a sideways moving camera."'),
('"NF-Features \\u2013 No-Feature-Features for Representing Non-textured Regions"', '"ECCV 2010"', '["Image Noise", "Interest Point", "Regular Feature", "Interest Point Detector", "Maximally Stable Ex', '"https://doi.org/10.1007/978-3-642-15552-9_10"', '"In order to achieve a complete image description, we introduce no-feature-features (NF-features) representing object regions where regular interest point detectors do not detect features. As these regions are usually non-textured, stable re-localization in different images with conventional methods is not possible. Therefore, a technique is presented which re-localizes once-detected NF-features using correspondences of regular features. Furthermore, a distinctive NF descriptor for non-textured regions is derived which has invariance towards affine transformations and changes in illumination. For the matching of NF descriptors, an approach is introduced that is based on local image statistics."'),
('"No Bias Left behind: Covariate Shift Adaptation for Discriminative 3D Pose Estimation"', '"ECCV 2012"', '["Importance Weight", "Kernel Regression", "Gaussian Process Regression", "Covariate Shift", "Relati', '"https://doi.org/10.1007/978-3-642-33765-9_48"', '"Discriminative, or (structured) prediction, methods have proved effective for variety of problems in computer vision; a notable example is 3D monocular pose estimation. All methods to date, however, relied on an assumption that training (source) and test (target) data come from the same underlying joint distribution. In many real cases, including standard datasets, this assumption is flawed. In presence of training set bias, the learning results in a biased model whose performance degrades on the (target) test set. Under the assumption of covariate shift we propose an unsupervised domain adaptation approach to address this problem. The approach takes the form of training instance re-weighting, where the weights are assigned based on the ratio of training and test marginals evaluated at the samples. Learning with the resulting weighted training samples, alleviates the bias in the learned models. We show the efficacy of our approach by proposing weighted variants of Kernel Regression (KR) and Twin Gaussian Processes (TGP). We show that our weighted variants outperform their un-weighted counterparts and improve on the state-of-the-art performance in the public (HumanEva) dataset."'),
('"Noise Modelling and Uncertainty Propagation for TOF Sensors"', '"ECCV 2012"', '["Noise Modelling", "Depth Image", "Depth Measurement", "Uncertainty Propagation", "Sensor Noise"]', '"https://doi.org/10.1007/978-3-642-33885-4_48"', '"Time-of-Flight (TOF) cameras are active real time depth sensors. One issue of TOF sensors is measurement noise. In this paper, we present a method for providing the uncertainty associated to 3D TOF measurements based on noise modelling. Measurement uncertainty is the combination of pixel detection error and sensor noise. First, a detailed noise characterization is presented. Then, a continuous model which gives the noise\\u2019s standard deviation for each depth-pixel is proposed. Finally, a closed-form approximation of 3D uncertainty from 2D pixel detection error is presented. An applicative example is provided that shows the use of our 3D uncertainty modelling on real data."'),
('"Noise-Resistant Affine Skeletons of Planar Curves"', '"ECCV 2000"', '["Real Image", "Medial Axis", "Planar Curf", "Simple Closed Curve", "Euclidean Case"]', '"https://doi.org/10.1007/3-540-45054-8_48"', '"A new definition of affine invariant skeletons for shape representation is introduced. A point belongs to the affine skeleton if and only if it is equidistant from at least two points of the curve, with the distance being a minima and given by the areas between the curve and its corresponding chords. The skeleton is robust, eliminating the need for curve denoising. Previous approaches have used either the Euclidean or affine distances, thereby resulting in a much less robust computation. We propose a simple method to compute the skeleton and give examples with real images, and show that the proposed definition works also for noisy data. We also demonstrate how to use this method to detect affine skew symmetry."'),
('"Non Linear Temporal Textures Synthesis: A Monte Carlo Approach"', '"ECCV 2006"', '["Nonnegative Matrix Factorization", "Gaussian Density", "Transition Kernel", "Texture Synthesis", "', '"https://doi.org/10.1007/11744047_22"', '"In this paper we consider the problem of temporal texture modeling and synthesis. A temporal texture (or dynamic texture) is seen as the output of a dynamical system driven by white noise. Experimental evidence shows that linear models such as those introduced in earlier work are sometimes inadequate to fully describe the time evolution of the dynamic scene. Extending upon recent work which is available in the literature, we tackle the synthesis using non-linear dynamical models. The non-linear model is never given explicitly but rather we describe a methodology to generate samples from the model. The method requires estimating the \\u201cstate\\u201d distribution and a linear dynamical model from the original clip which are then used respectively as target distribution and proposal mechanism in a rejection sampling step. We also report extensive experimental results comparing the proposed approach with the results obtained using linear models (Doretto et al.) and the \\u201cclosed-loop\\u201d approach presented at ECCV 2004 by Yuan et al."'),
('"Non-associative Higher-Order Markov Networks for Point Cloud Classification"', '"ECCV 2014"', '["Non-associative Markov networks", "Higher-order graphical models", "3D point clouds", "Semantic la', '"https://doi.org/10.1007/978-3-319-10602-1_33"', '"In this paper, we introduce a non-associative higher-order graphical model to tackle the problem of semantic labeling of 3D point clouds. For this task, existing higher-order models overlook the relationships between the different classes and simply encourage the nodes in the cliques to have consistent labelings. We address this issue by devising a set of non-associative context patterns that describe higher-order geometric relationships between different class labels within the cliques. To this end, we propose a method to extract informative cliques in 3D point clouds that provide more knowledge about the context of the scene. We evaluate our approach on three challenging outdoor point cloud datasets. Our experiments evidence the benefits of our non-associative higher-order Markov networks over state-of-the-art point cloud labeling techniques."'),
('"Non-causal Temporal Prior for Video Deblocking"', '"ECCV 2012"', '["Motion Estimation", "High Frequency Component", "Image Detail", "Block Match", "Block Boundary"]', '"https://doi.org/10.1007/978-3-642-33715-4_37"', '"Real-world video sequences coded at low bit rates suffer from compression artifacts, which are visually disruptive and can cause problems to computer vision algorithms. Unlike the denoising problem where the high frequency components of the signal are present in the noisy observation, most high frequency details are lost during compression and artificial discontinuities arise across the coding block boundaries. In addition to sparse spatial priors that can reduce the blocking artifacts for a single frame, temporal information is needed to recover the lost spatial details. However, establishing accurate temporal correspondences from the compressed videos is challenging because of the loss of high frequency details and the increase of false blocking artifacts. In this paper, we propose a non-causal temporal prior model to reduce video compression artifacts by propagating information from adjacent frames and iterating between image reconstruction and motion estimation. Experimental results on real-world sequences demonstrate that the deblocked videos by the proposed system have marginal statistics of high frequency components closer to those of the original ones, and are better input for standard edge and corner detectors than the coded ones."'),
('"Non-linear Bayesian Image Modelling"', '"ECCV 2000"', '["Face Image", "Independent Component Analysis", "Latent Variable Model", "Principal Component Analy', '"https://doi.org/10.1007/3-540-45054-8_1"', '"In recent years several techniques have been proposed for modelling the low-dimensional manifolds, or \\u2018subspaces\\u2019, of natural images. Examples include principal component analysis (as used for instance in \\u2018eigen-faces\\u2019), independent component analysis, and auto-encoder neural networks. Such methods suffer from a number of restrictions such as the limitation to linear manifolds or the absence of a probablistic representation. In this paper we exploit recent developments in the fields of variational inference and latent variable models to develop a novel and tractable probabilistic approach to modelling manifolds which can handle complex non-linearities. Our framework comprises a mixture of sub-space components in which both the number of components and the effective dimensionality of the subspaces are determined automatically as part of the Bayesian inference procedure. We illustrate our approach using two classical problems: modelling the manifold of face images and modelling the manifolds of hand-written digits."'),
('"Non-local Characterization of Scenery Images: Statistics, 3D Reasoning, and a Generative Model"', '"ECCV 2010"', '["Natural Image", "Background Region", "Land Region", "Foreground Object", "Aerial Image"]', '"https://doi.org/10.1007/978-3-642-15555-0_8"', '"This work focuses on characterizing scenery images. We semantically divide the objects in natural landscape scenes into background and foreground and show that the shapes of the regions associated with these two types are statistically different. We then focus on the background regions. We study statistical properties such as size and shape, location and relative location, the characteristics of the boundary curves and the correlation of the properties to the region\\u2019s semantic identity. Then we discuss the imaging process of a simplified 3D scene model and show how it explains the empirical observations. We further show that the observed properties suffice to characterize the gist of scenery images, propose a generative parametric graphical model, and use it to learn and generate semantic sketches of new images, which indeed look like those associated with natural scenery."'),
('"Non-Local Kernel Regression for Image and Video Restoration"', '"ECCV 2010"', '["Near Neighbor", "Image Patch", "Kernel Regression", "Bicubic Interpolation", "Similar Patch"]', '"https://doi.org/10.1007/978-3-642-15558-1_41"', '"This paper presents a non-local kernel regression (NL-KR) method for image and video restoration tasks, which exploits both the non-local self-similarity and local structural regularity in natural images. The non-local self-similarity is based on the observation that image patches tend to repeat themselves in natural images and videos; and the local structural regularity reveals that image patches have regular structures where accurate estimation of pixel values via regression is possible. Explicitly unifying both properties, the proposed non-local kernel regression framework is robust and applicable to various image and video restoration tasks. In this work, we are specifically interested in applying the NL-KR model to image and video super-resolution (SR) reconstruction. Extensive experimental results on both single images and realistic video sequences demonstrate the superiority of the proposed framework for SR tasks over previous works both qualitatively and quantitatively."'),
('"Non-local Regularization of Inverse Problems"', '"ECCV 2008"', '["Inverse Problem", "Compressive Sampling", "Graph Regularization", "Image Inpainting", "Proximity O', '"https://doi.org/10.1007/978-3-540-88690-7_5"', '"This article proposes a new framework to regularize linear inverse problems using the total variation on non-local graphs. This non-local graph allows to adapt the penalization to the geometry of the underlying function to recover. A fast algorithm computes iteratively both the solution of the regularization process and the non-local graph adapted to this solution. We show numerical applications of this method to the resolution of image processing inverse problems such as inpainting, super-resolution and compressive sampling."'),
('"Non-local Total Generalized Variation for Optical Flow Estimation"', '"ECCV 2014"', '["Regularization Term", "Scale Change", "Data Term", "Support Weight", "Total Generalize Variation"]', '"https://doi.org/10.1007/978-3-319-10590-1_29"', '"In this paper we introduce a novel higher-order regularization term. The proposed regularizer is a non-local extension of the popular second-order Total Generalized variation, which favors piecewise affine solutions and allows to incorporate soft-segmentation cues into the regularization term. These properties make this regularizer especially appealing for optical flow estimation, where it offers accurately localized motion boundaries and allows to resolve ambiguities in the matching term. We additionally propose a novel matching term which is robust to illumination and scale changes, two major sources of errors in optical flow estimation algorithms. We extensively evaluate the proposed regularizer and data term on two challenging benchmarks, where we are able to obtain state of the art results. Our method is currently ranked first among classical two-frame optical flow methods on the KITTI optical flow benchmark."'),
('"Non-parametric Higher-Order Random Fields for Image Segmentation"', '"ECCV 2014"', '["random fields", "biomedical image analysis", "higher-order models", "image denoising", "image segm', '"https://doi.org/10.1007/978-3-319-10599-4_18"', '"Models defined using higher-order potentials are becoming increasingly popular in computer vision. However, the exact representation of a general higher-order potential defined over many variables is computationally unfeasible. This has led prior works to adopt parametric potentials that can be compactly represented. This paper proposes a non-parametric higher-order model for image labeling problems that uses a patch-based representation of its potentials. We use the transformation scheme of [11, 25] to convert the higher-order potentials to a pair-wise form that can be handled using traditional inference algorithms. This representation is able to capture structure, geometrical and topological information of labels from training data and to provide more precise segmentations. Other tasks such as image denoising and reconstruction are also possible. We evaluate our method on denoising and segmentation problems with synthetic and real images."'),
('"Non-parametric Model for Background Subtraction"', '"ECCV 2000"', '["visual motion", "active and real time vision", "motion detection", "non-parametric estimation", "v', '"https://doi.org/10.1007/3-540-45053-X_48"', '"Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates."'),
('"Non-rigid Shape Registration: A Single Linear Least Squares Framework"', '"ECCV 2012"', '["Regularization Term", "Iterative Close Point", "Registration Error", "Thin Plate Spline", "Control', '"https://doi.org/10.1007/978-3-642-33786-4_20"', '"This paper proposes a non-rigid registration formulation capturing both global and local deformations in a single framework. This formulation is based on a quadratic estimation of the registration distance together with a quadratic regularization term. Hence, the optimal transformation parameters are easily obtained by solving a liner system of equations, which guarantee a fast convergence. Experimental results with challenging 2D and 3D shapes are presented to show the validity of the proposed framework. Furthermore, comparisons with the most relevant approaches are provided."'),
('"Nonlinear Cross-View Sample Enrichment for Action Recognition"', '"ECCV 2014"', '["Action recognition", "Kernel methods", "Canonical correlation analysis", "Viewpoint knowledge tran', '"https://doi.org/10.1007/978-3-319-16199-0_4"', '"Advanced action recognition methods are prone to limited generalization performances when trained on insufficient amount of data. This limitation results from the high expense to label training samples and their insufficiency to capture enough variability due to viewpoint changes."'),
('"Nonlinear Shape Statistics in Mumford\\u2014Shah Based Segmentation"', '"ECCV 2002"', '["Segmentation", "shape learning", "nonlinear statistics", "density estimation", "Mercer kernels", "', '"https://doi.org/10.1007/3-540-47967-8_7"', '"We present a variational integration of nonlinear shape statistics into a Mumford\\u2014Shah based segmentation process. The nonlinear statistics are derived from a set of training silhouettes by a novel method of density estimation which can be considered as an extension of kernel PCA to a stochastic framework."'),
('"Nonlocal Multiscale Hierarchical Decomposition on Graphs"', '"ECCV 2010"', '["Point Cloud", "Graph Structure", "Weighted Graph", "Image Denoising", "Exponential Weight"]', '"https://doi.org/10.1007/978-3-642-15561-1_46"', '"The decomposition of images into their meaningful components is one of the major tasks in computer vision. Tadmor, Nezzar and Vese [1] have proposed a general approach for multiscale hierarchical decomposition of images. On the basis of this work, we propose a multiscale hierarchical decomposition of functions on graphs. The decomposition is based on a discrete variational framework that makes it possible to process arbitrary discrete data sets with the natural introduction of nonlocal interactions. This leads to an approach that can be used for the decomposition of images, meshes, or arbitrary data sets by taking advantage of the graph structure. To have a fully automatic decomposition, the issue of parameter selection is fully addressed. We illustrate our approach with numerous decomposition results on images, meshes, and point clouds and show the benefits."'),
('"Nonmetric Priors for Continuous Multilabel Optimization"', '"ECCV 2012"', '["Distance Function", "Continuous Setting", "Convex Relaxation", "Block Artifact", "Label Distance"]', '"https://doi.org/10.1007/978-3-642-33786-4_16"', '"We propose a novel convex prior for multilabel optimization which allows to impose arbitrary distances between labels. Only symmetry, d(i,j)\\u2009\\u2265\\u20090 and d(i,i)\\u2009=\\u20090 are required. In contrast to previous grid based approaches for the nonmetric case, the proposed prior is formulated in the continuous setting avoiding grid artifacts. In particular, the model is easy to implement, provides a convex relaxation for the Mumford-Shah functional and yields comparable or superior results on the MSRC segmentation database comparing to metric or grid based approaches."'),
('"Nonparametric Estimation of Multiple Structures with Outliers"', '"WDV 2006"', '["Multiple Model", "Nonparametric Estimation", "Hough Transform", "Residual Distribution", "Multiple', '"https://doi.org/10.1007/978-3-540-70932-9_5"', '"Common problem encountered in the analysis of dynamic scene is the problem of simultaneous estimation of the number of models and their parameters. This problem becomes difficult as the measurement noise in the data increases and the data are further corrupted by outliers. This is especially the case in a variety of motion estimation problems, where the displacement between the views is large and the process of establishing correspondences is difficult. In this paper we propose a novel nonparametric sampling based method for estimating the number of models and their parameters. The main novelty of the proposed method lies in the analysis of the distribution of residuals of individual data points with respect to the set of hypotheses, generated by a RANSAC-like sampling process. We will show that the modes of the residual distributions directly reveal the presence of multiple models and facilitate the recovery of the individual models, without making any assumptions about the distribution of the outliers or the noise process. The proposed approach is capable of handling data with a large fraction of outliers. Experiments with both synthetic data and image pairs related by different motion models are presented to demonstrate the effectiveness of the proposed approach."'),
('"Nonparametric Gesture Labeling from Multi-modal Data"', '"ECCV 2014"', '["Gesture recognition", "Nonparametric estimation", "Multi-modal data"]', '"https://doi.org/10.1007/978-3-319-16178-5_35"', '"We present a new gesture recognition method using multi-modal data. Our approach solves a labeling problem, which means that gesture categories and their temporal ranges are determined at the same time. For that purpose, a generative probabilistic model is formalized and it is constructed by nonparametrically estimating multi-modal densities from a training dataset. In addition to the conventional skeletal joint based features, appearance information near the active hand in the RGB image is exploited to capture the detailed motion of fingers. The estimated log-likelihood function is used as the unary term for our Markov random field (MRF) model. The smoothness term is also incorporated to enforce temporal coherence of our model. The labeling results can then be obtained by the efficient dynamic programming technique. Experimental results demonstrate that our method provides effective gesture labeling results for the large-scale gesture dataset. Our method scores \\\\(0.8268\\\\) in the mean Jaccard index and is ranked 3rd in the gesture recognition track of the ChaLearn Looking at People (LAP) Challenge in 2014."'),
('"Nonrigid Image Registration Using Dynamic Higher-Order MRF Model"', '"ECCV 2008"', '["Energy Model", "Markov Random Field", "Factor Graph", "Normalize Cross Correlation", "Ordinary Nod', '"https://doi.org/10.1007/978-3-540-88682-2_29"', '"In this paper, we propose a nonrigid registration method using the Markov Random Field (MRF) model with a higher-order spatial prior. The registration is designed as finding a set of discrete displacement vectors on a deformable mesh, using the energy model defined by label sets relating to these vectors. This work provides two main ideas to improve the reliability and accuracy of the registration. First, we propose a new energy model which adopts a higher-order spatial prior for the smoothness cost. This model improves limitations of pairwise spatial priors which cannot fully incorporate the natural smoothness of deformations. Next we introduce a dynamic energy model to generate optimal displacements. This model works iteratively with optimal data cost while the spatial prior preserve the smoothness cost of previous iteration. For optimization, we convert the proposed model to pairwise MRF model to apply the tree-reweighted message passing (TRW). Concerning the complexity, we apply the decomposed scheme to reduce the label dimension of the proposed model and incorporate the linear constrained node (LCN) technique for efficient message passings. In experiments, we demonstrate the competitive performance of the proposed model compared with previous models, presenting both quantitative and qualitative analysis."'),
('"Nonrigid Shape and Motion from Multiple Perspective Views"', '"ECCV 2006"', '["Null Space", "Camera Motion", "Multiple View", "Shape Base", "Point Correspondence"]', '"https://doi.org/10.1007/11744047_16"', '"We consider the problem of nonrigid shape and motion recovery from point correspondences in multiple perspective views. It is well known that the constraints among multiple views of a rigid shape are multilinear on the image points and can be reduced to bilinear (epipolar) and trilinear constraints among two and three views, respectively. In this paper, we generalize this classic result by showing that the constraints among multiple views of a nonrigid shape consisting of K shape bases can be reduced to multilinear constraints among K + \\u2308 (K + 1)/2\\u2309, \\u22ef, 2K + 1 views. We then present a closed form solution to the reconstruction of a nonrigid shape consisting of two shape bases. We show that point correspondences in five views are related by a nonrigid quintifocal tensor, from which one can linearly compute nonrigid shape and motion. We also demonstrate the existence of intrinsic ambiguities in the reconstruction of camera translation, shape coefficients and shape bases. Examples show the effectiveness of our method on nonrigid scenes with significant perspective effects."'),
('"Nonrigid Surface Registration and Completion from RGBD Images"', '"ECCV 2014"', '["Nonrigid registration", "surface completion", "RGBD images"]', '"https://doi.org/10.1007/978-3-319-10605-2_5"', '"Nonrigid surface registration is a challenging problem that suffers from many ambiguities. Existing methods typically assume the availability of full volumetric data, or require a global model of the surface of interest. In this paper, we introduce an approach to nonrigid registration that performs on relatively low-quality RGBD images and does not assume prior knowledge of the global surface shape. To this end, we model the surface as a collection of patches, and infer the patch deformations by performing inference in a graphical model. Our representation lets us fill in the holes in the input depth maps, thus essentially achieving surface completion. Our experimental evaluation demonstrates the effectiveness of our approach on several sequences, as well as its robustness to missing data and occlusions."'),
('"Nonuniform Lattice Regression for Modeling the Camera Imaging Pipeline"', '"ECCV 2012"', '["Node Level", "Radiometric Calibration", "Lattice Regression", "Uniform Lattice", "Pixel Error"]', '"https://doi.org/10.1007/978-3-642-33718-5_40"', '"We describe a method to construct a sparse lookup table (LUT) that is effective in modeling the camera imaging pipeline that maps a RAW camera values to their sRGB output. This work builds on the recent in-camera color processing model proposed by Kim et al. [1] that included a 3D gamut-mapping function. The major drawback in [1] is the high computational cost of the 3D mapping function that uses radial basis functions (RBF) involving several thousand control points. We show how to construct a LUT using a novel nonuniform lattice regression method that adapts the LUT lattice to better fit the 3D gamut-mapping function. Our method offers not only a performance speedup of an order of magnitude faster than RBF, but also a compact mechanism to describe the imaging pipeline."'),
('"Normalized Cross-Correlation for Spherical Images"', '"ECCV 2004"', '["Spherical Harmonic", "Template Match", "Real Position", "Fourier Descriptor", "Normalize Corre"]', '"https://doi.org/10.1007/978-3-540-24671-8_43"', '"Recent advances in vision systems have spawned a new generation of image modalities. Most of today\\u2019s robot vehicles are equipped with omnidirectional sensors which facilitate navigation as well as immersive visualization. When an omnidirectional camera with a single viewpoint is calibrated, the original image can be warped to a spherical image. In this paper, we study the problem of template matching in spherical images. The natural transformation of a pattern on the sphere is a 3D rotation and template matching is the localization of a target in any orientation. Cross-correlation on the sphere is a function of 3D-rotation and it can be computed in a space-invariant way through a 3D inverse DFT of a linear combination of spherical harmonics. However, if we intend to normalize the cross-correlation, the computation of the local image variance is a space variant operation. In this paper, we present a new cross-correlation measure that correlates the image-pattern cross-correlation with the autocorrelation of the template with respect to orientation. Experimental results on artificial as well as real data show accurate localization performance with a variety of targets."'),
('"Normalized Gradient Vector Diffusion and Image Segmentation"', '"ECCV 2002"', '["Image segmentation", "Gradient vector diffusion", "Heat diffusion equation", "Active snake model",', '"https://doi.org/10.1007/3-540-47977-5_34"', '"In this paper, we present an approach for image segmentation, based on the existing Active Snake Model and Watershed-based Region Merging. Our algorithm includes initial segmentation using Normalized Gradient Vector Diffusion (NGVD) and region merging based on Region Adjacency Graph (RAG). We use a set of heat diffusion equations to generate a vector field over the image domain, which provides us with a natural way to define seeds as well as an external force to attract the active snakes. Then an initial segmentation of the original image can be obtained by a similar idea as seen in active snake model. Finally an RAG-based region merging technique is used to find the true segmentation as desired. The experimental results show that our NGVD-based region merging algorithm overcomes some problems as seen in classic active snake model. We will also see that our NGVD has several advantages over the traditional gradient vector diffusion."'),
('"Novel Skeletal Representation for Articulated Creatures"', '"ECCV 2004"', '["Geodesic Distance", "Medial Axis", "Polygonal Model", "Skeleton Graph", "Generalize Cylinder"]', '"https://doi.org/10.1007/978-3-540-24672-5_6"', '"Volumetric structures are frequently used as shape descriptors for 3D data. The capture of such data is being facilitated by developments in multi-view video and range scanning, extending to subjects that are alive and moving. In this paper, we examine vision-based modeling and the related representation of moving articulated creatures using spines. We define a spine as a branching axial structure representing the shape and topology of a 3D object\\u2019s limbs, and capturing the limbs\\u2019 correspondence and motion over time."'),
('"Novelty Detection in Image Sequences with Dynamic Background"', '"SMVP 2004"', '["Principal Component Analysis", "Singular Value Decomposition", "Foreground Object", "Principal Com', '"https://doi.org/10.1007/978-3-540-30212-4_11"', '"We propose a new scheme for novelty detection in image sequences capable of handling non-stationary background scenarious, such as waving trees, rain and snow. Novelty detection is the problem of classifying new observations from previous samples, as either novel or belonging to the background class. An adaptive background model, based on a linear PCA model in combination with local, spatial transformations, allows us to robustly model a variety of appearences. An incremental PCA algorithm is used, resulting in a fast and efficient detection algorithm. The system has been successfully applied to a number of different (outdoor) scenarious and compared to other approaches."'),
('"Null Space Approach of Fisher Discriminant Analysis for Face Recognition"', '"BioAW 2004"', '["Face Recognition", "Linear Discriminant Analysis", "Null Space", "Fisher Discriminant Analysis", "', '"https://doi.org/10.1007/978-3-540-25976-3_4"', '"The null space of the within-class scatter matrix is found to express most discriminative information for the small sample size problem (SSSP). The null space-based LDA takes full advantage of the null space while the other methods remove the null space. It proves to be optimal in performance. From the theoretical analysis, we present the NLDA algorithm and the most suitable situation for NLDA. Our method is simpler than all other null space approaches, it saves the computational cost and maintains the performance simultaneously. Furthermore, kernel technique is incorporated into discriminant analysis in the null space. Firstly, all samples are mapped to the kernel space through a better kernel function, called Cosine kernel, which is proposed to increase the discriminating capability of the original polynomial kernel function. Secondly, a truncated NLDA is employed. The novel approach only requires one eigen-value analysis and is also applicable to the large sample size problem. Experiments are carried out on different face data sets to demonstrate the effectiveness of the proposed methods."'),
('"Numerical Inversion of SRNFs for Efficient Elastic Shape Analysis of Star-Shaped Objects"', '"ECCV 2014"', '["Statistical shape analysis", "elastic shape analysis", "parameterized surface", "geodesic computat', '"https://doi.org/10.1007/978-3-319-10602-1_32"', '"The elastic shape analysis of surfaces has proven useful in several application areas, including medical image analysis, vision, and graphics."'),
('"Numerically Stable Optimization of Polynomial Solvers for Minimal Problems"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33712-3_8"', '"Numerous geometric problems in computer vision involve the solution of systems of polynomial equations. This is particularly true for so called minimal problems, but also for finding stationary points for overdetermined problems. The state-of-the-art is based on the use of numerical linear algebra on the large but sparse coefficient matrix that represents the original equations multiplied with a set of monomials. The key observation in this paper is that the speed and numerical stability of the solver depends heavily on (i) what multiplication monomials are used and (ii) the set of so called permissible monomials from which numerical linear algebra routines choose the basis of a certain quotient ring. In the paper we show that optimizing with respect to these two factors can give both significant improvements to numerical stability as compared to the state of the art, as well as highly compact solvers, while still retaining numerical stability. The methods are validated on several minimal problems that have previously been shown to be challenging with improvement over the current state of the art."'),
('"OBEliSK: Novel Knowledgebase of Object Features and Exchange Strategies"', '"ECCV 2014"', '["Ontologies", "Knowledge representation", "Handling affordances", "Semantic modelling", "Assistive ', '"https://doi.org/10.1007/978-3-319-16181-5_34"', '"This paper presents the design and development of a system intended for storing, querying and managing all required data related to a fluent human-robot object handover process. Our system acts as a bridge between visual perception and control systems in a robotic setup intended to collaborate with human partners, while the perception module provides information about the exchange environment. In order to achieve these goals, a semantic-ontological approach has been selected favouring system\\u2019s interoperability and extensibility, complemented with a set of utilities developed ad-hoc for easing the knowledge inference, query and management. As a result, the proposed knowledgebase provides a completeness level not previously reached in related state of the art approaches."'),
('"Object Categorization Based on a Supervised Mean Shift Algorithm"', '"ECCV 2012"', '["Target Object", "Visual Word", "Latent Dirichlet Allocation", "Object Categorization", "Feature Co', '"https://doi.org/10.1007/978-3-642-33885-4_64"', '"In this work, we present a C++ implementation of object categorization with the bag-of-word (BoW) framework. Unlike typical BoW models which consider the whole area of an image as the region of interest (ROI) for visual codebook generation, our implementation only considers the regions of target objects as ROIs and the unrelated backgrounds will be excluded for generating codebook. This is achieved by a supervised mean shift algorithm. Our work is on the benchmark SIVAL dataset and utilizes a Maximum Margin Supervised Topic Model for classification. The final performance of our work is quite encouraging."'),
('"Object Classification Using Heterogeneous Co-occurrence Features"', '"ECCV 2010"', '["Computer Vision", "IEEE Computer Society", "Color Histogram", "Gradient Orientation", "Heterogeneo', '"https://doi.org/10.1007/978-3-642-15552-9_16"', '"Co-occurrence features are effective for object classification because observing co-occurrence of two events is far more informative than observing occurrence of each event separately. For example, a color co-occurrence histogram captures co-occurrence of pairs of colors at a given distance while a color histogram just expresses frequency of each color. As one of such co-occurrence features, CoHOG (co-occurrence histograms of oriented gradients) has been proposed and a method using CoHOG with a linear classifier has shown a comparable performance with state-of-the-art pedestrian detection methods. According to recent studies, it has been suggested that combining heterogeneous features such as texture, shape, and color is useful for object classification. Therefore, we introduce three heterogeneous features based on co-occurrence called color-CoHOG, CoHED, and CoHD, respectively. Each heterogeneous features are evaluated on the INRIA person dataset and the Oxford 17/102 category flower datasets. The experimental results show that color-CoHOG is effective for the INRIA person dataset and CoHED is effective for the Oxford flower datasets. By combining above heterogeneous features, the proposed method achieves comparable classification performance to state-of-the-art methods on the above datasets. The results suggest that the proposed method using heterogeneous features can be used as an off-the-shelf method for various object classification tasks."'),
('"Object Co-detection via Efficient Inference in a Fully-Connected CRF"', '"ECCV 2014"', '["Object co-detection", "fully-connected CRFs"]', '"https://doi.org/10.1007/978-3-319-10578-9_22"', '"Object detection has seen a surge of interest in recent years, which has lead to increasingly effective techniques. These techniques, however, still mostly perform detection based on local evidence in the input image. While some progress has been made towards exploiting scene context, the resulting methods typically only consider a single image at a time. Intuitively, however, the information contained jointly in multiple images should help overcoming phenomena such as occlusion and poor resolution. In this paper, we address the co-detection problem that aims to leverage this collective power to achieve object detection simultaneously in all the images of a set. To this end, we formulate object co-detection as inference in a fully-connected CRF whose edges model the similarity between object candidates. We then learn a similarity function that allows us to efficiently perform inference in this fully-connected graph, even in the presence of many object candidates. This is in contrast with existing co-detection techniques that rely on exhaustive or greedy search, and thus do not scale well. Our experiments demonstrate the benefits of our approach on several co-detection datasets."'),
('"Object Co-detection"', '"ECCV 2012"', '["Training Image", "Object Detection", "Query Image", "Object Instance", "Part Representation"]', '"https://doi.org/10.1007/978-3-642-33718-5_7"', '"In this paper we introduce a new problem which we call object co-detection. Given a set of images with objects observed from two or multiple images, the goal of co-detection is to detect the objects, establish the identity of individual object instance, as well as estimate the viewpoint transformation of corresponding object instances. In designing a co-detector, we follow the intuition that an object has consistent appearance when observed from the same or different viewpoints. By modeling an object using state-of-the-art part-based representations such as [1,2], we measure appearance consistency between objects by comparing part appearance and geometry across images. This allows to effectively account for object self-occlusions and viewpoint transformations. Extensive experimental evaluation indicates that our co-detector obtains more accurate detection results than if objects were to be detected from each image individually. Moreover, we demonstrate the relevance of our co-detection scheme to other recognition problems such as single instance object recognition, wide-baseline matching, and image query."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Object Detection and Viewpoint Estimation with Auto-masking Neural Network"', '"ECCV 2014"', '["Object Detection", "Average Precision", "Image Patch", "Convolutional Neural Network", "Discrimina', '"https://doi.org/10.1007/978-3-319-10578-9_29"', '"Simultaneously detecting an object and determining its pose has become a popular research topic in recent years. Due to the large variances of the object appearance in images, it is critical to capture the discriminative object parts that can provide key information about the object pose. Recent part-based models have obtained state-of-the-art results for this task. However, such models either require manually defined object parts with heavy supervision or a complicated algorithm to find discriminative object parts. In this study, we have designed a novel deep architecture, called Auto-masking Neural Network (ANN), for object detection and viewpoint estimation. ANN can automatically learn to select the most discriminative object parts across different viewpoints from training images. We also propose a method of accurate continuous viewpoint estimation based on the output of ANN. Experimental results on related datasets show that ANN outperforms previous methods."'),
('"Object Detection by Contour Segment Networks"', '"ECCV 2006"', '["Object Detection", "Model Segment", "Image Edge", "Contour Segment", "Object Contour"]', '"https://doi.org/10.1007/11744078_2"', '"We propose a method for object detection in cluttered real images, given a single hand-drawn example as model. The image edges are partitioned into contour segments and organized in an image representation which encodes their interconnections: the Contour Segment Network. The object detection problem is formulated as finding paths through the network resembling the model outlines, and a computationally efficient detection technique is presented. An extensive experimental evaluation on detecting five diverse object classes over hundreds of images demonstrates that our method works in very cluttered images, allows for scale changes and considerable intra-class shape variation, is robust to interrupted contours, and is computationally efficient."'),
('"Object Detection from Large-Scale 3D Datasets Using Bottom-Up and Top-Down Descriptors"', '"ECCV 2008"', '["Point Cloud", "Target Object", "Object Detection", "Machine Intelligence", "Spin Image"]', '"https://doi.org/10.1007/978-3-540-88693-8_41"', '"We propose an approach for detecting objects in large-scale range datasets that combines bottom-up and top-down processes. In the bottom-up stage, fast-to-compute local descriptors are used to detect potential target objects. The object hypotheses are verified after alignment in a top-down stage using global descriptors that capture larger scale structure information. We have found that the combination of spin images and Extended Gaussian Images, as local and global descriptors respectively, provides a good trade-off between efficiency and accuracy. We present results on real outdoors scenes containing millions of scanned points and hundreds of targets. Our results compare favorably to the state of the art by being applicable to much larger scenes captured under less controlled conditions, by being able to detect object classes and not specific instances, and by being able to align the query with the best matching model accurately, thus obtaining precise segmentation."'),
('"Object Detection Using Strongly-Supervised Deformable Part Models"', '"ECCV 2012"', '["Object Detection", "Minimum Span Tree", "Star Model", "Object Part", "Stochastic Gradient Descent"', '"https://doi.org/10.1007/978-3-642-33718-5_60"', '"Deformable part-based models [1, 2] achieve state-of-the-art performance for object detection, but rely on heuristic initialization during training due to the optimization of non-convex cost function. This paper investigates limitations of such an initialization and extends earlier methods using additional supervision. We explore strong supervision in terms of annotated object parts and use it to (i) improve model initialization, (ii) optimize model structure, and (iii) handle partial occlusions. Our method is able to deal with sub-optimal and incomplete annotations of object parts and is shown to benefit from semi-supervised learning setups where part-level annotation is provided for a fraction of positive examples only. Experimental results are reported for the detection of six animal classes in PASCAL VOC 2007 and 2010 datasets. We demonstrate significant improvements in detection performance compared to the LSVM [1] and the Poselet [3] object detectors."'),
('"Object Level Grouping for Video Shots"', '"ECCV 2004"', '["Invariant Region", "Object Level", "Video Shot", "Query Region", "Reprojection Error"]', '"https://doi.org/10.1007/978-3-540-24671-8_7"', '"We describe a method for automatically associating image patches from frames of a movie shot into object-level groups. The method employs both the appearance and motion of the patches."'),
('"Object of Interest Detection by Saliency Learning"', '"ECCV 2010"', '["Salient Object", "Conditional Random Field", "Salient Region", "Saliency Detection", "Visual Salie', '"https://doi.org/10.1007/978-3-642-15552-9_46"', '"In this paper, we present a method for object of interest detection. This method is statistical in nature and hinges in a model which combines salient features using a mixture of linear support vector machines. It exploits a divide-and-conquer strategy by partitioning the feature space into sub-regions of linearly separable data-points. This yields a structured learning approach where we learn a linear support vector machine for each region, the mixture weights, and the combination parameters for each of the salient features at hand. Thus, the method learns the combination of salient features such that a mixture of classifiers can be used to recover objects of interest in the image. We illustrate the utility of the method by applying our algorithm to the MSRA Salient Object Database."'),
('"Object Reading: Text Recognition for Object Recognition"', '"ECCV 2012"', '["Object Recognition", "Optical Character Recognition", "Text Detection", "Text Recognition", "Scene', '"https://doi.org/10.1007/978-3-642-33885-4_46"', '"We propose to use text recognition to aid in visual object class recognition. To this end we first propose a new algorithm for text detection in natural images. The proposed text detection is based on saliency cues and a context fusion step. The algorithm does not need any parameter tuning and can deal with varying imaging conditions. We evaluate three different tasks: 1. Scene text recognition, where we increase the state-of-the-art by 0.17 on the ICDAR 2003 dataset. 2. Saliency based object recognition, where we outperform other state-of-the-art saliency methods for object recognition on the PASCAL VOC 2011 dataset. 3. Object recognition with the aid of recognized text, where we are the first to report multi-modal results on the IMET set. Results show that text helps for object class recognition if the text is not uniquely coupled to individual object instances."'),
('"Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary"', '"ECCV 2002"', '["Object recognition", "correspondence", "EM algorithm"]', '"https://doi.org/10.1007/3-540-47979-1_7"', '"We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well \\u2014 for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach."'),
('"Object Recognition by Integrating Multiple Image Segmentations"', '"ECCV 2008"', '["Ground Truth", "Image Segmentation", "Object Recognition", "Object Segmentation", "Ground Truth La', '"https://doi.org/10.1007/978-3-540-88690-7_36"', '"The joint tasks of object recognition and object segmentation from a single image are complex in their requirement of not only correct classification, but also deciding exactly which pixels belong to the object. Exploring all possible pixel subsets is prohibitively expensive, leading to recent approaches which use unsupervised image segmentation to reduce the size of the configuration space. Image segmentation, however, is known to be unstable, strongly affected by small image perturbations, feature choices, or different segmentation algorithms. This instability has led to advocacy for using multiple segmentations of an image. In this paper, we explore the question of how to best integrate the information from multiple bottom-up segmentations of an image to improve object recognition robustness. By integrating the image partition hypotheses in an intuitive combined top-down and bottom-up recognition approach, we improve object and feature support. We further explore possible extensions of our method and whether they provide improved performance. Results are presented on the MSRC 21-class data set and the Pascal VOC2007 object segmentation challenge."'),
('"Object Recognition Robust to Imperfect Depth Data"', '"ECCV 2012"', '["Object Recognition", "Depth Function", "Depth Measurement", "Depth Data", "Adaptive Model"]', '"https://doi.org/10.1007/978-3-642-33868-7_9"', '"In this paper, we present an adaptive data fusion model that robustly integrates depth and image only perception. Combining dense depth measurements with images can greatly enhance the performance of many computer vision algorithms, yet degraded depth measurements (e.g., missing data) can also cause dramatic performance losses to levels below image-only algorithms. We propose a generic fusion model based on maximum likelihood estimates of fused image-depth functions for both available and missing depth data. We demonstrate its application to each step of a state-of-the-art image-only object instance recognition pipeline. The resulting approach shows increased recognition performance over alternative data fusion approaches."'),
('"Object Recognition Using Coloured Receptive Fields"', '"ECCV 2000"', '["Object Recognition", "Texture & Colour", "Appearance-Based Vision", "Phicons"]', '"https://doi.org/10.1007/3-540-45054-8_11"', '"This paper describes an extension of a technique for the recognition and tracking of every day objects in cluttered scenes. The goal is to build a system in which ordinary desktop objects serve as physical icons in a vision based system for man-machine interaction. In such a system, the manipulation of objects replaces user commands."'),
('"Object Recognition Using Junctions"', '"ECCV 2010"', '["Object Recognition", "Object Detection", "Corner Point", "Junction Point", "Shape Match"]', '"https://doi.org/10.1007/978-3-642-15555-0_2"', '"In this paper, we propose an object detection/recognition algorithm based on a new set of shape-driven features and morphological operators. Each object class is modeled by the corner points (junctions) on its contour. We design two types of shape-context like features between the corner points, which are efficient to compute and effective in capturing the underlying shape deformation. In the testing stage, we use a recently proposed junction detection algorithm [1] to detect corner points/junctions on natural images. The detection and recognition of an object are then done by matching learned shape features to those in the input image with an efficient search strategy. The proposed system is robust to a certain degree of scale change and we obtained encouraging results on the ETHZ dataset. Our algorithm also has advantages of recognizing object parts and dealing with occlusions."'),
('"Object Recognition with Hierarchical Stel Models"', '"ECCV 2010"', '["Object Recognition", "Spatial Pyramid", "Histogram Intersection", "Spatial Pyramid Match", "Pyrami', '"https://doi.org/10.1007/978-3-642-15567-3_2"', '"We propose a new generative model, and a new image similarity kernel based on a linked hierarchy of probabilistic segmentations. The model is used to efficiently segment multiple images into a consistent set of image regions. The segmentations are provided at several levels of granularity and links among them are automatically provided. Model training and inference in it is faster than most local feature extraction algorithms, and yet the provided image segmentation, and the segment matching among images provide a rich backdrop for image recognition, segmentation and registration tasks."'),
('"Object Retrieval by Query with Sensibility Based on the KANSEI-Vocabulary Scale"', '"ECCV 2006"', '["Visual Information", "Image Retrieval", "Geometrical Form", "Contour Detection", "Object Retrieval', '"https://doi.org/10.1007/11754336_11"', '"Recently the demand for image retrieval and recognizable extraction corresponding to KANSEI (sensibility) has been increasing, and the studies focused on establishing those KANSEI-based systems have been progressing more than ever. In addition, the attempt to understand, measure and evaluate, and apply KANSEI to situational design or products will be required more and more in the future. Particularly, study of KANSEI-based image retrieval tools have especially been in the spotlight. So many investigators give a trial of using KANSEI for image retrieval. However, the research in this area is still under its primary stage because it is difficult to process higher-level contents as emotion or KANSEI of human. To solve this problem, we suggest the KANSEI-Vocabulary Scale by associating human sensibilities with shapes among visual information. And we construct the object retrieval system for evaluation of KANSEI-Vocabulary Scale by shape. In our evaluation results, we are able to retrieve object images with the most appropriate shape in term of the query\\u2019s KANSEI. Furthermore, the method achieves an average rate of 71% user\\u2019s satisfaction."'),
('"Object Segmentation by Long Term Analysis of Point Trajectories"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15555-0_21"', '"Unsupervised learning requires a grouping step that defines which data belong together. A natural way of grouping in images is the segmentation of objects or parts of objects. While pure bottom-up segmentation from static cues is well known to be ambiguous at the object level, the story changes as soon as objects move. In this paper, we present a method that uses long term point trajectories based on dense optical flow. Defining pair-wise distances between these trajectories allows to cluster them, which results in temporally consistent segmentations of moving objects in a video shot. In contrast to multi-body factorization, points and even whole objects may appear or disappear during the shot. We provide a benchmark dataset and an evaluation method for this so far uncovered setting."'),
('"Object, Scene and Actions: Combining Multiple Features for Human Action Recognition"', '"ECCV 2010"', '["Feature Channel", "Candidate Object", "Multiple Kernel Learning", "Multiple Instance Learn", "Scen', '"https://doi.org/10.1007/978-3-642-15549-9_36"', '"In many cases, human actions can be identified not only by the singular observation of the human body in motion, but also properties of the surrounding scene and the related objects. In this paper, we look into this problem and propose an approach for human action recognition that integrates multiple feature channels from several entities such as objects, scenes and people. We formulate the problem in a multiple instance learning (MIL) framework, based on multiple feature channels. By using a discriminative approach, we join multiple feature channels embedded to the MIL space. Our experiments over the large YouTube dataset show that scene and object information can be used to complement person features for human action recognition."'),
('"Object-Centric Spatial Pooling for Image Classification"', '"ECCV 2012"', '["Object Detector", "Outer Loop", "Foreground Region", "Spatial Pyramid Match", "Simple Dataset"]', '"https://doi.org/10.1007/978-3-642-33709-3_1"', '"Spatial pyramid matching (SPM) based pooling has been the dominant choice for state-of-art image classification systems. In contrast, we propose a novel object-centric spatial pooling (OCP) approach, following the intuition that knowing the location of the object of interest can be useful for image classification. OCP consists of two steps: (1) inferring the location of the objects, and (2) using the location information to pool foreground and background features separately to form the image-level representation. Step (1) is particularly challenging in a typical classification setting where precise object location annotations are not available during training. To address this challenge, we propose a framework that learns object detectors using only image-level class labels, or so-called weak labels. We validate our approach on the challenging PASCAL07 dataset. Our learned detectors are comparable in accuracy with state-of-the-art weakly supervised detection methods. More importantly, the resulting OCP approach significantly outperforms SPM-based pooling in image classification."'),
('"Object-Layout-Aware Image Retrieval for Personal Album Management"', '"ECCV 2012"', '["Object Layout", "Image Search", "Photo Search"]', '"https://doi.org/10.1007/978-3-642-33885-4_68"', '"This demo shows a real-time object-layout-aware image retrieval system for personal album management. The query of the system is image\\u2019s object layout and the system retrieves images based on the layout similarity of concerned objects in the query."'),
('"Objective Colour from Multispectral Imaging"', '"ECCV 2000"', '["Surface Albedo", "Multispectral Image", "Surface Patch", "Objective Colour", "Colour Descriptor"]', '"https://doi.org/10.1007/3-540-45054-8_24"', '"The light reflected from a surface depends on the scene geometry, the incident illumination and the surface material. One of the properties of the material is its albedo \\u03c1(\\u03bb) and its variation with respect to wavelength. The albedo of a surface is purely a physical property. Our perception of albedo is commonly referred to as colour. This paper presents a novel methodology for extracting the albedo of the various materials in the scene independent of incident light and scene geometry. A scene is captured under different narrow-band colour filters and the spectral derivatives of the scene are computed. The resulting spectral derivatives form a spectral gradient at each pixel. This spectral gradient is a normalized albedo descriptor which is invariant to scene geometry and incident illumination for diffuse surfaces."'),
('"Objects as Attributes for Scene Classification"', '"ECCV 2010"', '["Image Representation", "Spatial Pyramid", "British National Corpus", "Scene Dataset", "Scene Class', '"https://doi.org/10.1007/978-3-642-35749-7_5"', '"Robust low-level image features have proven to be effective representations for a variety of high-level visual recognition tasks, such as object recognition and scene classification. But as the visual recognition tasks become more challenging, the semantic gap between low-level feature representation and the meaning of the scenes increases. In this paper, we propose to use objects as attributes of scenes for scene classification. We represent images by collecting their responses to a large number of object detectors, or \\u201cobject filters\\u201d. Such representation carries high-level semantic information rather than low-level image feature information, making it more suitable for high-level visual recognition tasks. Using very simple, off-the-shelf classifiers such as SVM, we show that this object-level image representation can be used effectively for high-level visual tasks such as scene classification. Our results are superior to reported state-of-the-art performance on a number of standard datasets."'),
('"Occlusion and Motion Reasoning for Long-Term Tracking"', '"ECCV 2014"', '["Object Tracking", "Appearance Model", "Visual Tracking", "Object Label", "Background Label"]', '"https://doi.org/10.1007/978-3-319-10599-4_12"', '"Object tracking is a reoccurring problem in computer vision. Tracking-by-detection approaches, in particular Struck [20], have shown to be competitive in recent evaluations. However, such approaches fail in the presence of long-term occlusions as well as severe viewpoint changes of the object. In this paper we propose a principled way to combine occlusion and motion reasoning with a tracking-by-detection approach. Occlusion and motion reasoning is based on state-of-the-art long-term trajectories which are labeled as object or background tracks with an energy-based formulation. The overlap between labeled tracks and detected regions allows to identify occlusions. The motion changes of the object between consecutive frames can be estimated robustly from the geometric relation between object trajectories. If this geometric change is significant, an additional detector is trained. Experimental results show that our tracker obtains state-of-the-art results and handles occlusion and viewpoints changes better than competing tracking methods."'),
('"Occlusion Boundary Detection Using Pseudo-depth"', '"ECCV 2010"', '["Singular Value Decomposition", "Reference Image", "Motion Estimation", "Markov Random Field", "Mot', '"https://doi.org/10.1007/978-3-642-15561-1_39"', '"We address the problem of detecting occlusion boundaries from motion sequences, which is important for motion segmentation, estimating depth order, and related tasks. Previous work by Stein and Hebert has addressed this problem and obtained good results on a benchmarked dataset using two-dimensional image cues, motion estimation, and a global boundary model [1]. In this paper we describe a method for detecting occlusion boundaries which uses depth cues and local segmentation cues. More specifically, we show that crude scaled estimates of depth, which we call pseudo-depth, can be extracted from motion sequences containing a small number of image frames using standard SVD factorization methods followed by weak smoothing using a Markov Random Field defined over super-pixels. We then train a classifier for occlusion boundaries using pseudo-depth and local static boundary cues (adding motion cues only gives slightly better results). We evaluate performance on Stein and Hebert\\u2019s dataset and obtain results of similar average quality which are better in the low recall/high precision range. Note that our cues and methods are different from [1] \\u2013 in particular we did not use their sophisticated global boundary model \\u2013 and so we conjecture that a unified approach would yield even better results."'),
('"Occlusion Handling in Video Segmentation via Predictive Feedback"', '"ECCV 2012"', '["Segmentation Result", "Video Segmentation", "Object Label", "Consistent Label", "Occlusion Handlin', '"https://doi.org/10.1007/978-3-642-33885-4_24"', '"We present a method for unsupervised on-line dense video segmentation which utilizes sequential Bayesian estimation techniques to resolve partial and full occlusions. Consistent labeling through occlusions is vital for applications which move from low-level object labels to high-level semantic knowledge - tasks such as activity recognition or robot control. The proposed method forms a predictive loop between segmentation and tracking, with tracking predictions used to seed the segmentation kernel, and segmentation results used to update tracked models. All segmented labels are tracked, without the use of a-priori models, using parallel color-histogram particle filters. Predictions are combined into a probabilistic representation of image labels, a realization of which is used to seed segmentation. A simulated annealing relaxation process allows the realization to converge to a minimal energy segmented image. Found segments are subsequently used to repopulate the particle sets, closing the loop. Results on the Cranfield benchmark sequence demonstrate that the prediction mechanism allows on-line segmentation to maintain temporally consistent labels through partial & full occlusions, significant appearance changes, and rapid erratic movements. Additionally, we show that tracking performance matches state-of-the art tracking methods on several challenging benchmark sequences."'),
('"Omnidirectional Vision: Unified Model Using Conformal Geometry"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_41"', '"It has been proven that a catadioptric projection can be modeled by an equivalent spherical projection. In this paper we present an extension and improvement of those ideas using the conformal geometric algebra, a modern framework for the projective space of hyper-spheres. Using this mathematical system, the analysis of diverse catadioptric mirrors becomes transparent and computationally simpler. As a result, the algebraic burden is reduced, allowing the user to work in a much more effective framework for the development of algorithms for omnidirectional vision. This paper includes complementary experimental analysis related to omnidirectional vision guided robot navigation."'),
('"On Affine Invariant Clustering and Automatic Cast Listing in Movies"', '"ECCV 2002"', '["Computer Vision", "Cluster Algorithm", "Distance Function", "Trust Region", "Distance Matrice"]', '"https://doi.org/10.1007/3-540-47977-5_20"', '"We develop a distance metric for clustering and classification algorithms which is invariant to affine transformations and includes priors on the transformation parameters. Such clustering requirements are generic to a number of problems in computer vision."'),
('"On Calibration and Reconstruction from Planar Curves"', '"ECCV 2000"', '["Singular Point", "Algebraic Curf", "Fundamental Matrix", "Planar Curf", "Epipolar Line"]', '"https://doi.org/10.1007/3-540-45054-8_44"', '"We describe in this paper closed-form solutions to the following problems in multi-view geometry of n\\u2019th order curves: (i) recovery of the fundamental matrix from 4 or more conic matches in two views, (ii) recovery of the homography matrix from a single n\\u2019th order (n \\u2265 3) matching curve and, in turn, recovery of the fundamental matrix from two matching n\\u2019th order planar curves, and (iii) 3D reconstruction of a planar algebraic curve from two views."'),
('"On Calibration of a Low-Cost Time-of-Flight Camera"', '"ECCV 2014"', '["Depth Image", "Intrinsic Parameter", "Joint Optimization", "Color Camera", "Amplitude Image"]', '"https://doi.org/10.1007/978-3-319-16178-5_29"', '"Time-of-flight (ToF) cameras are becoming more and more popular in computer vision. In many applications 3D information delivered by a ToF camera is used, and it is very important to know the camera\\u2019s extrinsic and intrinsic parameters, as well as precise depth information. A straightforward algorithm to calibrate a ToF camera is to use a standard color camera calibration procedure [12], on the amplitude images. However, depth information delivered by ToF cameras is known to contain complex bias due to several error sources [6]. Additionally, it is desirable in many cases to determine the pose of the ToF camera relative to the other sensors used."'),
('"On Image Contours of Projective Shapes"', '"ECCV 2014"', '["Tangent Plane", "Projective Geometry", "Perspective Projection", "Visual Hull", "Gaussian Image"]', '"https://doi.org/10.1007/978-3-319-10593-2_48"', '"This paper revisits classical properties of the outlines of solid shapes bounded by smooth surfaces, and shows that they can be established in a purely projective setting, without appealing to Euclidean measurements such as normals or curvatures. In particular, we give new synthetic proofs of Koenderink\\u2019s famous theorem on convexities and concavities of the image contour, and of the fact that the rim turns in the same direction as the viewpoint in the tangent plane at a convex point, and in the opposite direction at a hyperbolic point. This suggests that projective geometry should not be viewed merely as an analytical device for linearizing calculations (its main role in structure from motion), but as the proper framework for studying the relation between solid shape and its perspective projections. Unlike previous work in this area, the proposed approach does not require an oriented setting, nor does it rely on any choice of coordinate system or analytical considerations."'),
('"On Learning Higher-Order Consistency Potentials for Multi-class Pixel Labeling"', '"ECCV 2012"', '["Gaussian Mixture Model", "Image Denoising", "Unary Potential", "Pairwise Potential", "Iterate Cond', '"https://doi.org/10.1007/978-3-642-33709-3_15"', '"Pairwise Markov random fields are an effective framework for solving many pixel labeling problems in computer vision. However, their performance is limited by their inability to capture higher-order correlations. Recently proposed higher-order models are showing superior performance to their pairwise counterparts. In this paper, we derive two variants of the higher-order lower linear envelop model and show how to perform tractable move-making inference in these models. We propose a novel use of this model for encoding consistency constraints over large sets of pixels. Importantly these pixel sets do not need to be contiguous. However, the consistency model has a large number of parameters to be tuned for good performance. We exploit the structured SVM paradigm to learn optimal parameters and show some practical techniques to overcome huge computation requirements. We evaluate our model on the problems of image denoising and semantic segmentation."'),
('"On Mean Pose and Variability of 3D Deformable Models"', '"ECCV 2014"', '["Shape dynamics", "Motion analysis", "Shape spaces"]', '"https://doi.org/10.1007/978-3-319-10605-2_19"', '"We present a novel methodology for the analysis of complex object shapes in motion observed by multiple video cameras. In particular, we propose to learn local surface rigidity probabilities (i.e., deformations), and to estimate a mean pose over a temporal sequence. Local deformations can be used for rigidity-based dynamic surface segmentation, while a mean pose can be used as a sequence keyframe or a cluster prototype and has therefore numerous applications, such as motion synthesis or sequential alignment for compression or morphing. We take advantage of recent advances in surface tracking techniques to formulate a generative model of 3D temporal sequences using a probabilistic framework, which conditions shape fitting over all frames to a simple set of intrinsic surface rigidity properties. Surface tracking and rigidity variable estimation can then be formulated as an Expectation-Maximization inference problem and solved by alternatively minimizing two nested fixed point iterations. We show that this framework provides a new fundamental building block for various applications of shape analysis, and achieves comparable tracking performance to state of the art surface tracking techniques on real datasets, even compared to approaches using strong kinematic priors such as rigid skeletons."'),
('"On Parameter Learning in CRF-Based Approaches to Object Class Image Segmentation"', '"ECCV 2010"', '["Image Region", "Conditional Random Field", "Factor Graph", "Parameter Learning", "Segmentation Acc', '"https://doi.org/10.1007/978-3-642-15567-3_8"', '"Recent progress in per-pixel object class labeling of natural images can be attributed to the use of multiple types of image features and sound statistical learning approaches. Within the latter, Conditional Random Fields (CRF) are prominently used for their ability to represent interactions between random variables. Despite their popularity in computer vision, parameter learning for CRFs has remained difficult, popular approaches being cross-validation and piecewise training."'),
('"On Pencils of Tangent Planes and the Recognition of Smooth 3D Shapes from Silhouettes"', '"ECCV 2002"', '["Feature Vector", "Feature Space", "Image Plane", "Signature Function", "Tangent Plane"]', '"https://doi.org/10.1007/3-540-47977-5_43"', '"This paper presents a geometric approach to recognizing smooth objects from their outlines. We define a signature function that associates feature vectors with objects and baselines connecting pairs of possible viewpoints. Feature vectors, which can be projective, affine, or Euclidean, are computed using the planes that pass through a fixed baseline and are also tangent to the object\\u2019s surface. In the proposed framework, matching a test outline to a set of training outlines is equivalent to finding intersections in feature space between the images of the training and the test signature functions. The paper presents experimental results for the case of internally calibrated perspective cameras, where the feature vectors are angles between epipolar tangent planes."'),
('"On Performance Characterization and Optimization for Image Retrieval"', '"ECCV 2002"', '["Performance Prediction", "Image Retrieval", "User Query", "Image Detector", "Vision Algorithm"]', '"https://doi.org/10.1007/3-540-47979-1_4"', '"In content-based image retrieval (CBIR) performance characterization is easily being neglected. A major difficulty lies in the fact that ground truth and the definition of benchmarks are extremely user and application dependent. This paper proposes a two-stage CBIR framework which allows to predict the behavior of the retrieval system as well as to optimize its performance. In particular, it is possible to maximize precision, recall, or jointly precision and recall. The framework is based on the detection of high-level concepts in images. These concepts correspond to vocabulary users can query the database with. Performance optimization is carried out on the basis of the user query, the performance of the concept detectors, and an estimated distribution of the concepts in the database. The optimization is transparent to the user and leads to a set of internal parameters that optimize the succeeding retrieval. Depending only on the query and the desired concept, precision and recall of the retrieval can be increased by up to 40%. The paper discusses the theoretical and empirical results of the optimization as well as its dependency on the estimate of the concept distribution."'),
('"On Recognizing Actions in Still Images via Multiple Features"', '"ECCV 2012"', '["Action Recognition", "Salient Object", "Object Region", "Human Action Recognition", "Multiple Inst', '"https://doi.org/10.1007/978-3-642-33885-4_27"', '"We propose a multi-cue based approach for recognizing human actions in still images, where relevant object regions are discovered and utilized in a weakly supervised manner. Our approach does not require any explicitly trained object detector or part/attribute annotation. Instead, a multiple instance learning approach is used over sets of object hypotheses in order to represent objects relevant to the actions. We test our method on the extensive Stanford 40 Actions dataset [1] and achieve significant performance gain compared to the state-of-the-art. Our results show that using multiple object hypotheses within multiple instance learning is effective for human action recognition in still images and such an object representation is suitable for using in conjunction with other visual features."'),
('"On Refractive Optical Flow"', '"ECCV 2004"', '["Background Image", "Environment Matting", "Attenuation Function", "Background Motion", "Specular S', '"https://doi.org/10.1007/978-3-540-24671-8_38"', '"This paper presents a novel generalization of the optical flow equation to the case of refraction, and it describes a method for recovering the refractive structure of an object from a video sequence acquired as the background behind the refracting object moves. By structure here we mean a representation of how the object warps and attenuates (or amplifies) the light passing through it. We distinguish between the cases when the background motion is known and unknown. We show that when the motion is unknown, the refractive structure can only be estimated up to a six-parameter family of solutions without additional sources of information. Methods for solving for the refractive structure are described in both cases. The performance of the algorithm is demonstrated on real data, and results of applying the estimated refractive structure to the task of environment matting and compositing are presented."'),
('"On Sampling Focal Length Values to Solve the Absolute Pose Problem"', '"ECCV 2014"', '["RANSAC", "n-point-pose (PnP)", "camera pose estimation"]', '"https://doi.org/10.1007/978-3-319-10593-2_54"', '"Estimating the absolute pose of a camera relative to a 3D representation of a scene is a fundamental step in many geometric Computer Vision applications. When the camera is calibrated, the pose can be computed very efficiently. If the calibration is unknown, the problem becomes much harder, resulting in slower solvers or solvers requiring more samples and thus significantly longer run-times for RANSAC. In this paper, we challenge the notion that using minimal solvers is always optimal and propose to compute the pose for a camera with unknown focal length by randomly sampling a focal length value and using an efficient pose solver for the now calibrated camera. Our main contribution is a novel sampling scheme that enables us to guide the sampling process towards promising focal length values and avoids considering all possible values once a good pose is found. The resulting RANSAC variant is significantly faster than current state-of-the-art pose solvers, especially for low inlier ratios, while achieving a similar or better pose accuracy."'),
('"On Shape and Material Recovery from Motion"', '"ECCV 2014"', '["Object Motion", "Camera Motion", "Shape Recovery", "Photometric Stereo", "Surface Depth"]', '"https://doi.org/10.1007/978-3-319-10584-0_14"', '"We present a framework for the joint recovery of the shape and reflectance of an object with dichromatic BRDF, using motion cues. We show that four (small or differential) motions of the object, or three motions of the camera, suffice to yield a linear system that decouples shape and BRDF. The theoretical benefit is that precise limits on shape and reflectance recovery using motion cues may be derived. We show that shape may be recovered for unknown isotropic BRDF and light source. Simultaneous reflectance estimation is shown ambiguous for general isotropic BRDFs, but possible for restricted BRDFs representing commong materials like metals, plastics and paints. The practical benefit of the decoupling is that joint shape and BRDF recovery need not rely on alternating methods, or restrictive priors. Further, our theory yields conditions for the joint estimability of shape, albedo, BRDF and directional lighting using motion cues. Surprisingly, such problems are shown to be well-posed even for some non-Lambertian material types. Experiments on measured BRDFs from the MERL database validate our theory."'),
('"On Tensor-Based PDEs and Their Corresponding Variational Formulations with Application to Color Ima', '"ECCV 2012"', '["Color Image", "Variational Formulation", "Image Structure", "Image Denoising", "Outer Product"]', '"https://doi.org/10.1007/978-3-642-33712-3_16"', '"The case when a partial differential equation (PDE) can be considered as an Euler-Lagrange (E-L) equation of an energy functional, consisting of a data term and a smoothness term is investigated. We show the necessary conditions for a PDE to be the E-L equation for a corresponding functional. This energy functional is applied to a color image denoising problem and it is shown that the method compares favorably to current state-of-the-art color image denoising techniques."'),
('"On the Convergence of Graph Matching: Graduated Assignment Revisited"', '"ECCV 2012"', '["Graph Match", "Quadratic Assignment Problem", "Discrete Domain", "Continuous Domain", "Spectral Ma', '"https://doi.org/10.1007/978-3-642-33712-3_59"', '"We focus on the problem of graph matching that is fundamental in computer vision and machine learning. Many state-of-the-arts frequently formulate it as integer quadratic programming, which incorporates both unary and second-order terms. This formulation is in general NP-hard thus obtaining an exact solution is computationally intractable. Therefore most algorithms seek the approximate optimum by relaxing techniques. This paper commences with the finding of the \\u201ccircular\\u201d character of solution chain obtained by the iterative Gradient Assignment (via Hungarian method) in the discrete domain, and proposes a method for guiding the solver converging to a fixed point, resulting a convergent algorithm for graph matching in discrete domain. Furthermore, we extend the algorithms to their counterparts in continuous domain, proving the classical graduated assignment algorithm will converge to a double-circular solution chain, and the proposed Soft Constrained Graduated Assignment (SCGA) method will converge to a fixed (discrete) point, both under wild conditions. Competitive performances are reported in both synthetic and real experiments."'),
('"On the Effects of Illumination Normalization with LBP-Based Watchlist Screening"', '"ECCV 2014"', '["Illumination normalization", "Local binary patterns", "Face screening", "Still-to-video face recog', '"https://doi.org/10.1007/978-3-319-16181-5_13"', '"Still-to-video face recognition (FR) is an important function in several video surveillance applications like watchlist screening, where faces captured over a network of video cameras are matched against reference stills belonging to target individuals. Screening of faces against a watchlist is a challenging problem due to variations in capturing conditions (e.g., pose and illumination), to camera inter-operability, and to the limited number of reference stills. In holistic approaches to FR, Local Binary Pattern (LBP) descriptors are often considered to represent facial captures and reference stills. Despite their efficiency, LBP descriptors are known as being sensitive to illumination changes. In this paper, the performance of still-to-video FR is compared when different passive illumination normalization techniques are applied prior to LBP feature extraction. This study focuses on representative retinex, self-quotient, diffusion, filtering, means de-noising, retina, wavelet and frequency-based techniques that are suitable for fast and accurate face screening. Experimental results obtained with videos from the Chokepoint dataset indicate that, although Multi-Scale Weberfaces and Tan and Triggs techniques tend to outperform others, the benefits of these techniques varies considerably according to the individual and illumination conditions. Results suggest that a combination of these techniques should be selected dynamically based on changing capture conditions."'),
('"On the Estimation of the Fundamental Matrix: A Convex Approach to Constrained Least-Squares"', '"ECCV 2000"', '["Linear Matrix Inequality", "Geometric Error", "Fundamental Matrix", "Point Correspondence", "Epipo', '"https://doi.org/10.1007/3-540-45054-8_16"', '"In this paper we consider the problem of estimating the fundamental matrix from point correspondences. It is well known that the most accurate estimates of this matrix are obtained by criteria minimizing geometric errors when the data are affected by noise. It is also well known that these criteria amount to solving non-convex optimization problems and, hence, their solution is affected by the optimization starting point. Generally, the starting point is chosen as the fundamental matrix estimated by a linear criterion but this estimate can be very inaccurate and, therefore, inadequate to initialize methods with other error criteria."'),
('"On the Evaluation of Scene Flow Estimation"', '"ECCV 2012"', '["Ground Truth", "Motion Estimation", "View Synthesis", "Image Prediction", "Temporal Correspondence', '"https://doi.org/10.1007/978-3-642-33868-7_15"', '"This paper surveys the state of the art in evaluating the performance of scene flow estimation and points out the difficulties in generating benchmarks with ground truth which have not allowed the development of general, reliable solutions. Hopefully, the renewed interest in dynamic 3D content, which has led to increased research in this area, will also lead to more rigorous evaluation and more effective algorithms. We begin by classifying methods that estimate depth, motion or both from multi-view sequences according to their parameterization of shape and motion. Then, we present several criteria for their evaluation, discuss their strengths and weaknesses and conclude with recommendations."'),
('"On the Motion and Appearance of Specularities in Image Sequences"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47969-4_34"', '"Real scenes are full of specularities (highlights and reflections), and yet most vision algorithms ignore them. In order to capture the appearance of realistic scenes, we need to model specularities as separate layers. In this paper, we study the behavior of specularities in static scenes as the camera moves, and describe their dependence on varying surface geometry, orientation, and scene point and camera locations. For a rectilinear camera motion with constant velocity, we study how the specular motion deviates from a straight trajectory (disparity deviation) and how much it violates the epipolar constraint (epipolar deviation). Surprisingly, for surfaces that are convex or not highly undulating, these deviations are usually quite small. We also study the appearance of specularities, i.e., how they interact with the body reflection, and with the usual occlusion ordering constraints applicable to diffuse opaque layers. We present a taxonomy of specularities based on their photometric properties as a guide for designing separation techniques. Finally, we propose a technique to extract specularities as a separate layer, and demonstrate it using an image sequence of a complex scene."'),
('"On the Non-linear Optimization of Projective Motion Using Minimal Parameters"', '"ECCV 2002"', '["Projection Matrix", "Fundamental Matrix", "Normalization Constraint", "Bundle Adjustment", "Epipol', '"https://doi.org/10.1007/3-540-47967-8_23"', '"I address the problem of optimizing projective motion over a minimal set of parameters. Most of the existing works overparameterize the problem. While this can simplify the estimation process and may ensure well-conditioning of the parameters, this also increases the computational cost since more unknowns than necessary are involved."'),
('"On the Performance Characterisation of Image Segmentation Algorithms: A Case Study"', '"ECCV 2000"', '["Receiver Operating Characteristic", "Ground Truth", "Receiver Operating Characteristic Curve", "As', '"https://doi.org/10.1007/3-540-45053-X_23"', '"An experimental vehicle is being developed for the purposes of precise crop treatment, with the aim of reducing chemical use and thereby improving quality and reducing both costs and environmental contamination. For differential treatment of crop and weed, the vehicle must discriminate between crop, weed and soil. We present a two stage algorithm designed for this purpose, and use this algorithm to illustrate how empirical discrepancy methods, notably the analysis of type I and type II statistical errors and receiver operating characteristic curves, may be used to compare algorithm performance over a set of test images which represent typical working conditions for the vehicle. Analysis of performance is presented for the two stages of the algorithm separately, and also for the combined algorithm. This analysis allows us to understand the effects of various types of misclassification error on the overall algorithm performance, and as such is a valuable methodology for computer vision engineers."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"On the Representation and Matching of Qualitative Shape at Multiple Scales"', '"ECCV 2002"', '["Directed Acyclic Graph", "Face Detection", "Gesture Recognition", "Coarse Scale", "Graph Match"]', '"https://doi.org/10.1007/3-540-47977-5_50"', '"We present a framework for representing and matching multi-scale, qualitative feature hierarchies. The coarse shape of an object is captured by a set of blobs and ridges, representing compact and elongated parts of an object. These parts, in turn, map to nodes in a directed acyclic graph, in which parent/child edges represent feature overlap, sibling edges join nodes with shared parents, and all edges encode geometric relations between the features. Given two feature hierarchies, represented as directed acyclic graphs, we present an algorithm for computing both similarity and node correspondence in the presence of noise and occlusion. Similarity, in turn, is a function of structural similarity, contextual similarity (geometric relations among neighboring nodes), and node contents similarity. Moreover, the weights of these components can be varied on a node by node basis, allowing a graph-based model to effectively parameterize the saliency of its constraints. We demonstrate the approach on two domains: gesture recognition and face detection."'),
('"On the Reprojection of 3D and 2D Scenes without Explicit Model Selection"', '"ECCV 2000"', '["Null Space", "Fundamental Matrix", "Estimation Matrix", "Projection Matrice", "Reprojection Error"', '"https://doi.org/10.1007/3-540-45054-8_61"', '"It is known that recovering projection matrices from planar configurations is ambiguous, thus, posing the problem of model selection \\u2014 is the scene planar (2D) or non-planar (3D)? For a 2D scene one would recover a homography matrix, whereas for a 3D scene one would recover the fundamental matrix or trifocal tensor. The task of model selection is especially problematic when the scene is neither 2D nor 3D \\u2014 for example a \\u201cthin\\u201d volume in space."'),
('"On the Significance of Real-World Conditions for Material Classification"', '"ECCV 2004"', '["Support Vector Machine", "Radial Basis Function", "Recognition Rate", "Markov Random Field", "Imag', '"https://doi.org/10.1007/978-3-540-24673-2_21"', '"Classifying materials from their appearance is a challenging problem, especially if illumination and pose conditions are permitted to change: highlights and shadows caused by 3D structure can radically alter a sample\\u2019s visual texture. Despite these difficulties, researchers have demonstrated impressive results on the CUReT database which contains many images of 61 materials under different conditions. A first contribution of this paper is to further advance the state-of-the-art by applying Support Vector Machines to this problem. To our knowledge, we record the best results to date on the CUReT database."'),
('"On the Statistical Determination of Optimal Camera Configurations in Large Scale Surveillance Netwo', '"ECCV 2012"', '["Camera placement", "optimization", "resersible jump Markov chain Monte Carlo", "simulated annealin', '"https://doi.org/10.1007/978-3-642-33718-5_4"', '"The selection of optimal camera configurations (camera locations, orientations etc.) for multi-camera networks remains an unsolved problem. Previous approaches largely focus on proposing various objective functions to achieve different tasks. Most of them, however, do not generalize well to large scale networks. To tackle this, we introduce a statistical formulation of the optimal selection of camera configurations as well as propose a Trans-Dimensional Simulated Annealing (TDSA) algorithm to effectively solve the problem. We compare our approach with a state-of-the-art method based on Binary Integer Programming (BIP) and show that our approach offers similar performance on small scale problems. However, we also demonstrate the capability of our approach in dealing with large scale problems and show that our approach produces better results than 2 alternative heuristics designed to deal with the scalability issue of BIP."'),
('"On the Structure and Properties of the Quadrifocal Tensor"', '"ECCV 2000"', '["Reference Plane", "Fundamental Matrix", "Projection Matrice", "Homography Matrix", "Trifocal Tenso', '"https://doi.org/10.1007/3-540-45054-8_46"', '"The quadrifocal tensor which connects image measurements along 4 views is not yet well understood as its counterparts the fundamental matrix and the trifocal tensor. This paper establishes the structure of the tensor as an \\u201cepipole-homography\\u201d pairing Q ijkl = v\\u2032j Hikl - v\\u2032k Hijl + v\\u2034l Hijk where v \\u2032 ,v \\u2032\\u2019 ,v \\u2034 are the epipoles in views 2,3,4, H is the \\u201chomography tensor\\u201d the 3-view analogue of the homography matrix, and the indices i,j,k,l are attached to views 1,2,3,4 respectively \\u2014 i.e., H ikl is the homography tensor of views 1,3,4."'),
('"On Utilising Template and Feature-based Correspondence in Multi-view Appearance Models"', '"ECCV 2000"', '["Appearance Model", "Kernel Principal Component Analysis", "Landmark Point", "Active Appearance Mod', '"https://doi.org/10.1007/3-540-45054-8_52"', '"In principle, the recovery and reconstruction of a 3D object from its 2D view projections require the parameterisation of its shape structure and surface reflectance properties. Explicit representation and recovery of such 3D information is notoriously difficult to achieve. Alternatively, a linear combination of 2D views can be used which requires the establishment of dense correspondence between views. This in general, is difficult to compute and necessarily expensive. In this paper we examine the use of affine and local feature-based transformations in establishing correspondences between very large pose variations. In doing so, we utilise a generic-view template, a generic 3D surface model and Kernel PCA for modelling shape and texture nonlinearities across views. The abilities of both approaches to reconstruct and recover faces from any 2D image are evaluated and compared."'),
('"On Weighting and Choosing Constraints for Optimally Reconstructing the Geometry of Image Triplets"', '"ECCV 2000"', '["Lateral Motion", "Forward Motion", "Object Point", "Projection Centre", "Projection Matrice"]', '"https://doi.org/10.1007/3-540-45053-X_43"', '"Optimally reconstructing the geometry of image triplets from point correspondences requires a proper weighting or selection of the used constraints between observed coordinates and unknown parameters. By analysing the ML-estimation process the paper solves a set of yet unsolved problems: (1) The minimal set of four linearily independent trilinearities (Shashua 1995, Hartley 1995) actually imposes only three constraints onto the geometry of the image triplet. The seeming contradiction between the number of used constraints, three vs. four, can be explained naturally using the normal equations. (2) Direct application of such an estimation suggests a pseudoinverse of a 4 x 4-matix having rank 3 which contains the covariance matrix of the homologeous image points to be the optimal weight matrix. (3) Instead of using this singluar weight matrix one could select three linearily dependent constraints. This is discussed for the two classical cases of forward and lateral motion, and clarifies the algebraic analyis of dependencies between trilinear constraints by Faugeras 1995."'),
('"One-Shot Optimal Exposure Control"', '"ECCV 2010"', '["Exposure Control", "Automatic Exposure Control", "Photon Noise", "Optimal Exposure", "Saturated Pi', '"https://doi.org/10.1007/978-3-642-15549-9_15"', '"We introduce an algorithm to estimate the optimal exposure parameters from the analysis of a single, possibly under- or over-exposed, image. This algorithm relies on a new quantitative measure of exposure quality, based on the average rendering error, that is, the difference between the original irradiance and its reconstructed value after processing and quantization. In order to estimate the exposure quality in the presence of saturated pixels, we fit a log-normal distribution to the brightness data, computed from the unsaturated pixels. Experimental results are presented comparing the estimated vs. \\u201cground truth\\u201d optimal exposure parameters under various illumination conditions."'),
('"Online 3D Reconstruction and 6-DoF Pose Estimation for RGB-D Sensors"', '"ECCV 2014"', '["Simultaneous Localization and Mapping", "RGB-D SLAM"]', '"https://doi.org/10.1007/978-3-319-16178-5_16"', '"In this paper, we propose an approach to Simultaneous Localization and Mapping (SLAM) for RGB-D sensors. Our system computes 6-DoF pose and sparse feature map of the environment. We propose a novel keyframe selection scheme based on the Fisher information, and new loop closing method that utilizes feature-to-landmark correspondences inspired by image-based localization. As a result, the system effectively mitigates drift that is frequently observed in visual odometry system. Our approach gives lowest relative pose error amongst any other approaches tested on public benchmark dataset. A set of 3D reconstruction results on publicly available RGB-D videos are presented."'),
('"Online Adaptive Gaussian Mixture Learning for Video Applications"', '"SMVP 2004"', '["Surveillance Video", "Video Application", "Mixture Parameter", "Recursive Filter", "Adaptive Windo', '"https://doi.org/10.1007/978-3-540-30212-4_10"', '"This paper presents an online EM learning algorithm for training adaptive Gaussian mixtures for non-stationary video data. Existing solutions are either slow in learning or computationally and storage inefficient. Our solution is derived based on sufficient statistics of the short-term distribution. To avoid unnecessary computation or storage, we show that the equivalent estimates can be accomplished by a set of recursive parameter update equations with one additional variable. The solution is evaluated against several existing algorithms on both synthetic data and surveillance videos. The results showed remarkable learning efficiency and robustness over current solutions."'),
('"Online Graph-Based Tracking"', '"ECCV 2014"', '["Online tracking", "Bayesian model averaging", "patch matching"]', '"https://doi.org/10.1007/978-3-319-10602-1_8"', '"Tracking by sequential Bayesian filtering relies on a graphical model with temporally ordered linear structure based on temporal smoothness assumption. This framework is convenient to propagate the posterior through the first-order Markov chain. However, density propagation from a single immediately preceding frame may be unreliable especially in challenging situations such as abrupt appearance changes, fast motion, occlusion, and so on. We propose a visual tracking algorithm based on more general graphical models, where multiple previous frames contribute to computing the posterior in the current frame and edges between frames are created upon inter-frame trackability. Such data-driven graphical model reflects sequence structures as well as target characteristics, and is more desirable to implement a robust tracking algorithm. The proposed tracking algorithm runs online and achieves outstanding performance with respect to the state-of-the-art trackers. We illustrate quantitative and qualitative performance of our algorithm in all the sequences in tracking benchmark and other challenging videos."'),
('"Online Learned Discriminative Part-Based Appearance Models for Multi-human Tracking"', '"ECCV 2012"', '["multi-human tracking", "online learned discriminative models"]', '"https://doi.org/10.1007/978-3-642-33718-5_35"', '"We introduce an online learning approach to produce discriminative part-based appearance models (DPAMs) for tracking multiple humans in real scenes by incorporating association based and category free tracking methods. Detection responses are gradually associated into tracklets in multiple levels to produce final tracks. Unlike most previous multi-target tracking approaches which do not explicitly consider occlusions in appearance modeling, we introduce a part based model that explicitly finds unoccluded parts by occlusion reasoning in each frame, so that occluded parts are removed in appearance modeling. Then DPAMs for each tracklet is online learned to distinguish a tracklet with others as well as the background, and is further used in a conservative category free tracking approach to partially overcome the missed detection problem as well as to reduce difficulties in tracklet associations under long gaps. We evaluate our approach on three public data sets, and show significant improvements compared with state-of-art methods."'),
('"Online Learning of Linear Predictors for Real-Time Tracking"', '"ECCV 2012"', '["template tracking", "template learning", "linear predictors"]', '"https://doi.org/10.1007/978-3-642-33718-5_34"', '"Although fast and reliable, real-time template tracking using linear predictors requires a long training time. The lack of the ability to learn new templates online prevents their use in applications that require fast learning. This especially holds for applications where the scene is not known a priori and multiple templates have to be added online. So far, linear predictors had to be either learned offline [1] or in an iterative manner by starting with a small sized template and growing it over time [2]. In this paper, we propose a fast and simple reformulation of the learning procedure that allows learning new linear predictors online."'),
('"Online Moving Camera Background Subtraction"', '"ECCV 2012"', '["Motion Vector", "Gaussian Mixture Model", "Background Subtraction", "Motion Model", "Appearance Mo', '"https://doi.org/10.1007/978-3-642-33783-3_17"', '"Recently several methods for background subtraction from moving camera were proposed. They use bottom up cues to segment video frames into foreground and background regions. Due to this lack of explicit models, they can easily fail to detect a foreground object when such cues are ambiguous in certain parts of the video. This becomes even more challenging when videos need to be processed online. We present a method which enables learning of pixel based models for foreground and background regions and, in addition, segments each frame in an online framework. The method uses long term trajectories along with a Bayesian filtering framework to estimate motion and appearance models. We compare our method to previous approaches and show results on challenging video sequences."'),
('"Online Sparse Matrix Gaussian Process Regression and Vision Applications"', '"ECCV 2008"', '["Online Learning", "Inertia Measurement Unit", "Sparse Matrix", "Radial Basis Function Kernel", "Tr', '"https://doi.org/10.1007/978-3-540-88682-2_36"', '"We present a new Gaussian Process inference algorithm, called Online Sparse Matrix Gaussian Processes (OSMGP), and demonstrate its merits with a few vision applications. The OSMGP is based on the observation that for kernels with local support, the Gram matrix is typically sparse. Maintaining and updating the sparse Cholesky factor of the Gram matrix can be done efficiently using Givens rotations. This leads to an exact, online algorithm whose update time scales linearly with the size of the Gram matrix. Further, if approximate updates are permissible, the Cholesky factor can be maintained at a constant size using hyperbolic rotations to remove certain rows and columns corresponding to discarded training examples. We demonstrate that, using these matrix downdates, online hyperparameter estimation can be included without affecting the linear runtime complexity of the algorithm. The OSMGP algorithm is applied to head-pose estimation and visual tracking problems. Experimental results demonstrate that the proposed method is accurate, efficient and generalizes well using online learning."'),
('"Online Spatio-temporal Structural Context Learning for Visual Tracking"', '"ECCV 2012"', '["Spatio-temporal", "context constraint", "subspaces learning", "multiple instance boosting", "uncon', '"https://doi.org/10.1007/978-3-642-33765-9_51"', '"Visual tracking is a challenging problem, because the target frequently change its appearance, randomly move its location and get occluded by other objects in unconstrained environments. The state changes of the target are temporally and spatially continuous, in this paper therefore, a robust Spatio-Temporal structural context based Tracker (STT) is presented to complete the tracking task in unconstrained environments. The temporal context capture the historical appearance information of the target to prevent the tracker from drifting to the background in a long term tracking. The spatial context model integrates contributors, which are the key-points automatically discovered around the target, to build a supporting field. The supporting field provides much more information than appearance of the target itself so that the location of the target will be predicted more precisely. Extensive experiments on various challenging databases demonstrate the superiority of our proposed tracker over other state-of-the-art trackers."'),
('"Online Tracking and Reacquisition Using Co-trained Generative and Discriminative Trackers"', '"ECCV 2008"', '["Unlabeled Data", "Discriminative Model", "Appearance Change", "Appearance Variation", "Generative ', '"https://doi.org/10.1007/978-3-540-88688-4_50"', '"Visual tracking is a challenging problem, as an object may change its appearance due to viewpoint variations, illumination changes, and occlusion. Also, an object may leave the field of view and then reappear. In order to track and reacquire an unknown object with limited labeling data, we propose to learn these changes online and build a model that describes all seen appearance while tracking. To address this semi-supervised learning problem, we propose a co-training based approach to continuously label incoming data and online update a hybrid discriminative generative model. The generative model uses a number of low dimension linear subspaces to describe the appearance of the object. In order to reacquire an object, the generative model encodes all the appearance variations that have been seen. A discriminative classifier is implemented as an online support vector machine, which is trained to focus on recent appearance variations. The online co-training of this hybrid approach accounts for appearance changes and allows reacquisition of an object after total occlusion. We demonstrate that under challenging situations, this method has strong reacquisition ability and robustness to distracters in background."'),
('"Online Video Registration of Dynamic Scenes Using Frame Prediction"', '"WDV 2006"', '["Motion Parallax", "Dynamic Texture", "Image Alignment", "Brightness Constancy", "Preceding Frame"]', '"https://doi.org/10.1007/978-3-540-70932-9_12"', '"An online approach is proposed for Video registration of dynamic scenes, such as scenes with dynamic textures, moving objects, motion parallax, etc. This approach has three steps: (i) Assume that a few frames are already registered. (ii) Using the registered frames, the next frame is predicted. (iii) A new video frame is registered to the predicted frame."'),
('"Online Video Segmentation by Bayesian Split-Merge Clustering"', '"ECCV 2012"', '["Normalize Mutual Information", "Dirichlet Process", "Adjust Rand Index", "Evolutionary Cluster", "', '"https://doi.org/10.1007/978-3-642-33765-9_61"', '"We present an online video segmentation algorithm based on a novel nonparametric Bayesian clustering method called Bayesian Split-Merge Clustering (BSMC). BSMC can efficiently cluster dynamically changing data through split and merge processes at each time step, where the decision for splitting and merging is made by approximate posterior distributions over partitions with Dirichlet Process (DP) priors. Moreover, BSMC sidesteps the difficult problem of finding the proper number of clusters by virtue of the flexibility of nonparametric Bayesian models. We naturally apply BSMC to online video segmentation, which is composed of three steps\\u2014pixel clustering, histogram-based merging and temporal matching. We demonstrate the performance of our algorithm on complex real video sequences compared to other existing methods."'),
('"Online, Real-Time Tracking Using a Category-to-Individual Detector"', '"ECCV 2014"', '["Target Object", "Tracking Algorithm", "Appearance Model", "Pedestrian Detection", "Individual Dete', '"https://doi.org/10.1007/978-3-319-10590-1_24"', '"A method for online, real-time tracking of objects is presented. Tracking is treated as a repeated detection problem where potential target objects are identified with a pre-trained category detector and object identity across frames is established by individual-specific detectors. The individual detectors are (re-)trained online from a single positive example whenever there is a coincident category detection. This ensures that the tracker is robust to drift. Real-time operation is possible since an individual-object detector is obtained through elementary manipulations of the thresholds of the category detector and therefore only minimal additional computations are required. Our tracking algorithm is benchmarked against nine state-of-the-art trackers on two large, publicly available and challenging video datasets. We find that our algorithm is 10% more accurate and nearly as fast as the fastest of the competing algorithms, and it is as accurate but 20 times faster than the most accurate of the competing algorithms."'),
('"OpenDR: An Approximate Differentiable Renderer"', '"ECCV 2014"', '["Inverse graphics", "Rendering", "Optimization", "Automatic Differentiation", "Software", "Programm', '"https://doi.org/10.1007/978-3-319-10584-0_11"', '"Inverse graphics attempts to take sensor data and infer 3D geometry, illumination, materials, and motions such that a graphics renderer could realistically reproduce the observed scene. Renderers, however, are designed to solve the forward process of image synthesis. To go in the other direction, we propose an approximate differentiable renderer (DR) that explicitly models the relationship between changes in model parameters and image observations. We describe a publicly available OpenDR framework that makes it easy to express a forward graphics model and then automatically obtain derivatives with respect to the model parameters and to optimize over them. Built on a new auto-differentiation package and OpenGL, OpenDR provides a local optimization method that can be incorporated into probabilistic programming frameworks. We demonstrate the power and simplicity of programming with OpenDR by using it to solve the problem of estimating human body shape from Kinect depth and RGB data."'),
('"Optical Flow Estimation with Channel Constancy"', '"ECCV 2014"', '["Optical flow", "channel representation", "pyramids", "large motions"]', '"https://doi.org/10.1007/978-3-319-10590-1_28"', '"Large motions remain a challenge for current optical flow algorithms. Traditionally, large motions are addressed using multi-resolution representations like Gaussian pyramids. To deal with large displacements, many pyramid levels are needed and, if an object is small, it may be invisible at the highest levels. To address this we decompose images using a channel representation (CR) and replace the standard brightness constancy assumption with a descriptor constancy assumption. CRs can be seen as an over-segmentation of the scene into layers based on some image feature. If the appearance of a foreground object differs from the background then its descriptor will be different and they will be represented in different layers. We create a pyramid by smoothing these layers, without mixing foreground and background or losing small objects. Our method estimates more accurate flow than the baseline on the MPI-Sintel benchmark, especially for fast motions and near motion boundaries."'),
('"Optimal Contour Closure by Superpixel Grouping"', '"ECCV 2010"', '["Image Edge", "Perceptual Grouping", "Contour Closure", "Contour Image", "Ground Truth Segmentation', '"https://doi.org/10.1007/978-3-642-15552-9_35"', '"Detecting contour closure, i.e., finding a cycle of disconnected contour fragments that separates an object from its background, is an important problem in perceptual grouping. Searching the entire space of possible groupings is intractable, and previous approaches have adopted powerful perceptual grouping heuristics, such as proximity and co-curvilinearity, to manage the search. We introduce a new formulation of the problem, by transforming the problem of finding cycles of contour fragments to finding subsets of superpixels whose collective boundary has strong edge support in the image. Our cost function, a ratio of a novel learned boundary gap measure to area, promotes spatially coherent sets of superpixels. Moreover, its properties support a global optimization procedure using parametric maxflow. We evaluate our framework by comparing it to two leading contour closure approaches, and find that it yields improved performance."'),
('"Optimal Essential Matrix Estimation via Inlier-Set Maximization"', '"ECCV 2014"', '["Essential matrix", "robust estimation", "global optimization", "branch-and-bound"]', '"https://doi.org/10.1007/978-3-319-10590-1_8"', '"In this paper, we extend the globally optimal \\u201crotation space search\\u201d method [11] to essential matrix estimation in the presence of feature mismatches or outliers. The problem is formulated as inlier-set cardinality maximization, and solved via branch-and-bound global optimization which searches the entire essential manifold formed by all essential matrices. Our main contributions include an explicit, geometrically meaningful essential manifold parametrization using a 5D direct product space of a solid 2D disk and a solid 3D ball, as well as efficient closed-form bounding functions. Experiments on both synthetic data and real images have confirmed the efficacy of our method. The method is mostly suitable for applications where robustness and accuracy are paramount. It can also be used as a benchmark for method evaluation."'),
('"Optimal Importance Sampling for Tracking in Image Sequences: Application to Point Tracking"', '"ECCV 2004"', '["Image Sequence", "Point Tracking", "Cluttered Background", "Importance Function", "Gaussian System', '"https://doi.org/10.1007/978-3-540-24672-5_24"', '"In this paper, we propose a particle filtering approach for tracking applications in image sequences. The system we propose combines a measurement equation and a dynamic equation which both depend on the image sequence. Taking into account several possible observations, the likelihood is modeled as a linear combination of Gaussian laws. Such a model allows inferring an analytic expression of the optimal importance function used in the diffusion process of the particle filter. It also enables building a relevant approximation of a validation gate. We demonstrate the significance of this model for a point tracking application."'),
('"Optimal Multi-frame Correspondence with Assignment Tensors"', '"ECCV 2006"', '["Cost Function", "Linear Complexity", "Measurement Matrix", "Correspondence Problem", "Matching Can', '"https://doi.org/10.1007/11744078_38"', '"Establishing correspondence between features of a set of images has been a long-standing issue amongst the computer vision community. We propose a method that solves the multi-frame correspondence problem by imposing a rank constraint on the observed scene, i.e. rigidity is assumed. Since our algorithm is based solely on a geometrical (global) criterion, it does not suffer from issues usually associated to local methods, such as the aperture problem."'),
('"Optimal Templates for Nonrigid Surface Reconstruction"', '"ECCV 2012"', '["Quadratic Model", "Reconstruction Error", "Geodesic Distance", "Image Observation", "Orthographic ', '"https://doi.org/10.1007/978-3-642-33718-5_50"', '"This paper addresses the problem of reconstructing a deforming surface from point observations in a monocular video sequence. Recent state-of-the-art approaches divide the surface into smaller patches to simplify the problem. Among these, one very promising approach reconstructs the patches individually using a quadratic deformation model. In this paper, we demonstrate limitations that affect its applicability to real-world data and propose an approach that overcomes these problems. In particular, we show how to eliminate the need for manually picking a template that is used to model the deformations. We evaluate our algorithm on both synthetic and real-world data sets and show that it systematically reduces the reconstruction error by a factor of up to ten."'),
('"Optimization Algorithms for the Selection of Key Frame Sequences of Variable Length"', '"ECCV 2002"', '["Greedy Algorithm", "Video Segment", "Original Video", "Video Summarization", "Longe Common Subsequ', '"https://doi.org/10.1007/3-540-47979-1_27"', '"This paper presents a novel optimization-based approach for video key frame selection. We define key frames to be a temporally ordered subsequence of the original video sequence, and the optimal k key frames are the subsequence of length k that optimizes an energy function we define on all subsequences. These optimal key subsequences form a hierarchy, with one such subsequence for every k less than the length of the video n, and this hierarchy can be retrieved all at once using a dynamic programming process with polynomial (On 3) computation time. To further reduce computation, an approximate solution based on a greedy algorithm can compute the key frame hierarchy in O(n\\u00b7log(n)). We also present a hybrid method, which flexibly captures the virtues of both approaches. Our empirical comparisons between the optimal and greedy solutions indicate their results are very close. We show that the greedy algorithm is more appropriate for video streaming and network applications where compression ratios may change dynamically, and provide a method to compute the appropriate times to advance through key frames during video playback of the compressed stream. Additionally, we exploit the results of the greedy algorithm to devise an interactive video content browser. To quantify our algorithms\\u2019 effectiveness, we propose a new evaluation measure, called \\u201cwell-distributed\\u201d key frames. Our experimental results on several videos show that both the optimal and the greedy algorithms outperform several popular existing algorithms in terms of summarization quality, computational time, and guaranteed convergence."'),
('"Optimization of Symmetric Transfer Error for Sub-frame Video Synchronization"', '"ECCV 2008"', '["Video Sequence", "Synchronization Algorithm", "Synthetic Sequence", "Feature Trajectory", "Frame A', '"https://doi.org/10.1007/978-3-540-88688-4_41"', '"In this work we present a method to synchronize video sequences of events that are acquired via uncalibrated cameras at unknown and dynamically varying temporal offsets. Unlike existing methods that synchronize videos of similar events (i.e., videos related to each other through the motion in the scene) up to an integer alignment, we establish sub-frame video synchronization. While contemporary synchronization algorithms implement a unidirectional alignment which biases the results towards a single reference sequence, we adopt a bi-directional or symmetrical alignment approach that results in a more optimal synchronization. To this end, we propose a novel symmetric transfer error which is dynamically minimized, and reduces the propagation of error from feature extraction and spatial mapping into temporal synchronization. The advantages of our approach are validated by tests conducted on (publicly available) real and synthetic sequences. We present qualitative and quantitative comparisons with another state-of-the-art algorithm. A unique application of this work in generating high-resolution 4D MRI data from multiple low-resolution MRI scans is described."'),
('"Optimization-Based Artifact Correction for Electron Microscopy Image Stacks"', '"ECCV 2014"', '["Denoising", "Volume Electron Microscopy", "Connectomics", "Optimization", "Segmentation"]', '"https://doi.org/10.1007/978-3-319-10605-2_15"', '"Investigations of biological ultrastructure, such as comprehensive mapping of connections within a nervous system, increasingly rely on large, high-resolution electron microscopy (EM) image volumes. However, discontinuities between the registered section images from which these volumes are assembled, due to variations in imaging conditions and section thickness, among other artifacts, impede truly 3-D analysis of these volumes. We propose an optimization procedure, called EMISAC (EM Image Stack Artifact Correction), to correct these discontinuities. EMISAC optimizes the parameters of spatially varying linear transformations of the data in order to minimize the squared norm of the gradient along the section axis, subject to detail-preserving regularization."'),
('"Optimizing Binary MRFs with Higher Order Cliques"', '"ECCV 2008"', '["Markov Random Field", "Neighborhood System", "Clique Size", "Computer Vision Problem", "Quadratic ', '"https://doi.org/10.1007/978-3-540-88690-7_8"', '"Widespread use of efficient and successful solutions of Computer Vision problems based on pairwise Markov Random Field (MRF) models raises a question: does any link exist between the pairwise and higher order MRFs such that the like solutions can be applied to the latter models? This work explores such a link for binary MRFs that allow us to represent Gibbs energy of signal interaction with a polynomial function. We show how a higher order polynomial can be efficiently transformed into a quadratic function. Then energy minimization tools for the pairwise MRF models can be easily applied to the higher order counterparts. Also, we propose a method to analytically estimate the potential parameter of the asymmetric Potts prior. The proposed framework demonstrates very promising experimental results of image segmentation and can be used to solve other Computer Vision problems."'),
('"Optimizing Complex Loss Functions in Structured Prediction"', '"ECCV 2010"', '["Loss Function", "Markov Random Field", "Piecewise Linear Approximation", "Compatibility Function",', '"https://doi.org/10.1007/978-3-642-15552-9_42"', '"In this paper we develop an algorithm for structured prediction that optimizes against complex performance measures, those which are a function of false positive and false negative counts. The approach can be directly applied to performance measures such as F \\u03b2 score (natural language processing), intersection over union (image segmentation), Precision/Recall at k (search engines) and ROC area (binary classifiers). We attack this optimization problem by approximating the loss function with a piecewise linear function and relaxing the obtained QP problem to a LP which we solve with an off-the-shelf LP solver. We present experiments on object class-specific segmentation and show significant improvement over baseline approaches that either use simple loss functions or simple compatibility functions on VOC 2009."'),
('"Optimizing Ranking Measures for Compact Binary Code Learning"', '"ECCV 2014"', '["Hash Function", "Column Generation", "Ranking Measure", "Locality Sensitive Hash", "Hash Code"]', '"https://doi.org/10.1007/978-3-319-10578-9_40"', '"Hashing has proven a valuable tool for large-scale information retrieval. Despite much success, existing hashing methods optimize over simple objectives such as the reconstruction error or graph Laplacian related loss functions, instead of the performance evaluation criteria of interest\\u2014multivariate performance measures such as the AUC and NDCG. Here we present a general framework (termed StructHash) that allows one to directly optimize multivariate performance measures. The resulting optimization problem can involve exponentially or infinitely many variables and constraints, which is more challenging than standard structured output learning. To solve the StructHash optimization problem, we use a combination of column generation and cutting-plane techniques. We demonstrate the generality of StructHash by applying it to ranking prediction and image retrieval, and show that it outperforms a few state-of-the-art hashing methods."'),
('"Optimum Subspace Learning and Error Correction for Tensors"', '"ECCV 2010"', '["Irregular Pattern", "Matrix Completion", "Tensor Decomposition", "Optimal Rank", "Robust Principal', '"https://doi.org/10.1007/978-3-642-15558-1_57"', '"Confronted with the high-dimensional tensor-like visual data, we derive a method for the decomposition of an observed tensor into a low-dimensional structure plus unbounded but sparse irregular patterns. The optimal rank-(R 1,R 2,...R n ) tensor decomposition model that we propose in this paper, could automatically explore the low-dimensional structure of the tensor data, seeking optimal dimension and basis for each mode and separating the irregular patterns. Consequently, our method accounts for the implicit multi-factor structure of tensor-like visual data in an explicit and concise manner. In addition, the optimal tensor decomposition is formulated as a convex optimization through relaxation technique. We then develop a block coordinate descent (BCD) based algorithm to efficiently solve the problem. In experiments, we show several applications of our method in computer vision and the results are promising."'),
('"Order-Preserving Sparse Coding for Sequence Classification"', '"ECCV 2012"', '["Feature Vector", "Input Sequence", "Synthetic Dataset", "Dynamic Time Warping", "Sparse Code"]', '"https://doi.org/10.1007/978-3-642-33709-3_13"', '"In this paper, we investigate order-preserving sparse coding for classifying multi-dimensional sequence data. Such a problem is often tackled by first decomposing the input sequence into individual frames and extracting features, then performing sparse coding or other processing for each frame based feature vector independently, and finally aggregating individual responses to classify the input sequence. However, this heuristic approach ignores the underlying temporal order of the input sequence frames, which in turn results in suboptimal discriminative capability. In this work, we introduce a temporal-order-preserving regularizer which aims to preserve the temporal order of the reconstruction coefficients. An efficient Nesterov-type smooth approximation method is developed for optimization of the new regularization criterion, with guaranteed error bounds. Extensive experiments for time series classification on a synthetic dataset, several machine learning benchmarks, and a challenging real-world RGB-D human activity dataset, show that the proposed coding scheme is discriminative and robust, and it outperforms previous art for sequence classification."'),
('"Orientation Covariant Aggregation of Local Descriptors with Embeddings"', '"ECCV 2014"', '["Vector Representation", "Local Descriptor", "Image Search", "Sift Descriptor", "Dominant Orientati', '"https://doi.org/10.1007/978-3-319-10599-4_25"', '"Image search systems based on local descriptors typically achieve orientation invariance by aligning the patches on their dominant orientations. Albeit successful, this choice introduces too much invariance because it does not guarantee that the patches are rotated consistently."'),
('"Oriented Visibility for Multiview Reconstruction"', '"ECCV 2006"', '["multiview reconstruction", "image-based modeling", "visibility", "dense stereo", "graph cuts", "di', '"https://doi.org/10.1007/11744078_18"', '"Visibility estimation is arguably the most difficult problem in dense 3D reconstruction from multiple arbitrary views. In this paper, we propose a simple new approach to estimating visibility based on position and orientation of local surface patches. Using our concept of oriented visibility, we present a new algorithm for multiview reconstruction based on exact global optimization of surface photoconsistency using graph cuts on a CW-complex. In contrast to many previous methods for 3D reconstruction from arbitrary views, our method does not depend on initialization and is robust to photometrically difficult situations."'),
('"OTC: A Novel Local Descriptor for Scene Classification"', '"ECCV 2014"', '["local descriptor", "scene classification", "scene recognition"]', '"https://doi.org/10.1007/978-3-319-10584-0_25"', '"Scene classification is the task of determining the scene type in which a photograph was taken. In this paper we present a novel local descriptor suited for such a task: Oriented Texture Curves (OTC). Our descriptor captures the texture of a patch along multiple orientations, while maintaining robustness to illumination changes, geometric distortions and local contrast differences. We show that our descriptor outperforms all state-of-the-art descriptors for scene classification algorithms on the most extensive scene classification benchmark to-date."'),
('"Output Regularized Metric Learning with Side Information"', '"ECCV 2008"', '["Distance Metric Learning", "Side Information", "Output Regularized Metric Learning", "Collaborativ', '"https://doi.org/10.1007/978-3-540-88690-7_27"', '"Distance metric learning has been widely investigated in machine learning and information retrieval. In this paper, we study a particular content-based image retrieval application of learning distance metrics from historical relevance feedback log data, which leads to a novel scenario called collaborative image retrieval. The log data provide the side information expressed as relevance judgements between image pairs. Exploiting the side information as well as inherent neighborhood structures among examples, we design a convex regularizer upon which a novel distance metric learning approach, named output regularized metric learning, is presented to tackle collaborative image retrieval. Different from previous distance metric methods, the proposed technique integrates synergistic information from both log data and unlabeled data through a regularization framework and pilots the desired metric toward the ideal output that satisfies pairwise constraints revealed by side information. The experiments on image retrieval tasks have been performed to validate the feasibility of the proposed distance metric technique."'),
('"Overconstrained Linear Estimation of Radial Distortion and Multi-view Geometry"', '"ECCV 2006"', '["Image Pair", "Normal Equation", "Bundle Adjustment", "Lens Distortion", "Distortion Parameter"]', '"https://doi.org/10.1007/11744023_20"', '"This paper introduces a new method for simultaneous estimation of lens distortion and multi-view geometry using only point correspondences. The new technique has significant advantages over the current state-of-the art in that it makes more effective use of correspondences arising from any number of views. Multi-view geometry in the presence of lens distortion can be expressed as a set of point correspondence constraints that are quadratic in the unknown distortion parameter. Previous work has demonstrated how the system can be solved efficiently as a quadratic eigenvalue problem by operating on the normal equations of the system. Although this approach is appropriate for situations in which only a minimal set of matchpoints are available, it does not take full advantage of extra correspondences in overconstrained situations, resulting in significant bias and many potential solutions. The new technique directly operates on the initial constraint equations and solves the quadratic eigenvalue problem in the case of rectangular matrices. The method is shown to contain significantly less bias on both controlled and real-world data and, in the case of a moving camera where additional views serve to constrain the number of solutions, an accurate estimate of both geometry and distortion is achieved."'),
('"P2\\u03a0: A Minimal Solution for Registration of 3D Points to 3D Planes"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15555-0_32"', '"This paper presents a class of minimal solutions for the 3D-to-3D registration problem in which the sensor data are 3D points and the corresponding object data are 3D planes. In order to compute the 6 degrees-of-freedom transformation between the sensor and the object, we need at least six points on three or more planes. We systematically investigate and develop pose estimation algorithms for several configurations, including all minimal configurations, that arise from the distribution of points on planes. The degenerate configurations are also identified. We point out that many existing and unsolved 2D-to-3D and 3D-to-3D pose estimation algorithms involving points, lines, and planes can be transformed into the problem of registering points to planes. In addition to simulations, we also demonstrate the algorithm\\u2019s effectiveness in two real-world applications: registration of a robotic arm with an object using a contact sensor, and registration of 3D point clouds that were obtained using multi-view reconstruction of planar city models."'),
('"Pairwise Clustering with Matrix Factorisation and the EM Algorithm"', '"ECCV 2002"', '["Motion Vector", "Matrix Factorisation", "Cluster Membership", "Link Weight", "Motion Segmentation"', '"https://doi.org/10.1007/3-540-47967-8_5"', '"In this paper we provide a direct link between the EM algorithm and matrix factorisation methods for grouping via pairwise clustering. We commence by placing the pairwise clustering process in the setting of the EM algorithm. We represent the clustering process using two sets of variables which need to be estimated. The first of these are cluster-membership indicators. The second are revised link-weights between pairs of nodes. We work with a model of the grouping process in which both sets of variables are drawn from a Bernoulli distribution. The main contributioin in this paper is to show how the cluster-memberships may be estimated using the leading eigenvector of the revised link-weight matrices. We also establish convergence conditions for the resulting pair-wise clustering process. The method is demonstrated on the problem of multiple moving object segmentation."'),
('"Pairwise Probabilistic Voting: Fast Place Recognition without RANSAC"', '"ECCV 2014"', '["Place Recognition", "Location Recognition", "Instance Recognition", "Image Retrieval", "Bag Of Wor', '"https://doi.org/10.1007/978-3-319-10605-2_33"', '"Place recognition currently suffers from a lack of scalability due to the need for strong geometric constraints, which as of yet are typically limited to RANSAC implementations. In this paper, we present a method to successfully achieve state-of-the-art performance, in both recognition accuracy and speed, without the need for RANSAC. We propose to discretise each feature pair in an image, in both appearance and 2D geometry, to create a triplet of words: one each for the appearance of the two features, and one for the pairwise geometry. This triplet is then passed through an inverted index to find examples of such pairwise configurations in the database. Finally, a global geometry constraint is enforced by considering the maximum-clique in an adjacency graph of pairwise correspondences. The discrete nature of the problem allows for tractable probabilistic scores to be assigned to each correspondence, and the least informative feature pairs can be eliminated from the database for memory and time efficiency. We demonstrate the performance of our method on several large-scale datasets, and show improvements over several baselines."'),
('"Pairwise Rotation Invariant Co-occurrence Local Binary Pattern"', '"ECCV 2012"', '["Local Binary Pattern", "Local Binary Pattern Feature", "Local Binary Pattern Operator", "Material ', '"https://doi.org/10.1007/978-3-642-33783-3_12"', '"In this work, we introduce a novel pairwise rotation invariant co-occurrence local binary pattern (PRI-CoLBP) feature which incorporates two types of context - spatial co-occurrence and orientation co-occurrence. Different from traditional rotation invariant local features, pairwise rotation invariant co-occurrence features preserve relative angle between the orientations of individual features. The relative angle depicts the local curvature information, which is discriminative and rotation invariant. Experimental results on the CUReT, Brodatz, KTH-TIPS texture dataset, Flickr Material dataset, and Oxford 102 Flower dataset further demonstrate the superior performance of the proposed feature on texture classification, material recognition and flower recognition tasks."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Palmprint Authentication System for Civil Applications"', '"BioAW 2004"', '["Gabor Filter", "Biometric System", "Recognition Module", "False Acceptance Rate", "Biometric Ident', '"https://doi.org/10.1007/978-3-540-25976-3_20"', '"In this paper, we present a novel biometric authentication system to identify a person\\u2019s identity by his/her palmprint. In contrast to existing palmprint systems for criminal applications, the proposed system targets at the civil applications, which require identifying a person in a large database with high confidence in real-time. The system is constituted by four major components: User Interface Module, Acquisition Module, Recognition Module and External Module. More than 7,000 palmprint images have been collected to test the performance of the system. The system can identify 400 palms with a low false acceptance rate, 0.02%, and a high genuine acceptance rate, 98.83%. For verification, the system can operate at a false acceptance rate, 0.017% and a false rejection rate, 0.86%. The execution time for the whole process including image collection, preprocessing, feature extraction and matching is less than 1 second."'),
('"PanoContext: A Whole-Room 3D Context Model for Panoramic Scene Understanding"', '"ECCV 2014"', '["Support Vector Machine", "Context Model", "Perspective Image", "Match Cost", "Indoor Scene"]', '"https://doi.org/10.1007/978-3-319-10599-4_43"', '"The field-of-view of standard cameras is very small, which is one of the main reasons that contextual information is not as useful as it should be for object detection. To overcome this limitation, we advocate the use of 360\\u00b0 full-view panoramas in scene understanding, and propose a whole-room context model in 3D. For an input panorama, our method outputs 3D bounding boxes of the room and all major objects inside, together with their semantic categories. Our method generates 3D hypotheses based on contextual constraints and ranks the hypotheses holistically, combining both bottom-up and top-down context information. To train our model, we construct an annotated panorama dataset and reconstruct the 3D model from single-view using manual annotation. Experiments show that solely based on 3D context without any image region category classifier, we can achieve a comparable performance with the state-of-the-art object detector. This demonstrates that when the FOV is large, context is as powerful as object appearance. All data and source code are available online."'),
('"Parallel Generalized Thresholding Scheme for Live Dense Geometry from a Handheld Camera"', '"ECCV 2010"', '["Input Image", "Data Term", "Graphic Hardware", "Total Variation Regularizer", "Dense Depth"]', '"https://doi.org/10.1007/978-3-642-35740-4_35"', '"Inspired by recent successes in parallelized optic flow estimation, we propose a variational method which allows to directly estimate dense depth fields from a single hand-held camera in real-time conditions. In particular we show how the central ingredient of the corresponding optic flow method, namely a thresholding scheme, can be generalized to the problem of geometric reconstruction considered in this paper and how it can be parallelized on recent graphics cards. We compare alternative parallelization strategies and experimentally validate that high-quality depth maps can be computed in a few milliseconds from a hand-held camera."'),
('"Parallel Variational Motion Estimation by Domain Decomposition and Cluster Computing"', '"ECCV 2004"', '["Motion Estimation", "Domain Decomposition", "Message Passing Interface", "Multigrid Method", "Doma', '"https://doi.org/10.1007/978-3-540-24673-2_17"', '"We present an approach to parallel variational optical flow computation on standard hardware by domain decomposition. Using an arbitrary partition of the image plane into rectangular subdomains, the global solution to the variational approach is obtained by iteratively combining local solutions which can be efficiently computed in parallel by separate multi-grid iterations for each subdomain. The approach is particularly suited for implementations on PC-clusters because inter-process communication between subdomains (i.e. processors) is minimized by restricting the exchange of data to a lower-dimensional interface. By applying a dedicated interface preconditioner, the necessary number of iterations between subdomains to achieve a fixed error is bounded independently of the number of subdomains. Our approach provides a major step towards real-time 2D image processing using off-the-shelf PC-hardware and facilitates the efficient application of variational approaches to large-scale image processing problems."'),
('"Parameter Estimates for a Pencil of Lines: Bounds and Estimators"', '"ECCV 2002"', '["Parameter Estimate", "Maximum Likelihood Estimator", "Line Data", "Fisher Information Matrix", "Li', '"https://doi.org/10.1007/3-540-47969-4_29"', '"Estimating the parameters of a pencil of lines is addressed. A statistical model for the measurements is developed, from which the Cramer Rao lower bound is determined. An estimator is derived, and its performance is simulated and compared to the bound. The estimator is shown to be asymptotically efficient, and superior to the classical least squares algorithm."'),
('"Parameterizing Object Detectors in the Continuous Pose Space"', '"ECCV 2014"', '["object detection", "continuous pose estimation"]', '"https://doi.org/10.1007/978-3-319-10593-2_30"', '"Object detection and pose estimation are interdependent problems in computer vision. Many past works decouple these problems, either by discretizing the continuous pose and training pose-specific object detectors, or by building pose estimators on top of detector outputs. In this paper, we propose a structured kernel machine approach to treat object detection and pose estimation jointly in a mutually benificial way. In our formulation, a unified, continuously parameterized, discriminative appearance model is learned over the entire pose space. We propose a cascaded discrete-continuous algorithm for efficient inference, and give effective online constraint generation strategies for learning our model using structural SVMs. On three standard benchmarks, our method performs better than, or on par with, state-of-the-art methods in the combined task of object detection and pose estimation."'),
('"Parametric and Non-parametric Methods for Linear Extraction"', '"SMVP 2004"', '["Edge Point", "Hough Transform", "Feature Orientation", "Variable Bandwidth", "Hough Space"]', '"https://doi.org/10.1007/978-3-540-30212-4_16"', '"This article presents two new approaches, one parametric and one non-parametric, to the linear grouping of image features. They are based on the Bayesian Hough Transform, which takes into account feature uncertainty. Our main contribution are two new ways to detect the most significant modes of the Hough Transform. Traditionally, this is done by non-maximum suppression. However, in truth, Hough bins measure the likelihoods not of single lines but of collection of lines. Therefore finding lines by non-maxima suppression is not appropriate. This article presents two alternatives. The first method uses bin integration, automatic pruning and fusion to perform mode detection. The second approach detects dominant modes using variable bandwidth mean shift. The advantages of these algorithms are that: (1) the uncertainties associated with feature measurements are taken into account during voting and mode estimation (2) dominant modes are detected in ways that are more correct and less sensitive to errors and biases than non-maxima suppression. The methods can be used with any feature type and any associated feature detection algorithm provided that it outputs a feature position, orientation and covariance matrices. Results illustrate the approaches."'),
('"Parametric Distributional Clustering for Image Segmentation"', '"ECCV 2002"', '["Image Segmentation", "Clustering", "Maximum Likelihood", "Information Theory"]', '"https://doi.org/10.1007/3-540-47977-5_38"', '"Unsupervised Image Segmentation is one of the central issues in Computer Vision. From the viewpoint of exploratory data analysis, segmentation can be formulated as a clustering problem in which pixels or small image patches are grouped together based on local feature information. In this contribution, parametrical distributional clustering (PDC) is presented as a novel approach to image segmentation. In contrast to noise sensitive point measurements, local distributions of image features provide a statistically robust description of the local image properties. The segmentation technique is formulated as a generative model in the maximum likelihood framework. Moreover, there exists an insightful connection to the novel information theoretic concept of the Information Bottleneck (Tishby et al. [17]), which emphasizes the compromise between efficient coding of an image and preservation of characteristic information in the measured feature distributions."'),
('"Parametric Manifold of an Object under Different Viewing Directions"', '"ECCV 2012"', '["pose manifold", "3D object", "in-depth rotations", "viewing directions", "appearance prediction", ', '"https://doi.org/10.1007/978-3-642-33715-4_14"', '"The appearance of a 3D object depends on both the viewing directions and illumination conditions. It is proven that all n-pixel images of a convex object with Lambertian surface under variable lighting from infinity form a convex polyhedral cone (called illumination cone) in n-dimensional space. This paper tries to answer the other half of the question: What is the set of images of an object under all viewing directions? A novel image representation is proposed, which transforms any n-pixel image of a 3D object to a vector in a 2n-dimensional pose space. In such a pose space, we prove that the transformed images of a 3D object under all viewing directions form a parametric manifold in a 6-dimensional linear subspace. With in-depth rotations along a single axis in particular, this manifold is an ellipse. Furthermore, we show that this parametric pose manifold of a convex object can be estimated from a few images in different poses and used to predict object\\u2019s appearances under unseen viewing directions. These results immediately suggest a number of approaches to object recognition, scene detection, and 3D modelling. Experiments on both synthetic data and real images were reported, which demonstrates the validity of the proposed representation."'),
('"Parametric View-Synthesis"', '"ECCV 2000"', '["Basis Image", "Camera Parameter", "Basis View", "Image Interpolation", "View Synthesis"]', '"https://doi.org/10.1007/3-540-45054-8_13"', '"We present a simple procedure for synthesising novel views, using two or more basis-images as input. It is possible for the user to interactively adjust the viewpoint, and for the corresponding image to be computed and rendered in real-time. Rather than employing a 3D model, our method is based on the linear relations which exist between images taken with an affine camera. We show how the \\u2018combination of views\\u2019 proposed by Ullman and Basri [19] can be appropriately parameterised when a sequence of five or more images is available. This is achieved by fitting polynomial models to the coefficients of the combination, where the latter are functions of the (unknown) camera parameters. We discuss an alternative approach, direct image-interpolation, and argue that our method is preferable when there is a large difference in orientation between the original gaze directions. We show the results of applying the parameterisation to a fixating camera, using both simulated and real input. Our observations are relevant to several applications, including visualisation, animation, and low-bandwidth communication."'),
('"Parsing Images into Region and Curve Processes"', '"ECCV 2002"', '["Markov Chain", "Hide Markov Model", "Curve Model", "Markov Chain Monte Carlo Method", "Reversible ', '"https://doi.org/10.1007/3-540-47977-5_26"', '"Natural scenes consist of a wide variety of stochastic patterns. While many patterns are represented well by statistical models in two dimensional regions as most image segmentation work assume, some other patterns are fundamentally one dimensional and thus cause major problems in segmentation. We call the former region processes and the latter curve processes. In this paper, we propose a stochastic algorithm for parsing an image into a number of region and curve processes. The paper makes the following contributions to the literature. Firstly, it presents a generative rope model for curve processes in the form of Hidden Markov Model (HMM). The hidden layer is a Markov chain with each element being an image base selected from an over-complete basis, such as Difference of Gaussians (DOG) or Difference of Offset Gaussians (DOOG) at various scales and orientations. The rope model accounts for the geometric smoothness and photometric coherence of the curve processes. Secondly, it integrates both 2D region models, such as textures, splines etc with 1D curve models under the Bayes framework. Because both region and curve models are generative, they compete to explain input images in a layered representation. Thirdly, it achieves global optimization by effective Markov chain Monte Carlo methods in the sense of maximizing a posterior probability. The Markov chain consists of reversible jumps and diffusions driven by bottom up information. The algorithm is applied to real images with satisfactory results. We verify the results through random synthesis and compare them against segmentations with region processes only."'),
('"Part Bricolage: Flow-Assisted Part-Based Graphs for Detecting Activities in Videos"', '"ECCV 2014"', '["Activity Understanding", "Pose Estimation", "Graph Structures"]', '"https://doi.org/10.1007/978-3-319-10599-4_38"', '"Space-time detection of human activities in videos can significantly enhance visual search. To handle such tasks, while solely using low-level features has been found somewhat insufficient for complex datasets; mid-level features (like body parts) that are normally considered, are not robustly accounted for their inaccuracy. Moreover, the activity detection mechanisms do not constructively utilize the importance and trustworthiness of the features."'),
('"Part-Based Feature Synthesis for Human Detection"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_10"', '"We introduce a new approach for learning part-based object detection through feature synthesis. Our method consists of an iterative process of feature generation and pruning. A feature generation procedure is presented in which basic part-based features are developed into a feature hierarchy using operators for part localization, part refining and part combination. Feature pruning is done using a new feature selection algorithm for linear SVM, termed Predictive Feature Selection (PFS), which is governed by weight prediction. The algorithm makes it possible to choose from O(106) features in an efficient but accurate manner. We analyze the validity and behavior of PFS and empirically demonstrate its speed and accuracy advantages over relevant competitors. We present an empirical evaluation of our method on three human detection datasets including the current de-facto benchmarks (the INRIA and Caltech pedestrian datasets) and a new challenging dataset of children images in difficult poses. The evaluation suggests that our approach is on a par with the best current methods and advances the state-of-the-art on the Caltech pedestrian training dataset."'),
('"Part-Based R-CNNs for Fine-Grained Category Detection"', '"ECCV 2014"', '["Fine-grained recognition", "object detection", "convolutional models"]', '"https://doi.org/10.1007/978-3-319-10590-1_54"', '"Semantic part localization can facilitate fine-grained categorization by explicitly isolating subtle appearance differences associated with specific object parts. Methods for pose-normalized representations have been proposed, but generally presume bounding box annotations at test time due to the difficulty of object detection. We propose a model for fine-grained categorization that overcomes these limitations by leveraging deep convolutional features computed on bottom-up region proposals. Our method learns whole-object and part detectors, enforces learned geometric constraints between them, and predicts a fine-grained category from a pose-normalized representation. Experiments on the Caltech-UCSD bird dataset confirm that our method outperforms state-of-the-art fine-grained categorization methods in an end-to-end evaluation without requiring a bounding box at test time."'),
('"Part-Pair Representation for Part Localization"', '"ECCV 2014"', '["part localization", "part-pair representation", "pose estimation"]', '"https://doi.org/10.1007/978-3-319-10605-2_30"', '"In this paper, we propose a novel part-pair representation for part localization. In this representation, an object is treated as a collection of part pairs to model its shape and appearance. By changing the set of pairs to be used, we are able to impose either stronger or weaker geometric constraints on the part configuration. As for the appearance, we build pair detectors for each part pair, which model the appearance of an object at different levels of granularities. Our method of part localization exploits the part-pair representation, featuring the combination of non-parametric exemplars and parametric regression models. Non-parametric exemplars help generate reliable part hypotheses from very noisy pair detections. Then, the regression models are used to group the part hypotheses in a flexible way to predict the part locations. We evaluate our method extensively on the dataset CUB-200-2011 [32], where we achieve significant improvement over the state-of-the-art method on bird part localization. We also experiment with human pose estimation, where our method produces comparable results to existing works."'),
('"Partial Difference Equations over Graphs: Morphological Processing of Arbitrary Discrete Data"', '"ECCV 2008"', '["Weight Function", "Texture Image", "Weighted Graph", "Mathematical Morphology", "Graph Topology"]', '"https://doi.org/10.1007/978-3-540-88690-7_50"', '"Mathematical Morphology (MM) offers a wide range of operators to address various image processing problems. These processing can be defined in terms of algebraic set or as partial differential equations (PDEs). In this paper, a novel approach is formalized as a framework of partial difference equations (PdEs) on weighted graphs. We introduce and analyze morphological operators in local and nonlocal configurations. Our framework recovers classical local algebraic and PDEs-based morphological methods in image processing context; generalizes them for nonlocal configurations and extends them to the treatment of any arbitrary discrete data that can be represented by a graph. It leads to considering a new field of application of MM processing: the case of high-dimensional multivariate unorganized data."'),
('"Partial Object Matching with Shapeme Histograms"', '"ECCV 2004"', '["Model Object", "Range Image", "Spin Image", "Query Object", "Discrimination Capability"]', '"https://doi.org/10.1007/978-3-540-24672-5_35"', '"Histogram of shape signature or prototypical shapes, called shapemes, have been used effectively in previous work for 2D/3D shape matching & recognition. We extend the idea of shapeme histogram to recognize partially observed query objects from a database of complete model objects. We propose to represent each model object as a collection of shapeme histograms, and match the query histogram to this representation in two steps: (i) compute a constrained projection of the query histogram onto the subspace spanned by all the shapeme histograms of the model, and (ii) compute a match measure between the query histogram and the projection. The first step is formulated as a constrained optimization problem that is solved by a sampling algorithm. The second step is formulated under a Bayesian framework where an implicit feature selection process is conducted to improve the discrimination capability of shapeme histograms. Results of matching partially viewed range objects with a 243 model database demonstrate better performance than the original shapeme histogram matching algorithm and other approaches."'),
('"Partition Min-Hash for Partial Duplicate Image Discovery"', '"ECCV 2010"', '["Hash Function", "Image Retrieval", "Visual Word", "Hash Table", "Query Image"]', '"https://doi.org/10.1007/978-3-642-15549-9_47"', '"In this paper, we propose Partition min-Hash (PmH), a novel hashing scheme for discovering partial duplicate images from a large database. Unlike the standard min-Hash algorithm that assumes a bag of words image representation, our approach utilizes the fact that duplicate regions among images are often localized. By theoretical analysis, simulation, and empirical study, we show that PmH outperforms standard min-Hash in terms of precision and recall, while being orders of magnitude faster. When combined with the start-of-the-art Geometric min-Hash algorithm, our approach speeds up hashing by 10 times without losing precision or recall. When given a fixed time budget, our method achieves much higher recall than the state-of-the-art."'),
('"Partitioned Sampling, Articulated Objects, and Interface-Quality Hand Tracking"', '"ECCV 2000"', '["Particle Filter", "Configuration Space", "Tracking Problem", "Posterior Probability Distribution",', '"https://doi.org/10.1007/3-540-45053-X_1"', '"Partitioned sampling is a technique which was introduced in [I7] for avoiding the high cost of particle filters when tracking more than one object. In fact this technique can reduce the curse of dimensionality in other situations too. This paper describes how to use partitioned sampling on articulated objects, obtaining results that would be impossible with standard sampling methods. Because partitioned sampling is the statistical analogue of a hierarchical search, it makes sense to use it on articulated objects, since links at the base of the object can be localised before moving on to search for subsequent links."'),
('"Passive Reflectometry"', '"ECCV 2008"', '["Input Image", "Synthetic Image", "Bilateral Symmetry", "Grazing Angle", "Light Probe"]', '"https://doi.org/10.1007/978-3-540-88693-8_63"', '"Different materials reflect light in different ways, so reflectance is a useful surface descriptor. Existing systems for measuring reflectance are cumbersome, however, and although the process can be streamlined using cameras, projectors and clever catadioptrics, it generally requires complex infrastructure. In this paper we propose a simpler method for inferring reflectance from images, one that eliminates the need for active lighting and exploits natural illumination instead. The method\\u2019s distinguishing property is its ability to handle a broad class of isotropic reflectance functions, including those that are neither radially-symmetric nor well-represented by low-parameter reflectance models. The key to the approach is a bi-variate representation of isotropic reflectance that enables a tractable inference algorithm while maintaining generality. The resulting method requires only a camera, a light probe, and as little as one HDR image of a known, curved, homogeneous surface."'),
('"Passive Tomography of Turbulence Strength"', '"ECCV 2014"', '["Normalize Root Mean Square Error", "Phase Screen", "Background Oriented Schlieren", "Underwater Im', '"https://doi.org/10.1007/978-3-319-10593-2_4"', '"Turbulence is studied extensively in remote sensing, astronomy, meteorology, aerodynamics and fluid dynamics. The strength of turbulence is a statistical measure of local variations in the turbulent medium. It influences engineering decisions made in these domains. Turbulence strength (TS) also affects safety of aircraft and tethered balloons, and reliability of free-space electromagnetic relays. We show that it is possible to estimate TS, without having to reconstruct instantaneous fluid flow fields. Instead, the TS field can be directly recovered, passively, using videos captured from different viewpoints. We formulate this as a linear tomography problem with a structure unique to turbulence fields. No tight synchronization between cameras is needed. Thus, realization is very simple to deploy using consumer-grade cameras. We experimentally demonstrate this both in a lab and in a large-scale uncontrolled complex outdoor environment, which includes industrial, rural and urban areas."'),
('"Patch Based Synthesis for Single Depth Image Super-Resolution"', '"ECCV 2012"', '["Depth Image", "Depth Discontinuity", "Pairwise Term", "Input Patch", "High Resolution Patch"]', '"https://doi.org/10.1007/978-3-642-33712-3_6"', '"We present an algorithm to synthetically increase the resolution of a solitary depth image using only a generic database of local patches. Modern range sensors measure depths with non-Gaussian noise and at lower starting resolutions than typical visible-light cameras. While patch based approaches for upsampling intensity images continue to improve, this is the first exploration of patching for depth images."'),
('"Patch Complexity, Finite Pixel Correlations and Optimal Denoising"', '"ECCV 2012"', '["Window Size", "Patch Size", "Scale Invariance", "Minimum Mean Square Error", "Natural Image"]', '"https://doi.org/10.1007/978-3-642-33715-4_6"', '"Image restoration tasks are ill-posed problems, typically solved with priors. Since the optimal prior is the exact unknown density of natural images, actual priors are only approximate and typically restricted to small patches. This raises several questions: How much may we hope to improve current restoration results with future sophisticated algorithms? And more fundamentally, even with perfect knowledge of natural image statistics, what is the inherent ambiguity of the problem? In addition, since most current methods are limited to finite support patches or kernels, what is the relation between the patch complexity of natural images, patch size, and restoration errors? Focusing on image denoising, we make several contributions. First, in light of computational constraints, we study the relation between denoising gain and sample size requirements in a non parametric approach. We present a law of diminishing return, namely that with increasing patch size, rare patches not only require a much larger dataset, but also gain little from it. This result suggests novel adaptive variable-sized patch schemes for denoising. Second, we study absolute denoising limits, regardless of the algorithm used, and the converge rate to them as a function of patch size. Scale invariance of natural images plays a key role here and implies both a strictly positive lower bound on denoising and a power law convergence. Extrapolating this parametric law gives a ballpark estimate of the best achievable denoising, suggesting that some improvement, although modest, is still possible."'),
('"Patch-Based Texture Edges and Segmentation"', '"ECCV 2006"', '["Deformable Model", "Active Contour Model", "Texture Synthesis", "Texture Segmentation", "Free Form', '"https://doi.org/10.1007/11744047_37"', '"A novel technique for extracting texture edges is introduced. It is based on the combination of two ideas: the patch-based approach, and non-parametric tests of distributions."'),
('"PatchMatchGraph: Building a Graph of Dense Patch Correspondences for Label Transfer"', '"ECCV 2012"', '["Good Match", "Target Image", "Search Move", "Adjacent Patch", "CIELab Colour Space"]', '"https://doi.org/10.1007/978-3-642-33715-4_32"', '"We address the problem of semantic segmentation, or multi-class pixel labeling, by constructing a graph of dense overlapping patch correspondences across large image sets. We then transfer annotations from labeled images to unlabeled images using the established patch correspondences. Unlike previous approaches to non-parametric label transfer our approach does not require an initial image retrieval step. Moreover, we operate on a graph for computing mappings between images, which avoids the need for exhaustive pairwise comparisons. Consequently, we can leverage offline computation to enhance performance at test time. We conduct extensive experiments to analyze different variants of our graph construction algorithm and evaluate multi-class pixel labeling performance on several challenging datasets."'),
('"Pedestrian Detection from a Moving Vehicle"', '"ECCV 2000"', '["Radial Basis Function", "Candidate Solution", "Object Detection", "Leaf Level", "Pedestrian Detect', '"https://doi.org/10.1007/3-540-45053-X_3"', '"This paper presents a prototype system for pedestrian detection on-board a moving vehicle. The system uses a generic two-step approach for efficient object detection. In the first step, contour features are used in a hierarchical template matching approach to efficiently \\u201clock\\u201d onto candidate solutions. Shape matching is based on Distance Transforms. By capturing the objects shape variability by means of a template hierarchy and using a combined coarse-to-fine approach in shape and parameter space, this method achieves very large speed-ups compared to a brute-force method. We have measured gains of several orders of magnitude. The second step utilizes the richer set of intensity features in a pattern classification approach to verify the candidate solutions (i.e. using Radial Basis Functions). We present experimental results on pedestrian detection off-line and on-board our Urban Traffic Assistant vehicle and discuss the challenges that lie ahead."'),
('"People Orientation Recognition by Mixtures of Wrapped Distributions on Random Trees"', '"ECCV 2012"', '["Orientation recognition", "Mixtures of Wrapped Distributions", "Random Trees"]', '"https://doi.org/10.1007/978-3-642-33715-4_20"', '"The recognition of people orientation in single images is still an open issue in several real cases, when the image resolution is poor, body parts cannot be distinguished and localized or motion cannot be exploited. However, the estimation of a person orientation, even an approximated one, could be very useful to improve people tracking and re-identification systems, or to provide a coarse alignment of body models on the input images. In these situations, holistic features seem to be more effective and faster than model based 3D reconstructions. In this paper we propose to describe the people appearance with multi-level HoG feature sets and to classify their orientation using an array of Extremely Randomized Trees classifiers trained on quantized directions. The outputs of the classifiers are then integrated into a global continuous probability density function using a Mixture of Approximated Wrapped Gaussian distributions. Experiments on the TUD Multiview Pedestrians, the Sarc3D, and the 3DPeS datasets confirm the efficacy of the method and the improvement with respect to state of the art approaches."'),
('"People Watching: Human Actions as a Cue for Single View Geometry"', '"ECCV 2012"', '["Single Image", "Functional Region", "Indoor Scene", "Scene Geometry", "People Detection"]', '"https://doi.org/10.1007/978-3-642-33715-4_53"', '"We present an approach which exploits the coupling between human actions and scene geometry. We investigate the use of human pose as a cue for single-view 3D scene understanding. Our method builds upon recent advances in still-image pose estimation to extract functional and geometric constraints about the scene. These constraints are then used to improve state-of-the-art single-view 3D scene understanding approaches. The proposed method is validated on a collection of monocular time-lapse sequences collected from YouTube and a dataset of still images of indoor scenes. We demonstrate that observing people performing different actions can significantly improve estimates of 3D scene geometry."'),
('"Per-patch Descriptor Selection Using Surface and Scene Properties"', '"ECCV 2012"', '["Average Precision", "Image Patch", "Image Descriptor", "Disturbance Level", "Descriptor Selection"', '"https://doi.org/10.1007/978-3-642-33783-3_13"', '"Local image descriptors are generally designed for describing all possible image patches. Such patches may be subject to complex variations in appearance due to incidental object, scene and recording conditions. Because of this, a single-best descriptor for accurate image representation under all conditions does not exist. Therefore, we propose to automatically select from a pool of descriptors the one that is best suitable based on object surface and scene properties. These properties are measured on the fly from a single image patch through a set of attributes. Attributes are input to a classifier which selects the best descriptor. Our experiments on a large dataset of colored object patches show that the proposed selection method outperforms the best single descriptor and a-priori combinations of the descriptor pool."'),
('"Perceptual Grouping from Motion Cues Using Tensor Voting in 4-D"', '"ECCV 2002"', '["Optical Flow", "Motion Capture", "Motion Boundary", "Perceptual Group", "Candidate Match"]', '"https://doi.org/10.1007/3-540-47977-5_28"', '"We present a novel approach for motion grouping from two frames, that recovers the dense velocity field, motion boundaries and regions, based on a 4-D Tensor Voting computational framework. Given two sparse sets of point tokens, we encode the image position and potential velocity for each token into a 4-D tensor. The voting process then enforces the motion smoothness while preserving motion discontinuities, thus selecting the correct velocity for each input point, as the most salient token. By performing an additional dense voting step we infer velocities at every pixel location, motion boundaries and regions. Using a 4-D space for this Tensor Voting approach is essential, since it allows for a spatial separation of the points according to both their velocities and image coordinates. Unlike other methods that optimize a specific objective function, our approach does not involve initialization or search in a parametric space, and therefore does not suffer from local optima or poor convergence problems. We demonstrate our method with synthetic and real images, by analyzing several difficult cases \\u2014 opaque and transparent motion, rigid and non-rigid motion, curves and surfaces in motion."'),
('"Perceptual Narratives of Space and Motion for Semantic Interpretation of Visual Data"', '"ECCV 2014"', '["Logic Programming", "Spatial Change", "Semantic Interpretation", "Dynamic Scene", "Twilight Zone"]', '"https://doi.org/10.1007/978-3-319-16181-5_24"', '"We propose a commonsense theory of space and motion for the high-level semantic interpretation of dynamic scenes. The theory provides primitives for commonsense representation and reasoning with qualitative spatial relations, depth profiles, and spatio-temporal change; these may be combined with probabilistic methods for modelling and hypothesising event and object relations. The proposed framework has been implemented as a general activity abstraction and reasoning engine, which we demonstrate by generating declaratively grounded visuo-spatial narratives of perceptual input from vision and depth sensors for a benchmark scenario."'),
('"Perceptually Inspired Layout-Aware Losses for Image Segmentation"', '"ECCV 2014"', '["structured prediction", "image segmentation", "loss-based learning", "max-margin learning", "perce', '"https://doi.org/10.1007/978-3-319-10605-2_43"', '"Interactive image segmentation is an important computer vision problem that has numerous real world applications. Models for image segmentation are generally trained to minimize the Hamming error in pixel labeling. The Hamming loss does not ensure that the topology/structure of the object being segmented is preserved and therefore is not a strong indicator of the quality of the segmentation as perceived by users. However, it is still ubiquitously used for training models because it decomposes over pixels and thus enables efficient learning. In this paper, we propose the use of a novel family of higher-order loss functions that encourage segmentations whose layout is similar to the ground-truth segmentation. Unlike the Hamming loss, these loss functions do not decompose over pixels and therefore cannot be directly used for loss-augmented inference. We show how our loss functions can be transformed to allow efficient learning and demonstrate the effectiveness of our method on a challenging segmentation dataset and validate the results using a user study. Our experimental results reveal that training with our layout-aware loss functions results in better segmentations that are preferred by users over segmentations obtained using conventional loss functions."'),
('"Performance Capture of Interacting Characters with Handheld Kinects"', '"ECCV 2012"', '["Point Cloud", "Ground Plane", "Motion Capture", "Sift Feature", "Model Vertex"]', '"https://doi.org/10.1007/978-3-642-33709-3_59"', '"We present an algorithm for marker-less performance capture of interacting humans using only three hand-held Kinect cameras. Our method reconstructs human skeletal poses, deforming surface geometry and camera poses for every time step of the depth video. Skeletal configurations and camera poses are found by solving a joint energy minimization problem which optimizes the alignment of RGBZ data from all cameras, as well as the alignment of human shape templates to the Kinect data. The energy function is based on a combination of geometric correspondence finding, implicit scene segmentation, and correspondence finding using image features. Only the combination of geometric and photometric correspondences and the integration of human pose and camera pose estimation enables reliable performance capture with only three sensors. As opposed to previous performance capture methods, our algorithm succeeds on general uncontrolled indoor scenes with potentially dynamic background, and it succeeds even if the cameras are moving."'),
('"Periocular Recognition Using Retinotopic Sampling and Gabor Decomposition"', '"ECCV 2012"', '["Biometrics", "periocular", "eye", "iris", "Log-Polar mapping", "Gabor decomposition"]', '"https://doi.org/10.1007/978-3-642-33868-7_31"', '"We present a new system for biometric recognition using periocular images based on retinotopic sampling grids and Gabor analysis of the local power spectrum. A number of aspects are studied, including: 1) grid adaptation to dimensions of the target eye vs. grids of constant size, 2) comparison between circular- and rectangular-shaped grids, 3) use of Gabor magnitude vs. phase vectors for recognition, 4) rotation compensation between query and test images, and 5) comparison with an iris machine expert. Results show that our system achieves competitive verification rates compared with other periocular recognition approaches. We also show that top verification rates can be obtained without rotation compensation, thus allowing to remove this step for computational efficiency. Also, the performance is not affected substantially if we use a grid of fixed dimensions, or it is even better in certain situations, avoiding the need of accurate detection of the iris region."'),
('"Person Identification in Natural Static Postures Using Kinect"', '"ECCV 2014"', '["Person identification", "Natural static posture", "Skeleton joints", "Kinect"]', '"https://doi.org/10.1007/978-3-319-16181-5_60"', '"Automatic person identification using un-obtrusive methods are of immense importance in the area of computer vision. Anthropometric approaches are robust to external factors including environmental illumination and obstructions due to hair, spectacles, hats or any other wearable. Recently, there have been efforts made on people identification using walking pattern of the skeleton data obtained from Kinect. In this paper we investigate the possibility of identification using static postures namely sitting and standing. Existing gait based identifications, mostly rely on the dynamics of the joints of the skeleton data. In case of static postures the motion information is not available, hence the identification mainly relies on the static distance information between the joints. Moreover, the variation of pose in a particular posture makes the identification more challenging. The proposed methodology, initially sub-divides the body-parts into static, dynamic and noisy parts followed by a combinatorial element responsible for selectively extracting features for each of those parts. Finally a radial basis function support vector machine classifier is used to perform the training and testing for the identification. Results indicate an identification accuracy of more than 97 % in terms of F-score for 10 people using a dataset created with various poses of natural sitting and standing posture."'),
('"Person Identification Using Full-Body Motion and Anthropometric Biometrics from Kinect Videos"', '"ECCV 2012"', '["Receiver Operating Characteristic Curve", "Machine Intelligence", "Gait Cycle", "Depth Image", "Eq', '"https://doi.org/10.1007/978-3-642-33885-4_10"', '"For person identification, motion and anthropometric biometrics are known to be less sensitive to photometric differences and more robust to obstructions such as glasses, hair, and hats. Existing gait-based methods are based on the accurate identification and acquisition of the gait cycle. This typically requires the subject to repeatedly perform a single action using a costly motion-capture facility, or 2D videos in simple backgrounds where the person can be easily segmented and tracked. For person identification these manufactured requirements limit the use gait-based biometrics in real scenarios that may have a variety of actions with varying levels of complexity. We propose a new person identification method that uses motion and anthropometric biometrics acquired from an inexpensive Kinect RGBD sensor. Different from previous gait-based methods we use all the body joints found by the Kinect SDK to analyze the motion patterns and anthropometric features over the entire track sequence. We show the proposed method can identify people that perform different actions (e.g. walk and run) with varying levels of complexity. When compared to a state-of-the-art gait-based method that uses depth images produced by the Kinect sensor the proposed method demonstrated better person identity performance."'),
('"Person Re-identification by Discriminatively Selecting Parts and Features"', '"ECCV 2014"', '["Pedestrian re-identification", "STEL segmentation", "Lasso regression"]', '"https://doi.org/10.1007/978-3-319-16199-0_11"', '"This paper presents a novel appearance-based method for person re-identification. The core idea is to rank and select different body parts on the basis of the discriminating power of their characteristic features. In our approach, we first segment the pedestrian images into meaningful parts, then we extract features from such parts as well as from the whole body and finally, we perform a salience analysis based on regression coefficients. Given a set of individuals, our method is able to estimate the different importance (or salience) of each body part automatically. To prove the effectiveness of our approach, we considered two standard datasets and we demonstrated through an exhaustive experimental section how our method improves significantly upon existing approaches, especially in multiple-shot scenarios."'),
('"Person Re-identification by Video Ranking"', '"ECCV 2014"', '["Image Sequence", "Action Recognition", "Dynamic Time Warping", "Gait Recognition", "Video Fragment', '"https://doi.org/10.1007/978-3-319-10593-2_45"', '"Current person re-identification (re-id) methods typically rely on single-frame imagery features, and ignore space-time information from image sequences. Single-frame (single-shot) visual appearance matching is inherently limited for person re-id in public spaces due to visual ambiguity arising from non-overlapping camera views where viewpoint and lighting changes can cause significant appearance variation. In this work, we present a novel model to automatically select the most discriminative video fragments from noisy image sequences of people where more reliable space-time features can be extracted, whilst simultaneously to learn a video ranking function for person re-id. Also, we introduce a new image sequence re-id dataset (iLIDS-VID) based on the i-LIDS MCT benchmark data. Using the iLIDS-VID and PRID 2011 sequence re-id datasets, we extensively conducted comparative evaluations to demonstrate the advantages of the proposed model over contemporary gait recognition, holistic image sequence matching and state-of-the-art single-shot/multi-shot based re-id methods."'),
('"Person Re-Identification Using Kernel-Based Metric Learning Methods"', '"ECCV 2014"', '["Feature Vector", "Ranking Algorithm", "Camera Network", "Hinge Loss", "Scatter Matrice"]', '"https://doi.org/10.1007/978-3-319-10584-0_1"', '"Re-identification of individuals across camera networks with limited or no overlapping fields of view remains challenging in spite of significant research efforts. In this paper, we propose the use, and extensively evaluate the performance, of four alternatives for re-ID classification: regularized Pairwise Constrained Component Analysis, kernel Local Fisher Discriminant Analysis, Marginal Fisher Analysis and a ranking ensemble voting scheme, used in conjunction with different sizes of sets of histogram-based features and linear, \\u03c7 2 and RBF-\\u03c7 2 kernels. Comparisons against the state-of-art show significant improvements in performance measured both in terms of Cumulative Match Characteristic curves (CMC) and Proportion of Uncertainty Removed (PUR) scores on the challenging VIPeR, iLIDS, CAVIAR and 3DPeS datasets."'),
('"Person Re-identification: What Features Are Important?"', '"ECCV 2012"', '["Random Forest", "Gabor Feature", "Pairwise Constraint", "Unsupervised Approach", "Feature Importan', '"https://doi.org/10.1007/978-3-642-33863-2_39"', '"State-of-the-art person re-identification methods seek robust person matching through combining various feature types. Often, these features are implicitly assigned with a single vector of global weights, which are assumed to be universally good for all individuals, independent to their different appearances. In this study, we show that certain features play more important role than others under different circumstances. Consequently, we propose a novel unsupervised approach for learning a bottom-up feature importance, so features extracted from different individuals are weighted adaptively driven by their unique and inherent appearance attributes. Extensive experiments on two public datasets demonstrate that attribute-sensitive feature importance facilitates more accurate person matching when it is fused together with global weights obtained using existing methods."'),
('"Personal Shopping Assistance and Navigator System for Visually Impaired People"', '"ECCV 2014"', '["Assistive technology", "Indoor navigation", "Visually impaired", "Augmented reality", "Mobile devi', '"https://doi.org/10.1007/978-3-319-16199-0_27"', '"In this paper, a personal assistant and navigator system for visually impaired people will be described. The showcase presented intends to demonstrate how partially sighted people could be aided by the technology in performing an ordinary activity, like going to a mall and moving inside it to find a specific product. We propose an Android application that integrates Pedestrian Dead Reckoning and Computer Vision algorithms, using an off-the-shelf Smartphone connected to a Smartwatch. The detection, recognition and pose estimation of specific objects or features in the scene derive an estimate of user location with sub-meter accuracy when combined with a hardware-sensor pedometer. The proposed prototype interfaces with a user by means of Augmented Reality, exploring a variety of sensorial modalities other than just visual overlay, namely audio and haptic modalities, to create a seamless immersive user experience. The interface and interaction of the preliminary platform have been studied through specific evaluation methods. The feedback gathered will be taken into consideration to further improve the proposed system."'),
('"Perspective Imaging under Structured Light"', '"ECCV 2010"', '["Spatial Frequency", "Periodic Pattern", "Depth Estimation", "Epipolar Line", "Perspective Image"]', '"https://doi.org/10.1007/978-3-642-15567-3_30"', '"Traditionally, \\u201cStructured Light\\u201d has been used to recover surface topology and estimate depth maps. A more recent development is the use of \\u201cStructured Light\\u201d in surpassing the fundamental limit on spatial resolution imposed by diffraction. But, its use in surpassing the diffraction limit remains confined to microscopy, due to issues that arise in macroscopic imaging: perspective foreshortening, aliasing and need for calibration. Also, no formal attempt has been made to unify the above embodiments, despite their common reliance on \\u201cStructured Light\\u201d."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Perspective n-View Multibody Structure-and-Motion Through Model Selection"', '"ECCV 2006"', '["Consecutive Frame", "Feature Track", "Dynamic Scene", "Temporal Consistency", "Motion Segmentation', '"https://doi.org/10.1007/11744023_47"', '"Multi-body structure-and-motion (MSaM) is the problem to establish the multiple-view geometry of an image sequence of a 3D scene, where the scene consists of multiple rigid objects moving relative to each other. So far, solutions have been proposed for several restricted settings, such as only two views, affine projection, and perspective projection of linearly moving points. We give a solution for sequences of several images, full perspective projection, and general rigid motion. It can deal with the fact that the set of correspondences changes over time, and is robust to outliers. The proposed solution is based on Monte-Carlo sampling and clustering of two-view motions, linking them through the sequence, and model selection to yield the best explanation for the entire sequence."'),
('"Perspective Nonrigid Shape and Motion Recovery"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_22"', '"We present a closed form solution to the nonrigid shape and motion (NRSM) problem from point correspondences in multiple perspective uncalibrated views. Under the assumption that the nonrigid object deforms as a linear combination of K rigid shapes, we show that the NRSM problem can be viewed as a reconstruction problem from multiple projections from \\u21193K to \\u21192. Therefore, one can linearly solve for the projection matrices by factorizing a multifocal tensor. However, this projective reconstruction in \\u21193K does not satisfy the constraints of the NRSM problem, because it is computed only up to a projective transformation in \\u21193K . Our key contribution is to show that, by exploiting algebraic dependencies among the entries of the projection matrices, one can upgrade the projective reconstruction to determine the affine configuration of the points in \\u211d3, and the motion of the camera relative to their centroid. Moreover, if K\\u2009\\u2265\\u20092, then either by using calibrated cameras, or by assuming a camera with fixed internal parameters, it is possible to compute the Euclidean structure by a closed form method."'),
('"Phase-Based Local Features"', '"ECCV 2002"', '["Image features", "Object recognition", "Vision systems engineering and evaluation", "Invariant loc', '"https://doi.org/10.1007/3-540-47969-4_19"', '"We introduce a new type of local feature based on the phase and amplitude responses of complex-valued steerable filters. The design of this local feature is motivated by a desire to obtain feature vectors which are semi-invariant under common image deformations, yet distinctive enough to provide useful identity information. A recent proposal for such local features involves combining differential invariants to particular image deformations, such as rotation. Our approach differs in that we consider a wider class of image deformations, including the addition of noise, along with both global and local brightness variations. We use steerable filters to make the feature robust to rotation. And we exploit the fact that phase data is often locally stable with respect to scale changes, noise, and common brightness changes. We provide empirical results comparing our local feature with one based on differential invariants. The results show that our phase-based local feature leads to better performance when dealing with common illumination changes and 2-D rotation, while giving comparable effects in terms of scale changes."'),
('"PHOG-Derived Aesthetic Measures Applied to Color Photographs of Artworks, Natural Scenes and Object', '"ECCV 2012"', '["Aesthetic", "art", "self-similarity", "complexity", "anisotropy", "Birkhoff-like measure", "Pyrami', '"https://doi.org/10.1007/978-3-642-33863-2_54"', '"Previous research in computational aesthetics has led to the identification of multiple image features that, in combination, can be related to the aesthetic quality of images, such as photographs. Moreover, it has been shown that aesthetic artworks possess specific higher-order statistical properties, such as a scale-invariant Fourier spectrum, that can be linked to coding mechanisms in the human visual system. In the present work, we derive novel measures based on a PHOG representation of images for image properties that have been studied in the context of the aesthetic assessment of images previously. We demonstrate that a large dataset of colored aesthetic paintings of Western provenance is characterized by a specific combination of the PHOG-derived aesthetic measures (high self-similarity, moderate complexity and low anisotropy). In this combination, the artworks differ significantly from seven other datasets of photographs that depict various types of natural and man-made scenes, patterns and objects. To the best of our knowledge, this is the first time that these features have been derived and evaluated on a large dataset of different image categories."'),
('"Photo and Video Quality Evaluation: Focusing on the Subject"', '"ECCV 2008"', '["Video Quality", "Image Quality Assessment", "Color Combination", "Vertical Derivative", "Video Qua', '"https://doi.org/10.1007/978-3-540-88690-7_29"', '"Traditionally, distinguishing between high quality professional photos and low quality amateurish photos is a human task. To automatically assess the quality of a photo that is consistent with humans perception is a challenging topic in computer vision. Various differences exist between photos taken by professionals and amateurs because of the use of photography techniques. Previous methods mainly use features extracted from the entire image. In this paper, based on professional photography techniques, we first extract the subject region from a photo, and then formulate a number of high-level semantic features based on this subject and background division. We test our features on a large and diverse photo database, and compare our method with the state of the art. Our method performs significantly better with a classification rate of 93% versus 72% by the best existing method. In addition, we conduct the first study on high-level video quality assessment. Our system achieves a precision of over 95% in a reasonable recall rate for both photo and video assessments. We also show excellent application results in web image search re-ranking."'),
('"Photo Sequencing"', '"ECCV 2012"', '["Dynamic Feature", "Reference Image", "Temporal Order", "Hamiltonian Path", "Dynamic Scene"]', '"https://doi.org/10.1007/978-3-642-33783-3_47"', '"Dynamic events such as family gatherings, concerts or sports events are often captured by a group of people. The set of still images obtained this way is rich in dynamic content but lacks accurate temporal information. We propose a method for photo-sequencing \\u2013 temporally ordering a set of still images taken asynchronously by a set of uncalibrated cameras. Photo-sequencing is an essential tool in analyzing (or visualizing) a dynamic scene captured by still images. The first step of the method detects sets of corresponding static and dynamic feature points across images. The static features are used to determine the epipolar geometry between pairs of images, and each dynamic feature votes for the temporal order of the images in which it appears. The partial orders provided by the dynamic features are not necessarily consistent, and we use rank aggregation to combine them into a globally consistent temporal order of images. We demonstrate successful photo sequencing on several challenging collections of images taken using a number of mobile phones."'),
('"Photo Uncrop"', '"ECCV 2014"', '["Computational photography", "image based rendering"]', '"https://doi.org/10.1007/978-3-319-10599-4_2"', '"We address the problem of extending the field of view of a photo\\u2014an operation we call uncrop. Given a reference photograph to be uncropped, our approach selects, reprojects, and composites a subset of Internet imagery taken near the reference into a larger image around the reference using the underlying scene geometry. The proposed Markov Random Field based approach is capable of handling large Internet photo collections with arbitrary viewpoints, dramatic appearance variation, and complicated scene layout. We show results that are visually compelling on a wide range of real-world landmarks."'),
('"Photo-Consistent Planar Patches from Unstructured Cloud of Points"', '"ECCV 2010"', '["Digital Surface Model", "Jaccard Distance", "Planar Patch", "Visibility Constraint", "Coplanar Poi', '"https://doi.org/10.1007/978-3-642-15555-0_43"', '"Planar patches are a very compact and stable intermediate representation of 3D scenes, as they are a good starting point for a complete automatic reconstruction of surfaces. This paper presents a novel method for extracting planar patches from an unstructured cloud of points that is produced by a typical structure and motion pipeline. The method integrates several constraints inside J-linkage, a robust algorithm for multiple models fitting. It makes use of information coming both from the 3D structure and the images. Several results show the effectiveness of the proposed approach."'),
('"Photometric Color Calibration of the Joint Monitor-Camera Response Function"', '"ECCV 2010"', '["Color Channel", "Brightness Level", "Photometric Stereo", "Camera Pixel", "Photometric Calibration', '"https://doi.org/10.1007/978-3-642-35740-4_4"', '"When recording presentations which include visualizations displayed on a monitor or with a video projector, the quality of the captured video suffers from color distortion and aliasing effects in the display area. A photometric calibration for the whole image can not compensate for these defects. In this paper, we present a per-pixel photometric calibration method that solves this problem.We measure the joint monitor-camera response function for every single camera pixel by displaying red, green, and blue screens at all brightness levels and capture them separately. These measurements are used to estimate the joint response function for every single pixel and all three color channels with the empirical model of response (EMoR). We apply the estimated response functions on subsequent captures of the display to calibrate them.Our method achieves a mean absolute error of about 0.66 brightness levels, averaged over all pixels of the image. The performance is also demonstrated with a calibration of a real captured photo, which is hardly distinguishable from the original."'),
('"Photometric Compensation to Dynamic Surfaces in a Projector-Camera System"', '"ECCV 2014"', '["Photometric compensation", "Dynamic surface", "Projector-camera system", "Augmented reality", "Ref', '"https://doi.org/10.1007/978-3-319-16199-0_20"', '"In this paper, a novel approach that allows color compensated projection on an arbitrary surface is presented. Assuming that the geometry of the surface is known, this method can be used in dynamic environments, where the surface color is not static. A simple calibration process is performed offline and only a single input image under reference illumination is sufficient for the estimation of the compensation. The system can recover the reflectance of the surface pixel-wise and provide an accurate photometric compensation to minimize the visibility of the projection surface. The color matching between the desired appearance of the projected image and the projection on the surface is performed in the device-independent color space CIE 1931 XYZ. The results of the evaluation confirm that this method provides a robust and accurate compensation even for surfaces with saturated colors and high spatial frequency patterns. This promising method can be the cornerstone of a real time projector-camera system for dynamic scenes."'),
('"Photometric Stereo for Dynamic Surface Orientations"', '"ECCV 2010"', '["Color Channel", "Surface Orientation", "Angular Error", "Orientation Error", "Dynamic Scene"]', '"https://doi.org/10.1007/978-3-642-15549-9_5"', '"We present a photometric stereo method for non-rigid objects of unknown and spatially varying materials. The prior art uses time-multiplexed illumination but assumes constant surface normals across several frames, fundamentally limiting the accuracy of the estimated normals. We explicitly account for time-varying surface orientations, and show that for unknown Lambertian materials, five images are sufficient to recover surface orientation in one frame. Our optimized system implementation exploits the physical properties of typical cameras and LEDs to reduce the required number of images to just three, and also facilitates frame-to-frame image alignment using standard optical flow methods, despite varying illumination. We demonstrate the system\\u2019s performance by computing surface orientations for several different moving, deforming objects."'),
('"Photometric Stereo from Maximum Feasible Lambertian Reflections"', '"ECCV 2010"', '["Mixed Integer Linear Programming", "Bidirectional Reflectance Distribution Function", "Photometric', '"https://doi.org/10.1007/978-3-642-15561-1_9"', '"We present a Lambertian photometric stereo algorithm robust to specularities and shadows and it is based on a maximum feasible subsystem (Max FS) framework. A Big-M method is developed to obtain the maximum subset of images that satisfy the Lambertian constraint among the whole set of captured photometric stereo images which include non-Lambertian reflections such as specularities and shadows. Our algorithm employs purely algebraic pixel-wise optimization without relying on probabilistic/physical reasoning or initialization, and it guarantees the global optimality. It can be applied to the image sets with the number of images ranging from four to hundreds, and we show that the computation time is reasonably short for a medium number of images (10~100). Experiments are carried out with various objects to demonstrate the effectiveness of the algorithm."'),
('"Physically Grounded Spatio-temporal Object Affordances"', '"ECCV 2014"', '["Object Affordances", "3D Object Models", "Functional Representation of Environment", "Generative G', '"https://doi.org/10.1007/978-3-319-10578-9_54"', '"Objects in human environments support various functionalities which govern how people interact with their environments in order to perform tasks. In this work, we discuss how to represent and learn a functional understanding of an environment in terms of object affordances. Such an understanding is useful for many applications such as activity detection and assistive robotics. Starting with a semantic notion of affordances, we present a generative model that takes a given environment and human intention into account, and grounds the affordances in the form of spatial locations on the object and temporal trajectories in the 3D environment. The probabilistic model also allows uncertainties and variations in the grounded affordances. We apply our approach on RGB-D videos from Cornell Activity Dataset, where we first show that we can successfully ground the affordances, and we then show that learning such affordances improves performance in the labeling tasks."'),
('"Piecewise Quadratic Reconstruction of Non-Rigid Surfaces from Monocular Sequences"', '"ECCV 2010"', '["Quadratic Model", "Bundle Adjustment", "Shared Point", "Rigid Factorization", "Real Video Sequence', '"https://doi.org/10.1007/978-3-642-15561-1_22"', '"In this paper we present a new method for the 3D reconstruction of highly deforming surfaces (for instance a flag waving in the wind) viewed by a single orthographic camera. We assume that the surface is described by a set of feature points which are tracked along an image sequence. Most non-rigid structure from motion algorithms assume a global deformation model where a rigid mean shape component accounts for most of the motion and the deformation modes are small deviations from it. However, in the case of strongly deforming objects, the deformations become more complex and a global model will often fail to explain the intricate deformations which are no longer small linear deviations from a strong mean component. Our proposed algorithm divides the surface into overlapping patches, reconstructs each of these patches individually using a quadratic deformation model and finally registers them imposing the constraint that points shared by patches must correspond to the same 3D points in space. We show good results on challenging motion capture and real video sequences with strong deformations where global methods fail to achieve good reconstructions."'),
('"Piecewise-Planar StereoScan:Structure and Motion from Plane Primitives"', '"ECCV 2014"', '["Structure and Motion", "Piecewise-Planar Reconstruction"]', '"https://doi.org/10.1007/978-3-319-10605-2_4"', '"This article describes a pipeline that receives as input a sequence of images acquired by a calibrated stereo rig and outputs the camera motion and a Piecewise-Planar Reconstruction (PPR) of the scene. It firstly detects the 3D planes viewed by each stereo pair from semi-dense depth estimation. This is followed by estimating the pose between consecutive views using a new closed-form minimal algorithm that relies in point correspondences only when plane correspondences are insufficient to fully constrain the motion. Finally, the camera motion and the PPR are jointly refined, alternating between discrete optimization for generating plane hypotheses and continuous bundle adjustment. The approach differs from previous works in PPR by determining the poses from plane-primitives, by jointly estimating motion and piecewise-planar structure, and by operating sequentially, being suitable for applications of SLAM and visual odometry. Experiments are carried in challenging wide-baseline datasets where conventional point-based SfM usually fails."'),
('"Pipe-Run Extraction and Reconstruction from Point Clouds"', '"ECCV 2014"', '["Point Cloud", "Principal Direction", "Point Cloud Data", "Global Similarity", "Major Radius"]', '"https://doi.org/10.1007/978-3-319-10578-9_2"', '"This paper presents automatic methods to extract and reconstruct industrial site pipe-runs from large-scale point clouds. We observe three key characteristics in this modeling problem, namely, primitives, similarities, and joints. While primitives capture the dominant cylindric shapes, similarities reveal the inter-primitive relations intrinsic to industrial structures because of human design and construction. Statistical analysis over point normals discovers primitive similarities from raw data to guide primitive fitting, increasing robustness to data noise and incompleteness. Finally, joints are automatically detected to close gaps and propagate connectivity information. The resulting model is more than a collection of 3D triangles, as it contains semantic labels for pipes as well as their connectivity."'),
('"Pipelining Localized Semantic Features for Fine-Grained Action Recognition"', '"ECCV 2014"', '["Object Detection", "Action Recognition", "Motion Feature", "Multiple Kernel Learning", "Dense Traj', '"https://doi.org/10.1007/978-3-319-10593-2_32"', '"In fine-grained action (object manipulation) recognition, it is important to encode object semantic (contextual) information, i.e., which object is being manipulated and how it is being operated. However, previous methods for action recognition often represent the semantic information in a global and coarse way and therefore cannot cope with fine-grained actions. In this work, we propose a representation and classification pipeline which seamlessly incorporates localized semantic information into every processing step for fine-grained action recognition. In the feature extraction stage, we explore the geometric information between local motion features and the surrounding objects. In the feature encoding stage, we develop a semantic-grouped locality-constrained linear coding (SG-LLC) method that captures the joint distributions between motion and object-in-use information. Finally, we propose a semantic-aware multiple kernel learning framework (SA-MKL) by utilizing the empirical joint distribution between action and object type for more discriminative action classification. Extensive experiments are performed on the large-scale and difficult fine-grained MPII cooking action dataset. The results show that by effectively accumulating localized semantic information into the action representation and classification pipeline, we significantly improve the fine-grained action classification performance over the existing methods."'),
('"Pixels, Stixels, and Objects"', '"ECCV 2012"', '["Motion State", "Intelligent Vehicle", "Occupancy Grid", "Dense Stereo", "IEEE Intelligent Vehicle ', '"https://doi.org/10.1007/978-3-642-33885-4_1"', '"Dense stereo vision has evolved into a powerful foundation for the next generation of intelligent vehicles. The high spatial and temporal resolution allows for robust obstacle detection in complex inner city scenarios, including pedestrian recognition and detection of partially hidden moving objects. Aiming at a vision architecture for efficiently solving an increasing number of vision tasks, the medium-level representation named Stixel World has been developed. This paper shows how this representation forms the foundation for a very efficient, robust and comprehensive understanding of traffic scenes. A recently proposed Stixel computation scheme allows the extraction of multiple objects per image column and generates a segmentation of the input data. The motion of the Stixels is obtained by applying the 6D-Vision principle to track Stixels over time. Subsequently, this allows for an optimal Stixel grouping such that all dynamic objects can be detected easily. Pose and motion of moving Stixel groups are used to initialize more specific object trackers. Moreover, appearance-based object recognition highly benefits from the attention control offered by the Stixel World, both in performance and efficiency."'),
('"Planar Structure Matching under Projective Uncertainty for Geolocation"', '"ECCV 2014"', '["uncertainty modeling", "geometric matching", "line segments"]', '"https://doi.org/10.1007/978-3-319-10584-0_18"', '"Image based geolocation aims to answer the question: where was this ground photograph taken? We present an approach to geolocalating a single image based on matching human delineated line segments in the ground image to automatically detected line segments in ortho images. Our approach is based on distance transform matching. By observing that the uncertainty of line segments is non-linearly amplified by projective transformations, we develop an uncertainty based representation and incorporate it into a geometric matching framework. We show that our approach is able to rule out a considerable portion of false candidate regions even in a database composed of geographic areas with similar visual appearances."'),
('"Plane + Parallax, Tensors and Factorization"', '"ECCV 2000"', '["Plane + parallax", "matching tensors", "projective reconstruction", "factorization", "structure fr', '"https://doi.org/10.1007/3-540-45054-8_34"', '"We study the special form that the general multi-image tensor formalism takes under the plane + parallax decomposition, including matching tensors and constraints, closure and depth recovery relations, and inter-tensor consistency constraints. Plane + parallax alignment greatly simplifies the algebra, and uncovers the underlying geometric content. We relate plane + parallax to the geometry of translating, calibrated cameras, and introduce a new parallax-factorizing projective reconstruction method based on this. Initial plane + parallax alignment reduces the problem to a single rank-one factorization of a matrix of rescaled parallaxes into a vector of projection centres and a vector of projective heights above the reference plane. The method extends to 3D lines represented by via-points and 3D planes represented by homographies."'),
('"Point of Gaze Estimation through Corneal Surface Reflection in an Active Illumination Environment"', '"ECCV 2012"', '["Corneal Surface", "Scene Image", "Visual Axis", "Pupil Center", "Environment Image"]', '"https://doi.org/10.1007/978-3-642-33709-3_12"', '"Eye gaze tracking (EGT) is a common problem with many applications in various fields. While recent methods have achieved improvements in accuracy and usability, current techniques still share several limitations. A major issue is the need for external calibration between the gaze camera system and the scene, which commonly restricts to static planar surfaces and leads to parallax errors. To overcome these issues, the paper proposes a novel scheme that uses the corneal imaging technique to directly analyze reflections from a scene illuminated with structured light. This comprises two major contributions: First, an analytic solution is developed for the forward projection problem to obtain the gaze reflection point (GRP), where light from the point of gaze (PoG) in the scene reflects at the corneal surface into an eye image. We also develop a method to compensate for the individual offset between the optical axis and true visual axis. Second, introducing active coded illumination enables robust and accurate matching at the GRP to obtain the PoG in a scene image, which is the first use of this technique in EGT and corneal reflection analysis. For this purpose, we designed a special high-power IR LED-array projector. Experimental evaluation with a prototype system shows that the proposed scheme achieves considerable accuracy and successfully supports depth-varying environments."'),
('"Polly: Telepresence from a Guide\\u2019s Shoulder"', '"ECCV 2014"', '["Telepresence", "Image stabilization", "Remote guiding", "Wearable", "Gimbal", "User feedback", "It', '"https://doi.org/10.1007/978-3-319-16199-0_36"', '"Polly is an inexpensive, portable telepresence device based on the metaphor of a parrot riding a guide\\u2019s shoulder and acting as proxy for remote participants. Although remote users may be anyone with a desire for \\u2018tele-visits\\u2019, we focus on limited mobility users. We present a series of prototypes and field tests that informed design iterations. Our current implementations utilize a smartphone on a stabilized, remotely controlled gimbal that can be hand held, placed on perches or carried by wearable frame. We describe findings from trials at campus, museum and faire tours with remote users, including quadriplegics. We found guides were more comfortable using Polly than a phone and that Polly was accepted by other people. Remote participants appreciated stabilized video and having control of the camera. One challenge is negotiation of movement and view control. Our tests suggest Polly is an effective alternative to telepresence robots, phones or fixed cameras."'),
('"Polyakov Action Minimization for Efficient Color Image Processing"', '"ECCV 2010"', '["Laplace-Beltrami", "diffusion", "optimization", "denoising", "PDEs"]', '"https://doi.org/10.1007/978-3-642-35740-4_5"', '"The Laplace-Beltrami operator is an extension of the Laplacian from flat domains to curved manifolds. It was proven to be useful for color image processing as it models a meaningful coupling between the color channels. This coupling is naturally expressed in the Beltrami framework in which a color image is regarded as a two dimensional manifold embedded in a hybrid, five-dimensional, spatial-chromatic (x,y,R,G,B) space."'),
('"Polynomial Regression on Riemannian Manifolds"', '"ECCV 2012"', '["Riemannian Manifold", "Corpus Callosum", "Polynomial Regression", "Parallel Transport", "Adjoint E', '"https://doi.org/10.1007/978-3-642-33712-3_1"', '"In this paper we develop the theory of parametric polynomial regression in Riemannian manifolds. The theory enables parametric analysis in a wide range of applications, including rigid and non-rigid kinematics as well as shape change of organs due to growth and aging. We show application of Riemannian polynomial regression to shape analysis in Kendall shape space. Results are presented, showing the power of polynomial regression on the classic rat skull growth data of Bookstein and the analysis of the shape changes associated with aging of the corpus callosum from the OASIS Alzheimer\\u2019s study."'),
('"Pose Estimation of Free-Form Objects"', '"ECCV 2004"', '["Surface Model", "Discrete Fourier Transform", "Geometric Algebra", "Conformal Geometric Algebra", ', '"https://doi.org/10.1007/978-3-540-24670-1_32"', '"In this contribution we present an approach for 2D-3D pose estimation of 3D free-form surface models. In our scenario we observe a free-form object in an image of a calibrated camera. Pose estimation means to estimate the relative position and orientation of the 3D object to the reference camera system. The object itself is modeled as a two-parametric 3D surface and extended by one-parametric contour parts of the object. A twist representation, which is equivalent to a Fourier representation allows for a low-pass approximation of the object model, which is advantageously applied to regularize the pose problem. The experiments show, that our developed algorithms are fast (200ms/frame) and accurate (1 o rotational error/frame)."'),
('"Pose Filter Based Hidden-CRF Models for Activity Detection"', '"ECCV 2014"', '["Activity detection", "Key-poses", "CRFs", "Latent-SVM"]', '"https://doi.org/10.1007/978-3-319-10605-2_46"', '"Detecting activities which involve a sequence of complex pose and motion changes in unsegmented videos is a challenging task, and common approaches use sequential graphical models to infer the human pose-state in every frame. We propose an alternative model based on detecting the key-poses in a video, where only the temporal positions of a few key-poses are inferred. We also introduce a novel pose summarization algorithm to automatically discover the key-poses of an activity. We learn a detection filter for each key-pose, which along with a bag-of-words root filter are combined in an HCRF model, whose parameters are learned using the latent-SVM optimization. We evaluate the performance of our model for detection on unsegmented videos on four human action datasets, which include challenging crowded scenes with dynamic backgrounds, inter-person occlusions, multi-human interactions and hard-to-detect daily use objects."'),
('"Pose Invariant Approach for Face Recognition at Distance"', '"ECCV 2012"', '["Face Recognition", "Facial Feature", "Local Binary Pattern", "Stereo Match", "Stereo Pair"]', '"https://doi.org/10.1007/978-3-642-33783-3_2"', '"We propose an automatic pose invariant approach for Face Recognition At a Distance (FRAD). Since face alignment is a crucial step in face recognition systems, we propose a novel facial features extraction model, which guides extended ASM to accurately align the face. Our main concern is to recognize human faces under uncontrolled environment at far distances accurately and fast. To achieve this goal, we perform an offline stage where 3D faces are reconstructed from stereo pair images. These 3D shapes are used to synthesize virtual 2D views in novel poses. To obtain good synthesized images from the 3D shape, we propose an accurate 3D reconstruction framework, which carefully handles illumination variance, occlusion, and the disparity discontinuity. The online phase is fast where a 2D image with unknown pose is matched with the closest virtual images in sampled poses. Experiments show that our approach outperforms the-state-of-the-art approaches."'),
('"Pose Invariant Face Recognition Under Arbitrary Unknown Lighting Using Spherical Harmonics"', '"BioAW 2004"', '["Face Recognition", "Face Image", "Training Image", "Basis Image", "Shape Error"]', '"https://doi.org/10.1007/978-3-540-25976-3_2"', '"We propose a new method for face recognition under arbitrary pose and illumination conditions, which requires only one training image per subject. Furthermore, no limitation on the pose and illumination conditions for the training image is necessary. Our method combines the strengths of Morphable models to capture the variability of 3D face shape and a spherical harmonic representation for the illumination. Morphable models are successful in 3D face reconstructions from one single image. Recent research demonstrates that the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace using spherical harmonics representation. In this paper, we show that we can recover the 3D faces with texture information from one single training image under arbitrary illumination conditions and perform robust pose and illumination invariant face recognition by using the recovered 3D faces. During training, given an image under arbitrary illumination, we first compute the shape parameters from a shape error estimated by the displacements of a set of feature points. Then we estimate the illumination coefficients and texture information using the spherical harmonics illumination representation. The reconstructed 3D models serve as generative models to render sets of basis images of each subject for different poses. During testing, we recognize the face for which there exists a weighted combination of basis images that is the closest to the test face image. We provide a series of experiments on approximately 5000 images from the CMU-PIE database. We achieve high recognition rates for images under a wide range of illumination conditions, including multiple sources of illumination."'),
('"Pose Invariant Face Recognition Using Linear Pose Transformation in Feature Space"', '"CVHCI 2004"', '["Face Recognition", "Recognition Rate", "Face Image", "Reconstruction Error", "Kernel Principal Com', '"https://doi.org/10.1007/978-3-540-24837-8_20"', '"Recognizing human face is one of the most important part in biometrics. However, drastic change of facial pose makes it a difficult problem. In this paper, we propose linear pose transformation method in feature space. At first, we extracted features from input face image at each pose. Then, we used extracted features to transform an input pose image into its corresponding frontal pose image. The experimental results show that recognition rate with pose transformation is much better than the result without pose transformation."'),
('"Pose Locality Constrained Representation for 3D Human Pose Reconstruction"', '"ECCV 2014"', '["3D human pose reconstruction", "subspace clustering", "hierarchical pose tree"]', '"https://doi.org/10.1007/978-3-319-10590-1_12"', '"Reconstructing 3D human poses from a single 2D image is an ill-posed problem without considering the human body model. Explicitly enforcing physiological constraints is known to be non-convex and usually leads to difficulty in finding an optimal solution. An attractive alternative is to learn a prior model of the human body from a set of human pose data. In this paper, we develop a new approach, namely pose locality constrained representation (PLCR), to model the 3D human body and use it to improve 3D human pose reconstruction. In this approach, the human pose space is first hierarchically divided into lower-dimensional pose subspaces by subspace clustering. After that, a block-structural pose dictionary is constructed by concatenating the basis poses from all the pose subspaces. Finally, PLCR utilizes the block-structural pose dictionary to explicitly encourage pose locality in human-body modeling \\u2013 nonzero coefficients are only assigned to the basis poses from a small number of pose subspaces that are close to each other in the pose-subspace hierarchy. We combine PLCR into the matching-pursuit based 3D human-pose reconstruction algorithm and show that the proposed PLCR-based algorithm outperforms the state-of-the-art algorithm that uses the standard sparse representation and physiological regularity in reconstructing a variety of human poses from both synthetic data and real images."'),
('"Pose Machines: Articulated Pose Estimation via Inference Machines"', '"ECCV 2014"', '["Random Forest", "Context Feature", "Composite Part", "Approximate Inference", "Pictorial Structure', '"https://doi.org/10.1007/978-3-319-10605-2_3"', '"State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks."'),
('"Pose Priors for Simultaneously Solving Alignment and Correspondence"', '"ECCV 2008"', '["Object Recognition", "Gaussian Mixture Model", "Gaussian Component", "Repetitive Pattern", "Uncert', '"https://doi.org/10.1007/978-3-540-88688-4_30"', '"Estimating a camera pose given a set of 3D-object and 2D-image feature points is a well understood problem when correspondences are given. However, when such correspondences cannot be established a priori, one must simultaneously compute them along with the pose. Most current approaches to solving this problem are too computationally intensive to be practical. An interesting exception is the SoftPosit algorithm, that looks for the solution as the minimum of a suitable objective function. It is arguably one of the best algorithms but its iterative nature means it can fail in the presence of clutter, occlusions, or repetitive patterns. In this paper, we propose an approach that overcomes this limitation by taking advantage of the fact that, in practice, some prior on the camera pose is often available. We model it as a Gaussian Mixture Model that we progressively refine by hypothesizing new correspondences. This rapidly reduces the number of potential matches for each 3D point and lets us explore the pose space more thoroughly than SoftPosit at a similar computational cost. We will demonstrate the superior performance of our approach on both synthetic and real data."'),
('"Pose-Invariant Face Recognition in Videos for Human-Machine Interaction"', '"ECCV 2012"', '["Face Recognition", "Face Image", "Locally Linear Embedding", "Locality Preserve Projection", "Acti', '"https://doi.org/10.1007/978-3-642-33868-7_56"', '"Human-machine interaction is a hot topic nowadays in the communities of computer vision and robotics. In this context, face recognition algorithms (used as primary cue for a person\\u2019s identity assessment) work well under controlled conditions but degrade significantly when tested in real-world environments. This is mostly due to the difficulty of simultaneously handling variations in illumination, pose, and occlusions. In this paper, we propose a novel approach for robust pose-invariant face recognition for human-robot interaction based on the real-time fitting of a 3D deformable model to input images taken from video sequences. More concrete, our approach generates a rectified face image irrespective with the actual head-pose orientation. Experimental results performed on Honda video database, using several manifold learning techniques, show a distinct advantage of the proposed method over the standard 2D appearance-based snapshot approach."'),
('"PoseCut: Simultaneous Segmentation and 3D Pose Estimation of Humans Using Dynamic Graph-Cuts"', '"ECCV 2006"', '["Image Segmentation", "Gaussian Mixture Model", "Segmentation Result", "Markov Random Field", "Appe', '"https://doi.org/10.1007/11744047_49"', '"We present a novel algorithm for performing integrated segmentation and 3D pose estimation of a human body from multiple views. Unlike other related state of the art techniques which focus on either segmentation or pose estimation individually, our approach tackles these two tasks together. Normally, when optimizing for pose, it is traditional to use some fixed set of features, e.g. edges or chamfer maps. In contrast, our novel approach consists of optimizing a cost function based on a Markov Random Field (MRF). This has the advantage that we can use all the information in the image: edges, background and foreground appearances, as well as the prior information on the shape and pose of the subject and combine them in a Bayesian framework. Previously, optimizing such a cost function would have been computationally infeasible. However, our recent research in dynamic graph cuts allows this to be done much more efficiently than before. We demonstrate the efficacy of our approach on challenging motion sequences. Note that although we target the human pose inference problem in the paper, our method is completely generic and can be used to segment and infer the pose of any specified rigid, deformable or articulated object."'),
('"Practical Autocalibration"', '"ECCV 2010"', '["Focal Length", "Intrinsic Parameter", "Principal Point", "Projective Reconstruction", "Aggregate C', '"https://doi.org/10.1007/978-3-642-15549-9_57"', '"As it has been noted several times in literature, the difficult part of autocalibration efforts resides in the structural non-linearity of the search for the plane at infinity. In this paper we present a robust and versatile autocalibration method based on the enumeration of the inherently bounded space of the intrinsic parameters of two cameras in order to find the collineation of space that upgrades a given projective reconstruction to Euclidean. Each sample of the search space (which reduces to a finite subset of \\u211d2 under mild assumptions) defines a consistent plane at infinity. This in turn produces a tentative, approximate Euclidean upgrade of the whole reconstruction which is then scored according to the expected intrinsic parameters of a Euclidean camera. This approach has been compared with several other algorithms on both synthetic and concrete cases, obtaining favourable results."'),
('"Practical Global Optimization for Multiview Geometry"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744023_46"', '"This paper presents a practical method for finding the provably globally optimal solution to numerous problems in projective geometry including multiview triangulation, camera resectioning and homography estimation. Unlike traditional methods which may get trapped in local minima due to the non-convex nature of these problems, this approach provides a theoretical guarantee of global optimality. The formulation relies on recent developments in fractional programming and the theory of convex underestimators and allows a unified framework for minimizing the standard L 2-norm of reprojection errors which is optimal under Gaussian noise as well as the more robust L 1-norm which is less sensitive to outliers. The efficacy of our algorithm is empirically demonstrated by good performance on experiments for both synthetic and real data. An open source MATLAB toolbox that implements the algorithm is also made available to facilitate further research."'),
('"Practical Interface Experiments with Implant Technology"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_2"', '"In this paper results are shown to indicate the efficacy of a direct connection between the human nervous system and a computer network. Experimental results obtained thus far from a study lasting for over 3 months are presented, with particular emphasis placed on the direct interaction between the human nervous system and a piece of wearable technology. An overview of the present state of neural implants is given, as well as a range of application areas considered thus far. A view is also taken as to what may be possible with implant technology as a general purpose human-computer interface for the future."'),
('"Practical Methods for Convex Multi-view Reconstruction"', '"ECCV 2010"', '["Reprojection Error", "Proximity Operator", "Camera Center", "Camera Rotation", "Trifocal Tensor"]', '"https://doi.org/10.1007/978-3-642-15561-1_26"', '"Globally optimal formulations of geometric computer vision problems comprise an exciting topic in multiple view geometry. These approaches are unaffected by the quality of a provided initial solution, can directly identify outliers in the given data, and provide a better theoretical understanding of geometric vision problems. The disadvantage of these methods are the substantial computational costs, which limit the tractable problem size significantly, and the tendency of reducing a particular geometric problem to one of the standard programs well-understood in convex optimization. We select a view on these geometric vision tasks inspired by recent progress made on other low-level vision problems using very simple (and easy to parallelize) methods. Our view also enables the utilization of geometrically more meaningful cost functions, which cannot be represented by one of the standard optimization problems. We also demonstrate in the numerical experiments, that our proposed method scales better with respect to the problem size than standard optimization codes."'),
('"Practical Time Bundle Adjustment for 3D Reconstruction on the GPU"', '"ECCV 2010"', '["Shared Memory", "Global Memory", "Camera Parameter", "Thread Block", "Visibility Mask"]', '"https://doi.org/10.1007/978-3-642-35740-4_33"', '"Large-scale 3D reconstruction has received a lot of attention recently. Bundle adjustment is a key component of the reconstruction pipeline and often its slowest and most computational resource intensive. It hasn\\u2019t been parallelized effectively so far. In this paper, we present a hybrid implementation of sparse bundle adjustment on the GPU using CUDA, with the CPU working in parallel. The algorithm is decomposed into smaller steps, each of which is scheduled on the GPU or the CPU. We develop efficient kernels for the steps and make use of existing libraries for several steps. Our implementation outperforms the CPU implementation significantly, achieving a speedup of 30-40 times over the standard CPU implementation for datasets with upto 500 images on an Nvidia Tesla C2050 GPU."'),
('"Precision-Recall-Classification Evaluation Framework: Application to Depth Estimation on Single Ima', '"ECCV 2014"', '["Precision-Recall", "Detection", "Classification", "Depth ordering"]', '"https://doi.org/10.1007/978-3-319-10590-1_42"', '"Many computer vision applications involve algorithms that can be decomposed in two main steps. In a first step, events or objects are detected and, in a second step, detections are assigned to classes. Examples of such \\u201cdetection plus classification\\u201d problems can be found in human pose classification, object recognition or action classification among others. In this paper, we focus on a special case: depth ordering on single images. In this problem, the detection step consists of the image segmentation, and the classification step assigns a depth gradient to each contour or a depth order to each region. We discuss the limitations of the classical Precision-Recall evaluation framework for these kind of problems and define an extended framework called \\u201cPrecision-Recall-Classfication\\u201d (PRC). Then, we apply this framework to depth ordering problems and design two specific PRC measures to evaluate both the local and the global depth consistencies. We use these measures to evaluate precisely state of the art depth ordering systems for monocular images. We also propose an extension to the method of [2] applying an optimal graph cut on a hierarchical segmentation structure. The resulting system is proven to provide better results than state of the art algorithms."'),
('"Predicting Actions from Static Scenes"', '"ECCV 2014"', '["Action prediction", "scene recognition", "functional properties"]', '"https://doi.org/10.1007/978-3-319-10602-1_28"', '"Human actions naturally co-occur with scenes. In this work we aim to discover action-scene correlation for a large number of scene categories and to use such correlation for action prediction. Towards this goal, we collect a new SUN Action dataset with manual annotations of typical human actions for 397 scenes. We next discover action-scene associations and demonstrate that scene categories can be well identified from their associated actions. Using discovered associations, we address a new task of predicting human actions for images of static scenes. We evaluate prediction of 23 and 38 action classes for images of indoor and outdoor scenes respectively and show promising results. We also propose a new application of geo-localized action prediction and demonstrate ability of our method to automatically answer queries such as \\u201cWhere is a good place for a picnic?\\u201d or \\u201cCan I cycle along this path?\\u201d."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Predicting Disparity Windows for Real-Time Stereo"', '"ECCV 2000"', '["Augmented Reality", "Temporal Coherence", "Full Image", "Full Frame", "Stereo System"]', '"https://doi.org/10.1007/3-540-45054-8_15"', '"New applications in fields such as augmented or virtualized reality have created a demand for dense, accurate real-time stereo reconstruction. Our goal is to reconstruct a user and her office environment for networked tele-immersion, which requires accurate depth values in a relatively large workspace. In order to cope with the combinatorics of stereo correspondence we can exploit the temporal coherence of image sequences by using coarse optical flow estimates to bound disparity search ranges at the next iteration. We use a simple flood fill segmentation method to cluster similar disparity values into overlapping windows and predict their motion over time using a single optical flow calculation per window. We assume that a contiguous region of disparity represents a single smooth surface which allows us to restrict our search to a narrow disparity range. The values in the range may vary over time as objects move nearer or farther away in Z, but we can limit the number of disparities to a feasible search size per window. Further, the disparity search and optical flow calculation are independent for each window, and allow natural distribution over a multi-processor architecture."'),
('"Predicting Facial Beauty without Landmarks"', '"ECCV 2010"', '["Face Image", "Multiscale Model", "Facial Attractiveness", "Absolute Score", "Luminance Channel"]', '"https://doi.org/10.1007/978-3-642-15567-3_32"', '"A fundamental task in artificial intelligence and computer vision is to build machines that can behave like a human in recognizing a broad range of visual concepts. This paper aims to investigate and develop intelligent systems for learning the concept of female facial beauty and producing human-like predictors. Artists and social scientists have long been fascinated by the notion of facial beauty, but study by computer scientists has only begun in the last few years. Our work is notably different from and goes beyond previous works in several aspects: 1) we focus on fully-automatic learning approaches that do not require costly manual annotation of landmark facial features but simply take the raw pixels as inputs; 2) our study is based on a collection of data that is an order of magnitude larger than that of any previous study; 3) we imposed no restrictions in terms of pose, lighting, background, expression, age, and ethnicity on the face images used for training and testing. These factors significantly increased the difficulty of the learning task. We show that a biologically-inspired model with multiple layers of trainable feature extractors can produce results that are much more human-like than the previously used eigenface approach. Finally, we develop a novel visualization method to interpret the learned model and revealed the existence of several beautiful features that go beyond the current averageness and symmetry hypotheses."'),
('"Principal Component Analysis over Continuous Subspaces and Intersection of Half-Spaces"', '"ECCV 2002"', '["Covariance Matrix", "Independent Component Analysis", "Principal Vector", "False Detection Rate", ', '"https://doi.org/10.1007/3-540-47977-5_42"', '"Principal Component Analysis (PCA) is one of the most popular techniques for dimensionality reduction of multivariate data points with application areas covering many branches of science. However, conventional PCA handles the multivariate data in a discrete manner only, i.e., the covariance matrix represents only sample data points rather than higher-order data representations."'),
('"Principal Geodesic Analysis on Symmetric Spaces: Statistics of Diffusion Tensors"', '"MMBIA 2004"', '["Diffusion Tensor Image", "Symmetric Space", "Tangent Vector", "Diffusion Tensor", "Isotropy Subgro', '"https://doi.org/10.1007/978-3-540-27816-0_8"', '"Diffusion tensor magnetic resonance imaging (DT-MRI) is emerging as an important tool in medical image analysis of the brain. However, relatively little work has been done on producing statistics of diffusion tensors. A main difficulty is that the space of diffusion tensors, i.e., the space of symmetric, positive-definite matrices, does not form a vector space. Therefore, standard linear statistical techniques do not apply. We show that the space of diffusion tensors is a type of curved manifold known as a Riemannian symmetric space. We then develop methods for producing statistics, namely averages and modes of variation, in this space. In our previous work we introduced principal geodesic analysis, a generalization of principal component analysis, to compute the modes of variation of data in Lie groups. In this work we expand the method of principal geodesic analysis to symmetric spaces and apply it to the computation of the variability of diffusion tensor data. We expect that these methods will be useful in the registration of diffusion tensor images, the production of statistical atlases from diffusion tensor data, and the quantification of the anatomical variability caused by disease."'),
('"Prior-Based Piecewise-Smooth Segmentation by Template Competitive Deformation Using Partitions of U', '"ECCV 2008"', '["Image Segmentation", "Active Contour", "Synthetic Image", "Region Competition", "Active Shape Mode', '"https://doi.org/10.1007/978-3-540-88690-7_47"', '"We propose a new algorithm for two-phase, piecewise-smooth segmentation with shape prior. The image is segmented by a binary template that is deformed by a regular geometric transformation. The choice of the template together with the constraint on the transformation introduce the shape prior. The deformation is guided by the maximization of the likelihood of foreground and background intensity models, so that we can refer to this approach as Competitive Deformation. In each region, the intensity is modelled as a smooth approximation of the original image. We represent the transformation using a Partition of Unity Finite Element Method, which consists in representing each component with polynomial approximations within local patches. A conformity constraint between the patches provides a way to control the globality of the deformation. We show several results on synthetic images, as well as on medical data from different modalities."'),
('"Priors for Large Photo Collections and What They Reveal about Cameras"', '"ECCV 2008"', '["Average Image", "Camera Model", "Photo Collection", "Blue Joint", "Joint Histogram"]', '"https://doi.org/10.1007/978-3-540-88693-8_6"', '"A large photo collection downloaded from the internet spans a wide range of scenes, cameras, and photographers. In this paper we introduce several novel priors for statistics of such large photo collections that are independent of these factors. We then propose that properties of these factors can be recovered by examining the deviation between these statistical priors and the statistics of a slice of the overall photo collection that holds one factor constant. Specifically, we recover the radiometric properties of a particular camera model by collecting numerous images captured by it, and examining the deviation of this collection\\u2019s statistics from that of a broader photo collection whose camera-specific effects have been removed. We show that using this approach we can recover both a camera model\\u2019s non-linear response function and the spatially-varying vignetting of the camera\\u2019s different lens settings. All this is achieved using publicly available photographs, without requiring images captured under controlled conditions or physical access to the cameras. We also apply this concept to identify bad pixels on the detectors of specific camera instances. We conclude with a discussion of future applications of this general approach to other common computer vision problems."'),
('"Privacy of Facial Soft Biometrics: Suppressing Gender But Retaining Identity"', '"ECCV 2014"', '["Face Image", "Equal Error Rate", "Image Warping", "Gender Information", "Face Matcher"]', '"https://doi.org/10.1007/978-3-319-16181-5_52"', '"We consider the problem of perturbing a face image in such a way that it cannot be used to ascertain soft biometric attributes such as age, gender and race, but can be used for automatic face recognition. Such an exercise is useful for extending different levels of privacy to a face image in a central database. In this work, we focus on masking the gender information in a face image with respect to an automated gender estimation scheme, while retaining its ability to be used by a face matcher. To facilitate this privacy-enhancing technique, the input face image is combined with another face image via a morphing scheme resulting in a mixed image. The mixing process can be used to progressively modify the input image such that its gender information is progressively suppressed; however, the modified images can still be used for recognition purposes if necessary. Preliminary experiments on the MUCT database suggest the potential of the scheme in imparting \\u201cdifferential privacy\\u201d to face images."'),
('"Probabalistic Models and Informative Subspaces for Audiovisual Correspondence"', '"ECCV 2002"', '["Mutual Information", "Video Sequence", "Video Frame", "Audio Signal", "Information Theoretic Appro', '"https://doi.org/10.1007/3-540-47977-5_39"', '"We propose a probabalistic model of single source multi-modal generation and show how algorithms for maximizing mutual information can find the correspondences between components of each signal. We show how non-parametric techniques for finding informative subspaces can capture the complex statistical relationship between signals in different modalities. We extend a previous technique for finding informative subspaces to include new priors on the projection weights, yielding more robust results. Applied to human speakers, our model can find the relationship between audio speech and video of facial motion, and partially segment out background events in both channels. We present new results on the problem of audio-visual verification, and show how the audio and video of a speaker can be matched even when no prior model of the speaker\\u2019s voice or appearance is available."'),
('"Probabilistic and Voting Approaches to Cue Integration for Figure-Ground Segmentation"', '"ECCV 2002"', '["Gaussian Mixture Model", "Expectation Maximization Algorithm", "Foreground Object", "Motion Segmen', '"https://doi.org/10.1007/3-540-47977-5_31"', '"This paper describes techniques for fusing the output of multiple cues to robustly and accurately segment foreground objects from the background in image sequences. Two different methods for cue integration are presented and tested. The first is a probabilistic approach which at each pixel computes the likelihood of observations over all cues before assigning pixels to foreground or background layers using Bayes Rule. The second method allows each cue to make a decision independent of the other cues before fusing their outputs with a weighted sum. A further important contribution of our work concerns demonstrating how models for some cues can be learnt and subsequently adapted online. In particular, regions of coherent motion are used to train distributions for colour and for a simple texture descriptor. An additional aspect of our framework is in providing mechanisms for suppressing cues when they are believed to be unreliable, for instance during training or when they disagree with the general consensus. Results on extended video sequences are presented."'),
('"Probabilistic Deformable Surface Tracking from Multiple Videos"', '"ECCV 2010"', '["Visual Hull", "Reprojection Error", "Multiple Video", "Data Acquisition Process", "Reference Mesh"', '"https://doi.org/10.1007/978-3-642-15561-1_24"', '"In this paper, we address the problem of tracking the temporal evolution of arbitrary shapes observed in multi-camera setups. This is motivated by the ever growing number of applications that require consistent shape information along temporal sequences. The approach we propose considers a temporal sequence of independently reconstructed surfaces and iteratively deforms a reference mesh to fit these observations. To effectively cope with outlying and missing geometry, we introduce a novel probabilistic mesh deformation framework. Using generic local rigidity priors and accounting for the uncertainty in the data acquisition process, this framework effectively handles missing data, relatively large reconstruction artefacts and multiple objects. Extensive experiments demonstrate the effectiveness and robustness of the method on various 4D datasets."'),
('"Probabilistic Human Recognition from Video"', '"ECCV 2002"', '["Posterior Probability", "State Vector", "Face Recognition", "Video Sequence", "Conditional Entropy', '"https://doi.org/10.1007/3-540-47977-5_45"', '"This paper presents a method for incorporating temporal information in a video sequence for the task of human recognition. A time series state space model, parameterized by a tracking state vector and a recognizing identity variable, is proposed to simultaneously characterize the kinematics and identity. Two sequential importance sampling (SIS) methods, a brute-force version and an efficient version, are developed to provide numerical solutions to the model. The joint distribution of both state vector and identity variable is estimated at each time instant and then propagated to the next time instant. Marginalization over the state vector yields a robust estimate of the posterior distribution of the identity variable. Due to the propagation of identity and kinematics, a degeneracy in posterior probability of the identity variable is achieved to give improved recognition. This evolving behavior is characterized using changes in entropy. The effectiveness of this approach is illustrated using experimental results on low resolution face data and upper body data."'),
('"Probabilistic Linear Discriminant Analysis"', '"ECCV 2006"', '["Linear Discriminant Analysis", "Gaussian Mixture Model", "Class Center", "Scatter Matrice", "Class', '"https://doi.org/10.1007/11744085_41"', '"Linear dimensionality reduction methods, such as LDA, are often used in object recognition for feature extraction, but do not address the problem of how to use these features for recognition. In this paper, we propose Probabilistic LDA, a generative probability model with which we can both extract the features and combine them for recognition. The latent variables of PLDA represent both the class of the object and the view of the object within a class. By making examples of the same class share the class variable, we show how to train PLDA and use it for recognition on previously unseen classes. The usual LDA features are derived as a result of training PLDA, but in addition have a probability model attached to them, which automatically gives more weight to the more discriminative features. With PLDA, we can build a model of a previously unseen class from a single example, and can combine multiple examples for a better representation of the class. We show applications to classification, hypothesis testing, class inference, and clustering, on classes not observed during training."'),
('"Probabilistic Multi-view Correspondence in a Distributed Setting with No Central Server"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_35"', '"We present a probabilistic algorithm for finding correspondences across multiple images. The algorithm runs in a distributed setting, where each camera is attached to a separate computing unit, and the cameras communicate over a network. No central computer is involved in the computation. The algorithm runs with low computational and communication cost. Our distributed algorithm assumes access to a standard pairwise wide-baseline stereo matching algorithm (\\\\(\\\\mathcal{WBS}\\\\)) and our goal is to minimize the number of images transmitted over the network, as well as the number of times the \\\\(\\\\mathcal{WBS}\\\\) is computed. We employ the theory of random graphs to provide an efficient probabilistic algorithm that performs \\\\(\\\\mathcal{WBS}\\\\) on a small number of image pairs, followed by a correspondence propagation phase. The heart of the paper is a theoretical analysis of the number of times \\\\(\\\\mathcal{WBS}\\\\) must be performed to ensure that an overwhelming portion of the correspondence information is extracted. The analysis is extended to show how to combat computer and communication failures, which are expected to occur in such settings, as well as correspondence misses. This analysis yields an efficient distributed algorithm, but it can also be used to improve the performance of centralized algorithms for correspondence."'),
('"Probabilistic Search for Object Segmentation and Recognition"', '"ECCV 2002"', '["Point Feature", "Object Recognition", "Hash Table", "Range Data", "Object Parameter"]', '"https://doi.org/10.1007/3-540-47977-5_52"', '"The problem of searching for a model-based scene interpretation is analyzed within a probabilistic framework. Object models are formulated as generative models for range data of the scene. A new statistical criterion, the truncated object probability, is introduced to infer an optimal sequence of object hypotheses to be evaluated for their match to the data. The truncated probability is partly determined by prior knowledge of the objects and partly learned from data. Some experiments on sequence quality and object segmentation and recognition from stereo data are presented. The article recovers classic concepts from object recognition (grouping, geometric hashing, alignment) from the probabilistic perspective and adds insight into the optimal ordering of object hypotheses for evaluation. Moreover, it introduces point-relation densities, a key component of the truncated probability, as statistical models of local surface shape."'),
('"Probabilistic Spatial-Temporal Segmentation of Multiple Sclerosis Lesions"', '"MMBIA 2004"', '["Multiple Sclerosis", "Gaussian Mixture Model", "Multiple Sclerosis Lesion", "Minimum Description L', '"https://doi.org/10.1007/978-3-540-27816-0_23"', '"In this paper we describe the application of a novel statistical video-modeling scheme to sequences of multiple sclerosis (MS) images taken over time. The analysis of the image-sequence input as a single entity, as opposed to a sequence of separate frames, is a unique feature of the proposed framework. Coherent space-time regions in a four-dimensional feature space (intensity, position (x,y), and time) and corresponding coherent segments in the video content are extracted by unsupervised clustering via Gaussian mixture modeling (GMM). The Expectation-Maximization (EM) algorithm is used to determine the parameters of the model according to the maximum likelihood principle. MS lesions are automatically detected, segmented and tracked in time by context-based classification mechanisms. Qualitative and quantitative results of the proposed methodology are shown for a sequence of 24 T2-weighted MR images, which was acquired from a relapsing-remitting MS patient over a period of approximately a year. The validation of the framework was performed by a comparison to an expert radiologist\\u2019s manual delineation."'),
('"Probabilistic Temporal Head Pose Estimation Using a Hierarchical Graphical Model"', '"ECCV 2014"', '["Face", "hierarchical", "probabilistic", "video", "graphical", "temporal", "head pose"]', '"https://doi.org/10.1007/978-3-319-10590-1_22"', '"We present a hierarchical graphical model to probabilistically estimate head pose angles from real-world videos, that leverages the temporal pose information over video frames. The proposed model employs a number of complementary facial features, and performs feature level, probabilistic classifier level and temporal level fusion. Extensive experiments are performed to analyze the pose estimation performance for different combination of features, different levels of the proposed hierarchical model and for different face databases. Experiments show that the proposed head pose model improves on the current state-of-the-art for the unconstrained McGillFaces [10] and the constrained CMU Multi-PIE [14] databases, increasing the pose classification accuracy compared to the current top performing method by 19.38% and 19.89%, respectively."'),
('"Probabilistic Tracking of the Soccer Ball"', '"SMVP 2004"', '["Soccer Player", "Soccer Game", "Proposal Density", "Probabilistic Tracking", "Ball Area"]', '"https://doi.org/10.1007/978-3-540-30212-4_5"', '"This paper proposes an algorithm for tracking the ball in a soccer video sequence. Two major issues in ball tracking are 1) the image portion of the ball in a frame is very small, having blurred white color, and 2) the interaction with players causes overlapping or occlusion and makes it almost impossible to detect the ball area in a frame or consecutive frames. The first is solved by accumulating the image measurements in time after removing the players\\u2019 blobs. The resultant image plays a role of proposal density for generating random particles in particle filtering. The second problem makes the ball invisible for time periods. Our tracker then finds adjacent players, marks them as potential ball holders, and pursues them until a new accumulated measurement sufficient for the ball tracking comes out. The experiment shows a good performance on a pretty long soccer match sequence in spite of the ball being frequently occluded by players."'),
('"Programmable Aperture Camera Using LCoS"', '"ECCV 2010"', '["Liquid Crystal Display", "Polarize Beam Splitter", "High Dynamic Range Imaging", "Defocused Image"', '"https://doi.org/10.1007/978-3-642-15567-3_25"', '"Since 1960s, aperture patterns have been studied extensively and a variety of coded apertures have been proposed for various applications, including extended depth of field, defocus deblurring, depth from defocus, light field acquisition, etc. Researches have shown that optimal aperture patterns can be quite different due to different applications, imaging conditions, or scene contents. In addition, many coded aperture techniques require aperture patterns to be temporally changed during capturing. As a result, it is often necessary to have a programmable aperture camera whose aperture pattern can be dynamically changed as needed in order to capture more useful information."'),
('"Programmable Automotive Headlights"', '"ECCV 2014"', '["Adaptive headlights", "reactive visual system", "computational illumination"]', '"https://doi.org/10.1007/978-3-319-10593-2_49"', '"The primary goal of an automotive headlight is to improve safety in low light and poor weather conditions. But, despite decades of innovation on light sources, more than half of accidents occur at night even with less traffic on the road. Recent developments in adaptive lighting have addressed some limitations of standard headlights, however, they have limited flexibility - switching between high and low beams, turning off beams toward the opposing lane, or rotating the beam as the vehicle turns - and are not designed for all driving environments. This paper introduces an ultra-low latency reactive visual system that can sense, react, and adapt quickly to any environment while moving at highway speeds. Our single hardware design can be programmed to perform a variety of tasks. Anti-glare high beams, improved driver visibility during snowstorms, increased contrast of lanes, markings, and sidewalks, and early visual warning of obstacles are demonstrated."'),
('"Progressive Mode-Seeking on Graphs for Sparse Feature Matching"', '"ECCV 2014"', '["Feature matching", "Mode-seeking"]', '"https://doi.org/10.1007/978-3-319-10605-2_51"', '"Sparse feature matching poses three challenges to graph-based methods: (1) the combinatorial nature makes the number of possible matches huge; (2) most possible matches might be outliers; (3) high computational complexity is often incurred. In this paper, to resolve these issues, we propose a simple, yet surprisingly effective approach to explore the huge matching space in order to significantly boost true matches while avoiding outliers. The key idea is to perform mode-seeking on graphs progressively based on our proposed guided graph density. We further design a density-aware sampling technique to considerably accelerate mode-seeking. Experimental study on various benchmark data sets demonstrates that our method is several orders faster than the state-of-the-art methods while achieving much higher precision and recall."'),
('"Projected Texture for Object Classification"', '"ECCV 2008"', '["Height Variation", "Shape Recovery", "Object Surface", "Equal Error Rate", "Projected Pattern"]', '"https://doi.org/10.1007/978-3-540-88690-7_46"', '"Algorithms for classification of 3D objects either recover the depth information lost during imaging using multiple images, structured lighting, image cues, etc. or work directly the images for classification. While the latter class of algorithms are more efficient and robust in comparison, they are less accurate due to the lack of depth information. We propose the use of structured lighting patterns projected on the object, which gets deformed according to the shape of the object. Since our goal is object classification and not shape recovery, we characterize the deformations using simple texture measures, thus avoiding the error prone and computationally expensive step of depth recovery. Moreover, since the deformations encode depth variations of the object, the 3D shape information is implicitly used for classification. We show that the information thus derived can significantly improve the accuracy of object classification algorithms, and derive the theoretical limits on height variations that can be captured by a particular projector-camera setup. A 3D texture classification algorithm derived from the proposed approach achieves a ten-fold reduction in error rate on a dataset of 30 classes, when compared to state-of-the-art image based approaches. We also demonstrate the effectiveness of the approach for a hand geometry based authentication system, which achieves a four-fold reduction in the equal error rate on a dataset containing 149 users."'),
('"Propagative Hough Voting for Human Activity Recognition"', '"ECCV 2012"', '["Training Data", "Video Clip", "Activity Recognition", "Interest Point", "Activity Search"]', '"https://doi.org/10.1007/978-3-642-33712-3_50"', '"Hough-transform based voting has been successfully applied to both object and activity detections. However, most current Hough voting methods will suffer when insufficient training data is provided. To address this problem, we propose propagative Hough voting for activity analysis. Instead of letting local features vote individually, we perform feature voting using random projection trees (RPT) which leverage the low-dimension manifold structure to match feature points in the high-dimensional feature space. Our RPT can index the unlabeled feature points in an unsupervised way. After the trees are constructed, the label and spatial-temporal configuration information are propagated from the training samples to the testing data via RPT. The proposed activity recognition method does not rely on human detection and tracking, and can well handle the scale and intra-class variations of the activity patterns. The superior performances on two benchmarked activity datasets validate that our method outperforms the state-of-the-art techniques not only when there is sufficient training data such as in activity recognition, but also when there is limited training data such as in activity search with one query example."'),
('"Properties of the Catadioptric Fundamental Matrix"', '"ECCV 2002"', '["Image Point", "Singular Vector", "Polar Plane", "Stereographic Projection", "Line Image"]', '"https://doi.org/10.1007/3-540-47967-8_10"', '"The geometry of two uncalibrated views obtained with a parabolic catadioptric device is the subject of this paper. We introduce the notion of circle space, a natural representation of line images, and the set of incidence preserving transformations on this circle space which happens to equal the Lorentz group. In this space, there is a bilinear constraint on transformed image coordinates in two parabolic catadioptric views involving what we call the catadioptric fundamental matrix. We prove that the angle between corresponding epipolar curves is preserved and that the transformed image of the absolute conic is in the kernel of that matrix, thus enabling a Euclidean reconstruction from two views. We establish the necessary and sufficient conditions for a matrix to be a catadioptric fundamental matrix."'),
('"Prosemantic Image Retrieval"', '"ECCV 2012"', '["Image Retrieval", "Relevance Feedback", "Relevant Image", "Text Annotation", "Ancillary Informatio', '"https://doi.org/10.1007/978-3-642-33885-4_72"', '"In this technical demonstration we present a content-based image retrieval system based on the \\u2018query by example\\u2019 paradigm. The system effectiveness will be proved for both category and target search on two standard image databases, even without a \\u201cgood\\u201d initial example and ancillary information, such as device metadata, text annotations, etc. These results are obtained by incorporating in the system our recently proposed prosemantic features coupled with a relevance feedback mechanism, and by maximizing novelty and diversity in the result sets."'),
('"Pseudo-bound Optimization for Binary Energies"', '"ECCV 2014"', '["Binary energy minimization", "high-order and non-submodular functions", "auxiliary functions", "pa', '"https://doi.org/10.1007/978-3-319-10602-1_45"', '"High-order and non-submodular pairwise energies are important for image segmentation, surface matching, deconvolution, tracking and other computer vision problems. Minimization of such energies is generally NP-hard. One standard approximation approach is to optimize an auxiliary function - an upper bound of the original energy across the entire solution space. This bound must be amenable to fast global solvers. Ideally, it should also closely approximate the original functional, but it is very difficult to find such upper bounds in practice."'),
('"Pseudo-entropy Similarity for Human Biometrics"', '"BioAW 2002"', '["Emotional Intelligence", "Object Pair", "Multimedia Object", "Algorithmic Notation", "Artificial I', '"https://doi.org/10.1007/3-540-47917-1_8"', '"With complex multimedia data, we see the emergence of biometric identification/authentication systems in which the fundamental operation is the similarity assessment of natural information-carrying objects. We have developed a similarity measure JeK, based on new notion of pseudo-entropy. The measure exhibits several features that match experimental findings in multimedia perception. We show how the measure can be used for identification/authentication of complex biometric objects, such as faces and its emotions, voices, and so on. We address the use of the pseudo-entropy measure JeK to deal with relations among the varied properties of 1D, 2D and 3D biometric objects."'),
('"Putting the Pieces Together: Regularized Multi-part Shape Matching"', '"ECCV 2012"', '["Iterative Close Point", "Data Term", "Partial Match", "Alignment Error", "Rigid Transformation"]', '"https://doi.org/10.1007/978-3-642-33863-2_1"', '"Multi-part shape matching is an important class of problems, arising in many fields such as computational archaeology, biology, geometry processing, computer graphics and vision. In this paper, we address the problem of simultaneous matching and segmentation of multiple shapes. We assume to be given a reference shape and multiple parts partially matching the reference. Each of these parts can have additional clutter, have overlap with other parts, or there might be missing parts. We show experimental results of efficient and accurate assembly of fractured synthetic and real objects."'),
('"Qualitative Spatiotemporal Analysis Using an Oriented Energy Representation"', '"ECCV 2000"', '["Coherent Motion", "Spatiotemporal Data", "Spatiotemporal Information", "Spatiotemporal Structure",', '"https://doi.org/10.1007/3-540-45053-X_49"', '"This paper presents an approach to representing and analyzing spatiotemporal information in support of making qualitative, yet semantically meaningful distinctions at the earliest stages of processing. A small set of primitive classes of spatiotemporal structure are proposed that correspond to categories of stationary, coherently moving, incoherently moving, flickering, scintillating and \\u201ctoo unstructured to support further inference\\u201d. It is shown how these classes can be represented and distinguished in a uniform fashion in terms of oriented energy signatures. Further, empirical results are presented that illustrate the use of the approach in application to natural imagery. The importance of the described work is twofold: (i) From a theoretical point of view a semantically meaningful decomposition of spatiotemporal information is developed. (ii) From a practical point of view, the developed approach has the potential to impact real world image understanding and analysis applications. As examples: The approach could be used to support early focus of attention and cueing mechanisms that guide subsequent activities by an intelligent agent; the approach could provide the representational substrate for indexing video and other spatiotemporal data."'),
('"Quality Assessment of Non-dense Image Correspondences"', '"ECCV 2012"', '["Correspondence Algorithm", "Endpoint Error", "Histogram Distance", "Stereo Disparity", "Dense Corr', '"https://doi.org/10.1007/978-3-642-33868-7_12"', '"Non-dense image correspondence estimation algorithms are known for their speed, robustness and accuracy. However, current evaluation methods evaluate correspondences point-wise and consider only correspondences that are actually estimated. They cannot evaluate the fact that some algorithms might leave important scene correspondences undetected - correspondences which might be vital for succeeding applications. Additionally, often the reference correspondences for real world scenes are also sparse. Outliers that do not hit a reference measurement can remain undetected with the current, point-wise evaluation methods. To assess the quality of correspondence fields we propose a histogram based evaluation metric that does not rely on point-wise comparison and is therefore robust to sparsity in estimate as well as reference."'),
('"Quantification of Growth and Motion Using Non-rigid Registration"', '"CVAMIA 2006"', '["Image Registration", "Cardiac Motion", "Epicardial Surface", "Computer Assist Tomography", "Medica', '"https://doi.org/10.1007/11889762_5"', '"Three-dimensional (3D) and four-dimensional (4D) imaging of dynamic structures is a rapidly developing area of research in medical imaging. Non-rigid registration plays an important role for the analysis of these datasets. In this paper we will show some of the work of our group using non-rigid registration techniques for the detection of temporal changes such as growth in brain MR images. We will also show how non-rigid registration can be used to analyze the motion of the heart from cardiac MR images."'),
('"Quantifying Micro-expressions with Constraint Local Model and Local Binary Pattern"', '"ECCV 2014"', '["Quantification", "Micro-expression", "Dynamic information", "Constraint Local Model (CLM)", "Local', '"https://doi.org/10.1007/978-3-319-16178-5_20"', '"Micro-expression may reveal genuine emotions that people try to conceal. However, it\\u2019s difficult to measure it. We selected two feature extraction methods to analyze micro-expressions by assessing the dynamic information. The Constraint Local Model (CLM) algorithm is employed to detect faces and track feature points. Based on these points, the ROIs (Regions of Interest) on the face are drawn for further analysis. In addition, Local Binary Pattern (LBP) algorithm is employed to extract texture information from the ROIs and measure the differences between frames. The results from the proposed methods are compared with manual coding. These two proposed methods show good performance, with sensitivity and reliability. This is a pilot study on quantifying micro-expression movement for psychological research purpose. These methods would assist behavior researchers in measuring facial movements on various facets and at a deeper level."'),
('"Quasi-conformal Flat Representation of Triangulated Surfaces for Computerized Tomography"', '"CVAMIA 2006"', '["Riemannian Manifold", "Quasiconformal Mapping", "Angle Dilatation", "Circle Packing", "Triangulate', '"https://doi.org/10.1007/11889762_14"', '"In this paper we present a simple method for flattening of triangulated surfaces for mapping and imaging. The method is based on classical results of F. Gehring and Y. V\\u00e4is\\u00e4l\\u00e4 regarding the existence of quasi-conformal and quasi-isometric mappings between Riemannian manifolds. A random starting triangle version of the algorithm is presented. A curvature based version is also applicable. In addition the algorithm enables the user to compute the maximal distortion and dilatation errors. Moreover, the algorithm makes no use to derivatives, hence it is robust and suitable for analysis of noisy data. The algorithm is tested on data obtained from real CT images of the human brain cortex and colon, as well as on a synthetic model of the human skull."'),
('"Quasi-Dense Reconstruction from Image Sequence"', '"ECCV 2002"', '["Fundamental Matrix", "Camera Position", "Projective Reconstruction", "Texture Scene", "Seed Match"', '"https://doi.org/10.1007/3-540-47967-8_9"', '"This paper proposes a quasi-dense reconstruction from uncalibrated sequence. The main innovation is that all geometry is computed based on re-sampled quasi-dense correspondences rather than the standard sparse points of interest. It not only produces more accurate and robust reconstruction due to highly redundant and well spread input data, but also fills the gap of insufficiency of sparse reconstruction for visualization application. The computational engine is the quasi-dense 2-view and the quasi-dense 3-view algorithms developed in this paper. Experiments on real sequences demonstrate the superior performance of quasi-dense w.r.t. sparse reconstruction both in accuracy and robustness."'),
('"Quasi-Random Sampling for Condensation"', '"ECCV 2000"', '["Process Noise", "Importance Sampling", "IEEE International Conf", "Pedestrian Detection", "Standar', '"https://doi.org/10.1007/3-540-45053-X_9"', '"The problem of tracking pedestrians from a moving car is a challenging one. The Condensation tracking algorithm is appealing for its generality and potential for real-time implementation. However, the conventional Condensation tracker is known to have difficulty with high-dimensional state spaces and unknown motion models. This paper presents an improved algorithm that addresses these problems by using a simplified motion model, and employing quasi-Monte Carlo techniques to efficiently sample the resulting tracking problem in the high-dimensional state space. For N sample points, these techniques achieve sampling errors of O(N-1), as opposed to O(N-1/2) for conventional Monte Carlo techniques. We illustrate the algorithm by tracking objects in both synthetic and real sequences, and show that it achieves reliable tracking and significant speed-ups over conventional Monte Carlo techniques."'),
('"Quaternion-Based Spectral Saliency Detection for Eye Fixation Prediction"', '"ECCV 2012"', '["Color Space", "Area Under Curve", "Saliency Detection", "Visual Saliency", "Saliency Model"]', '"https://doi.org/10.1007/978-3-642-33709-3_9"', '"In recent years, several authors have reported that spectral saliency detection methods provide state-of-the-art performance in predicting human gaze in images (see, e.g., [1\\u20133]). We systematically integrate and evaluate quaternion DCT- and FFT-based spectral saliency detection [3,4], weighted quaternion color space components [5], and the use of multiple resolutions [1]. Furthermore, we propose the use of the eigenaxes and eigenangles for spectral saliency models that are based on the quaternion Fourier transform. We demonstrate the outstanding performance on the Bruce-Tsotsos (Toronto), Judd (MIT), and Kootstra- Schomacker eye-tracking data sets."'),
('"Query Specific Fusion for Image Retrieval"', '"ECCV 2012"', '["Image Retrieval", "Query Image", "Retrieval Method", "Retrieval Result", "Rank Aggregation"]', '"https://doi.org/10.1007/978-3-642-33709-3_47"', '"Recent image retrieval algorithms based on local features indexed by a vocabulary tree and holistic features indexed by compact hashing codes both demonstrate excellent scalability. However, their retrieval precision may vary dramatically among queries. This motivates us to investigate how to fuse the ordered retrieval sets given by multiple retrieval methods, to further enhance the retrieval precision. Thus, we propose a graph-based query specific fusion approach where multiple retrieval sets are merged and reranked by conducting a link analysis on a fused graph. The retrieval quality of an individual method is measured by the consistency of the top candidates\\u2019 nearest neighborhoods. Hence, the proposed method is capable of adaptively integrating the strengths of the retrieval methods using local or holistic features for different queries without any supervision. Extensive experiments demonstrate competitive performance on 4 public datasets, i.e., the UKbench, Corel-5K, Holidays and San Francisco Landmarks datasets."'),
('"Quick Shift and Kernel Methods for Mode Seeking"', '"ECCV 2008"', '["Image Segmentation", "Kernel Method", "Kernel Matrix", "Kernel Space", "Gaussian Window"]', '"https://doi.org/10.1007/978-3-540-88693-8_52"', '"We show that the complexity of the recently introduced medoid-shift algorithm in clustering N points is O(N 2), with a small constant, if the underlying distance is Euclidean. This makes medoid shift considerably faster than mean shift, contrarily to what previously believed. We then exploit kernel methods to extend both mean shift and the improved medoid shift to a large family of distances, with complexity bounded by the effective rank of the resulting kernel matrix, and with explicit regularization constraints. Finally, we show that, under certain conditions, medoid shift fails to cluster data points belonging to the same mode, resulting in over-fragmentation. We propose remedies for this problem, by introducing a novel, simple and extremely efficient clustering algorithm, called quick shift, that explicitly trades off under- and over-fragmentation. Like medoid shift, quick shift operates in non-Euclidean spaces in a straightforward manner. We also show that the accelerated medoid shift can be used to initialize mean shift for increased efficiency. We illustrate our algorithms to clustering data on manifolds, image segmentation, and the automatic discovery of visual categories."'),
('"Radial Bright Channel Prior for Single Image Vignetting Correction"', '"ECCV 2014"', '["radial bright channel prior", "vignetting correction"]', '"https://doi.org/10.1007/978-3-319-10605-2_13"', '"This paper presents a novel prior, radial bright channel (RBC) prior, for single image vignetting correction. The RBC prior is derived from a statistical property of vignetting-free images: for the pixels sharing the same radius in polar coordinates of an image, at least one pixel has a high intensity value at some color channel. Exploiting the prior, we can effectively estimate and correct the vignetting effect of a given image. We represent the vignetting effect as an 1D function of the distance from the optical center, and estimate the function using the RBC prior. As it works completely in 1D, our method provides high efficiency in terms of computation and storage costs. Experimental results demonstrate that our method runs an order of magnitude faster than previous work, while producing higher quality results of vignetting correction."'),
('"Rainbow Flash Camera: Depth Edge Extraction Using Complementary Colors"', '"ECCV 2012"', '["Multi-flash camera", "depth edge extraction", "color multiplexing", "complementary color", "hue ci', '"https://doi.org/10.1007/978-3-642-33783-3_37"', '"We present a novel color multiplexing method for extracting depth edges in a scene. It has been shown that casting shadows from different light positions provides a simple yet robust cue for extracting depth edges. Instead of flashing a single light source at a time as in conventional methods, our method flashes all light sources simultaneously to reduce the number of captured images. We use a ring light source around a camera and arrange colors on the ring such that the colors form a hue circle. Because complementary colors are arranged at any position and its antipole on the ring, shadow regions where a half of the hue circle is occluded are colorized according to the orientations of depth edges, while non-shadow regions where all the hues are mixed have a neutral color in the captured image. In an ideal situation, the colored shadows in a single image directly provide depth edges and their orientations. In practice, we present a robust depth edge extraction algorithm using an additional image captured by rotating the hue circle with 180\\u00b0. We demonstrate the advantages of our approach using a camera prototype consisting of a standard camera and 8 color LEDs."'),
('"Random Forest for Image Annotation"', '"ECCV 2012"', '["Random Forest", "Image Annotation", "Semantic Nearest Neighbor"]', '"https://doi.org/10.1007/978-3-642-33783-3_7"', '"In this paper, we present a novel method for image annotation and made three contributions. Firstly, we propose to use the tags contained in the training images as the supervising information to guide the generation of random trees, thus enabling the retrieved nearest neighbor images not only visually alike but also semantically related. Secondly, different from conventional decision tree methods, which fuse the information contained at each leaf node individually, our method treats the random forest as a whole, and introduces the new concepts of semantic nearest neighbors (SNN) and semantic similarity measure (SSM). Thirdly, we annotate an image from the tags of its SNN based on SSM and have developed a novel learning to rank algorithm to systematically assign the optimal tags to the image. The new technique is intrinsically scalable and we will present experimental results to demonstrate that it is competitive to state of the art methods."'),
('"Random Walks, Constrained Multiple Hypothesis Testing and Image Enhancement"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744023_30"', '"Image restoration is a keen problem of low level vision. In this paper, we propose a novel \\u2013 assumption-free on the noise model \\u2013 technique based on random walks for image enhancement. Our method explores multiple neighbors sets (or hypotheses) that can be used for pixel denoising, through a particle filtering approach. This technique associates weights for each hypotheses according to its relevance and its contribution in the denoising process. Towards accounting for the image structure, we introduce perturbations based on local statistical properties of the image. In other words, particle evolution are controlled by the image structure leading to a filtering window adapted to the image content. Promising experimental results demonstrate the potential of such an approach."'),
('"Randomized Locality Sensitive Vocabularies for Bag-of-Features Model"', '"ECCV 2010"', '["Random Forest", "Hash Function", "Visual Word", "Reproduce Kernel Hilbert Space", "Mean Average Pr', '"https://doi.org/10.1007/978-3-642-15558-1_54"', '"Visual vocabulary construction is an integral part of the popular Bag-of-Features (BOF) model. When visual data scale up (in terms of the dimensionality of features or/and the number of samples), most existing algorithms (e.g. k-means) become unfavorable due to the prohibitive time and space requirements. In this paper we propose the random locality sensitive vocabulary (RLSV) scheme towards efficient visual vocabulary construction in such scenarios. Integrating ideas from the Locality Sensitive Hashing (LSH) and the Random Forest (RF), RLSV generates and aggregates multiple visual vocabularies based on random projections, without taking clustering or training efforts. This simple scheme demonstrates superior time and space efficiency over prior methods, in both theory and practice, while often achieving comparable or even better performances. Besides, extensions to supervised and kernelized vocabulary constructions are also discussed and experimented with."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Randomized Spatial Partition for Scene Recognition"', '"ECCV 2012"', '["Random Partition", "Spatial Layout", "Scene Recognition"]', '"https://doi.org/10.1007/978-3-642-33709-3_52"', '"The spatial layout of images plays a critical role in natural scene analysis. Despite previous work, e.g., spatial pyramid matching, how to design optimal spatial layout for scene classification remains an open problem due to the large variations of scene categories. This paper presents a novel image representation method, with the objective to characterize the image layout by various patterns, in the form of randomized spatial partition (RSP). The RSP-based image representation makes it possible to mine the most descriptive image layout pattern for each category of scenes, and then combine them by training a discriminative classifier, i.e., the proposed ORSP classifier. Besides RSP image representation, another powerful classifier, called the BRSP classifier, is also proposed. By weighting and boosting a sequence of various partition patterns, the BRSP classifier is more robust to the intra-class variations hence leads to a more accurate classification. Both RSP-based classifiers are tested on three publicly available scene datasets. The experimental results highlight the effectiveness of the proposed methods."'),
('"Range Flow for Varying Illumination"', '"ECCV 2008"', '["Motion Estimate", "Illumination Change", "Intensity Constraint", "Vary Illumination", "Model TAYLO', '"https://doi.org/10.1007/978-3-540-88682-2_39"', '"In this paper range flow estimation is extended to handle brightness changes in image data caused by inhomogeneous illumination. Standard range flow computes 3d velocity fields from range and intensity image sequences. To this end it combines a depth change model and a brightness constancy model. In this contribution, the brightness constancy model is exchanged by (1) a gradient constancy model, (2) a combination of gradient and brightness constancy constraint that has been used successfully for optical flow estimation in literature, and (3) a physics-based brightness change model. Insensitivity to brightness changes can also be achieved by prefiltering of the input intensity data. High pass or homomorphic filtering are the most well known approaches from literature. In performance tests therefore the well known version and the novel versions of range flow estimation are investigated on prefiltered or non-prefiltered data using synthetic ground-truth and real data from a botanical experiment."'),
('"Rank Classification of Linear Line Structure in Determining Trifocal Tensor"', '"ECCV 2008"', '["Motion Estimation", "Camera Motion", "Line Structure", "Estimation Matrix", "Linear Line"]', '"https://doi.org/10.1007/978-3-540-88688-4_54"', '"The problem we address is: given line correspondences over three views, what is the condition of the line correspondences for the spatial relation of the three associated camera positions to be uniquely recoverable? We tackle the problem from the perspective of trifocal tensor, a quantity that captures the relative positions of the cameras in relation to the three views. We show that the rank of the matrix that leads to the estimation of the tensor reduces to 7, 11, 15 respectively for line pencil, point star, and ruled plane, which are structures that belong to linear line space; and 12, 19, 23 for general ruled surface, general linear congruence, and general linear line complex. These critical structures are quite typical in reality, and thus the findings are important to the validity and stability of practically all algorithms related to structure from motion and projective reconstruction using line correspondences."'),
('"Rank Minimization with Structured Data Patterns"', '"ECCV 2014"', '["Singular Value Decomposition", "Rank Function", "Measurement Matrix", "Convex Relaxation", "Convex', '"https://doi.org/10.1007/978-3-319-10578-9_17"', '"The problem of finding a low rank approximation of a given measurement matrix is of key interest in computer vision. If all the elements of the measurement matrix are available, the problem can be solved using factorization. However, in the case of missing data no satisfactory solution exists. Recent approaches replace the rank term with the weaker (but convex) nuclear norm. In this paper we show that this heuristic works poorly on problems where the locations of the missing entries are highly correlated and structured which is a common situation in many applications."'),
('"Ranking Domain-Specific Highlights by Analyzing Edited Videos"', '"ECCV 2014"', '["Video highlight detection", "latent ranking"]', '"https://doi.org/10.1007/978-3-319-10590-1_51"', '"We present a fully automatic system for ranking domain-specific highlights in unconstrained personal videos by analyzing online edited videos. A novel latent linear ranking model is proposed to handle noisy training data harvested online. Specifically, given a search query (domain) such as \\u201csurfing\\u201d, our system mines the Youtube database to find pairs of raw and corresponding edited videos. Leveraging the assumption that edited video is more likely to contain highlights than the trimmed parts of the raw video, we obtain pair-wise ranking constraints to train our model. The learning task is challenging due to the amount of noise and variation in the mined data. Hence, a latent loss function is incorporated to robustly deal with the noise. We efficiently learn the latent model on a large number of videos (about 700 minutes in all) using a novel EM-like self-paced model selection procedure. Our latent ranking model outperforms its classification counterpart, a motion analysis baseline [15], and a fully-supervised ranking system that requires labels from Amazon Mechanical Turk. Finally, we show that impressive highlights can be retrieved without additional human supervision for domains like skating, surfing, skiing, gymnastics, parkour, and dog activity in unconstrained personal videos."'),
('"Re-identification of Pedestrians in Crowds Using Dynamic Time Warping"', '"ECCV 2012"', '["Single Shot", "Dynamic Time Warping", "Appearance Feature", "Human Detection", "Camera Network"]', '"https://doi.org/10.1007/978-3-642-33863-2_42"', '"This paper presents a new tracking algorithm to solve on-line the \\u2018Tag and Track\\u2019 problem in a crowded scene with a network of CCTV Pan, Tilt and Zoom (PTZ) cameras. The dataset is very challenging as the non-overlapping cameras exhibit pan tilt and zoom motions, both smoothly and abruptly. Therefore a tracking-by-detection approach is combined with a re-identification method based on appearance features to solve the re-acquisition problem between non overlapping camera views and crowds occlusions. However, conventional re-identification techniques of multi target trackers, which consist of learning an online appearance model to differentiate the target of interest from other people in the scene, are not suitable for this scenario because the tagged pedestrian moves in an environment where pedestrians walking with them are constantly changing. Therefore, a novel multiple shots re-identification technique is proposed which combines a standard single shot re-identification, based on offline training to recognize humans from different views, with a Dynamic Time Warping (DTW) distance."'),
('"Re-identification with RGB-D Sensors"', '"ECCV 2012"', '["Re-identification", "RGB-D sensors", "Kinect"]', '"https://doi.org/10.1007/978-3-642-33863-2_43"', '"People re-identification is a fundamental operation for any multi-camera surveillance scenario. Until now, it has been performed by exploiting primarily appearance cues, hypothesizing that the individuals cannot change their clothes. In this paper, we relax this constraint by presenting a set of 3D soft-biometric cues, being insensitive to appearance variations, that are gathered using RGB-D technology. The joint use of these characteristics provides encouraging performances on a benchmark of 79 people, that have been captured in different days and with different clothing. This promotes a novel research direction for the re-identification community, supported also by the fact that a new brand of affordable RGB-D cameras have recently invaded the worldwide market."'),
('"Re-presentations of Art Collections"', '"ECCV 2014"', '[]', '"https://doi.org/10.1007/978-3-319-16178-5_6"', '"The objective of this paper is to show how modern computer vision methods can be used to aid the art or book historian in analysing large digital art collections."'),
('"Read My Lips: Continuous Signer Independent Weakly Supervised Viseme Recognition"', '"ECCV 2014"', '["Sign Language Recognition", "Viseme Recognition", "Mouthing", "Lip Reading"]', '"https://doi.org/10.1007/978-3-319-10590-1_19"', '"This work presents a framework to recognise signer independent mouthings in continuous sign language, with no manual annotations needed. Mouthings represent lip-movements that correspond to pronunciations of words or parts of them during signing. Research on sign language recognition has focused extensively on the hands as features. But sign language is multi-modal and a full understanding particularly with respect to its lexical variety, language idioms and grammatical structures is not possible without further exploring the remaining information channels. To our knowledge no previous work has explored dedicated viseme recognition in the context of sign language recognition. The approach is trained on over 180.000 unlabelled frames and reaches 47.1% precision on the frame level. Generalisation across individuals and the influence of context-dependent visemes are analysed."'),
('"Reading Ancient Coins: Automatically Identifying Denarii Using Obverse Legend Seeded Retrieval"', '"ECCV 2012"', '["Recognition", "Text", "Image", "Reverse", "Motif", "Inscription"]', '"https://doi.org/10.1007/978-3-642-33765-9_23"', '"The aim of this paper is to automatically identify a Roman Imperial denarius from a single query photograph of its obverse and reverse. Such functionality has the potential to contribute greatly to various national schemes which encourage laymen to report their finds to local museums. Our work introduces a series of novelties: (i) this is the first paper which describes a method for extracting the legend of an ancient coin from a photograph; (ii) we are also the first to suggest the idea and propose a method for identifying a coin using a series of carefully engineered retrievals, each harnessed for further information using visual or meta-data processing; (iii) we show how in addition to a unique standard reference number for a query coin, the proposed system can be used to extract salient coin information (issuing authority, obverse and reverse descriptions, mint date) and retrieve images of other coins of the same type."'),
('"Reading Out the Synaptic Echoes of Low-Level Perception in V1"', '"ECCV 2012"', '["Visual Cortex", "Receptive Field", "Apparent Motion", "Primary Visual Cortex", "Gabor Patch"]', '"https://doi.org/10.1007/978-3-642-33863-2_50"', '"Primary visual cortex (V1) in the mammalian brain computes on the fly perceptual primitives (form, motion, visual flow) from the feedforward bombardment of retinal events channeled through the thalamus. At the same time, it integrates the distributed feedback of higher cortical areas involved in more elaborate cognitive functions. The reverberating activity evoked by the interplay between these two streams has been hypothesized to form the trace of the low-level computational operations written on the \\u201chigh resolution buffer\\u201d of primary cortical areas [1]. In vivo intracellular electrophysiology in V1 offers the unique possibility of listening to the synaptic echoes of the effective perceptual network at work. On the basis of the comparison between functional synaptic imaging and voltage sensitive dye imaging, I will show that the emergence of macroscopic features of perception (Gestalt and motion flow related percepts) in early sensory cortical areas can be predicted from the read-out of analog graded events (synaptic potentials) operating at a more microscopic integration level."'),
('"Real Time Detection of Social Interactions in Surveillance Video"', '"ECCV 2012"', '["Video Sequence", "Surveillance Video", "Real Time Detection", "Casual Interaction", "Social Force ', '"https://doi.org/10.1007/978-3-642-33885-4_12"', '"In this paper we present a novel method to detect the presence of social interactions occurring in a surveillance scenario. The algorithm we propose complements motion features with proxemics cues, so as to link the human motion with the contextual and environmental information. The extracted features are analyzed through a multi-class SVM. Testing has been carried out distinguishing between casual and intentional interactions, where intentional events are further subdivided into normal and abnormal behaviors. The algorithm is validated on benchmark datasets, as well as on a new dataset specifically designed for interactions analysis."'),
('"Real Time Feature Based 3-D Deformable Face Tracking"', '"ECCV 2008"', '["Interest Point", "Semantic Feature", "Active Appearance Model", "Active Shape Model", "Deformable ', '"https://doi.org/10.1007/978-3-540-88688-4_53"', '"In this paper, we develop a novel framework for 3D tracking of the non-rigid face deformation from a single camera. The difficulty of the problem lies in the fact that 3D deformation parameter estimation becomes unstable when there are few reliable facial features correspondences. Unfortunately, this often occurs in real tracking scenario when there is significant illumination change, motion blur or large pose variation. In order to extract more information of feature correspondences, the proposed framework integrates three types of features which discriminate face deformation across different views: 1) the semantic features which provide constant correspondences between 3D model points and major facial features; 2) the silhouette features which provide dynamic correspondences between 3D model points and facial silhouette under varying views; 3) the online tracking features that provide redundant correspondences between 3D model points and salient image features. The integration of these complementary features is important for robust estimation of the 3D parameters. In order to estimate the high dimensional 3D deformation parameters, we develop a hierarchical parameter estimation algorithm to robustly estimate both rigid and non-rigid 3D parameters. We show the importance of both features fusion and hierarchical parameter estimation for reliable tracking 3D face deformation. Experiments demonstrate the robustness and accuracy of the proposed algorithm especially in the cases of agile head motion, drastic illumination change, and large pose change up to profile view."'),
('"Real-Time 3D Motion Capture by Monocular Vision and Virtual Rendering"', '"ECCV 2012"', '["3D motion capture", "monocular vision", "3D/2D registration", "particle filtering", "real-time com', '"https://doi.org/10.1007/978-3-642-33885-4_77"', '"Avatars in networked 3D virtual environments allow users to interact over the Internet and to get some feeling of virtual telepresence. However, avatar control may be tedious. Motion capture systems based on 3D sensors have recently reached the consumer market, but webcams and camera-phones are more widespread and cheaper. The proposed demonstration aims at animating a user\\u2019s avatar from real time 3D motion capture by monoscopic computer vision, thus allowing virtual telepresence to anyone using a personal computer with a webcam or a camera-phone. This kind of immersion allows new gesture-based communication channels to be opened in a virtual inhabited 3D space."'),
('"Real-Time Accurate Geo-Localization of a MAV with Omnidirectional Visual Odometry and GPS"', '"ECCV 2014"', '["Visual odometry", "Incremental bundle adjustment", "Fisheye camera", "Multi-camera system", "Omnid', '"https://doi.org/10.1007/978-3-319-16178-5_18"', '"This paper presents a system for direct geo-localization of a MAV in an unknown environment using visual odometry and precise real time kinematic (RTK) GPS information. Visual odometry is performed with a multi-camera system with four fisheye cameras that cover a wide field of view which leads to better constraints for localization due to long tracks and a better intersection geometry. Visual observations from the acquired image sequences are refined with a high accuracy on selected keyframes by an incremental bundle adjustment using the iSAM2 algorithm. The optional integration of GPS information yields long-time stability and provides a direct geo-referenced solution. Experiments show the high accuracy which is below 3 cm standard deviation in position."'),
('"Real-Time Camera Tracking: When is High Frame-Rate Best?"', '"ECCV 2012"', '["Pareto Front", "Dense Tracking", "Motion Blur", "Camera Tracking", "Real Camera"]', '"https://doi.org/10.1007/978-3-642-33786-4_17"', '"Higher frame-rates promise better tracking of rapid motion, but advanced real-time vision systems rarely exceed the standard 10\\u201360Hz range, arguing that the computation required would be too great. Actually, increasing frame-rate is mitigated by reduced computational cost per frame in trackers which take advantage of prediction. Additionally, when we consider the physics of image formation, high frame-rate implies that the upper bound on shutter time is reduced, leading to less motion blur but more noise. So, putting these factors together, how are application-dependent performance requirements of accuracy, robustness and computational cost optimised as frame-rate varies? Using 3D camera tracking as our test problem, and analysing a fundamental dense whole image alignment approach, we open up a route to a systematic investigation via the careful synthesis of photorealistic video using ray-tracing of a detailed 3D scene, experimentally obtained photometric response and noise models, and rapid camera motions. Our multi-frame-rate, multi-resolution, multi-light-level dataset is based on tens of thousands of hours of CPU rendering time. Our experiments lead to quantitative conclusions about frame-rate selection and highlight the crucial role of full consideration of physical image formation in pushing tracking performance."'),
('"Real-Time Compressive Tracking"', '"ECCV 2012"', '["Local Binary Pattern", "Tracking Algorithm", "Appearance Model", "Random Projection", "Restricted ', '"https://doi.org/10.1007/978-3-642-33712-3_62"', '"It is a challenging task to develop effective and efficient appearance models for robust object tracking due to factors such as pose variation, illumination change, occlusion, and motion blur. Existing online tracking algorithms often update models with samples from observations in recent frames. While much success has been demonstrated, numerous issues remain to be addressed. First, while these adaptive appearance models are data-dependent, there does not exist sufficient amount of data for online algorithms to learn at the outset. Second, online tracking algorithms often encounter the drift problems. As a result of self-taught learning, these mis-aligned samples are likely to be added and degrade the appearance models. In this paper, we propose a simple yet effective and efficient tracking algorithm with an appearance model based on features extracted from the multi-scale image feature space with data-independent basis. Our appearance model employs non-adaptive random projections that preserve the structure of the image feature space of objects. A very sparse measurement matrix is adopted to efficiently extract the features for the appearance model. We compress samples of foreground targets and the background using the same sparse measurement matrix. The tracking task is formulated as a binary classification via a naive Bayes classifier with online update in the compressed domain. The proposed compressive tracking algorithm runs in real-time and performs favorably against state-of-the-art algorithms on challenging sequences in terms of efficiency, accuracy and robustness."'),
('"Real-Time Emotion Recognition from Natural Bodily Expressions in Child-Robot Interaction"', '"ECCV 2014"', '["Spontaneous emotion recognition", "Child-robot interaction", "Bodily expressions"]', '"https://doi.org/10.1007/978-3-319-16199-0_30"', '"Emotion perception and interpretation is one of the key desired capabilities of assistive robots, which could largely enhance the quality and naturalness in human-robot interaction. According to psychological studies, bodily communication has an important role in human social behaviours. However, it is very challenging to model such affective bodily expressions, especially in a naturalistic setting, considering the variety of expressive patterns, as well as the difficulty of acquiring reliable data. In this paper, we investigate the spontaneous dimensional emotion prediction problem in a child-robot interaction scenario. The paper presents emotion elicitation, data acquisition, 3D skeletal representation, feature design and machine learning algorithms. Experimental results have shown good predictive performance on the variation trends of emotional dimensions, especially the arousal dimension."'),
('"Real-Time Exemplar-Based Face Sketch Synthesis"', '"ECCV 2014"', '["Face Hallucination", "Texture Synthesis"]', '"https://doi.org/10.1007/978-3-319-10599-4_51"', '"This paper proposes a simple yet effective face sketch synthesis method. Similar to existing exemplar-based methods, a training dataset containing photo-sketch pairs is required, and a K-NN photo patch search is performed between a test photo and every training exemplar for sketch patch selection. Instead of using the Markov Random Field to optimize global sketch patch selection, this paper formulates face sketch synthesis as an image denoising problem which can be solved efficiently using the proposed method. Real-time performance can be obtained on a state-of-the-art GPU. Meanwhile quantitative evaluations on face sketch recognition and user study demonstrate the effectiveness of the proposed method. In addition, the proposed method can be directly extended to the temporal domain for consistent video sketch synthesis, which is of great importance in digital entertainment."'),
('"Real-Time Human Pose Tracking from Range Data"', '"ECCV 2012"', '["Motion Capture", "Depth Image", "Body Model", "Iterative Close Point", "Tracking Accuracy"]', '"https://doi.org/10.1007/978-3-642-33783-3_53"', '"Tracking human pose in real-time is a difficult problem with many interesting applications. Existing solutions suffer from a variety of problems, especially when confronted with unusual human poses. In this paper, we derive an algorithm for tracking human pose in real-time from depth sequences based on MAP inference in a probabilistic temporal model. The key idea is to extend the iterative closest points (ICP) objective by modeling the constraint that the observed subject cannot enter free space, the area of space in front of the true range measurements. Our primary contribution is an extension to the articulated ICP algorithm that can efficiently enforce this constraint. The resulting filter runs at 125 frames per second using a single desktop CPU core. We provide extensive experimental results on challenging real-world data, which show that the algorithm outperforms the previous state-of-the-art trackers both in computational efficiency and accuracy."'),
('"Real-Time Image Registration of RGB Webcams and Colorless 3D Time-of-Flight Cameras"', '"ECCV 2012"', '["Color Image", "Distance Image", "Computer Vision Application", "Stereo Vision System", "Deep Plane', '"https://doi.org/10.1007/978-3-642-33885-4_59"', '"In line with the boom of 3D movies and cutting edge technologies, range cameras are increasingly common. Among others, time-of-flight (TOF) cameras give it the ability to capture three-dimensional images that reveal object\\u2019s distances. A shortcoming of these sensors however, lies in that the majority does not provide color information (not even gray). Therefore they are useless in computer vision applications for which color is crucial. The PMD [vision] \\u00ae CamCube 3.0 is one example of an expensive colorless TOF camera. In this work, we attempt the addition of color to this camera by means of inexpensive resources. A regular webcam is stuck on top of the CamCube and its color images are registered into the TOF distance maps. To get this done, we developed an algorithm to enable real-time registration based solely on depth. Thus, this algorithm requires neither intrinsic parameters nor mono-calibration of none of the cameras. We finally show a tracking application in which a stumble is foretold if an object approaches following a threatening trajectory."'),
('"Real-Time Interactive Path Extraction with On-the-Fly Adaptation of the External Forces"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47977-5_53"', '"The aim of this work is to propose an adaptation of optimal path based interactive tools for image segmentation (related to Live-Wire [12] and Intelligent Scissors [18] approaches). We efficiently use both discrete [10] and continuous [6] path search approaches. The segmentation relies on the notion of energy function and we introduce the possibility of complete on-the-fly adaptation of each individual energy term, as well as of their relative weights. Non-specialist users have then a full control of the drawing process which automatically selects the most relevant set of features to steer the path extraction. Tests have been performed on a large variety of medical images."'),
('"Real-Time Minimization of the Piecewise Smooth Mumford-Shah Functional"', '"ECCV 2014"', '["Mumford-Shah functional", "non-convex optimization", "real-time", "primal-dual"]', '"https://doi.org/10.1007/978-3-319-10605-2_9"', '"We propose an algorithm for efficiently minimizing the piecewise smooth Mumford-Shah functional. The algorithm is based on an extension of a recent primal-dual algorithm from convex to non-convex optimization problems. The key idea is to rewrite the proximal operator in the primal-dual algorithm using Moreau\\u2019s identity. The resulting algorithm computes piecewise smooth approximations of color images at 15-20 frames per second at VGA resolution using GPU acceleration. Compared to convex relaxation approaches [18], it is orders of magnitude faster and does not require a discretization of color values. In contrast to the popular Ambrosio-Tortorelli approach [2], it naturally combines piecewise smooth and piecewise constant approximations, it does not require an epsilon-approximation and it is not based on an alternation scheme. The achieved energies are in practice at most 5% off the optimal value for one-dimensional problems. Numerous experiments demonstrate that the proposed algorithm is well-suited to perform discontinuity-preserving smoothing and real-time video cartooning."'),
('"Real-Time Non-rigid Shape Recovery Via Active Appearance Models for Augmented Reality"', '"ECCV 2006"', '["Augmented Reality", "Active Appearance Model", "Augmented Reality Application", "Promising Experim', '"https://doi.org/10.1007/11744023_15"', '"One main challenge in Augmented Reality (AR) applications is to keep track of video objects with their movement, orientation, size, and position accurately. This poses a challenging task to recover non-rigid shape and global pose in real-time AR applications. This paper proposes a novel two-stage scheme for online non-rigid shape recovery toward AR applications using Active Appearance Models (AAMs). First, we construct 3D shape models from AAMs offline, which do not involve processing of the 3D scan data. Based on the computed 3D shape models, we propose an efficient online algorithm to estimate both 3D pose and non-rigid shape parameters via local bundle adjustment for building up point correspondences. Our approach, without manual intervention, can recover the 3D non-rigid shape effectively from either real-time video sequences or single image. The recovered 3D pose parameters can be used for AR registrations. Furthermore, the facial feature can be tracked simultaneously, which is critical for many face related applications. We evaluate our algorithms on several video sequences. Promising experimental results demonstrate our proposed scheme is effective and significant for real-time AR applications."'),
('"Real-Time Person Tracking and Pointing Gesture Recognition for Human-Robot Interaction"', '"CVHCI 2004"', '["Hand Position", "Gesture Recognition", "American Sign", "Head Orientation", "Disparity Information', '"https://doi.org/10.1007/978-3-540-24837-8_4"', '"In this paper, we present our approach for visual tracking of head, hands and head orientation. Given the images provided by a calibrated stereo-camera, color and disparity information are integrated into a multi-hypotheses tracking framework in order to find the 3D-positions of the respective body parts. Based on the hands\\u2019 motion, an HMM-based approach is applied to recognize pointing gestures. We show experimentally, that the gesture recognition performance can be improved significantly by using visually gained information about head orientation as an additional feature. Our system aims at applications in the field of human-robot interaction, where it is important to do run-on recognition in real-time, to allow for robot\\u2019s egomotion and not to rely on manual initialization."'),
('"Real-Time Plane Segmentation and Obstacle Detection of 3D Point Clouds for Indoor Scenes"', '"ECCV 2012"', '["plane segmentation", "point cloud", "obstacle detection"]', '"https://doi.org/10.1007/978-3-642-33868-7_3"', '"Scene analysis is an important issue in computer vision and extracting structural information is one of the fundamental techniques. Taking advantage of depth camera, we propose a novel fast plane segmentation algorithm and use it to detect obstacles in indoor environment. The proposed algorithm has two steps: the initial segmentation and the refined segmentation. Firstly, depth image is converted into 3D point cloud and divided into voxels, which are less sensitive to noises compared with pixels. Then area-growing algorithm is used to extract the candidate planes according to the normal of each voxel. Secondly, each point that hasn\\u2019t been classified to any plane is examined whether it actually belongs to a plane. The two-step strategy has been proven to be a fast segmentation method with high accuracy. The experimental results demonstrate that our method can segment planes and detect obstacles in real-time with high accuracy for indoor scenes."'),
('"Real-Time Shape Analysis of a Human Body in Clothing Using Time-Series Part-Labeled Volumes"', '"ECCV 2008"', '["Body Part", "Latent Variable Model", "Reconstructed Volume", "Visual Hull", "Hierarchical Search"]', '"https://doi.org/10.1007/978-3-540-88690-7_51"', '"We propose a real-time method for simultaneously refining the reconstructed volume of a human body with loose-fitting clothing and identifying body-parts in it. Time-series volumes, which are acquired by a slow but sophisticated 3D reconstruction algorithm, with body-part labels are obtained offline. The time-series sample volumes are represented by trajectories in the eigenspaces using PCA. An input visual hull reconstructed online is projected into the eigenspace and compared with the trajectories in order to find similar high-precision samples with body-part labels. The hierarchical search taking into account 3D reconstruction errors can achieve robust and fast matching. Experimental results demonstrate that our method can refine the input visual hull including loose-fitting clothing and identify its body-parts in real time."'),
('"Real-Time Spatiotemporal Stereo Matching Using the Dual-Cross-Bilateral Grid"', '"ECCV 2010"', '["Stereo Match", "Cost Aggregation", "CIELAB Colour Space", "Stereo Video", "Support Window"]', '"https://doi.org/10.1007/978-3-642-15558-1_37"', '"We introduce a real-time stereo matching technique based on a reformulation of Yoon and Kweon\\u2019s adaptive support weights algorithm [1]. Our implementation uses the bilateral grid to achieve a speedup of 200\\u00d7 compared to a straightforward full-kernel GPU implementation, making it the fastest technique on the Middlebury website. We introduce a colour component into our greyscale approach to recover precision and increase discriminability. Using our implementation, we speed up spatial-depth superresolution 100\\u00d7. We further present a spatiotemporal stereo matching approach based on our technique that incorporates temporal evidence in real time (> 14 fps). Our technique visibly reduces flickering and outperforms per-frame approaches in the presence of image noise. We have created five synthetic stereo videos, with ground truth disparity maps, to quantitatively evaluate depth estimation from stereo video. Source code and datasets are available on our project website."'),
('"Real-Time Specular Highlight Removal Using Bilateral Filtering"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_7"', '"In this paper, we propose a simple but effective specular highlight removal method using a single input image. Our method is based on a key observation - the maximum fraction of the diffuse color component (so called maximum diffuse chromaticity in the literature) in local patches in color images changes smoothly. Using this property, we can estimate the maximum diffuse chromaticity values of the specular pixels by directly applying low-pass filter to the maximum fraction of the color components of the original image, such that the maximum diffuse chromaticity values can be propagated from the diffuse pixels to the specular pixels. The diffuse color at each pixel can then be computed as a nonlinear function of the estimated maximum diffuse chromaticity. Our method can be directly extended for multi-color surfaces if edge-preserving filters (e.g., bilateral filter) are used such that the smoothing can be guided by the maximum diffuse chromaticity. But maximum diffuse chromaticity is to be estimated. We thus present an approximation and demonstrate its effectiveness. Recent development in fast bilateral filtering techniques enables our method to run over 200\\u00d7 faster than the state-of-the-art on a standard CPU and differentiates our method from previous work."'),
('"Real-Time Spherical Mosaicing Using Whole Image Alignment"', '"ECCV 2010"', '["Image Alignment", "Camera Intrinsic Parameter", "Spherical Panorama", "Live Camera", "Perceptual A', '"https://doi.org/10.1007/978-3-642-15558-1_6"', '"When a purely rotating camera observes a general scene, overlapping views are related by a parallax-free warp which can be estimated by direct image alignment methods that iterate to optimise photo-consistency. However, building globally consistent mosaics from video has usually been tackled as an off-line task, while sequential methods suitable for real-time implementation have often suffered from long-term drift. In this paper we present a high performance real-time video mosaicing algorithm based on parallel image alignment via ESM (Efficient Second-order Minimisation) and global optimisation of a map of keyframes over the whole viewsphere. We present real-time results for drift-free camera rotation tracking and globally consistent spherical mosaicing from a variety of cameras in real scenes, demonstrating high global accuracy and the ability to track very rapid rotation while maintaining solid 30Hz operation. We also show that automatic camera calibration refinement can be straightforwardly built into our framework."'),
('"Real-Time Tracking of Multiple Articulated Structures in Multiple Views"', '"ECCV 2000"', '["Visual Servoing", "Multiple Camera", "Rigid Component", "Euclidean Transformation", "Video Feed"]', '"https://doi.org/10.1007/3-540-45053-X_2"', '"This paper describes a highly flexible approach to real-time frame-rate tracking in complex camera and structures configurations, including the use of multiple cameras and the tracking of multiple or articulated targets. A powerful and general method is presented for expressing and solving the constraints which exist in these configurations in a principled manner. This method exploits the geometric structure present in the Lie group and Lie algebra formalism to express the constraints that derive from structures such as hinges or a common ground plane. This method makes use of the adjoint representation to simplify the constraints which are then applied by means of Lagrange multipliers."'),
('"Real-Time Tracking of Multiple Skin-Colored Objects with a Possibly Moving Camera"', '"ECCV 2004"', '["Skin Color", "Prior Probability", "Image Point", "Skin Pixel", "Object Hypothesis"]', '"https://doi.org/10.1007/978-3-540-24672-5_29"', '"This paper presents a method for tracking multiple skin-colored objects in images acquired by a possibly moving camera. The proposed method encompasses a collection of techniques that enable the modeling and detection of skin-colored objects as well as their temporal association in image sequences. Skin-colored objects are detected with a Bayesian classifier which is bootstrapped with a small set of training data. Then, an off-line iterative training procedure is employed to refine the classifier using additional training images. On-line adaptation of skin-color probabilities is used to enable the classifier to cope with illumination changes. Tracking over time is realized through a novel technique which can handle multiple skin-colored objects. Such objects may move in complex trajectories and occlude each other in the field of view of a possibly moving camera. Moreover, the number of tracked objects may vary in time. A prototype implementation of the developed system operates on 320x240 live video in real time (28Hz) on a conventional Pentium 4 processor. Representative experimental results from the application of this prototype to image sequences are also provided."'),
('"Real-Time Tracking with Classifiers"', '"WDV 2006"', '["Support Vector Machine", "Observation Model", "Visual Tracking", "Vehicle Detection", "Vehicle Tra', '"https://doi.org/10.1007/978-3-540-70932-9_17"', '"Two basic facts motivate this paper: (1) particle filter based trackers have become increasingly powerful in recent years, and (2) object detectors using statistical learning algorithms often work at a near real-time rate."'),
('"Real-Time Upper Body Detection and 3D Pose Estimation in Monoscopic Images"', '"ECCV 2006"', '["Mixture Model", "Body Part", "Face Detection", "False Detection", "Integral Image"]', '"https://doi.org/10.1007/11744078_11"', '"This paper presents a novel solution to the difficult task of both detecting and estimating the 3D pose of humans in monoscopic images. The approach consists of two parts. Firstly the location of a human is identified by a probabalistic assembly of detected body parts. Detectors for the face, torso and hands are learnt using adaBoost. A pose likliehood is then obtained using an a priori mixture model on body configuration and possible configurations assembled from available evidence using RANSAC. Once a human has been detected, the location is used to initialise a matching algorithm which matches the silhouette and edge map of a subject with a 3D model. This is done efficiently using chamfer matching, integral images and pose estimation from the initial detection stage. We demonstrate the application of the approach to large, cluttered natural images and at near framerate operation (16fps) on lower resolution video streams."'),
('"Really Quick Shift: Image Segmentation on a GPU"', '"ECCV 2010"', '["super-pixels", "segmentation", "CUDA", "GPU programming"]', '"https://doi.org/10.1007/978-3-642-35740-4_27"', '"The paper presents an exact GPU implementation of the quick shift image segmentation algorithm. Variants of the implementation which use global memory and texture caching are presented, and the paper shows that a method backed by texture caching can produce a 10-50X speedup for practical images, making computation of super-pixels possible at 5-10Hz on modest sized (256x256) images."'),
('"Reasoning about Object Affordances in a Knowledge Base Representation"', '"ECCV 2014"', '["Markov Random Fields", "Categorical Attribute", "Visual Attribute", "Knowledge Graph", "Partial Ob', '"https://doi.org/10.1007/978-3-319-10605-2_27"', '"Reasoning about objects and their affordances is a fundamental problem for visual intelligence. Most of the previous work casts this problem as a classification task where separate classifiers are trained to label objects, recognize attributes, or assign affordances. In this work, we consider the problem of object affordance reasoning using a knowledge base representation. Diverse information of objects are first harvested from images and other meta-data sources. We then learn a knowledge base (KB) using a Markov Logic Network (MLN). Given the learned KB, we show that a diverse set of visual inference tasks can be done in this unified framework without training separate classifiers, including zero-shot affordance prediction and object recognition given human poses."'),
('"Recognition and Segmentation of 3-D Human Action Using HMM and Multi-class AdaBoost"', '"ECCV 2006"', '["Hide Markov Model", "Action Class", "Action Recognition", "Joint Position", "Observation Sequence"', '"https://doi.org/10.1007/11744085_28"', '"Our goal is to automatically segment and recognize basic human actions, such as stand, walk and wave hands, from a sequence of joint positions or pose angles. Such recognition is difficult due to high dimensionality of the data and large spatial and temporal variations in the same action. We decompose the high dimensional 3-D joint space into a set of feature spaces where each feature corresponds to the motion of a single joint or combination of related multiple joints. For each feature, the dynamics of each action class is learned with one HMM. Given a sequence, the observation probability is computed in each HMM and a weak classifier for that feature is formed based on those probabilities. The weak classifiers with strong discriminative power are then combined by the Multi-Class AdaBoost (AdaBoost.M2) algorithm. A dynamic programming algorithm is applied to segment and recognize actions simultaneously. Results of recognizing 22 actions on a large number of motion capture sequences as well as several annotated and automatically tracked sequences show the effectiveness of the proposed algorithms."'),
('"Recognition by Probabilistic Hypothesis Construction"', '"ECCV 2004"', '["Model Feature", "Training Image", "Constellation Model", "Probabilistic Framework", "Scene Image"]', '"https://doi.org/10.1007/978-3-540-24670-1_5"', '"We present a probabilistic framework for recognizing objects in images of cluttered scenes. Hundreds of objects may be considered and searched in parallel. Each object is learned from a single training image and modeled by the visual appearance of a set of features, and their position with respect to a common reference frame. The recognition process computes identity and position of objects in the scene by finding the best interpretation of the scene in terms of learned objects. Features detected in an input image are either paired with database features, or marked as clutters. Each hypothesis is scored using a generative model of the image which is defined using the learned objects and a model for clutter. While the space of possible hypotheses is enormously large, one may find the best hypothesis efficiently \\u2013 we explore some heuristics to do so. Our algorithm compares favorably with state-of-the-art recognition systems."'),
('"Recognition of Facial Attributes Using Adaptive Sparse Representations of Random Patches"', '"ECCV 2014"', '["Sparse representation", "Soft biometrics", "Gender recognition", "Race recognition", "Facial expre', '"https://doi.org/10.1007/978-3-319-16181-5_59"', '"It is well known that some facial attributes \\u2013like soft biometric traits\\u2013 can increase the performance of traditional biometric systems and help recognition based on human descriptions. In addition, other facial attributes \\u2013like facial expressions\\u2013 can be used in human\\u2013computer interfaces, image retrieval, talking heads and human emotion analysis. This paper addresses the problem of automated recognition of facial attributes by proposing a new general approach called Adaptive Sparse Representation of Random Patches (ASR+). In the learning stage, random patches are extracted from representative face images of each class (e.g., in gender recognition \\u2013a two-class problem\\u2013, images of females/males) in order to construct representative dictionaries. In the testing stage, random test patches of the query image are extracted, and for each test patch a dictionary is built concatenating the \\u2018best\\u2019 representative dictionary of each class. Using this adapted dictionary, each test patch is classified following the Sparse Representation Classification (SRC) methodology. Finally, the query image is classified by patch voting. Thus, our approach is able to learn a model for each recognition task dealing with a larger degree of variability in ambient lighting, pose, expression, occlusion, face size and distance from the camera. Experiments were carried out on seven face databases in order to recognize facial expression, gender, race and disguise. Results show that ASR+ deals well with unconstrained conditions, outperforming various representative methods in the literature in many complex scenarios."'),
('"Recognizing Actions across Cameras by Exploring the Correlated Subspace"', '"ECCV 2012"', '["Support Vector Machine", "Action Recognition", "Canonical Correlation Analysis", "Target Domain", ', '"https://doi.org/10.1007/978-3-642-33863-2_34"', '"We present a novel transfer learning approach to cross-camera action recognition. Inspired by canonical correlation analysis (CCA), we first extract the spatio-temporal visual words from videos captured at different views, and derive a correlation subspace as a joint representation for different bag-of-words models at different views. Different from prior CCA-based approaches which simply train standard classifiers such as SVM in the resulting subspace, we explore the domain transfer ability of CCA in the correlation subspace, in which each dimension has a different capability in correlating source and target data. In our work, we propose a novel SVM with a correlation regularizer which incorporates such ability into the design of the SVM. Experiments on the IXMAS dataset verify the effectiveness of our method, which is shown to outperform state-of-the-art transfer learning approaches without taking such domain transfer ability into consideration."'),
('"Recognizing and Tracking Human Action"', '"ECCV 2002"', '["Human motion", "tracking", "shape", "correspondence"]', '"https://doi.org/10.1007/3-540-47969-4_42"', '"Human activity can be described as a sequence of 3D body postures. The traditional approach to recognition and 3D reconstruction of human activity has been to track motion in 3D, mainly using advanced geometric and dynamic models. In this paper we reverse this process. View based activity recognition serves as an input to a human body location tracker with the ultimate goal of 3D reanimation in mind. We demonstrate that specific human actions can be detected from single frame postures in a video sequence. By recognizing the image of a person\\u2019s posture as corresponding to a particular key frame from a set of stored key frames, it is possible to map body locations from the key frames to actual frames. This is achieved using a shape matching algorithm based on qualitative similarity that computes point to point correspondence between shapes, together with information about appearance. As the mapping is from fixed key frames, our tracking does not suffer from the problem of having to reinitialise when it gets lost. It is effectively a closed loop. We present experimental results both for recognition and tracking for a sequence of a tennis player."'),
('"Recognizing City Identity via Attribute Analysis of Geo-tagged Images"', '"ECCV 2014"', '["Geo-tagged image analysis", "attribute", "spatial analysis", "city identity", "urban planning"]', '"https://doi.org/10.1007/978-3-319-10578-9_34"', '"After hundreds of years of human settlement, each city has formed a distinct identity, distinguishing itself from other cities. In this work, we propose to characterize the identity of a city via an attribute analysis of 2 million geo-tagged images from 21 cities over 3 continents. First, we estimate the scene attributes of these images and use this representation to build a higher-level set of 7 city attributes, tailored to the form and function of cities. Then, we conduct the city identity recognition experiments on the geo-tagged images and identify images with salient city identity on each city attribute. Based on the misclassification rate of the city identity recognition, we analyze the visual similarity among different cities. Finally, we discuss the potential application of computer vision to urban planning."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Recognizing Complex Events in Videos by Learning Key Static-Dynamic Evidences"', '"ECCV 2014"', '["Video Event Detection", "Infinite Push", "Key Evidence Selection", "ADMM"]', '"https://doi.org/10.1007/978-3-319-10578-9_44"', '"Complex events consist of various human interactions with different objects in diverse environments. The evidences needed to recognize events may occur in short time periods with variable lengths and can happen anywhere in a video. This fact prevents conventional machine learning algorithms from effectively recognizing the events. In this paper, we propose a novel method that can automatically identify the key evidences in videos for detecting complex events. Both static instances (objects) and dynamic instances (actions) are considered by sampling frames and temporal segments respectively. To compare the characteristic power of heterogeneous instances, we embed static and dynamic instances into a multiple instance learning framework via instance similarity measures, and cast the problem as an Evidence Selective Ranking (ESR) process. We impose \\u21131 norm to select key evidences while using the Infinite Push Loss Function to enforce positive videos to have higher detection scores than negative videos. The Alternating Direction Method of Multipliers (ADMM) algorithm is used to solve the optimization problem. Experiments on large-scale video datasets show that our method can improve the detection accuracy while providing the unique capability in discovering key evidences of each complex event."'),
('"Recognizing Complex Events Using Large Margin Joint Low-Level Event Model"', '"ECCV 2012"', '["Action Recognition", "Event Recognition", "Training Video", "Birthday Party", "MFCC Feature"]', '"https://doi.org/10.1007/978-3-642-33765-9_31"', '"In this paper we address the challenging problem of complex event recognition by using low-level events. In this problem, each complex event is captured by a long video in which several low-level events happen. The dataset contains several videos and due to the large number of videos and complexity of the events, the available annotation for the low-level events is very noisy which makes the detection task even more challenging. To tackle these problems we model the joint relationship between the low-level events in a graph where we consider a node for each low-level event and whenever there is a correlation between two low-level events the graph has an edge between the corresponding nodes. In addition, for decreasing the effect of weak and/or irrelevant low-level event detectors we consider the presence/absence of low-level events as hidden variables and learn a discriminative model by using latent SVM formulation. Using our learned model for the complex event recognition, we can also apply it for improving the detection of the low-level events in video clips which enables us to discover a conceptual description of the video. Thus our model can do complex event recognition and explain a video in terms of low-level events in a single framework. We have evaluated our proposed method over the most challenging multimedia event detection dataset. The experimental results reveals that the proposed method performs well compared to the baseline method. Further, our results of conceptual description of video shows that our model is learned quite well to handle the noisy annotation and surpass the low-level event detectors which are directly trained on the raw features."'),
('"Recognizing Daily Activities in Realistic Environments Through Depth-Based User Tracking and Hidden', '"ECCV 2014"', '["Adl recognition", "User location trajectories", "Posture", "Gestures", "Point-cloud features", "Hi', '"https://doi.org/10.1007/978-3-319-16199-0_57"', '"This paper presents a novel framework for the automatic recognition of Activities of Daily Living (ADLs), such as cooking, eating, dishwashing and watching TV, based on depth video processing and Hidden Conditional Random Fields (HCRFs). Depth video is provided by low-cost RGB-D sensors unobtrusively installed in the house. The user\\u2019s location, posture, as well as point cloud -based features related to gestures are extracted; a standing/sitting posture detector, as well as novel features expressing head and hand gestures are introduced herein. To model the target activities, we employed discriminative HCRFs and compared them to HMMs. Through experimental evaluation, HCRFs outperformed HMMs in location trajectories-based ADL detection. By fusing trajectories data with posture and the proposed gesture features, ADL detection performance was found to further improve, leading to recognition rates at the level of 90.5 % for five target activities in a naturalistic home environment."'),
('"Recognizing Materials from Virtual Examples"', '"ECCV 2012"', '["Domain Adaptation", "Alignment Procedure", "Virtual Data", "Material Database", "Virtual Material"', '"https://doi.org/10.1007/978-3-642-33765-9_25"', '"Due to the strong impact of machine learning methods on visual recognition, performance on many perception task is driven by the availability of sufficient training data. A promising direction which has gained new relevance in recent years is the generation of virtual training examples by means of computer graphics methods in order to provide richer training sets for recognition and detection on real data. Success stories of this paradigm have been mostly reported for the synthesis of shape features and 3D depth maps. Therefore we investigate in this paper if and how appearance descriptors can be transferred from the virtual world to real examples. We study two popular appearance descriptors on the task of material categorization as it is a pure appearance-driven task. Beyond this initial study, we also investigate different approach of combining and adapting virtual and real data in order to bridge the gap between rendered and real-data. Our study is carried out using a new database of virtual materials VIPS that complements the existing KTH-TIPS material database."'),
('"Recognizing Objects in Range Data Using Regional Point Descriptors"', '"ECCV 2004"', '["Point Cloud", "Basis Point", "Recognition Rate", "Spin Image", "Support Region"]', '"https://doi.org/10.1007/978-3-540-24672-5_18"', '"Recognition of three dimensional (3D) objects in noisy and cluttered scenes is a challenging problem in 3D computer vision. One approach that has been successful in past research is the regional shape descriptor. In this paper, we introduce two new regional shape descriptors: 3D shape contexts and harmonic shape contexts. We evaluate the performance of these descriptors on the task of recognizing vehicles in range scans of scenes using a database of 56 cars. We compare the two novel descriptors to an existing descriptor, the spin image, showing that the shape context based descriptors have a higher recognition rate on noisy scenes and that 3D shape contexts outperform the others on cluttered scenes."'),
('"Recognizing Partially Occluded Faces from a Single Sample Per Class Using String-Based Matching"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_36"', '"Automatically recognizing human faces with partial occlusions is one of the most challenging problems in face analysis community. This paper presents a novel string-based face recognition approach to address the partial occlusion problem in face recognition. In this approach, a new face representation, Stringface, is constructed to integrate the relational organization of intermediate-level features (line segments) into a high-level global structure (string). The matching of two faces is done by matching two Stringfaces through a string-to-string matching scheme, which is able to efficiently find the most discriminative local parts (substrings) for recognition without making any assumption on the distributions of the deformed facial regions. The proposed approach is compared against the state-of-the-art algorithms using both the AR database and FRGC (Face Recognition Grand Challenge) ver2.0 database. Very encouraging experimental results demonstrate, for the first time, the feasibility and effectiveness of a high-level syntactic method in face recognition, showing a new strategy for face representation and recognition."'),
('"Recognizing Products: A Per-exemplar Multi-label Image Classification Approach"', '"ECCV 2014"', '["Test Image", "Training Image", "Average Precision", "Genetic Algorithm Optimization", "Fisher Vect', '"https://doi.org/10.1007/978-3-319-10605-2_29"', '"Large-scale instance-level image retrieval aims at retrieving specific instances of objects or scenes. Simultaneously retrieving multiple objects in a test image adds to the difficulty of the problem, especially if the objects are visually similar. This paper presents an efficient approach for per-exemplar multi-label image classification, which targets the recognition and localization of products in retail store images. We achieve runtime efficiency through the use of discriminative random forests, deformable dense pixel matching and genetic algorithm optimization. Cross-dataset recognition is performed, where our training images are taken in ideal conditions with only one single training image per product label, while the evaluation set is taken using a mobile phone in real-life scenarios in completely different conditions. In addition, we provide a large novel dataset and labeling tools for products image search, to motivate further research efforts on multi-label retail products image classification. The proposed approach achieves promising results in terms of both accuracy and runtime efficiency on 680 annotated images of our dataset, and 885 test images of GroZi-120 dataset. We make our dataset of 8350 different product images and the 680 test images from retail stores with complete annotations available to the wider community."'),
('"Recognizing Walking People"', '"ECCV 2000"', '["structure from motion", "calibration", "object recognition"]', '"https://doi.org/10.1007/3-540-45054-8_31"', '"We present a method for recognition of walking people in monocular image sequences based on extraction of coordinates of specific point locations on the body. The method works by comparison of sequences of recorded coordinates with a library of sequences from different individuals. The comparison is based on the evaluation of view invariant and calibration independent view consistency constraints. These constraints are functions of corresponding image coordinates in two views and are satisfied whenever the two views are projected from the same 3D object. By evaluating the view consistency constraints for each pair of frames in a sequence of a walking person and a stored sequence we get a matrix of consistency values that ideally are zero whenever the pair of images depict the same 3D-posture. The method is virtually parameter free and computes a consistency residual between a pair of sequences that can be used as a distance for clustering and classification. Using interactively extracted data we present experimental results that are superior to those of previously published algorithms both in terms of performance and generality."'),
('"Reconstructing 3D Human Pose from 2D Image Landmarks"', '"ECCV 2012"', '["Sparse Representation", "Motion Capture", "Limb Length", "Orthogonal Match Pursuit", "Active Appea', '"https://doi.org/10.1007/978-3-642-33765-9_41"', '"Reconstructing an arbitrary configuration of 3D points from their projection in an image is an ill-posed problem. When the points hold semantic meaning, such as anatomical landmarks on a body, human observers can often infer a plausible 3D configuration, drawing on extensive visual memory. We present an activity-independent method to recover the 3D configuration of a human figure from 2D locations of anatomical landmarks in a single image, leveraging a large motion capture corpus as a proxy for visual memory. Our method solves for anthropometrically regular body pose and explicitly estimates the camera via a matching pursuit algorithm operating on the image projections. Anthropometric regularity (i.e., that limbs obey known proportions) is a highly informative prior, but directly applying such constraints is intractable. Instead, we enforce a necessary condition on the sum of squared limb-lengths that can be solved for in closed form to discourage implausible configurations in 3D. We evaluate performance on a wide variety of human poses captured from different viewpoints and show generalization to novel 3D configurations and robustness to missing data."'),
('"Reconstructing the World\\u2019s Museums"', '"ECCV 2012"', '["Markov Random Field", "Wall Model", "Laser Point", "Indoor Scene", "Constructive Solid Geometry"]', '"https://doi.org/10.1007/978-3-642-33718-5_48"', '"Photorealistic maps are a useful navigational guide for large indoor environments, such as museums and businesses. However, it is impossible to acquire photographs covering a large indoor environment from aerial viewpoints. This paper presents a 3D reconstruction and visualization system to automatically produce clean and well-regularized texture-mapped 3D models for large indoor scenes, from ground-level photographs and 3D laser points. The key component is a new algorithm called \\u201cInverse CSG\\u201d for reconstructing a scene in a Constructive Solid Geometry (CSG) representation consisting of volumetric primitives, which imposes powerful regularization constraints to exploit structural regularities. We also propose several techniques to adjust the 3D model to make it suitable for rendering the 3D maps from aerial viewpoints. The visualization system enables users to easily browse a large scale indoor environment from a bird\\u2019s-eye view, locate specific room interiors, fly into a place of interest, view immersive ground-level panorama views, and zoom out again, all with seamless 3D transitions. We demonstrate our system on various museums, including the Metropolitan Museum of Art in New York City \\u2013 one of the largest art galleries in the world."'),
('"Reconstruction from Projections Using Grassmann Tensors"', '"ECCV 2004"', '["Projective Space", "Linear Subspace", "Image Space", "Diagonal Block", "Generic Projection"]', '"https://doi.org/10.1007/978-3-540-24670-1_28"', '"In this paper a general method is given for reconstruction of a set of feature points in an arbitrary dimensional projective space from their projections into lower dimensional spaces. The method extends the methods applied in the well-studied problem of reconstruction of a set of scene points in \\\\(\\\\mathcal {P}^3\\\\) given their projections in a set of images. In this case, the bifocal, trifocal and quadrifocal tensors are used to carry out this computation. It is shown that similar methods will apply in a much more general context, and hence may be applied to projections from \\\\(\\\\mathcal {P}^n\\\\) to \\\\(\\\\mathcal {P}^m\\\\), which have been used in the analysis of dynamic scenes. For sufficiently many generic projections, reconstruction of the scene is shown to be unique up to projectivity, except in the case of projections onto one-dimensional image spaces (lines)."'),
('"Reconstruction from Uncalibrated Sequences with a Hierarchy of Trifocal Tensors"', '"ECCV 2000"', '["Line Triple", "Bundle Adjustment", "Sweet Spot", "Structure From Motion", "Projective Reconstructi', '"https://doi.org/10.1007/3-540-45054-8_42"', '"This paper considers projective reconstruction with a hierarchical computational structure of trifocal tensors that integrates feature tracking and geometrical validation of the feature tracks. The algorithm was embedded into a system aimed at completely automatic Euclidean reconstruction from uncalibrated handheld amateur video sequences. The algorithm was tested as part of this system on a number of sequences grabbed directly from a low-end video camera without editing. The proposed approach can be considered a generalisation of a scheme of [Fitzgibbon and Zisserman, ECCV \\u201998]. The proposed scheme tries to adapt itself to the motion and frame rate in the sequence by finding good triplets of views from which accurate and unique trifocal tensors can be calculated. This is in contrast to the assumption that three consecutive views in the video sequence are a good choice. Using trifocal tensors with a wider span suppresses error accumulation and makes the scheme less reliant on bundle adjustment. The proposed computational structure may also be used with fundamental matrices as the basic building block."'),
('"Reconstruction of 3-D Symmetric Curves from Perspective Images without Discrete Features"', '"ECCV 2004"', '["Central Plane", "Planar Curf", "Single View", "Shape Error", "Perspective Image"]', '"https://doi.org/10.1007/978-3-540-24672-5_42"', '"The shapes of many natural and man-made objects have curved contours. The images of such contours usually do not have sufficient distinctive features to apply conventional feature-based reconstruction algorithms. This paper shows that both the shape of curves in 3-D space and the camera poses can be accurately reconstructed from their perspective images with unknown point correspondences given that the curves have certain invariant properties such as symmetry. We show that in such cases the minimum number of views needed for a solution is remarkably small: one for planar curves and two for nonplanar curves (of arbitrary shapes), which is significantly less than what is required by most existing algorithms for general curves. Our solutions rely on minimizing the L 2-distance between the shapes of the curves reconstructed via the \\u201cepipolar geometry\\u201d of symmetric curves. Both simulations and experiments on real images are presented to demonstrate the effectiveness of our approach."'),
('"Reconstruction of Canal Surfaces from Single Images Under Exact Perspective"', '"ECCV 2006"', '["Machine Intelligence", "Camera Calibration", "Edge Point", "Tangency Point", "Camera Parameter"]', '"https://doi.org/10.1007/11744023_23"', '"This paper addresses the reconstruction of canal surfaces from single images. A canal surface is obtained as the envelope of a family of spheres of constant radius, whose center is swept along a space curve, called axis. Previous studies either used approximate relationships (quasi-invariants), or they addressed the recognition based on a geometric model. In this paper we show that, under broad conditions, canal surfaces can be reconstructed from single images under exact perspective. In particular, canal surfaces with planar axis can even be reconstructed from a single fully-uncalibrated image. An automatic reconstruction method has been implemented. Simulations and experimental results on real images are also presented."'),
('"Recording and Playback of Camera Shake: Benchmarking Blind Deconvolution with a Real-World Database', '"ECCV 2012"', '["blind deconvolution", "camera shake", "benchmark", "motion blur"]', '"https://doi.org/10.1007/978-3-642-33786-4_3"', '"Motion blur due to camera shake is one of the predominant sources of degradation in handheld photography. Single image blind deconvolution (BD) or motion deblurring aims at restoring a sharp latent image from the blurred recorded picture without knowing the camera motion that took place during the exposure. BD is a long-standing problem, but has attracted much attention recently, cumulating in several algorithms able to restore photos degraded by real camera motion in high quality. In this paper, we present a benchmark dataset for motion deblurring that allows quantitative performance evaluation and comparison of recent approaches featuring non-uniform blur models. To this end, we record and analyse real camera motion, which is played back on a robot platform such that we can record a sequence of sharp images sampling the six dimensional camera motion trajectory. The goal of deblurring is to recover one of these sharp images, and our dataset contains all information to assess how closely various algorithms approximate that goal. In a comprehensive comparison, we evaluate state-of-the-art single image BD algorithms incorporating uniform and non-uniform blur models."'),
('"Recovering Light Directions and Camera Poses from a Single Sphere"', '"ECCV 2008"', '["Augmented Reality", "Light Direction", "Sphere Center", "Single Sphere", "Texture Scene"]', '"https://doi.org/10.1007/978-3-540-88682-2_48"', '"This paper introduces a novel method for recovering both the light directions and camera poses from a single sphere. Traditional methods for estimating light directions using spheres either assume both the radius and center of the sphere being known precisely, or they depend on multiple calibrated views to recover these parameters. It will be shown in this paper that the light directions can be uniquely determined from the specular highlights observed in a single view of a sphere without knowing or recovering the exact radius and center of the sphere. Besides, if the sphere is being observed by multiple cameras, its images will uniquely define the translation vector of each camera from a common world origin centered at the sphere center. It will be shown that the relative rotations between the cameras can be recovered using two or more light directions estimated from each view. Closed form solutions for recovering the light directions and camera poses are presented, and experimental results on both synthetic and real data show the practicality of the proposed method."'),
('"Recovering Local Shape of a Mirror Surface from Reflection of a Regular Grid"', '"ECCV 2004"', '["Image Plane", "Mirror Surface", "Checkerboard Pattern", "Scene Point", "Specular Surface"]', '"https://doi.org/10.1007/978-3-540-24672-5_37"', '"We present a new technique to recover the shape of an unknown smooth specular surface from a single image. A calibrated camera faces a specular surface reflecting a calibrated scene (for instance a checkerboard or grid pattern). The mapping from the scene pattern to its reflected distorted image in the camera changes the local geometrical structure of the scene pattern. We show that if measurements of both local orientation and scale of the distorted scene in the image plane are available, this mapping can be inverted. Specifically, we prove that surface position and shape up to third order can be derived as a function of such local measurements when two orientations are available at the same point (e.g. a corner). Our results generalize previous work [1, 2] where the mirror surface geometry was recovered only up to first order from at least three intersecting lines. We validate our theoretical results with both numerical simulations and experiments with real surfaces."'),
('"Recovering Scene Geometry under Wavy Fluid via Distortion and Defocus Analysis"', '"ECCV 2014"', '["underwater 3D reconstruction", "dynamic fluid surface recovery", "refractive blur", "distortion", ', '"https://doi.org/10.1007/978-3-319-10602-1_16"', '"In this paper, we consider scenes that are immersed in transparent refractive media with a dynamic surface. We take the first steps to reconstruct both the 3D fluid surface shape and the 3D structure of immersed scene simultaneously by utilizing distortion and defocus clues. We demonstrate that the images captured through a refractive dynamic fluid surface are the distorted and blurred versions of all-in-focused (AIF) images captured through a flat fluid surface. The amounts of distortion and refractive blur are formulated by the shape of fluid surface, scene depth and camera parameters, based on our refractive geometry model of a finite aperture imaging system. An iterative optimization algorithm is proposed to reconstruct the distortion and immersed scene depth, which are then used to infer the 3D fluid surface. We validate and demonstrate the effectiveness of our approach on a variety of synthetic and real scenes under different fluid surfaces."'),
('"Recovering Surfaces from the Restoring Force"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_40"', '"We present a new theoretical method and experimental results for direct recovery of the curvatures, the principal curvature directions, and the surface itself by explicit integration of the Gauss map. The method does not rely on polygonal approximations, smoothing of the data, or model fitting. It is based on the observation that one can recover the surface restoring force from the Gauss map, and (i) applies to orientable surfaces of arbitrary topology (not necessarily closed); (ii) uses only first order linear differential equations; (iii) avoids the use of unstable computations; (iv) provides tools for filtering noise from the sampled data. The method can be used for stable extraction of surfaces and surface shape invariants, in particular, in applications requiring accurate quantitative measurements."'),
('"Recovery of Reflectances and Varying Illuminants from Multiple Views"', '"ECCV 2002"', '["Gray Level", "Surface Element", "Multiple Image", "Multiple View", "Recovery Error"]', '"https://doi.org/10.1007/3-540-47977-5_11"', '"We introduce a new methodology for radiometric reconstruction from multiple images. It opens new possibilities because it allows simultaneous recovery of varying unknown illuminants (one per image), surface albedoes, and cameras\\u2019 radiometric responses. Designed to complement geometric reconstruction techniques, it only requires as input the geometry of the scene and of the cameras. Unlike photometric stereo approaches, it is not restricted to images taken from a single viewpoint. Linear and non-linear implementations in the Lambertian case are proposed; simulation results are discussed and compared to related work to demonstrate the gain in stability; and results on real images are shown."'),
('"Recursive Bilateral Filtering"', '"ECCV 2012"', '["Grayscale Image", "Stereo Match", "Color Edge", "Bilateral Filter", "Tone Mapping"]', '"https://doi.org/10.1007/978-3-642-33718-5_29"', '"This paper proposes a recursive implementation of the bilateral filter. Unlike previous methods, this implementation yields an bilateral filter whose computational complexity is linear in both input size and dimensionality. The proposed implementation demonstrates that the bilateral filter can be as efficient as the recent edge-preserving filtering methods, especially for high-dimensional images. Let the number of pixels contained in the image be N, and the number of channels be D, the computational complexity of the proposed implementation will be O(ND). It is more efficient than the state-of-the-art bilateral filtering methods that have a computational complexity of O(ND 2) [1] (linear in the image size but polynomial in dimensionality) or O(Nlog(N)D) [2] (linear in the dimensionality thus faster than [1] for high-dimensional filtering). Specifically, the proposed implementation takes about 43 ms to process a one megapixel color image (and about 14 ms to process a 1 megapixel grayscale image) which is about 18 \\u00d7 faster than [1] and 86\\u00d7 faster than [2]. The experiments were conducted on a MacBook Air laptop computer with a 1.8 GHz Intel Core i7 CPU and 4 GB memory. The memory complexity of the proposed implementation is also low: as few as the image memory will be required (memory for the images before and after filtering is excluded). This paper also derives a new filter named gradient domain bilateral filter from the proposed recursive implementation. Unlike the bilateral filter, it performs bilateral filtering on the gradient domain. It can be used for edge-preserving filtering but avoids sharp edges that are observed to cause visible artifacts in some computer graphics tasks. The proposed implementations were proved to be effective for a number of computer vision and computer graphics applications, including stylization, tone mapping, detail enhancement and stereo matching."'),
('"Recursive Coarse-to-Fine Localization for Fast Object Detection"', '"ECCV 2010"', '["Object Detection", "Object Model", "Resolution Level", "Human Detection", "Feature Resolution"]', '"https://doi.org/10.1007/978-3-642-15567-3_21"', '"Cascading techniques are commonly used to speed-up the scan of an image for object detection. However, cascades of detectors are slow to train due to the high number of detectors and corresponding thresholds to learn. Furthermore, they do not use any prior knowledge about the scene structure to decide where to focus the search. To handle these problems, we propose a new way to scan an image, where we couple a recursive coarse-to-fine refinement together with spatial constraints of the object location. For doing that we split an image into a set of uniformly distributed neighborhood regions, and for each of these we apply a local greedy search over feature resolutions. The neighborhood is defined as a scanning region that only one object can occupy. Therefore the best hypothesis is obtained as the location with maximum score and no thresholds are needed. We present an implementation of our method using a pyramid of HOG features and we evaluate it on two standard databases, VOC2007 and INRIA dataset. Results show that the Recursive Coarse-to-Fine Localization (RCFL) achieves a 12x speed-up compared to standard sliding windows. Compared with a cascade of multiple resolutions approach our method has slightly better performance in speed and Average-Precision. Furthermore, in contrast to cascading approach, the speed-up is independent of image conditions, the number of detected objects and clutter."'),
('"Recursive Structure from Motion Using Hybrid Matching Constraints with Error Feedback"', '"WDV 2006"', '["Motion Estimation", "Linear Constraint", "Correction Step", "Translational Velocity", "Point Corre', '"https://doi.org/10.1007/978-3-540-70932-9_22"', '"We propose an algorithm for recursive estimation of structure and motion in rigid body perspective dynamic systems, based on the novel concept of continuous-differential matching constraints for the estimation of the velocity parameters. The parameter estimation procedure is fused with a continuous-discrete extended Kalman filter for the state estimation. Also, the structure and motion estimation processes are connected by a reprojection error constraint, where feedback of the structure estimates is used to recursively obtain corrections to the motion parameters, leading to more accurate estimates and a more robust performance of the method. The main advantages of the presented algorithm are that after initialization, only three observed object point correspondences between consecutive pairs of views are required for the sequential motion estimation, and that both the parameter update and the correction step are performed using linear constraints only. Simulated experiments are provided to demonstrate the performance of the method."'),
('"Reduced Analytical Dependency Modeling for Classifier Fusion"', '"ECCV 2012"', '["Dependency modeling", "analytical function", "classifier fusion", "pattern classification"]', '"https://doi.org/10.1007/978-3-642-33712-3_57"', '"This paper addresses the independent assumption issue in classifier fusion process. In the last decade, dependency modeling techniques were developed under some specific assumptions which may not be valid in practical applications. In this paper, using analytical functions on posterior probabilities of each feature, we propose a new framework to model dependency without those assumptions. With the analytical dependency model (ADM), we give an equivalent condition to the independent assumption from the properties of marginal distributions, and show that the proposed ADM can model dependency. Since ADM may contain infinite number of undetermined coefficients, we further propose a reduced form of ADM, based on the convergent properties of analytical functions. Finally, under the regularized least square criterion, an optimal Reduced Analytical Dependency Model (RADM) is learned by approximating posterior probabilities such that all training samples are correctly classified. Experimental results show that the proposed RADM outperforms existing classifier fusion methods on Digit, Flower, Face and Human Action databases."'),
('"Refining Mitochondria Segmentation in Electron Microscopy Imagery with Active Surfaces"', '"ECCV 2014"', '["Energy Function", "Image Gradient", "Initial Segmentation", "Mitochondrion Membrane", "Laplacian S', '"https://doi.org/10.1007/978-3-319-16220-1_26"', '"We present an active surface-based method for refining the boundary surfaces of mitochondria segmentation data. We exploit thefact that mitochondria have thick dark membranes, so referencing the image data at the inner membrane can help drive a more accurate delineation of the outer membrane surface. Given the initial boundary prediction from a machine learning-based segmentation algorithm as input, we compare several cost functions used to drive an explicit update scheme to locally refine 3D mesh surfaces, and results are presented on electron microscopy imagery. Our resulting surfaces are seen to fit very accurately to the mitochondria membranes, more accurately even than the available hand-annotations of the data."'),
('"Reflectance and Natural Illumination from a Single Image"', '"ECCV 2012"', '["Ground Truth", "Single Image", "Lighting Environment", "Photometric Stereo", "Functional Principal', '"https://doi.org/10.1007/978-3-642-33783-3_42"', '"Estimating reflectance and natural illumination from a single image of an object of known shape is a challenging task due to the ambiguities between reflectance and illumination. Although there is an inherent limitation in what can be recovered as the reflectance band-limits the illumination, explicitly estimating both is desirable for many computer vision applications. Achieving this estimation requires that we derive and impose strong constraints on both variables. We introduce a probabilistic formulation that seamlessly incorporates such constraints as priors to arrive at the maximum a posteriori estimates of reflectance and natural illumination. We begin by showing that reflectance modulates the natural illumination in a way that increases its entropy. Based on this observation, we impose a prior on the illumination that favors lower entropy while conforming to natural image statistics. We also impose a prior on the reflectance based on the directional statistics BRDF model that constrains the estimate to lie within the bounds and variability of real-world materials. Experimental results on a number of synthetic and real images show that the method is able to achieve accurate joint estimation for different combinations of materials and lighting."'),
('"Reformulating and Optimizing the Mumford-Shah Functional on a Graph \\u2014 A Faster, Lower Energy S', '"ECCV 2008"', '["Gradient Descent", "Contour Evolution", "Contour Location", "Contour Optimization", "Lower Energy ', '"https://doi.org/10.1007/978-3-540-88682-2_20"', '"Active contour formulations predominate current minimization of the Mumford-Shah functional (MSF) for image segmentation and filtering. Unfortunately, these formulations necessitate optimization of the contour by evolving via gradient descent, which is known for its sensitivity to initialization and the tendency to produce undesirable local minima. In order to reduce these problems, we reformulate the corresponding MSF on an arbitrary graph and apply combinatorial optimization to produce a fast, low-energy solution. The solution provided by this graph formulation is compared with the solution computed via traditional narrow-band level set methods. This comparison demonstrates that our graph formulation and optimization produces lower energy solutions than gradient descent based contour evolution methods in significantly less time. Finally, by avoiding evolution of the contour via gradient descent, we demonstrate that our optimization of the MSF is capable of evolving the contour with non-local movement."'),
('"Refraction Wiggles for Measuring Fluid Depth and Velocity from Video"', '"ECCV 2014"', '["Particle Image Velocimetry", "Camera Center", "Brightness Constancy", "Background Orient Schlieren', '"https://doi.org/10.1007/978-3-319-10578-9_50"', '"We present principled algorithms for measuring the velocity and 3D location of refractive fluids, such as hot air or gas, from natural videos with textured backgrounds. Our main observation is that intensity variations related to movements of refractive fluid elements, as observed by one or more video cameras, are consistent over small space-time volumes. We call these intensity variations \\u201crefraction wiggles\\u201d, and use them as features for tracking and stereo fusion to recover the fluid motion and depth from video sequences. We give algorithms for 1) measuring the (2D, projected) motion of refractive fluids in monocular videos, and 2) recovering the 3D position of points on the fluid from stereo cameras. Unlike pixel intensities, wiggles can be extremely subtle and cannot be known with the same level of confidence for all pixels, depending on factors such as background texture and physical properties of the fluid. We thus carefully model uncertainty in our algorithms for robust estimation of fluid motion and depth. We show results on controlled sequences, synthetic simulations, and natural videos. Different from previous approaches for measuring refractive flow, our methods operate directly on videos captured with ordinary cameras, do not require auxiliary sensors, light sources or designed backgrounds, and can correctly detect the motion and location of refractive fluids even when they are invisible to the naked eye."'),
('"Refractive Calibration of Underwater Cameras"', '"ECCV 2012"', '["Color Correction", "World Coordinate System", "Radial Distortion", "Perspective Camera", "Camera C', '"https://doi.org/10.1007/978-3-642-33715-4_61"', '"In underwater computer vision, images are influenced by the water in two different ways. First, while still traveling through the water, light is absorbed and scattered, both of which are wavelength dependent, thus create the typical green or blue hue in underwater images. Secondly, when entering the underwater housing, the rays are refracted, affecting image formation geometrically. When using underwater images in for example Structure-from-Motion applications, both effects need to be taken into account. Therefore, we present a novel method for calibrating the parameters of an underwater camera housing. An evolutionary optimization algorithm is coupled with an analysis-by-synthesis approach, which allows to calibrate the parameters of a light propagation model for the local water body. This leads to a highly accurate calibration method for camera-glass distance and glass normal with respect to the optical axis. In addition, a model for the distance dependent effect of water on light propagation is parametrized and can be used for color correction."'),
('"Region Covariance: A Fast Descriptor for Detection and Classification"', '"ECCV 2006"', '["Covariance Matrice", "Object Detection", "IEEE Conf", "Illumination Change", "Integral Image"]', '"https://doi.org/10.1007/11744047_45"', '"We describe a new region descriptor and apply it to two problems, object detection and texture classification. The covariance of d-features, e.g., the three-dimensional color vector, the norm of first and second derivatives of intensity with respect to x and y, etc., characterizes a region of interest. We describe a fast method for computation of covariances based on integral images. The idea presented here is more general than the image sums or histograms, which were already published before, and with a series of integral images the covariances are obtained by a few arithmetic operations. Covariance matrices do not lie on Euclidean space, therefore we use a distance metric involving generalized eigenvalues which also follows from the Lie group structure of positive definite matrices. Feature matching is a simple nearest neighbor search under the distance metric and performed extremely rapidly using the integral images. The performance of the covariance features is superior to other methods, as it is shown, and large rotations and illumination changes are also absorbed by the covariance matrix."'),
('"Region Graphs for Organizing Image Collections"', '"ECCV 2010"', '["Convex Hull", "Edge Weight", "Graph Construction", "Image Collection", "Region Graph"]', '"https://doi.org/10.1007/978-3-642-35740-4_19"', '"In this paper we consider large image collections and their organization into meaningful data structures upon which applications can be build (e.g. navigation or reconstruction). In contrast to structures that only reflect local relationships between pairs of images we propose to account for the information an image brings to a collection with respect to all other images. Our approach builds on abstracting from image domains and focusing on image regions, thereby reducing the influence of outliers and background clutter. We introduce a graph structure based on these regions which encodes the overlap between them. The contribution of an image to a collection is then related to the amount of overlap of its regions with the other images in the collection. We demonstrate our graph based structure with several applications: image set reduction, canonical view selection and image-based navigation. The data sets used in our experiments range from small examples to large image collections with thousands of images."'),
('"Region Matching with Missing Parts"', '"ECCV 2002"', '["shape", "variational", "registration", "missing part", "inpainting"]', '"https://doi.org/10.1007/3-540-47977-5_4"', '"We present a variational approach to the problem of registering planar shapes despite missing parts. Registration is achieved through the evolution of a partial differential equation that simultaneously estimates the shape of the missing region, the underlying \\u201ccomplete shape\\u201d and the collection of group elements (Euclidean or affine) corresponding to the registration. Our technique applies both to shapes, for instance represented as characteristic functions (binary images), and to grayscale images, where all intensity levels evolve simultaneously in a partial differential equation. It can therefore be used to perform \\u201cregion inpainting\\u201d and to register collections of images despite occlusions. The novelty of the approach lies on the fact that, rather than estimating the missing region in each image independently, we pose the problem as a joint registration with respect to an underlying \\u201ccomplete shape\\u201d from which the complete version of the original data is obtained via a group action."'),
('"Region-Based 2D Deformable Generalized Cylinder for Narrow Structures Segmentation"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_29"', '"In this paper, we present a region-based deformable cylinder model, extending the work on classical region-based active contours and gradient-based ribbon snakes. Defined by a central curve playing the role of the medial axis and a variable thickness, the model is endowed with a region-dependent term.This energy follows the narrow band principle, in order to handle local region properties while overcoming limitations of classical edge-based models. The energy is subsequently transformed and derived in order to allow implementation on a polygonal line deformed with gradient descent. The model is used to extract path-like objects in medical and aerial images."'),
('"Region-Based Object Recognition Using Shape-from-Shading"', '"ECCV 2000"', '["Object Recognition", "Recognition Rate", "Recognition Performance", "Shape Index", "Region Size"]', '"https://doi.org/10.1007/3-540-45054-8_30"', '"This paper investigates whether regions of uniform surface topography can be extracted from intensity images using shape-from-shading and subsequently used for the purposes of 3D object recognition. We draw on the constant shape-index maximal patch representation of Dorai and Jain. We commence by showing that the resulting shape-index regions are stable under different viewing angles. Based on this observation we investigate the effectiveness of various structural representations and region attributes for 3D object recognition. We show that region curvedness and a string ordering of the regions according to size provides recognition accuracy of about 96%. By polling various recognition schemes, including a graph matching method, we show that a recognition rate of 98\\u201399% is achievable."'),
('"Region-Based Segmentation on Evolving Surfaces with Application to 3D Reconstruction of Shape and P', '"ECCV 2004"', '["variational methods", "Mumford-Shah functional", "image segmentation", "multi-view stereo", "level', '"https://doi.org/10.1007/978-3-540-24671-8_9"', '"We consider the problem of estimating the shape and radiance of a scene from a calibrated set of images under the assumption that the scene is Lambertian and its radiance is piecewise constant. We model the radiance segmentation explicitly using smooth curves on the surface that bound regions of constant radiance. We pose the scene reconstruction problem in a variational framework, where the unknowns are the surface, the radiance values and the segmenting curves. We propose an iterative procedure to minimize a global cost functional that combines geometric priors on both the surface and the curves with a data fitness score. We carry out the numerical implementation in the level set framework."'),
('"Registration and Modeling of Elastic Deformations of Fingerprints"', '"BioAW 2004"', '["Elastic Deformation", "Point Spread Function", "Active Contour", "Fingerprint Image", "Distortion ', '"https://doi.org/10.1007/978-3-540-25976-3_8"', '"We apply the methods of the theory of elasticity for two important problems in fingerprint based authentication: (1) registration of deformations up to the level of pixel-wise correspondence of two images; (2) parametric modeling and exact measurement of the natural deformations. The approach is based on the numerical solution of Navier linear PDE, the registration being provided even for the cases of significant losses and errors in the initial correspondences of minutiae that may be caused by various noise and distortion factors. Relatively compact and theoretically grounded model of the deformations is proposed, which allows to obtain the estimations of discrepancies in the most extreme cases."'),
('"Registration Assisted Image Smoothing and Segmentation"', '"ECCV 2002"', '["Variational Principle", "Image Segmentation", "Image Registration", "Source Image", "Target Image"', '"https://doi.org/10.1007/3-540-47979-1_37"', '"Image segmentation is a fundamental problem in Image Processing, Computer Vision and Medical Imaging with numerous applications. In this paper, we address the atlas-based image segmentation problem which involves registration of the atlas to the subject or target image in order to achieve the segmentation of the target image. Thus, the target image is segmented with the assistance of a registration process. We present a novel variational formulation of this registration assisted image segmentation problem which leads to solving a coupled set of nonlinear PDEs that are solved using efficient numerical schemes. Our work is a departure from earlier methods in that we have a unified variational principle wherein registration and segmentation are simultaneously achieved. We present several 2D examples on synthetic and real data sets along with quantitative accuracy estimates of the registration."'),
('"Registration with a Moving Zoom Lens Camera for Augmented Reality Applications"', '"ECCV 2000"', '["Augmented Reality", "Interest Point", "Camera Motion", "Invariant Point", "Internal Parameter"]', '"https://doi.org/10.1007/3-540-45053-X_37"', '"We focus in this paper on the problem of adding computer-generated objects in video sequences that have been shot with a zoom lens camera. While numerous papers have been devoted to registration with fixed focal length, little attention has been brought to zoom lens cameras. In this paper, we propose an efficient two-stage algorithm for handling zoom changing which are are likely to happen in a video sequence. We first attempt to partition the video into camera motions and zoom variations. Then, classical registration methods are used on the image frames labeled camera motion while keeping the internal parameters constant, whereas the zoom parameters are only updated for the frames labeled zoom variations. Results are presented demonstrating registration on various sequences. Augmented video sequences are also shown."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Regular Texture Analysis as Statistical Model Selection"', '"ECCV 2008"', '["Interest Point", "Marginal Likelihood", "Lattice Hypothesis", "Regular Texture", "Statistical Mode', '"https://doi.org/10.1007/978-3-540-88693-8_18"', '"An approach to the analysis of images of regular texture is proposed in which lattice hypotheses are used to define statistical models. These models are then compared in terms of their ability to explain the image. A method based on this approach is described in which lattice hypotheses are generated using analysis of peaks in the image autocorrelation function, statistical models are based on Gaussian or Gaussian mixture clusters, and model comparison is performed using the marginal likelihood as approximated by the Bayes Information Criterion (BIC). Experiments on public domain regular texture images and a commercial textile image archive demonstrate substantially improved accuracy compared to two competing methods. The method is also used for classification of texture images as regular or irregular. An application to thumbnail image extraction is discussed."'),
('"Regularised Range Flow"', '"ECCV 2000"', '["range flow", "range image sequences", "regularisation", "shape", "visual motion"]', '"https://doi.org/10.1007/3-540-45053-X_50"', '"Extending a differential total least squares method for range flow estimation we present an iterative regularisation approach to compute dense range flow fields. We demonstrate how this algorithm can be used to detect motion discontinuities. This can can be used to segment the data into independently moving regions. The different types of aperture problem encountered are discussed. Our regularisation scheme then takes the various types of flow vectors and combines them into a smooth flow field within the previously segmented regions. A quantitative performance analysis is presented on both synthetic and real data. The proposed algorithm is also applied to range data from castor oil plants obtained with the Biris laser range sensor to study the 3-D motion of plant leaves."'),
('"Regularized Bayesian Metric Learning for Person Re-identification"', '"ECCV 2014"', '["Person re-identification", "Metric learning", "Regularization"]', '"https://doi.org/10.1007/978-3-319-16199-0_15"', '"Person re-identification across disjoint cameras has attracted increasing interest in computer vision due to its wide potential applications in visual surveillance. In this paper, we propose a new regularized Bayesian metric learning (RBML) method for person re-identification. While numerous metric learning methods have been proposed for person re-identification in recent years, most of them suffer from the small sample size (SSS) problem because there are not enough training samples in most practical person re-identification systems, so that the within-class and between-class variations can be well estimated to learn the distance metric. To address this, we propose a RBML method to model and regulate the eigen-spectrums of these two covariance matrices in a parametric manner, so that discriminative information can be better exploited. Experimental results on three widely used datasets demonstrate the advantage of our proposed RBML over the state-of-the-art person re-identification methods."'),
('"Regularized Partial Matching of Rigid Shapes"', '"ECCV 2008"', '["Hausdorff Distance", "Iterative Close Point", "Rigid Motion", "Partial Match", "Boundary Length"]', '"https://doi.org/10.1007/978-3-540-88688-4_11"', '"Matching of rigid shapes is an important problem in numerous applications across the boundary of computer vision, pattern recognition and computer graphics communities. A particularly challenging setting of this problem is partial matching, where the two shapes are dissimilar in general, but have significant similar parts. In this paper, we show a rigorous approach allowing to find matching parts of rigid shapes with controllable size and regularity. The regularity term we use is similar to the spirit of the Mumford-Shah functional, extended to non-Euclidean spaces. Numerical experiments show that the regularized partial matching produces better results compared to the non-regularized one."'),
('"Regularized Shock Filters and Complex Diffusion"', '"ECCV 2002"', '["Shock filters", "deblurring", "denoising", "image enhancement", "complex diffusion", "image featur', '"https://doi.org/10.1007/3-540-47969-4_27"', '"We address the issue of regularizing Osher and Rudin\\u2019s shock filter, used for image deblurring, in order to allow processes that are more robust against noise. Previous solutions to the problem suggested adding some sort of diffusion term to the shock equation. We analyze and prove some properties of coupled shock and diffusion processes. Finally we propose an original solution of adding a complex diffusion term to the shock equation. This new term is used to smooth out noise and indicate inflection points simultaneously. The imaginary value, which is an approximated smoothed second derivative scaled by time, is used to control the process. This results in a robust deblurring process that performs well also on noisy signals."'),
('"Relating Things and Stuff by High-Order Potential Modeling"', '"ECCV 2012"', '["Conditional Random Field", "Image Element", "Object Segmentation", "Object Instance", "Energy Mini', '"https://doi.org/10.1007/978-3-642-33885-4_30"', '"In the last few years, substantially different approaches have been adopted for segmenting and detecting \\u201cthings\\u201d (object categories that have a well defined shape such as people and cars) and \\u201cstuff\\u201d (object categories which have an amorphous spatial extent such as grass and sky). This paper proposes a framework for scene understanding that relates both things and stuff by using a novel way of modeling high order potentials. This representation allows us to enforce labelling consistency between hypotheses of detected objects (things) and image segments (stuff) in a single graphical model. We show that an efficient graph-cut algorithm can be used to perform maximum a posteriori (MAP) inference in this model. We evaluate our method on the Stanford dataset [1] by comparing it against state-of-the-art methods for object segmentation and detection."'),
('"Relationship Between Visual Complexity and Aesthetics: Application to Beauty Prediction of Photos"', '"ECCV 2014"', '["Aesthetic assessment", "Visual complexity", "Beauty prediction"]', '"https://doi.org/10.1007/978-3-319-16178-5_2"', '"Automatic evaluation of visual content by its aesthetic merit is becoming exceedingly important as the available volume of such content is expanding rapidly. Complexity is believed to be an important indicator of aesthetic assessment and widely used. However, psychological theories concerning complexity are only verified on limited situations, and the relationship between complexity and aesthetic experience on extensive scope of application is not yet clear. To this end, we designed an experiment to test human perception on the complexity of various photos. Then we propose a set of visual complexity features and show that the complexity level calculated from the proposed features have a near-monotonic relationship with human beings\\u2019 beauty expectation on thousands of photos. Further applications on beauty predication and quality assessment demonstrate the effectiveness of proposed method."'),
('"Relative Bundle Adjustment Based on Trifocal Constraints"', '"ECCV 2010"', '["Kinematic Chain", "Object Point", "Camera Parameter", "Camera Position", "Bundle Adjustment"]', '"https://doi.org/10.1007/978-3-642-35740-4_22"', '"In this paper we propose a novel approach to bundle adjustment for large-scale camera configurations. The method does not need to include the 3D points in the optimization as parameters. Additionally, we model the parameters of a camera only relative to a nearby camera to achieve a stable estimation of all cameras. This guarantees to yield a normal equation system with a numerical condition, which practically is independent of the number of images. Secondly, instead of using the classical perspective relation between object point, camera and image point, we use epipolar and trifocal constraints to implicitly establish the relations between the cameras via the object structure. This avoids the explicit reference to 3D points thereby handling points far from the camera in a numerically stable fashion. We demonstrate the resulting stability and high convergence rates using synthetic and real data."'),
('"Relative Camera Localisation in Non-overlapping Camera Networks Using Multiple Trajectories"', '"ECCV 2012"', '["Particle Swarm Optimisation", "Calibration Parameter", "Synthetic Dataset", "Metropolis Algorithm"', '"https://doi.org/10.1007/978-3-642-33885-4_15"', '"In this article we present an automatic camera calibration algorithm using multiple trajectories in a multiple camera network with non-overlapping field-of-views (FOV). Visible trajectories within a camera FOV are assumed to be measured with respect to the camera local co-ordinate system. Calibration is performed by aligning each camera local co-ordinate system with a pre-defined global co-ordinate system using three steps. Firstly, extrinsic pair-wise calibration parameters are estimated using particle swarm optimisation and Kalman filtering. The resulting pair-wise calibration estimates are used to generate an initial estimate of network calibration parameters, which are corrected to account for accumulation errors using particle swarm optimisation-based local search. Finally, a Bayesian framework with Metropolis algorithm is adopted and the posterior distribution over the network calibration parameters are estimated. We validate our algorithm using studio and synthetic datasets and compare our approach with existing state-of-the-art algorithms."'),
('"Relative Pose Estimation and Fusion of Omnidirectional and Lidar Cameras"', '"ECCV 2014"', '["Omnidirectional camera", "Lidar", "Pose estimation", "Fusion"]', '"https://doi.org/10.1007/978-3-319-16181-5_49"', '"This paper presents a novel approach for the extrinsic parameter estimation of omnidirectional cameras with respect to a 3D Lidar coordinate frame. The method works without specific setup and calibration targets, using only a pair of 2D-3D data. Pose estimation is formulated as a 2D-3D nonlinear shape registration task which is solved without point correspondences or complex similarity metrics. It relies on a set of corresponding regions, and pose parameters are obtained by solving a small system of nonlinear equations. The efficiency and robustness of the proposed method was confirmed on both synthetic and real data in urban environment."'),
('"Relaxed Pairwise Learned Metric for Person Re-identification"', '"ECCV 2012"', '["Linear Discriminant Analysis", "Image Pair", "Camera View", "Match Rate", "Person Image Pair"]', '"https://doi.org/10.1007/978-3-642-33783-3_56"', '"Matching persons across non-overlapping cameras is a rather challenging task. Thus, successful methods often build on complex feature representations or sophisticated learners. A recent trend to tackle this problem is to use metric learning to find a suitable space for matching samples from different cameras. However, most of these approaches ignore the transition from one camera to the other. In this paper, we propose to learn a metric from pairs of samples from different cameras. In this way, even less sophisticated features describing color and texture information are sufficient for finally getting state-of-the-art classification results. Moreover, once the metric has been learned, only linear projections are necessary at search time, where a simple nearest neighbor classification is performed. The approach is demonstrated on three publicly available datasets of different complexity, where it can be seen that state-of-the-art results can be obtained at much lower computational costs."'),
('"Relevant Feature Selection for Human Pose Estimation and Localization in Cluttered Images"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_32"', '"We address the problem of estimating human body pose from a single image with cluttered background. We train multiple local linear regressors for estimating the 3D pose from a feature vector of gradient orientation histograms. Each linear regressor is capable of selecting relevant components of the feature vector depending on pose by training it on a pose cluster which is a subset of the training samples with similar pose. For discriminating the pose clusters, we use kernel Support Vector Machines (SVM) with pose-dependent feature selection. We achieve feature selection for kernel SVMs by estimating scale parameters of RBF kernel through minimization of the radius/margin bound, which is an upper bound of the expected generalization error, with efficient gradient descent. Human detection is also possible with these SVMs. Quantitative experiments show the effectiveness of pose-dependent feature selection to both human detection and pose estimation."'),
('"Reliable Fiducial Detection in Natural Scenes"', '"ECCV 2004"', '["Augmented Reality", "Natural Scene", "Motion Blur", "Adaptive Thresholding", "Engineer Detector"]', '"https://doi.org/10.1007/978-3-540-24673-2_38"', '"Reliable detection of fiducial targets in real-world images is addressed in this paper. We show that even the best existing schemes are fragile when exposed to other than laboratory imaging conditions, and introduce an approach which delivers significant improvements in reliability at moderate computational cost. The key to these improvements is in the use of machine learning techniques, which have recently shown impressive results for the general object detection problem, for example in face detection. Although fiducial detection is an apparently simple special case, this paper shows why robustness to lighting, scale and foreshortening can be addressed within the machine learning framework with greater reliability than previous, more ad-hoc, fiducial detection schemes."'),
('"Removing Shadows from Images"', '"ECCV 2002"', '["Texture", "shading, & colour", "shadow removal", "lightness recovery", "illuminant invariance"]', '"https://doi.org/10.1007/3-540-47979-1_55"', '"Illumination conditions cause problems for many computer vision algorithms. In particular, shadows in an image can cause segmentation, tracking, or recognition algorithms to fail. In this paper we propose a method to process a 3-band colour image to locate, and subsequently remove shadows. The result is a 3-band colour image which contains all the original salient information in the image, except that the shadows are gone."'),
('"Removing the Example from Example-Based Photometric Stereo"', '"ECCV 2010"', '["Reference Object", "Observation Vector", "Outdoor Scene", "Photometric Stereo", "Photo Collection"', '"https://doi.org/10.1007/978-3-642-35740-4_16"', '"We introduce an example-based photometric stereo approach that does not require explicit reference objects. Instead, we use a robust multi-view stereo technique to create a partial reconstruction of the scene which serves as scene-intrinsic reference geometry. Similar to the standard approach, we then transfer normals from reconstructed to unreconstructed regions based on robust photometric matching. In contrast to traditional reference objects, the scene-intrinsic reference geometry is neither noise free nor does it necessarily contain all possible normal directions for given materials. We therefore propose several modifications that allow us to reconstruct high quality normal maps. During integration, we combine both normal and positional information yielding high quality reconstructions. We show results on several datasets including an example based on data solely collected from the Internet."'),
('"Renormalization Returns: Hyper-renormalization and Its Applications"', '"ECCV 2012"', '["Minimization Principle", "Generalize Eigenvalue Problem", "Geometric Estimation", "True Shape", "R', '"https://doi.org/10.1007/978-3-642-33712-3_28"', '"The technique of \\u201crenormalization\\u201d for geometric estimation attracted much attention when it was proposed in early 1990s for having higher accuracy than any other then known methods. Later, it was replaced by minimization of the reprojection error. This paper points out that renormalization can be modified so that it outperforms reprojection error minimization. The key fact is that renormalization directly specifies equations to solve, just as the \\u201cestimation equation\\u201d approach in statistics, rather than minimizing some cost. Exploiting this fact, we modify the problem so that the solution has zero bias up to high order error terms; we call the resulting scheme hyper-renormalization. We apply it to ellipse fitting to demonstrate that it indeed surpasses reprojection error minimization. We conclude that it is the best method available today."'),
('"Repairing Sparse Low-Rank Texture"', '"ECCV 2012"', '["Low-Rank and Sparse Matrix Recovery", "Texture Completion", "Image Repairing"]', '"https://doi.org/10.1007/978-3-642-33715-4_35"', '"In this paper, we show how to harness both low-rank and sparse structures in regular or near regular textures for image completion. Our method leverages the new convex optimization for low-rank and sparse signal recovery and can automatically correctly repair the global structure of a corrupted texture, even without precise information about the regions to be completed. Through extensive simulations, we show our method can complete and repair textures corrupted by errors with both random and contiguous supports better than existing low-rank matrix recovery methods. Through experimental comparisons with existing image completion systems (such as Photoshop) our method demonstrate significant advantage over local patch based texture synthesis techniques in dealing with large corruption, non-uniform texture, and large perspective deformation."'),
('"Representing Edge Models via Local Principal Component Analysis"', '"ECCV 2002"', '["Pattern Analysis", "Edge Detection", "Machine Intelligence", "Reconstruction Error", "Vector Quant', '"https://doi.org/10.1007/3-540-47969-4_26"', '"Edge detection depends not only upon the assumed model of what an edge is, but also on how this model is represented. The problem of how to represent the edge model is typically neglected, despite the fact that the representation is a bottleneck for both computational cost and accuracy. We propose to represent edge models by a partition of the edge manifold corresponding to the edge model, where each local element of the partition is described by its principal components. We describe the construction of this representation and demonstrate its benefits for various edge models."'),
('"Representing Pairwise Spatial and Temporal Relations for Action Recognition"', '"ECCV 2010"', '["Action Recognition", "Temporal Relation", "Multiple Kernel Learning", "Pairwise Relationship", "Fe', '"https://doi.org/10.1007/978-3-642-15549-9_37"', '"The popular bag-of-words paradigm for action recognition tasks is based on building histograms of quantized features, typically at the cost of discarding all information about relationships between them. However, although the beneficial nature of including these relationships seems obvious, in practice finding good representations for feature relationships in video is difficult. We propose a simple and computationally efficient method for expressing pairwise relationships between quantized features that combines the power of discriminative representations with key aspects of Na\\u00efve Bayes. We demonstrate how our technique can augment both appearance- and motion-based features, and that it significantly improves performance on both types of features."'),
('"Representing Roots on the Basis of Reeb Graphs in Plant Phenotyping"', '"ECCV 2014"', '["Root representation", "Root structure analysis", "Topological graphs", "Reeb graphs", "Graph-based', '"https://doi.org/10.1007/978-3-319-16220-1_6"', '"This paper presents a new representation for root images based on Reeb graphs. The representation proposed captures lengths and distances in root structures as well as locations of branches, numbers of lateral roots and the locations of the root tips. An analysis of root images using Reeb graphs is presented and results are compared to ground truth measurements. This paper shows, that the Reeb graph based approach not only captures the characteristics needed for phenotyping of plants, but it also provides a solution to the problem of overlapping roots in the images. Using a Reeb graph based representation, such overlaps can be directly detected without further analysis, during the computation of the graph."'),
('"Resampling Structure from Motion"', '"ECCV 2010"', '["Local Group", "Camera Motion", "Local Geometry", "Merging Process", "Bundle Adjustment"]', '"https://doi.org/10.1007/978-3-642-15552-9_1"', '"This paper proposes a hierarchical framework that resamples 3D reconstructed points to reduce computation cost on time and memory for very large-scale Structure from Motion. The goal is to maintain accuracy and stability similar for different resample rates. We consider this problem in a level-of-detail perspective, from a very large scale global and sparse bundle adjustment to a very detailed and local dense optimization. The dense matching are resampled by exploring the redundancy using local invariant properties, while 3D points are resampled by exploring the redundancy using their covariance and their distribution in both 3D and image space. Detailed experiments on our resample framework are provided. We also demonstrate the proposed framework on large-scale examples. The results show that the proposed resample scheme can produce a 3D reconstruction with the stability similar to quasi dense methods, while the problem size is as neat as sparse methods."'),
('"Resolution Selection Using Generalized Entropies of Multiresolution Histograms"', '"ECCV 2002"', '["Block Size", "Maximum Entropy", "Generalize Entropy", "Shannon Entropy", "Resolution Selection"]', '"https://doi.org/10.1007/3-540-47969-4_15"', '"The performances of many image analysis tasks depend on the image resolution at which they are applied. Traditionally, resolution selection methods rely on spatial derivatives of image intensities. Differential measurements, however, are sensitive to noise and are local. They cannot characterize patterns, such as textures, which are defined over extensive image regions. In this work, we present a novel tool for resolution selection that considers sufficiently large image regions and is robust to noise. It is based on the generalized entropies of the histograms of an image at multiple resolutions. We first examine, in general, the variation of histogram entropies with image resolution. Then, we examine the sensitivity of this variation for shapes and textures in an image. Finally, we discuss the significance of resolutions of maximum histogram entropy. It is shown that computing features at these resolutions increases the discriminability between images. It is also shown that maximum histogram entropy values can be used to improve optical flow estimates for block based algorithms in image sequences with a changing zoom factor."'),
('"Resolution-Aware Fitting of Active Appearance Models to Low Resolution Images"', '"ECCV 2006"', '["Root Mean Square", "Active Appearance Model", "Tracking Experiment", "Face Tracking", "Geometric D', '"https://doi.org/10.1007/11744047_7"', '"Active Appearance Models (AAM) are compact representations of the shape and appearance of objects. Fitting AAMs to images is a difficult, non-linear optimization task. Traditional approaches minimize the L2 norm error between the model instance and the input image warped onto the model coordinate frame. While this works well for high resolution data, the fitting accuracy degrades quickly at lower resolutions. In this paper, we show that a careful design of the fitting criterion can overcome many of the low resolution challenges. In our resolution-aware formulation (RAF), we explicitly account for the finite size sensing elements of digital cameras, and simultaneously model the processes of object appearance variation, geometric deformation, and image formation. As such, our Gauss-Newton gradient descent algorithm not only synthesizes model instances as a function of estimated parameters, but also simulates the formation of low resolution images in a digital camera. We compare the RAF algorithm against a state-of-the-art tracker across a variety of resolution and model complexity levels. Experimental results show that RAF considerably improves the estimation accuracy of both shape and appearance parameters when fitting to low resolution data."'),
('"Resolution-Enhanced Photometric Stereo"', '"ECCV 2006"', '["Gaussian Mixture Model", "Minimum Description Length", "Resolution Enhancement", "Integrability Co', '"https://doi.org/10.1007/11744078_5"', '"Conventional photometric stereo has a fundamental limitation that the scale of recovered geometry is limited to the resolution of the input images. However, surfaces that contain sub-pixel geometric structures are not well modelled by a single normal direction per pixel. In this work, we propose a technique for resolution-enhanced photometric stereo, in which surface geometry is computed at a resolution higher than that of the input images. To achieve this goal, our method first utilizes a generalized reflectance model to recover the distribution of surface normals inside each pixel. This normal distribution is then used to infer sub-pixel structures on a surface of uniform material by spatially arranging the normals among pixels at a higher resolution according to a minimum description length criterion on 3D textons over the surface. With the presented method, high resolution geometry that is lost in conventional photometric stereo can be recovered from low resolution input images."'),
('"Retexturing Single Views Using Texture and Shading"', '"ECCV 2006"', '["Screen Print", "Texture Synthesis", "Single View", "Nonrigid Motion", "Explicit Correspondence"]', '"https://doi.org/10.1007/11744085_6"', '"We present a method for retexturing non-rigid objects from a single viewpoint. Without reconstructing 3D geometry, we create realistic video with shape cues at two scales. At a coarse scale, a track of the deforming surface in 2D allows us to erase the old texture and overwrite it with a new texture. At a fine scale, estimates of the local irradiance provide strong cues of fine scale structure in the actual lighting environment. Computing irradiance from explicit correspondence is difficult and unreliable, so we limit our reconstructions to screen printing \\u2014 a common printing techniques with a finite number of colors. Our irradiance estimates are computed in a local manner: pixels are classified according to color, then irradiance is computed given the color. We demonstrate results in two situations: on a special shirt designed for easy retexturing and on natural clothing with screen prints. Because of the quality of the results, we believe that this technique has wide applications in special effects and advertising."'),
('"Rethinking the Prior Model for Stereo"', '"ECCV 2006"', '["Convex Hull", "Human Vision System", "Human Vision", "Prior Model", "Stereo Pair"]', '"https://doi.org/10.1007/11744078_41"', '"Sometimes called the smoothing assumption, the prior model of a stereo matching algorithm is the algorithm\\u2019s expectation on the surfaces in the world. Any stereo algorithm makes assumptions about the probability to see each surface that can be represented in its representation system. Although the past decade has seen much continued progress in stereo matching algorithms, the prior models used in them have not changed much in three decades: most algorithms still use a smoothing prior that minimizes some function of the difference of depths between neighboring sites, sometimes allowing for discontinuities."'),
('"Retrieving Actions in Group Contexts"', '"ECCV 2010"', '["Action Recognition", "Surveillance Video", "Retrieval Task", "Foreground Pixel", "Group Context"]', '"https://doi.org/10.1007/978-3-642-35749-7_14"', '"We develop methods for action retrieval from surveillance video using contextual feature representations. The novelty of our proposed approach is two-fold. First, we introduce a new feature representation called the action context (AC) descriptor. The AC descriptor encodes information about not only the action of an individual person in the video, but also the behaviour of other people nearby. This feature representation is inspired by the fact that the context of what other people are doing provides very useful cues for recognizing the actions of each individual. Second, we formulate our problem as a retrieval/ranking task, which is different from previous work on action classification. We develop an action retrieval technique based on rank-SVM, a state-of-the-art approach for solving ranking problems. We apply our proposed approach on two real-world datasets. The first dataset consists of videos of multiple people performing several group activities. The second dataset consists of surveillance videos from a nursing home environment. Our experimental results show the advantage of using contextual information for disambiguating different actions and the benefit of using rank-SVMs instead of regular SVMs for video retrieval problems."'),
('"Reverse Training: An Efficient Approach for Image Set Classification"', '"ECCV 2014"', '["Image Set Classification", "Face and Object Recognition"]', '"https://doi.org/10.1007/978-3-319-10599-4_50"', '"This paper introduces a new approach, called reverse training, to efficiently extend binary classifiers for the task of multi-class image set classification. Unlike existing binary to multi-class extension strategies, which require multiple binary classifiers, the proposed approach is very efficient since it trains a single binary classifier to optimally discriminate the class of the query image set from all others. For this purpose, the classifier is trained with the images of the query set (labelled positive) and a randomly sampled subset of the training data (labelled negative). The trained classifier is then evaluated on rest of the training images. The class of these images with their largest percentage classified as positive is predicted as the class of the query image set. The confidence level of the prediction is also computed and integrated into the proposed approach to further enhance its robustness and accuracy. Extensive experiments and comparisons with existing methods show that the proposed approach achieves state of the art performance for face and object recognition on a number of datasets."'),
('"Revisiting Single-View Shape Tensors: Theory and Applications"', '"ECCV 2002"', '["Linear Constraint", "Projection Matrice", "Reconstruction Formula", "Homography Matrix", "Trifocal', '"https://doi.org/10.1007/3-540-47967-8_27"', '"Given the projection of a sufficient number of points it is possible to algebraically eliminate the camera parameters and obtain view-invariant functions of image coordinates and space coordinates. These single view invariants have been introduced in the past, however, they are not as well understood as their dual multi-view tensors. In this paper we revisit the dual tensors (bilinear, trilinear and quadlinear), both the general and the reference-plane reduced version, and describe the complete set of synthetic constraints, properties of the tensor slices, reprojection equations, non-linear constraints and reconstruction formulas. We then apply some of the new results, such as the dual reprojection equations, for multi-view point tracking under occlusions."'),
('"Revisiting the Brightness Constraint: Probabilistic Formulation and Algorithms"', '"ECCV 2006"', '["Patch Size", "Great Circle", "Single Pixel", "Smoothness Assumption", "Orientation Tensor"]', '"https://doi.org/10.1007/11744078_14"', '"In this paper we introduce a principled approach to modeling the image brightness constraint for optical flow algorithms. Using a simple noise model, we derive a probabilistic representation for optical flow. This representation subsumes existing approaches to flow modeling, provides insights into the behaviour and limitations of existing methods and leads to modified algorithms that outperform other approaches that use the brightness constraint. Based on this representation we develop algorithms for flow estimation using different smoothness assumptions, namely constant and affine flow. Experiments on standard data sets demonstrate the superiority of our approach."'),
('"Reweighted Random Walks for Graph Matching"', '"ECCV 2010"', '["Random Walk", "Edge Density", "Graph Match", "Objective Score", "Spectral Match"]', '"https://doi.org/10.1007/978-3-642-15555-0_36"', '"Graph matching is an essential problem in computer vision and machine learning. In this paper, we introduce a random walk view on the problem and propose a robust graph matching algorithm against outliers and deformation. Matching between two graphs is formulated as node selection on an association graph whose nodes represent candidate correspondences between the two graphs. The solution is obtained by simulating random walks with reweighting jumps enforcing the matching constraints on the association graph. Our algorithm achieves noise-robust graph matching by iteratively updating and exploiting the confidences of candidate correspondences. In a practical sense, our work is of particular importance since the real-world matching problem is made difficult by the presence of noise and outliers. Extensive and comparative experiments demonstrate that it outperforms the state-of-the-art graph matching algorithms especially in the presence of outliers and deformation."'),
('"RGBD Salient Object Detection: A Benchmark and Algorithms"', '"ECCV 2014"', '["Depth Image", "Salient Object", "Salient Region", "Saliency Detection", "Visual Saliency"]', '"https://doi.org/10.1007/978-3-319-10578-9_7"', '"Although depth information plays an important role in the human vision system, it is not yet well-explored in existing visual saliency computational models. In this work, we first introduce a large scale RGBD image dataset to address the problem of data deficiency in current research of RGBD salient object detection. To make sure that most existing RGB saliency models can still be adequate in RGBD scenarios, we continue to provide a simple fusion framework that combines existing RGB-produced saliency with new depth-induced saliency, the former one is estimated from existing RGB models while the latter one is based on the proposed multi-contextual contrast model. Moreover, a specialized multi-stage RGBD model is also proposed which takes account of both depth and appearance cues derived from low-level feature contrast, mid-level region grouping and high-level priors enhancement. Extensive experiments show the effectiveness and superiority of our model which can accurately locate the salient objects from RGBD images, and also assign consistent saliency values for the target objects."'),
('"Riemannian Anisotropic Diffusion for Tensor Valued Images"', '"ECCV 2008"', '["Fractional Anisotropy", "Diffusion Tensor", "Anisotropic Diffusion", "Structure Tensor", "Diffusio', '"https://doi.org/10.1007/978-3-540-88693-8_24"', '"Tensor valued images, for instance originating from diffusion tensor magnetic resonance imaging (DT-MRI), have become more and more important over the last couple of years. Due to the nonlinear structure of such data it is nontrivial to adapt well-established image processing techniques to them. In this contribution we derive anisotropic diffusion equations for tensor-valued images based on the intrinsic Riemannian geometric structure of the space of symmetric positive tensors. In contrast to anisotropic diffusion approaches proposed so far, which are based on the Euclidian metric, our approach considers the nonlinear structure of positive definite tensors by means of the intrinsic Riemannian metric. Together with an intrinsic numerical scheme our approach overcomes a main drawback of former proposed anisotropic diffusion approaches, the so-called eigenvalue swelling effect. Experiments on synthetic data as well as real DT-MRI data demonstrate the value of a sound differential geometric formulation of diffusion processes for tensor valued data."'),
('"Riemannian Manifold Learning for Nonlinear Dimensionality Reduction"', '"ECCV 2006"', '["Riemannian Manifold", "Simplicial Complex", "Edge Point", "Neural Information Processing System", ', '"https://doi.org/10.1007/11744023_4"', '"In recent years, nonlinear dimensionality reduction (NLDR) techniques have attracted much attention in visual perception and many other areas of science. We propose an efficient algorithm called Riemannian manifold learning (RML). A Riemannian manifold can be constructed in the form of a simplicial complex, and thus its intrinsic dimension can be reliably estimated. Then the NLDR problem is solved by constructing Riemannian normal coordinates (RNC). Experimental results demonstrate that our algorithm can learn the data\\u2019s intrinsic geometric structure, yielding uniformly distributed and well organized low-dimensional embedding data."'),
('"Riemannian Sparse Coding for Positive Definite Matrices"', '"ECCV 2014"', '["Sparse coding", "Riemannian distance", "Region covariances"]', '"https://doi.org/10.1007/978-3-319-10578-9_20"', '"Inspired by the great success of sparse coding for vector valued data, our goal is to represent symmetric positive definite (SPD) data matrices as sparse linear combinations of atoms from a dictionary, where each atom itself is an SPD matrix. Since SPD matrices follow a non-Euclidean (in fact a Riemannian) geometry, existing sparse coding techniques for Euclidean data cannot be directly extended. Prior works have approached this problem by defining a sparse coding loss function using either extrinsic similarity measures (such as the log-Euclidean distance) or kernelized variants of statistical measures (such as the Stein divergence, Jeffrey\\u2019s divergence, etc.). In contrast, we propose to use the intrinsic Riemannian distance on the manifold of SPD matrices. Our main contribution is a novel mathematical model for sparse coding of SPD matrices; we also present a computationally simple algorithm for optimizing our model. Experiments on several computer vision datasets showcase superior classification and retrieval performance compared with state-of-the-art approaches."'),
('"Ring-Light Photometric Stereo"', '"ECCV 2010"', '["Planar Rotation", "Lighting Direction", "Prototype Device", "Photometric Stereo", "Depth Discontin', '"https://doi.org/10.1007/978-3-642-15552-9_20"', '"We propose a novel algorithm for uncalibrated photometric stereo. While most of previous methods rely on various assumptions on scene properties, we exploit constraints in lighting configurations. We first derive an ambiguous reconstruction by requiring lights to lie on a view centered cone. This reconstruction is upgraded to Euclidean by constraints derived from lights of equal intensity and multiple view geometry. Compared to previous methods, our algorithm deals with more general data and achieves high accuracy. Another advantage of our method is that we can model weak perspective effects of lighting, while previous methods often assume orthographical illumination. We use both synthetic and real data to evaluate our algorithm. We further build a hardware prototype to demonstrate our approach."'),
('"Road Scene Segmentation from a Single Image"', '"ECCV 2012"', '["Local Binary Pattern", "Single Image", "Texture Descriptor", "Convolutional Neural Network", "Pede', '"https://doi.org/10.1007/978-3-642-33786-4_28"', '"Road scene segmentation is important in computer vision for different applications such as autonomous driving and pedestrian detection. Recovering the 3D structure of road scenes provides relevant contextual information to improve their understanding."'),
('"Road-Crossing Assistance by Traffic Flow Analysis"', '"ECCV 2014"', '["Road crossing assistance", "Approaching traffic analysis", "Optic flow", "Blind", "Visually impair', '"https://doi.org/10.1007/978-3-319-16199-0_26"', '"We present a system to alert visually impaired pedestrians of vehicles approaching a road-crossing without traffic control. The system is computationally efficient, requires low-cost hardware, and can be mounted on existing street infrastructure, such as sign or lighting poles. The incoming video stream, showing the approaching traffic, is transformed to a one-dimensional signal, that is forwarded to a decision module. Preliminary experimental results indicate promising probability-of-detection and false alarm rates, while providing sufficiently early warning to the pedestrian. The planned target hardware is a solar-charged low cost Android device."'),
('"Robust 3D Action Recognition with Random Occupancy Patterns"', '"ECCV 2012"', '["Action Recognition", "Depth Sequence", "Sparse Code", "Hand Gesture Recognition", "Depth Camera"]', '"https://doi.org/10.1007/978-3-642-33709-3_62"', '"We study the problem of action recognition from depth sequences captured by depth cameras, where noise and occlusion are common problems because they are captured with a single commodity camera. In order to deal with these issues, we extract semi-local features called random occupancy pattern (ROP) features, which employ a novel sampling scheme that effectively explores an extremely large sampling space. We also utilize a sparse coding approach to robustly encode these features. The proposed approach does not require careful parameter tuning. Its training is very fast due to the use of the high-dimensional integral image, and it is robust to the occlusions. Our technique is evaluated on two datasets captured by commodity depth cameras: an action dataset and a hand gesture dataset. Our classification results are superior to those obtained by the state of the art approaches on both datasets."'),
('"Robust 3D Pose Estimation and Efficient 2D Region-Based Segmentation from a 3D Shape Prior"', '"ECCV 2008"', '["Segmentation Result", "Active Contour", "Rigid Object", "Shape Prior", "Local Image Feature"]', '"https://doi.org/10.1007/978-3-540-88688-4_13"', '"In this work, we present an approach to jointly segment a rigid object in a 2D image and estimate its 3D pose, using the knowledge of a 3D model. We naturally couple the two processes together into a unique energy functional that is minimized through a variational approach. Our methodology differs from the standard monocular 3D pose estimation algorithms since it does not rely on local image features. Instead, we use global image statistics to drive the pose estimation process. This confers a satisfying level of robustness to noise and initialization for our algorithm, and bypasses the need to establish correspondences between image and object features. Moreover, our methodology possesses the typical qualities of region-based active contour techniques with shape priors, such as robustness to occlusions or missing information, without the need to evolve an infinite dimensional curve. Another novelty of the proposed contribution is to use a unique 3D model surface of the object, instead of learning a large collection of 2D shapes to accommodate for the diverse aspects that a 3D object can take when imaged by a camera. Experimental results on both synthetic and real images are provided, which highlight the robust performance of the technique on challenging tracking and segmentation applications."'),
('"Robust 3D Segmentation of Multiple Moving Objects Under Weak Perspective"', '"WDV 2006"', '["Feature Point", "Measurement Matrix", "Motion Error", "Rigid Object", "Rank Estimation"]', '"https://doi.org/10.1007/978-3-540-70932-9_4"', '"A scene containing multiple independently moving, possibly occluding, rigid objects is considered under the weak perspective camera model. We obtain a set of feature points tracked across a number of frames and address the problem of 3D motion segmentation of the objects in presence of measurement noise and outliers. We extend the robust structure from motion (SfM) method [5] to 3D motion segmentation and apply it to realistic, contaminated tracking data with occlusion. A number of approaches to 3D motion segmentation have already been proposed [3,6,14,15]. However, most of them were not developed for, and tested on, noisy and outlier-corrupted data that often occurs in practice. Due to the consistent use of robust techniques at all critical steps, our approach can cope with such data, as demonstrated in a number of tests with synthetic and real image sequences."'),
('"Robust Active Shape Model Search"', '"ECCV 2002"', '["Medical Image Understanding", "Shape", "Active Shape Models", "Robust Parameter Estimation", "M-es', '"https://doi.org/10.1007/3-540-47979-1_35"', '"Active shape models (ASMs) have been shown to be a powerful tool to aid the interpretation of images. ASM model parameter estimation is based on the assumption that residuals between model fit and data have a Gaussian distribution. However, in many real applications, specifically those found in the area of medical image analysis, this assumption may be inaccurate. Robust parameter estimation methods have been used elsewhere in machine vision and provide a promising method of improving ASM search performance. This paper formulates M-estimator and random sampling approaches to robust parameter estimation in the context of ASM search. These methods have been applied to several sets of medical images where ASM search robustness problems have previously been encountered. Robust parameter estimation is shown to increase tolerance to outliers, which can lead to improved search robustness and accuracy."'),
('"Robust and Accurate Non-parametric Estimation of Reflectance Using Basis Decomposition and Correcti', '"ECCV 2014"', '["Non-parametric BRDF estimation", "reflectance", "basis decompostion", "correction function", "erro', '"https://doi.org/10.1007/978-3-319-10605-2_25"', '"A common approach to non-parametric BRDF estimation is the approximation of the sparsely measured input using basis decomposition. In this paper we greatly improve the fitting accuracy of such methods by iteratively applying a novel correction function to an initial estimate. We also introduce a basis to efficiently represent such a function. Based on this general concept we propose an iterative algorithm that is able to explicitly identify and treat outliers in the input data. Our method is invariant to different error metrics which alleviates the error-prone choice of an appropriate one for the given input. We evaluate our method based on a large set of experiments generated from 100 real-world BRDFs and 16 newly measured materials. The experiments show that our method outperforms other evaluated state-of-the-art basis decomposition methods by an order of magnitude in the perceptual sense for outlier ratios up to 40%."'),
('"Robust and Accurate Shape Model Fitting Using Random Forest Regression Voting"', '"ECCV 2012"', '["Facial Feature", "Shape Model", "Sparse Grid", "Active Appearance Model", "Statistical Shape Model', '"https://doi.org/10.1007/978-3-642-33786-4_21"', '"A widely used approach for locating points on deformable objects is to generate feature response images for each point, then to fit a shape model to the response images. We demonstrate that Random Forest regression can be used to generate high quality response images quickly. Rather than using a generative or a discriminative model to evaluate each pixel, a regressor is used to cast votes for the optimal position. We show this leads to fast and accurate matching when combined with a statistical shape model. We evaluate the technique in detail, and compare with a range of commonly used alternatives on several different datasets. We show that the random forest regression method is significantly faster and more accurate than equivalent discriminative, or boosted regression based methods trained on the same data."'),
('"Robust and Computationally Efficient Face Detection Using Gaussian Derivative Features of Higher Or', '"ECCV 2012"', '["Higher-Order Gaussian Derivatives", "Cascade of Classifiers", "Face Detection", "Half-Octave Gauss', '"https://doi.org/10.1007/978-3-642-33885-4_57"', '"In this paper, we show that a cascade of classifiers using Gaussian derivatives features up to fourth order can be used efficiently to improve the detection performance and robustness as well when compared with the popular approaches using Haar-like features or using Gaussian derivatives of lower order. We also present a new training method that structures the cascade detection so as to use the least expensive derivatives in the initial stages, so as to reduce the overall computational cost of detection. We demonstrate these improvements with experiments using two publicly available datasets (MIT+CMU and FDDB), in the face detection problem, in addition we perform several experiment to show the robustness of Gaussian derivatives when several transformations are presented in the image."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Robust and Efficient Photo-Consistency Estimation for Volumetric 3D Reconstruction"', '"ECCV 2006"', '["Image Patch", "Visual Hull", "View Synthesis", "Neighboring Voxels", "Texture Size"]', '"https://doi.org/10.1007/11744047_14"', '"Estimating photo-consistency is one of the most important ingredients for any 3D stereo reconstruction technique that is based on a volumetric scene representation. This paper presents a new, illumination invariant photo-consistency measure for high quality, volumetric 3D reconstruction from calibrated images. In contrast to current standard methods such as normalized cross-correlation it supports unconstrained camera setups and non-planar surface approximations. We show how this measure can be embedded into a highly efficient, completely hardware accelerated volumetric reconstruction pipeline by exploiting current graphics processors. We provide examples of high quality reconstructions with computation times of only a few seconds to minutes, even for large numbers of cameras and high volumetric resolutions."'),
('"Robust and Efficient Subspace Segmentation via Least Squares Regression"', '"ECCV 2012"', '["Sparse Representation", "Spectral Cluster", "Little Square Regression", "Segmentation Accuracy", "', '"https://doi.org/10.1007/978-3-642-33786-4_26"', '"This paper studies the subspace segmentation problem which aims to segment data drawn from a union of multiple linear subspaces. Recent works by using sparse representation, low rank representation and their extensions attract much attention. If the subspaces from which the data drawn are independent or orthogonal, they are able to obtain a block diagonal affinity matrix, which usually leads to a correct segmentation. The main differences among them are their objective functions. We theoretically show that if the objective function satisfies some conditions, and the data are sufficiently drawn from independent subspaces, the obtained affinity matrix is always block diagonal. Furthermore, the data sampling can be insufficient if the subspaces are orthogonal. Some existing methods are all special cases. Then we present the Least Squares Regression (LSR) method for subspace segmentation. It takes advantage of data correlation, which is common in real data. LSR encourages a grouping effect which tends to group highly correlated data together. Experimental results on the Hopkins 155 database and Extended Yale Database B show that our method significantly outperforms state-of-the-art methods. Beyond segmentation accuracy, all experiments demonstrate that LSR is much more efficient."'),
('"Robust and Fast Collaborative Tracking with Two Stage Sparse Optimization"', '"ECCV 2010"', '["Sparse Representation", "Reconstruction Error", "Appearance Model", "Visual Tracking", "Tracking R', '"https://doi.org/10.1007/978-3-642-15561-1_45"', '"The sparse representation has been widely used in many areas and utilized for visual tracking. Tracking with sparse representation is formulated as searching for samples with minimal reconstruction errors from learned template subspace. However, the computational cost makes it unsuitable to utilize high dimensional advanced features which are often important for robust tracking under dynamic environment. Based on the observations that a target can be reconstructed from several templates, and only some of the features with discriminative power are significant to separate the target from the background, we propose a novel online tracking algorithm with two stage sparse optimization to jointly minimize the target reconstruction error and maximize the discriminative power. As the target template and discriminative features usually have temporal and spatial relationship, dynamic group sparsity (DGS) is utilized in our algorithm. The proposed method is compared with three state-of-art trackers using five public challenging sequences, which exhibit appearance changes, heavy occlusions, and pose variations. Our algorithm is shown to outperform these methods."'),
('"Robust and Practical Face Recognition via Structured Sparsity"', '"ECCV 2012"', '["Face Recognition", "Face Image", "Training Image", "Sparse Representation", "Near Neighbor"]', '"https://doi.org/10.1007/978-3-642-33765-9_24"', '"Sparse representation based classification (SRC) methods have recently drawn much attention in face recognition, due to their good performance and robustness against misalignment, illumination variation, and occlusion. They assume the errors caused by image variations can be modeled as pixel-wisely sparse. However, in many practical scenarios these errors are not truly pixel-wisely sparse but rather sparsely distributed with structures, i.e., they constitute contiguous regions distributed at different face positions. In this paper, we introduce a class of structured sparsity-inducing norms into the SRC framework, to model various corruptions in face images caused by misalignment, shadow (due to illumination change), and occlusion. For practical face recognition, we develop an automatic face alignment method based on minimizing the structured sparsity norm. Experiments on benchmark face datasets show improved performance over SRC and other alternative methods."'),
('"Robust Attentive Behavior Detection by Non-linear Head Pose Embedding and Estimation"', '"ECCV 2006"', '["Radial Basis Function", "Video Sequence", "Focus Attention", "Locally Linear Embedding", "Head Ori', '"https://doi.org/10.1007/11744078_28"', '"We present a new scheme to robustly detect a type of human attentive behavior, which we call frequent change in focus of attention (FCFA), from video sequences. FCFA behavior can be easily perceived by people as temporal changes of human head pose (normally the pan angle). For recognition of this behavior by computer, we propose an algorithm to estimate the head pan angle in each frame of the sequence within a normal range of the head tilt angles. Developed from the ISOMAP, we learn a non-linear head pose embedding space in 2-D, which is suitable as a feature space for person-independent head pose estimation. These features are used in a mapping system to map the high dimensional head images into the 2-D feature space from which the head pan angle is calculated very simply. The non-linear person-independent mapping system is composed of two parts: 1) Radial Basis Function (RBF) interpolation, and 2) an adaptive local fitting technique. The results show that head orientation can be estimated robustly. Following the head pan angle estimation, an entropy-based classifier is used to characterize the attentive behaviors. The experimental results show that entropy of the head pan angle is a good measure, which is quite distinct for FCFA and focused attention behavior. Thus by setting an experimental threshold on the entropy value we can successfully and robustly detect FCFA behavior."'),
('"Robust Bundle Adjustment Revisited"', '"ECCV 2014"', '["Bundle adjustment", "nonlinear least-squares optimization", "robust cost function"]', '"https://doi.org/10.1007/978-3-319-10602-1_50"', '"In this work we address robust estimation in the bundle adjustment procedure. Typically, bundle adjustment is not solved via a generic optimization algorithm, but usually cast as a nonlinear least-squares problem instance. In order to handle gross outliers in bundle adjustment the least-squares formulation must be robustified. We investigate several approaches to make least-squares objectives robust while retaining the least-squares nature to use existing efficient solvers. In particular, we highlight a method based on lifting a robust cost function into a higher dimensional representation, and show how the lifted formulation is efficiently implemented in a Gauss-Newton framework. In our experiments the proposed lifting-based approach almost always yields the best (i.e. lowest) objectives."'),
('"Robust Computer Vision through Kernel Density Estimation"', '"ECCV 2002"', '["Kernel Density Estimation", "Vision Task", "Robust Regression", "Scale Estimate", "Kernel Density ', '"https://doi.org/10.1007/3-540-47969-4_16"', '"Two new techniques based on nonparametric estimation of probability densities are introduced which improve on the performance of equivalent robust methods currently employed in computer vision. The first technique draws from the projection pursuit paradigm in statistics, and carries out regression M-estimation with a weak dependence on the accuracy of the scale estimate. The second technique exploits the properties of the multivariate adaptive mean shift, and accomplishes the fusion of uncertain measurements arising from an unknown number of sources. As an example, the two techniques are extensively used in an algorithm for the recovery of multiple structures from heavily corrupted data."'),
('"Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition"', '"BioAW 2004"', '["Gabor Filter", "Iris Image", "Iris Recognition", "Ordinal Measure", "Iris Pattern"]', '"https://doi.org/10.1007/978-3-540-25976-3_25"', '"The randomness of iris pattern makes it one of the most reliable biometric traits. On the other hand, the complex iris image structure and various sources of intra-class variations result in the difficulty of iris representation. Although diverse iris recognition methods have been proposed, the fundamentals of iris recognition have not a unified answer. As a breakthrough of this problem, we found that several accurate iris recognition algorithms share a same idea \\u2014 local ordinal encoding, which is the representation well-suited for iris recognition. After further analysis and summarization, a general framework of iris recognition is formulated in this paper. This work discovered the secret of iris recognition. With the guidance of this framework, a novel iris recognition method based on robust estimating the direction of image gradient vector is developed. Extensive experimental results demonstrate our idea."'),
('"Robust Estimation of Pigment Distributions from Multiband Skin Images and Its Application to Realis', '"ECCV 2012"', '["Skin", "Pigment distribution estimation", "Multiband image", "The Kubelka-Munk theory", "Multi-res', '"https://doi.org/10.1007/978-3-642-33868-7_42"', '"This paper describes a robust method for estimating pigment distributions on a skin surface from multiband images. The spatial distributions of the pigments such as melanin, oxy-hemoglobin and deoxy-hemoglobin give rise to a color texture. The distributions are estimated by using the Kubelka-Munk theory. The accuracy of estimating the pigment distributions is affected by a fine texture of sulcus cutis and a broad texture of shade caused by three-dimensional body shape. In order to separate these textures from the color texture, wavelet-based multi-resolution analysis (MRA) is applied to the multiband images before the pigment estimation, because the textures of sulcus cutis and shade predominantly have low and high spatial frequency components in the multiband skin images, respectively. Realistic skin image is synthesized from modified pigment distributions with additional features such as stain, inflammation and bruise by changing the concentrations of melanin, oxy-hemoglobin and deoxy-hemoglobin, respectively. The experimental results of skin image synthesis show good feasibility of the proposed method."'),
('"Robust Expression-Invariant Face Recognition from Partially Missing Data"', '"ECCV 2006"', '["Facial Expression", "Face Recognition", "Geodesic Distance", "Iterative Close Point", "Isometric E', '"https://doi.org/10.1007/11744078_31"', '"Recent studies on three-dimensional face recognition proposed to model facial expressions as isometries of the facial surface. Based on this model, expression-invariant signatures of the face were constructed by means of approximate isometric embedding into flat spaces. Here, we apply a new method for measuring isometry-invariant similarity between faces by embedding one facial surface into another. We demonstrate that our approach has several significant advantages, one of which is the ability to handle partially missing data. Promising face recognition results are obtained in numerical experiments even when the facial surfaces are severely occluded."'),
('"Robust Extraction of the Optic Nerve Head in Optical Coherence Tomography"', '"MMBIA 2004"', '["Optic Nerve", "Optical Coherence Tomography", "Optic Nerve Head", "Optical Coherence Tomography Im', '"https://doi.org/10.1007/978-3-540-27816-0_34"', '"Glaucoma is a leading cause of blindness. While glaucoma is a treatable and controllable disease, there is still no cure available. Early diagnosis is important in order to prevent severe vision loss. Many current diagnostic techniques are subjective and variable. This provides motivation for a more objective and repeatable method. Optical Coherence Tomography (OCT) is a relatively new imaging technique that is proving useful in diagnosing, monitoring, and studying glaucoma. OCT, like ultrasound, suffers from signal dependent noise which can make accurate, automatic segmentation of images difficult. In this article we propose a method to automatically extract the optic nerve and retinal boundaries from axial OCT scans through the optic nerve head. We also propose a method to automatically segment the curve to extract the nerve head profile that is important in diagnosing and monitoring glaucoma."'),
('"Robust Face Alignment Based on Hierarchical Classifier Network"', '"ECCV 2006"', '["Feature Point", "Face Image", "Facial Feature", "Principle Component Analysis", "Face Detection"]', '"https://doi.org/10.1007/11754336_1"', '"Robust face alignment is crucial for many face processing applications. As face detection only gives a rough estimation of face region, one important problem is how to align facial shapes starting from this rough estimation, especially on face images with expression and pose changes. We propose a novel method of face alignment by building a hierarchical classifier network, connecting face detection and face alignment into a smooth coarse-to-fine procedure. Classifiers are trained to recognize feature textures in different scales from entire face to local patterns. A multi-layer structure is employed to organize the classifiers, which begins with one classifier at the first layer and gradually refines the localization of feature points by more classifiers in the following layers. A Bayesian framework is configured for the inference of the feature points between the layers. The boosted classifiers detects facial features discriminately from its local neighborhood, while the inference between the layers constrains the searching space. Extensive experiments are reported to show its accuracy and robustness."'),
('"Robust Face Recognition Using Dynamic Space Warping"', '"BioAW 2002"', '["Face Recognition", "Dynamic Time Warping", "Face Model", "Gallery Image", "Gray Level Histogram"]', '"https://doi.org/10.1007/3-540-47917-1_13"', '"The utility of face recognition for multimedia indexing is enhanced by using accurate detection and alignment of salient invariant face features. The face recognition can be performed using template matching or a feature-based-approach, but both these methods suffer from occlusion and require an a priori model for extracting information. To avoid these drawbacks, we present in this paper a complete scheme for face recognition based on salient feature extraction in challenging conditions, which is performed without an a priori or learned model. These features are used in a matching process that overcomes occlusion effects and facial expressions using the dynamic space warping which aligns each feature in the query image, if possible, with its corresponding feature in the gallery set. Thus, we make face recognition robust to low frequency variations (like the presence of occlusion, etc) as well as to high frequency variations (like expression, gender, etc). A maximum likelihood scheme is used to make the recognition process more precise, as is shown in the experiments."'),
('"Robust Face Recognition Using Probabilistic Facial Trait Code"', '"ECCV 2010"', '["Face Recognition", "Facial Image", "Sparse Representation", "Local Binary Pattern", "Subspace Lear', '"https://doi.org/10.1007/978-3-642-15549-9_20"', '"Recently, Facial Trait Code (FTC) was proposed for solving face recognition, and was reported with promising recognition rates. However, several simplifications in the FTC encoding make it unable to handle the most rigorous face recognition scenario in which only one facial image per individual is available for enrollment in the gallery set and the probe set includes faces under variations caused by illumination, expression, pose or misalignment. In this study, we propose the Probabilistic Facial Trait Code (PFTC) with a novel encoding scheme and a probabilistic codeword distance measure. We also proposed the Pattern-Specific Subspace Learning (PSSL) scheme that encodes and recognizes faces robustly under aforementioned variations. The proposed PFTC was evaluated and compared with state-of-the-art algorithms, including the FTC, the algorithm using sparse representation, and the one using Local Binary Pattern. Our experimental study considered factors such as the number of enrollment allowed in the gallery, the variation among gallery or probe set, and reported results for both identification and verification problems. The proposed PFTC yielded significant better recognition rates in most of the scenarios than all the states-of-the-art algorithms evaluated in this study."'),
('"Robust Fitting by Adaptive-Scale Residual Consensus"', '"ECCV 2004"', '["Standard Variance", "Fundamental Matrix", "Robust Estimator", "Point Pair", "Breakdown Point"]', '"https://doi.org/10.1007/978-3-540-24672-5_9"', '"Computer vision tasks often require the robust fit of a model to some data. In a robust fit, two major steps should be taken: i) robustly estimate the parameters of a model, and ii) differentiate inliers from outliers. We propose a new estimator called Adaptive-Scale Residual Consensus (ASRC). ASRC scores a model based on both the residuals of inliers and the corresponding scale estimate determined by those inliers. ASRC is very robust to multiple-structural data containing a high percentage of outliers. Compared with RANSAC, ASRC requires no pre-determined inlier threshold as it can simultaneously estimate the parameters of a model and the scale of inliers belonging to that model. Experiments show that ASRC has better robustness to heavily corrupted data than other robust methods. Our experiments address two important computer vision tasks: range image segmentation and fundamental matrix calculation. However, the range of potential applications is much broader than these."'),
('"Robust Fitting for Multiple View Geometry"', '"ECCV 2012"', '["Computer Vision", "Loss Function", "Active Constraint", "Residual Function", "Point Correspondence', '"https://doi.org/10.1007/978-3-642-33718-5_53"', '"How hard are geometric vision problems with outliers? We show that for most fitting problems, a solution that minimizes the number of outliers can be found with an algorithm that has polynomial time-complexity in the number of points (independent of the rate of outliers). Further, and perhaps more interestingly, other cost functions such as the truncated L2-norm can also be handled within the same framework with the same time complexity."'),
('"Robust Foreground Detection Using Smoothness and Arbitrariness Constraints"', '"ECCV 2014"', '["Markov Random Field", "Foreground Object", "Dynamic Background", "Multiple Instance Learning", "Ro', '"https://doi.org/10.1007/978-3-319-10584-0_35"', '"Foreground detection plays a core role in a wide spectrum of applications such as tracking and behavior analysis. It, especially for videos captured by fixed cameras, can be posed as a component decomposition problem, the background of which is typically assumed to lie in a low dimensional subspace. However, in real world cases, dynamic backgrounds like waving trees and water ripples violate the assumption. Besides, noises caused by the image capturing process and, camouflage and lingering foreground objects would also significantly increase the difficulty of accurate foreground detection. That is to say, simply imposing the correlation constraint on the background is no longer sufficient for such cases. To overcome the difficulties mentioned above, this paper proposes to further take into account foreground characteristics including 1) the smoothness: the foreground object should appear coherently in spatial domain and move smoothly in temporal, and 2) the arbitrariness: the appearance of foreground could be with arbitrary colors or intensities. With the consideration of the smoothness and the arbitrariness of foreground as well as the correlation of (static) background, we formulate the problem in a unified framework from a probabilistic perspective, and design an effective algorithm to seek the optimal solution. Experimental results on both synthetic and real data demonstrate the clear advantages of our method compared to the state of the art alternatives."'),
('"Robust Fusion: Extreme Value Theory for Recognition Score Normalization"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_35"', '"Recognition problems in computer vision often benefit from a fusion of different algorithms and/or sensors, with score level fusion being among the most widely used fusion approaches. Choosing an appropriate score normalization technique before fusion is a fundamentally difficult problem because of the disparate nature of the underlying distributions of scores for different sources of data. Further complications are introduced when one or more fusion inputs outright fail or have adversarial inputs, which we find in the fields of biometrics and forgery detection. Ideally a score normalization should be robust to model assumptions, modeling errors, and parameter estimation errors, as well as robust to algorithm failure. In this paper, we introduce the w-score, a new technique for robust recognition score normalization. We do not assume a match or non-match distribution, but instead suggest that the top scores of a recognition system\\u2019s non-match scores follow the statistical Extreme Value Theory, and show how to use that to provide consistent robust normalization with a strong statistical basis."'),
('"Robust Global Translations with 1DSfM"', '"ECCV 2014"', '["Structure from Motion", "translations problem", "robust estimation"]', '"https://doi.org/10.1007/978-3-319-10578-9_5"', '"We present a simple, effective method for solving structure from motion problems by averaging epipolar geometries. Based on recent successes in solving for global camera rotations using averaging schemes, we focus on the problem of solving for 3D camera translations given a network of noisy pairwise camera translation directions (or 3D point observations). To do this well, we have two main insights. First, we propose a method for removing outliers from problem instances by solving simpler low-dimensional subproblems, which we refer to as 1DSfM problems. Second, we present a simple, principled averaging scheme. We demonstrate this new method in the wild on Internet photo collections."'),
('"Robust Head Pose Estimation Using Supervised Manifold Learning"', '"ECCV 2010"', '["Face Recognition", "Face Image", "Support Vector Regression", "Illumination Variation", "Locality ', '"https://doi.org/10.1007/978-3-642-15567-3_38"', '"We address the problem of fine-grain head pose angle estimation from a single 2D face image as a continuous regression problem. Currently the state of the art, and a promising line of research, on head pose estimation seems to be that of nonlinear manifold embedding techniques, which learn an \\u201doptimal\\u201d low-dimensional manifold that models the nonlinear and continuous variation of face appearance with pose angle. Furthermore, supervised manifold learning techniques attempt to achieve this robustly in the presence of latent variables in the training set (especially identity, illumination, and facial expression), by incorporating head pose angle information accompanying the training samples. Most of these techniques are designed with the classification scenario in mind, however, and are not directly applicable to the regression scenario where continuous numeric values (pose angles), rather than class labels (discrete poses), are available. In this paper, we propose to deal with the regression case in a principled way. We present a taxonomy of methods for incorporating continuous pose angle information into one or more stages of the manifold learning process, and discuss its implementation for Neighborhood Preserving Embedding (NPE) and Locality Preserving Projection (LPP). Experiments are carried out on a face dataset containing significant identity and illumination variations, and the results show that our regression-based approach far outperforms previous supervised manifold learning methods for head pose estimation."'),
('"Robust Head Tracking with Particles Based on Multiple Cues Fusion"', '"ECCV 2006"', '["Face Detection", "Color Histogram", "Observation Model", "Cluttered Background", "Active Appearanc', '"https://doi.org/10.1007/11754336_4"', '"This paper presents a fully automatic and highly robust head tracking algorithm based on the latest advances in real-time multi-view face detection techniques and multiple cues fusion under particle filter framework. Visual cues designed for general object tracking problem hardly suffice for robust head tracking under diverse or even severe circumstances, making it a necessity to utilize higher level information which is object-specific. To this end we introduce a vector-boosted multi-view face detector [2] as the \\u201cface cue\\u201d in addition to two other general visual cues targeting the entire head, color spatiogram[3] and contour gradient. Data fusion is done by an extended particle filter which supports multiple distinct yet interrelated state vectors (referring to face and head in our tracking context). Furthermore, pose information provided by the face cue is exploited to help achieve improved accuracy and efficiency in the fusion. Experiments show that our algorithm is highly robust against target position, size and pose change as well as unfavorable conditions such as occlusion, poor illumination and cluttered background."'),
('"Robust Homography Estimation from Planar Contours Based on Convexity"', '"ECCV 2006"', '["Convex Hull", "Cross Ratio", "Visual Servoing", "Symbol Recognition", "Canonical Frame"]', '"https://doi.org/10.1007/11744023_9"', '"We propose a homography estimation method from the contours of planar regions. Standard projective invariants such as cross ratios or canonical frames based on hot points obtained from local differential properties are extremely unstable in real images suffering from pixelization, thresholding artifacts, and other noise sources. We explore alternative constructions based on global convexity properties of the contour such as discrete tangents and concavities. We show that a projective frame can be robustly extracted from arbitrary shapes with at least one appreciable concavity. Algorithmic complexity and stability are theoretically discussed and experimentally evaluated in a number of real applications including projective shape matching, alignment and pose estimation. We conclude that the procedure is computationally efficient and notably robust given the ill-conditioned nature of the problem."'),
('"Robust Identification of Object Elasticity"', '"MMBIA 2004"', '["Extended Kalman Filter", "Noisy Input", "Poisson Input", "Modulus Distribution", "Object Elasticit', '"https://doi.org/10.1007/978-3-540-27816-0_37"', '"Quantification of object elasticity properties has important technical implications as well as significant practical applications, such as civil structural integrity inspection, machine fatigue assessment, and medical disease diagnosis. In general, given noisy measurements on the kinematic states of the objects from imaging or other data, the aim is to recover the elasticity parameters for assumed material constitutive models of the objects. Various versions of the least-square (LS) methods have been widely used in practice, which, however, do not perform well under reasonably realistic levels of disturbances. Another popular strategy, based on the extended Kalman filter (EKF), is also far from optimal and subject to divergence if either the initializations are poor or the noises are not Gaussian. In this paper, we propose a robust system identification paradigm for the quantitative analysis of object elasticity. It is derived and extended from the \\\\(\\\\mathcal{H}_\\\\infty\\\\) filtering principles and is particularly powerful for real-world situations where the types and levels of the disturbances are unknown. Specifically, we show the results of applying this strategy to synthetic data for accuracy assessment and for comparison to LS and EKF results, and using canine magnetic resonance imaging data for the recovery of myocardial material parameters."'),
('"Robust Instance Recognition in Presence of Occlusion and Clutter"', '"ECCV 2014"', '["Point Cloud", "Depth Image", "Object Instance", "Kinect Sensor", "Dominant Orientation"]', '"https://doi.org/10.1007/978-3-319-10605-2_34"', '"We present a robust learning based instance recognition framework from single view point clouds. Our framework is able to handle real-world instance recognition challenges, i.e, clutter, similar looking distractors and occlusion. Recent algorithms have separately tried to address the problem of clutter [9] and occlusion [16] but fail when these challenges are combined. In comparison we handle all challenges within a single framework. Our framework uses a soft label Random Forest [5] to learn discriminative shape features of an object and use them to classify both its location and pose. We propose a novel iterative training scheme for forests which maximizes the margin between classes to improve recognition accuracy, as compared to a conventional training procedure. The learnt forest outperforms template matching, DPM [7] in presence of similar looking distractors. Using occlusion information, computed from the depth data, the forest learns to emphasize the shape features from the visible regions thus making it robust to occlusion. We benchmark our system with the state-of-the-art recognition systems [9,7] in challenging scenes drawn from the largest publicly available dataset. To complement the lack of occlusion tests in this dataset, we introduce our Desk3D dataset and demonstrate that our algorithm outperforms other methods in all settings."'),
('"Robust Learning from Normals for 3D Face Recognition"', '"ECCV 2012"', '["Face Recognition", "Azimuth Angle", "Equal Error Rate", "Facial Surface", "Photometric Stereo"]', '"https://doi.org/10.1007/978-3-642-33868-7_23"', '"We introduce novel subspace-based methods for learning from the azimuth angle of surface normals for 3D face recognition. We show that the normal azimuth angles combined with Principal Component Analysis (PCA) using a cosine-based distance measure can be used for robust face recognition from facial surfaces. The proposed algorithms are well-suited for all types of 3D facial data including data produced by range cameras (depth images), photometric stereo (PS) and shade-from-X (SfX) algorithms. We demonstrate the robustness of the proposed algorithms both in 3D face reconstruction from synthetically occluded samples, as well as, in face recognition using the FRGC v2 3D face database and the recently collected Photoface database where the proposed method achieves state-of-the-art results. An important aspect of our method is that it can achieve good face recognition/verification performance by using raw 3D scans without any heavy preprocessing (i.e., model fitting, surface smoothing etc.)."'),
('"Robust Luminance and Chromaticity for Matte Regression in Polynomial Texture Mapping"', '"ECCV 2012"', '["Radial Basis Function", "Radial Basis Function Network", "Robust Regression", "Light Direction", "', '"https://doi.org/10.1007/978-3-642-33868-7_36"', '"Polynomial Texture Mapping (PTM) is a technique employed in a variety of settings, from museums to in-the-field image capture to multi-illuminant microscopy. It consists of illuminating the surface in question with lights from a collection of light directions, each light in turn. To date, the most accurate interpolation employed in PTM consists of two stages: a matte regression stage followed by a further specularity/shadow interpolation. For the first stage, recovering an underlying matte model so as to acquire surface albedo, normals and chromaticity, PTM employs polynomial regression at each pixel, mapping light-direction to luminance. A more accurate model excludes outlier values deriving from specularities and shadows by employing a robust regression from 6-D polynomials to 1-D luminance. Robust methods are guaranteed to automatically find the best representation of the underlying matte content. Here, we retain the idea of using robust methods but instead investigate using a much simpler robust 1-D mode-finder, acting on luminance and on chromaticity components. We then go on to increase accuracy by carrying out 3-D to 1-D regression: this strikes a balance between the best method and the fastest method, with greatly diminished complexity and another large speedup. We show that little accuracy is lost using this much simpler method, and demonstrate the effectiveness of the new method on several image datasets."'),
('"Robust Motion Segmentation with Unknown Correspondences"', '"ECCV 2014"', '["Motion segmentation", "point correspondence", "subspace clustering", "partial permutation matrix"]', '"https://doi.org/10.1007/978-3-319-10599-4_14"', '"Motion segmentation can be addressed as a subspace clustering problem, assuming that the trajectories of interest points are known. However, establishing point correspondences is in itself a challenging task. Most existing approaches tackle the correspondence estimation and motion segmentation problems separately. In this paper, we introduce an approach to performing motion segmentation without any prior knowledge of point correspondences. We formulate this problem in terms of Partial Permutation Matrices (PPMs) and aim to match feature descriptors while simultaneously encouraging point trajectories to satisfy subspace constraints. This lets us handle outliers in both point locations and feature appearance. The resulting optimization problem can be solved via the Alternating Direction Method of Multipliers (ADMM), where each subproblem has an efficient solution. Our experimental evaluation on synthetic and real sequences clearly evidences the benefits of our formulation over the traditional sequential approach that first estimates correspondences and then performs motion segmentation."'),
('"Robust Multi-body Motion Tracking Using Commute Time Clustering"', '"ECCV 2006"', '["Heat Kernel", "Laplacian Matrix", "Locality Preserve Projection", "Motion Segmentation", "Commute ', '"https://doi.org/10.1007/11744023_13"', '"The presence of noise renders the classical factorization method almost impractical for real-world multi-body motion tracking problems. The main problem stems from the effect of noise on the shape interaction matrix, which looses its block-diagonal structure and as a result the assignment of elements to objects becomes difficult. The aim in this paper is to overcome this problem using graph-spectral embedding and the k-means algorithm. To this end we develop a representation based on the commute time between nodes on a graph. The commute time (i.e. the expected time taken for a random walk to travel between two nodes and return) can be computed from the Laplacian spectrum using the discrete Green\\u2019s function, and is an important property of the random walk on a graph. The commute time is a more robust measure of the proximity of data than the raw proximity matrix. Our embedding procedure preserves commute time, and is closely akin to kernel PCA, the Laplacian eigenmap and the diffusion map. We illustrate the results both on the synthetic image sequences and real world video sequences, and compare our results with several alternative methods."'),
('"Robust Multi-View Boosting with Priors"', '"ECCV 2010"', '["Loss Function", "Random Forest", "Unlabeled Data", "Weak Learner", "Unlabeled Sample"]', '"https://doi.org/10.1007/978-3-642-15558-1_56"', '"Many learning tasks for computer vision problems can be described by multiple views or multiple features. These views can be exploited in order to learn from unlabeled data, a.k.a. \\u201cmulti-view learning\\u201d. In these methods, usually the classifiers iteratively label each other a subset of the unlabeled data and ignore the rest. In this work, we propose a new multi-view boosting algorithm that, unlike other approaches, specifically encodes the uncertainties over the unlabeled samples in terms of given priors. Instead of ignoring the unlabeled samples during the training phase of each view, we use the different views to provide an aggregated prior which is then used as a regularization term inside a semi-supervised boosting method. Since we target multi-class applications, we first introduce a multi-class boosting algorithm based on maximizing the mutli-class classification margin. Then, we propose our multi-class semi-supervised boosting algorithm which is able to use priors as a regularization component over the unlabeled data. Since the priors may contain a significant amount of noise, we introduce a new loss function for the unlabeled regularization which is robust to noisy priors. Experimentally, we show that the multi-class boosting algorithms achieves state-of-the-art results in machine learning benchmarks. We also show that the new proposed loss function is more robust compared to other alternatives. Finally, we demonstrate the advantages of our multi-view boosting approach for object category recognition and visual object tracking tasks, compared to other multi-view learning methods."'),
('"Robust Multi-view Face Detection Using Error Correcting Output Codes"', '"ECCV 2006"', '["Support Vector Machine", "Base Classifier", "Face Detection", "Binary Classifier", "Support Vector', '"https://doi.org/10.1007/11744085_1"', '"This paper presents a novel method to solve multi-view face detection problem by Error Correcting Output Codes (ECOC). The motivation is that face patterns can be divided into separated classes across views, and ECOC multi-class method can improve the robustness of multi-view face detection compared with the view-based methods because of its inherent error-tolerant ability. One key issue with ECOC-based multi-class classifier is how to construct effective binary classifiers. Besides applying ECOC to multi-view face detection, this paper emphasizes on designing efficient binary classifiers by learning informative features through minimizing the error rate of the ensemble ECOC multi-class classifier. Aiming at designing efficient binary classifiers, we employ spatial histograms as the representation, which provide an over-complete set of optional features that can be efficiently computed from the original images. In addition, the binary classifier is constructed as a coarse to fine procedure using fast histogram matching followed by accurate Support Vector Machine (SVM). The experimental results show that the proposed method is robust to multi-view faces, and achieves performance comparable to that of state-of-the-art approaches to multi-view face detection."'),
('"Robust Multiple Structures Estimation with J-Linkage"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_41"', '"This paper tackles the problem of fitting multiple instances of a model to data corrupted by noise and outliers. The proposed solution is based on random sampling and conceptual data representation. Each point is represented with the characteristic function of the set of random models that fit the point. A tailored agglomerative clustering, called J-linkage, is used to group points belonging to the same model. The method does not require prior specification of the number of models, nor it necessitate parameters tuning. Experimental results demonstrate the superior performances of the algorithm."'),
('"Robust Object Tracking by Hierarchical Association of Detection Responses"', '"ECCV 2008"', '["False Alarm", "Detection Response", "Middle Level", "Pedestrian Detector", "Multiple Object Tracki', '"https://doi.org/10.1007/978-3-540-88688-4_58"', '"We present a detection-based three-level hierarchical association approach to robustly track multiple objects in crowded environments from a single camera. At the low level, reliable tracklets (i.e. short tracks for further analysis) are generated by linking detection responses based on conservative affinity constraints. At the middle level, these tracklets are further associated to form longer tracklets based on more complex affinity measures. The association is formulated as a MAP problem and solved by the Hungarian algorithm. At the high level, entries, exits and scene occluders are estimated using the already computed tracklets, which are used to refine the final trajectories. This approach is applied to the pedestrian class and evaluated on two challenging datasets. The experimental results show a great improvement in performance compared to previous methods."'),
('"Robust Optimal Pose Estimation"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_12"', '"We study the problem of estimating the position and orientation of a calibrated camera from an image of a known scene. A common problem in camera pose estimation is the existence of false correspondences between image features and modeled 3D points. Existing techniques such as ransac to handle outliers have no guarantee of optimality. In contrast, we work with a natural extension of the L \\u2009\\u221e\\u2009 norm to the outlier case. Using a simple result from classical geometry, we derive necessary conditions for L \\u2009\\u221e\\u2009 optimality and show how to use them in a branch and bound setting to find the optimum and to detect outliers. The algorithm has been evaluated on synthetic as well as real data showing good empirical performance. In addition, for cases with no outliers, we demonstrate shorter execution times than existing optimal algorithms."'),
('"Robust Parameterized Component Analysis"', '"ECCV 2002"', '["Training Image", "Motion Parameter", "Appearance Model", "Geometric Transformation", "Facial Anima', '"https://doi.org/10.1007/3-540-47979-1_44"', '"Principal Component Analysis (PCA) has been successfully applied to construct linear models of shape, graylevel, and motion. In particular, PCA has been widely used to model the variation in the appearance of people\\u2019s faces. We extend previous work on facial modeling for tracking faces in video sequences as they undergo significant changes due to facial expressions. Here we develop person-specific facial appearance models (PSFAM), which use modular PCA to model complex intra-person appearance changes. Such models require aligned visual training data; in previous work, this has involved a time consuming and error-prone hand alignment and cropping process. Instead, we introduce parameterized component analysis to learn a subspace that is invariant to affine (or higher order) geometric transformations. The automatic learning of a PSFAM given a training image sequence is posed as a continuous optimization problem and is solved with a mixture of stochastic and deterministic techniques achieving sub-pixel accuracy. We illustrate the use of the 2D PSFAM model with several applications including video-conferencing, realistic avatar animation and eye tracking."'),
('"Robust Player Gesture Spotting and Recognition in Low-Resolution Sports Video"', '"ECCV 2006"', '["Input Image", "Gesture Recognition", "Sport Video", "Silhouette Image", "Foreground Image"]', '"https://doi.org/10.1007/11744085_27"', '"The determination of the player\\u2019s gestures and actions in sports video is a key task in automating the analysis of the video material at a high level. In many sports views, the camera covers a large part of the sports arena, so that the resolution of player\\u2019s region is low. This makes the determination of the player\\u2019s gestures and actions a challenging task, especially if there is large camera motion. To overcome these problems, we propose a method based on curvature scale space templates of the player\\u2019s silhouette. The use of curvature scale space makes the method robust to noise and our method is robust to significant shape corruption of a part of player\\u2019s silhouette. We also propose a new recognition method which is robust to noisy sequences of data and needs only a small amount of training data."'),
('"Robust Point Matching Revisited: A Concave Optimization Approach"', '"ECCV 2012"', '["Energy Function", "Chinese Character", "Iterative Close Point", "Point Correspondence", "Convex En', '"https://doi.org/10.1007/978-3-642-33709-3_19"', '"The well-known robust point matching (RPM) method uses deterministic annealing for optimization, and it has two problems. First, it cannot guarantee the global optimality of the solution and tends to align the centers of two point sets. Second, deformation needs to be regularized to avoid the generation of undesirable results. To address these problems, in this paper we first show that the energy function of RPM can be reduced to a concave function with very few non-rigid terms after eliminating the transformation variables and applying linear transformation; we then propose to use concave optimization technique to minimize the resulting energy function. The proposed method scales well with problem size, achieves the globally optimal solution, and does not need regularization for simple transformations such as similarity transform. Experiments on synthetic and real data validate the advantages of our method in comparison with state-of-the-art methods."'),
('"Robust Real-Time Visual Tracking Using Pixel-Wise Posteriors"', '"ECCV 2008"', '["Visual Tracking", "Pixel Location", "Probabilistic Framework", "Rigid Transformation", "Motion Blu', '"https://doi.org/10.1007/978-3-540-88688-4_61"', '"We derive a probabilistic framework for robust, real-time, visual tracking of previously unseen objects from a moving camera. The tracking problem is handled using a bag-of-pixels representation and comprises a rigid registration between frames, a segmentation and online appearance learning. The registration compensates for rigid motion, segmentation models any residual shape deformation and the online appearance learning provides continual refinement of both the object and background appearance models. The key to the success of our method is the use of pixel-wise posteriors, as opposed to likelihoods. We demonstrate the superior performance of our tracker by comparing cost function statistics against those commonly used in the visual tracking literature. Our comparison method provides a way of summarising tracking performance using lots of data from a variety of different sequences."'),
('"Robust Registration of 3-D Ultrasound Images Based on Gabor Filter and Mean-Shift Method"', '"MMBIA 2004"', '["Ultrasound Image", "Gabor Filter", "Speckle Noise", "Edge Feature", "Nonrigid Registration"]', '"https://doi.org/10.1007/978-3-540-27816-0_26"', '"A novel robust method is presented for the registration of 3-D ultrasound images. The proposed method improves the performance of the voxel property-based affine registration in two aspects. First, a set of wavelet-like Gabor filters is used to extract the texture and edge features of the voxels. By using these features, the smoothness of the similarity function in large scale can be improved. Furthermore, adopting edge information can improve the registration accuracy. Second, a robust maximization method based on the mean-shift algorithm and Powell\\u2019s direction set method is proposed. The implicitly embedded smoothing process of the mean-shift algorithm can effectively remove the local fluctuation of the similarity function and significantly improve the robustness of optimization. Experimental results demonstrate the robust and accurate performance of the proposed method in the registration of 3-D ultrasound fetal head images."'),
('"Robust Regression"', '"ECCV 2012"', '["Robust methods", "errors in variables", "intra-sample outliers"]', '"https://doi.org/10.1007/978-3-642-33765-9_44"', '"Discriminative methods (e.g., kernel regression, SVM) have been extensively used to solve problems such as object recognition, image alignment and pose estimation from images. Regression methods typically map image features (X) to continuous (e.g., pose) or discrete (e.g., object category) values. A major drawback of existing regression methods is that samples are directly projected onto a subspace and hence fail to account for outliers which are common in realistic training sets due to occlusion, specular reflections or noise. It is important to notice that in existing regression methods, and discriminative methods in general, the regressor variables X are assumed to be noise free. Due to this assumption, discriminative methods experience significant degrades in performance when gross outliers are present."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Robust Scale Estimation from Ensemble Inlier Sets for Random Sample Consensus Methods"', '"ECCV 2008"', '["Median Absolute Deviation", "Scale Estimation", "Associate Model", "Sequential Probability Ratio T', '"https://doi.org/10.1007/978-3-540-88690-7_14"', '"This paper proposes a RANSAC modification that performs automatic estimation of the scale of inlier noise. The scale estimation takes advantage of accumulated inlier sets from all proposed models. It is shown that the proposed method gives robust results in case of high outlier ratio data, in spite that no user specified threshold is needed. The method also improves sampling efficiency, without requiring any auxiliary information other than the data to be modeled."'),
('"Robust Scene Text Detection with Convolution Neural Network Induced MSER Trees"', '"ECCV 2014"', '["Maximally Stable Extremal Regions (MSERs)", "convolutional neural network (CNN)", "text-like outli', '"https://doi.org/10.1007/978-3-319-10593-2_33"', '"Maximally Stable Extremal Regions (MSERs) have achieved great success in scene text detection. However, this low-level pixel operation inherently limits its capability for handling complex text information efficiently (e. g. connections between text or background components), leading to the difficulty in distinguishing texts from background components. In this paper, we propose a novel framework to tackle this problem by leveraging the high capability of convolutional neural network (CNN). In contrast to recent methods using a set of low-level heuristic features, the CNN network is capable of learning high-level features to robustly identify text components from text-like outliers (e.g. bikes, windows, or leaves). Our approach takes advantages of both MSERs and sliding-window based methods. The MSERs operator dramatically reduces the number of windows scanned and enhances detection of the low-quality texts. While the sliding-window with CNN is applied to correctly separate the connections of multiple characters in components. The proposed system achieved strong robustness against a number of extreme text variations and serious real-world problems. It was evaluated on the ICDAR 2011 benchmark dataset, and achieved over 78% in F-measure, which is significantly higher than previous methods."'),
('"Robust Sparse Coding and Compressed Sensing with the Difference Map"', '"ECCV 2014"', '["Sparse coding", "compressed sensing"]', '"https://doi.org/10.1007/978-3-319-10578-9_21"', '"In compressed sensing, we wish to reconstruct a sparse signal x from observed data y. In sparse coding, on the other hand, we wish to find a representation of an observed signal y as a sparse linear combination, with coefficients x, of elements from an overcomplete dictionary. While many algorithms are competitive at both problems when x is very sparse, it can be challenging to recover x when it is less sparse. We present the Difference Map, which excels at sparse recovery when sparseness is lower. The Difference Map out-performs the state of the art with reconstruction from random measurements and natural image reconstruction via sparse coding."'),
('"Robust Tracking with Weighted Online Structured Learning"', '"ECCV 2012"', '["Online Learning", "Online Algorithm", "Appearance Model", "Visual Tracking", "Robust Tracking"]', '"https://doi.org/10.1007/978-3-642-33712-3_12"', '"Robust visual tracking requires constant update of the target appearance model, but without losing track of previous appearance information. One of the difficulties with the online learning approach to this problem has been a lack of flexibility in the modelling of the inevitable variations in target and scene appearance over time. The traditional online learning approach to the problem treats each example equally, which leads to previous appearances being forgotten too quickly and a lack of emphasis on the most current observations. Through analysis of the visual tracking problem, we develop instead a novel weighted form of online risk which allows more subtlety in its representation. However, the traditional online learning framework does not accommodate this weighted form. We thus also propose a principled approach to weighted online learning using weighted reservoir sampling and provide a weighted regret bound as a theoretical guarantee of performance. The proposed novel online learning framework can handle examples with different importance weights for binary, multiclass, and even structured output labels in both linear and non-linear kernels. Applying the method to tracking results in an algorithm which is both efficient and accurate even in the presence of severe appearance changes. Experimental results show that the proposed tracker outperforms the current state-of-the-art."'),
('"Robust Visual Tracking Based on an Effective Appearance Model"', '"ECCV 2008"', '["Video Stream", "Appearance Model", "Visual Tracking", "Tracking Result", "Spatial Weighting"]', '"https://doi.org/10.1007/978-3-540-88693-8_29"', '"Most existing appearance models for visual tracking usually construct a pixel-based representation of object appearance so that they are incapable of fully capturing both global and local spatial layout information of object appearance. In order to address this problem, we propose a novel spatial Log-Euclidean appearance model (referred as SLAM) under the recently introduced Log-Euclidean Riemannian metric [23]. SLAM is capable of capturing both the global and local spatial layout information of object appearance by constructing a block-based Log-Euclidean eigenspace representation. Specifically, the process of learning the proposed SLAM consists of five steps\\u2014appearance block division, online Log-Euclidean eigenspace learning, local spatial weighting, global spatial weighting, and likelihood evaluation. Furthermore, a novel online Log-Euclidean Riemannian subspace learning algorithm (IRSL) [14] is applied to incrementally update the proposed SLAM. Tracking is then led by the Bayesian state inference framework in which a particle filter is used for propagating sample distributions over the time. Theoretic analysis and experimental evaluations demonstrate the promise and effectiveness of the proposed SLAM."'),
('"Robust Visual Tracking for Multiple Targets"', '"ECCV 2006"', '["Video Frame", "Particle Filter", "Camera Motion", "Tracking Result", "Proposal Distribution"]', '"https://doi.org/10.1007/11744085_9"', '"We address the problem of robust multi-target tracking within the application of hockey player tracking. The particle filter technique is adopted and modified to fit into the multi-target tracking framework. A rectification technique is employed to find the correspondence between the video frame coordinates and the standard hockey rink coordinates so that the system can compensate for camera motion and improve the dynamics of the players. A global nearest neighbor data association algorithm is introduced to assign boosting detections to the existing tracks for the proposal distribution in particle filters. The mean-shift algorithm is embedded into the particle filter framework to stabilize the trajectories of the targets for robust tracking during mutual occlusion. Experimental results show that our system is able to automatically and robustly track a variable number of targets and correctly maintain their identities regardless of background clutter, camera motion and frequent mutual occlusion between targets."'),
('"Robust Visual Tracking with Double Bounding Box Model"', '"ECCV 2014"', '["Tracking Method", "Background Region", "Visual Tracking", "Tracking Result", "Likelihood Score"]', '"https://doi.org/10.1007/978-3-319-10590-1_25"', '"A novel tracking algorithm that can track a highly non-rigid target robustly is proposed using a new bounding box representation called the Double Bounding Box (DBB). In the DBB, a target is described by the combination of the Inner Bounding Box (IBB) and the Outer Bounding Box (OBB). Then our objective of visual tracking is changed to find the IBB and OBB instead of a single bounding box, where the IBB and OBB can be easily obtained by the Dempster-Shafer (DS) theory. If the target is highly non-rigid, any single bounding box cannot include all foreground regions while excluding all background regions. Using the DBB, our method does not directly handle the ambiguous regions, which include both the foreground and background regions. Hence, it can solve the inherent ambiguity of the single bounding box representation and thus can track highly non-rigid targets robustly. Our method finally finds the best state of the target using a new Constrained Markov Chain Monte Carlo (CMCMC)-based sampling method with the constraint that the OBB should include the IBB. Experimental results show that our method tracks non-rigid targets accurately and robustly, and outperforms state-of-the-art methods."'),
('"ROCHADE: Robust Checkerboard Advanced Detection for Camera Calibration"', '"ECCV 2014"', '["Checkerboard Detection", "Saddle-Based Subpixel Refinement", "Multi Camera Calibration", "Low Reso', '"https://doi.org/10.1007/978-3-319-10593-2_50"', '"We present a new checkerboard detection algorithm which is able to detect checkerboards at extreme poses, or checkerboards which are highly distorted due to lens distortion even on low-resolution images. On the detected pattern we apply a surface fitting based subpixel refinement specifically tailored for checkerboard X-junctions. Finally, we investigate how the accuracy of a checkerboard detector affects the overall calibration result in multi-camera setups. The proposed method is evaluated on real images captured with different camera models to show its wide applicability. Quantitative comparisons to OpenCV\\u2019s checkerboard detector show that the proposed method detects up to 80% more checkerboards and detects corner points more accurately, even under strong perspective distortion as often present in wide baseline stereo setups."'),
('"Rolling Guidance Filter"', '"ECCV 2014"', '["Image filter", "scale-aware processing", "edge preserving"]', '"https://doi.org/10.1007/978-3-319-10578-9_53"', '"Images contain many levels of important structures and edges. Compared to masses of research to make filters edge preserving, finding scale-aware local operations was seldom addressed in a practical way, albeit similarly vital in image processing and computer vision. We propose a new framework to filter images with the complete control of detail smoothing under a scale measure. It is based on a rolling guidance implemented in an iterative manner that converges quickly. Our method is simple in implementation, easy to understand, fully extensible to accommodate various data operations, and fast to produce results. Our implementation achieves realtime performance and produces artifact-free results in separating different scale structures. This filter also introduces several inspiring properties different from previous edge-preserving ones."'),
('"Rotation Invariant Non-rigid Shape Matching in Cluttered Scenes"', '"ECCV 2010"', '["Rotation Invariant", "Point Match", "Viterbi Algorithm", "Point Correspondence", "Rest Point"]', '"https://doi.org/10.1007/978-3-642-15555-0_37"', '"This paper presents a novel and efficient method for locating deformable shapes in cluttered scenes. The shapes to be detected may undergo arbitrary translational and rotational changes, and they can be non-rigidly deformed, occluded and corrupted by clutters. All these problems make the accurate and robust shape matching very difficult. By using a new shape representation, which involves a powerful feature descriptor, the proposed method can overcome the above difficulties successfully, and it possesses the property of global optimality. The experiments on both synthetic and real data validated that the proposed algorithm is robust to various types of disturbances. It can robustly detect the desired shapes in complex and highly cluttered scenes."'),
('"Saliency Based Opportunistic Search for Object Part Extraction and Labeling"', '"ECCV 2008"', '["Object Detection", "Shape Model", "Semantic Context", "Shape Match", "Object Part"]', '"https://doi.org/10.1007/978-3-540-88693-8_56"', '"We study the task of object part extraction and labeling, which seeks to understand objects beyond simply identifiying their bounding boxes. We start from bottom-up segmentation of images and search for correspondences between object parts in a few shape models and segments in images. Segments comprising different object parts in the image are usually not equally salient due to uneven contrast, illumination conditions, clutter, occlusion and pose changes. Moreover, object parts may have different scales and some parts are only distinctive and recognizable in a large scale. Therefore, we utilize a multi-scale shape representation of objects and their parts, figural contextual information of the whole object and semantic contextual information for parts. Instead of searching over a large segmentation space, we present a saliency based opportunistic search framework to explore bottom-up segmentation by gradually expanding and bounding the search domain. We tested our approach on a challenging statue face dataset and 3 human face datasets. Results show that our approach significantly outperforms Active Shape Models using far fewer exemplars. Our framework can be applied to other object categories."'),
('"Saliency Detection with Flash and No-flash Image Pairs"', '"ECCV 2014"', '["Saliency detection", "Flash photography", "Background elimination", "Surface orientation"]', '"https://doi.org/10.1007/978-3-319-10578-9_8"', '"In this paper, we propose a new saliency detection method using a pair of flash and no-flash images. Our approach is inspired by two observations. First, only the foreground objects are significantly brightened by the flash as they are relatively nearer to the camera than the background. Second, the brightness variations introduced by the flash provide hints to surface orientation changes. Accordingly, the first observation is explored to form the background prior to eliminate background distraction. The second observation provides a new orientation cue to compute surface orientation contrast. These photometric cues from the two observations are independent of visual attributes like color, and they provide new and robust distinctiveness to support salient object detection. The second observation further leads to the introduction of new spatial priors to constrain the regions rendered salient to be compact both in the image plane and in 3D space. We have constructed a new flash/no-flash image dataset. Experiments on this dataset show that the proposed method successfully identifies salient objects from various challenging scenes that the state-of-the-art methods usually fail."'),
('"Saliency in Crowd"', '"ECCV 2014"', '["visual attention", "saliency", "crowd", "multiple kernel learning"]', '"https://doi.org/10.1007/978-3-319-10584-0_2"', '"Theories and models on saliency that predict where people look at focus on regular-density scenes. A crowded scene is characterized by the co-occurrence of a relatively large number of regions/objects that would have stood out if in a regular scene, and what drives attention in crowd can be significantly different from the conclusions in the regular setting. This work presents a first focused study on saliency in crowd. To facilitate saliency in crowd study, a new dataset of 500 images is constructed with eye tracking data from 16 viewers and annotation data on faces (the dataset will be publicly available with the paper). Statistical analyses point to key observations on features and mechanisms of saliency in scenes with different crowd levels and provide insights as of whether conventional saliency models hold in crowding scenes. Finally a new model for saliency prediction that takes into account the crowding information is proposed, and multiple kernel learning (MKL) is used as a core computational module to integrate various features at both low- and high-levels. Extensive experiments demonstrate the superior performance of the proposed model compared with the state-of-the-art in saliency computation."'),
('"Saliency Maps of High Dynamic Range Images"', '"ECCV 2010"', '["Saliency Map", "High Dynamic Range", "Eye Tracking"]', '"https://doi.org/10.1007/978-3-642-35740-4_10"', '"A number of computational models of visual attention have been proposed based on the concept of saliency map. Some of them have been validated as predictors of the visual scan-path of observers looking at images and videos, using oculometric data. They are widely used for Computer Graphics applications, mainly for image rendering, in order to avoid spending too much computing time on non salient areas, and in video coding, in order to keep a better image quality in salient areas. However, these algorithms were not used so far with High Dynamic Range (HDR) inputs. In this paper, we show that in the case of HDR images, the predictions using algorithms based on Itti, Koch and Niebur [1] are less accurate than with 8-bit images. To improve the saliency computation for HDR inputs, we propose a new algorithm derived from Itti and Koch [3]. From an eye tracking experiment with a HDR scene, we show that this algorithm leads to good results for the saliency map computation, with a better fit between the saliency map and the ocular fixation map than Itti, Koch and Niebur\\u2019s algorithm. These results may impact image retargeting issues, for the display of HDR images on both LDR and HDR display devices."'),
('"Saliency Modeling from Image Histograms"', '"ECCV 2012"', '["Attention", "saliency modeling", "co-occurrence histogram"]', '"https://doi.org/10.1007/978-3-642-33786-4_24"', '"We proposed a computational visual saliency modeling technique. The proposed technique makes use of a color co-occurrence histogram (CCH) that captures not only \\u201chow many\\u201d but also \\u201cwhere and how\\u201d image pixels are composed into a visually perceivable image. Hence the CCH encodes image saliency information that is usually perceived as the discontinuity between an image region or object and its surrounding. The proposed technique has a number of distinctive characteristics: It is fast, discriminative, tolerant to image scale variation, and involves minimal parameter tuning. Experiments over benchmarking datasets show that it predicts fixational eye tracking points accurately and a superior AUC of 71.25 is obtained."'),
('"Saliency Weighted Features for Person Re-identification"', '"ECCV 2014"', '["Local Binary Pattern", "Salient Region", "Saliency Detection", "Local Binary Pattern Feature", "Ca', '"https://doi.org/10.1007/978-3-319-16199-0_14"', '"In this work we propose a novel person re-identification approach. The solution, inspired by human gazing capabilities, wants to identify the salient regions of a given person. Such regions are used as a weighting tool in the image feature extraction process. Then, such novel representation is combined with a set of other visual features in a pairwise-based multiple metric learning framework. Finally, the learned metrics are fused to get the distance between image pairs and to re-identify a person. The proposed method is evaluated on three different benchmark datasets and compared with best state-of-the-art approaches to show its overall superior performance."'),
('"Salient Color Names for Person Re-identification"', '"ECCV 2014"', '["Salient color names", "color descriptor", "feature representation", "person re-identification"]', '"https://doi.org/10.1007/978-3-319-10590-1_35"', '"Color naming, which relates colors with color names, can help people with a semantic analysis of images in many computer vision applications. In this paper, we propose a novel salient color names based color descriptor (SCNCD) to describe colors. SCNCD utilizes salient color names to guarantee that a higher probability will be assigned to the color name which is nearer to the color. Based on SCNCD, color distributions over color names in different color spaces are then obtained and fused to generate a feature representation. Moreover, the effect of background information is employed and analyzed for person re-identification. With a simple metric learning method, the proposed approach outperforms the state-of-the-art performance (without user\\u2019s feedback optimization) on two challenging datasets (VIPeR and PRID 450S). More importantly, the proposed feature can be obtained very fast if we compute SCNCD of each color in advance."'),
('"Salient Montages from Unconstrained Videos"', '"ECCV 2014"', '["video summarization", "video saliency detection"]', '"https://doi.org/10.1007/978-3-319-10584-0_31"', '"We present a novel method to generate salient montages from unconstrained videos, by finding \\u201cmontageable moments\\u201d and identifying the salient people and actions to depict in each montage. Our method addresses the need for generating concise visualizations from the increasingly large number of videos being captured from portable devices. Our main contributions are (1) the process of finding salient people and moments to form a montage, and (2) the application of this method to videos taken \\u201cin the wild\\u201d where the camera moves freely. As such, we demonstrate results on head-mounted cameras, where the camera moves constantly, as well as on videos downloaded from YouTube. Our approach can operate on videos of any length; some will contain many montageable moments, while others may have none. We demonstrate that a novel \\u201cmontageability\\u201d score can be used to retrieve results with relatively high precision which allows us to present high quality montages to users."'),
('"Salient Object Detection: A Benchmark"', '"ECCV 2012"', '["Large Object", "Salient Object", "Model Ranking", "Salient Region", "Saliency Detection"]', '"https://doi.org/10.1007/978-3-642-33709-3_30"', '"Several salient object detection approaches have been published which have been assessed using different evaluation scores and datasets resulting in discrepancy in model comparison. This calls for a methodological framework to compare existing models and evaluate their pros and cons. We analyze benchmark datasets and scoring techniques and, for the first time, provide a quantitative comparison of 35 state-of-the-art saliency detection models. We find that some models perform consistently better than the others. Saliency models that intend to predict eye fixations perform lower on segmentation datasets compared to salient object detection algorithms. Further, we propose combined models which show that integration of the few best models outperforms all models over other datasets. By analyzing the consistency among the best models and among humans for each scene, we identify the scenes where models or humans fail to detect the most salient object. We highlight the current issues and propose future research directions."'),
('"Sample Sufficiency and PCA Dimension for Statistical Shape Models"', '"ECCV 2008"', '["Principal Component Analysis", "Synthetic Dataset", "Principal Component Analysis Model", "Princip', '"https://doi.org/10.1007/978-3-540-88693-8_36"', '"Statistical shape modelling(SSM) is a popular technique in computer vision applications, where the variation of shape of a given structure is modelled by principal component analysis (PCA) on a set of training samples. The issue of sample size sufficiency is not generally considered. In this paper, we propose a framework to investigate the sources of SSM inaccuracy. Based on this framework, we propose a procedure to determine sample size sufficiency by testing whether the training data stabilises the SSM. Also, the number of principal modes to retain (PCA dimension) is usually chosen using rules that aim to cover a percentage of the total variance or to limit the residual to a threshold. However, an ideal rule should retain modes that correspond to real structural variation and discard those that are dominated by noise. We show that these commonly used rules are not reliable, and we propose a new rule that uses bootstrap stability analysis on mode directions to determine the PCA dimension."'),
('"Sampling Representative Examples for Dimensionality Reduction and Recognition \\u2013 Bootstrap Bump', '"ECCV 2006"', '["Bootstrap Sample", "Sampling Ratio", "Quadratic Discriminant Analysis", "Gait Recognition", "Compu', '"https://doi.org/10.1007/11744078_22"', '"We present a novel method for dimensionality reduction and recognition based on Linear Discriminant Analysis (LDA), which specifically deals with the Small Sample Size (SSS) problem in Computer Vision applications. Unlike the traditional methods, which impose specific assumptions to address the SSS problem, our approach introduces a variant of bootstrap bumping technique, which is a general framework in statistics for model search and inference. An intermediate linear representation is first hypothesized from each bootstrap sample. Then LDA is performed in the reduced subspace. Lastly, the final model is selected among all hypotheses for the best classification. Experiments on synthetic and real datasets demonstrate the advantages of our Bootstrap Bumping LDA (BB-LDA) approach over the traditional LDA based methods."'),
('"Sampling Strategies for Bag-of-Features Image Classification"', '"ECCV 2006"', '["Interest Point", "Sift Descriptor", "Interest Operator", "Codebook Size", "Interest Point Detector', '"https://doi.org/10.1007/11744085_38"', '"Bag-of-features representations have recently become popular for content based image classification owing to their simplicity and good performance. They evolved from texton methods in texture analysis. The basic idea is to treat images as loose collections of independent patches, sampling a representative set of patches from the image, evaluating a visual descriptor vector for each patch independently, and using the resulting distribution of samples in descriptor space as a characterization of the image. The four main implementation choices are thus how to sample patches, how to describe them, how to characterize the resulting distributions and how to classify images based on the result. We concentrate on the first issue, showing experimentally that for a representative selection of commonly used test databases and for moderate to large numbers of samples, random sampling gives equal or better classifiers than the sophisticated multiscale interest operators that are in common use. Although interest operators work well for small numbers of samples, the single most important factor governing performance is the number of patches sampled from the test image and ultimately interest operators can not provide enough patches to compete. We also study the influence of other factors including codebook size and creation method, histogram normalization method and minimum scale for feature extraction."'),
('"Scalable 6-DOF Localization on Mobile Devices"', '"ECCV 2014"', '["Mobile Device", "Orientation Error", "Bundle Adjustment", "Global Localization", "Reprojection Err', '"https://doi.org/10.1007/978-3-319-10605-2_18"', '"Recent improvements in image-based localization have produced powerful methods that scale up to the massive 3D models emerging from modern Structure-from-Motion techniques. However, these approaches are too resource intensive to run in real-time, let alone to be implemented on mobile devices. In this paper, we propose to combine the scalability of such a global localization system running on a server with the speed and precision of a local pose tracker on a mobile device. Our approach is both scalable and drift-free by design and eliminates the need for loop closure. We propose two strategies to combine the information provided by local tracking and global localization. We evaluate our system on a large-scale dataset of the historic inner city of Aachen where it achieves interactive framerates at a localization error of less than 50cm while using less than 5MB of memory on the mobile device."'),
('"Scale Dependent Differential Geometry for the Measurement of Center Line and Diameter in 3D Curvili', '"ECCV 2000"', '["Center Line", "Diameter Measurement", "Line Direction", "Synthetic Image", "Line Structure"]', '"https://doi.org/10.1007/3-540-45054-8_56"', '"We introduce a 3D tracing method based on differential geometry in Gaussian blurred images. The line point detection part of the tracing method starts with calculation of the line direction from the eigenvectors of the Hessian matrix. The sub-voxel center line position is estimated from a second order Taylor approximation of the 2D intensity profile perpendicular to the line. In curved line structures the method turns out to be biased. We model the bias in center line position using the first order Taylor expansion of the gradient in scale and position. Based on this model we found that the bias in a torus with a generalized line profile was proportional to \\u03c32. This result was applied in a procedure to remove the bias and to measure the radius of curvature in a curved line structure."'),
('"Scale Invariant Action Recognition Using Compound Features Mined from Dense Spatio-temporal Corners', '"ECCV 2008"', '["Association Rule", "Action Recognition", "Interest Point", "Frequent Itemset", "Mining Association', '"https://doi.org/10.1007/978-3-540-88682-2_18"', '"The use of sparse invariant features to recognise classes of actions or objects has become common in the literature. However, features are often \\u201dengineered\\u201d to be both sparse and invariant to transformation and it is assumed that they provide the greatest discriminative information. To tackle activity recognition, we propose learning compound features that are assembled from simple 2D corners in both space and time. Each corner is encoded in relation to its neighbours and from an over complete set (in excess of 1 million possible features), compound features are extracted using data mining. The final classifier, consisting of sets of compound features, can then be applied to recognise and localise an activity in real-time while providing superior performance to other state-of-the-art approaches (including those based upon sparse feature detectors). Furthermore, the approach requires only weak supervision in the form of class labels for each training sequence. No ground truth position or temporal alignment is required during training."'),
('"Scale Invariant Optical Flow"', '"ECCV 2012"', '["Optical Flow", "Scale Variation", "Feature Match", "Optical Flow Estimation", "Descriptor Match"]', '"https://doi.org/10.1007/978-3-642-33709-3_28"', '"Scale variation commonly arises in images/videos, which cannot be naturally dealt with by optical flow. Invariant feature matching, on the contrary, provides sparse matching and could fail for regions without conspicuous structures. We aim to establish dense correspondence between frames containing objects in different scales and contribute a new framework taking pixel-wise scales into consideration in optical flow estimation. We propose an effective numerical scheme, which iteratively optimizes discrete scale variables and continuous flow ones. This scheme notably expands the practicality of optical flow in natural scenes containing various types of object motion."'),
('"Scale Robust Multi View Stereo"', '"ECCV 2012"', '["Point Cloud", "Stereo Match", "Local Outlier", "Matching Error", "Visual Hull"]', '"https://doi.org/10.1007/978-3-642-33712-3_29"', '"We present a Multi View Stereo approach for huge unstructured image datasets that can deal with large variations in surface sampling rate of single images. Our method reconstructs surface parts always in the best available resolution. It considers scaling not only for large scale differences, but also between arbitrary small ones for a weighted merging of the best partial reconstructions. We create depth maps with our GPU based depth map algorithm, that also performs normal optimization. It matches several images that are found with a heuristic image selection method, to a reference image. We remove outliers by comparing depth maps against each other with a fast but reliable GPU approach. Then, we merge the different reconstructions from depth maps in 3D space by selecting the best points and optimizing them with not selected points. Finally, we create the surface by using a Delaunay graph cut."'),
('"Scale-Dependent/Invariant Local 3D Shape Descriptors for Fully Automatic Registration of Multiple S', '"ECCV 2008"', '["Geodesic Distance", "Shape Descriptor", "Range Image", "Registration Algorithm", "Automatic Regist', '"https://doi.org/10.1007/978-3-540-88690-7_33"', '"Despite the ubiquitous use of range images in various computer vision applications, little has been investigated about the size variation of the local geometric structures captured in the range images. In this paper, we show that, through canonical geometric scale-space analysis, this geometric scale-variability embedded in a range image can be exploited as a rich source of discriminative information regarding the captured geometry. We extend previous work on geometric scale-space analysis of 3D models to analyze the scale-variability of a range image and to detect scale-dependent 3D features \\u2013 geometric features with their inherent scales. We derive novel local 3D shape descriptors that encode the local shape information within the inherent support region of each feature. We show that the resulting set of scale-dependent local shape descriptors can be used in an efficient hierarchical registration algorithm for aligning range images with the same global scale. We also show that local 3D shape descriptors invariant to the scale variation can be derived and used to align range images with significantly different global scales. Finally, we demonstrate that the scale-dependent/invariant local 3D shape descriptors can even be used to fully automatically register multiple sets of range images with varying global scales corresponding to multiple objects."'),
('"Scale-Space Diagnostic Criterion for Microscopic Image Analysis"', '"MMBIA 2004"', '["Chronic Lymphocytic Leukemia", "Malignant Mesothelioma", "Chronic Lymphatic Leukemia", "Linear Fis', '"https://doi.org/10.1007/978-3-540-27816-0_35"', '"In this paper, a new criterion for diagnostics of hematopoietic tumors from images of cell nuclei of lymphatic nodes is presented. A method for image analysis of lymphatic node specimens is developed on the basis of the scale-space approach. A diagnostically important criterion is defined as a total amount of points of spatial intensity extrema in the families of blurred images generated by the given image of a cell nucleus. The procedure for calculating criterion values is presented. Testing of the obtained criterion is carried out using different classifiers. The accuracy of diagnostics is greater than 81% for collective classifiers."'),
('"Scene Aligned Pooling for Complex Video Recognition"', '"ECCV 2012"', '["Local Binary Pattern", "Vector Quantization", "Sift Feature", "Video Event", "Wedding Ceremony"]', '"https://doi.org/10.1007/978-3-642-33709-3_49"', '"Real-world videos often contain dynamic backgrounds and evolving people activities, especially for those web videos generated by users in unconstrained scenarios. This paper proposes a new visual representation, namely scene aligned pooling, for the task of event recognition in complex videos. Based on the observation that a video clip is often composed with shots of different scenes, the key idea of scene aligned pooling is to decompose any video features into concurrent scene components, and to construct classification models adaptive to different scenes. The experiments on two large scale real-world datasets including the TRECVID Multimedia Event Detection 2011 and the Human Motion Recognition Databases (HMDB) show that our new visual representation can consistently improve various kinds of visual features such as different low-level color and texture features, or middle-level histogram of local descriptors such as SIFT, or space-time interest points, and high level semantic model features, by a significant margin. For example, we improve the-state-of-the-art accuracy on HMDB dataset by 20% in terms of accuracy."'),
('"Scene and Motion Reconstruction from Defocused and Motion-Blurred Images via Anisotropic Diffusion"', '"ECCV 2004"', '["Image Plane", "Point Spread Function", "Blind Deconvolution", "Camera Shutter", "Defocused Image"]', '"https://doi.org/10.1007/978-3-540-24670-1_20"', '"We propose a solution to the problem of inferring the depth map, radiance and motion of a scene from a collection of motion-blurred and defocused images. We model motion-blur and defocus as an anisotropic diffusion process, whose initial conditions depend on the radiance and whose diffusion tensor encodes the shape of the scene, the motion field and the optics parameters. We show that this model is well-posed and propose an efficient algorithm to infer the unknowns of the model. Inference is performed by minimizing the discrepancy between the measured blurred images and the ones synthesized via forward diffusion. Since the problem is ill-posed, we also introduce additional Tikhonov regularization terms. The resulting method is fast and robust to noise as shown by experiments with both synthetic and real data."'),
('"Scene Carving: Scene Consistent Image Retargeting"', '"ECCV 2010"', '["Background Image", "Object Occlusion", "Object Protection", "Object Positionings", "Occlusion Boun', '"https://doi.org/10.1007/978-3-642-15549-9_11"', '"Image retargeting algorithms often create visually disturbing distortion. We introduce the property of scene consistency, which is held by images which contain no object distortion and have the correct object depth ordering. We present two new image retargeting algorithms that preserve scene consistency. These algorithms make use of a user-provided relative depth map, which can be created easily using a simple GrabCut-style interface. Our algorithms generalize seam carving. We decompose the image retargeting procedure into (a) removing image content with minimal distortion and (b) re-arrangement of known objects within the scene to maximize their visibility. Our algorithms optimize objectives (a) and (b) jointly. However, they differ considerably in how they achieve this. We discuss this in detail and present examples illustrating the rationale of preserving scene consistency in retargeting."'),
('"Scene Chronology"', '"ECCV 2014"', '["Structure from motion", "temporal reasoning", "4D modeling"]', '"https://doi.org/10.1007/978-3-319-10584-0_40"', '"We present a new method for taking an urban scene reconstructed from a large Internet photo collection and reasoning about its change in appearance through time. Our method estimates when individual 3D points in the scene existed, then uses spatial and temporal affinity between points to segment the scene into spatio-temporally consistent clusters. The result of this segmentation is a set of spatio-temporal objects that often correspond to meaningful units, such as billboards, signs, street art, and other dynamic scene elements, along with estimates of when each existed. Our method is robust and scalable to scenes with hundreds of thousands of images and billions of noisy, individual point observations. We demonstrate our system on several large-scale scenes, and demonstrate an application to time stamping photos. Our work can serve to chronicle a scene over time, documenting its history and discovering dynamic elements in a way that can be easily explored and visualized."'),
('"Scene Classification via Hypergraph-Based Semantic Attributes Subnetworks Identification"', '"ECCV 2014"', '["Scene classification", "Semantic attribute", "Hypergraph", "SVM"]', '"https://doi.org/10.1007/978-3-319-10584-0_24"', '"Scene classification is an important issue in computer vision area. However, it is still a challenging problem due to the variability, ambiguity, and scale change that exist commonly in images. In this paper, we propose a novel hypergraph-based modeling that considers the higher-order relationship of semantic attributes in a scene and apply it to scene classification. By searching subnetworks on a hypergraph, we extract the interaction subnetworks of the semantic attributes that are optimized for classifying individual scene categories. In addition, we propose a method to aggregate the expression values of the member semantic attributes which belongs to the explored subnetworks using the transformation method via likelihood ratio based estimation. Intensive experiment shows that the discrimination power of the feature vector generated by the proposed method is better than the existing methods. Consequently, it is shown that the proposed method outperforms the conventional methods in the scene classification task."'),
('"Scene Classification Via pLSA"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_40"', '"Given a set of images of scenes containing multiple object categories (e.g. grass, roads, buildings) our objective is to discover these objects in each image in an unsupervised manner, and to use this object distribution to perform scene classification. We achieve this discovery using probabilistic Latent Semantic Analysis (pLSA), a generative model from the statistical text literature, here applied to a bag of visual words representation for each image. The scene classification on the object distribution is carried out by a k-nearest neighbour classifier."'),
('"Scene Discovery by Matrix Factorization"', '"ECCV 2008"', '["Matrix Factorization", "Latent Dirichlet Allocation Model", "Supervise Approach", "Word Annotation', '"https://doi.org/10.1007/978-3-540-88693-8_33"', '"What constitutes a scene? Defining a meaningful vocabulary for scene discovery is a challenging problem that has important consequences for object recognition. We consider scenes to depict correlated objects and present visual similarity. We introduce a max-margin factorization model that finds a low dimensional subspace with high discriminative power for correlated annotations. We postulate this space should allow us to discover a large number of scenes in unsupervised data; we show scene discrimination results on par with supervised approaches. This model also produces state of the art word prediction results including good annotation completion."'),
('"Scene Recognition on the Semantic Manifold"', '"ECCV 2012"', '["Recognition Rate", "Geodesic Distance", "Semantic Space", "Linear Support Vector Machine", "Scene ', '"https://doi.org/10.1007/978-3-642-33765-9_26"', '"A new architecture, denoted spatial pyramid matching on the semantic manifold (SPMSM), is proposed for scene recognition. SPMSM is based on a recent image representation on a semantic probability simplex, which is now augmented with a rough encoding of spatial information. A connection between the semantic simplex and a Riemmanian manifold is established, so as to equip the architecture with a similarity measure that respects the manifold structure of the semantic space. It is then argued that the closed-form geodesic distance between two manifold points is a natural measure of similarity between images. This leads to a conditionally positive definite kernel that can be used with any SVM classifier. An approximation of the geodesic distance reveals connections to the well-known Bhattacharyya kernel, and is explored to derive an explicit feature embedding for this kernel, by simple square-rooting. This enables a low-complexity SVM implementation, using a linear SVM on the embedded features. Several experiments are reported, comparing SPMSM to state-of-the-art recognition methods. SPMSM is shown to achieve the best recognition rates in the literature for two large datasets (MIT Indoor and SUN) and rates equivalent or superior to the state-of-the-art on a number of smaller datasets. In all cases, the resulting SVM also has much smaller dimensionality and requires much fewer support vectors than previous classifiers. This guarantees much smaller complexity and suggests improved generalization beyond the datasets considered."'),
('"Scene Segmentation for Behaviour Correlation"', '"ECCV 2008"', '["False Alarm", "Video Clip", "Anomaly Detection", "Spectral Cluster", "Image Event"]', '"https://doi.org/10.1007/978-3-540-88693-8_28"', '"This paper presents a novel framework for detecting abnormal pedestrian and vehicle behaviour by modelling cross-correlation among different co-occurring objects both locally and globally in a given scene. We address this problem by first segmenting a scene into semantic regions according to how object events occur globally in the scene, and second modelling concurrent correlations among regional object events both locally (within the same region) and globally (across different regions). Instead of tracking objects, the model represents behaviour based on classification of atomic video events, designed to be more suitable for analysing crowded scenes. The proposed system works in an unsupervised manner throughout using automatic model order selection to estimate its parameters given video data of a scene for a brief training period. We demonstrate the effectiveness of this system with experiments on public road traffic data."'),
('"Scene Segmentation Using the Wisdom of Crowds"', '"ECCV 2008"', '["Static Scene", "Photo Collection", "Dirichlet Process Mixture", "Scene Segmentation", "Ground Trut', '"https://doi.org/10.1007/978-3-540-88688-4_40"', '"Given a collection of images of a static scene taken by many different people, we identify and segment interesting objects. To solve this problem, we use the distribution of images in the collection along with a new field-of-view cue, which leverages the observation that people tend to take photos that frame an object of interest within the field of view. Hence, image features that appear together in many images are likely to be part of the same object. We evaluate the effectiveness of this cue by comparing the segmentations computed by our method against hand-labeled ones for several different models. We also show how the results of our segmentations can be used to highlight important objects in the scene and label them using noisy user-specified textual tag data. These methods are demonstrated on photos of several popular tourist sites downloaded from the Internet."'),
('"Scene Semantics from Long-Term Observation of People"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_21"', '"Our everyday objects support various tasks and can be used by people for different purposes. While object classification is a widely studied topic in computer vision, recognition of object function, i.e., what people can do with an object and how they do it, is rarely addressed. In this paper we construct a functional object description with the aim to recognize objects by the way people interact with them. We describe scene objects (sofas, tables, chairs) by associated human poses and object appearance. Our model is learned discriminatively from automatically estimated body poses in many realistic scenes. In particular, we make use of time-lapse videos from YouTube providing a rich source of common human-object interactions and minimizing the effort of manual object annotation. We show how the models learned from human observations significantly improve object recognition and enable prediction of characteristic human poses in new scenes. Results are shown on a dataset of more than 400,000 frames obtained from 146 time-lapse videos of challenging and realistic indoor scenes."'),
('"Scene-Dependent Intention Recognition for Task Communication with Reduced Human-Robot Interaction"', '"ECCV 2014"', '["Intention recognition", "Human robot interaction", "Intelligent robots", "Robot vision systems"]', '"https://doi.org/10.1007/978-3-319-16199-0_51"', '"In order for assistive robots to collaborate effectively with humans, they must be endowed with the ability to perceive scenes and more importantly, recognize human intentions. These intentions are often inferred from observed physical actions and direct communication from fully-functional individuals. For individuals with reduced capabilities, it may be difficult or impossible to perform physical actions or easily communicate. Therefore, their intentions must be inferred differently. To this end, we propose an intention recognition framework that is appropriate for persons with limited physical capabilities. This framework determines and learns human intentions based on scene objects, the actions that can be performed on them, and past interaction history. It is based on a Markov model formulation entitled Object-Action Intention Networks, which constitute the crux of a computer vision-based human-robot collaborative system that reduces the necessary interactions for communicating tasks to a robot. Evaluations were conducted on multiple scenes comprised of multiple possible object categories and actions. We achieve approximately 81% reduction in interactions overall after learning, when compared to other intention recognition approaches."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"SceneNet: A Perceptual Ontology for Scene Understanding"', '"ECCV 2014"', '["Scene understanding", "Scene gist recognition", "Scene categories", "Perceptual relations", "Perce', '"https://doi.org/10.1007/978-3-319-16181-5_27"', '"Scene recognition systems which attempt to deal with a large number of scene categories currently lack proper knowledge about the perceptual ontology of scene categories and would enjoy significant advantage from a perceptually meaningful scene representation. In this work we perform a large-scale human study to create \\u201cSceneNet\\u201d, an online ontology database for scene understanding that organizes scene categories according to their perceptual relationships. This perceptual ontology suggests that perceptual relationships do not always conform the semantic structure between categories, and it entails a lower dimensional perceptual space with \\u201cperceptually meaningful\\u201d Euclidean distance, where each embedded category is represented by a single prototype. Using the SceneNet ontology and database we derive a computational scheme for learning non-linear mapping of scene images into the perceptual space, where each scene image is closest to its category prototype than to any other prototype by a large margin. Then, we demonstrate how this approach facilitates improvements in large-scale scene categorization over state-of-the-art methods and existing semantic ontologies, and how it reveals novel perceptual findings about the discriminative power of visual attributes and the typicality of scenes."'),
('"Schr\\u00f6dinger Diffusion for Shape Analysis with Texture"', '"ECCV 2012"', '["Laplace-Beltrami operator", "textured shape retrieval", "diffusion distance", "Schr\\u00f6dinger op', '"https://doi.org/10.1007/978-3-642-33863-2_13"', '"In recent years, quantities derived from the heat equation have become popular in shape processing and analysis of triangulated surfaces. Such measures are often robust with respect to different kinds of perturbations, including near-isometries, topological noise and partialities. Here, we propose to exploit the semigroup of a Schr\\u00f6dinger operator in order to deal with texture data, while maintaining the desirable properties of the heat kernel. We define a family of Schr\\u00f6dinger diffusion distances analogous to the ones associated to the heat kernels, and show that they are continuous under perturbations of the data. As an application, we introduce a method for retrieval of textured shapes through comparison of Schr\\u00f6dinger diffusion distance histograms with the earth\\u2019s mover distance, and present some numerical experiments showing superior performance compared to an analogous method that ignores the texture."'),
('"Schwarps: Locally Projective Image Warps Based on 2D Schwarzian Derivatives"', '"ECCV 2014"', '["Schwarzian Penalizer", "Bending Energy", "Projective Differential Invariants", "Image Warps"]', '"https://doi.org/10.1007/978-3-319-10593-2_1"', '"Image warps -or just warps- capture the geometric deformation existing between two images of a deforming surface. The current approach to enforce a warp\\u2019s smoothness is to penalize its second order partial derivatives. Because this favors locally affine warps, this fails to capture the local projective component of the image deformation. This may have a negative impact on applications such as image registration and deformable 3D reconstruction. We propose a novel penalty designed to smooth the warp while capturing the deformation\\u2019s local projective structure. Our penalty is based on equivalents to the Schwarzian derivatives, which are projective differential invariants exactly preserved by homographies. We propose a methodology to derive a set of Partial Differential Equations with homographies as solutions. We call this system the Schwarzian equations and we explicitly derive them for 2D functions using differential properties of homographies. We name as Schwarp a warp which is estimated by penalizing the residual of Schwarzian equations. Experimental evaluation shows that Schwarps outperform existing warps in modeling and extrapolation power, and lead to far better results in Shape-from-Template and camera calibration from a deformable surface."'),
('"Script Data for Attribute-Based Recognition of Composite Activities"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_11"', '"State-of-the-art human activity recognition methods build on discriminative learning which requires a representative training set for good performance. This leads to scalability issues for the recognition of large sets of highly diverse activities. In this paper we leverage the fact that many human activities are compositional and that the essential components of the activities can be obtained from textual descriptions or scripts. To share and transfer knowledge between composite activities we model them by a common set of attributes corresponding to basic actions and object participants. This attribute representation allows to incorporate script data that delivers new variations of a composite activity or even to unseen composite activities. In our experiments on 41 composite cooking tasks, we found that script data to successfully capture the high variability of composite activities. We show improvements in a supervised case where training data for all composite cooking tasks is available, but we are also able to recognize unseen composites by just using script data and without any manual video annotation."'),
('"Seam Segment Carving: Retargeting Images to Irregularly-Shaped Image Domains"', '"ECCV 2012"', '["Image Retargeting", "Seam Carving"]', '"https://doi.org/10.1007/978-3-642-33783-3_23"', '"Image retargeting algorithms aim to adapt the image to the display screen with the goal of preserving the image content as much as possible. However, existing methods and research efforts have mostly been directed towards retargeting algorithms that retarget images to rectangular domains. This significantly hampers its application to broader classes of display devices and platforms for which the display area can be of any origins and shapes. For example, seam carving-based methods retarget images by carving out seams that run from the top to the bottom of the images, and this results in changing the width and therefore aspect ratio of the image without changing the shape of the image boundary in any essential way. However, by carving out appropriately-chosen seam segments, seams that are not required to cut across the entire image, it is then possible to retarget the images to a broader array of image domains with non-rectangular boundaries. Based on this simple idea of carving out the seam segments, the main contribution of this paper is a novel image retargeting algorithm that is capable of retargeting images to non-rectangular domains. We evaluate the effectiveness of the proposed method on a number of challenging indoor and outdoor scene images, and the results demonstrate that the proposed algorithm is both efficient and effective, and it is capable of providing good-quality retargeted images for a variety of interesting boundary shapes."'),
('"Seamless Image Stitching in the Gradient Domain"', '"ECCV 2004"', '["Input Image", "Texture Synthesis", "Panoramic Image", "Image Inpainting", "Gradient Domain"]', '"https://doi.org/10.1007/978-3-540-24673-2_31"', '"Image stitching is used to combine several individual images having some overlap into a composite image. The quality of image stitching is measured by the similarity of the stitched image to each of the input images, and by the visibility of the seam between the stitched images."'),
('"Search Space Reduction for MRF Stereo"', '"ECCV 2008"', '["Markov Random Field", "Search Range", "Stereo Image", "Stereo Match", "Match Cost"]', '"https://doi.org/10.1007/978-3-540-88682-2_44"', '"We present an algorithm to reduce per-pixel search ranges for Markov Random Fields-based stereo algorithms. Our algorithm is based on the intuitions that reliably matched pixels need less regularization in the energy minimization and neighboring pixels should have similar disparity search ranges if their pixel values are similar. We propose a novel bi-labeling process to classify reliable and unreliable pixels that incorporate left-right consistency checks. We then propagate the reliable disparities into unreliable regions to form a complete disparity map and construct per-pixel search ranges based on the difference between the disparity map after propagation and the one computed from a winner-take-all method. Experimental results evaluated on the Middlebury stereo benchmark show our proposed algorithm is able to achieve 77% average reduction rate while preserving satisfactory accuracy."'),
('"Searching the World\\u2019s Herbaria: A System for Visual Identification of Plant Species"', '"ECCV 2008"', '["Augmented Reality", "Type Specimen", "Leaf Shape", "Shape Match", "Leaf Image"]', '"https://doi.org/10.1007/978-3-540-88693-8_9"', '"We describe a working computer vision system that aids in the identification of plant species. A user photographs an isolated leaf on a blank background, and the system extracts the leaf shape and matches it to the shape of leaves of known species. In a few seconds, the system displays the top matching species, along with textual descriptions and additional images. This system is currently in use by botanists at the Smithsonian Institution National Museum of Natural History. The primary contributions of this paper are: a description of a working computer vision system and its user interface for an important new application area; the introduction of three new datasets containing thousands of single leaf images, each labeled by species and verified by botanists at the US National Herbarium; recognition results for two of the three leaf datasets; and descriptions throughout of practical lessons learned in constructing this system."'),
('"SEEDS: Superpixels Extracted via Energy-Driven Sampling"', '"ECCV 2012"', '["superpixels", "segmentation"]', '"https://doi.org/10.1007/978-3-642-33786-4_2"', '"Superpixel algorithms aim to over-segment the image by grouping pixels that belong to the same object. Many state-of-the-art superpixel algorithms rely on minimizing objective functions to enforce color homogeneity. The optimization is accomplished by sophisticated methods that progressively build the superpixels, typically by adding cuts or growing superpixels. As a result, they are computationally too expensive for real-time applications. We introduce a new approach based on a simple hill-climbing optimization. Starting from an initial superpixel partitioning, it continuously refines the superpixels by modifying the boundaries. We define a robust and fast to evaluate energy function, based on enforcing color similarity between the boundaries and the superpixel color histogram. In a series of experiments, we show that we achieve an excellent compromise between accuracy and efficiency. We are able to achieve a performance comparable to the state-of-the-art, but in real-time on a single Intel i7 CPU at 2.8GHz."'),
('"Seeing is Worse than Believing: Reading People\\u2019s Minds Better than Computer-Vision Methods Rec', '"ECCV 2014"', '["action recognition", "fMRI"]', '"https://doi.org/10.1007/978-3-319-10602-1_40"', '"We had human subjects perform a one-out-of-six class action recognition task from video stimuli while undergoing functional magnetic resonance imaging (fMRI). Support-vector machines (SVMs) were trained on the recovered brain scans to classify actions observed during imaging, yielding average classification accuracy of 69.73% when tested on scans from the same subject and of 34.80% when tested on scans from different subjects. An apples-to-apples comparison was performed with all publicly available software that implements state-of-the-art action recognition on the same video corpus with the same cross-validation regimen and same partitioning into training and test sets, yielding classification accuracies between 31.25% and 52.34%. This indicates that one can read people\\u2019s minds better than state-of-the-art computer-vision methods can perform action recognition."'),
('"Seeing People in Social Context: Recognizing People and Social Relationships"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15555-0_13"', '"The people in an image are generally not strangers, but instead often share social relationships such as husband-wife, siblings, grandparent-child, father-child, or mother-child. Further, the social relationship between a pair of people influences the relative position and appearance of the people in the image. This paper explores using familial social relationships as context for recognizing people and for recognizing the social relationships between pairs of people. We introduce a model for representing the interaction between social relationship, facial appearance, and identity. We show that the family relationship a pair of people share influences the relative pairwise features between them. The experiments on a set of personal collections show significant improvement in people recognition is achieved by modeling social relationships, even in a weak label setting that is attractive in practical applications. Furthermore, we show the social relationships are effectively recognized in images from a separate test image collection."'),
('"Seeing through Obscure Glass"', '"ECCV 2010"', '["Latent Image", "Small Aperture", "Blur Kernel", "Entrance Pupil", "Camera Placement"]', '"https://doi.org/10.1007/978-3-642-15567-3_27"', '"Obscure glass is textured glass designed to separate spaces and \\u201cobscure\\u201d visibility between the spaces. Such glass is used to provide privacy while still allowing light to flow into a space, and is often found in homes and offices. We propose and explore the challenge of \\u201cseeing through\\u201d obscure glass, using both optical and digital techniques. In some cases \\u2013 such as when the textured surface is on the side of the observer \\u2013 we find that simple household substances and cameras with small apertures enable a surprising level of visibility through the obscure glass. In other cases, where optical techniques are not usable, we find that we can model the action of obscure glass as convolution of spatially varying kernels and reconstruct an image of the scene on the opposite side of the obscure glass with surprising detail."'),
('"Segmentation and Guidance of Multiple Rigid Objects for Intra-operative Endoscopic Vision"', '"WDV 2006"', '["Surgical Instrument", "Insertion Point", "Laparoscopic Instrument", "Visual Servoing", "Perspectiv', '"https://doi.org/10.1007/978-3-540-70932-9_24"', '"This paper presents an endoscopic vision framework for model-based 3D guidance of surgical instruments used in robotized laparoscopic surgery. In order to develop such a system, a variety of challenging segmentation, tracking and reconstruction problems must be solved. With this minimally invasive surgical technique, every single instrument has to pass through an insertion point in the abdominal wall and is mounted on the end-effector of a surgical robot which can be controlled by automatic visual feedback. The motion of any laparoscopic instrument is then constrained and the goal of the automated task is to safety bring instruments at desired locations while avoiding undesirable contact with internal organs. For this \\u201deye-to-hands\\u201d configuration with a stationary camera, most control strategies require the knowledge of the out-of-field of view insertion points location and we demonstrate it can be achieved in vivo thanks to a sequence of (instrument) motions without markers and without the need of an external measurement device. In so doing, we firstly present a real-time region-based color segmentation which integrates this motion constraint to initiate the search for region seeds. Secondly, a novel pose algorithm for the wide class of cylindrical-shaped instruments is developed which can handle partial occlusions as it is often the case in the abdominal cavity. The foreseen application is a good training ground to evaluate the robustness of segmentation algorithms and positioning techniques since main difficulties came from the scene understanding and its dynamical variations. Experiments in the lab and in real surgical conditions have been conducted. The experimental validation is demonstrated through the 3D positioning of instruments\\u2019 axes (4 DOFs) which must lead to motionless insertion points disturbed by the breathing motion."'),
('"Segmentation and Recognition Using Structure from Motion Point Clouds"', '"ECCV 2008"', '["Point Cloud", "Video Sequence", "Object Recognition", "Feature Track", "Motion Point"]', '"https://doi.org/10.1007/978-3-540-88682-2_5"', '"We propose an algorithm for semantic segmentation based on 3D point clouds derived from ego-motion. We motivate five simple cues designed to model specific patterns of motion and 3D world structure that vary with object category. We introduce features that project the 3D cues back to the 2D image plane while modeling spatial layout and context. A randomized decision forest combines many such features to achieve a coherent 2D segmentation and recognize the object categories present. Our main contribution is to show how semantic segmentation is possible based solely on motion-derived 3D world structure. Our method works well on sparse, noisy point clouds, and unlike existing approaches, does not need appearance-based descriptors."'),
('"Segmentation Based Particle Filtering for Real-Time 2D Object Tracking"', '"ECCV 2012"', '["Online Learning", "Visual Tracking", "Particle Sample", "Foreground Object", "Appearance Change"]', '"https://doi.org/10.1007/978-3-642-33765-9_60"', '"We address the problem of visual tracking of arbitrary objects that undergo significant scale and appearance changes. The classical tracking methods rely on the bounding box surrounding the target object. Regardless of the tracking approach, the use of bounding box quite often introduces background information. This information propagates in time and its accumulation quite often results in drift and tracking failure. This is particularly the case with the particle filtering approach that is often used for visual tracking. However, it always uses a bounding box around the object to compute features of the particle samples. Since this causes the drift, we propose to use segmentation for sampling. Relying on segmentation and computing the colour and gradient orientation histograms from these segmented particle samples allows the tracker to easily adapt to the object\\u2019s deformations, occlusions, orientation, scale and appearance changes. We propose two particle sampling strategies based on segmentation. In the first, segmentation is done for every propagated particle sample, while in the second only the strongest particle sample is segmented. Depending on this decision there is obviously a trade-off between speed and performance."'),
('"Segmentation of Abdominal Aortic Aneurysms with a Non-parametric Appearance Model"', '"MMBIA 2004"', '[]', '"https://doi.org/10.1007/978-3-540-27816-0_22"', '"This paper presents a new method to segment abdominal aortic aneurysms from CT angiography scans. The outer contour of lumen and thrombus are delineated with independent 3D deformable models. First the lumen is segmented based on two user indicated positions, and then the resulting surface is used to initialize the automated thrombus segmentation method. For the lumen, the image-derived deformation term is based on a simple grey level appearance model, while, for the thrombus, appearance is modelled with a non-parametric pattern classification technique (k-nearest neighbours). The intensity profile along the surface normal is used as classification feature. Manual segmentations are used for training the classifier: samples are collected inside, outside and at the given boundary. During deformation, the method determines the most likely class corresponding to the intensity profile at each vertex. A vertex is pushed outwards when the class is inside; inwards when the class is outside; and no deformation occurs when the class is boundary. Results of a preliminary evaluation study on 9 scans show the method\\u2019s behaviour with respect to the number of neighbours used for classification and to the distance for collecting inside and outside samples."'),
('"Segmentation of Dynamic Emission Tomography Data in Projection Space"', '"CVAMIA 2006"', '["Positron Emission Tomography", "Projection Space", "Time Activity Curve", "Dynamic Positron Emissi', '"https://doi.org/10.1007/11889762_10"', '"Dynamic emission tomography is a technique used for quantifying the biochemical and physiological processes within the body. For certain neuroimaging applications, like kinetic modelling in positron emission tomography (PET), segmenting the measured data into a fewer number of regions-of-interest (ROI) is an important procedure needed for calculation of regional time-activity curves (TACs). Conventional estimation of regional activities in image domain suffers from substantial errors due to the reconstruction artifacts and segmentation inaccuracies. In this study, we present an approach for separating the dynamic tomographic data directly in the projection space using the least-squares method. Sinogram ROIs are the fractional parts of different tissue types measured at each voxel. Regional TACs can be estimated from the segmented sinogram ROIs, thereby avoiding the image reconstruction step. The introduced technique was validated with the two dynamic synthetic phantoms simulated based on 11C- and 18F-labelled tracer distributions. From the quantitative point of view, TAC estimation from the segmented sinograms yielded more accurate results compared to the image-based segmentation."'),
('"Segmentation of High Angular Resolution Diffusion MRI Modeled as a Field of von Mises-Fisher Mixtur', '"ECCV 2006"', '["Symmetric Space", "Gaussian Mixture Model", "Orientation Distribution Function", "Spherical Harmon', '"https://doi.org/10.1007/11744078_36"', '"High angular resolution diffusion imaging (HARDI) permits the computation of water molecule displacement probabilities over a sphere of possible displacement directions. This probability is often referred to as the orientation distribution function (ODF). In this paper we present a novel model for the diffusion ODF namely, a mixture of von Mises-Fisher (vMF) distributions. Our model is compact in that it requires very few variables to model complicated ODF geometries which occur specifically in the presence of heterogeneous nerve fiber orientation. We also present a Riemannian geometric framework for computing intrinsic distances, in closed-form, and performing interpolation between ODFs represented by vMF mixtures. As an example, we apply the intrinsic distance within a hidden Markov measure field segmentation scheme. We present results of this segmentation for HARDI images of rat spinal cords \\u2013 which show distinct regions within both the white and gray matter. It should be noted that such a fine level of parcellation of the gray and white matter cannot be obtained either from contrast MRI scans or Diffusion Tensor MRI scans. We validate the segmentation algorithm by applying it to synthetic data sets where the ground truth is known."'),
('"Segmentation of Medical Images with a Shape and Motion Model: A Bayesian Perspective"', '"MMBIA 2004"', '["Segmentation Algorithm", "Motion Model", "Shape Model", "Active Shape Model", "Likelihood Term"]', '"https://doi.org/10.1007/978-3-540-27816-0_14"', '"This paper describes a Bayesian framework for the segmentation of a temporal sequence of medical images, where both shape and motion prior information are integrated into a stochastic model. With this approach, we aim to take into account all the information available to compute an optimum solution, thus increasing the robustness and accuracy of the shape and motion reconstruction. The segmentation algorithm we develop is based on sequential Monte Carlo sampling methods previously applied in tracking applications. Moreover, we show how stochastic shape models can be constructed using a global shape description based on orthonormal functions. This makes our approach independent of the dimension of the object (2D or 3D) and on the particular shape parameterization used. Results of the segmentation method applied to cardiac cine MR images are presented."'),
('"Segmentation over Detection by Coupled Global and Local Sparse Representations"', '"ECCV 2012"', '["Training Image", "Object Detection", "Sparse Representation", "Foreground Object", "Object Segment', '"https://doi.org/10.1007/978-3-642-33715-4_48"', '"Motivated by the rising performances of object detection algorithms, we investigate how to further precisely segment out objects within the output bounding boxes. The task is formulated as a unified optimization problem, pursuing a unique latent object mask in non-parametric manner. For a given test image, the objects are first detected by detectors. Then for each detected bounding box, the objects of the same category along with their object masks are extracted from the training set. The latent mask of the object within the bounding box is inferred based on three objectives: 1) the latent mask should be coherent, subject to sparse errors caused by within-category diversities, with the global bounding-box-level mask inferred by sparse representation over the bounding boxes of the same category within the training set; 2) the latent mask should be coherent with local patch-level mask inferred by sparse representation of the individual patch over all spatially nearby (handling local deformations) patches of the same category in the training set; and 3) mask property within each sufficiently small super-pixel should be consistent. All these three objectives are integrated into a unified optimization problem, and finally the sparse representation coefficients and the latent mask are alternately optimized based on Lasso optimization and smooth approximation followed by Accelerated Proximal Gradient method, respectively. Extensive experiments on the Pascal VOC object segmentation datasets, VOC2007 and VOC2010, show that our proposed algorithm achieves competitive results with the state-of-the-art learning based algorithms, and is superior over other detection based object segmentation algorithms."'),
('"Segmentation Propagation in ImageNet"', '"ECCV 2012"', '["Gaussian Mixture Model", "Appearance Model", "Related Classis", "Foreground Object", "Segmentation', '"https://doi.org/10.1007/978-3-642-33786-4_34"', '"ImageNet is a large-scale hierarchical database of object classes. We propose to automatically populate it with pixelwise segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes. The key idea is to recursively exploit images segmented so far to guide the segmentation of new images. At each stage this propagation process expands into the images which are easiest to segment at that point in time, e.g. by moving to the semantically most related classes to those segmented so far. The propagation of segmentation occurs both (a) at the image level, by transferring existing segmentations to estimate the probability of a pixel to be foreground, and (b) at the class level, by jointly segmenting images of the same class and by importing the appearance models of classes that are already segmented. Through an experiment on 577 classes and 500k images we show that our technique (i) annotates a wide range of classes with accurate segmentations; (ii) effectively exploits the hierarchical structure of ImageNet; (iii) scales efficiently; (iv) outperforms a baseline GrabCut [1] initialized on the image center, as well as our recent segmentation transfer technique [2] on which this paper is based. Moreover, our method also delivers state-of-the-art results on the recent iCoseg dataset for co-segmentation."'),
('"Segmentation with Non-linear Regional Constraints via Line-Search Cuts"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33718-5_42"', '"This paper is concerned with energy-based image segmentation problems. We introduce a general class of regional functionals defined as an arbitrary non-linear combination of regional unary terms. Such (high-order) functionals are very useful in vision and medical applications and some special cases appear in prior art. For example, our general class of functionals includes but is not restricted to soft constraints on segment volume, its appearance histogram, or shape."'),
('"Segmenting Cell Images: A Deterministic Relaxation Approach"', '"MMBIA 2004"', '["Class Label", "Cell Image", "Segmentation Algorithm", "Segmentation Result", "Initial Segmentation', '"https://doi.org/10.1007/978-3-540-27816-0_24"', '"Automatic segmentation of digital cell images into four regions, namely nucleus, cytoplasm, red blood cell (rbc), and background, is an important step for pathological measurements. Using an adaptive thresholding of the histogram, the cell image can be roughly segmented into three regions: nucleus, a mixture of cytoplasm and rbc\\u2019s, and background. This segmentation is served as an initial segmentation for our iterative image segmentation algorithm. Specifically, MAP (maximum a posteriori) criterion formulated by the Bayesian framework with the original image data and local variance image field (LVIF) is used to update the class labels iteratively by a deterministic relaxation algorithm. Finally, we draw a line to separate the touching rbc from the cytoplasm."'),
('"Segmenting Dynamic Textures with Ising Descriptors, ARX Models and Level Sets"', '"WDV 2006"', '["Video Sequence", "ARMA Model", "Initial Contour", "Dynamic Texture", "Geodesic Active Contour"]', '"https://doi.org/10.1007/978-3-540-70932-9_10"', '"We present a new algorithm for segmenting a scene consisting of multiple moving dynamic textures. We model the spatial statistics of a dynamic texture with a set of second order Ising descriptors whose temporal evolution of is governed by an AutoRegressive eXogenous (ARX) model. Given this model, we cast the dynamic texture segmentation problem in a variational framework in which we minimize the spatial-temporal variance of the stochastic part of the model. This energy functional is shown to depend explicitly on both the appearance and dynamics of the scene. Our framework naturally handles intensity and texture based image segmentation as well as dynamics based video segmentation as particular cases. Several experiments show the applicability of our method to segmenting scenes using only dynamics, only appearance, and both dynamics and appearance."'),
('"Segmenting Fiber Bundles in Diffusion Tensor Images"', '"ECCV 2008"', '["Corpus Callosum", "Fiber Bundle", "Diffusion Tensor", "Spectral Cluster", "Locally Linear Embeddin', '"https://doi.org/10.1007/978-3-540-88690-7_18"', '"We consider the problem of segmenting fiber bundles in diffusion tensor images. We cast this problem as a manifold clustering problem in which different fiber bundles correspond to different submanifolds of the space of diffusion tensors. We first learn a local representation of the diffusion tensor data using a generalization of the locally linear embedding (LLE) algorithm from Euclidean to diffusion tensor data. Such a generalization exploits geometric properties of the space of symmetric positive semi-definite matrices, particularly its Riemannian metric. Then, under the assumption that different fiber bundles are physically distinct, we show that the null space of a matrix built from the local representation gives the segmentation of the fiber bundles. Our method is computationally simple, can handle large deformations of the principal direction along the fiber tracts, and performs automatic segmentation without requiring previous fiber tracking. Results on synthetic and real diffusion tensor images are also presented."'),
('"Segmenting Highly Articulated Video Objects with Weak-Prior Random Forests"', '"ECCV 2006"', '["Random Forest", "Prior Probability", "Image Patch", "Prior Model", "Video Object"]', '"https://doi.org/10.1007/11744085_29"', '"We address the problem of segmenting highly articulated video objects in a wide variety of poses. The main idea of our approach is to model the prior information of object appearance via random forests. To automatically extract an object from a video sequence, we first build a random forest based on image patches sampled from the initial template. Owing to the nature of using a randomized technique and simple features, the modeled prior information is considered weak, but on the other hand appropriate for our application. Furthermore, the random forest can be dynamically updated to generate prior probabilities about the configurations of the object in subsequent image frames. The algorithm then combines the prior probabilities with low-level region information to produce a sequence of figure-ground segmentations. Overall, the proposed segmentation technique is useful and flexible in that one can easily integrate different cues and efficiently select discriminating features to model object appearance and handle various articulations."'),
('"Segmenting Salient Objects from Images and Videos"', '"ECCV 2010"', '["Video Sequence", "Salient Object", "Conditional Random Field", "Saliency Detection", "Conditional ', '"https://doi.org/10.1007/978-3-642-15555-0_27"', '"In this paper we introduce a new salient object segmentation method, which is based on combining a saliency measure with a conditional random field (CRF) model. The proposed saliency measure is formulated using a statistical framework and local feature contrast in illumination, color, and motion information. The resulting saliency map is then used in a CRF model to define an energy minimization based segmentation approach, which aims to recover well-defined salient objects. The method is efficiently implemented by using the integral histogram approach and graph cut solvers. Compared to previous approaches the introduced method is among the few which are applicable to both still images and videos including motion cues. The experiments show that our approach outperforms the current state-of-the-art methods in both qualitative and quantitative terms."'),
('"Selecting Influential Examples: Active Learning with Expected Model Output Changes"', '"ECCV 2014"', '["active learning", "Gaussian processes", "visual recognition", "exploration-exploitation trade-off"', '"https://doi.org/10.1007/978-3-319-10593-2_37"', '"In this paper, we introduce a new general strategy for active learning. The key idea of our approach is to measure the expected change of model outputs, a concept that generalizes previous methods based on expected model change and incorporates the underlying data distribution. For each example of an unlabeled set, the expected change of model predictions is calculated and marginalized over the unknown label. This results in a score for each unlabeled example that can be used for active learning with a broad range of models and learning algorithms. In particular, we show how to derive very efficient active learning methods for Gaussian process regression, which implement this general strategy, and link them to previous methods. We analyze our algorithms and compare them to a broad range of previous active learning strategies in experiments showing that they outperform state-of-the-art on well-established benchmark datasets in the area of visual object recognition."'),
('"Self-Adapting Feature Layers"', '"ECCV 2010"', '["Face Recognition", "Recognition Rate", "Facial Feature", "Feature Position", "Active Appearance Mo', '"https://doi.org/10.1007/978-3-642-15549-9_22"', '"This paper presents a new approach for fitting a 3D morphable model to images of faces, using self-adapting feature layers (SAFL). The algorithm integrates feature detection into an iterative analysis-by-synthesis framework, combining the robustness of feature search with the flexibility of model fitting. Templates for facial features are created and updated while the fitting algorithm converges, so the templates adapt to the pose, illumination, shape and texture of the individual face. Unlike most existing feature-based methods, the algorithm does not search for the image locations with maximum response, which may be prone to errors, but forms a tradeoff between feature likeness, global feature configuration and image reconstruction error."'),
('"Self-calibration of a General Radially Symmetric Distortion Model"', '"ECCV 2006"', '["Interest Point", "Optical Center", "View Angle", "Camera Model", "Distortion Function"]', '"https://doi.org/10.1007/11744085_15"', '"We present a new approach for self-calibrating the distortion function and the distortion center of cameras with general radially symmetric distortion. In contrast to most current models, we propose a model encompassing fisheye lenses as well as catadioptric cameras with a view angle larger than 180\\u00b0."'),
('"Self-explanatory Sparse Representation for Image Classification"', '"ECCV 2014"', '["Reproducing Kernel Hilbert Spaces", "Sparse Representation", "Multiple View", "Image Classificatio', '"https://doi.org/10.1007/978-3-319-10605-2_39"', '"Traditional sparse representation algorithms usually operate in a single Euclidean space. This paper leverages a self-explanatory reformulation of sparse representation, i.e., linking the learned dictionary atoms with the original feature spaces explicitly, to extend simultaneous dictionary learning and sparse coding into reproducing kernel Hilbert spaces (RKHS). The resulting single-view self-explanatory sparse representation (SSSR) is applicable to an arbitrary kernel space and has the nice property that the derivatives with respect to parameters of the coding are independent of the chosen kernel. With SSSR, multiple-view self-explanatory sparse representation (MSSR) is proposed to capture and combine various salient regions and structures from different kernel spaces. This is equivalent to learning a nonlinear structured dictionary, whose complexity is reduced by learning a set of smaller dictionary blocks via SSSR. SSSR and MSSR are then incorporated into a spatial pyramid matching framework and developed for image classification. Extensive experimental results on four benchmark datasets, including UIUC-Sports, Scene 15, Caltech-101, and Caltech-256, demonstrate the effectiveness of our proposed algorithm."'),
('"Self-Organization of Randomly Placed Sensors"', '"ECCV 2002"', '["Distributed sensing", "sensor self-organization", "calibration"]', '"https://doi.org/10.1007/3-540-47979-1_10"', '"This paper investigates one problem arising from ubiquitous sensing: can the position of a set of randomly placed sensors be automatically determined even if they do not have an overlapping field of view. (If the view overlapped, then standard stereo auto-calibration can be used.) This paper shows that the problem is solveable. Distant moving features allow accurate orientation registration. Given the sensor orientations, nearby linearly moving features allow full pose registration, up to a scale factor."'),
('"Self-similar Sketch"', '"ECCV 2012"', '["self-similarity", "feature detector", "vanishing point estimation", "UFL"]', '"https://doi.org/10.1007/978-3-642-33709-3_7"', '"We introduce the self-similar sketch, a new method for the extraction of intermediate image features that combines three principles: detection of self-similarity structures, nonaccidental alignment, and instance-specific modelling. The method searches for self-similar image structures that form nonaccidental patterns, for example collinear arrangements. We demonstrate a simple implementation of this idea where self-similar structures are found by looking for SIFT descriptors that map to the same visual words in image-specific vocabularies. This results in a visual word map which is searched for elongated connected components. Finally, segments are fitted to these connected components, extracting linear image structures beyond the ones that can be captured by conventional edge detectors, as the latter implicitly assume a specific appearance for the edges (steps). The resulting collection of segments constitutes a \\u201csketch\\u201d of the image. This is applied to the task of estimating vanishing points, horizon, and zenith in standard benchmark data, obtaining state-of-the-art results. We also propose a new vanishing point estimation algorithm based on recently introduced techniques for the continuous-discrete optimisation of energies arising from model selection priors."'),
('"Semantic Aware Video Transcription Using Random Forest Classifiers"', '"ECCV 2014"', '["Video transcription", "random forest", "skim-gram language model"]', '"https://doi.org/10.1007/978-3-319-10590-1_50"', '"This paper focuses on transcription generation in the form of subject, verb, object (SVO) triplets for videos in the wild, given off-the-shelf visual concept detectors. This problem is challenging due to the availability of sentence only annotations, the unreliability of concept detectors, and the lack of training samples for many words. Facing these challenges, we propose a Semantic Aware Transcription (SAT) framework based on Random Forest classifiers. It takes concept detection results as input, and outputs a distribution of English words. SAT uses video, sentence pairs for training. It hierarchically learns node splits by grouping semantically similar words, measured by a continuous skip-gram language model. This not only addresses the sparsity of training samples per word, but also yields semantically reasonable errors during transcription. SAT provides a systematic way to measure the relatedness of a concept detector to real words, which helps us understand the relationship between current visual detectors and words in a semantic space. Experiments on a large video dataset with 1,970 clips and 85,550 sentences are used to demonstrate our idea."'),
('"Semantic Concept Classification by Joint Semi-supervised Learning of Feature Subspaces and Support ', '"ECCV 2008"', '["Unlabeled Data", "Semantic Concept", "Generalize Eigenvalue Problem", "Locality Preserve Projectio', '"https://doi.org/10.1007/978-3-540-88693-8_20"', '"The scarcity of labeled training data relative to the high-dimensionality multi-modal features is one of the major obstacles for semantic concept classification of images and videos. Semi-supervised learning leverages the large amount of unlabeled data in developing effective classifiers. Feature subspace learning finds optimal feature subspaces for representing data and helping classification. In this paper, we present a novel algorithm, Locality Preserving Semi-supervised Support Vector Machines (LPSSVM), to jointly learn an optimal feature subspace as well as a large margin SVM classifier. Over both labeled and unlabeled data, an optimal feature subspace is learned that can maintain the smoothness of local neighborhoods as well as being discriminative for classification. Simultaneously, an SVM classifier is optimized in the learned feature subspace to have large margin. The resulting classifier can be readily used to handle unseen test data. Additionally, we show that the LPSSVM algorithm can be used in a Reproducing Kernel Hilbert Space for nonlinear classification. We extensively evaluate the proposed algorithm over four types of data sets: a toy problem, two UCI data sets, the Caltech 101 data set for image classification, and the challenging Kodak\\u2019s consumer video data set for semantic concept detection. Promising results are obtained which clearly confirm the effectiveness of the proposed method."'),
('"Semantic Image Segmentation Using Visible and Near-Infrared Channels"', '"ECCV 2012"', '["Conditional Random Field", "Regularization Part", "Fisher Vector", "Pairwise Potential", "Conditio', '"https://doi.org/10.1007/978-3-642-33868-7_46"', '"Recent progress in computational photography has shown that we can acquire physical information beyond visible (RGB) image representations. In particular, we can acquire near-infrared (NIR) cues with only slight modification to any standard digital camera. In this paper, we study whether this extra channel can improve semantic image segmentation. Based on a state-of-the-art segmentation framework and a novel manually segmented image database that contains 4-channel images (RGB+NIR), we study how to best incorporate the specific characteristics of the NIR response. We show that it leads to improved performances for 7 classes out of 10 in the proposed dataset and discuss the results with respect to the physical properties of the NIR response."'),
('"Semantic Label Sharing for Learning with Many Categories"', '"ECCV 2010"', '["Semantic Distance", "Semantic Label", "Training Pair", "Internet Search Engine", "Search Engine Re', '"https://doi.org/10.1007/978-3-642-15549-9_55"', '"In an object recognition scenario with tens of thousands of categories, even a small number of labels per category leads to a very large number of total labels required. We propose a simple method of label sharing between semantically similar categories. We leverage the WordNet hierarchy to define semantic distance between any two categories and use this semantic distance to share labels. Our approach can be used with any classifier. Experimental results on a range of datasets, upto 80 million images and 75,000 categories in size, show that despite the simplicity of the approach, it leads to significant improvements in performance."'),
('"Semantic Road Segmentation via Multi-scale Ensembles of Learned Features"', '"ECCV 2012"', '["Convolutional Neural Network", "Conditional Random Field", "Weighted Linear Combination", "Unary P', '"https://doi.org/10.1007/978-3-642-33868-7_58"', '"Semantic segmentation refers to the process of assigning an object label (e.g., building, road, sidewalk, car, pedestrian) to every pixel in an image. Common approaches formulate the task as a random field labeling problem modeling the interactions between labels by combining local and contextual features such as color, depth, edges, SIFT or HoG. These models are trained to maximize the likelihood of the correct classification given a training set. However, these approaches rely on hand\\u2013designed features (e.g., texture, SIFT or HoG) and a higher computational time required in the inference process."'),
('"Semantic Segmentation of Urban Scenes Using Dense Depth Maps"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15561-1_51"', '"In this paper we present a framework for semantic scene parsing and object recognition based on dense depth maps. Five view-independent 3D features that vary with object class are extracted from dense depth maps at a superpixel level for training a classifier using randomized decision forest technique. Our formulation integrates multiple features in a Markov Random Field (MRF) framework to segment and recognize different object classes in query street scene images. We evaluate our method both quantitatively and qualitatively on the challenging Cambridge-driving Labeled Video Database (CamVid). The result shows that only using dense depth information, we can achieve overall better accurate segmentation and recognition than that from sparse 3D features or appearance, or even the combination of sparse 3D features and appearance, advancing state-of-the-art performance. Furthermore, by aligning 3D dense depth based features into a unified coordinate frame, our algorithm can handle the special case of view changes between training and testing scenarios. Preliminary evaluation in cross training and testing shows promising results."'),
('"Semantic Segmentation with Second-Order Pooling"', '"ECCV 2012"', '["Semantic Segmentation", "Feature Pooling"]', '"https://doi.org/10.1007/978-3-642-33786-4_32"', '"Feature extraction, coding and pooling, are important components on many contemporary object recognition paradigms. In this paper we explore novel pooling techniques that encode the second-order statistics of local descriptors inside a region. To achieve this effect, we introduce multiplicative second-order analogues of average and max-pooling that together with appropriate non-linearities lead to state-of-the-art performance on free-form region recognition, without any type of feature coding. Instead of coding, we found that enriching local descriptors with additional image information leads to large performance gains, especially in conjunction with the proposed pooling methodology. We show that second-order pooling over free-form regions produces results superior to those of the winning systems in the Pascal VOC 2011 semantic segmentation challenge, with models that are 20,000 times faster."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Semantic-Analysis Object Recognition: Automatic Training Set Generation Using Textual Tags"', '"ECCV 2014"', '["Training set", "Semantic", "Ontology", "Semantic similarity", "Image retrieval", "Textual tags", "', '"https://doi.org/10.1007/978-3-319-16181-5_22"', '"Training sets of images for object recognition are the pillars on which classifiers base their performances. We have built a framework to support the entire process of image and textual retrieval from search engines, which, giving an input keyword, performs a statistical and a semantic analysis and automatically builds a training set. We have focused our attention on textual information and we have explored, with several experiments, three different approaches to automatically discriminate between positive and negative images: keyword position, tag frequency and semantic analysis. We present the best results for each approach."'),
('"Semantics Discovery for Image Indexing"', '"ECCV 2004"', '["Support Vector Machine", "Image Retrieval", "Average Precision", "Image Block", "Image Indexing"]', '"https://doi.org/10.1007/978-3-540-24670-1_21"', '"To bridge the gap between low-level features and high-level semantic queries in image retrieval, detecting meaningful visual entities (e.g. faces, sky, foliage, buildings etc) based on trained pattern classifiers has become an active research trend. However, a drawback of the supervised learning approach is the human effort to provide labeled regions as training samples. In this paper, we propose a new three-stage hybrid framework to discover local semantic patterns and generate their samples for training with minimal human intervention. Support vector machines (SVM) are first trained on local image blocks from a small number of images labeled as several semantic categories. Then to bootstrap the local semantics, image blocks that produce high SVM outputs are grouped into Discovered Semantic Regions (DSRs) using fuzzy c-means clustering. The training samples for these DSRs are automatically induced from cluster memberships and subject to support vector machine learning to form local semantic detectors for DSRs. An image is then indexed as a tessellation of DSR histograms and matched using histogram intersection. We evaluate our method against the linear fusion of color and texture features using 16 semantic queries on 2400 heterogeneous consumer photos. The DSR models achieved a promising 26% improvement in average precision over that of the feature fusion approach."'),
('"Semi-automatic Motion Segmentation with Motion Layer Mosaics"', '"ECCV 2008"', '["Segmentation Process", "Foreground Object", "Minimum Description Length", "Consecutive Image", "Di', '"https://doi.org/10.1007/978-3-540-88690-7_16"', '"A new method for motion segmentation based on reference motion layer mosaics is presented. We assume that the scene is composed of a set of layers whose motion is well described by parametric models. This usual assumption is compatible with the notion of motion layer mosaic, which allows a compact representation of the sequence with a small number of mosaics only. We segment the sequence using a reduced number of distant image-to-mosaic comparisons instead of a larger number of close image-to-image comparisons. Apart from computational advantage, another interest lies in the fact that motions estimated between distant images are more likely to be different from one region to another than when estimated between consecutive images. This helps the segmentation process. The segmentation is obtained by graph cut minimization of a cost function which includes an original image-to-mosaic data term. At the end of the segmentation process, it may happen that the obtained boundaries are not precisely the expected ones. Often the user has no other possibility than modifying manually every segmentation one after another or than starting over all again the process with different parameters. We propose an original easy way for the user to manually correct the possible errors on the mosaics themselves. These corrections are then propagated to all the images of the corresponding video interval thanks to a second segmentation pass. Experimental results demonstrate the potential of our approach."'),
('"Semi-intrinsic Mean Shift on Riemannian Manifolds"', '"ECCV 2012"', '["Riemannian Manifold", "Vector Bundle", "Tangent Space", "Heat Kernel", "Reproduce Kernel Hilbert S', '"https://doi.org/10.1007/978-3-642-33718-5_25"', '"The original mean shift algorithm [1] on Euclidean spaces (MS) was extended in [2] to operate on general Riemannian manifolds. This extension is extrinsic (Ext-MS) since the mode seeking is performed on the tangent spaces [3], where the underlying curvature is not fully considered (tangent spaces are only valid in a small neighborhood). In [3] was proposed an intrinsic mean shift designed to operate on two particular Riemannian manifolds (IntGS-MS), i.e. Grassmann and Stiefel manifolds (using manifold-dedicated density kernels). It is then natural to ask whether mean shift could be intrinsically extended to work on a large class of manifolds. We propose a novel paradigm to intrinsically reformulate the mean shift on general Riemannian manifolds. This is accomplished by embedding the Riemannian manifold into a Reproducing Kernel Hilbert Space (RKHS) by using a general and mathematically well-founded Riemannian kernel function, i.e. heat kernel [5]. The key issue is that when the data is implicitly mapped to the Hilbert space, the curvature of the manifold is taken into account (i.e. exploits the underlying information of the data). The inherent optimization is then performed on the embedded space. Theoretic analysis and experimental results demonstrate the promise and effectiveness of this novel paradigm."'),
('"Semi-Nonnegative Matrix Factorization for Motion Segmentation with Missing Data"', '"ECCV 2012"', '["Motion Segmentation", "Semi-Nonnegative Matrix Factorization(SNMF)", "Missing Data"]', '"https://doi.org/10.1007/978-3-642-33786-4_30"', '"Motion segmentation is an old problem that is receiving renewed interest because of its role in video analysis. In this paper, we present a Semi-Nonnegative Matrix Factorization (SNMF)method that models dense point tracks in terms of their optical flow, and decomposes sets of point tracks into semantically meaningful motion components. We show that this formulation of SNMF with missing values outperforms the state-of-the-art algorithm of Brox and Malik in terms of accuracy on 10-frame video segments from the Berkeley test set, while being over 100 times faster. We then show how SNMF can be applied to longer videos using sliding windows. The result is competitive in terms of accuracy with Brox and Malik\\u2019s algorithm, while still being two orders of magnitude faster."'),
('"Semi-supervised Learning of Facial Attributes in Video"', '"ECCV 2010"', '["Facial Feature", "Video Data", "Average Precision", "Point Track", "Video Track"]', '"https://doi.org/10.1007/978-3-642-35749-7_4"', '"In this work we investigate a weakly-supervised approach to learning facial attributes of humans in video. Given a small set of images labeled with attributes and a much larger unlabeled set of video tracks, we train a classifier to recognize these attributes in video data. We make two contributions. First, we show that training on video data improves classification performance over training on images alone. Second, and more significantly, we show that tracks in video provide a natural mechanism for generalizing training data \\u2013 in this case to new poses, lighting conditions and expressions. The advantage of our method is demonstrated on the classification of gender and age attributes in the movie \\u201cLove, Actually\\u201d. We show that the semi-supervised approach adds a significant performance boost, for example for gender increasing average precision from 0.75 on static images alone to 0.85."'),
('"Semi-supervised On-Line Boosting for Robust Tracking"', '"ECCV 2008"', '["Feature Selection", "Unlabeled Data", "Tracking Loop", "Robust Tracking", "Unlabeled Sample"]', '"https://doi.org/10.1007/978-3-540-88682-2_19"', '"Recently, on-line adaptation of binary classifiers for tracking have been investigated. On-line learning allows for simple classifiers since only the current view of the object from its surrounding background needs to be discriminiated. However, on-line adaption faces one key problem: Each update of the tracker may introduce an error which, finally, can lead to tracking failure (drifting). The contribution of this paper is a novel on-line semi-supervised boosting method which significantly alleviates the drifting problem in tracking applications. This allows to limit the drifting problem while still staying adaptive to appearance changes. The main idea is to formulate the update process in a semi-supervised fashion as combined decision of a given prior and an on-line classifier. This comes without any parameter tuning. In the experiments, we demonstrate real-time tracking of our SemiBoost tracker on several challenging test sequences where our tracker outperforms other on-line tracking methods."'),
('"Semidefinite Programming Heuristics for Surface Reconstruction Ambiguities"', '"ECCV 2008"', '["Singular Vector", "Photometric Stereo", "Quadratic Cost Function", "Sweep Line", "Compute Surface"', '"https://doi.org/10.1007/978-3-540-88682-2_11"', '"We consider the problem of reconstructing a smooth surface under constraints that have discrete ambiguities. These problems arise in areas such as shape from texture, shape from shading, photometric stereo and shape from defocus. While the problem is computationally hard, heuristics based on semidefinite programming may reveal the shape of the surface."'),
('"Sensitivity of Calibration to Principal Point Position"', '"ECCV 2002"', '["Focal Length", "Fundamental Matrix", "Principal Point", "Reconstruction Accuracy", "Epipolar Line"', '"https://doi.org/10.1007/3-540-47967-8_29"', '"A common practice when carrying out self-calibration and Euclidean reconstruction from one or more views is to start with a guess at the principal point of the camera. The general belief is that inaccuracies in the estimation of the principal point do not have a significant effect on the other calibration parameters, or on reconstruction accuracy. It is the purpose of this paper to refute that belief. Indeed, it is demonstrated that the determination of the focal length of the camera is tied up very closely with the estimate of the principal point. Small changes in the estimated (sometimes merely guessed) principal point can cause very large changes in the estimated focal length, and the accuracy of reconstruction. In fact, the relative uncertainty in the focal length is inversely proportional to the distance of the principal point to the epipolar line. This analysis is geometric and exact, rather than experimental."'),
('"Separability Oriented Preprocessing for Illumination-Insensitive Face Recognition"', '"ECCV 2012"', '["Separability oriented", "illumination preprocessing", "lighting-invariant", "face recognition"]', '"https://doi.org/10.1007/978-3-642-33786-4_23"', '"In the last decade, some illumination preprocessing approaches were proposed to eliminate the lighting variation in face images for lighting-invariant face recognition. However, we find surprisingly that existing preprocessing methods were seldom modeled to directly enhance the separability of different faces, which should have been the essential goal. To address the issue, we propose to explicitly exploit maximizing separability of different subjects\\u2019 faces as the preprocessing objective. With this in mind, a novel approach, named by us Separability Oriented Preprocessing (SOP), is proposed to enhance face images by maximizing the Fisher separability criterion in scale-space. Extensive experiments on both laboratory-controlled and real-world face databases using different recognition methods show the effectiveness of the proposed approach."'),
('"Separable Spatiotemporal Priors for Convex Reconstruction of Time-Varying 3D Point Clouds"', '"ECCV 2014"', '["Matrix normal", "trace-norm", "spatiotemporal", "missing data"]', '"https://doi.org/10.1007/978-3-319-10578-9_14"', '"Reconstructing 3D motion data is highly under-constrained due to several common sources of data loss during measurement, such as projection, occlusion, or miscorrespondence. We present a statistical model of 3D motion data, based on the Kronecker structure of the spatiotemporal covariance of natural motion, as a prior on 3D motion. This prior is expressed as a matrix normal distribution, composed of separable and compact row and column covariances. We relate the marginals of the distribution to the shape, trajectory, and shape-trajectory models of prior art. When the marginal shape distribution is not available from training data, we show how placing a hierarchical prior over shapes results in a convex MAP solution in terms of the trace-norm. The matrix normal distribution, fit to a single sequence, outperforms state-of-the-art methods at reconstructing 3D motion data in the presence of significant data loss, while providing covariance estimates of the imputed points."'),
('"Separating Specular, Diffuse, and Subsurface Scattering Reflectances from Photometric Images"', '"ECCV 2004"', '["Point Spread Function", "Synthetic Image", "Surface Patch", "Diffuse Component", "Single Scatterin', '"https://doi.org/10.1007/978-3-540-24671-8_33"', '"While subsurface scattering is common in many real objects, almost all separation algorithms focus on extracting specular and diffuse components from real images. In this paper, we propose an appearance-based approach to separate non-directional subsurface scattering reflectance from photometric images, in addition to the separation of the off-specular and non-Lambertian diffuse components. Our mathematical model sufficiently accounts for the photometric response due to non-directional subsurface scattering, and allows for a practical image acquisition system to capture its contribution. Relighting the scene is possible by employing the separated reflectances. We argue that it is sometimes necessary to separate subsurface scattering component, which is essential to highlight removal, when the object reflectance cannot be modeled by specular and diffuse components alone."'),
('"Separating Transparent Layers through Layer Information Exchange"', '"ECCV 2004"', '["Mutual Information", "Video Sequence", "Transfer Factor", "Entire Image", "Pixel Position"]', '"https://doi.org/10.1007/978-3-540-24673-2_27"', '"In this paper we present an approach for separating two transparent layers in images and video sequences. Given two initial unknown physical mixtures, I 1 and I 2, of real scene layers, L 1 and L 2, we seek a layer separation which minimizes the structural correlations across the two layers, at every image point. Such a separation is achieved by transferring local grayscale structure from one image to the other wherever it is highly correlated with the underlying local grayscale structure in the other image, and vice versa. This bi-directional transfer operation, which we call the \\u201clayer information exchange\\u201d, is performed on diminishing window sizes, from global image windows (i.e., the entire image), down to local image windows, thus detecting similar grayscale structures at varying scales across pixels. We show the applicability of this approach to various real-world scenarios, including image and video transparency separation. In particular, we show that this approach can be used for separating transparent layers in images obtained under different polarizations, as well as for separating complex non-rigid transparent motions in video sequences. These can be done without prior knowledge of the layer mixing model (simple additive, alpha-mated composition with an unknown alpha-map, or other), and under unknown complex temporal changes (e.g., unknown varying lighting conditions)."'),
('"Sequence-to-Sequence Self Calibration"', '"ECCV 2002"', '["Self-Calibration", "Multi-View Invariants"]', '"https://doi.org/10.1007/3-540-47967-8_25"', '"We present a linear method for self-calibration of a moving rig when no correspondences are available between the cameras. Such a scenario occurs, for example, when the cameras have different viewing angles, different zoom factors or different spectral ranges. It is assumed that during the motion of the rig, the relative viewing angle between the cameras remains fixed and is known. Except for the fixed relative viewing angle, any of the internal parameters and any of the other external parameters of the cameras may vary freely. The calibration is done by linearly computing multilinear invariants, expressing the relations between the optical axes of the cameras during the motion. A solution is then extracted from these invariants. Given the affine calibration, the metric calibration is known to be achieved linearly (e.g. by assuming zero skew). Thus an automatic solution is presented for self calibration of a class of moving rigs with varying internal parameters. This solution is achieved without using any correspondences between the cameras, and requires only solving linear equations."'),
('"Sequential Max-Margin Event Detectors"', '"ECCV 2014"', '["Event Detection", "Activity Recognition", "Time Series Analysis", "Multi-Modal Action Detection"]', '"https://doi.org/10.1007/978-3-319-10578-9_27"', '"Many applications in computer vision (e.g., games, human computer interaction) require a reliable and early detector of visual events. Existing event detection methods rely on one-versus-all or multi-class classifiers that do not scale well to online detection of large number of events. This paper proposes Sequential Max-Margin Event Detectors (SMMED) to efficiently detect an event in the presence of a large number of event classes. SMMED sequentially discards classes until only one class is identified as the detected class. This approach has two main benefits w.r.t. standard approaches: (1) It provides an efficient solution for early detection of events in the presence of large number of classes, and (2) it is computationally efficient because only a subset of likely classes are evaluated. The benefits of SMMED in comparison with existing approaches is illustrated in three databases using different modalities: MSRDaliy Activity (3D depth videos), UCF101 (RGB videos) and the CMU-Multi-Modal Action Detection (MAD) database (depth, RGB and skeleton). The CMU-MAD was recorded to target the problem of event detection (not classification), and the data and labels are available at http://humansensing.cs.cmu.edu/mad/ ."'),
('"Sequential Non-Rigid Structure-from-Motion with the 3D-Implicit Low-Rank Shape Model"', '"ECCV 2010"', '["Shape Model", "Current Frame", "Basis Shape", "Bundle Adjustment", "Reprojection Error"]', '"https://doi.org/10.1007/978-3-642-15552-9_2"', '"So far the Non-Rigid Structure-from-Motion problem has been tackled using a batch approach. All the frames are processed at once after the video acquisition takes place. In this paper we propose an incremental approach to the estimation of deformable models. Image frames are processed online in a sequential fashion. The shape is initialised to a rigid model from the first few frames. Subsequently, the problem is formulated as a model based camera tracking problem, where the pose of the camera and the mixing coefficients are updated every frame. New modes are added incrementally when the current model cannot model the current frame well enough. We define a criterion based on image reprojection error to decide whether or not the model must be updated after the arrival of a new frame. The new mode is estimated performing bundle adjustment on a window of frames. To represent the shape, we depart from the traditional explicit low-rank shape model and propose a variant that we call the 3D-implicit low-rank shape model. This alternative model results in a simpler formulation of the motion matrix and provides the ability to represent degenerate deformation modes. We illustrate our approach with experiments on motion capture sequences with ground truth 3D data and with real video sequences."'),
('"Sequential Spectral Learning to Hash with Multiple Representations"', '"ECCV 2012"', '["Hash Function", "Binary Code", "Multiple Representation", "Visual Descriptor", "Locality Sensitive', '"https://doi.org/10.1007/978-3-642-33715-4_39"', '"Learning to hash involves learning hash functions from a set of images for embedding high-dimensional visual descriptors into a similarity-preserving low-dimensional Hamming space. Most of existing methods resort to a single representation of images, that is, only one type of visual descriptors is used to learn a hash function to assign binary codes to images. However, images are often described by multiple different visual descriptors (such as SIFT, GIST, HOG), so it is desirable to incorporate these multiple representations into learning a hash function, leading to multi-view hashing. In this paper we present a sequential spectral learning approach to multi-view hashing where a hash function is sequentially determined by solving the successive maximization of local variances subject to decorrelation constraints. We compute multi-view local variances by \\u03b1-averaging view-specific distance matrices such that the best averaged distance matrix is determined by minimizing its \\u03b1-divergence from view-specific distance matrices. We also present a scalable implementation, exploiting a fast approximate k-NN graph construction method, in which \\u03b1-averaged distances computed in small partitions determined by recursive spectral bisection are gradually merged in conquer steps until whole examples are used. Numerical experiments on Caltech-256, CIFAR-20, and NUS-WIDE datasets confirm the high performance of our method, in comparison to single-view spectral hashing as well as existing multi-view hashing methods."'),
('"SERBoost: Semi-supervised Boosting with Expectation Regularization"', '"ECCV 2008"', '["Loss Function", "Random Forest", "Maximum Entropy", "Interest Point", "Unlabeled Data"]', '"https://doi.org/10.1007/978-3-540-88690-7_44"', '"The application of semi-supervised learning algorithms to large scale vision problems suffers from the bad scaling behavior of most methods. Based on the Expectation Regularization principle, we propose a novel semi-supervised boosting method, called SERBoost that can be applied to large scale vision problems. The complexity is mainly dominated by the base learners. The algorithm provides a margin regularizer for the boosting cost function and shows a principled way of utilizing prior knowledge. We demonstrate the performance of SERBoost on the Pascal VOC2006 set and compare it to other supervised and semi-supervised methods, where SERBoost shows improvements both in terms of classification accuracy and computational speed."'),
('"Set Based Discriminative Ranking for Recognition"', '"ECCV 2012"', '["Face Recognition", "Simultaneous Optimization", "Geometric Distance", "Object Recognition Task", "', '"https://doi.org/10.1007/978-3-642-33712-3_36"', '"Recently both face recognition and body-based person re-identification have been extended from single-image based scenarios to video-based or even more generally image-set based problems. Set-based recognition brings new research and application opportunities while at the same time raises great modeling and optimization challenges. How to make the best use of the available multiple samples for each individual while at the same time not be disturbed by the great within-set variations is considered by us to be the major issue. Due to the difficulty of designing a global optimal learning model, most existing solutions are still based on unsupervised matching, which can be further categorized into three groups: a) set-based signature generation, b) direct set-to-set matching, and c) between-set distance finding. The first two count on good feature representation while the third explores data set structure and set-based distance measurement. The main shortage of them is the lack of learning-based discrimination ability. In this paper, we propose a set-based discriminative ranking model (SBDR), which iterates between set-to-set distance finding and discriminative feature space projection to achieve simultaneous optimization of these two. Extensive experiments on widely-used face recognition and person re-identification datasets not only demonstrate the superiority of our approach, but also shed some light on its properties and application domain."'),
('"Shadow Graphs and Surface Reconstruction"', '"ECCV 2002"', '["Surface Geometry", "Shadow Graph", "Shading", "Optimization"]', '"https://doi.org/10.1007/3-540-47967-8_3"', '"We present a method to solve shape-from-shadow using shadow graphs which give a new graph-based representation for shadow constraints. It can be shown that the shadow graph alone is enough to solve the shape-from-shadow problem from a dense set of images. Shadow graphs provide a simpler and more systematic approach to represent and integrate shadow constraints from multiple images. To recover shape from a sparse set of images, we propose a method for integrated shadow and shading constraints. Previous shape-from-shadow algorithms do not consider shading constraints while shape-from-shading usually assumes there is no shadow. Our method is based on collecting a set of images from a fixed viewpoint as a known light source changes its position. It first builds a shadow graph from shadow constraints from which an upper bound for each pixel can be derived if the height values of a small number of pixels are initialized properly. Finally, a constrained optimization procedure is designed to make the results from shape-from-shading consistent with the upper bounds derived from the shadow constraints. Our technique is demonstrated on both synthetic and real imagery."'),
('"Shadows in Three-Source Photometric Stereo"', '"ECCV 2008"', '["Height Function", "Regularization Scheme", "Shadow Region", "Photometric Stereo", "Shadowed Pixel"', '"https://doi.org/10.1007/978-3-540-88682-2_23"', '"Shadows are one of the most significant difficulties of the photometric stereo method. When four or more images are available, local surface orientation is overdetermined and the shadowed pixels can be discarded. In this paper we look at the challenging case when only three images under three different illuminations are available. In this case, when one of the three pixel intensity constraints is missing due to shadow, a 1 dof ambiguity per pixel arises. We show that using integrability one can resolve this ambiguity and use the remaining two constraints to reconstruct the geometry in the shadow regions. As the problem becomes ill-posed in the presence of noise, we describe a regularization scheme that improves the numerical performance of the algorithm while preserving data. We propose a simple MRF optimization scheme to identify and segment shadow regions in the image. Finally the paper describes how this theory applies in the framework of color photometric stereo where one is restricted to only three images. Experiments on synthetic and real image sequences are presented."'),
('"Shape Analysis and Fuzzy Control for 3D Competitive Segmentation of Brain Structures with Level Set', '"ECCV 2006"', '["Gaussian Mixture Model", "Fuzzy Control", "Fuzzy Controller", "Shape Model", "Shape Analysis"]', '"https://doi.org/10.1007/11744023_36"', '"We propose a new method to segment 3D structures with competitive level sets driven by a shape model and fuzzy control. To this end, several contours evolve simultaneously toward previously defined targets. The main contribution of this paper is the original introduction of prior information provided by a shape model, which is used as an anatomical atlas, into a fuzzy decision system. The shape information is combined with the intensity distribution of the image and the relative position of the contours. This combination automatically determines the directional term of the evolution equation of each level set. This leads to a local expansion or contraction of the contours, in order to match the borders of their respective targets. The shape model is produced with a principal component analysis, and the resulting mean shape and variations are used to estimate the target location and the fuzzy states corresponding to the distance between the current contour and the target. By combining shape analysis and fuzzy control, we take advantage of both approaches to improve the level set segmentation process with prior information. Experiments are shown for the 3D segmentation of deep brain structures from MRI and a quantitative evaluation is performed on a 18 volumes dataset."'),
('"Shape Analysis of Planar Objects with Arbitrary Topologies Using Conformal Geometry"', '"ECCV 2010"', '["Boundary Component", "Conformal Module", "Planar Domain", "Planar Object", "Riemann Sphere"]', '"https://doi.org/10.1007/978-3-642-15555-0_49"', '"The study of 2D shapes is a central problem in the field of computer vision. In 2D shape analysis, classification and recognition of objects from their observed silhouettes are extremely crucial and yet difficult. It usually involves an efficient representation of 2D shape space with natural metric, so that its mathematical structure can be used for further analysis. Although significant progress has been made for the study of 2D simply-connected shapes, very few works have been done on the study of 2D objects with arbitrary topologies. In this work, we propose a representation of general 2D domains with arbitrary topologies using conformal geometry. A natural metric can be defined on the proposed representation space, which gives a metric to measure dissimilarities between objects. The main idea is to map the exterior and interior of the domain conformally to unit disks and circle domains, using holomorphic 1-forms. A set of diffeomorphisms from the unit circle \\\\(\\\\mathbb{S}^1\\\\) to itself can be obtained, which together with the conformal modules are used to define the shape signature. We prove mathematically that our proposed signature uniquely represents shapes with arbitrary topologies. We also introduce a reconstruction algorithm to obtain shapes from their signatures. This completes our framework and allows us to move back and forth between shapes and signatures. Experiments show the efficacy of our proposed algorithm as a stable shape representation scheme."'),
('"Shape and Radiance Estimation from the Information Divergence of Blurred Images"', '"ECCV 2000"', '["Information Divergence", "Prior Assumption", "Blind Deconvolution", "Radiance Estimation", "Lens D', '"https://doi.org/10.1007/3-540-45054-8_49"', '"We formulate the problem of reconstructing the shape and radiance of a scene as the minimization of the information divergence between blurred images, and propose an algorithm that is provably convergent and guarantees that the solution is admissible, in the sense of corresponding to a positive radiance and imaging kernel. The motivation for the use of information divergence comes from the work of Csisz\\u00e1r [5], while the fundamental elements of the proof of convergence come from work by Snyder et al. [14], extended to handle unknown imaging kernels (i.e. the shape of the scene)."'),
('"Shape and Reflectance from Natural Illumination"', '"ECCV 2012"', '["Lighting Environment", "Surface Patch", "Surface Orientation", "Angular Error", "Photometric Stere', '"https://doi.org/10.1007/978-3-642-33718-5_38"', '"We introduce a method to jointly estimate the BRDF and geometry of an object from a single image under known, but uncontrolled, natural illumination. We show that this previously unexplored problem becomes tractable when one exploits the orientation clues embedded in the lighting environment. Intuitively, unique regions in the lighting environment act analogously to the point light sources of traditional photometric stereo; they strongly constrain the orientation of the surface patches that reflect them. The reflectance, which acts as a bandpass filter on the lighting environment, determines the necessary scale of such regions. Accurate reflectance estimation, however, relies on accurate surface orientation information. Thus, these two factors must be estimated jointly. To do so, we derive a probabilistic formulation and introduce priors to address situations where the reflectance and lighting environment do not sufficiently constrain the geometry of the object. Through extensive experimentation we show what this space looks like, and offer insights into what problems become solvable in various categories of real-world natural illumination environments."'),
('"Shape and View Independent Reflectance Map from Multiple Views"', '"ECCV 2004"', '["Ground Truth", "Input Image", "Range Image", "Multiple View", "Visual Hull"]', '"https://doi.org/10.1007/978-3-540-24673-2_48"', '"We consider the problem of estimating the 3D shape and reflectance properties of an object made of a single material from a calibrated set of multiple views. To model reflectance, we propose a View Independent Reflectance Map (VIRM) and derive it from Torrance-Sparrow BRDF model. Reflectance estimation then amounts to estimating VIRM parameters. We represent object shape using surface triangulation. We pose the estimation problem as one of minimizing cost of matching input images, and the images synthesized using shape and reflectance estimates. We show that by enforcing a constant value of VIRM as a global constraint, we can minimize the matching cost function by iterating between VIRM and shape estimation. Experiment results on both synthetic and real objects show that our algorithm is effective in recovering the 3D shape as well as non-lambertian reflectance information. Our algorithm does not require that light sources be known or calibrated using special objects, thus making it more flexible than other photometric stereo or shape from shading methods. The estimated VIRM can be used to synthesize views of other objects."'),
('"Shape from Angle Regularity"', '"ECCV 2012"', '["Articulation Line", "Plane Orientation", "Relative Depth", "Outdoor Scene", "Orientation Label"]', '"https://doi.org/10.1007/978-3-642-33783-3_1"', '"This paper deals with automatic Single View Reconstruction (SVR) of multi-planar scenes characterized by a profusion of straight lines and mutually orthogonal line-pairs. We provide a new shape-from-X constraint based on this regularity of angles between line-pairs in man-made scenes. First, we show how the presence of such regular angles can be used for 2D rectification of an image of a plane. Further, we propose an automatic SVR method assuming there are enough orthogonal line-pairs available on each plane. This angle regularity is only imposed on physically intersecting line-pairs, making it a local constraint. Unlike earlier literature, our approach does not make restrictive assumptions about the orientation of the planes or the camera and works for both indoor and outdoor scenes. Results are shown on challenging images which would be difficult to reconstruct for existing automatic SVR algorithms."'),
('"Shape from Fluorescence"', '"ECCV 2012"', '["3D Reconstruction", "Photometric Stereo", "Shape from Shading", "Fluorescence", "Subsurface Scatte', '"https://doi.org/10.1007/978-3-642-33786-4_22"', '"Beyond day glow highlighters and psychedelic black light posters, it has been estimated that fluorescence is a property exhibited by 20% of objects. When a fluorescent material is illuminated with a short wavelength light, it re-emits light at a longer wavelength isotropically in a similar manner as a Lambertian surface reflects light. This hitherto neglected property opens the doors to using fluorescence to reconstruct 3D shape with some of the same techniques as for Lambertian surfaces \\u2013 even when the surface\\u2019s reflectance is highly non-Lambertian. Thus, performing reconstruction using fluorescence has advantages over purely Lambertian surfaces. Single image shape-from-shading and calibrated Lambertian photometric stereo can be applied to fluorescence images to reveal 3D shape. When performing uncalibrated photometric stereo, both fluorescence and reflectance can be used to recover Euclidean shape and resolve the generalized bas relief ambiguity. Finally for objects that fluoresce in wavelengths distinct from their reflectance (such as plants and vegetables), reconstructions do not suffer from problems due to inter-reflections. We validate these claims through experiments."'),
('"Shape from Light Field Meets Robust PCA"', '"ECCV 2014"', '["light field", "nuclear norm", "low rank"]', '"https://doi.org/10.1007/978-3-319-10599-4_48"', '"In this paper we propose a new type of matching term for multi-view stereo reconstruction. Our model is based on the assumption, that if one warps the images of the various views to a common warping center and considers each warped image as one row in a matrix, then this matrix will have low rank. This also implies, that we assume a certain amount of overlap between the views after the warping has been performed. Such an assumption is obviously met in the case of light field data, which motivated us to demonstrate the proposed model for this type of data. Our final model is a large scale convex optimization problem, where the low rank minimization is relaxed via the nuclear norm. We present qualitative and quantitative experiments, where the proposed model achieves excellent results."'),
('"Shape from Second-Bounce of Light Transport"', '"ECCV 2010"', '["Form Factor", "Shape Recovery", "Global Illumination", "Light Transport", "Photometric Stereo"]', '"https://doi.org/10.1007/978-3-642-15552-9_21"', '"This paper describes a method to recover scene geometry from the second-bounce of light transport. We show that form factors (up to a scaling ambiguity) can be derived from the second-bounce component of light transport in a Lambertian case. The form factors carry information of the geometric relationship between every pair of scene points, i.e., distance between scene points and relative surface orientations. Modelling the scene as polygonal, we develop a method to recover the scene geometry up to a scaling ambiguity from the form factors by optimization. Unlike other shape-from-intensity methods, our method simultaneously estimates depth and surface normal; therefore, our method can handle discontinuous surfaces as it can avoid surface normal integration. Various simulation and real-world experiments demonstrate the correctness of the proposed theory of shape recovery from light transport."'),
('"Shape from Shading and Viscosity Solutions"', '"ECCV 2002"', '["Shape from Shading", "viscosity solutions", "existence and uniqueness of a solution", "Hamilton-Ja', '"https://doi.org/10.1007/3-540-47967-8_53"', '"This article presents an approach to the shape from shading problem which is based upon the notion of viscosity solutions to the shading partial differential equation, in effect a Hamilton-Jacobi equation. The power of this approach is twofolds: 1) it allows nonsmooth, i.e. nondifferentiable, solutions which allows to recover objects with sharp troughs and creases and 2) it provides a framework for deriving a numerical scheme for computing approximations on a discrete grid of these solutions as well as for proving its correctness, i.e. the convergence of these approximations to the solution when the grid size vanishes."'),
('"Shape from Single Scattering for Translucent Objects"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33709-3_27"', '"Translucent objects strongly scatter incident light. Scattering makes the problem of estimating shape of translucent objects difficult, because reflective or transmitted light cannot be reliably extracted from the scattering. In this paper, we propose a new shape estimation method by directly utilizing scattering measurements. Although volumetric scattering is a complex phenomenon, single scattering can be relatively easily modeled because it is a simple one-bounce collision of light to a particle in a medium. Based on this observation, our method determines the shape of objects from the observed intensities of the single scattering and its attenuation. We develop a solution method that simultaneously determines scattering parameters and the shape based on energy minimization. We demonstrate the effectiveness of the proposed approach by extensive experiments using synthetic and real data."'),
('"Shape from Texture without Boundaries"', '"ECCV 2002"', '["Shape from texture", "texture", "computer vision", "surface fitting"]', '"https://doi.org/10.1007/3-540-47977-5_15"', '"We describe a shape from texture method that constructs a maximum a posteriori estimate of surface coefficients using only the deformation of individual texture elements. Our method does not need to use either the boundary of the observed surface or any assumption about the overall distribution of elements. The method assumes that texture elements are of a limited number of types of fixed shape. We show that, with this assumption and assuming generic view and texture, each texture element yields the surface gradient unique up to a two-fold ambiguity. Furthermore, texture elements that are not from one of the types can be identified and ignored. An EM-like procedure yields a surface reconstruction from the data. The method is defined for othographic views \\u2014 an extension to perspective views appears to be complex, but possible. Examples of reconstructions for synthetic images of surfaces are provided, and compared with ground truth. We also provide examples of reconstructions for images of real scenes. We show that our method for recovering local texture imaging transformations can be used to retexture objects in images of real scenes."'),
('"Shape in a Box"', '"ECCV 2014"', '["Photometric stereo", "Mutual illumination", "Shape recovery"]', '"https://doi.org/10.1007/978-3-319-16199-0_24"', '"Many techniques have been developed in computer vision to recover three-dimensional shape from two-dimensional images. These techniques impose various combinations of assumptions/restrictions of conditions to produce a representation of shape (e.g. a depth/height map). Although great progress has been made it is a problem which remains far from solved, with most methods requiring a non-passive imaging environment. In this paper we develop on a variant of photometric stereo called \\u201cShape from color\\u201d (SFC). We remove the restriction of known, direct light sources by exploiting mutual illumination; we simply take pictures of objects within a colourful box, hence \\u201cShape in a Box\\u201d. We discuss the engineering process used to develop our set-up and demonstrate experimentally that our passive imaging environment recovers shape to the same accuracy as SFC. A second contribution of this paper is to benchmark our approach using real objects with known ground truth, including some 3D printed objects."'),
('"Shape Matching and Recognition \\u2013 Using Generative Models and Informative Features"', '"ECCV 2004"', '["Image Retrieval", "Real Image", "Target Shape", "Shape Match", "Informative Feature"]', '"https://doi.org/10.1007/978-3-540-24672-5_16"', '"We present an algorithm for shape matching and recognition based on a generative model for how one shape can be generated by the other. This generative model allows for a class of transformations, such as affine and non-rigid transformations, and induces a similarity measure between shapes. The matching process is formulated in the EM algorithm. To have a fast algorithm and avoid local minima, we show how the EM algorithm can be approximated by using informative features, which have two key properties\\u2013invariant and representative. They are also similar to the proposal probabilities used in DDMCMC [13]. The formulation allows us to know when and why approximations can be made and justifies the use of bottom-up features, which are used in a wide range of vision problems. This integrates generative models and feature-based approaches within the EM framework and helps clarifying the relationships between different algorithms for this problem such as shape contexts [3] and softassign [5]. We test the algorithm on a variety of data sets including MPEG7 CE-Shape-1, Kimia silhouettes, and real images of street scenes. We demonstrate very effective performance and compare our results with existing algorithms. Finally, we briefly illustrate how our approach can be generalized to a wider range of problems including object detection."'),
('"Shape Matching by Segmentation Averaging"', '"ECCV 2008"', '["Image Segmentation", "Segmentation Algorithm", "Object Detection", "Spatial Pyramid", "Spatial Pyr', '"https://doi.org/10.1007/978-3-540-88682-2_43"', '"We use segmentations to match images by shape. To address the unreliability of segmentations, we give a closed form approximation to an average over all segmentations. Our technique has many extensions, yielding new algorithms for tracking, object detection, segmentation, and edge-preserving smoothing. For segmentation, instead of a maximum a posteriori approach, we compute the \\u201ccentral\\u201d segmentation minimizing the average distance to all segmentations of an image. Our methods for segmentation and object detection perform competitively, and we also show promising results in tracking and edge\\u2013preserving smoothing."'),
('"Shape Priors for Level Set Representations"', '"ECCV 2002"', '["Active Contour", "Shape Model", "Active Contour Model", "Variational Framework", "Geodesic Active ', '"https://doi.org/10.1007/3-540-47967-8_6"', '"Level Set Representations, the pioneering framework introduced by Osher and Sethian [14] is the most common choice for the implementation of variational frameworks in Computer Vision since it is implicit, intrinsic, parameter and topology free. However, many Computer vision applications refer to entities with physical meanings that follow a shape form with a certain degree of variability. In this paper, we propose a novel energetic form to introduce shape constraints to level set representations. This formulation exploits all advantages of these representations resulting on a very elegant approach that can deal with a large number of parametric as well as continuous transformations. Furthermore, it can be combined with existing well known level set-based segmentation approaches leading to paradigms that can deal with noisy, occluded and missing or physically corrupted data. Encouraging experimental results are obtained using synthetic and real images."'),
('"Shape Reconstruction from 3D and 2D Data Using PDE-Based Deformable Surfaces"', '"ECCV 2004"', '["Point Cloud", "Collision Detection", "Deformable Model", "Volumetric Data", "Subdivision Surface"]', '"https://doi.org/10.1007/978-3-540-24672-5_19"', '"In this paper, we propose a new PDE-based methodology for deformable surfaces that is capable of automatically evolving its shape to capture the geometric boundary of the data and simultaneously discover its underlying topological structure. Our model can handle multiple types of data (such as volumetric data, 3D point clouds and 2D image data), using a common mathematical framework. The deformation behavior of the model is governed by partial differential equations (e.g. the weighted minimal surface flow). Unlike the level-set approach, our model always has an explicit representation of geometry and topology. The regularity of the model and the stability of the numerical integration process are ensured by a powerful Laplacian tangential smoothing operator. By allowing local adaptive refinement of the mesh, the model can accurately represent sharp features. We have applied our model for shape reconstruction from volumetric data, unorganized 3D point clouds and multiple view images. The versatility and robustness of our model allow its application to the challenging problem of multiple view reconstruction. Our approach is unique in its combination of simultaneous use of a high number of arbitrary camera views with an explicit mesh that is intuitive and easy-to-interact-with. Our model-based approach automatically selects the best views for reconstruction, allows for visibility checking and progressive refinement of the model as more images become available. The results of our extensive experiments on synthetic and real data demonstrate robustness, high reconstruction accuracy and visual quality."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Shape Sharing for Object Segmentation"', '"ECCV 2012"', '["Test Image", "Global Shape", "Object Segmentation", "Segmentation Quality", "Shape Prior"]', '"https://doi.org/10.1007/978-3-642-33786-4_33"', '"We introduce a category-independent shape prior for object segmentation. Existing shape priors assume class-specific knowledge, and thus are restricted to cases where the object class is known in advance. The main insight of our approach is that shapes are often shared between objects of different categories. To exploit this \\u201cshape sharing\\u201d phenomenon, we develop a non-parametric prior that transfers object shapes from an exemplar database to a test image based on local shape matching. The transferred shape priors are then enforced in a graph-cut formulation to produce a pool of object segment hypotheses. Unlike previous multiple segmentation methods, our approach benefits from global shape cues; unlike previous top-down methods, it assumes no class-specific training and thus enhances segmentation even for unfamiliar categories. On the challenging PASCAL 2010 and Berkeley Segmentation datasets, we show it outperforms the state-of-the-art in bottom-up or category-independent segmentation."'),
('"Shape-Based Retrieval of Heart Sounds for Disease Similarity Detection"', '"ECCV 2008"', '["Sound pattern analysis", "audio retrieval", "curve analysis", "healthcare application"]', '"https://doi.org/10.1007/978-3-540-88688-4_42"', '"Retrieval of similar heart sounds from a sound database has applications in physician training, diagnostic screening, and decision support. In this paper, we exploit a visual rendering of heart sounds and model the morphological variations of audio envelopes through a constrained non-rigid translation transform. Similar heart sounds are then retrieved by recovering the corresponding alignment transform using a variant of shape-based dynamic time warping. Results of similar heart sound retrieval are demonstrated for various diseases on a large database of heart sounds."'),
('"Shape-from-Silhouette with Two Mirrors and an Uncalibrated Camera"', '"ECCV 2006"', '["Focal Length", "Closed Form Solution", "Real Image", "Virtual Object", "Synthetic Image"]', '"https://doi.org/10.1007/11744047_13"', '"Two planar mirrors are positioned to show five views of an object, and snapshots are captured from different viewpoints. We present closed form solutions for calculating the focal length, principal point, mirror and camera poses directly from the silhouette outlines of the object and its reflections. In the noisy case, these equations are used to form initial parameter estimates that are refined using iterative minimisation. The self-calibration allows the visual cones from each silhouette to be specified in a common reference frame so that the visual hull can be constructed. The proposed setup provides a simple method for creating 3D multimedia content that does not rely on specialised equipment. Experimental results demonstrate the reconstruction of a toy horse and a locust from real images. Synthetic images are used to quantify the sensitivity of the self-calibration to quantisation noise. In terms of the silhouette calibration ratio, degradation in silhouette quality has a greater effect on silhouette set consistency than computed calibration parameters."'),
('"Shapecollage: Occlusion-Aware, Example-Based Shape Interpretation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33712-3_48"', '"This paper presents an example-based method to interpret a 3D shape from a single image depicting that shape. A major difficulty in applying an example-based approach to shape interpretation is the combinatorial explosion of shape possibilities that occur at occluding contours. Our key technical contribution is a new shape patch representation and corresponding pairwise compatibility terms that allow for flexible matching of overlapping patches, avoiding the combinatorial explosion by allowing patches to explain only the parts of the image they best fit. We infer the best set of localized shape patches over a graph of keypoints at multiple scales to produce a discontinuous shape representation we term a shape collage. To reconstruct a smooth result, we fit a surface to the collage using the predicted confidence of each shape patch. We demonstrate the method on shapes depicted in line drawing, diffuse and glossy shading, and textured styles."'),
('"ShapeForest: Building Constrained Statistical Shape Models with Decision Trees"', '"ECCV 2014"', '["Shape Model", "Shape Space", "Active Contour Model", "Reconstruction Accuracy", "Active Appearance', '"https://doi.org/10.1007/978-3-319-10578-9_39"', '"Constrained local models (CLM) are frequently used to locate points on deformable objects. They usually consist of feature response images, defining the local update of object points and a shape prior used to regularize the final shape. Due to the complex shape variation within an object class this is a challenging problem. However in many segmentation tasks a simpler object representation is available in form of sparse landmarks which can be reliably detected from images. In this work we propose ShapeForest, a novel shape representation which is able to model complex shape variation, preserves local shape information and incorporates prior knowledge during shape space inference. Based on a sparse landmark representation associated with each shape the ShapeForest, trained using decision trees and geometric features, selects a subset of relevant shapes to construct an instance specific parametric shape model. Hereby the ShapeForest learns the association between the geometric features and shape variability. During testing, based on the estimated sparse landmark representation a constrained shape space is constructed and used for shape initialization and regularization during the iterative shape refinement within the CLM framework. We demonstrate the effectiveness of our approach on a set of medical segmentation problems where our database contains complex morphological and pathological variations of several anatomical structures."'),
('"Shaping Art with Art: Morphological Analysis for Investigating Artistic Reproductions"', '"ECCV 2012"', '["Transformation Model", "Thin Plate Spline", "Local Transformation", "Landmark Point", "Contour Ext', '"https://doi.org/10.1007/978-3-642-33863-2_59"', '"Whereas one part of art history is a history of inventions, the other part is a history of transfer, of variations and copies. Art history wants to understand the differences between these, in order to learn about artistic choices and stylistic variations. In this paper we develop a method that can detect variations between artworks and their reproductions, in particular deformations in shape. Specifically, we present a novel algorithm which automatically finds regions which share the same transformation between original and its reproduction. We do this by minimizing an energy function which measures the distortion between local transformations of the shape. Thereby, the grouping and registration problem are addressed jointly and model complexity is obtained using a stability analysis. Moreover, our method allows art historians to evaluate the exactness of a copy by identifying which contours where considered relevant to copy. The proposed shape-based approach thus helps to investigate art through the art of reproduction."'),
('"Shift-Invariant Dynamic Texture Recognition"', '"ECCV 2006"', '["Feature Vector", "Multivariate Time Series", "Dynamic Texture", "Texture Scene", "Univariate Time ', '"https://doi.org/10.1007/11744047_42"', '"We address the problem of recognition of natural motions such as water, smoke and wind-blown vegetation. Such dynamic scenes exhibit characteristic stochastic motions, and we ask whether the scene contents can be recognized using motion information alone. Previous work on this problem has considered only the case where the texture samples have sufficient overlap to allow registration, so that the visual content of the scene is very similar between examples. In this paper we investigate the recognition of entirely non-overlapping views of the same underlying motion, specifically excluding appearance-based cues."'),
('"Shock-Based Indexing into Large Shape Databases"', '"ECCV 2002"', '["Similarity metric", "object recognition", "shape matching", "shape retrieval", "categorization", "', '"https://doi.org/10.1007/3-540-47977-5_48"', '"This paper examines issues arising in applying a previously developed edit-distance shock graph matching technique to indexing into large shape databases. This approach compares the shock graph topology and attributes to produce a similarity metric, and results in 100% recognition rate in querying a database of approximately 200 shapes. However, indexing into a significantly larger database is faced with both the lack of a suitable database, and more significantly with the expense related to computing the metric. We have thus (i) gathered shapes from a variety of sources to create a database of over 1000 shapes from forty categories as a stage towards developing an approach for indexing into a much larger database; (ii) developed a coarse-scale approximate similarly measure which relies on the shock graph topology and a very coarse sampling of link attributes. We show that this is a good first-order approximation of the similarly metric and is two orders of magnitude more efficient to compute. An interesting outcome of using this efficient but approximate similarity measure is that the approximation naturally demands a notion of categories to give high precision; (iii) developed an exemplar-based indexing scheme which discards a large number of non-matching shapes solely based on distance to exemplars, coarse scale representatives of each category. The use of a coarse-scale matching measure in conjunction with a coarse-scale sampling of the database leads to a significant reduction in the computational effort without discarding correct matches, thus paving the way for indexing into databases of tens of thousands of shapes."'),
('"Shrinkage Expansion Adaptive Metric Learning"', '"ECCV 2014"', '["Shrinkage-expansion rule", "adaptive bound constraints", "pairwise constrained metric learning", "', '"https://doi.org/10.1007/978-3-319-10584-0_30"', '"Conventional pairwise constrained metric learning methods usually restrict the distance between samples of a similar pair to be lower than a fixed upper bound, and the distance between samples of a dissimilar pair higher than a fixed lower bound. Such fixed bound based constraints, however, may not work well when the intra- and inter-class variations are complex. In this paper, we propose a shrinkage expansion adaptive metric learning (SEAML) method by defining a novel shrinkage-expansion rule for adaptive pairwise constraints. SEAML is very effective in learning metrics from data with complex distributions. Meanwhile, it also suggests a new rule to assess the similarity between a pair of samples based on whether their distance is shrunk or expanded after metric learning. Our extensive experimental results demonstrated that SEAML achieves better performance than state-of-the-art metric learning methods. In addition, the proposed shrinkage-expansion adaptive pairwise constraints can be readily applied to many other pairwise constrained metric learning algorithms, and boost significantly their performance in applications such as face verification on LFW and PubFig databases."'),
('"SIFT and Shape Context for Feature-Based Nonlinear Registration of Thoracic CT Images"', '"CVAMIA 2006"', '["Matching Cost", "Registration Approach", "Nonlinear Registration", "Shape Context Descriptor", "De', '"https://doi.org/10.1007/11889762_7"', '"Nonlinear image registration is a prerequisite for various medical image analysis applications. Many data acquisition protocols suffer from problems due to breathing motion which has to be taken into account for further analysis. Intensity based nonlinear registration is often used to align differing images, however this requires a large computational effort, is sensitive to intensity variations and has problems with matching small structures. In this work a feature-based image registration method is proposed that combines runtime efficiency with good registration accuracy by making use of a fully automatic feature matching and registration approach. The algorithm stages are 3D corner detection, calculation of local (SIFT) and global (Shape Context) 3D descriptors, robust feature matching and calculation of a dense displacement field. An evaluation of the algorithm on seven synthetic and four clinical data sets is presented. The quantitative and qualitative evaluations show lower runtime and superior results when compared to the Demons algorithm."'),
('"SIFT Flow: Dense Correspondence across Different Scenes"', '"ECCV 2008"', '["Query Image", "Alignment Algorithm", "Motion Prediction", "Sift Feature", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-540-88690-7_3"', '"While image registration has been studied in different areas of computer vision, aligning images depicting different scenes remains a challenging problem, closer to recognition than to image matching. Analogous to optical flow, where an image is aligned to its temporally adjacent frame, we propose SIFT flow, a method to align an image to its neighbors in a large image collection consisting of a variety of scenes. For a query image, histogram intersection on a bag-of-visual-words representation is used to find the set of nearest neighbors in the database. The SIFT flow algorithm then consists of matching densely sampled SIFT features between the two images, while preserving spatial discontinuities. The use of SIFT features allows robust matching across different scene/object appearances and the discontinuity-preserving spatial model allows matching of objects located at different parts of the scene. Experiments show that the proposed approach is able to robustly align complicated scenes with large spatial distortions. We collect a large database of videos and apply the SIFT flow algorithm to two applications: (i) motion field prediction from a single static image and (ii) motion synthesis via transfer of moving objects."'),
('"Sign Language Recognition Using Convolutional Neural Networks"', '"ECCV 2014"', '["Convolutional neural network", "Deep learning", "Gesture recognition", "Sign language recognition"', '"https://doi.org/10.1007/978-3-319-16178-5_40"', '"There is an undeniable communication problem between the Deaf community and the hearing majority. Innovations in automatic sign language recognition try to tear down this communication barrier. Our contribution considers a recognition system using the Microsoft Kinect, convolutional neural networks (CNNs) and GPU acceleration. Instead of constructing complex handcrafted features, CNNs are able to automate the process of feature construction. We are able to recognize 20 Italian gestures with high accuracy. The predictive model is able to generalize on users and surroundings not occurring during training with a cross-validation accuracy of 91.7%. Our model achieves a mean Jaccard Index of 0.789 in the ChaLearn 2014 Looking at People gesture spotting competition."'),
('"Signature-Based Document Image Retrieval"', '"ECCV 2008"', '["Pattern Anal", "Document Image", "Retrieval Performance", "Iterative Close Point", "Mean Average P', '"https://doi.org/10.1007/978-3-540-88690-7_56"', '"As the most pervasive method of individual identification and document authentication, signatures present convincing evidence and provide an important form of indexing for effective document image processing and retrieval in a broad range of applications. In this work, we developed a fully automatic signature-based document image retrieval system that handles: 1) Automatic detection and segmentation of signatures from document images and 2) Translation, scale, and rotation invariant signature matching for document image retrieval. We treat signature retrieval in the unconstrained setting of non-rigid shape matching and retrieval, and quantitatively study shape representations, shape matching algorithms, measures of dissimilarity, and the use of multiple query instances in document image retrieval. Extensive experiments using large real world collections of English and Arabic machine printed and handwritten documents demonstrate the excellent performance of our system. To the best of our knowledge, this is the first automatic retrieval system for general document images by using signatures as queries, without manual annotation of the image collection."'),
('"Significantly Different Textures: A Computational Model of Pre-attentive Texture Segmentation"', '"ECCV 2000"', '["Decision Stage", "Internal Noise", "Resultant Vector", "Texture Segmentation", "Orientation Estima', '"https://doi.org/10.1007/3-540-45053-X_13"', '"Recent human vision research [1] suggests modelling preattentive texture segmentation by taking a set of feature samples from a local region on each side of a hypothesized edge, and then performing standard statistical tests to determine if the two samples differ significantly in their mean or variance. If the difference is significant at a specified level of confidence, a human observer will tend to pre-attentively see a texture edge at that location. I present an algorithm based upon these results, with a well specified decision stage and intuitive, easily fit parameters. Previous models of pre-attentive texture segmentation have poorly specified decision stages, more unknown free parameters, and in some cases incorrectly model human performance. The algorithm uses heuristics for guessing the orientation of a texture edge at a given location, thus improving computational efficiency by performing the statistical tests at only one orientation for each spatial location."'),
('"Silhouette-Based Method for Object Classification and Human Action Recognition in Video"', '"ECCV 2006"', '["Action Recognition", "Object Classification", "Foreground Pixel", "Human Action Recognition", "For', '"https://doi.org/10.1007/11754336_7"', '"In this paper we present an instance based machine learning algorithm and system for real-time object classification and human action recognition which can help to build intelligent surveillance systems. The proposed method makes use of object silhouettes to classify objects and actions of humans present in a scene monitored by a stationary camera. An adaptive background subtract-tion model is used for object segmentation. Template matching based supervised learning method is adopted to classify objects into classes like human, human group and vehicle; and human actions into predefined classes like walking, boxing and kicking by making use of object silhouettes."'),
('"Similarity Constrained Latent Support Vector Machine: An Application to Weakly Supervised Action Cl', '"ECCV 2012"', '["Latent Variable", "Action Recognition", "Latent Region", "Test Video", "Training Video"]', '"https://doi.org/10.1007/978-3-642-33786-4_5"', '"We present a novel algorithm for weakly supervised action classification in videos. We assume we are given training videos annotated only with action class labels. We learn a model that can classify unseen test videos, as well as localize a region of interest in the video that captures the discriminative essence of the action class. A novel Similarity Constrained Latent Support Vector Machine model is developed to operationalize this goal. This model specifies that videos should be classified correctly, and that the latent regions of interest chosen should be coherent over videos of an action class. The resulting learning problem is challenging, and we show how dual decomposition can be employed to render it tractable. Experimental results demonstrate the efficacy of the method."'),
('"Similarity Features for Facial Event Analysis"', '"ECCV 2008"', '["Facial Expression", "Training Sample", "Local Binary Pattern", "Facial Expression Recognition", "F', '"https://doi.org/10.1007/978-3-540-88682-2_52"', '"Each facial event will give rise to complex facial appearance variation. In this paper, we propose similarity features to describe the facial appearance for video-based facial event analysis. Inspired by the kernel features, for each sample, we compare it with the reference set with a similarity function, and we take the log-weighted summarization of the similarities as its similarity feature. Due to the distinctness of the apex images of facial events, we use their cluster-centers as the references. In order to capture the temporal dynamics, we use the K-means algorithm to divide the similarity features into several clusters in temporal domain, and each cluster is modeled by a Gaussian distribution. Based on the Gaussian models, we further map the similarity features into dynamic binary patterns to handle the issue of time resolution, which embed the time-warping operation implicitly. The haar-like descriptor is used to extract the visual features of facial appearance, and Adaboost is performed to learn the final classifiers. Extensive experiments carried on the Cohn-Kanade database show the promising performance of the proposed method."'),
('"Similarity-Invariant Sketch-Based Image Retrieval in Large Databases"', '"ECCV 2014"', '["Image Retrieval", "Shape Representation and Matching"]', '"https://doi.org/10.1007/978-3-319-10599-4_26"', '"Proliferation of touch-based devices has made the idea of sketch-based image retrieval practical. While many methods exist for sketch-based image retrieval on small datasets, little work has been done on large (web)-scale image retrieval. In this paper, we present an efficient approach for image retrieval from millions of images based on user-drawn sketches. Unlike existing methods which are sensitive to even translation or scale variations, our method handles translation, scale, rotation (similarity) and small deformations. To make online retrieval fast, each database image is preprocessed to extract sequences of contour segments (chains) that capture sufficient shape information which are represented by succinct variable length descriptors. Chain similarities are computed by a fast Dynamic Programming-based approximate substring matching algorithm, which enables partial matching of chains. Finally, hierarchical k-medoids based indexing is used for very fast retrieval in a few seconds on databases with millions of images. Qualitative and quantitative results clearly demonstrate superiority of the approach over existing methods."'),
('"Simultaneous Compaction and Factorization of Sparse Image Motion Matrices"', '"ECCV 2012"', '["IEEE Computer Society", "Matrix Factorization", "Measurement Matrix", "Generalization Error", "Imp', '"https://doi.org/10.1007/978-3-642-33783-3_33"', '"Matrices that collect the image coordinates of point features tracked through video \\u2013 one column per feature \\u2013 have often low rank, either exactly or approximately. This observation has led to many matrix factorization methods for 3D reconstruction, motion segmentation, or regularization of feature trajectories. However, temporary occlusions, image noise, and variations in lighting, pose, or object geometry often confound trackers. A feature that reappears after a temporary tracking failure \\u2013 whatever the cause \\u2013 is regarded as a new feature by typical tracking systems, resulting in very sparse matrices with many columns and rendering factorization problematic. We propose a method to simultaneously factor and compact such a matrix by merging groups of columns that correspond to the same feature into single columns. This combination of compaction and factorization makes trackers more resilient to changes in appearance and short occlusions. Preliminary experiments show that imputation of missing matrix entries \\u2013 and therefore matrix factorization \\u2013 becomes significantly more reliable as a result. Clean column merging also required us to develop a history-sensitive feature reinitialization method we call feature snapping that aligns merged feature trajectory segments precisely to each other."'),
('"Simultaneous Detection and Registration for Ileo-Cecal Valve Detection in 3D CT Colonography"', '"ECCV 2008"', '["Compute Tomography Colonography", "Ileocecal Valve", "Prior Learning", "Compute Tomography Volume"', '"https://doi.org/10.1007/978-3-540-88693-8_34"', '"Object detection and recognition has achieved a significant progress in recent years. However robust 3D object detection and segmentation in noisy 3D data volumes remains a challenging problem. Localizing an object generally requires its spatial configuration (i.e., pose, size) being aligned with the trained object model, while estimation of an object\\u2019s spatial configuration is only valid at locations where the object appears. Detecting object while exhaustively searching its spatial parameters, is computationally prohibitive due to the high dimensionality of 3D search space. In this paper, we circumvent this computational complexity by proposing a novel framework capable of incrementally learning the object parameters (IPL) of location, pose and scale. This method is based on a sequence of binary encodings of the projected true positives from the original 3D object annotations (i.e., the projections of the global optima from the global space into the sections of subspaces). The training samples in each projected subspace are labeled as positive or negative, according their spatial registration distances towards annotations as ground-truth. Each encoding process can be considered as a general binary classification problem and is implemented using probabilistic boosting tree algorithm. We validate our approach with extensive experiments and performance evaluations for Ileo-Cecal Valve (ICV) detection in both clean and tagged 3D CT colonography scans. Our final ICV detection system also includes an optional prior learning procedure for IPL which further speeds up the detection."'),
('"Simultaneous Detection and Segmentation"', '"ECCV 2014"', '["detection", "segmentation", "convolutional networks"]', '"https://doi.org/10.1007/978-3-319-10584-0_20"', '"We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances. We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN [16]), introducing a novel architecture tailored for SDS. We then use category-specific, top-down figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work."'),
('"Simultaneous Feature and Dictionary Learning for Image Set Based Face Recognition"', '"ECCV 2014"', '["Face recognition", "image set", "feature learning", "dictionary learning", "simultaneous learning"', '"https://doi.org/10.1007/978-3-319-10590-1_18"', '"In this paper, we propose a simultaneous feature and dictionary learning (SFDL) method for image set based face recognition, where each training and testing example contains a face image set captured from different poses, illuminations, expressions and resolutions. While several feature learning and dictionary learning methods have been proposed for image set based face recognition in recent years, most of them learn the features and dictionaries separately, which may not be powerful enough because some discriminative information for dictionary learning may be compromised in the feature learning stage if they are applied sequentially, and vice versa. To address this, we propose a SFDL method to learn discriminative features and dictionaries simultaneously from raw face images so that discriminative information can be jointly exploited. Extensive experimental results on four widely used face datasets show that our method achieves better performance than state-of-the-art image set based face recognition methods."'),
('"Simultaneous Image Classification and Annotation via Biased Random Walk on Tri-relational Graph"', '"ECCV 2012"', '["Random Walk", "Class Label", "Image Annotation", "Confusion Matrice", "Category Class"]', '"https://doi.org/10.1007/978-3-642-33783-3_59"', '"Image annotation as well as classification are both critical and challenging work in computer vision research. Due to the rapid increasing number of images and inevitable biased annotation or classification by the human curator, it is desired to have an automatic way. Recently, there are lots of methods proposed regarding image classification or image annotation. However, people usually treat the above two tasks independently and tackle them separately. Actually, there is a relationship between the image class label and image annotation terms. As we know, an image with the sport class label rowing is more likely to be annotated with the terms water, boat and oar than the terms wall, net and floor, which are the descriptions of indoor sports."'),
('"Simultaneous Motion Detection and Background Reconstruction with a Mixed-State Conditional Markov R', '"ECCV 2008"', '["Reference Image", "Background Subtraction", "Motion Detection", "Spatial Regularization", "Backgro', '"https://doi.org/10.1007/978-3-540-88682-2_10"', '"We consider the problem of motion detection by background subtraction. An accurate estimation of the background is only possible if we locate the moving objects; meanwhile, a correct motion detection is achieved if we have a good available background model. This work proposes a new direction in the way such problems are considered. The main idea is to formulate this class of problem as a joint decision-estimation unique step. The goal is to exploit the way two processes interact, even if they are of a dissimilar nature (symbolic-continuous), by means of a recently introduced framework called mixed-state Markov random fields. In this paper, we will describe the theory behind such a novel statistical framework, that subsequently will allows us to formulate the specific joint problem of motion detection and background reconstruction. Experiments on real sequences and comparisons with existing methods will give a significant support to our approach. Further implications for video sequence inpainting will be also discussed."'),
('"Simultaneous Nonrigid Registration of Multiple Point Sets and Atlas Construction"', '"ECCV 2006"', '["Fetal Alcohol Syndrome", "Nonrigid Registration", "Active Shape Model", "Correspondence Problem", ', '"https://doi.org/10.1007/11744078_43"', '"Estimating a meaningful average or mean shape from a set of shapes represented by unlabeled point-sets is a challenging problem since, usually this involves solving for point correspondence under a non-rigid motion setting. In this paper, we propose a novel and robust algorithm that is capable of simultaneously computing the mean shape from multiple unlabeled point-sets (represented by finite mixtures) and registering them nonrigidly to this emerging mean shape. This algorithm avoids the correspondence problem by minimizing the Jensen-Shannon (JS) divergence between the point sets represented as finite mixtures. We derive the analytic gradient of the cost function namely, the JS-divergence, in order to efficiently achieve the optimal solution. The cost function is fully symmetric with no bias toward any of the given shapes to be registered and whose mean is being sought. Our algorithm can be especially useful for creating atlases of various shapes present in images as well as for simultaneously (rigidly or non-rigidly) registering 3D range data sets without having to establish any correspondence. We present experimental results on non-rigidly registering 2D as well as 3D real data (point sets)."'),
('"Simultaneous Object Pose and Velocity Computation Using a Single View from a Rolling Shutter Camera', '"ECCV 2006"', '["Classical Algorithm", "Single View", "Bundle Adjustment", "Velocity Parameter", "Velocity Computat', '"https://doi.org/10.1007/11744047_5"', '"An original concept for computing instantaneous 3D pose and 3D velocity of fast moving objects using a single view is proposed, implemented and validated. It takes advantage of the image deformations induced by rolling shutter in CMOS image sensors. First of all, after analysing the rolling shutter phenomenon, we introduce an original model of the image formation when using such a camera, based on a general model of moving rigid sets of 3D points. Using 2D-3D point correspondences, we derive two complementary methods, compensating for the rolling shutter deformations to deliver an accurate 3D pose and exploiting them to also estimate the full 3D velocity. The first solution is a general one based on non-linear optimization and bundle adjustment, usable for any object, while the second one is a closed-form linear solution valid for planar objects. The resulting algorithms enable us to transform a CMOS low cost and low power camera into an innovative and powerful velocity sensor. Finally, experimental results with real data confirm the relevance and accuracy of the approach."'),
('"Simultaneous Object Recognition and Segmentation by Image Exploration"', '"ECCV 2004"', '["Test Image", "Model Image", "Expansion Phase", "Correct Match", "Viewpoint Change"]', '"https://doi.org/10.1007/978-3-540-24670-1_4"', '"Methods based on local, viewpoint invariant features have proven capable of recognizing objects in spite of viewpoint changes, occlusion and clutter. However, these approaches fail when these factors are too strong, due to the limited repeatability and discriminative power of the features. As additional shortcomings, the objects need to be rigid and only their approximate location is found. We present a novel Object Recognition approach which overcomes these limitations. An initial set of feature correspondences is first generated. The method anchors on it and then gradually explores the surrounding area, trying to construct more and more matching features, increasingly farther from the initial ones. The resulting process covers the object with matches, and simultaneously separates the correct matches from the wrong ones. Hence, recognition and segmentation are achieved at the same time. Only very few correct initial matches suffice for reliable recognition. The experimental results demonstrate the stronger power of the presented method in dealing with extensive clutter, dominant occlusion, large scale and viewpoint changes. Moreover non-rigid deformations are explicitly taken into account, and the approximative contours of the object are produced. The approach can extend any viewpoint invariant feature extractor."'),
('"Simultaneous Segmentation and Figure/Ground Organization Using Angular Embedding"', '"ECCV 2010"', '["Image Segmentation", "Natural Image", "Illusory Contour", "Edge Pixel", "Ground Organization"]', '"https://doi.org/10.1007/978-3-642-15552-9_33"', '"Image segmentation and figure/ground organization are fundamental steps in visual perception. This paper introduces an algorithm that couples these tasks together in a single grouping framework driven by low-level image cues. By encoding both affinity and ordering preferences in a common representation and solving an Angular Embedding problem, we allow segmentation cues to influence figure/ground assignment and figure/ground cues to influence segmentation. Results are comparable to state-of-the-art automatic image segmentation systems, while additionally providing a global figure/ground ordering on regions."'),
('"Simultaneous Shape and Pose Adaption of Articulated Models Using Linear Optimization"', '"ECCV 2012"', '["Shape Adaption", "Pose Estimation", "Mesh Editing", "Linear Optimization"]', '"https://doi.org/10.1007/978-3-642-33718-5_52"', '"We propose a novel formulation to express the attachment of a polygonal surface to a skeleton using purely linear terms. This enables to simultaneously adapt the pose and shape of an articulated model in an efficient way. Our work is motivated by the difficulty to constrain a mesh when adapting it to multi-view silhouette images. However, such an adaption is essential when capturing the detailed temporal evolution of skin and clothing of a human actor without markers. While related work is only able to ensure surface consistency during mesh adaption, our coupled optimization of the skeleton creates structural stability and minimizes the sensibility to occlusions and outliers in input images. We demonstrate the benefits of our approach in an extensive evaluation. The skeleton attachment considerably reduces implausible deformations, especially when the number of input views is limited."'),
('"Simultaneous Visual Recognition of Manipulation Actions and Manipulated Objects"', '"ECCV 2008"', '["Object Recognition", "Gesture Recognition", "Gradient Orientation", "Late Fusion", "Hand Shape"]', '"https://doi.org/10.1007/978-3-540-88688-4_25"', '"The visual analysis of human manipulation actions is of interest for e.g. human-robot interaction applications where a robot learns how to perform a task by watching a human. In this paper, a method for classifying manipulation actions in the context of the objects manipulated, and classifying objects in the context of the actions used to manipulate them is presented. Hand and object features are extracted from the video sequence using a segmentation based approach. A shape based representation is used for both the hand and the object. Experiments show this representation suitable for representing generic shape classes. The action-object correlation over time is then modeled using conditional random fields. Experimental comparison show great improvement in classification rate when the action-object correlation is taken into account, compared to separate classification of manipulation actions and manipulated objects."'),
('"Single Axis Geometry by Fitting Conics"', '"ECCV 2002"', '["Rotation Angle", "Rotation Axis", "Fundamental Matrix", "Single Axis", "Conic Locus"]', '"https://doi.org/10.1007/3-540-47969-4_36"', '"In this paper, we describe a new approach for recovering 3D geometry from an uncalibrated image sequence of a single axis (turn-table) motion. Unlike previous methods, the computation of multiple views encoded by the fundamental matrix or trifocal tensor is not required. Instead, the new approach is based on fitting a conic locus to corresponding image points over multiple views. It is then shown that the geometry of single axis motion can be recovered given at least two such conics. In the case of two conics the reconstruction may have a two fold ambiguity, but this ambiguity is removed if three conics are used."'),
('"Single Color One-Shot Scan Using Topology Information"', '"ECCV 2012"', '["Topology Information", "Grid Pattern", "Dynamic Scene", "Corner Angle", "Rhombus Tiling"]', '"https://doi.org/10.1007/978-3-642-33885-4_49"', '"In this paper, we propose a new technique to achieve one-shot scan using single color and static pattern projector; such a method is ideal for acquisition of a moving object. Since a projector-camera systems generally have uncertainties on retrieving correspondences between the captured image and the projected pattern, many solutions have been proposed. Especially for one-shot scan, which means that only a single image is used for reconstruction, positional information of a pixel on the projected pattern should be encoded by spatial and/or color information. Although color information is frequently used for encoding, it is severely affected by texture and material of the object. In this paper, we propose a technique to solve the problem by using topological information instead of colors. Our technique successfully realizes one-shot scan with monochrome pattern."'),
('"Single Image Deblurring Using Motion Density Functions"', '"ECCV 2010"', '["Camera Motion", "Latent Image", "Blind Deconvolution", "Ground Truth Image", "Blur Kernel"]', '"https://doi.org/10.1007/978-3-642-15549-9_13"', '"We present a novel single image deblurring method to estimate spatially non-uniform blur that results from camera shake. We use existing spatially invariant deconvolution methods in a local and robust way to compute initial estimates of the latent image. The camera motion is represented as a Motion Density Function (MDF) which records the fraction of time spent in each discretized portion of the space of all possible camera poses. Spatially varying blur kernels are derived directly from the MDF. We show that 6D camera motion is well approximated by 3 degrees of motion (in-plane translation and rotation) and analyze the scope of this approximation. We present results on both synthetic and captured data. Our system out-performs current approaches which make the assumption of spatially invariant blur."'),
('"Single Image Shadow Removal via Neighbor-Based Region Relighting"', '"ECCV 2014"', '["Shadow", "Removal", "Illumination", "SVM", "Histogram Matching", "Texture", "Recovery Image Proces', '"https://doi.org/10.1007/978-3-319-16199-0_22"', '"In this paper we present a novel method for shadow removal in single images. For each shadow region we use a trained classifier to identify a neighboring lit region of the same material. Given a pair of lit-shadow regions we perform a region relighting transformation based on histogram matching of luminance values between the shadow region and the lit region. Then, we adjust the CIELAB \\\\(a\\\\) and \\\\(b\\\\) channels of the shadow region by adding constant offsets based on the difference of the median shadow and lit pixel values. We demonstrate that our approach produces results that outperform the state of the art by evaluating our method using a publicly available benchmark dataset."'),
('"Single-Image Super-Resolution: A Benchmark"', '"ECCV 2014"', '["Single-image super-resolution", "performance evaluation", "metrics", "Gaussian blur kernel width"]', '"https://doi.org/10.1007/978-3-319-10593-2_25"', '"Single-image super-resolution is of great importance for vision applications, and numerous algorithms have been proposed in recent years. Despite the demonstrated success, these results are often generated based on different assumptions using different datasets and metrics. In this paper, we present a systematic benchmark evaluation for state-of-the-art single-image super-resolution algorithms. In addition to quantitative evaluations based on conventional full-reference metrics, human subject studies are carried out to evaluate image quality based on visual perception. The benchmark evaluations demonstrate the performance and limitations of state-of-the-art algorithms which sheds light on future research in single-image super-resolution."'),
('"Size Does Matter: Improving Object Recognition and 3D Reconstruction with Cross-Media Analysis of I', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15549-9_53"', '"Most of the recent work on image-based object recognition and 3D reconstruction has focused on improving the underlying algorithms. In this paper we present a method to automatically improve the quality of the reference database, which, as we will show, also affects recognition and reconstruction performances significantly. Starting out from a reference database of clustered images we expand small clusters. This is done by exploiting cross-media information, which allows for crawling of additional images. For large clusters redundant information is removed by scene analysis. We show how these techniques make object recognition and 3D reconstruction both more efficient and more precise - we observed up to 14.8% improvement for the recognition task. Furthermore, the methods are completely data-driven and fully automatic."'),
('"Size Matters: Exhaustive Geometric Verification for Image Retrieval Accepted for ECCV 2012"', '"ECCV 2012"', '["Image Retrieval", "Query Image", "Inverted Index", "Size Matter", "Vocabulary Size"]', '"https://doi.org/10.1007/978-3-642-33709-3_48"', '"The overreaching goals in large-scale image retrieval are bigger, better and cheaper. For systems based on local features we show how to get both efficient geometric verification of every match and unprecedented speed for the low sparsity situation."'),
('"SlamDunk: Affordable Real-Time RGB-D SLAM"', '"ECCV 2014"', '["RGB-D SLAM", "Real time SLAM", "Relative bundle adjustment", "Camera Tracking"]', '"https://doi.org/10.1007/978-3-319-16178-5_28"', '"We propose an effective, real-time solution to the RGB-D SLAM problem dubbed SlamDunk. Our proposal features a multi-view camera tracking approach based on a dynamic local map of the workspace, enables metric loop closure seamlessly and preserves local consistency by means of relative bundle adjustment principles. SlamDunk requires a few threads, low memory consumption and runs at 30 Hz on a standard desktop computer without hardware acceleration by a GPGPU card. As such, it renders real-time dense SLAM affordable on commodity hardware. SlamDunk permits highly responsive interactive operation in a variety of workspaces and scenarios, such as scanning small objects or densely reconstructing large-scale environments. We provide quantitative and qualitative experiments in diverse settings to demonstrate the accuracy and robustness of the proposed approach."'),
('"Sliding Shapes for 3D Object Detection in Depth Images"', '"ECCV 2014"', '["Point Cloud", "Object Detection", "Depth Image", "Computer Graphic", "Indoor Scene"]', '"https://doi.org/10.1007/978-3-319-10599-4_41"', '"The depth information of RGB-D sensors has greatly simplified some common challenges in computer vision and enabled breakthroughs for several tasks. In this paper, we propose to use depth maps for object detection and design a 3D detector to overcome the major difficulties for recognition, namely the variations of texture, illumination, shape, viewpoint, clutter, occlusion, self-occlusion and sensor noises. We take a collection of 3D CAD models and render each CAD model from hundreds of viewpoints to obtain synthetic depth maps. For each depth rendering, we extract features from the 3D point cloud and train an Exemplar-SVM classifier. During testing and hard-negative mining, we slide a 3D detection window in 3D space. Experiment results show that our 3D detector significantly outperforms the state-of-the-art algorithms for both RGB and RGB-D images, and achieves about \\u00d71.7 improvement on average precision compared to DPM and R-CNN. All source code and data are available online."'),
('"Smart Camera Reconfiguration in Assisted Home Environments for Elderly Care"', '"ECCV 2014"', '["Elderly care", "Real time video analysis", "Automatic camera reconfiguration"]', '"https://doi.org/10.1007/978-3-319-16220-1_4"', '"Researchers of different fields have been involved in human behavior analysis during the last years. The successful recognition of human activities from video analysis is still a challenging problem. Within this context, applications targeting elderly care are of considerable interest both for public and industrial bodies, especially considering the aging society we are living in. Ambient intelligence (AmI) technologies, intended as the possibility of automatically detecting and reacting to the status of the environment and of the persons, is probably the major enabling factor. AmI technologies require suitable networks of sensors and actuators, as well as adequate processing and communication technologies. In this paper we propose an innovative solution based on a real time analysis of video with application in the field of elderly care. The system performs anomaly detection and proposes the automatic reconfiguration of the camera network for better monitoring of the ongoing event. The developed framework is tested on a publicly available dataset and has also been deployed and evaluated in a real environment."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"SMD: A Locally Stable Monotonic Change Invariant Feature Descriptor"', '"ECCV 2008"', '["Feature Point", "Local Binary Pattern", "Feature Descriptor", "Illumination Change", "Stability Fa', '"https://doi.org/10.1007/978-3-540-88688-4_20"', '"Extraction and matching of discriminative feature points in images is an important problem in computer vision with applications in image classification, object recognition, mosaicing, automatic 3D reconstruction and stereo. Features are represented and matched via descriptors that must be invariant to small errors in the localization and scale of the extracted feature point, viewpoint changes, and other kinds of changes such as illumination, image compression and blur. While currently used feature descriptors are able to deal with many of such changes, they are not invariant to a generic monotonic change in the intensities, which occurs in many cases. Furthermore, their performance degrades rapidly with many image degradations such as blur and compression where the intensity transformation is non-linear. In this paper, we present a new feature descriptor that obtains invariance to a monotonic change in the intensity of the patch by looking at orders between certain pixels in the patch. An order change between pixels indicates a difference between the patches which is penalized. Summation of such penalties over carefully chosen pixel pairs that are stable to small errors in their localization and are independent of each other leads to a robust measure of change between two features. Promising results were obtained using this approach that show significant improvement over existing methods, especially in the case of illumination change, blur and JPEG compression where the intensity of the points changes from one image to the next."'),
('"Smooth Image Segmentation by Nonparametric Bayesian Inference"', '"ECCV 2006"', '["Segmentation Result", "Base Measure", "Synthetic Aperture Radar Image", "Sampling Algorithm", "Dir', '"https://doi.org/10.1007/11744023_35"', '"A nonparametric Bayesian model for histogram clustering is proposed to automatically determine the number of segments when Markov Random Field constraints enforce smooth class assignments. The nonparametric nature of this model is implemented by a Dirichlet process prior to control the number of clusters. The resulting posterior can be sampled by a modification of a conjugate-case sampling algorithm for Dirichlet process mixture models. This sampling procedure estimates segmentations as efficiently as clustering procedures in the strictly conjugate case. The sampling algorithm can process both single-channel and multi-channel image data. Experimental results are presented for real-world synthetic aperture radar and magnetic resonance imaging data."'),
('"Smoothing Impulsive Noise Using Nonlinear Diffusion Filtering"', '"MMBIA 2004"', '["Median Filter", "Anisotropic Diffusion", "Nonlinear Diffusion", "Synthetic Image", "Normalize Mean', '"https://doi.org/10.1007/978-3-540-27816-0_10"', '"A new anisotropic diffusion-filtering scheme to smooth images with heavy-tailed or binary noise types similar to salt&pepper noise is presented. The proposed scheme estimates edge gradient from an image that is smoothed or \\u201dregularized\\u201d with a median filter. Its performance was demonstrated on synthetic images that were corrupted by Gaussian, salt&pepper and Weibull noises, and actual medical images. The visual and quantitative evaluation of the scheme demonstrated comparable or better performance."'),
('"Snippet Based Trajectory Statistics Histograms for Assistive Technologies"', '"ECCV 2014"', '["Assisted living systems", "Medical device usage", "Action recognition"]', '"https://doi.org/10.1007/978-3-319-16220-1_1"', '"Due to increasing hospital costs and traveling time, more and more patients decide to use medical devices at home without traveling to the hospital. However, these devices are not always very straight-forward for usage, and the recent reports show that there are many injuries and even deaths caused by the wrong use of these devices. Since human supervision during every usage is impractical, there is a need for computer vision systems that would recognize actions and detect if the patient has done something wrong. In this paper, we propose to use Snippet Based Trajectory Statistics Histograms descriptor to recognize actions in two medical device usage problems; inhaler device usage and infusion pump usage. Snippet Based Trajectory Statistics Histograms encodes the motion and position statistics of densely extracted trajectories from a video. Our experiments show that by using Snippet Based Trajectory Statistics Histograms technique, we improve the overall performance for both tasks. Additionally, this method does not require heavy computation, and is suitable for real-time systems."'),
('"SocialSync: Sub-Frame Synchronization in a Smartphone Camera Network"', '"ECCV 2014"', '["Multiple viewpoints", "Camera array", "Camera network", "Synchronization", "Smartphone", "Mobile d', '"https://doi.org/10.1007/978-3-319-16181-5_43"', '"SocialSync is a sub-frame synchronization protocol for capturing images simultaneously using a smartphone camera network. By synchronizing image captures to within a frame period, multiple smartphone cameras, which are often in use in social settings, can be used for a variety of applications including light field capture, depth estimation, and free viewpoint television. Currently, smartphone camera networks are limited to capturing static scenes due to motion artifacts caused by frame misalignment. Because frame misalignment in smartphones camera networks is caused by variability in the camera system, we characterize frame capture on mobile devices by analyzing the statistics of camera setup latency and frame delivery within an Android app. Next, we develop the SocialSync protocol to achieve sub-frame synchronization between devices by estimating frame capture timestamps to within millisecond accuracy. Finally, we demonstrate the effectiveness of SocialSync on mobile devices by reducing motion-induced artifacts when recovering the light field."'),
('"Soft Cost Aggregation with Multi-resolution Fusion"', '"ECCV 2014"', '["Multi-resolution fusion", "Cost aggregation", "Stereo matching", "Interactive segmentation"]', '"https://doi.org/10.1007/978-3-319-10602-1_2"', '"This paper presents a simple and effective cost volume aggregation framework for addressing pixels labeling problem. Our idea is based on the observation that incorrect labelings are greatly reduced in cost volume aggregation results from low resolutions. However, image details may be lost in the low resolution results. To take advantage of the results from low resolution for reducing these incorrect labelings while preserving details, we propose a multi-resolution cost aggregation method (MultiAgg) by using a soft fusion scheme based on min-convolution. We implement our MultiAgg in applications on stereo matching and interactive image segmentation. Experimental results show that our method significantly outperforms conventional cost aggregation methods in labeling accuracy. Moreover, although MultiAgg is a simple and straight-forward method, it produces results which are close to or even better than those from iterative methods based on global optimization."'),
('"Soft Inextensibility Constraints for Template-Free Non-rigid Reconstruction"', '"ECCV 2012"', '["Non-rigid reconstruction", "inextensiblility priors", "MRF optimization"]', '"https://doi.org/10.1007/978-3-642-33712-3_31"', '"In this paper, we exploit an inextensibility prior as a way to better constrain the highly ambiguous problem of non-rigid reconstruction from monocular views. While this widely applicable prior has been used before combined with the strong assumption of a known 3D-template, our work achieves template-free reconstruction using only inextensibility constraints. We show how to formulate an energy function that includes soft inextensibility constraints and rely on existing discrete optimisation methods to minimise it. Our method has all of the following advantages: (i) it can be applied to two tasks that have been so far considered independently \\u2013 template based reconstruction and non-rigid structure from motion \\u2013 producing comparable or better results than the state-of-the art methods; (ii) it can perform template-free reconstruction from as few as two images; and (iii) it does not require post-processing stitching or surface smoothing."'),
('"SoftPOSIT: Simultaneous Pose and Correspondence Determination"', '"ECCV 2002"', '["Object recognition", "autonomous navigation", "POSIT", "SoftAssign"]', '"https://doi.org/10.1007/3-540-47977-5_46"', '"The problem of pose estimation arises in many areas of computer vision, including object recognition, object tracking, site inspection and updating, and autonomous navigation using scene models. We present a new algorithm, called SoftPOSIT, for determining the pose of a 3D object from a single 2D image in the case that correspondences between model points and image points are unknown. The algorithm combines Gold\\u2019s iterative SoftAssign algorithm [19, 20] for computing correspondences and DeMenthon\\u2019s iterative POSIT algorithm [13] for computing object pose under a full-perspective camera model. Our algorithm, unlike most previous algorithms for this problem, does not have to hypothesize small sets of matches and then verify the remaining image points. Instead, all possible matches are treated identically throughout the search for an optimal pose. The performance of the algorithm is extensively evaluated in Monte Carlo simulations on synthetic data under a variety of levels of clutter, occlusion, and image noise. These tests show that the algorithm performs well in a variety of difficult scenarios, and empirical evidence suggests that the algorithm has a run-time complexity that is better than previous methods by a factor equal to the number of image points. The algorithm is being applied to the practical problem of autonomous vehicle navigation in a city through registration of a 3D architectural models of buildings to images obtained from an on-board camera."'),
('"Solving Image Registration Problems Using Interior Point Methods"', '"ECCV 2008"', '["Image Registration", "Target Image", "Interior Point Method", "Deformation Model", "Thin Plate Spl', '"https://doi.org/10.1007/978-3-540-88693-8_47"', '"This paper describes a novel approach to recovering a parametric deformation that optimally registers one image to another. The method proceeds by constructing a global convex approximation to the match function which can be optimized using interior point methods. The paper also describes how one can exploit the structure of the resulting optimization problem to develop efficient and effective matching algorithms. Results obtained by applying the proposed scheme to a variety of images are presented."'),
('"Solving Square Jigsaw Puzzles with Loop Constraints"', '"ECCV 2014"', '["Square Jigsaw Puzzles", "Loop Constraints"]', '"https://doi.org/10.1007/978-3-319-10599-4_3"', '"We present a novel algorithm based on \\u201cloop constraints\\u201d for assembling non-overlapping square-piece jigsaw puzzles where the rotation and the position of each piece are unknown. Our algorithm finds small loops of puzzle pieces which form consistent cycles. These small loops are in turn aggregated into higher order \\u201cloops of loops\\u201d in a bottom-up fashion. In contrast to previous puzzle solvers which avoid or ignore puzzle cycles, we specifically seek out and exploit these loops as a form of outlier rejection. Our algorithm significantly outperforms state-of-the-art algorithms in puzzle reconstruction accuracy. For the most challenging type of image puzzles with unknown piece rotation we reduce the reconstruction error by up to 70%. We determine an upper bound on reconstruction accuracy for various data sets and show that, in some cases, our algorithm nearly matches the upper bound."'),
('"Some Faces are More Equal than Others: Hierarchical Organization for Accurate and Efficient Large-S', '"ECCV 2014"', '["Face Recognition", "Leaf Node", "Face Image", "Local Binary Pattern", "Hierarchical Organization"]', '"https://doi.org/10.1007/978-3-319-16181-5_12"', '"This paper presents a novel method for hierarchically organizing large face databases, with application to efficient identity-based face retrieval. The method relies on metric learning with local binary pattern (LBP) features. On one hand, LBP features have proved to be highly resilient to various appearance changes due to illumination and contrast variations while being extremely efficient to calculate. On the other hand, metric learning (ML) approaches have been proved very successful for face verification \\u2018in the wild\\u2019, i.e. in uncontrolled face images with large amounts of variations in pose, expression, appearances, lighting, etc. While such ML based approaches compress high dimensional features into low dimensional spaces using discriminatively learned projections, the complexity of retrieval is still significant for large scale databases (with millions of faces). The present paper shows that learning such discriminative projections locally while organizing the database hierarchically leads to a more accurate and efficient system. The proposed method is validated on the standard Labeled Faces in the Wild (LFW) benchmark dataset with millions of additional distracting face images collected from photos on the internet."'),
('"Some Objects Are More Equal Than Others: Measuring and Predicting Importance"', '"ECCV 2008"', '["Human Observer", "Measured Importance", "Median Order", "Human Annotation", "Spatial Pyramid Match', '"https://doi.org/10.1007/978-3-540-88682-2_40"', '"We observe that everyday images contain dozens of objects, and that humans, in describing these images, give different priority to these objects. We argue that a goal of visual recognition is, therefore, not only to detect and classify objects but also to associate with each a level of priority which we call \\u2018importance\\u2019. We propose a definition of importance and show how this may be estimated reliably from data harvested from human observers. We conclude by showing that a first-order estimate of importance may be computed from a number of simple image region measurements and does not require access to image meaning."'),
('"Something Old, Something New, Something Borrowed, Something Blue"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_1"', '"My first paper of a \\u201cComputer Vision\\u201d signature (on invariants related to optic flow) dates from 1975. I have published in Computer Vision (next to work in cybernetics, psychology, physics, mathematics and philosophy) till my retirement earlier this year (hence the slightly blue feeling), thus my career roughly covers the history of the field. \\u201cVision\\u201d has diverse connotations. The fundamental dichotomy is between \\u201coptically guided action\\u201d and \\u201cvisual experience\\u201d. The former applies to much of biology and computer vision and involves only concepts from science and engineering (e.g., \\u201cinverse optics\\u201d), the latter involves intention and meaning and thus additionally involves concepts from psychology and philosophy. David Marr\\u2019s notion of \\u201cvision\\u201d is an uneasy blend of the two: On the one hand the goal is to create a \\u201crepresentation of the scene in front of the eye\\u201d (involving intention and meaning), on the other hand the means by which this is attempted are essentially \\u201cinverse optics\\u201d. Although this has nominally become something of the \\u201cStandard Model\\u201d of CV, it is actually incoherent. It is the latter notion of \\u201cvision\\u201d that has always interested me most, mainly because one is still grappling with basic concepts. It has been my aspiration to turn it into science, although in this I failed. Yet much has happened (something old) and is happening now (something new). I will discuss some of the issues that seem crucial to me, mostly illustrated through my own work, though I shamelessly borrow from friends in the CV community where I see fit."'),
('"Space-Time Tracking"', '"ECCV 2002"', '["Tracking Algorithm", "Prior Model", "Rank Constraint", "Reliable Point", "Traditional Tracking"]', '"https://doi.org/10.1007/3-540-47969-4_53"', '"We propose a new tracking technique that is able to capture non-rigid motion by exploiting a space-time rank constraint. Most tracking methods use a prior model in order to deal with challenging local features. The model usually has to be trained on carefully hand-labeled example data before the tracking algorithm can be used. Our new model-free tracking technique can overcome such limitations. This can be achieved in redefining the problem. Instead of first training a model and then tracking the model parameters, we are able to derive trajectory constraints first, and then estimate the model. This reduces the search space significantly and allows for a better feature disambiguation that would not be possible with traditional trackers. We demonstrate that sampling in the trajectory space, instead of in the space of shape configurations, allows us to track challenging footage without use of prior models."'),
('"Space-Time-Scale Registration of Dynamic Scene Reconstructions"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_14"', '"The paper presents a method for multi-dimensional registration of two video streams. The sequences are captured by two hand-held cameras moving independently with respect to each other, both observing one object rigidly moving apart from the background. The method is based on uncalibrated Structure-from-Motion (SfM) to extract 3D models for the foreground object and the background, as well as for their relative motion. It fixes the relative scales between the scene parts within and between the videos. It also provides the registration between all partial 3D models, and the temporal synchronization between the videos. The crux is that not a single point on the foreground or background needs to be in common between both video streams. Extensions to more than two cameras and multiple foreground objects are possible."'),
('"Space-Variant Descriptor Sampling for Action Recognition Based on Saliency and Eye Movements"', '"ECCV 2012"', '["action recognition", "saliency maps", "eye movements", "bag of features", "descriptor pruning"]', '"https://doi.org/10.1007/978-3-642-33786-4_7"', '"Algorithms using \\u201cbag of features\\u201d-style video representations currently achieve state-of-the-art performance on action recognition tasks, such as the challenging Hollywood2 benchmark [1,2,3]. These algorithms are based on local spatiotemporal descriptors that can be extracted either sparsely (at interest points) or densely (on regular grids), with dense sampling typically leading to the best performance [1]. Here, we investigate the benefit of space-variant processing of inputs, inspired by attentional mechanisms in the human visual system. We employ saliency-mapping algorithms to find informative regions and descriptors corresponding to these regions are either used exclusively, or are given greater representational weight (additional codebook vectors). This approach is evaluated with three state-of-the-art action recognition algorithms [1,2,3], and using several saliency algorithms. We also use saliency maps derived from human eye movements to probe the limits of the approach. Saliency-based pruning allows up to 70% of descriptors to be discarded, while maintaining high performance on Hollywood2. Meanwhile, pruning of 20-50% (depending on model) can even improve recognition. Further improvements can be obtained by combining representations learned separately on salience-pruned and unpruned descriptor sets. Not surprisingly, using the human eye movement data gives the best mean Average Precision (mAP; 61.9%), providing an upper bound on what is possible with a high-quality saliency map. Even without such external data, the Dense Trajectories model [1] enhanced by automated saliency-based descriptor sampling achieves the best mAP (60.0%) reported on Hollywood2 to date."'),
('"SPADE: Scalar Product Accelerator by Integer Decomposition for Object Detection"', '"ECCV 2014"', '["linear classifier", "binary features", "object detection"]', '"https://doi.org/10.1007/978-3-319-10602-1_18"', '"We propose a method for accelerating computation of an object detector based on a linear classifier when objects are expressed by binary feature vectors. Our key idea is to decompose a real-valued weight vector of the linear classifier into a weighted sum of a few ternary basis vectors so as to preserve the original classification scores. Our data-dependent decomposition algorithm can approximate the original classification scores by a small number of the ternary basis vectors with an allowable error. Instead of using the original real-valued weight vector, the approximated classification score can be obtained by evaluating the few inner products between the binary feature vector and the ternary basis vectors, which can be computed using extremely fast logical operations. We also show that each evaluation of the inner products can be cascaded for incorporating early termination. Our experiments revealed that the linear filtering used in a HOG-based object detector becomes 36.9\\u00d7 faster than the original implementation with 1.5% loss of accuracy for 0.1 false positives per image in pedestrian detection task."'),
('"Sparse Additive Subspace Clustering"', '"ECCV 2014"', '["Cluster Accuracy", "Subspace Cluster", "Sparse Pattern", "Motion Segmentation", "Spectral Cluster ', '"https://doi.org/10.1007/978-3-319-10578-9_42"', '"In this paper, we introduce and investigate a sparse additive model for subspace clustering problems. Our approach, named SASC (Sparse Additive Subspace Clustering), is essentially a functional extension of the Sparse Subspace Clustering (SSC) of Elhamifar & Vidal [7] to the additive nonparametric setting. To make our model computationally tractable, we express SASC in terms of a finite set of basis functions, and thus the formulated model can be estimated via solving a sequence of grouped Lasso optimization problems. We provide theoretical guarantees on the subspace recovery performance of our model. Empirical results on synthetic and real data demonstrate the effectiveness of SASC for clustering noisy data points into their original subspaces."'),
('"Sparse Coding and Dictionary Learning for Symmetric Positive Definite Matrices: A Kernel Approach"', '"ECCV 2012"', '["Riemannian Manifold", "Face Recognition", "Recognition Accuracy", "Sparse Representation", "Sparse', '"https://doi.org/10.1007/978-3-642-33709-3_16"', '"Recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non-Euclidean geometry. This paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices, which form a Riemannian manifold. With the aid of the recently introduced Stein kernel (related to a symmetric version of Bregman matrix divergence), we propose to perform sparse coding by embedding Riemannian manifolds into reproducing kernel Hilbert spaces. This leads to a convex and kernel version of the Lasso problem, which can be solved efficiently. We furthermore propose an algorithm for learning a Riemannian dictionary (used for sparse coding), closely tied to the Stein kernel. Experiments on several classification tasks (face recognition, texture classification, person re-identification) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as tensor sparse coding, Riemannian locality preserving projection, and symmetry-driven accumulation of local features."'),
('"Sparse Dictionaries for Semantic Segmentation"', '"ECCV 2014"', '["discriminative sparse dictionary learning", "conditional random fields", "semantic segmentation"]', '"https://doi.org/10.1007/978-3-319-10602-1_36"', '"A popular trend in semantic segmentation is to use top-down object information to improve bottom-up segmentation. For instance, the classification scores of the Bag of Features (BoF) model for image classification have been used to build a top-down categorization cost in a Conditional Random Field (CRF) model for semantic segmentation. Recent work shows that discriminative sparse dictionary learning (DSDL) can improve upon the unsupervised K-means dictionary learning method used in the BoF model due to the ability of DSDL to capture discriminative features from different classes. However, to the best of our knowledge, DSDL has not been used for building a top-down categorization cost for semantic segmentation. In this paper, we propose a CRF model that incorporates a DSDL based top-down cost for semantic segmentation. We show that the new CRF energy can be minimized using existing efficient discrete optimization techniques. Moreover, we propose a new method for jointly learning the CRF parameters, object classifiers and the visual dictionary. Our experiments demonstrate that by jointly learning these parameters, the feature representation becomes more discriminative and the segmentation performance improves with respect to that of state-of-the-art methods that use unsupervised K-means dictionary learning."'),
('"Sparse Embedding: A Framework for Sparsity Promoting Dimensionality Reduction"', '"ECCV 2012"', '["Dimensionality Reduction", "Sparse Representation", "Sparse Code", "Dictionary Learning", "Sparse ', '"https://doi.org/10.1007/978-3-642-33783-3_30"', '"We introduce a novel framework, called sparse embedding (SE), for simultaneous dimensionality reduction and dictionary learning. We formulate an optimization problem for learning a transformation from the original signal domain to a lower-dimensional one in a way that preserves the sparse structure of data. We propose an efficient optimization algorithm and present its non-linear extension based on the kernel methods. One of the key features of our method is that it is computationally efficient as the learning is done in the lower-dimensional space and it discards the irrelevant part of the signal that derails the dictionary learning process. Various experiments show that our method is able to capture the meaningful structure of data and can perform significantly better than many competitive algorithms on signal recovery and object classification tasks."'),
('"Sparse Finite Elements for Geodesic Contours with Level-Sets"', '"ECCV 2004"', '["Simplicial Complex", "Sparse Representation", "Active Contour", "Active Contour Model", "Geodesic ', '"https://doi.org/10.1007/978-3-540-24671-8_31"', '"Level-set methods have been shown to be an effective way to solve optimisation problems that involve closed curves. They are well known for their capacity to deal with flexible topology and do not require manual initialisation. Computational complexity has previously been addressed by using banded algorithms which restrict computation to the vicinity of the zero set of the level-set function. So far, such schemes have used finite difference representations which suffer from limited accuracy and require re-initialisation procedures to stabilise the evolution. This paper shows how banded computation can be achieved using finite elements. We give details of the novel representation and show how to build the signed distance constraint into the presented numerical scheme. We apply the algorithm to the geodesic contour problem (including the automatic detection of nested contours) and demonstrate its performance on a variety of images. The resulting algorithm has several advantages which are demonstrated in the paper: it is inherently stable and avoids re-initialisation; it is convergent and more accurate because of the capabilities of finite elements; it achieves maximum sparsity because with finite elements the band can be effectively of width 1."'),
('"Sparse Flexible Models of Local Features"', '"ECCV 2006"', '["Local Feature", "Test Image", "Model Feature", "Equal Error Rate", "Pairwise Variance"]', '"https://doi.org/10.1007/11744078_3"', '"In recent years there has been growing interest in recognition models using local image features for applications ranging from long range motion matching to object class recognition systems. Currently, many state-of-the-art approaches have models involving very restrictive priors in terms of the number of local features and their spatial relations. The adoption of such priors in those models are necessary for simplifying both the learning and inference tasks. Also, most of the state-of-the-art learning approaches are semi-supervised batch processes, which considerably reduce their suitability in dynamic environments, where unannotated new images are continuously presented to the learning system. In this work we propose: 1) a new model representation that has a less restrictive prior on the geometry and number of local features, where the geometry of each local feature is influenced by its k closest neighbors and models may contain hundreds of features; and 2) a novel unsupervised on-line learning algorithm that is capable of estimating the model parameters efficiently and accurately. We implement a visual class recognition system using the new model and learning method proposed here, and demonstrate that our system produces competitive classification and localization results compared to state-of-the-art methods. Moreover, we show that the learning algorithm is able to model not only classes with consistent texture (e.g., faces), but also classes with shape only (e.g., leaves), classes with a common shape but with a great variability in terms of internal texture (e.g., cups), and classes of flexible objects (e.g., snake)."'),
('"Sparse Long-Range Random Field and Its Application to Image Denoising"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88690-7_26"', '"Many recent techniques for low-level vision problems such as image denoising are formulated in terms of Markov random field (MRF) or conditional random field (CRF) models. Nonetheless, the role of the underlying graph structure is still not well understood. On the one hand there are pairwise structures where each node is connected to its local neighbors. These models tend to allow for fast algorithms but do not capture important higher-order statistics of natural scenes. On the other hand there are more powerful models such as Field of Experts (FoE) that consider joint distributions over larger cliques in order to capture image statistics but are computationally challenging. In this paper we consider a graph structure with longer range connections that is designed to both capture important image statistics and be computationally efficient. This structure incorporates long-range connections in a manner that limits the cliques to size 3, thereby capturing important second-order image statistics while still allowing efficient optimization due to the small clique size. We evaluate our approach by testing the models on widely used datasets. The results show that our method is comparable to the current state-of-the-art in terms of PSNR, is better at preserving fine-scale detail and producing natural-looking output, and is more than an order of magnitude faster."'),
('"Sparse Non-linear Least Squares Optimization for Geometric Vision"', '"ECCV 2010"', '["Bundle Adjustment", "Sparsity Pattern", "Trifocal Tensor", "Multiple View Geometry", "Camera Matri', '"https://doi.org/10.1007/978-3-642-15552-9_4"', '"Several estimation problems in vision involve the minimization of cumulative geometric error using non-linear least-squares fitting. Typically, this error is characterized by the lack of interdependence among certain subgroups of the parameters to be estimated, which leads to minimization problems possessing a sparse structure. Taking advantage of this sparseness during minimization is known to achieve enormous computational savings. Nevertheless, since the underlying sparsity pattern is problem-dependent, its exploitation for a particular estimation problem requires non-trivial implementation effort, which often discourages its pursuance in practice. Based on recent developments in sparse linear solvers, this paper provides an overview of sparseLM, a general-purpose software package for sparse non-linear least squares that can exhibit arbitrary sparseness and presents results from its application to important sparse estimation problems in geometric vision."'),
('"Sparse Representations and Distance Learning for Attribute Based Category Recognition"', '"ECCV 2010"', '["Attribute Based Object Recognition", "Sparse Representations Classification", "Distance Metric Lea', '"https://doi.org/10.1007/978-3-642-35749-7_3"', '"While traditional approaches in object recognition require the specification of training examples from each class and the application of class specific classifiers, in real world situations, the immensity of the number of image classes makes this task daunting. A novel approach in object recognition is attribute based classification, where instead of training classifiers for the recognition of specific object class instances, classifiers are trained on attributes of the object images and these attributes are subsequently used for the object recognition. The attributes based paradigm offers significant advantages including the ability to train classifiers without any visual examples. We begin by discussing a scenario for object recognition on mobile devices where the attribute prediction and the attribute-to-class mapping are decoupled in order to meet the specific resource constraints of mobile systems. We next present two extensions on the attribute based classification paradigm by introducing alternative approaches in attribute prediction and attribute-to-class mapping. For the attribute prediction, we employ the recently proposed Sparse Representations Classification scheme that offers significant benefits compared to the previous SVM based approaches, such as increased accuracy and elimination of the training stage. For the attribute-to-class mapping, we employ a Distance Metric Learning algorithm that automatically infers the significance of each attribute instead of assuming uniform attribute importance. The benefits of the proposed extensions are validated through experimental results."'),
('"Sparse Spatio-spectral Representation for Hyperspectral Image Super-resolution"', '"ECCV 2014"', '["Hyperspectral", "super-resolution", "spatio-spectral", "sparse representation"]', '"https://doi.org/10.1007/978-3-319-10584-0_5"', '"Existing hyperspectral imaging systems produce low spatial resolution images due to hardware constraints. We propose a sparse representation based approach for hyperspectral image super-resolution. The proposed approach first extracts distinct reflectance spectra of the scene from the available hyperspectral image. Then, the signal sparsity, non-negativity and the spatial structure in the scene are exploited to explain a high-spatial but low-spectral resolution image of the same scene in terms of the extracted spectra. This is done by learning a sparse code with an algorithm G-SOMP+. Finally, the learned sparse code is used with the extracted scene spectra to estimate the super-resolution hyperspectral image. Comparison of the proposed approach with the state-of-the-art methods on both ground-based and remotely-sensed public hyperspectral image databases shows that the presented method achieves the lowest error rate on all test images in the three datasets."'),
('"Sparse Structures in L-Infinity Norm Minimization for Structure and Motion Reconstruction"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88682-2_59"', '"This paper presents a study on how to numerically solve the feasibility test problem which is the core of the bisection algorithm for minimizing the L \\u2009\\u221e\\u2009 error functions. We consider a strategy that minimizes the maximum infeasibility. The minimization can be performed using several numerical computation methods, among which the barrier method and the primal-dual method are examined. In both of the methods, the inequalities are sequentially approximated by log-barrier functions. An initial feasible solution is found easily by the construction of the feasibility problem, and Newton-style update computes the optimal solution iteratively. When we apply the methods to the problem of estimating the structure and motion, every Newton update requires solving a very large system of linear equations. We show that the sparse bundle-adjustment technique, previously developed for structure and motion estimation, can be utilized during the Newton update. In the primal-dual interior-point method, in contrast to the barrier method, the sparse structure is all destroyed due to an extra constraint introduced for finding an initial solution. However, we show that this problem can be overcome by utilizing the matrix inversion lemma which allows us to exploit the sparsity in the same manner as in the barrier method. We finally show that the sparsity appears in both of the L \\u2009\\u221e\\u2009 formulations - linear programming and second-order cone programming."'),
('"Sparselet Models for Efficient Multiclass Object Detection"', '"ECCV 2012"', '["Sparse Coding", "Object Detection", "Deformable Part Models"]', '"https://doi.org/10.1007/978-3-642-33709-3_57"', '"We develop an intermediate representation for deformable part models and show that this representation has favorable performance characteristics for multi-class problems when the number of classes is high. Our model uses sparse coding of part filters to represent each filter as a sparse linear combination of shared dictionary elements. This leads to a universal set of parts that are shared among all object classes. Reconstruction of the original part filter responses via sparse matrix-vector product reduces computation relative to conventional part filter convolutions. Our model is well suited to a parallel implementation, and we report a new GPU DPM implementation that takes advantage of sparse coding of part filters. The speed-up offered by our intermediate representation and parallel computation enable real-time DPM detection of 20 different object classes on a laptop computer."'),
('"Spatial and Angular Variational Super-Resolution of 4D Light Fields"', '"ECCV 2012"', '["Ground Truth", "Center View", "View Synthesis", "Scene Geometry", "Intermediate View"]', '"https://doi.org/10.1007/978-3-642-33715-4_44"', '"We present a variational framework to generate super-resolved novel views from 4D light field data sampled at low resolution, for example by a plenoptic camera. In contrast to previous work, we formulate the problem of view synthesis as a continuous inverse problem, which allows us to correctly take into account foreshortening effects caused by scene geometry transformations. High-accuracy depth maps for the input views are locally estimated using epipolar plane image analysis, which yields floating point depth precision without the need for expensive matching cost minimization. The disparity maps are further improved by increasing angular resolution with synthesized intermediate views. Minimization of the super-resolution model energy is performed with state of the art convex optimization algorithms within seconds."'),
('"Spatial Intensity Correction of Fluorescent Confocal Laser Scanning Microscope Images"', '"CVAMIA 2006"', '["Histogram Equalization", "Intensity Correction", "Kernel Size", "Speckle Noise", "Confocal Laser S', '"https://doi.org/10.1007/11889762_13"', '"Fluorescent confocal laser scanning microscope (CLSM) imaging has become popular in medical domain for the purpose of 3D information extraction. 3D information is extracted either by visual inspection or by automated techniques. Nonetheless, 3D information extraction from CLSM suffers from significant lateral intensity heterogeneity. We propose a novel lateral intensity heterogeneity correction technique to improve accurate image analysis, e.g., quantitative analysis, segmentation, or visualization. The proposed technique is novel in terms of its design (spatially adaptive mean-weight filtering) and application (CLSM), as well as its properties and full automation. The key properties of the intensity correction techniques include adjustment of intensity heterogeneity, preservation of fine structural details, and enhancement of image contrast. The full automation is achieved by data-driven parameter optimization and introduction of several evaluation metrics. We evaluated the performance by comparing with three other techniques, four quality metrics, and two realistic synthetic images and one real CLSM image."'),
('"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition"', '"ECCV 2014"', '["Object Detection", "Spatial Pyramid", "Convolutional Layer", "Spatial Pyramid Match", "Deep Networ', '"https://doi.org/10.1007/978-3-319-10578-9_23"', '"Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224\\u00d7224) input image. This requirement is \\u201cartificial\\u201d and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, \\u201cspatial pyramid pooling\\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101."'),
('"Spatial Segmentation of Temporal Texture Using Mixture Linear Models"', '"WDV 2006"', '["Video Sequence", "Dynamic Texture", "Spatial Segmentation", "Motion Texture", "Mixture Linear Mode', '"https://doi.org/10.1007/978-3-540-70932-9_11"', '"In this paper we propose a novel approach for the spatial segmentation of video sequences containing multiple temporal textures. This work is based on the notion that a single temporal texture can be represented by a low-dimensional linear model. For scenes containing multiple temporal textures, e.g. trees swaying adjacent a flowing river, we extend the single linear model to a mixture of linear models and segment the scene by identifying subspaces within the data using robust generalized principal component analysis (GPCA). Computation is reduced to minutes in Matlab by first identifying models from a sampling of the sequence and using the derived models to segment the remaining data. The effectiveness of our method has been demonstrated in several examples including an application in biomedical image analysis."'),
('"Spatial Statistics of Visual Keypoints for Texture Recognition"', '"ECCV 2010"', '["Point Process", "Visual Word", "Training Image", "Texture Image", "Spatial Statistic"]', '"https://doi.org/10.1007/978-3-642-15561-1_55"', '"In this paper, we propose a new descriptor of texture images based on the characterization of the spatial patterns of image keypoints. Regarding the set of visual keypoints of a given texture sample as the realization of marked point process, we define texture features from multivariate spatial statistics. Our approach initially relies on the construction of a codebook of the visual signatures of the keypoints. Here these visual signatures are given by SIFT feature vectors and the codebooks are issued from a hierarchical clustering algorithm suitable for processing large high-dimensional dataset. The texture descriptor is formed by cooccurrence statistics of neighboring keypoint pairs for different neighborhood radii. The proposed descriptor inherits the invariance properties of the SIFT w.r.t. contrast change and geometric image transformation (rotation, scaling). An application to texture recognition using the discriminative classifiers, namely: k-NN, SVM and random forest, is considered and a quantitative evaluation is reported for two case-studies: UIUC texture database and real sonar textures. The proposed approach favourably compares to previous work. We further discuss the properties of the proposed descriptor, including dimensionality aspects."'),
('"Spatial-Temporal Granularity-Tunable Gradients Partition (STGGP) Descriptors for Human Detection"', '"ECCV 2010"', '["Object Detection", "Generalize Plane", "Motion Information", "Space Partition", "Human Detection"]', '"https://doi.org/10.1007/978-3-642-15549-9_24"', '"This paper presents a novel descriptor for human detection in video sequence. It is referred to as spatial-temporal granularity -tunable gradients partition (STGGP), which is an extension of granularity-tunable gradients partition (GGP) from the still image domain to the spatial-temporal domain. Specifically, the moving human body is considered as a 3-dimensional entity in the spatial-temporal domain. Then in 3D Hough space, we define the generalized plane as a primitive to parse the structure of this 3D entity. The advantage of the generalized plane is that it can tolerate imperfect planes with certain level of uncertainty in rotation and translation. The robustness to the uncertainty is controlled quantitatively by the granularity parameters defined explicitly in the generalized plane. This property endows the STGGP descriptors versatile ability to represent both the deterministic structures and the statistical summarizations of the object. Moreover, the STGGP descriptor encodes much heterogeneous information such as the gradients\\u2019 strength, position, and distribution, as well as their temporal motion to enrich its representation ability. We evaluate the STGGP on human detection in sequence on the public datasets and very promising results have been achieved."'),
('"SpatialBoost: Adding Spatial Reasoning to AdaBoost"', '"ECCV 2006"', '["Feature Vector", "Image Segmentation", "Gesture Recognition", "Neural Information Processing Syste', '"https://doi.org/10.1007/11744085_30"', '"SpatialBoost extends AdaBoost to incorporate spatial reasoning. We demonstrate the effectiveness of SpatialBoost on the problem of interactive image segmentation. Our application takes as input a tri-map of the original image, trains SpatialBoost on the pixels of the object and the background and use the trained classifier to classify the unlabeled pixels. The spatial reasoning is introduced in the form of weak classifiers that attempt to infer pixel label from the pixel labels of surrounding pixels, after each boosting iteration. We call this variant of AdaBoost \\u2014 SpatialBoost. We then extend the application to work with \\u201cGrabCut\\u201d. In GrabCut the user casually marks a rectangle around the object, instead of tediously marking a tri-map, and we pose the segmentation as the problem of learning with outliers, where we know that only positive pixels (i.e. pixels that are assumed to belong to the object) might be outliers and in fact should belong to the background."'),
('"Spatially Homogeneous Dynamic Textures"', '"ECCV 2004"', '["Video Sequence", "Singular Value Decomposition", "Texture Synthesis", "Dynamic Texture", "Temporal', '"https://doi.org/10.1007/978-3-540-24671-8_47"', '"We address the problem of modeling the spatial and temporal second-order statistics of video sequences that exhibit both spatial and temporal regularity, intended in a statistical sense. We model such sequences as dynamic multiscale autoregressive models, and introduce an efficient algorithm to learn the model parameters. We then show how the model can be used to synthesize novel sequences that extend the original ones in both space and time, and illustrate the power, and limitations, of the models we propose with a number of real image sequences."'),
('"Spatially-Sensitive Affine-Invariant Image Descriptors"', '"ECCV 2010"', '["Image Retrieval", "Visual Word", "Spatial Relation", "Retrieval Performance", "Image Descriptor"]', '"https://doi.org/10.1007/978-3-642-15552-9_15"', '"Invariant image descriptors play an important role in many computer vision and pattern recognition problems such as image search and retrieval. A dominant paradigm today is that of \\u201cbags of features\\u201d, a representation of images as distributions of primitive visual elements. The main disadvantage of this approach is the loss of spatial relations between features, which often carry important information about the image. In this paper, we show how to construct spatially-sensitive image descriptors in which both the features and their relation are affine-invariant. Our construction is based on a vocabulary of pairs of features coupled with a vocabulary of invariant spatial relations between the features. Experimental results show the advantage of our approach in image retrieval applications."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Spatio-chromatic Opponent Features"', '"ECCV 2014"', '["Colour descriptors", "image categorization", "colour-opponency", "biologically-inspired", "pooling', '"https://doi.org/10.1007/978-3-319-10602-1_6"', '"This work proposes colour opponent features that are based on low-level models of mammalian colour visual processing. A key step is the construction of opponent spatio-chromatic feature maps by filtering colour planes with Gaussians of unequal spreads. Weighted combination of these planes yields a spatial center-surround effect across chromatic channels. The resulting feature spaces \\u2013 substantially different to CIELAB and other colour-opponent spaces obtained by colour-plane differencing \\u2013 are further processed to assign local spatial orientations. The nature of the initial spatio-chromatic processing requires a customised approach to generating gradient-like fields, which is also described. The resulting direction-encoding responses are then pooled to form compact descriptors. The individual performance of the new descriptors was found to be substantially higher than those arising from spatial processing of standard opponent colour spaces, and these are the first chromatic descriptors that appear to achieve such performance levels individually. For all stages, parametrisations are suggested that allow successful optimisation using categorization performance as an objective. Classification benchmarks on Pascal VOC 2007 and Bird-200-2011 are presented to show the merits of these new features."'),
('"Spatio-temporal Embedding for Statistical Face Recognition from Video"', '"ECCV 2006"', '["Face Recognition", "Video Sequence", "Audio Signal", "Frame Synchronization", "Dimensionality Redu', '"https://doi.org/10.1007/11744047_29"', '"This paper addresses the problem of how to learn an appropriate feature representation from video to benefit video-based face recognition. By simultaneously exploiting the spatial and temporal information, the problem is posed as learning Spatio-Temporal Embedding (STE) from raw video. STE of a video sequence is defined as its condensed version capturing the essence of space-time characteristics of the video. Relying on the co-occurrence statistics and supervised signatures provided by training videos, STE preserves the intrinsic temporal structures hidden in video volume, meanwhile encodes the discriminative cues into the spatial domain. To conduct STE, we propose two novel techniques, Bayesian keyframe learning and nonparametric discriminant embedding (NDE), for temporal and spatial learning, respectively. In terms of learned STEs, we derive a statistical formulation to the recognition problem with a probabilistic fusion model. On a large face video database containing more than 200 training and testing sequences, our approach consistently outperforms state-of-the-art methods, achieving a perfect recognition accuracy."'),
('"Spatio-temporal Event Classification Using Time-Series Kernel Based Structured Sparsity"', '"ECCV 2014"', '["structured sparsity", "time-series kernels", "facial expression classification", "gesture recognit', '"https://doi.org/10.1007/978-3-319-10593-2_10"', '"In many behavioral domains, such as facial expression and gesture, sparse structure is prevalent. This sparsity would be well suited for event detection but for one problem. Features typically are confounded by alignment error in space and time. As a consequence, high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information. We propose a Kernel Structured Sparsity (KSS) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework, and it can rely on simple features. We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem. We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding. KSS outperformed both sparse and non-sparse methods that utilize complex image features and their temporal extensions. In the case of early facial event classification KSS had 10% higher accuracy as measured by F 1 score over kernel SVM methods."'),
('"Spatio-temporal Matching for Human Detection in Video"', '"ECCV 2014"', '["Discrete Cosine Transform", "Motion Capture", "Video Segment", "Camera View", "Bilinear Model"]', '"https://doi.org/10.1007/978-3-319-10599-4_5"', '"Detection and tracking humans in videos have been long-standing problems in computer vision. Most successful approaches (e.g., deformable parts models) heavily rely on discriminative models to build appearance detectors for body joints and generative models to constrain possible body configurations (e.g., trees). While these 2D models have been successfully applied to images (and with less success to videos), a major challenge is to generalize these models to cope with camera views. In order to achieve view-invariance, these 2D models typically require a large amount of training data across views that is difficult to gather and time-consuming to label. Unlike existing 2D models, this paper formulates the problem of human detection in videos as spatio-temporal matching (STM) between a 3D motion capture model and trajectories in videos. Our algorithm estimates the camera view and selects a subset of tracked trajectories that matches the motion of the 3D model. The STM is efficiently solved with linear programming, and it is robust to tracking mismatches, occlusions and outliers. To the best of our knowledge this is the first paper that solves the correspondence between video and 3D motion capture data for human pose detection. Experiments on the Human3.6M and Berkeley MHAD databases illustrate the benefits of our method over state-of-the-art approaches."'),
('"Spatio-Temporal Multifeature for Facial Analysis"', '"ECCV 2012"', '["Facial Expression", "Face Recognition", "Gesture Recognition", "Facial Expression Recognition", "F', '"https://doi.org/10.1007/978-3-642-33868-7_20"', '"Human faces are 3D complex objects consisting of geometrical and appearance variations. They exhibit local and global variations when observed over time. In our daily life communication, human faces are seen in actions conveying a set of information during interaction. Cognitive science explains that human brains are capable of extracting this set of information very efficiently resulting in a better interaction with others. Our goal is to extract a single feature set which represents multiple facial characteristics. This problem is addressed by the analysis of different feature components on facial classifications using a 3D surface model. We propose a unified framework which is capable to extract multiple information from the human faces and at the same time robust against rigid and non-rigid facial deformations. A single feature vector corresponding to a given image is representative of person\\u2019s identity, facial expressions, gender and age estimation. This feature set is called spatio-temporal multifeature (STMF) extracted from image sequences. An STMF is configured with three different feature components which is tested thoroughly to evidence its validity. The experimental results from four different databases show that this feature set provides high accuracy and at the same time exhibits robustness. The results have been discussed comparatively with different approaches."'),
('"Spatio-temporal Object Detection Proposals"', '"ECCV 2014"', '["Fisher Vector", "Virtual Edge", "Proposal Generation", "Expensive Feature", "Video Object Segmenta', '"https://doi.org/10.1007/978-3-319-10578-9_48"', '"Spatio-temporal detection of actions and events in video is a challenging problem. Besides the difficulties related to recognition, a major challenge for detection in video is the size of the search space defined by spatio-temporal tubes formed by sequences of bounding boxes along the frames. Recently methods that generate unsupervised detection proposals have proven to be very effective for object detection in still images. These methods open the possibility to use strong but computationally expensive features since only a relatively small number of detection hypotheses need to be assessed. In this paper we make two contributions towards exploiting detection proposals for spatio-temporal detection problems. First, we extend a recent 2D object proposal method, to produce spatio-temporal proposals by a randomized supervoxel merging process. We introduce spatial, temporal, and spatio-temporal pairwise supervoxel features that are used to guide the merging process. Second, we propose a new efficient supervoxel method. We experimentally evaluate our detection proposals, in combination with our new supervoxel method as well as existing ones. This evaluation shows that our supervoxels lead to more accurate proposals when compared to using existing state-of-the-art supervoxel methods."'),
('"Spatio-Temporal Phrases for Activity Recognition"', '"ECCV 2012"', '["Activity Recognition", "Spatio-Temporal Phrases"]', '"https://doi.org/10.1007/978-3-642-33712-3_51"', '"The local feature based approaches have become popular for activity recognition. A local feature captures the local movement and appearance of a local region in a video, and thus can be ambiguous; e.g., it cannot tell whether a movement is from a person\\u2019s hand or foot, when the camera is far away from the person. To better distinguish different types of activities, people have proposed using the combination of local features to encode the relationships of local movements. Due to the computation limit, previous work only creates a combination from neighboring features in space and/or time. In this paper, we propose an approach that efficiently identifies both local and long-range motion interactions; taking the \\u201cpush\\u201d activity as an example, our approach can capture the combination of the hand movement of one person and the foot response of another person, the local features of which are both spatially and temporally far away from each other. Our computational complexity is in linear time to the number of local features in a video. The extensive experiments show that our approach is generically effective for recognizing a wide variety of activities and activities spanning a long term, compared to a number of state-of-the-art methods."'),
('"Spatio-temporal SIFT and Its Application to Human Action Classification"', '"ECCV 2012"', '["Action Recognition", "Interest Point", "Scale Invariant Feature Transform", "Human Action Recognit', '"https://doi.org/10.1007/978-3-642-33863-2_30"', '"This paper presents a space-time extension of scale-invariant feature transform (SIFT) originally applied to the 2-dimensional (2D) volumetric images. Most of the previous extensions dealt with 3-dimensional (3D) spacial information using a combination of a 2D detector and a 3D descriptor for applications such as medical image analysis. In this work we build a spatio-temporal difference-of-Gaussian (DoG) pyramid to detect the local extrema, aiming at processing video streams. Interest points are extracted not only from the spatial plane (xy) but also from the planes along the time axis (xt and yt). The space-time extension was evaluated using the human action classification task. Experiments with the KTH and the UCF sports datasets show that the approach was able to produce results comparable to the state-of-the-arts."'),
('"Spatio-temporal Video Representation with Locality-Constrained Linear Coding"', '"ECCV 2012"', '["Action Recognition", "Interest Point", "Sparse Code", "Human Action Recognition", "Gaussian Pyrami', '"https://doi.org/10.1007/978-3-642-33885-4_11"', '"This paper presents a spatio-temporal coding technique for a video sequence. The framework is based on a space-time extension of scale-invariant feature transform (SIFT) combined with locality-constrained linear coding (LLC). The coding scheme projects each spatio-temporal descriptor into a local coordinate representation produced by max pooling. The extension is evaluated using human action classification tasks. Experiments with the KTH, Weizmann, UCF sports and Hollywood datasets indicate that the approach is able to produce results comparable to the state-of-the-art."'),
('"Spatiotemporal Background Subtraction Using Minimum Spanning Tree and Optical Flow"', '"ECCV 2014"', '["Background Modeling", "Video Segmentation", "Tracking", "Optical Flow"]', '"https://doi.org/10.1007/978-3-319-10584-0_34"', '"Background modeling and subtraction is a fundamental research topic in computer vision. Pixel-level background model uses a Gaussian mixture model (GMM) or kernel density estimation to represent the distribution of each pixel value. Each pixel will be process independently and thus is very efficient. However, it is not robust to noise due to sudden illumination changes. Region-based background model uses local texture information around a pixel to suppress the noise but is vulnerable to periodic changes of pixel values and is relatively slow. A straightforward combination of the two cannot maintain the advantages of the two. This paper proposes a real-time integration based on robust estimator. Recent efficient minimum spanning tree based aggregation technique is used to enable robust estimators like M-smoother to run in real time and effectively suppress the noisy background estimates obtained from Gaussian mixture models. The refined background estimates are then used to update the Gaussian mixture models at each pixel location. Additionally, optical flow estimation can be used to track the foreground pixels and integrated with a temporal M-smoother to ensure temporally-consistent background subtraction. The experimental results are evaluated on both synthetic and real-world benchmarks, showing that our algorithm is the top performer."'),
('"Spatiotemporal Descriptor for Wide-Baseline Stereo Reconstruction of Non-rigid and Ambiguous Scenes', '"ECCV 2012"', '["stereo", "spatiotemporal", "appearance descriptors"]', '"https://doi.org/10.1007/978-3-642-33712-3_32"', '"This paper studies the use of temporal consistency to match appearance descriptors and handle complex ambiguities when computing dynamic depth maps from stereo. Previous attempts have designed 3D descriptors over the spacetime volume and have been mostly used for monocular action recognition, as they cannot deal with perspective changes. Our approach is based on a state-of-the-art 2D dense appearance descriptor which we extend in time by means of optical flow priors, and can be applied to wide-baseline stereo setups. The basic idea behind our approach is to capture the changes around a feature point in time instead of trying to describe the spatiotemporal volume. We demonstrate its effectiveness on very ambiguous synthetic video sequences with ground truth data, as well as real sequences."'),
('"Spatiotemporal Features for Effective Facial Expression Recognition"', '"ECCV 2010"', '["Facial expression analysis", "spatiotemporal features", "face prism"]', '"https://doi.org/10.1007/978-3-642-35749-7_16"', '"We consider two novel representations and feature extraction schemes for automatic recognition of emotion related facial expressions. In one scheme facial landmark points are tracked over successive video frames using an effective detector and tracker to extract landmark trajectories. Features are extracted from landmark trajectories using Independent Component Analysis (ICA) method. In the alternative scheme, the evolution of the emotion expression on the face is captured by stacking normalized and aligned faces into a spatiotemporal face cube. Emotion descriptors are then 3D Discrete Cosine Transform (DCT) features from this prism or DCT & ICA features. Several classifier configurations are used and their performance determined in detecting the 6 basic emotions. Decision fusion applied to classifiers improved the recognition performance of best classifier by 9 percentage points. The proposed method was evaluated user independently on the Cohn-Kanade facial expression database and a state-of-the-art 95.34 % recognition performance is achieved."'),
('"Spectra Estimation of Fluorescent and Reflective Scenes by Using Ordinary Illuminants"', '"ECCV 2014"', '["Fluorescence", "reflectance", "hyperspectral imaging"]', '"https://doi.org/10.1007/978-3-319-10602-1_13"', '"The spectrum behavior of a typical fluorescent object is regulated by its reflectance, absorption and emission spectra. It was shown that two high-frequency and complementary illuminations in the spectral domain can be used to simultaneously estimate reflectance and emission spectra. In spite of its accuracy, such specialized illuminations are not easily accessible. This motivates us to explore the feasibility of using ordinary illuminants to achieve this task with comparable accuracy. We show that three hyperspectral images under wideband and independent illuminants are both necessary and sufficient, and successfully develop a convex optimization method for solving. We also disclose the reason why using one or two images is inadequate, although embedding the linear low-dimensional models of reflectance and emission would lead to an apparently overconstrained equation system. In addition, we propose a novel four-parameter model to express absorption and emission spectra, which is more compact and discriminative than the linear model. Based on this model, we present an absorption spectra estimation method in the presence of three illuminations. The correctness and accuracy of our proposed model and methods have been verified."'),
('"Spectral Clustering for Robust Motion Segmentation"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_32"', '"In this paper, we propose a robust motion segmentation method using the techniques of matrix factorization and subspace separation. We first show that the shape interaction matrix can be derived using QR decomposition rather than Singular Value Decomposition(SVD) which also leads to a simple proof of the shape subspace separation theorem. Using the shape interaction matrix, we solve the motion segmentation problems by the spectral clustering techniques. We exploit multi-way Min-Max cut clustering method and provide a novel approach for cluster membership assignment. We further show that we can combine a cluster refinement method based on subspace separation with the graph clustering method to improve its robustness in the presence of noise. The proposed method yields very good performance for both synthetic and real image sequences."'),
('"Spectral Clustering with a Convex Regularizer on Millions of Images"', '"ECCV 2014"', '["Spectral Cluster", "Latent Semantic Analysis", "Normalize Mutual Information", "Nonnegative Matrix', '"https://doi.org/10.1007/978-3-319-10578-9_19"', '"This paper focuses on efficient algorithms for single and multi-view spectral clustering with a convex regularization term for very large scale image datasets. In computer vision applications, multiple views denote distinct image-derived feature representations that inform the clustering. Separately, the regularization encodes high level advice such as tags or user interaction in identifying similar objects across examples. Depending on the specific task, schemes to exploit such information may lead to a smooth or non-smooth regularization function. We present stochastic gradient descent methods for optimizing spectral clustering objectives with such convex regularizers for datasets with up to a hundred million examples. We prove that under mild conditions the local convergence rate is \\\\(O(1/\\\\sqrt{T})\\\\) where T is the number of iterations; further, our analysis shows that the convergence improves linearly by increasing the number of threads. We give extensive experimental results on a range of vision datasets demonstrating the algorithm\\u2019s empirical behavior."'),
('"Spectral Demons \\u2013 Image Registration via Global Spectral Correspondence"', '"ECCV 2012"', '["Large Deformation", "Image Registration", "Shape Retrieval", "Global Scope", "Transformation Error', '"https://doi.org/10.1007/978-3-642-33709-3_3"', '"Image registration is a building block for many applications in computer vision and medical imaging. However the current methods are limited when large and highly non-local deformations are present. In this paper, we introduce a new direct feature matching technique for non-parametric image registration where efficient nearest-neighbor searches find global correspondences between intensity, spatial and geometric information. We exploit graph spectral representations that are invariant to isometry under complex deformations. Our direct feature matching technique is used within the established Demons framework for diffeomorphic image registration. Our method, called Spectral Demons, can capture very large, complex and highly non-local deformations between images. We evaluate the improvements of our method on 2D and 3D images and demonstrate substantial improvement over the conventional Demons algorithm for large deformations."'),
('"Spectral Edge Image Fusion: Theory and Applications"', '"ECCV 2014"', '["Image fusion", "gradient-based", "contrast", "dimensional reduction", "colour", "colour display"]', '"https://doi.org/10.1007/978-3-319-10602-1_5"', '"This paper describes a novel approach to the fusion of multidimensional images for colour displays. The goal of the method is to generate an output image whose gradient matches that of the input as closely as possible. It achieves this using a constrained contrast mapping paradigm in the gradient domain, where the structure tensor of a high-dimensional gradient representation is mapped exactly to that of a low-dimensional gradient field which is subsequently reintegrated to generate an output. Constraints on the output colours are provided by an initial RGB rendering to produce \\u2018naturalistic\\u2019 colours: we provide a theorem for projecting higher-D contrast onto the initial colour gradients such that they remain close to the original gradients whilst maintaining exact high-D contrast. The solution to this constrained optimisation is closed-form, allowing for a very simple and hence fast and efficient algorithm. Our approach is generic in that it can map any N-D image data to any M-D output, and can be used in a variety of applications using the same basic algorithm. In this paper we focus on the problem of mapping N-D inputs to 3-D colour outputs. We present results in three applications: hyperspectral remote sensing, fusion of colour and near-infrared images, and colour visualisation of MRI Diffusion-Tensor imaging."'),
('"Spectral Partitioning with Indefinite Kernels Using the Nystr\\u00f6m Extension"', '"ECCV 2002"', '["Image Segmentation", "Gaussian Mixture Model", "Hadamard Product", "Spectral Graph", "Weighted Adj', '"https://doi.org/10.1007/3-540-47977-5_35"', '"Fowlkes et al. [7] recently introduced an approximation to the Normalized Cut (NCut) grouping algorithm [18] based on random subsampling and the Nystr\\u00f6m extension. As presented, their method is restricted to the case where W, the weighted adjacency matrix, is positive definite. Although many common measures of image similarity (i.e. kernels) are positive definite, a popular example being Gaussian-weighted distance, there are important cases that are not. In this work, we present a modification to Nystr\\u00f6m-NCut that does not require W to be positive definite. The modification only affects the orthogonalization step, and in doing so it necessitates one additional O(m 3) operation, where m is the number of random samples used in the approximation. As such it is of interest to know which kernels are positive definite and which are indefinite. In addressing this issue, we further develop connections between NCut and related methods in the kernel machines literature. We provide a proof that the Gaussian-weighted chi-squared kernel is positive definite, which has thus far only been conjectured. We also explore the performance of the approximation algorithm on a variety of grouping cues including contour, color and texture."'),
('"Spectral Simplification of Graphs"', '"ECCV 2004"', '["Edit Distance", "Original Graph", "Laplacian Matrix", "Laplace Eigenvalue", "Graph Edit Distance"]', '"https://doi.org/10.1007/978-3-540-24673-2_10"', '"Although inexact graph-matching is a problem of potentially exponential complexity, the problem may be simplified by decomposing the graphs to be matched into smaller subgraphs. If this is done, then the process may cast into a hierarchical framework and hence rendered suitable for parallel computation. In this paper we describe a spectral method which can be used to partition graphs into non-overlapping subgraphs. In particular, we demonstrate how the Fiedler-vector of the Laplacian matrix can be used to decompose graphs into non-overlapping neighbourhoods that can be used for the purposes of both matching and clustering."'),
('"Spectral Solution of Large-Scale Extrinsic Camera Calibration as a Graph Embedding Problem"', '"ECCV 2004"', '["Independent Component Analysis", "Orientation Error", "Directional Constraint", "Consistency Error', '"https://doi.org/10.1007/978-3-540-24671-8_21"', '"Extrinsic calibration of large-scale ad hoc networks of cameras is posed as the following problem: Calculate the locations of N mobile, rotationally aligned cameras distributed over an urban region, subsets of which view some common environmental features. We show that this leads to a novel class of graph embedding problems that admit closed-form solutions in linear time via partial spectral decomposition of a quadratic form. The minimum squared error (mse) solution determines locations of cameras and/or features in any number of dimensions. The spectrum also indicates insufficiently constrained problems, which can be decomposed into well-constrained rigid subproblems and analyzed to determine useful new views for missing constraints. We demonstrate the method with large networks of mobile cameras distributed over an urban environment, using directional constraints that have been extracted automatically from commonly viewed features. Spectral solutions yield layouts that are consistent in some cases to a fraction of a millimeter, substantially improving the state of the art. Global layout of large camera networks can be computed in a fraction of a second."'),
('"Specularities Reduce Ambiguity of Uncalibrated Photometric Stereo"', '"ECCV 2002"', '["Light Direction", "Viewing Direction", "Illumination Direction", "Integrability Constraint", "Phot', '"https://doi.org/10.1007/3-540-47967-8_4"', '"Lambertian photometric stereo with uncalibrated light directions and intensities determines the surface normals only up to an invertible linear transformation. We show that if object reflectance is a sum of Lambertian and specular terms, the ambiguity reduces into a 2dof group of transformations (compositions of isotropic scaling, rotation around the viewing vector, and change in coordinate frame handedness)."'),
('"Specularity Removal in Images and Videos: A PDE Approach"', '"ECCV 2006"', '["Color Space", "Diffuse Component", "Texture Scene", "Specular Component", "Diffuse Color"]', '"https://doi.org/10.1007/11744023_43"', '"We present a unified framework for separating specular and diffuse reflection components in images and videos of textured scenes. This can be used for specularity removal and for independently processing, filtering, and recombining the two components. Beginning with a partial separation provided by an illumination-dependent color space, the challenge is to complete the separation using spatio-temporal information. This is accomplished by evolving a partial differential equation (PDE) that iteratively erodes the specular component at each pixel. A family of PDEs appropriate for differing image sources (still images vs. videos), differing prior information (e.g., highly vs. lightly textured scenes), or differing prior computations (e.g., optical flow) is introduced. In contrast to many other methods, explicit segmentation and/or manual intervention are not required. We present results on high-quality images and video acquired in the laboratory in addition to images taken from the Internet. Results on the latter demonstrate robustness to low dynamic range, JPEG artifacts, and lack of knowledge of illuminant color. Empirical comparison to physical removal of specularities using polarization is provided. Finally, an application termed dichromatic editing is presented in which the diffuse and the specular components are processed independently to produce a variety of visual effects."'),
('"Specularity, the Zeta-image, and Information-Theoretic Illuminant Estimation"', '"ECCV 2012"', '["Angular Error", "Colour Constancy", "Planar Constraint", "Specular Component", "Specular Point"]', '"https://doi.org/10.1007/978-3-642-33868-7_41"', '"Identification of illumination is an important problem in imaging. In this paper we present a new and effective physics-based colour constancy algorithm which makes use of a novel log-relative-chromaticity planar constraint. We call the new feature the Zeta-image. We show that this new feature is tied to a novel application of the Kullback-Leibler Divergence, here applied to chromaticity values instead of probabilities. The new method requires no training data or tunable parameters. Moreover it is simple to implement and very fast. Our experimental results across datasets of real images show the proposed method significantly outperforms other unsupervised methods while its estimation accuracy is comparable with more complex, supervised, methods. As well, the new planar constraint can be used as a post-processing stage for any candidate colour constancy method in order to improve its accuracy."'),
('"Spike-Based Image Processing: Can We Reproduce Biological Vision in Hardware?"', '"ECCV 2012"', '["Memristive Device", "Simple Neural Network", "Biological Vision", "Biological Vision System", "Lar', '"https://doi.org/10.1007/978-3-642-33863-2_53"', '"Over the past 15 years, we have developed software image processing systems that attempt to reproduce the sorts of spike-based processing strategies used in biological vision. The basic idea is that sophisticated visual processing can be achieved with a single wave of spikes by using the relative timing of spikes in different neurons as an efficient code. While software simulations are certainly an option, it is now becoming clear that it may well be possible to reproduce the same sorts of ideas in specific hardware. Firstly, several groups have now developed spiking retina chips in which the pixel elements send the equivalent of spikes in response to particular events such as increases or a decreases in local luminance. Importantly, such chips are fully asynchronous, allowing image processing to break free of the standard frame based approach. We have recently shown how simple neural network architectures can use the output of such dynamic spiking retinas to perform sophisticated tasks by using a biologically inspired learning rule based on Spike-Time Dependent Plasticity (STDP). Such systems can learn to detect meaningful patterns that repeat in a purely unsupervised way. For example, after just a few minutes of training, a network composed of a first layer of 60 neurons and a second layer of 10 neurons was able to form neurons that could effectively count the number of cars going by on the different lanes of a freeway. For the moment, this work has just used simulations. However, there is a real possibility that the same processing strategies could be implemented in memristor-based hardware devices. If so, it will become possible to build intelligent image processing systems capable of learning to recognize significant events without the need for conventional computational hardware."'),
('"Spring Lattice Counting Grids: Scene Recognition Using Deformable Positional Constraints"', '"ECCV 2012"', '["Spring Lattice", "Latent Dirichlet Allocation", "Visual Stream", "Scene Recognition", "Indoor Scen', '"https://doi.org/10.1007/978-3-642-33783-3_60"', '"Adopting the Counting Grid (CG) representation [1], the Spring Lattice Counting Grid (SLCG) model uses a grid of feature counts to capture the spatial layout that a variety of images tend to follow. The images are mapped to the counting grid with their features rearranged so as to strike a balance between the mapping quality and the extent of the necessary rearrangement. In particular, the feature sets originating from different image sectors are mapped to different sub-windows in the counting grid in a configuration that is close, but not exactly the same as the configuration of the source sectors. The distribution over deformations of the sector configuration is learnable using a new spring lattice model, while the rearrangement of features within a sector is unconstrained. As a result, the CG model gains a more appropriate level of invariance to realistic image transformations like view point changes, rotations or scales. We tested SLCG on standard scene recognition datasets and on a dataset collected with a wearable camera which recorded the wearer\\u2019s visual input over three weeks. Our algorithm is capable of correctly classifying the visited locations more than 80% of the time, outperforming previous approaches to visual location recognition. At this level of performance, a variety of real-world applications of wearable cameras become feasible."'),
('"SRA: Fast Removal of General Multipath for ToF Sensors"', '"ECCV 2014"', '["Canonical Transformation", "Mean Absolute Error", "Orthogonal Match Pursuit", "General Multipath",', '"https://doi.org/10.1007/978-3-319-10590-1_16"', '"A major issue with Time of Flight sensors is the presence of multipath interference. We present Sparse Reflections Analysis (SRA), an algorithm for removing this interference which has two main advantages. First, it allows for very general forms of multipath, including interference with three or more paths, diffuse multipath resulting from Lambertian surfaces, and combinations thereof. SRA removes this general multipath with robust techniques based on L 1 optimization. Second, due to a novel dimension reduction, we are able to produce a very fast version of SRA, which is able to run at frame rate. Experimental results on both synthetic data with ground truth, as well as real images of challenging scenes, validate the approach."'),
('"Stable Spectral Mesh Filtering"', '"ECCV 2012"', '["Computational Geometry and Object Modeling", "Hierarchy and geometric transformations", "Laplace-B', '"https://doi.org/10.1007/978-3-642-33863-2_9"', '"The rapid development of 3D acquisition technology has brought with itself the need to perform standard signal processing operations such as filters on 3D data. It has been shown that the eigenfunctions of the Laplace-Beltrami operator (manifold harmonics) of a surface play the role of the Fourier basis in the Euclidean space; it is thus possible to formulate signal analysis and synthesis in the manifold harmonics basis. In particular, geometry filtering can be carried out in the manifold harmonics domain by decomposing the embedding coordinates of the shape in this basis. However, since the basis functions depend on the shape itself, such filtering is valid only for weak (near all-pass) filters, and produces severe artifacts otherwise. In this paper, we analyze this problem and propose the fractional filtering approach, wherein we apply iteratively weak fractional powers of the filter, followed by the update of the basis functions. Experimental results show that such a process produces more plausible and meaningful results."'),
('"Stacked Deformable Part Model with Shape Regression for Object Part Localization"', '"ECCV 2014"', '["Object Detection", "Deep Neural Network", "Object Part", "Active Appearance Model", "Regression Ma', '"https://doi.org/10.1007/978-3-319-10605-2_37"', '"This paper explores the localization of pre-defined semantic object parts, which is much more challenging than traditional object detection and very important for applications such as face recognition, HCI and fine-grained object recognition. To address this problem, we make two critical improvements over the widely used deformable part model (DPM). The first is that we use appearance based shape regression to globally estimate the anchor location of each part and then locally refine each part according to the estimated anchor location under the constraint of DPM. The DPM with shape regression (SR-DPM) is more flexible than the traditional DPM by relaxing the fixed anchor location of each part. It enjoys the efficient dynamic programming inference as traditional DPM and can be discriminatively trained via a coordinate descent procedure. The second is that we propose to stack multiple SR-DPMs, where each layer uses the output of previous SR-DPM as the input to progressively refine the result. It provides an analogy to deep neural network while benefiting from hand-crafted feature and model. The proposed methods are applied to human pose estimation, face alignment and general object part localization tasks and achieve state-of-the-art performance."'),
('"Stacked Hierarchical Labeling"', '"ECCV 2010"', '["Graphical Model", "Training Image", "Inference Procedure", "Parent Region", "Semantic Label"]', '"https://doi.org/10.1007/978-3-642-15567-3_5"', '"In this work we propose a hierarchical approach for labeling semantic objects and regions in scenes. Our approach is reminiscent of early vision literature in that we use a decomposition of the image in order to encode relational and spatial information. In contrast to much existing work on structured prediction for scene understanding, we bypass a global probabilistic model and instead directly train a hierarchical inference procedure inspired by the message passing mechanics of some approximate inference procedures in graphical models. This approach mitigates both the theoretical and empirical difficulties of learning probabilistic models when exact inference is intractable. In particular, we draw from recent work in machine learning and break the complex inference process into a hierarchical series of simple machine learning subproblems. Each subproblem in the hierarchy is designed to capture the image and contextual statistics in the scene. This hierarchy spans coarse-to-fine regions and explicitly models the mixtures of semantic labels that may be present due to imperfect segmentation. To avoid cascading of errors and overfitting, we train the learning problems in sequence to ensure robustness to likely errors earlier in the inference sequence and leverage the stacking approach developed by Cohen et al"'),
('"Star Shape Prior for Graph-Cut Image Segmentation"', '"ECCV 2008"', '["Image Segmentation", "Object Segment", "Shape Constraint", "Shape Prior", "Geodesic Active Contour', '"https://doi.org/10.1007/978-3-540-88690-7_34"', '"In recent years, segmentation with graph cuts is increasingly used for a variety of applications, such as photo/video editing, medical image processing, etc. One of the most common applications of graph cut segmentation is extracting an object of interest from its background. If there is any knowledge about the object shape (i.e. a shape prior), incorporating this knowledge helps to achieve a more robust segmentation. In this paper, we show how to implement a star shape prior into graph cut segmentation. This is a generic shape prior, i.e. it is not specific to any particular object, but rather applies to a wide class of objects, in particular to convex objects. Our major assumption is that the center of the star shape is known, for example, it can be provided by the user. The star shape prior has an additional important benefit - it allows an inclusion of a term in the objective function which encourages a longer object boundary. This helps to alleviate the bias of a graph cut towards shorter segmentation boundaries. In fact, we show that in many cases, with this new term we can achieve an accurate object segmentation with only a single pixel, the center of the object, provided by the user, which is rarely possible with standard graph cut interactive segmentation."'),
('"State Estimation in a Document Image and Its Application in Text Block Identification and Text Line', '"ECCV 2010"', '["State Estimation", "Delaunay Triangulation", "Document Image", "Text Line", "Text Region"]', '"https://doi.org/10.1007/978-3-642-15552-9_31"', '"This paper proposes a new approach to the estimation of document states such as interline spacing and text line orientation, which facilitates a number of tasks in document image processing. The proposed method can be applied to spatially varying states as well as invariant ones, so that general cases including images of complex layout, camera-captured images, and handwritten ones can also be handled. Specifically, we find CCs (Connected Components) in a document image and assign a state to each of them. Then the states of CCs are estimated using an energy minimization framework, where the cost function is designed based on frequency domain analysis and minimized via graph-cuts. Using the estimated states, we also develop a new algorithm that performs text block identification and text line extraction. Roughly speaking, we can segment an image into text blocks by cutting the distant connections among the CCs (compared to the estimated interline spacing), and we can group the CCs into text lines using a bottom-up grouping along the estimated text line orientation. Experimental results on a variety of document images show that our method is efficient and provides promising results in several document image processing tasks."'),
('"Statistical Analysis of Global Motion Chains"', '"ECCV 2008"', '["Video Clip", "Camera Motion", "Typical Motion", "Global Motion", "Motion Chain"]', '"https://doi.org/10.1007/978-3-540-88688-4_51"', '"Multiple elements such as lighting, colors, dialogue, and camera motion contribute to the style of a movie. Among them, camera motion is commonly overlooked yet a crucial point. For instance, documentaries tend to use long smooth pans whereas action movies usually have short and dynamic movements. This information, also referred to as global motion, could be leveraged by various applications in video clustering, stabilization, and editing. We perform analyses to study the in-class characteristics of these motions as well as their relationship with motions of other movie types. In particular, we model global motion as a multi-scale distribution of transformation matrices from frame to frame. Secondly, we quantify the difference between pairs of videos using the KL-divergence of these distributions. Finally, we demonstrate an application modeling and clustering commercial and amateur videos. Experiments performed show advantage compared to the usage of some local motion-based approaches."'),
('"Statistical and Spatial Consensus Collection for Detector Adaptation"', '"ECCV 2014"', '["Pedestrian Detection", "Unsupervised Domain Adaptation", "RANSAC"]', '"https://doi.org/10.1007/978-3-319-10578-9_30"', '"The increasing interest in automatic adaptation of pedestrian detectors toward specific scenarios is motivated by the drop of performance of common detectors, especially in video-surveillance low resolution images. Different works have been recently proposed for unsupervised adaptation. However, most of these works do not completely solve the drifting problem: initial false positive target samples used for training can lead the model to drift. We propose to transform the outlier rejection problem in a weak classifier selection approach. A large set of weak classifiers are trained with random subsets of unsupervised target data and their performance is measured on a labeled source dataset. We can then select the most accurate classifiers in order to build an ensemble of weakly dependent detectors for the target domain. The experimental results we obtained on two benchmarks show that our system outperforms other pedestrian adaptation state-of-the-art methods."'),
('"Statistical Characterization of Morphological Operator Sequences"', '"ECCV 2002"', '["False Alarm", "False Alarm Probability", "License Plate", "Operator Sequence", "Morphological Oper', '"https://doi.org/10.1007/3-540-47979-1_40"', '"Detection followed by morphological processing is commonly used in machine vision. However, choosing the morphological operators and parameters is often done in a heuristic manner since a statistical characterization of their performance is not easily derivable. If we consider a morphology operator sequence as a classifier distinguishing between two patterns, the automatic choice of the operator sequence and parameters is possible if one derives the misclassification distribution as a function of the input signal distributions, the operator sequence, and parameter choices. The main essence of this paper is the illustration that misclassification statistics, the distribution of bit errors measured by the Hamming distance, can be computed by using an embeddable Markov chain approach. License plate extraction is used as a case study to illustrate the utility of the theory on real data."'),
('"Statistical Foreground Modelling for Object Localisation"', '"ECCV 2000"', '["Layered Sampling", "Conditioned Likelihood", "Importance Function", "Pattern Theory", "Factor Samp', '"https://doi.org/10.1007/3-540-45053-X_20"', '"A Bayesian approach to object localisation is feasible given suitable likelihood models for image observations. Such a likelihood involves statistical modelling - and learning - both of the object foreground and of the scene background. Statistical background models are already quite well understood. Here we propose a \\u201cconditioned likelihood\\u201d model for the foreground, conditioned on variations both in object appearance and illumination. Its effectiveness in localising a variety of objects is demonstrated."'),
('"Statistical Imaging for Modeling and Identification of Bacterial Types"', '"MMBIA 2004"', '["Feature Vector", "Gaussian Mixture Model", "Shape Index", "Phage Typing", "Bacterial Type"]', '"https://doi.org/10.1007/978-3-540-27816-0_28"', '"An automatic tool is developed to identify microbiological data types using computer-vision and statistical modeling techniques. In bacteriophage (phage) typing, representative profiles of bacterial types are extracted. Currently, systems rely on the subjective reading of the profiles by a human expert. This process is time-consuming and prone to errors. The statistical methodology presented in this work, provides for an automated, objective and robust analysis of the visual data, along with the ability to cope with increasing data volumes. Validation is performed by a comparison to an expert manual segmentation and labeling of the phage profiles."'),
('"Statistical Inference of Motion in the Invisible"', '"ECCV 2012"', '["Collision Avoidance", "Multiple Camera", "Interior Point Algorithm", "Camera Network", "Object Tra', '"https://doi.org/10.1007/978-3-642-33765-9_39"', '"This paper focuses on the unexplored problem of inferring motion of objects that are invisible to all cameras in a multiple camera setup. As opposed to methods for learning relationships between disjoint cameras, we take the next step to actually infer the exact spatiotemporal behavior of objects while they are invisible. Given object trajectories within disjoint cameras\\u2019 FOVs (field-of-view), we introduce constraints on the behavior of objects as they travel through the unobservable areas that lie in between. These constraints include vehicle following (the trajectories of vehicles adjacent to each other at entry and exit are time-shifted relative to each other), collision avoidance (no two trajectories pass through the same location at the same time) and temporal smoothness (restricts the allowable movements of vehicles based on physical limits). The constraints are embedded in a generalized, global cost function for the entire scene, incorporating influences of all objects, followed by a bounded minimization using an interior point algorithm, to obtain trajectory representations of objects that define their exact dynamics and behavior while invisible. Finally, a statistical representation of motion in the entire scene is estimated to obtain a probabilistic distribution representing individual behaviors, such as turns, constant velocity motion, deceleration to a stop, and acceleration from rest for evaluation and visualization. Experiments are reported on real world videos from multiple disjoint cameras in NGSIM data set, and qualitative as well as quantitative analysis confirms the validity of our approach."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Statistical Learning of Evaluation Function for ASM/AAM Image Alignment"', '"BioAW 2004"', '["Face Recognition", "Reconstruction Error", "Alignment Result", "Edge Feature", "Active Appearance ', '"https://doi.org/10.1007/978-3-540-25976-3_5"', '"Alignment between the input and target objects has great impact on the performance of image analysis and recognition system, such as those for medical image and face recognition. Active Shape Models (ASM) [1] and Active Appearance Models (AAM) [2, 3] provide an important framework for this task. However, an effective method for the evaluation of ASM/AAM alignment results has been lacking. Without an alignment quality evaluation mechanism, a bad alignment cannot be identified and this can drop system performance."'),
('"Statistical Learning of Multi-view Face Detection"', '"ECCV 2002"', '["Support Vector Machine", "Feature Selection", "Statistical Learn", "Face Detection", "Heuristic As', '"https://doi.org/10.1007/3-540-47979-1_5"', '"A new boosting algorithm, called FloatBoost, is proposed to overcome the monotonicity problem of the sequential AdaBoost learning. AdaBoost [1,2] is a sequential forward search procedure using the greedy selection strategy. The premise offered by the sequential procedure can be broken-down when the monotonicity assumption, i.e. that when adding a new feature to the current set, the value of the performance criterion does not decrease, is violated. FloatBoost incorporates the idea of Floating Search [3] into AdaBoost to solve the non-monotonicity problem encountered in the sequential search of AdaBoost."'),
('"Statistical Modeling of Texture Sketch"', '"ECCV 2002"', '["Independent component analysis", "Matching pursuit", "Minimax entropy learning", "Sparse coding", ', '"https://doi.org/10.1007/3-540-47977-5_16"', '"Recent results on sparse coding and independent component analysis suggest that human vision first represents a visual image by a linear superposition of a relatively small number of localized, elongate, oriented image bases. With this representation, the sketch of an image consists of the locations, orientations, and elongations of the image bases, and the sketch can be visually illustrated by depicting each image base by a linelet of the same length and orientation. Built on the insight of sparse and independent component analysis, we propose a two-level generative model for textures. At the bottom-level, the texture image is represented by a linear superposition of image bases. At the top-level, a Markov model is assumed for the placement of the image bases or the sketch, and the model is characterized by a set of simple geometrical feature statistics."'),
('"Statistical Pose Averaging with Non-isotropic and Incomplete Relative Measurements"', '"ECCV 2014"', '["Pose averaging", "Riemannian geometry", "Error propagation", "Anisotropic filtering", "Incomplete ', '"https://doi.org/10.1007/978-3-319-10602-1_52"', '"In the last few years there has been a growing interest in optimization methods for averaging pose measurements between a set of cameras or objects (obtained, for instance, using epipolar geometry or pose estimation). Alas, existing approaches do not take into consideration that measurements might have different uncertainties (i.e., the noise might not be isotropically distributed), or that they might be incomplete (e.g., they might be known only up to a rotation around a fixed axis). We propose a Riemannian optimization framework which addresses these cases by using covariance matrices, and test it on synthetic and real data."'),
('"Statistical Priors for Efficient Combinatorial Optimization Via Graph Cuts"', '"ECCV 2006"', '["Bayesian Inference", "Combinatorial Optimization Problem", "Markov Chain Monte Carlo Method", "Sta', '"https://doi.org/10.1007/11744078_21"', '"Bayesian inference provides a powerful framework to optimally integrate statistically learned prior knowledge into numerous computer vision algorithms. While the Bayesian approach has been successfully applied in the Markov random field literature, the resulting combinatorial optimization problems have been commonly treated with rather inefficient and inexact general purpose optimization methods such as Simulated Annealing. An efficient method to compute the global optima of certain classes of cost functions defined on binary-valued variables is given by graph min-cuts. In this paper, we propose to reconsider the problem of statistical learning for Bayesian inference in the context of efficient optimization schemes. Specifically, we address the question: Which prior information may be learned while retaining the ability to apply Graph Cut optimization? We provide a framework to learn and impose prior knowledge on the distribution of pairs and triplets of labels. As an illustration, we demonstrate that one can optimally restore binary textures from very noisy images with runtimes on the order of a second while imposing hundreds of statistically learned constraints per pixel."'),
('"Statistical Shape Analysis for Population Studies via Level-Set Based Shape Morphing"', '"ECCV 2012"', '["Caudate Nucleus", "Schizophrenia Patient", "Signed Distance Function", "Medical Image Analysis", "', '"https://doi.org/10.1007/978-3-642-33863-2_5"', '"We present a method that allows the detection, localization and quantification of statistically significant morphological differences in complex brain structures between populations. This is accomplished by a novel level-set framework for shape morphing and a multi-shape dissimilarity-measure derived by a modified version of the Hausdorff distance. The proposed method does not require explicit one-to-one point correspondences and is fast, robust and easy to implement regardless of the topological complexity of the anatomical surface under study."'),
('"Statistical Significance as an Aid to System Performance Evaluation"', '"ECCV 2000"', '["Binary Code", "Matching Process", "Merit Function", "True Assignment", "System Performance Evaluat', '"https://doi.org/10.1007/3-540-45053-X_24"', '"Using forensic fingerprint identification as a testbed, a statistical framework for analyzing system performance is presented. Each set of fingerprint features is represented by a collection of binary codes. The matching process is equated to measuring the Hamming distances between feature sets. After performing matching experiments on a small data base, the number of independent degrees of freedom intrinsic to the fingerprint population is estimated. Using this information, a set of independent Bernoulli trials is used to predict the success of the system with respect to a particular dataset."'),
('"Statistically Learned Deformable Eye Models"', '"ECCV 2014"', '["Eye alignment", "Eye tracking", "Active appearance models", "Constrained local models", "Supervise', '"https://doi.org/10.1007/978-3-319-16178-5_19"', '"In this paper we study the feasibility of using standard deformable model fitting techniques to accurately track the deformation and motion of the human eye. To this end, we propose two highly detailed shape annotation schemes (open and close eyes), with \\\\(+30\\\\) feature landmark points, high resolution eye images. We build extremely detailed Active Appearance Models (AAM), Constrained Local Models (CLM) and Supervised Descent Method (SDM) models of the human eye and report preliminary experiments comparing the relative performance of the previous techniques on the problem of eye alignment."'),
('"Statistics of Pairwise Co-occurring Local Spatio-temporal Features for Human Action Recognition"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_31"', '"The bag-of-words approach with local spatio-temporal features have become a popular video representation for action recognition in videos. Together these techniques have demonstrated high recognition results for a number of action classes. Recent approaches have typically focused on capturing global statistics of features. However, existing methods ignore relations between features and thus may not be discriminative enough. Therefore, we propose a novel feature representation which captures statistics of pairwise co-occurring local spatio-temporal features. Our representation captures not only global distribution of features but also focuses on geometric and appearance (both visual and motion) relations among the features. Calculating a set of bag-of-words representations with different geometrical arrangement among the features, we keep an important association between appearance and geometric information. Using two benchmark datasets for human action recognition, we demonstrate that our representation enhances the discriminative power of features and improves action recognition performance."'),
('"Statistics of Patch Offsets for Image Completion"', '"ECCV 2012"', '["Salient Object", "Coherence Measure", "Image Editing", "Image Inpainting", "Similar Patch"]', '"https://doi.org/10.1007/978-3-642-33709-3_2"', '"Image completion involves filling missing parts in images. In this paper we address this problem through the statistics of patch offsets. We observe that if we match similar patches in the image and obtain their offsets (relative positions), the statistics of these offsets are sparsely distributed. We further observe that a few dominant offsets provide reliable information for completing the image. With these offsets we fill the missing region by combining a stack of shifted images via optimization. A variety of experiments show that our method yields generally better results and is faster than existing state-of-the-art methods."'),
('"Steering in Scale Space to Optimally Detect Image Structures"', '"ECCV 2004"', '["Energy Maximum", "Image Structure", "Scale Space", "Radial Frequency", "Scale Selection"]', '"https://doi.org/10.1007/978-3-540-24670-1_37"', '"Detecting low-level image features such as edges and ridges with spatial filters is improved if the scale of the features are known a priori. Scale-space representations and wavelet pyramids address the problem by using filters over multiple scales. However, the scales of the filters are still fixed beforehand and the number of scales is limited by computational power. The filtering operations are thus not adapted to detect image structures at their optimal or intrinsic scales. We adopt the steering approach to obtain filter responses at arbitrary scales from a small set of filters at scales chosen to accurately sample the \\u201cscale space\\u201d within a given range. In particular, we use the Moore-Penrose inverse to learn the steering coefficients, which we then regress by polynomial function fitting to the scale parameter in order to steer the filter responses continuously across scales. We show that the extrema of the polynomial steering functions can be easily computed to detect interesting features such as phase-independent energy maxima. Such points of energy maxima in our \\u03b1-scale-space correspond to the intrinsic scale of the filtered image structures. We apply the technique to several well-known images to segment image structures which are mostly characterised by their intrinsic scale."'),
('"Stereo Autocalibration from One Plane"', '"ECCV 2000"', '[]', '"https://doi.org/10.1007/3-540-45053-X_40"', '"This paper describes a method for autocalibrating a stereo rig. A planar object performing general and unknown motions is observed by the stereo rig and, based on point correspondences only, the autocalibration of the stereo rig is computed. A stratified approach is used and the autocalibration is computed by estimating first the epipolar geometry of the rig, then the plane at infinity \\u03a0\\u221e (affine calibration) and finally the absolute conic \\u03a9\\u221e (Euclidean calibration). We show that the affine and Euclidean calibrations involve quadratic constraints and we describe an algorithm to solve them based on a conic intersection technique. Experiments with both synthetic and real data are used to evaluate the performance of the method."'),
('"Stereo Based 3D Tracking and Scene Learning, Employing Particle Filtering within EM"', '"ECCV 2004"', '["Expectation Maximiza", "Expectation Maximiza Algorithm", "Appearance Model", "Stereo Vision", "Ste', '"https://doi.org/10.1007/978-3-540-24673-2_44"', '"We present a generative probabilistic model for 3D scenes with stereo views. With this model, we track an object in 3 dimensions while simultaneously learning its appearance and the appearance of the background. By using a generative model for the scene, we are able to aggregate evidence over time. In addition, the probabilistic model naturally handles sources of variability."'),
('"Stereo Matching Using Belief Propagation"', '"ECCV 2002"', '["Belief Propagation", "Stereo Vision", "Stereo Match", "Line Process", "Markov Network"]', '"https://doi.org/10.1007/3-540-47967-8_34"', '"In this paper, we formulate the stereo matching problem as a Markov network consisting of three coupled Markov random fields (MRF\\u2019s). These three MRF\\u2019s model a smooth field for depth/disparity, a line process for depth discontinuity and a binary process for occlusion, respectively. After eliminating the line process and the binary process by introducing two robust functions, we obtain the maximum a posteriori (MAP) estimation in the Markov network by applying a Bayesian belief propagation (BP) algorithm. Furthermore, we extend our basic stereo model to incorporate other visual cues (e.g., image segmentation) that are not modeled in the three MRF\\u2019s, and again obtain the MAP solution. Experimental results demonstrate that our method outperforms the state-of-art stereo algorithms for most test cases."'),
('"Stereo Matching with Segmentation-Based Cooperation"', '"ECCV 2002"', '["Stereoscopic Vision", "Occlusion Detection", "Cooperative Algorithm"]', '"https://doi.org/10.1007/3-540-47967-8_37"', '"In this paper we present a new stereo matching algorithm that produces accurate dense disparity maps and explicitly detects occluded areas. This algorithm extends the original cooperative algorithms in two ways. First, we design a method of adjusting the initial matching score volume to guarantee that correct matches have high matching scores. This method propagates \\u201cgood\\u201d disparity information within or among image segments based on certain disparity confidence measurement criterion, thus improving the robustness of the algorithm. Second, we develop a scheme of choosing local support areas by enforcing the image segmentation information. This scheme sees that the depth discontinuities coincide with the color or intensity boundaries. As a result, the foreground fattening errors are drastically reduced. Extensive experimental results demonstrate the effectiveness of our algorithm, both quantitatively and qualitatively. Comparison between our algorithm and some other representative algorithms is also reported."'),
('"Stereo Matching: An Outlier Confidence Approach"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88693-8_57"', '"One of the major challenges in stereo matching is to handle partial occlusions. In this paper, we introduce the Outlier Confidence (OC) which dynamically measures how likely one pixel is occluded. Then the occlusion information is softly incorporated into our model. A global optimization is applied to robustly estimating the disparities for both the occluded and non-occluded pixels. Compared to color segmentation with plane fitting which globally partitions the image, our OC model locally infers the possible disparity values for the outlier pixels using a reliable color sample refinement scheme. Experiments on the Middlebury dataset show that the proposed two-frame stereo matching method performs satisfactorily on the stereo images."'),
('"Stereo Using Monocular Cues within the Tensor Voting Framework"', '"ECCV 2004"', '["Stereo Match", "Correct Match", "Epipolar Line", "Candidate Match", "Stereo Correspondence"]', '"https://doi.org/10.1007/978-3-540-24673-2_47"', '"We address the fundamental problem of matching two static images. Significant progress has been made in this area, but the correspondence problem has not been solved. Most of the remaining difficulties are caused by occlusion and lack of texture. We propose an approach that addresses these difficulties within a perceptual organization framework, taking into account both binocular and monocular sources of information. Geometric and color information from the scene is used for grouping, complementing each other\\u2019s strengths. We begin by generating matching hypotheses for every pixel in such a way that a variety of matching techniques can be integrated, thus allowing us to combine their particular advantages. Correct matches are detected based on the support they receive from their neighboring candidate matches in 3-D, after tensor voting. They are grouped into smooth surfaces, the projections of which on the images serve as the reliable set of matches. The use of segmentation based on geometric cues to infer the color distributions of scene surfaces is arguably the most significant contribution of our research. The inferred reliable set of matches guides the generation of disparity hypotheses for the unmatched pixels. The match for an unmatched pixel is selected among a set of candidates as the one that is a good continuation of the surface, and also compatible with the observed color distribution of the surface in both images. Thus, information is propagated from more to less reliable pixels considering both geometric and color information. We present results on standard stereo pairs."'),
('"Stereovision-Based Head Tracking Using Color and Ellipse Fitting in a Particle Filter"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_16"', '"This paper proposes the use of a particle filter combined with color, depth information, gradient and shape features as an efficient and effective way of dealing with tracking of a head on the basis of image stream coming from a mobile stereovision camera. The head is modeled in the 2D image domain by an ellipse. A weighting function is used to include spatial information in color histogram representing the interior of the ellipse. The lengths of the ellipse\\u2019s minor axis are determined on the basis of depth information. The dissimilarity between the current model of the tracked object and target candidates is indicated by a metric based on Bhattacharyya coefficient. Variations of the color representation as a consequence of ellipse\\u2019s size change are handled by taking advantage of the scale invariance of the similarity measure. The color histogram and parameters of the ellipse are dynamically updated over time to discriminate in the next iteration between the candidate and actual head representation. This makes possible to track not only a face profile which has been shot during initialization of the tracker but in addition different profiles of the face as well as the head can be tracked. Experimental results which were obtained on long image sequences in a typical office environment show the feasibility of our approach to perform tracking of a head undergoing complex changes of shape and appearance against a varying background. The resulting system runs in real-time on a standard laptop computer installed on a real mobile agent."'),
('"Stitching and Reconstruction of Linear-Pushbroom Panoramic Images for Planar Scenes"', '"ECCV 2004"', '["Panoramic Image", "Camera Coordinate System", "Line Sensor", "Destination Image", "Image Stitching', '"https://doi.org/10.1007/978-3-540-24671-8_15"', '"This paper proposes a method to integrate multiple linear-pushbroom panoramic images. The integration can be performed in real time. The technique is feasible on planar scene such as large-scale paintings or aerial/satellite images that are considered to be planar. The image integration consists of two steps: stitching and Euclidean reconstruction. For the image stitching, a minimum of five pairs of non-collinear image corresponding points are required in general cases. In some special configurations when there is column-to-column image correspondence between two panoramas, the number of image corresponding points required can be reduced to three. As for the Euclidean reconstruction, five pairs of non-collinear image corresponding points on the image boundaries are sufficient."'),
('"Stixels Motion Estimation without Optical Flow Computation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33783-3_38"', '"This paper presents a new approach to estimate the motion of objects seen from a stereo rig mounted on a ground mobile robot. We exploit the prior knowledge on ground plane presence and rough shape of objects, to extract a simplified world model, named stixel world. The contribution of this paper is to show that stixels motion can be estimated directly solving a single dynamic programming problem instead of an image wide optical flow computation. We compare this new method with baseline methods, show competitive results quality-wise, and a significant gain speed-wise."'),
('"Stixmantics: A Medium-Level Model for Real-Time Semantic Scene Understanding"', '"ECCV 2014"', '["semantic scene understanding", "bag-of-features", "region classification", "real-time", "stereo vi', '"https://doi.org/10.1007/978-3-319-10602-1_35"', '"In this paper we present Stixmantics, a novel medium-level scene representation for real-time visual semantic scene understanding. Relevant scene structure, motion and object class information is encoded using so-called Stixels as primitive elements. Sparse feature-point trajectories are used to estimate the 3D motion field and to enforce temporal consistency of semantic labels. Spatial label coherency is obtained by using a CRF framework."'),
('"Stochastic Tracking of 3D Human Figures Using 2D Image Motion"', '"ECCV 2000"', '["Joint Angle", "Image Motion", "Perspective Projection", "Posterior Probability Distribution", "Pri', '"https://doi.org/10.1007/3-540-45053-X_45"', '"A probabilistic method for tracking 3D articulated human figures in monocular image sequences is presented. Within a Bayesian framework, we define a generative model of image appearance, a robust likelihood function based on image graylevel differences, and a prior probability distribution over pose and joint angles that models how humans move. The posterior probability distribution over model parameters is represented using a discrete set of samples and is propagated over time using particle filtering. The approach extends previous work on parameterized optical flow estimation to exploit a complex 3D articulated motion model. It also extends previous work on human motion tracking by including a perspective camera model, by modeling limb self occlusion, and by recovering 3D motion from a monocular sequence. The explicit posterior probability distribution represents ambiguities due to image matching, model singularities, and perspective projection. The method relies only on a frame-to-frame assumption of brightness constancy and hence is able to track people under changing viewpoints, in grayscale image sequences, and with complex unknown backgrounds."'),
('"Stratified Self Calibration from Screw-Transform Manifolds"', '"ECCV 2002"', '["Fundamental Matrix", "Fundamental Matrice", "Screw Axis", "Projective Reconstruction", "Camera Mat', '"https://doi.org/10.1007/3-540-47979-1_9"', '"This paper introduces a new, stratified approach for the metric self calibration of a camera with fixed internal parameters. The method works by intersecting modulus-constraint manifolds, which are a specific type of screw-transform manifold. Through the addition of a single scalar parameter, a 2-dimensional modulus-constraint manifold can become a 3-dimensional Kruppa-constraint manifold allowing for direct self calibration from disjoint pairs of views. In this way, we demonstrate that screw-transform manifolds represent a single, unified approach to performing both stratified and direct self calibration. This paper also shows how to generate the screw-transform manifold arising from turntable (i.e., pairwise-planar) motion and discusses some important considerations for creating a working algorithm from these ideas."'),
('"Streaming Hierarchical Video Segmentation"', '"ECCV 2012"', '["Segmentation Result", "Approximation Framework", "Subsequence Versus", "Video Segmentation", "Hier', '"https://doi.org/10.1007/978-3-642-33783-3_45"', '"The use of video segmentation as an early processing step in video analysis lags behind the use of image segmentation for image analysis, despite many available video segmentation methods. A major reason for this lag is simply that videos are an order of magnitude bigger than images; yet most methods require all voxels in the video to be loaded into memory, which is clearly prohibitive for even medium length videos. We address this limitation by proposing an approximation framework for streaming hierarchical video segmentation motivated by data stream algorithms: each video frame is processed only once and does not change the segmentation of previous frames. We implement the graph-based hierarchical segmentation method within our streaming framework; our method is the first streaming hierarchical video segmentation method proposed. We perform thorough experimental analysis on a benchmark video data set and longer videos. Our results indicate the graph-based streaming hierarchical method outperforms other streaming video segmentation methods and performs nearly as well as the full-video hierarchical graph-based method."'),
('"Strengthening the Effectiveness of Pedestrian Detection with Spatially Pooled Features"', '"ECCV 2014"', '["Local Binary Pattern", "IEEE Conf", "Sparse Code", "Weak Learner", "Pedestrian Detection"]', '"https://doi.org/10.1007/978-3-319-10593-2_36"', '"We propose a simple yet effective approach to the problem of pedestrian detection which outperforms the current state-of-the-art. Our new features are built on the basis of low-level visual features and spatial pooling. Incorporating spatial pooling improves the translational invariance and thus the robustness of the detection process. We then directly optimise the partial area under the ROC curve (pAUC) measure, which concentrates detection performance in the range of most practical importance. The combination of these factors leads to a pedestrian detector which outperforms all competitors on all of the standard benchmark datasets. We advance state-of-the-art results by lowering the average miss rate from 13% to 11% on the INRIA benchmark, 41% to 37% on the ETH benchmark, 51% to 42% on the TUD-Brussels benchmark and 36% to 29% on the Caltech-USA benchmark."'),
('"Stretching Bayesian Learning in the Relevance Feedback of Image Retrieval"', '"ECCV 2004"', '["Support Vector Machine", "Image Retrieval", "Image Database", "Negative Sample", "Query Image"]', '"https://doi.org/10.1007/978-3-540-24672-5_28"', '"This paper is about the work on user relevance feedback in image retrieval. We take this problem as a standard two-class pattern classification problem aiming at refining the retrieval precision by learning through the user relevance feedback data. However, we have investigated the problem by noting two important unique characteristics of the problem: small sample collection and asymmetric sample distributions between positive and negative samples. We have developed a novel approach to stretching Bayesian learning to solve for this problem by explicitly exploiting the two unique characteristics, which is the methodology of BAyesian Learning in Asymmetric and Small sample collections, thus called BALAS. Different learning strategies are used for positive and negative sample collections in BALAS, respectively, based on the two unique characteristics. By defining the relevancy confidence as the relevant posterior probability, we have developed an integrated ranking scheme in BALAS which complementarily combines the subjective relevancy confidence and the objective feature-based distance measure to capture the overall retrieval semantics. The experimental evaluations have confirmed the rationale of the proposed ranking scheme, and have also demonstrated that BALAS is superior to an existing relevance feedback method in the current literature in capturing the overall retrieval semantics."'),
('"Structure and Motion for Dynamic Scenes \\u2014 The Case of Points Moving in Planes"', '"ECCV 2002"', '["Motion Plane", "Stereo Pair", "Dynamic Scene", "Linear Trajectory", "Projective Reconstruction"]', '"https://doi.org/10.1007/3-540-47967-8_58"', '"We consider dynamic scenes consisting of moving points whose motion is constrained to happen in one of a pencil of planes. This is for example the case when rigid objects move independently, but on a common ground plane (each point moves in one of a pencil of planes parallel to the ground plane). We consider stereo pairs of the dynamic scene, taken by a moving stereo system, that allow to obtain 3D reconstructions of the scene, for different time instants. We derive matching constraints for pairs of such 3D reconstructions, especially we introduce a simple tensor, that encapsulates parts of the motion of the stereo system and parts of the scene structure. This tensor allows to partially recover the dynamic structure of the scene. Complete recovery of structure and motion can be performed in a number of ways, e.g. using the information of static points or linear trajectories. We also develop a special self-calibration method for the considered scenario."'),
('"Structure and Motion from Images of Smooth Textureless Objects"', '"ECCV 2004"', '["Motion Estimation", "Projection Matrix", "Projection Matrice", "Epipolar Line", "Epipolar Geometry', '"https://doi.org/10.1007/978-3-540-24671-8_23"', '"This paper addresses the problem of estimating the 3D shape of a smooth textureless solid from multiple images acquired under orthographic projection from unknown and unconstrained viewpoints. In this setting, the only reliable image features are the object\\u2019s silhouettes, and the only true stereo correspondence between pairs of silhouettes are the frontier points where two viewing rays intersect in the tangent plane of the surface. An algorithm for identifying geometrically-consistent frontier points candidates while estimating the cameras\\u2019 projection matrices is presented. This algorithm uses the signature representation of the dual of image silhouettes to identify promising correspondences, and it exploits the redundancy of multiple epipolar geometries to retain the consistent ones. The visual hull of the observed solid is finally reconstructed from the recovered viewpoints. The proposed approach has been implemented, and experiments with six real image sequences are presented, including a comparison between ground-truth and recovered camera configurations, and sample visual hulls computed by the algorithm."'),
('"Structure and Motion Problems for Multiple Rigidly Moving Cameras"', '"ECCV 2004"', '["Multiple Camera", "Autonomous Navigation", "Motion Problem", "Minimal Case", "Camera Centre"]', '"https://doi.org/10.1007/978-3-540-24672-5_20"', '"Vision (both using one-dimensional and two-dimensional retina) is useful for the autonomous navigation of vehicles. In this paper the case of a vehicle equipped with multiple cameras with non-overlapping views is considered. The geometry and algebra of such a moving platform of cameras are considered. In particular we formulate and solve structure and motion problems for a few novel cases of such moving platforms. For the case of two-dimensional retina cameras (ordinary cameras) there are two minimal cases of three points in two platform positions and two points in three platform positions. For the case of one-dimensional retina cameras there are three minimal structure and motion problems. In this paper we consider one of these (6 points in 3 platform positions). The theory has been tested on synthetic data."'),
('"Structure from Many Perspective Images with Occlusions"', '"ECCV 2002"', '["projective reconstruction", "structure from motion", "wide baseline stereo", "factorization"]', '"https://doi.org/10.1007/3-540-47967-8_24"', '"This paper proposes a method for recovery of projective shape and motion from multiple images by factorization of a matrix containing the images of all scene points. Compared to previous methods, this method can handle perspective views and occlusions jointly. The projective depths of image points are estimated by the method of Sturm & Triggs [11] using epipolar geometry. Occlusions are solved by the extension of the method by Jacobs [8] for filling of missing data. This extension can exploit the geometry of perspective camera so that both points with known and unknown projective depths are used. Many ways of combining the two methods exist, and therefore several of them have been examined and the one with the best results is presented. The new method gives accurate results in practical situations, as demonstrated here with a series of experiments on laboratory and outdoor image sets. It becomes clear that the method is particularly suited for wide base-line multiple view stereo."'),
('"Structure from Motion of Parallel Lines"', '"ECCV 2004"', '["Computer Vision", "Parallel Line", "Camera Calibration", "Line Texture", "World Line"]', '"https://doi.org/10.1007/978-3-540-24673-2_19"', '"We investigate the camera geometry of lines parallel in the world. In particular, we formalize the known rotational constraints and add new linear constraints on camera position. The constraints on camera position do not require the cameras to be viewing the same lines, thus providing applications for occluded scenes and calibration of cameras for which fields of view do not intersect. The constraints can also be viewed as constraints of camera geometry with planar patch coordinate systems, and provide a way to investigate texture in a deeper way than has been done to date."'),
('"Structure from Planar Motions with Small Baselines"', '"ECCV 2002"', '["Planar Motion", "Hybrid Algorithm", "Image Point", "Singular Vector", "Translation Vector"]', '"https://doi.org/10.1007/3-540-47967-8_26"', '"We study the multi-frame structure from motion problem when the camera translates on a plane with small baselines and arbitrary rotations. This case shows up in many practical applications, for example, in ground robot navigation. We consider the framework for small baselines presented in [8], in which a factorization method is used to compute the structure and motion parameters accurately, efficiently and with guaranteed convergence. When the camera translates on a plane, the algorithm in [8] cannot be applied because the estimation matrix drops rank, causing the equations to be no longer linear. In this paper, we show how to linearly solve those equations, while preserving the accuracy, speed and convergence properties of the non-planar algorithm. We evaluate the proposed algorithms on synthetic and real image sequences, and compare our results with those of the optimal algorithm. The proposed algorithms are very fast and accurate, have less than 0.3% outliers and work well for small-to-medium baselines and non-planar as well as planar motions."'),
('"Structure of Applicable Surfaces from Single Views"', '"ECCV 2004"', '["Surface", "Differential Geometry", "Applicable Surfaces", "Shape from X"]', '"https://doi.org/10.1007/978-3-540-24672-5_38"', '"The deformation of applicable surfaces such as sheets of paper satisfies the differential geometric constraints of isometry (lengths and areas are conserved) and vanishing Gaussian curvature. We show that these constraints lead to a closed set of equations that allow recovery of the full geometric structure from a single image of the surface and knowledge of its undeformed shape. We show that these partial differential equations can be reduced to the Hopf equation that arises in non-linear wave propagation, and deformations of the paper can be interpreted in terms of the characteristics of this equation. A new exact integration of these equations is developed that relates the 3-D structure of the applicable surface to an image. The solution is tested by comparison with particular exact solutions. We present results for both the forward and the inverse 3D structure recovery problem."'),
('"Structured Image Segmentation Using Kernelized Features"', '"ECCV 2012"', '["Feature Vector", "Image Segmentation", "Unary Term", "Optimal Label", "Structural Kernel"]', '"https://doi.org/10.1007/978-3-642-33709-3_29"', '"Most state-of-the-art approaches to image segmentation formulate the problem using Conditional Random Fields. These models typically include a unary term and a pairwise term, whose parameters must be carefully chosen for optimal performance. Recently, structured learning approaches such as Structured SVMs (SSVM) have made it possible to jointly learn these model parameters. However, they have been limited to linear kernels, since more powerful non-linear kernels cause the learning to become prohibitively expensive. In this paper, we introduce an approach to \\u201ckernelize\\u201d the features so that a linear SSVM framework can leverage the power of non-linear kernels without incurring the high computational cost. We demonstrate the advantages of this approach in a series of image segmentation experiments on the MSRC data set as well as 2D and 3D datasets containing imagery of neural tissue acquired with electron microscopes."'),
('"Structured Output Ordinal Regression for Dynamic Facial Emotion Intensity Prediction"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15558-1_47"', '"We consider the task of labeling facial emotion intensities in videos, where the emotion intensities to be predicted have ordinal scales (e.g., low, medium, and high) that change in time. A significant challenge is that the rates of increase and decrease differ substantially across subjects. Moreover, the actual absolute differences of intensity values carry little information, with their relative order being more important. To solve the intensity prediction problem we propose a new dynamic ranking model that models the signal intensity at each time as a label on an ordinal scale and links the temporally proximal labels using dynamic smoothness constraints. This new model extends the successful static ordinal regression to a structured (dynamic) setting by using an analogy with Conditional Random Field (CRF) models in structured classification. We show that, although non-convex, the new model can be accurately learned using efficient gradient search. The predictions resulting from this dynamic ranking model show significant improvements over the regular CRFs, which fail to consider ordinal relationships between predicted labels. We also observe substantial improvements over static ranking models that do not exploit temporal dependencies of ordinal predictions. We demonstrate the benefits of our algorithm on the Cohn-Kanade dataset for the dynamic facial emotion intensity prediction problem and illustrate its performance in a controlled synthetic setting."'),
('"Structuring Visual Words in 3D for Arbitrary-View Object Localization"', '"ECCV 2008"', '["Graphic Processing Unit", "Visual Word", "Object Class", "Camera Parameter", "Exemplar Model"]', '"https://doi.org/10.1007/978-3-540-88690-7_54"', '"We propose a novel and efficient method for generic arbitrary-view object class detection and localization. In contrast to existing single-view and multi-view methods using complicated mechanisms for relating the structural information in different parts of the objects or different viewpoints, we aim at representing the structural information in their true 3D locations. Uncalibrated multi-view images from a hand-held camera are used to reconstruct the 3D visual word models in the training stage. In the testing stage, beyond bounding boxes, our method can automatically determine the locations and outlines of multiple objects in the test image with occlusion handling, and can accurately estimate both the intrinsic and extrinsic camera parameters in an optimized way. With exemplar models, our method can also handle shape deformation for intra-class variance. To handle large data sets from models, we propose several speedup techniques to make the prediction efficient. Experimental results obtained based on some standard data sets demonstrate the effectiveness of the proposed approach."'),
('"Student-t Mixture Filter for Robust, Real-Time Visual Tracking"', '"ECCV 2008"', '["Posterior Distribution", "Heavy Tail", "Approximate Inference", "High Order Cumulants", "Data Conf', '"https://doi.org/10.1007/978-3-540-88690-7_28"', '"Filtering is a key problem in modern information theory; from a series of noisy measurement, one would like to estimate the state of some system. A number of solutions exist in the literature, such as the Kalman filter or the various particle and hybrid filters, but each has its drawbacks."'),
('"Study of the Distinctiveness of Level 2 and Level 3 Features in Fragmentary Fingerprint Comparison"', '"BioAW 2004"', '[]', '"https://doi.org/10.1007/978-3-540-25976-3_12"', '"In this paper we present the results of an experiment which aims to provide an insight into the problems related to the fingerprint recognition from its fragment. Level 2 and Level 3 features are considered, and their distinctive potential is estimated in respect to the considered area of a fingerprint fragment. We conclude that the use of level 3 features can offer at least a comparable recognition potential from a small area fingerprint fragment, as the level 2 features offer for fragments of larger area."'),
('"Studying Aesthetics in Photographic Images Using a Computational Approach"', '"ECCV 2006"', '["Support Vector Machine", "Visual Feature", "Image Retrieval", "Photographic Image", "Aesthetic Qua', '"https://doi.org/10.1007/11744078_23"', '"Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities of photographs is a highly subjective task. Hence, there is no unanimously agreed standard for measuring aesthetic value. In spite of the lack of firm rules, certain features in photographic images are believed, by many, to please humans more than certain others. In this paper, we treat the challenge of automatically inferring aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated online photo sharing Website as data source. We extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images. Automated classifiers are built using support vector machines and classification trees. Linear regression on polynomial terms of the features is also applied to infer numerical aesthetics ratings. The work attempts to explore the relationship between emotions which pictures arouse in people, and their low-level content. Potential applications include content-based image retrieval and digital photography."'),
('"Sub-pixel Layout for Super-Resolution with Images in the Octic Group"', '"ECCV 2014"', '["Super-resolution", "CCD sensor", "Sub-pixel layout", "Octic group"]', '"https://doi.org/10.1007/978-3-319-10590-1_17"', '"This paper presents a novel super-resolution framework by exploring the properties of non-conventional pixel layouts and shapes. We show that recording multiple images, transformed in the octic group, with a sensor of asymmetric sub-pixel layout increases the spatial sampling compared to a conventional sensor with a rectilinear grid of pixels and hence increases the image resolution. We further prove a theoretical bound for achieving well-posed super-resolution with a designated magnification factor w.r.t. the number and distribution of sub-pixels. We also propose strategies for selecting good sub-pixel layouts and effective super-resolution algorithms for our setup. The experimental results validate the proposed theory and solution, which have the potential to guide the future CCD layout design with super-resolution functionality."'),
('"Submodular Relaxation for MRFs with High-Order Potentials"', '"ECCV 2012"', '["Markov random fields", "energy minimization", "MAP-inference", "dual decomposition", "high-order p', '"https://doi.org/10.1007/978-3-642-33885-4_31"', '"In the paper we propose a novel dual decomposition scheme for approximate MAP-inference in Markov Random Fields with sparse high-order potentials, i.e. potentials encouraging relatively a small number of variable configurations. We construct a Lagrangian dual of the problem in such a way that it can be efficiently evaluated by minimizing a submodular function with a min-cut/max-flow algorithm. We show the equivalence of this relaxation to a specific type of linear program and derive the conditions under which it is equivalent to generally tighter LP-relaxation solved in [1]. Unlike the latter our relaxation has significantly less dual variables and hence is much easier to solve. We demonstrate its faster convergence on several synthetic and real problems."'),
('"Subspace Classification for Face Recognition"', '"BioAW 2002"', '["Face Recognition", "Face Image", "Gabor Filter", "Learning Stage", "Subspace Learning"]', '"https://doi.org/10.1007/3-540-47917-1_14"', '"This paper introduces a new subspace classification approach for face recognition. One or more MKL subspaces are created for each individual, starting from the feature vectors extracted through a bank of Gabor filters. The advantages of this method with respect to other well-know approaches are experimentally proved; in particular, our subspace approach effectively captures the intra-class variability, thus allowing to better discriminate between known and unknown faces."'),
('"Subspace Estimation Using Projection Based M-Estimators over Grassmann Manifolds"', '"ECCV 2006"', '["Parallel Transport", "Conjugate Gradient Algorithm", "Grassmann Manifold", "Motion Segmentation", ', '"https://doi.org/10.1007/11744023_24"', '"We propose a solution to the problem of robust subspace estimation using the projection based M-estimator. The new method handles more outliers than inliers, does not require a user defined scale of the noise affecting the inliers, handles noncentered data and nonorthogonal subspaces. Other robust methods like RANSAC, use an input for the scale, while methods for subspace segmentation, like GPCA, are not robust. Synthetic data and three real cases of multibody factorization show the superiority of our method, in spite of user independence."'),
('"Subspace Learning in Krein Spaces: Complete Kernel Fisher Discriminant Analysis with Indefinite Ker', '"ECCV 2012"', '["subspace learning", "indefinite kernels", "face recognition"]', '"https://doi.org/10.1007/978-3-642-33765-9_35"', '"Positive definite kernels, such as Gaussian Radial Basis Functions (GRBF), have been widely used in computer vision for designing feature extraction and classification algorithms. In many cases non-positive definite (npd) kernels and non metric similarity/dissimilarity measures naturally arise (e.g., Hausdorff distance, Kullback Leibler Divergences and Compact Support (CS) Kernels). Hence, there is a practical and theoretical need to properly handle npd kernels within feature extraction and classification frameworks. Recently, classifiers such as Support Vector Machines (SVMs) with npd kernels, Indefinite Kernel Fisher Discriminant Analysis (IKFDA) and Indefinite Kernel Quadratic Analysis (IKQA) were proposed. In this paper we propose feature extraction methods using indefinite kernels. In particular, first we propose an Indefinite Kernel Principal Component Analysis (IKPCA). Then, we properly define optimization problems that find discriminant projections with indefinite kernels and propose a Complete Indefinite Kernel Fisher Discriminant Analysis (CIKFDA) that solves the proposed problems. We show the power of the proposed frameworks in a fully automatic face recognition scenario."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Subspace Procrustes Analysis"', '"ECCV 2014"', '["Mean Square Error", "Shape Model", "Deformable Model", "Rigid Transformation", "Active Appearance ', '"https://doi.org/10.1007/978-3-319-16178-5_46"', '"Procrustes Analysis (PA) has been a popular technique to align and build \\\\(2\\\\)-D statistical models of shapes. Given a set of \\\\(2\\\\)-D shapes PA is applied to remove rigid transformations. Then, a non-rigid \\\\(2\\\\)-D model is computed by modeling (e.g., PCA) the residual. Although PA has been widely used, it has several limitations for modeling \\\\(2\\\\)-D shapes: occluded landmarks and missing data can result in local minima solutions, and there is no guarantee that the \\\\(2\\\\)-D shapes provide a uniform sampling of the \\\\(3\\\\)-D space of rotations for the object. To address previous issues, this paper proposes Subspace PA (SPA). Given several instances of a \\\\(3\\\\)-D object, SPA computes the mean and a \\\\(2\\\\)-D subspace that can simultaneously model all rigid and non-rigid deformations of the \\\\(3\\\\)-D object. We propose a discrete (DSPA) and continuous (CSPA) formulation for SPA, assuming that \\\\(3\\\\)-D samples of an object are provided. DSPA extends the traditional PA, and produces unbiased \\\\(2\\\\)-D models by uniformly sampling different views of the \\\\(3\\\\)-D object. CSPA provides a continuous approach to uniformly sample the space of \\\\(3\\\\)-D rotations, being more efficient in space and time. Experiments using SPA to learn \\\\(2\\\\)-D models of bodies from motion capture data illustrate the benefits of our approach."'),
('"Subtraction-Based Forward Obstacle Detection Using Illumination Insensitive Feature for Driving-Sup', '"ECCV 2012"', '["Road Surface", "Illumination Condition", "Dynamic Time Warping", "Stereo Match", "Obstacle Detecti', '"https://doi.org/10.1007/978-3-642-33868-7_51"', '"This paper proposes a method for detecting general obstacles on a road by subtracting present and past in-vehicle camera images. The image-subtraction-based object detection approach can be applied to detect any kind of obstacles although the existing learning-based methods detect only specific obstacles. To detect general obstacles, the proposed method first computes a frame-by-frame correspondence between the present and the past in-vehicle camera image sequences, and then registrates road surfaces between the frames. Finally, obstacles are detected by applying image subtraction to the registrated road surface regions with an illumination insensitive feature for robust detection. Experiments were conducted by using several image sequences captured by an actual in-vehicle camera to confirm the effectiveness of the proposed method. The experimental results shows that the proposed method can detect general obstacles accurately at a distance enough to avoid them safely even in situations with different illuminations."'),
('"Super-Resolution of 3D Face"', '"ECCV 2006"', '["Face Database", "Tail Structure", "Superresolution Image", "Image Formation Process", "Hallucinate', '"https://doi.org/10.1007/11744047_30"', '"Super-resolution is a technique to restore the detailed information from the degenerated data. Lots of previous work is for 2D images while super-resolution of 3D models was little addressed. This paper focuses on the super-resolution of 3D human faces. We firstly extend the 2D image pyramid model to the progressive resolution chain (PRC) model in 3D domain, to describe the detail variation during resolution decreasing. Then a consistent planar representation of 3D faces is presented, which enables the analysis and comparison among the features of the same facial part for the subsequent restoration process. Finally, formulated as solving an iterative quadratic system by maximizing a posteriori, a 3D restoration algorithm using PRC features is given. The experimental results on USF HumanID 3D face database demonstrate the effectiveness of the proposed approach."'),
('"Super-Resolution-Based Inpainting"', '"ECCV 2012"', '["examplar-based inpainting", "super-resolution"]', '"https://doi.org/10.1007/978-3-642-33783-3_40"', '"This paper introduces a new examplar-based inpainting framework. A coarse version of the input image is first inpainted by a non-parametric patch sampling. Compared to existing approaches, some improvements have been done (e.g. filling order computation, combination of K nearest neighbours). The inpainted of a coarse version of the input image allows to reduce the computational complexity, to be less sensitive to noise and to work with the dominant orientations of image structures. From the low-resolution inpainted image, a single-image super-resolution is applied to recover the details of missing areas. Experimental results on natural images and texture synthesis demonstrate the effectiveness of the proposed method."'),
('"Superfaces: A Super-Resolution Model for 3D Faces"', '"ECCV 2012"', '["Root Mean Square Error", "Face Model", "Average Root Mean Square Error", "Super Resolution", "Faci', '"https://doi.org/10.1007/978-3-642-33863-2_8"', '"Face recognition based on the analysis of 3D scans has been an active research subject over the last few years. However, the impact of the resolution of 3D scans on the recognition process has not been addressed explicitly yet being of primal importance after the introduction of a new generation of low cost 4D scanning devices. These devices are capable of combined depth/rgb acquisition over time with a low resolution compared to the 3D scanners typically used in 3D face recognition benchmarks. In this paper, we define a super-resolution model for 3D faces by which a sequence of low-resolution 3D scans can be processed to extract a higher resolution 3D face model, namely the superface model. The proposed solution relies on the Scaled ICP procedure to align the low-resolution 3D models with each other and estimate the value of the high-resolution 3D model based on the statistics of values of the low-resolution scans in corresponding points. The approach is validated on a data set that includes, for each subject, one sequence of low-resolution 3D face scans and one ground-truth high-resolution 3D face model acquired through a high-resolution 3D scanner. In this way, results of the super-resolution process are evaluated qualitatively and quantitatively by measuring the error between the superface and the ground-truth."'),
('"SuperFloxels: A Mid-level Representation for Video Sequences"', '"ECCV 2012"', '["Video Sequence", "Cluster Center", "Motion Boundary", "Object Segmentation", "Motion Segmentation"', '"https://doi.org/10.1007/978-3-642-33885-4_14"', '"We describe an approach for grouping trajectories extracted from a video that preserves motion discontinuities due, for instance, to occlusions, but not color or intensity boundaries. Our method takes as input trajectories with variable length and onset time, and outputs a membership function as well as an indicator function denoting the exemplar trajectory of each group. This can be used for several applications such as compression, segmentation, and background removal."'),
('"SuperParsing: Scalable Nonparametric Image Parsing with Superpixels"', '"ECCV 2010"', '["Training Image", "Query Image", "Markov Random Field", "Semantic Label", "Boost Decision Tree"]', '"https://doi.org/10.1007/978-3-642-15555-0_26"', '"This paper presents a simple and effective nonparametric approach to the problem of image parsing, or labeling image regions (in our case, superpixels produced by bottom-up segmentation) with their categories. This approach requires no training, and it can easily scale to datasets with tens of thousands of images and hundreds of labels. It works by scene-level matching with global image descriptors, followed by superpixel-level matching with local features and efficient Markov random field (MRF) optimization for incorporating neighborhood context. Our MRF setup can also compute a simultaneous labeling of image regions into semantic classes (e.g., tree, building, car) and geometric classes (sky, vertical, ground). Our system outperforms the state-of-the-art nonparametric method based on SIFT Flow on a dataset of 2,688 images and 33 labels. In addition, we report per-pixel rates on a larger dataset of 15,150 images and 170 labels. To our knowledge, this is the first complete evaluation of image parsing on a dataset of this size, and it establishes a new benchmark for the problem."'),
('"Superpixel Graph Label Transfer with Learned Distance Metric"', '"ECCV 2014"', '["Feature Vector", "Training Image", "Learn Distance", "Graph Construction", "Dataset Size"]', '"https://doi.org/10.1007/978-3-319-10590-1_41"', '"We present a fast approximate nearest neighbor algorithm for semantic segmentation. Our algorithm builds a graph over superpixels from an annotated set of training images. Edges in the graph represent approximate nearest neighbors in feature space. At test time we match superpixels from a novel image to the training images by adding the novel image to the graph. A move-making search algorithm allows us to leverage the graph and image structure for finding matches. We then transfer labels from the training images to the image under test. To promote good matches between superpixels we propose to learn a distance metric that weights the edges in our graph. Our approach is evaluated on four standard semantic segmentation datasets and achieves results comparable with the state-of-the-art."'),
('"Superpixels and Supervoxels in an Energy Optimization Framework"', '"ECCV 2010"', '["Patch Size", "Salient Object", "Variable Patch", "Superpixel Segmentation", "Ground Truth Segment"', '"https://doi.org/10.1007/978-3-642-15555-0_16"', '"Many methods for object recognition, segmentation, etc., rely on a tessellation of an image into \\u201csuperpixels\\u201d. A superpixel is an image patch which is better aligned with intensity edges than a rectangular patch. Superpixels can be extracted with any segmentation algorithm, however, most of them produce highly irregular superpixels, with widely varying sizes and shapes. A more regular space tessellation may be desired. We formulate the superpixel partitioning problem in an energy minimization framework, and optimize with graph cuts. Our energy function explicitly encourages regular superpixels. We explore variations of the basic energy, which allow a trade-off between a less regular tessellation but more accurate boundaries or better efficiency. Our advantage over previous work is computational efficiency, principled optimization, and applicability to 3D \\u201csupervoxel\\u201d segmentation. We achieve high boundary recall on images and spatial coherence on video. We also show that compact superpixels improve accuracy on a simple application of salient object segmentation."'),
('"Supervised and Unsupervised Clustering with Probabilistic Shift"', '"ECCV 2010"', '["Spectral Cluster", "Transition Probability Matrix", "Unsupervised Cluster", "Cluster Core", "Shift', '"https://doi.org/10.1007/978-3-642-15555-0_47"', '"We present a novel scale adaptive, nonparametric approach to clustering point patterns. Clusters are detected by moving all points to their cluster cores using shift vectors. First, we propose a novel scale selection criterion based on local density isotropy which determines the neighborhoods over which the shift vectors are computed. We then construct a directed graph induced by these shift vectors. Clustering is obtained by simulating random walks on this digraph. We also examine the spectral properties of a similarity matrix obtained from the directed graph to obtain a K-way partitioning of the data. Additionally, we use the eigenvector alignment algorithm of [1] to automatically determine the number of clusters in the dataset. We also compare our approach with supervised[2] and completely unsupervised spectral clustering[1], normalized cuts[3], K-Means, and adaptive bandwidth meanshift[4] on MNIST digits, USPS digits and UCI machine learning data."'),
('"Supervised Assessment of Segmentation Hierarchies"', '"ECCV 2012"', '["Ground Truth", "Random Tree", "Boundary Pixel", "Boundary Match", "Quad Tree"]', '"https://doi.org/10.1007/978-3-642-33765-9_58"', '"This paper addresses the problem of the supervised assessment of hierarchical region-based image representations. Given the large amount of partitions represented in such structures, the supervised assessment approaches in the literature are based on selecting a reduced set of representative partitions and evaluating their quality. Assessment results, therefore, depend on the partition selection strategy used. Instead, we propose to find the partition in the tree that best matches the ground-truth partition, that is, the upper-bound partition selection. We show that different partition selection algorithms can lead to different conclusions regarding the quality of the assessed trees and that the upper-bound partition selection provides the following advantages: 1) it does not limit the assessment to a reduced set of partitions, and 2) it better discriminates the random trees from actual ones, which reflects a better qualitative behavior. We model the problem as a Linear Fractional Combinatorial Optimization (LFCO) problem, which makes the upper-bound selection feasible and efficient."'),
('"Supervised Descriptor Learning for Non-Rigid Shape Matching"', '"ECCV 2014"', '["Shape matching", "Correspondences", "Feature learning"]', '"https://doi.org/10.1007/978-3-319-16220-1_20"', '"We present a novel method for computing correspondences between pairs of non-rigid shapes. Unlike the majority of existing techniques that assume a deformation model, such as intrinsic isometries, a priori and use a pre-defined set of point or part descriptors, we consider the problem of learning a correspondence model given a collection of reference pairs with known mappings between them. Our formulation is purely intrinsic and does not rely on a consistent parametrization or spatial positions of vertices on the shapes. Instead, we consider the problem of finding the optimal set of descriptors that can be jointly used to reproduce the given reference maps. We show how this problem can be formalized and solved for efficiently by using the recently proposed functional maps framework. Moreover, we demonstrate how to extract the functional subspaces that can be mapped reliably across shapes. This gives us a way to not only obtain better functional correspondences, but also to associate a confidence value to the different parts of the mappings. We demonstrate the efficiency and usefulness of the proposedapproach on a variety of challenging shape matching tasks."'),
('"Supervised Earth Mover\\u2019s Distance Learning and Its Computer Vision Applications"', '"ECCV 2012"', '["Face Recognition", "Face Image", "Image Retrieval", "Reference Identity", "Computer Vision Applica', '"https://doi.org/10.1007/978-3-642-33718-5_32"', '"The Earth Mover\\u2019s Distance (EMD) is an intuitive and natural distance metric for comparing two histograms or probability distributions. It provides a distance value as well as a flow-network indicating how the probability mass is optimally transported between the bins. In traditional EMD, the ground distance between the bins is pre-defined. Instead, we propose to jointly optimize the ground distance matrix and the EMD flow-network based on a partial ordering of histogram distances in an optimization framework. Our method is further extended to accept information from general labeled pairs. The trained ground distance better reflects the cross-bin relationships, hence produces more accurate EMD values and flow-networks. Two computer vision applications are used to demonstrate the effectiveness of the algorithm: first, we apply the optimized EMD value to face verification, and achieve state-of-the-art performance on the PubFig and the LFW data sets; second, the learned EMD flow-network is used to analyze face attribute changes, obtaining consistent paths that demonstrate intuitive transitions on certain facial attributes."'),
('"Supervised Geodesic Propagation for Semantic Label Transfer"', '"ECCV 2012"', '["Input Image", "Geodesic Distance", "Similar Image", "Propagation Indicator", "Seed Selection"]', '"https://doi.org/10.1007/978-3-642-33712-3_40"', '"In this paper we propose a novel semantic label transfer method using supervised geodesic propagation (SGP). We use supervised learning to guide the seed selection and the label propagation. Given an input image, we first retrieve its similar image set from annotated databases. A Joint Boost model is learned on the similar image set of the input image. Then the recognition proposal map of the input image is inferred by this learned model. The initial distance map is defined by the proposal map: the higher probability, the smaller distance. In each iteration step of the geodesic propagation, the seed is selected as the one with the smallest distance from the undetermined superpixels. We learn a classifier as an indicator to indicate whether to propagate labels between two neighboring superpixels. The training samples of the indicator are annotated neighboring pairs from the similar image set. The geodesic distances of its neighbors are updated according to the combination of the texture and boundary features and the indication value. Experiments on three datasets show that our method outperforms the traditional learning based methods and the previous label transfer method for the semantic segmentation work."'),
('"Supervised Label Transfer for Semantic Segmentation of Street Scenes"', '"ECCV 2010"', '[]', '"https://doi.org/10.1007/978-3-642-15555-0_41"', '"In this paper, we propose a robust supervised label transfer method for the semantic segmentation of street scenes. Given an input image of street scene, we first find multiple image sets from the training database consisting of images with annotation, each of which can cover all semantic categories in the input image. Then, we establish dense correspondence between the input image and each found image sets with a proposed KNN-MRF matching scheme. It is followed by a matching correspondences classification that tries to reduce the number of semantically incorrect correspondences with trained matching correspondences classification models for different categories. With those matching correspondences classified as semantically correct correspondences, we infer the confidence values of each super pixel belonging to different semantic categories, and integrate them and spatial smoothness constraint in a markov random field to segment the input image. Experiments on three datasets show our method outperforms the traditional learning based methods and the previous nonparametric label transfer method, for the semantic segmentation of street scenes."'),
('"Supervoxel-Consistent Foreground Propagation in Video"', '"ECCV 2014"', '["Markov Random Field", "Foreground Object", "Object Segmentation", "Foreground Region", "Video Segm', '"https://doi.org/10.1007/978-3-319-10593-2_43"', '"A major challenge in video segmentation is that the foreground object may move quickly in the scene at the same time its appearance and shape evolves over time. While pairwise potentials used in graph-based algorithms help smooth labels between neighboring (super)pixels in space and time, they offer only a myopic view of consistency and can be misled by inter-frame optical flow errors. We propose a higher order supervoxel label consistency potential for semi-supervised foreground segmentation. Given an initial frame with manual annotation for the foreground object, our approach propagates the foreground region through time, leveraging bottom-up supervoxels to guide its estimates towards long-range coherent regions. We validate our approach on three challenging datasets and achieve state-of-the-art results."'),
('"Support Blob Machines"', '"ECCV 2004"', '["Support Vector Machine", "Support Vector", "Loss Function", "Quadratic Programming", "Support Vect', '"https://doi.org/10.1007/978-3-540-24673-2_2"', '"A novel generalization of linear scale space is presented. The generalization allows for a sparse approximation of the function at a certain scale."'),
('"Support Vector Guided Dictionary Learning"', '"ECCV 2014"', '["Dictionary learning", "support vector machine", "sparse representation", "Fisher discrimination"]', '"https://doi.org/10.1007/978-3-319-10593-2_41"', '"Discriminative dictionary learning aims to learn a dictionary from training samples to enhance the discriminative capability of their coding vectors. Several discrimination terms have been proposed by assessing the prediction loss (e.g., logistic regression) or class separation criterion (e.g., Fisher discrimination criterion) on the coding vectors. In this paper, we provide a new insight on discriminative dictionary learning. Specifically, we formulate the discrimination term as the weighted summation of the squared distances between all pairs of coding vectors. The discrimination term in the state-of-the-art Fisher discrimination dictionary learning (FDDL) method can be explained as a special case of our model, where the weights are simply determined by the numbers of samples of each class. We then propose a parameterization method to adaptively determine the weight of each coding vector pair, which leads to a support vector guided dictionary learning (SVGDL) model. Compared with FDDL, SVGDL can adaptively assign different weights to different pairs of coding vectors. More importantly, SVGDL automatically selects only a few critical pairs to assign non-zero weights, resulting in better generalization ability for pattern recognition tasks. The experimental results on a series of benchmark databases show that SVGDL outperforms many state-of-the-art discriminative dictionary learning methods."'),
('"SURF: Speeded Up Robust Features"', '"ECCV 2006"', '["Hessian Matrix", "Interest Point", "Integral Image", "Robust Feature", "Viewpoint Change"]', '"https://doi.org/10.1007/11744023_32"', '"In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster."'),
('"Surface Extraction from Volumetric Images Using Deformable Meshes: A Comparative Study"', '"ECCV 2002"', '["External Force", "Active Contour", "Synthetic Image", "Deformable Model", "Active Contour Model"]', '"https://doi.org/10.1007/3-540-47977-5_23"', '"Deformable models are by their formulation able to solve surface extraction problem from noisy volumetric images. This is since they use image independent information, in form of internal energy or internal forces, in addition to image data to achieve the goal. However, it is not a simple task to deform initially given surface meshes to a good representation of the target surface in the presence of noise. Several methods to do this have been proposed and in this study a few recent ones are compared. Basically, we supply an image and an arbitrary but reasonable initialization and examine how well the target surface is captured with different methods for controlling the deformation of the mesh. Experiments with synthetic images as well as medical images are performed and results are reported and discussed. With synthetic images, the quality of results is measured also quantitatively. No optimal method was found, but the properties of different methods in distinct situations were highlighted."'),
('"Surface Matching and Registration by Landmark Curve-Driven Canonical Quasiconformal Mapping"', '"ECCV 2014"', '["Conformal Mapping", "Quasiconformal Mapping", "Conformal Module", "Registration Method", "Shape Re', '"https://doi.org/10.1007/978-3-319-10590-1_46"', '"This work presents a novel surface matching and registration method based on the landmark curve-driven canonical surface quasiconformal mapping, where an open genus zero surface decorated with landmark curves is mapped to a canonical domain with horizontal or vertical straight segments and the local shapes are preserved as much as possible. The key idea of the canonical mapping is to minimize the harmonic energy with the landmark curve straightening constraints and generate a quasi-holomorphic 1-form which is zero in one parameter along landmark and results in a quasiconformal mapping. The mapping exists and is unique and intrinsic to surface and landmark geometry. The novel shape representation provides a conformal invariant shape signature. We use it as Teichm\\u00fcller coordinates to construct a subspace of the conventional Teichm\\u00fcller space which considers geometry feature details and therefore increases the discriminative ability for matching. Furthermore, we present a novel and efficient registration method for surfaces with landmark curve constraints by computing an optimal mapping over the canonical domains with straight segments, where the curve constraints become linear forms. Due to the linearity of 1-form and harmonic map, the algorithms are easy to compute, efficient and practical. Experiments on human face and brain surfaces demonstrate the efficiency and efficacy and the potential for broader shape analysis applications."'),
('"Surface Matching with Large Deformations and Arbitrary Topology: A Geodesic Distance Evolution Sche', '"ECCV 2000"', '["Large Deformation", "Topological Change", "Geodesic Distance", "Eulerian Formulation", "Projected ', '"https://doi.org/10.1007/3-540-45054-8_50"', '"A general formulation for geodesic distance propagation of surfaces is presented. Starting from a surface lying on a 3-manifold in IR4, we set up a partial differential equation governing the propagation of surfaces at equal geodesic distance (on the 3-manifold) from the given original surface. This propagation scheme generalizes a result of Kimmel et al. [11] and provides a way to compute distance maps on manifolds. Moreover, the propagation equation is generalized to any number of dimensions. Using an eulerian formulation with level-sets, it gives stable numerical algorithms for computing distance maps. This theory is used to present a new method for surface matching which generalizes a curve matching method [5]. Matching paths are obtained as the orbits of the vector field defined as the sum of two distance maps\\u2019 gradient values. This surface matching technique applies to the case of large deformation and topological changes."'),
('"Surface Normal Deconvolution: Photometric Stereo for Optically Thick Translucent Objects"', '"ECCV 2014"', '["Convolution Kernel", "Blind Deconvolution", "Photometric Stereo", "Translucent Material", "Subsurf', '"https://doi.org/10.1007/978-3-319-10605-2_23"', '"This paper presents a photometric stereo method that works for optically thick translucent objects exhibiting subsurface scattering. Our method is built upon the previous studies showing that subsurface scattering is approximated as convolution with a blurring kernel. We extend this observation and show that the original surface normal convolved with the scattering kernel corresponds to the blurred surface normal that can be obtained by a conventional photometric stereo technique. Based on this observation, we cast the photometric stereo problem for optically thick translucent objects as a deconvolution problem, and develop a method to recover accurate surface normals. Experimental results of both synthetic and real-world scenes show the effectiveness of the proposed method."'),
('"Surface Reconstruction by Propagating 3D Stereo Data in Multiple 2D Images"', '"ECCV 2004"', '["Computer Vision", "Surface Reconstruction", "Multiple Image", "Surface Patch", "Epipolar Line"]', '"https://doi.org/10.1007/978-3-540-24670-1_13"', '"We present a novel approach to surface reconstruction from multiple images. The central idea is to explore the integration of both 3D stereo data and 2D calibrated images. This is motivated by the fact that only robust and accurate feature points that survived the geometry scrutiny of multiple images are reconstructed in space. The density insufficiency and the inevitable holes in the stereo data should be filled in by using information from multiple images. The idea is therefore to first construct small surface patches from stereo points, then to progressively propagate only reliable patches in their neighborhood from images into the whole surface using a best-first strategy. The problem reduces to searching for an optimal local surface patch going through a given set of stereo points from images. This constrained optimization for a surface patch could be handled by a local graph-cut that we develop. Real experiments demonstrate the usability and accuracy of the approach."'),
('"Surface Reconstruction of Plant Shoots from Multiple Views"', '"ECCV 2014"', '["Plant phenotyping", "Multi-view reconstruction", "3D", "Level sets"]', '"https://doi.org/10.1007/978-3-319-16220-1_12"', '"Increased adoption of the systems approach to biological research has focused attention on the use of quantitative models of biological objects. This includes a need for realistic 3D representations of plant shoots for quantification and modelling. We present a fully automatic approach to image-based 3D plant reconstruction. The reconstructed plants are represented as a series of small planar sections that together model the more complex architecture of the leaf surfaces. The boundary of each leaf patch is refined using the level set method, optimising the model based on image information, curvature constraints and the position of neighbouring surfaces. The reconstruction process makes few assumptions about the nature of the plant material being reconstructed, and as such is applicable to a wide variety of plant species and topologies, and can be extended to canopy-scale imaging. We demonstrate the effectiveness of our approach on datasets of wheat and rice plants, as well as a novel virtual dataset that allows us to compute distance measures of reconstruction accuracy."'),
('"Surface Visibility Probabilities in 3D Cluttered Scenes"', '"ECCV 2008"', '["Partial Occlusion", "Surface Visibility", "Sphere Center", "Image Speed", "Visibility Probability"', '"https://doi.org/10.1007/978-3-540-88682-2_31"', '"Many methods for 3D reconstruction in computer vision rely on probability models, for example, Bayesian reasoning. Here we introduce a probability model of surface visibilities in densely cluttered 3D scenes. The scenes consist of a large number of small surfaces distributed randomly in a 3D view volume. An example is the leaves or branches on a tree. We derive probabilities for surface visibility, instantaneous image velocity under egomotion, and binocular half\\u2013occlusions in these scenes. The probabilities depend on parameters such as scene depth, object size, 3D density, observer speed, and binocular baseline. We verify the correctness of our models using computer graphics simulations, and briefly discuss applications of the model to stereo and motion."'),
('"Surviving Dominant Planes in Uncalibrated Structure and Motion Recovery"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47967-8_56"', '"In this paper we address the problem of uncalibrated structure and motion recovery from image sequences that contain dominant planes in some of the views. Traditional approaches fail when the features common to three consecutive views are all located on a plane. This happens because in the uncalibrated case there is a fundamental ambiguity in relating the structure before and after the plane. This is, however, a situation that is often hard to avoid in man-made environments. We propose a complete approach that detects the problem and defers the computation of parameters that are ambiguous in projective space (i.e. the registration between partial reconstructions only sharing a common plane and poses of cameras only seeing planar features) till after self-calibration. Also a new linear self-calibration algorithm is proposed that couples the intrinsics between multiple subsequences. The final result is a complete metric 3D reconstruction of both structure and motion for the whole sequence. Experimental results on real image sequences show that the approach yields very good results."'),
('"Symmetric Geodesic Shape Averaging and Shape Interpolation"', '"MMBIA 2004"', '["Geodesic Distance", "Registration Algorithm", "Shape Average", "Average Transformation", "Eulerian', '"https://doi.org/10.1007/978-3-540-27816-0_9"', '"Structural image registration is often achieved through diffeomorphic transformations. The formalism associated with the diffeomorphic framework allows one to define curved distances which are often more appropriate for morphological comparisons of anatomy. However, the correspondence problem as well as the metric distances across the database depend upon the chosen reference anatomy, requiring average transformations to be estimated. The goal of this paper is to develop an algorithm which, given a database of images, estimates an average shape based on the geodesic distances of curved, time-dependent transformations. Specifically, this paper will develop direct, efficient, symmetric methods for generating average anatomical shapes from diffeomorphic registration algorithms. The need for these types of averages is illustrated with synthetic examples and the novel algorithm is compared to the usual approach of averaging linear transformations. Furthermore, the same algorithm will be used for shape interpolation that is independent of the multi-scale framework used."'),
('"Symmetric Sub-pixel Stereo Matching"', '"ECCV 2002"', '["Stereo Match", "Match Cost", "Stereo Algorithm", "Occlude Area", "Stereo Match Algorithm"]', '"https://doi.org/10.1007/3-540-47967-8_35"', '"Two central issues in stereo algorithm design are the matching criterion and the underlying smoothness assumptions. In this paper we propose a new stereo algorithm with novel approaches to both issues. We start with a careful analysis of the properties of the continuous disparity space image (DSI), and derive a new matching cost based on the reconstructed image signals. We then use a symmetric matching process that employs visibility constraints to assign disparities to a large fraction of pixels with minimal smoothness assumptions. While the matching operates on integer disparities, sub-pixel information is maintained throughout the process. Global smoothness assumptions are delayed until a later stage in which disparities are assigned in textureless and occluded areas. We validate our approach with experimental results on stereo images with ground truth."'),
('"Symmetrical Dense Optical Flow Estimation with Occlusions Detection"', '"ECCV 2002"', '["Optical Flow", "Real Image", "Smoothness Constraint", "Occlude Region", "Symmetric Method"]', '"https://doi.org/10.1007/3-540-47969-4_48"', '"Traditional techniques of dense optical flow estimation don\\u2019t generally yield symmetrical solutions: the results will differ if they are applied between images I 1 and I 2 or between images I 2 and I 1. In this work, we present a method to recover a dense optical flow field map from two images, while explicitely taking into account the symmetry across the images as well as possible occlusions and discontinuities in the flow field. The idea is to consider both displacements vectors from I 1 to I 2 and I 2 to I 1 and to minimise an energy functional that explicitely encodes all those properties. This variational problem is then solved using the gradient flow defined by the Euler-Lagrange equations associated to the energy. In order to reduce the risk to be trapped within some irrelevant minimum, a focusing strategy based on a multi-resolution technique is used to converge toward the solution. Promising experimental results on both synthetic and real images are presented to illustrate the capabilities of this symmetrical variational approach to recover accurate optical flow."'),
('"Synchronization of Two Independently Moving Cameras without Feature Correspondences"', '"ECCV 2014"', '["Video Sequence", "Intrinsic Parameter", "Epipolar Geometry", "Synchronization Algorithm", "Homogen', '"https://doi.org/10.1007/978-3-319-10590-1_13"', '"In this work, a method that synchronizes two video sequences is proposed. Unlike previous methods, which require the existence of correspondences between features tracked in the two sequences, and/or that the cameras are static or jointly moving, the proposed approach does not impose any of these constraints. It works when the cameras move independently, even if different features are tracked in the two sequences. The assumptions underlying the proposed strategy are that the intrinsic parameters of the cameras are known and that two rigid objects, with independent motions on the scene, are visible in both sequences. The relative motion between these objects is used as clue for the synchronization. The extrinsic parameters of the cameras are assumed to be unknown. A new synchronization algorithm for static or jointly moving cameras that see (possibly) different parts of a common rigidly moving object is also proposed. Proof-of-concept experiments that illustrate the performance of these methods are presented, as well as a comparison with a state-of-the-art approach."'),
('"Synthesizing Dynamic Texture with Closed-Loop Linear Dynamic System"', '"ECCV 2004"', '["Unit Circle", "Visual Quality", "Hide State", "Linear Dynamic System", "Texture Synthesis"]', '"https://doi.org/10.1007/978-3-540-24671-8_48"', '"Dynamic texture can be defined as a temporally continuous and infinitely varying stream of images that exhibit certain temporal statistics. Linear dynamic system (LDS) represented by the state-space equation has been proposed to model dynamic texture [12]. LDS can be used to synthesize dynamic texture by sampling the system noise. However, the visual quality of the synthesized dynamic texture using noise-driven LDS is often unsatisfactory. In this paper, we regard the noise-driven LDS as an open-loop control system and analyze its stability through its pole placement. We show that the noise-driven LDS can produce good quality dynamic texture if the LDS is oscillatory. To deal with an LDS not oscillatory, we present a novel approach, called closed-loop LDS (CLDS) where feedback control is introduced into the system. Using the succeeding hidden states as an input reference signal, we design a feedback controller based on the difference between the current state and the reference state. An iterative algorithm is proposed to generate dynamic textures. Experimental results demonstrate that CLDS can produce dynamic texture sequences with promising visual quality."'),
('"Taking Mobile Multi-object Tracking to the Next Level: People, Unknown Objects, and Carried Items"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33715-4_41"', '"In this paper, we aim to take mobile multi-object tracking to the next level. Current approaches work in a tracking-by-detection framework, which limits them to object categories for which pre-trained detector models are available. In contrast, we propose a novel tracking-before-detection approach that can track both known and unknown object categories in very challenging street scenes. Our approach relies on noisy stereo depth data in order to segment and track objects in 3D. At its core is a novel, compact 3D representation that allows us to robustly track a large variety of objects, while building up models of their 3D shape online. In addition to improving tracking performance, this representation allows us to detect anomalous shapes, such as carried items on a person\\u2019s body. We evaluate our approach on several challenging video sequences of busy pedestrian zones and show that it outperforms state-of-the-art approaches."'),
('"Taxonomic Multi-class Prediction and Person Layout Using Efficient Structured Ranking"', '"ECCV 2012"', '["Ordinal Regression", "Layout Problem", "Structure Output", "Structure Ranking", "Computer Vision P', '"https://doi.org/10.1007/978-3-642-33709-3_18"', '"In computer vision efficient multi-class classification is becoming a key problem as the field develops and the number of object classes to be identified increases. Often objects might have some sort of structure such as a taxonomy in which the mis-classification score for object classes close by, using tree distance within the taxonomy, should be less than for those far apart. This is an example of multi-class classification in which the loss function has a special structure. Another example in vision is for the ubiquitous pictorial structure or parts based model. In this case we would like the mis-classification score to be proportional to the number of parts misclassified."'),
('"Team Activity Recognition in Sports"', '"ECCV 2012"', '["Support Vector Machine", "Poisson Equation", "Activity Recognition", "Test Frame", "Team Activity"', '"https://doi.org/10.1007/978-3-642-33786-4_6"', '"We introduce a novel approach for team activity recognition in sports. Given the positions of team players from a plan view of the playing field at any given time, we solve a particular Poisson equation to generate a smooth distribution defined on whole playground, termed the position distribution of the team. Computing the position distribution for each frame provides a sequence of distributions, which we process to extract motion features for team activity recognition. The motion features are obtained at each frame using frame differencing and optical flow. We investigate the use of the proposed motion descriptors with Support Vector Machines (SVM) classification, and evaluate on a publicly available European handball dataset. Results show that our approach can classify six different team activities and performs better than a method that extracts features from the explicitly defined positions. Our method is new and different from other trajectory-based methods. These methods extract activity features using the explicitly defined trajectories, where the players have specific positions at any given time, and ignore the rest of the playground. In our work, on the other hand, given the specific positions of the team players at a frame, we construct a position distribution for the team on the whole playground and process the sequence of position distribution images to extract motion features for activity recognition. Results show that our approach is effective."'),
('"Technical Demonstration on Model Based Training, Detection and Pose Estimation of Texture-Less 3D O', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_60"', '"In this technical demonstration, we will show our framework of automatic modeling, detection, and tracking of arbitrary texture-less 3D objects with a Kinect. The detection is mainly based on the recent template-based LINEMOD approach [1] while the automatic template learning from reconstructed 3D models, the fast pose estimation and the quick and robust false positive removal is a novel addition."'),
('"Temporal Dithering of Illumination for Fast Active Vision"', '"ECCV 2008"', '["High Speed Camera", "Active Vision", "Lighting Direction", "Input Intensity", "Dynamic Scene"]', '"https://doi.org/10.1007/978-3-540-88693-8_61"', '"Active vision techniques use programmable light sources, such as projectors, whose intensities can be controlled over space and time. We present a broad framework for fast active vision using Digital Light Processing (DLP) projectors. The digital micromirror array (DMD) in a DLP projector is capable of switching mirrors \\u201con\\u201d and \\u201coff\\u201d at high speeds (106/s). An off-the-shelf DLP projector, however, effectively operates at much lower rates (30-60Hz) by emitting smaller intensities that are integrated over time by a sensor (eye or camera) to produce the desired brightness value. Our key idea is to exploit this \\u201ctemporal dithering\\u201d of illumination, as observed by a high-speed camera. The dithering encodes each brightness value uniquely and may be used in conjunction with virtually any active vision technique. We apply our approach to five well-known problems: (a) structured light-based range finding, (b) photometric stereo, (c) illumination de-multiplexing, (d) high frequency preserving motion-blur and (e) separation of direct and global scene components, achieving significant speedups in performance. In all our methods, the projector receives a single image as input whereas the camera acquires a sequence of frames."'),
('"Temporal Factorization vs. Spatial Factorization"', '"ECCV 2004"', '["Video Clip", "Temporal Factorization", "Spectral Cluster", "Spatial Factorization", "Temporal Clus', '"https://doi.org/10.1007/978-3-540-24671-8_34"', '"The traditional subspace-based approaches to segmentation (often referred to as multi-body factorization approaches) provide spatial clustering/segmentation by grouping together points moving with consistent motions. We are exploring a dual approach to factorization, i.e., obtaining temporal clustering/segmentation by grouping together frames capturing consistent shapes. Temporal cuts are thus detected at non-rigid changes in the shape of the scene/object. In addition it provides a clustering of the frames with consistent shape (but not necessarily same motion). For example, in a sequence showing a face which appears serious at some frames, and is smiling in other frames, all the \\u201cserious expression\\u201d frames will be grouped together and separated from all the \\u201csmile\\u201d frames which will be classified as a second group, even though the head may meanwhile undergo various random motions."'),
('"Temporal Surface Tracking Using Mesh Evolution"', '"ECCV 2008"', '["Topological Change", "Geodesic Distance", "Visual Hull", "Surface Tracking", "Surface Protrusion"]', '"https://doi.org/10.1007/978-3-540-88688-4_3"', '"In this paper, we address the problem of surface tracking in multiple camera environments and over time sequences. In order to fully track a surface undergoing significant deformations, we cast the problem as a mesh evolution over time. Such an evolution is driven by 3D displacement fields estimated between meshes recovered independently at different time frames. Geometric and photometric information is used to identify a robust set of matching vertices. This provides a sparse displacement field that is densified over the mesh by Laplacian diffusion. In contrast to existing approaches that evolve meshes, we do not assume a known model or a fixed topology. The contribution is a novel mesh evolution based framework that allows to fully track, over long sequences, an unknown surface encountering deformations, including topological changes. Results on very challenging and publicly available image based 3D mesh sequences demonstrate the ability of our framework to efficiently recover surface motions ."'),
('"Ten Years of Pedestrian Detection, What Have We Learned?"', '"ECCV 2014"', '["Object Detection", "Deep Learning", "Convolutional Neural Network", "Human Detection", "Pedestrian', '"https://doi.org/10.1007/978-3-319-16181-5_47"', '"Paper-by-paper results make it easy to miss the forest for the trees. We analyse the remarkable progress of the last decade by discussing the main ideas explored in the 40+ detectors currently present in the Caltech pedestrian detection benchmark. We observe that there exist three families of approaches, all currently reaching similar detection quality. Based on our analysis, we study the complementarity of the most promising ideas by combining multiple published strategies. This new decision forest detector achieves the current best known performance on the challenging Caltech-USA dataset."'),
('"Tensor Field Segmentation Using Region Based Active Contour Model"', '"ECCV 2004"', '["Active Contour", "Structure Tensor", "Active Contour Model", "Texture Segmentation", "Move Object ', '"https://doi.org/10.1007/978-3-540-24673-2_25"', '"Tensor fields (matrix valued data sets) have recently attracted increased attention in the fields of image processing, computer vision, visualization and medical imaging. Tensor field segmentation is an important problem in tensor field analysis and has not been addressed adequately in the past. In this paper, we present an effective region-based active contour model for tensor field segmentation and show its application to diffusion tensor magnetic resonance images (MRI) as well as for the texture segmentation problem in computer vision. Specifically, we present a variational principle for an active contour using the Euclidean difference of tensors as a discriminant. The variational formulation is valid for piecewise smooth regions, however, for the sake of simplicity of exposition, we present the piecewise constant region model in detail. This variational principle is a generalization of the region-based active contour to matrix valued functions. It naturally leads to a curve evolution equation for tensor field segmentation, which is subsequently expressed in a level set framework and solved numerically. Synthetic and real data experiments involving the segmentation of diffusion tensor MRI as well as structure tensors obtained from real texture data are shown to depict the performance of the proposed model."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Tensor Sparse Coding for Region Covariances"', '"ECCV 2010"', '["Face Recognition", "Sparse Representation", "Sparse Code", "Region Covariance", "Pedestrian Detect', '"https://doi.org/10.1007/978-3-642-15561-1_52"', '"Sparse representation of signals has been the focus of much research in the recent years. A vast majority of existing algorithms deal with vectors, and higher\\u2013order data like images are usually vectorized before processing. However, the structure of the data may be lost in the process, leading to poor representation and overall performance degradation. In this paper we propose a novel approach for sparse representation of positive definite matrices, where vectorization would have destroyed the inherent structure of the data. The sparse decomposition of a positive definite matrix is formulated as a convex optimization problem, which falls under the category of determinant maximization (MAXDET) problems [1], for which efficient interior point algorithms exist. Experimental results are shown with simulated examples as well as in real\\u2013world computer vision applications, demonstrating the suitability of the new model. This forms the first step toward extending the cornucopia of sparsity-based algorithms to positive definite matrices."'),
('"Text Image Deblurring Using Text-Specific Properties"', '"ECCV 2012"', '["Text Image", "Latent Image", "Document Image", "Kernel Estimation", "Text Region"]', '"https://doi.org/10.1007/978-3-642-33715-4_38"', '"State-of-the-art blind image deconvolution approaches have difficulties when dealing with text images, since they rely on natural image statistics which do not respect the special properties of text images. On the other hand, previous document image restoring systems and the recently proposed black-and-white document image deblurring method [1] are limited, and cannot handle large motion blurs and complex background. We propose a novel text image deblurring method which takes into account the specific properties of text images. Our method extends the commonly used optimization framework for image deblurring to allow domain-specific properties to be incorporated in the optimization process. Experimental results show that our method can generate higher quality deblurring results on text images than previous approaches."'),
('"Texton Correlation for Recognition"', '"ECCV 2004"', '["Face Recognition", "Equal Error Rate", "Locational Independence", "Illumination Direction", "Condi', '"https://doi.org/10.1007/978-3-540-24670-1_16"', '"We study the problem of object, in particular face, recognition under varying imaging conditions. Objects are represented using local characteristic features called textons. Appearance variations due to changing conditions are encoded by the correlations between the textons. We propose two solutions to model these correlations. The first one assumes locational independence. We call it the conditional texton distribution model. The second captures the second order variations across locations using Fisher linear discriminant analysis. We call it the Fisher texton model. Our two models are effective in the problem of face recognition from a single image across a wide range of illuminations, poses, and time."'),
('"TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-class Object Recognition and Se', '"ECCV 2006"', '["Class Label", "Training Image", "Object Class", "Context Modeling", "Conditional Random Field"]', '"https://doi.org/10.1007/11744023_1"', '"This paper proposes a new approach to learning a discriminative model of object classes, incorporating appearance, shape and context information efficiently. The learned model is used for automatic visual recognition and semantic segmentation of photographs. Our discriminative model exploits novel features, based on textons, which jointly model shape and texture. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating these classifiers in a conditional random field. Efficient training of the model on very large datasets is achieved by exploiting both random feature selection and piecewise training methods."'),
('"Texture Boundary Detection for Real-Time Tracking"', '"ECCV 2004"', '["Hide Markov Model", "Texture Segmentation", "Cluttered Background", "Detect Change Point", "Textur', '"https://doi.org/10.1007/978-3-540-24671-8_45"', '"We propose an approach to texture boundary detection that only requires a line-search in the direction normal to the edge. It is therefore very fast and can be incorporated into a real-time 3\\u2013D pose estimation algorithm that retains the speed of those that rely solely on gradient properties along object contours but does not fail in the presence of highly textured object and clutter."'),
('"Texture Regimes for Entropy-Based Multiscale Image Analysis"', '"ECCV 2010"', '["Dictionary Learning", "Texture Segmentation", "Critical Scale", "Dictionary Element", "Nuisance Fa', '"https://doi.org/10.1007/978-3-642-15558-1_50"', '"We present an approach to multiscale image analysis. It hinges on an operative definition of texture that involves a \\u201csmall region\\u201d, where some (unknown) statistic is aggregated, and a \\u201clarge region\\u201d within which it is stationary. At each point, multiple small and large regions co-exist at multiple scales, as image structures are pooled by the scaling and quantization process to form \\u201ctextures\\u201d and then transitions between textures define again \\u201cstructures.\\u201d We present a technique to learn and agglomerate sparse bases at multiple scales. To do so efficiently, we propose an analysis of cluster statistics after a clustering step is performed, and a new clustering method with linear-time performance. In both cases, we can infer all the \\u201csmall\\u201d and \\u201clarge\\u201d regions at multiple scale in one shot."'),
('"Texture Similarity Measure Using Kullback-Leibler Divergence between Gamma Distributions"', '"ECCV 2002"', '["Similarity Measure", "Gamma Distribution", "Gabor Filter", "Magnitude Response", "Gabor Wavelet"]', '"https://doi.org/10.1007/3-540-47977-5_9"', '"We propose a texture similarity measure based on the Kullback-Leibler divergence between gamma distributions (KLGamma). We conjecture that the spatially smoothed Gabor filter magnitude responses of some classes of visually homogeneous stochastic textures are gamma distributed. Classification experiments with disjoint test and training images, show that the KLGamma measure performs better than other parametric measures. It approaches, and under some conditions exceeds, the classification performance of the best non-parametric measures based on binned marginal histograms, although it has a computational cost at least an order of magnitude less. Thus, the KLGamma measure is well suited for use in real-time image segmentation algorithms and time-critical texture classification and retrieval from large databases."'),
('"Texture-Based Leaf Identification"', '"ECCV 2014"', '["Support Vector Machine", "Local Binary Pattern", "Probabilistic Neural Network", "Zernike Moment",', '"https://doi.org/10.1007/978-3-319-16220-1_14"', '"A novel approach to visual leaf identification is proposed. A leaf is represented by a pair of local feature histograms, one computed from the leaf interior, the other from the border. The histogrammed local features are an improved version of a recently proposed rotation and scale invariant descriptor based on local binary patterns (LBPs)."'),
('"Texture-Consistent Shadow Removal"', '"ECCV 2008"', '["Texture Characteristic", "Sampling Line", "Illumination Change", "Shadow Area", "Shadow Region"]', '"https://doi.org/10.1007/978-3-540-88693-8_32"', '"This paper presents an approach to shadow removal that preserves texture consistency between the original shadow and lit area. Illumination reduction in the shadow area not only darkens that area, but also changes the texture characteristics there. We achieve texture-consistent shadow removal by constructing a shadow-free and texture-consistent gradient field. First, we estimate an illumination change surface which causes the shadow and remove the gradients it induces. We approximate the illumination change surface with illumination change splines across the shadow boundary. We formulate estimating these splines as an optimization problem which balances the smoothness between the neighboring splines and their fitness to the image data. Second, we sample the shadow effect on the texture characteristics in the umbra and lit area near the shadow boundary, and remove it by transforming the gradients inside the shadow area to be compatible with the lit area. Experiments on photos from Flickr demonstrate the effectiveness of our method."'),
('"The 3D Jigsaw Puzzle: Mapping Large Indoor Spaces"', '"ECCV 2014"', '["Indoor scene reconstruction", "maps", "3D jigsaw puzzle"]', '"https://doi.org/10.1007/978-3-319-10578-9_1"', '"We introduce an approach for analyzing annotated maps of a site, together with Internet photos, to reconstruct large indoor spaces of famous tourist sites. While current 3D reconstruction algorithms often produce a set of disconnected components (3D pieces) for indoor scenes due to scene coverage or matching failures, we make use of a provided map to lay out the 3D pieces in a global coordinate system. Our approach leverages position, orientation, and shape cues extracted from the map and 3D pieces and optimizes a global objective to recover the global layout of the pieces. We introduce a novel crowd flow cue that measures how people move across the site to recover 3D geometry orientation. We show compelling results on major tourist sites."'),
('"The 4-Source Photometric Stereo Under General Unknown Lighting"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744078_6"', '"Many previous works on photometric stereo have shown how to recover the shape and reflectance properties of an object using multiple images taken under a fixed viewpoint and variable lighting conditions. However, most of them only dealt with a single point light source in each image. In this paper, we show how to perform photometric stereo with four images which are taken under distant but general lighting conditions. Our method is based on the representation that uses low-order spherical harmonics for Lambertian objects. Attached shadows are considered in this representation. We show that the lighting conditions can be estimated regardless of object shape and reflectance properties. The estimated illumination conditions can then help to recover the shape and reflectance properties."'),
('"The Alignment Between 3-D Data and Articulated Shapes with Bending Surfaces"', '"ECCV 2006"', '["Model Point", "Kinematic Chain", "Alignment Method", "Joint Parameter", "Surface Registration"]', '"https://doi.org/10.1007/11744078_45"', '"In this paper we address the problem of aligning 3-D data with articulated shapes. This problem resides at the core of many motion tracking methods with applications in human motion capture, action recognition, medical-image analysis, etc. We describe an articulated and bending surface representation well suited for this task as well as a method which aligns (or registers) such a surface to 3-D data. Articulated objects, e.g., humans and animals, are covered with clothes and skin which may be seen as textured surfaces. These surfaces are both articulated and deformable and one realistic way to model them is to assume that they bend in the neighborhood of the shape\\u2019s joints. We will introduce a surface-bending model as a function of the articulated-motion parameters. This combined articulated-motion and surface-bending model better predicts the observed phenomena in the data and therefore is well suited for surface registration. Given a set of sparse 3-D data (gathered with a stereo camera pair) and a textured, articulated, and bending surface, we describe a register-and-fit method that proceeds as follows. First, the data-to-surface registration problem is formalized as a classifier and is carried out using an EM algorithm. Second, the data-to-surface fitting problem is carried out by minimizing the distance from the registered data points to the surface over the joint variables. In order to illustrate the method we applied it to the problem of hand tracking. A hand model with 27 degrees of freedom is successfully registered and fitted to a sequence of 3-D data points gathered with a stereo camera pair."'),
('"The Aspect Transition Graph: An Affordance-Based Model"', '"ECCV 2014"', '["Robotic perception", "Object recognition", "Belief-space planning"]', '"https://doi.org/10.1007/978-3-319-16181-5_36"', '"In this work we introduce the Aspect Transition Graph (ATG), an affordance-based model that is grounded in the robot\\u2019s own actions and perceptions. An ATG summarizes how observations of an object or the environment changes in the course of interaction. Through the Robonaut 2 simulator, we demonstrate that by exploiting these learned models the robot can recognize objects and manipulate them to reach certain goal state."'),
('"The Beltrami Flow over Triangulated Manifolds"', '"MMBIA 2004"', '["Image Processing Application", "Triangulate Surface", "Intrinsic Formulation", "Polyakov Action", ', '"https://doi.org/10.1007/978-3-540-27816-0_12"', '"In several image processing applications one has to deal with noisy images defined on surfaces, like electric impulsions or diffusion tensors on the cortex. We propose a new regularization technique for data defined on triangulated surfaces: the Beltrami flow over intrinsic manifolds. This technique overcomes the over \\u2013 smoothing of the L 2 and the stair-casing effects of the L 1 flow for strongly noised images. To do so, we locally estimate the differential operators and then perform temporal finite differences. We present the implementation for scalar images defined in 2 dimensional manifolds and experimental results."'),
('"The Bi-directional Framework for Unifying Parametric Image Alignment Approaches"', '"ECCV 2008"', '["Image Alignment", "Inverse Additive", "Reference Coordinate Frame", "Inverse Compositional", "Forw', '"https://doi.org/10.1007/978-3-540-88690-7_30"', '"In this paper, a generic bi-directional framework is proposed for parametric image alignment, that extends the classification of [1]. Four main categories (Forward, Inverse, Dependent and Bi-directional) form the basis of a consistent set of subclasses, onto which state-of-the-art methods have been mapped. New formulations for the ESM [2] and the Inverse Additive [3] algorithms are proposed, that show the ability of this framework to unify existing approaches. New explicit equivalence relationships are given for the case of first-order optimization that provide some insights into the choice of an update rule in iterative algorithms."'),
('"The Construction of 3 Dimensional Models Using an Active Computer Vision System"', '"ECCV 2000"', '["Reverse Engineering", "Camera Calibration", "NURBS Surface", "High Resolution Data", "Computer Con', '"https://doi.org/10.1007/3-540-45053-X_12"', '"The initial development and assessment of an active computer vision system is described, which is designed to meet the growing demand for 3 dimensional models of real-world objects. Details are provided of the hardware platform employed, which uses a modified gantry robot to manoeuvre the system camera and a purpose-built computer controlled turntable on which the object to be modelled is placed. The system software and its computer control system are also described along with the occluding contour technique developed to automatically produce initial models of objects. Examples of models constructed by the system are presented and experimental results are discussed, including results which indicate that the occluding contour technique can be used in an original manner to identify regions of the object surface which require further modelling and also to determine subsequent viewpoints for the camera."'),
('"The Design and Preliminary Evaluation of a Finger-Mounted Camera and Feedback System to Enable Read', '"ECCV 2014"', '["Accessibility", "Wearables", "Real-time OCR", "Text reading for blind"]', '"https://doi.org/10.1007/978-3-319-16199-0_43"', '"We introduce the preliminary design of a novel vision-augmented touch system called HandSight intended to support activities of daily living (ADLs) by sensing and feeding back non-tactile information about the physical world as it is touched. Though we are interested in supporting a range of ADL applications, here we focus specifically on reading printed text. We discuss our vision for HandSight, describe its current implementation and results from an initial performance analysis of finger-based text scanning. We then present a user study with four visually impaired participants (three blind) exploring how to continuously guide a user\\u2019s finger across text using three feedback conditions (haptic, audio, and both). Though preliminary, our results show that participants valued the ability to access printed material, and that, in contrast to previous findings, audio finger guidance may result in the best reading performance."'),
('"The Effect of Illuminant Rotation on Texture Filters: Lissajous\\u2019s Ellipses"', '"ECCV 2002"', '["Texture", "illumination", "texture features"]', '"https://doi.org/10.1007/3-540-47977-5_19"', '"Changes in the angle of illumination incident upon a 3D surface texture can significantly change its appearance. These changes can affect the output of texture features to such an extent that they cause complete misclassification. We present new theory and experimental results that show that changes in illumination tilt angle cause texture clusters to describe Lissajous\\u2019s ellipses in feature space. We focus on texture features that may be modelled as a linear filter followed by an energy estimation process e.g. Laws filters, Gabor filters, ring and wedge filters. This general texture filter model is combined with a linear approximation of Lambert\\u2019s cosine law to predict that the outputs of these filters are sinusoidal functions of illuminant tilt. Experimentation with 30 real textures verifies this proposal. Furthermore we use these results to show that the clusters of distinct textures describe different elliptical paths in feature space as illuminant tilt varies. These results have significant implications for illuminant tilt invariant texture classification."'),
('"The Generalized PatchMatch Correspondence Algorithm"', '"ECCV 2010"', '["Object Detection", "Image Denoising", "Sift Descriptor", "Average PSNR", "Clone Detection"]', '"https://doi.org/10.1007/978-3-642-15558-1_3"', '"PatchMatch is a fast algorithm for computing dense approximate nearest neighbor correspondences between patches of two image regions [1]. This paper generalizes PatchMatch in three ways: (1) to find k nearest neighbors, as opposed to just one, (2) to search across scales and rotations, in addition to just translations, and (3) to match using arbitrary descriptors and distances, not just sum-of-squared-differences on patch colors. In addition, we offer new search and parallelization strategies that further accelerate the method, and we show performance improvements over standard kd-tree techniques across a variety of inputs. In contrast to many previous matching algorithms, which for efficiency reasons have restricted matching to sparse interest points, or spatially proximate matches, our algorithm can efficiently find global, dense matches, even while matching across all scales and rotations. This is especially useful for computer vision applications, where our algorithm can be used as an efficient general-purpose component. We explore a variety of vision applications: denoising, finding forgeries by detecting cloned regions, symmetry detection, and object detection."'),
('"The HDA+ Data Set for Research on Fully Automated Re-identification Systems"', '"ECCV 2014"', '["Recall Statistic", "Pedestrian Detector", "Camera Network", "Gait Energy Image", "Local Fisher Dis', '"https://doi.org/10.1007/978-3-319-16199-0_17"', '"There are no available datasets to evaluate integrated Pedestrian Detectors and Re-Identification systems, and the standard evaluation metric for Re-Identification (Cumulative Matching Characteristic curves) does not properly assess the errors that arise from integrating Pedestrian Detectors with Re-Identification (False Positives and Missed Detections). Real world Re-Identification systems require Pedestrian Detectors to be able to function automatically and the integration of Pedestrian Detector algorithms with Re-Identification produces errors that must be dealt with. We provide not only a dataset that allows for the evaluation of integrated Pedestrian Detector and Re-Identification systems but also sample Pedestrian Detection data and meaningful evaluation metrics and software, such as to make it \\u201cone-click easy\\u201d to test your own Re-Identification algorithm in an Integrated PD+REID system without having to implement a Pedestrian Detector algorithm yourself. We also provide body-part detection data on top of the manually labeled data and the Pedestrian Detection data, such as to make it trivial to extract your features from relevant local regions (actual body-parts). Finally we provide camera synchronization data to allow for the testing of inter-camera tracking algorithms. We expect this dataset and software to be widely used and boost research in integrated Pedestrian Detector and Re-Identification systems, bringing them closer to reality."'),
('"The Isophotic Metric and Its Application to Feature Sensitive Morphology on Surfaces"', '"ECCV 2004"', '["Principal Curvature", "Parameter Domain", "Mathematical Morphology", "Implicit Surface", "Triangle', '"https://doi.org/10.1007/978-3-540-24673-2_45"', '"We introduce the isophotic metric, a new metric on surfaces, in which the length of a surface curve is not just dependent on the curve itself, but also on the variation of the surface normals along it. A weak variation of the normals brings the isophotic length of a curve close to its Euclidean length, whereas a strong normal variation increases the isophotic length. We actually have a whole family of metrics, with a parameter that controls the amount by which the normals influence the metric. We are interested here in surfaces with features such as smoothed edges, which are characterized by a significant deviation of the two principal curvatures. The isophotic metric is sensitive to those features: paths along features are close to geodesics in the isophotic metric, paths across features have high isophotic length. This shape effect makes the isophotic metric useful for a number of applications. We address feature sensitive image processing with mathematical morphology on surfaces, feature sensitive geometric design on surfaces, and feature sensitive local neighborhood definition and region growing as an aid in the segmentation process for reverse engineering of geometric objects."'),
('"The Kullback-Leibler Kernel as a Framework for Discriminant and Localized Representations for Visua', '"ECCV 2004"', '["Support Vector Machine", "Discrete Cosine Transform", "Gaussian Mixture Model", "Localize Represen', '"https://doi.org/10.1007/978-3-540-24672-5_34"', '"The recognition accuracy of current discriminant architectures for visual recognition is hampered by the dependence on holistic image representations, where images are represented as vectors in a high-dimensional space. Such representations lead to complex classification problems due to the need to 1) restrict image resolution and 2) model complex manifolds due to variations in pose, lighting, and other imaging variables. Localized representations, where images are represented as bags of low-dimensional vectors, are significantly less affected by these problems but have traditionally been difficult to combine with discriminant classifiers such as the support vector machine (SVM). This limitation has recently been lifted by the introduction of probabilistic SVM kernels, such as the Kullback-Leibler (KL) kernel. In this work we investigate the advantages of using this kernel as a means to combine discriminant recognition with localized representations. We derive a taxonomy of kernels based on the combination of the KL-kernel with various probabilistic representation previously proposed in the recognition literature. Experimental evaluation shows that these kernels can significantly outperform traditional SVM solutions for recognition."'),
('"The Lazy Flipper: Efficient Depth-Limited Exhaustive Search in Discrete Graphical Models"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33786-4_12"', '"We propose a new exhaustive search algorithm for optimization in discrete graphical models. When pursued to the full search depth (typically intractable), it is guaranteed to converge to a global optimum, passing through a series of monotonously improving local optima that are guaranteed to be optimal within a given and increasing Hamming distance. For a search depth of 1, it specializes to ICM. Between these extremes, a tradeoff between approximation quality and runtime is established. We show this experimentally by improving approximations for the non-submodular models in the MRF benchmark [1] and Decision Tree Fields [2]."'),
('"The Least-Squares Error for Structure from Infinitesimal Motion"', '"ECCV 2004"', '["Image Point", "True Error", "Bundle Adjustment", "True Motion", "Projective Error"]', '"https://doi.org/10.1007/978-3-540-24673-2_43"', '"We analyze the least\\u2013squares error for structure from motion (SFM) with a single infinitesimal motion (\\u201cstructure from optical flow\\u201d). We present approximations to the noiseless error over two, complementary regions of motion estimates: roughly forward and non\\u2013forward translations. Experiments show that these capture the error\\u2019s detailed behavior over the entire motion range. They can be used to derive new error properties, including generalizations of the bas\\u2013relief ambiguity. As examples, we explain the error\\u2019s complexity for epipoles near the field of view; for planar scenes, we derive a new, double bas\\u2013relief ambiguity and prove the absence of local minima. For nonplanar scenes, our approximations simplify under reasonable assumptions. We show that our analysis applies even for large noise, and that the projective error has less information for estimating motion than the calibrated error. Our results make possible a comprehensive error analysis of SFM."'),
('"The Leiden Augmented Reality System (LARS)"', '"ECCV 2012"', '["augmented reality", "visual analysis", "interfaces", "HCI"]', '"https://doi.org/10.1007/978-3-642-33885-4_71"', '"Most augmented reality toolkits require special markers to be used. In our system any designated object in the environment can be used instead of special markers. Furthermore, our system was designed to work with low contrast surfaces (such as the wrinkles on the hands of the users). We used a constellation of maximally discriminative salient points derived from the environment to position a 3D rendered entity. These salient features are combined with local texture to give greater detection stability which results in less jitter to the user. We present a real time system which has sufficiently low computational requirements that it works with typical hardware found on modern laptops, tablets, and smartphones."'),
('"The Localized Consistency Principle for Image Matching under Non-uniform Illumination Variation and', '"ECCV 2002"', '["Similarity Measure", "Reference Image", "Imaging Function", "Transformation Parameter", "Image Mat', '"https://doi.org/10.1007/3-540-47969-4_14"', '"This paper proposes an image matching method that is robust to illumination variation and affine distortion. Our idea is to do image matching through establishing an imaging function that describes the functional relationship relating intensity values between two images. Similar methodology has been proposed by Viola [11] and Lai & Fang [6]. Viola proposed to do image matching through establishment of an imaging function based on a consistency principle. Lai & Fang proposed a parametric form of the imaging function. In cases where the illumination variation is not globally uniform and the parametric form of imaging function is not obvious, one needs to have a more robust method. Our method aims to take care of spatially non-uniform illumination variation and affine distortion. Central to our method is the proposal of a localized consistency principle, implemented through a non-parametric way of estimating the imaging function. The estimation is effected through optimizing a similarity measure that is robust under spatially non-uniform illumination variation and affine distortion. Experimental results are presented from both synthetic and real data. Encouraging results were obtained."'),
('"The Naked Truth: Estimating Body Shape Under Clothing"', '"ECCV 2008"', '["Body Shape", "Visual Hull", "Image Silhouette", "Human Shape", "Foreground Segmentation"]', '"https://doi.org/10.1007/978-3-540-88688-4_2"', '"We propose a method to estimate the detailed 3D shape of a person from images of that person wearing clothing. The approach exploits a model of human body shapes that is learned from a database of over 2000 range scans. We show that the parameters of this shape model can be recovered independently of body pose. We further propose a generalization of the visual hull to account for the fact that observed silhouettes of clothed people do not provide a tight bound on the true 3D shape. With clothed subjects, different poses provide different constraints on the possible underlying 3D body shape. We consequently combine constraints across pose to more accurately estimate 3D body shape in the presence of occluding clothing. Finally we use the recovered 3D shape to estimate the gender of subjects and then employ gender-specific body models to refine our shape estimates. Results on a novel database of thousands of images of clothed and \\u201cnaked\\u201d subjects, as well as sequences from the HumanEva dataset, suggest the method may be accurate enough for biometric shape analysis in video."'),
('"The Narrow-Band Assumption in Log-Chromaticity Space"', '"ECCV 2010"', '["Sensor Sensitivity", "Invariant Line", "Blue Channel", "Shadow Detection", "Arbitrary Image"]', '"https://doi.org/10.1007/978-3-642-35740-4_7"', '"Despite the strengths and popularity of the log-cromaticity space (LCS), there is still a significant amount of concern regarding its narrow-band assumption (NBA). Though not always necessary, this assumption is relatively common, as it leads to elegant formulations. We present a scheme for evaluating whether a deviation from the NBA will have an impact on the expected LCS values. We also introduce two metrics for measuring the divergence from the expected behavior under the NBA in LCS. Lastly, we empirically analyze how different types of reflectance spectra are affected in varying degrees by this assumption. For example, experiments with real and synthetic data show that the violation of the NBA typically has insignificant impact on bright unsaturated colors."'),
('"The Quadratic-Chi Histogram Distance Family"', '"ECCV 2010"', '["Image Retrieval", "Similarity Matrix", "Query Image", "Scale Invariant Feature Transform", "Shape ', '"https://doi.org/10.1007/978-3-642-15552-9_54"', '"We present a new histogram distance family, the Quadratic-Chi (QC). QC members are Quadratic-Form distances with a cross-bin \\u03c7 2-like normalization. The cross-bin \\u03c7 2-like normalization reduces the effect of large bins having undo influence. Normalization was shown to be helpful in many cases, where the \\u03c7 2 histogram distance outperformed the L 2 norm. However, \\u03c7 2 is sensitive to quantization effects, such as caused by light changes, shape deformations etc. The Quadratic-Form part of QC members takes care of cross-bin relationships (e.g. red and orange), alleviating the quantization problem. We present two new cross-bin histogram distance properties: Similarity-Matrix-Quantization-Invariance and Sparseness-Invariance and show that QC distances have these properties. We also show that experimentally they boost performance. QC distances computation time complexity is linear in the number of non-zero entries in the bin-similarity matrix and histograms and it can easily be parallelized. We present results for image retrieval using the Scale Invariant Feature Transform (SIFT) and color image descriptors. In addition, we present results for shape classification using Shape Context (SC) and Inner Distance Shape Context (IDSC). We show that the new QC members outperform state of the art distances for these tasks, while having a short running time. The experimental results show that both the cross-bin property and the normalization are important."'),
('"The Quality of Catadioptric Imaging \\u2013 Application to Omnidirectional Stereo"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_47"', '"We investigate the influence of the mirror shape on the imaging quality of catadioptric sensors. For axially symmetrical mirrors we calculate the locations of the virtual image points considering incident quasi-parallel light rays. Using second order approximations, we give analytical expressions for the two limiting surfaces of this \\u201cvirtual image zone\\u201d. This is different to numerical or ray tracing approaches for the estimation of the blur region, e.g. [1]. We show how these equations can be used to estimate the image blur caused by the shape of the mirror. As examples, we present two different omnidirectional stereo sensors with single camera and equi-angular mirrors that are used on mobile robots. To obtain a larger stereo baseline one of these sensors consists of two separated mirror of the same angular magnification and differs from a similar configuration proposed by Ollis et al. [2]. We calculate the caustic surfaces and show that this stereo configuration can be approximated by two single view points yielding an effective vertical stereo baseline of approx. 3.7cm. An example of panoramic disparity computation using a physiologically motivated stereo algorithm is given."'),
('"The Relevance of Non-generic Events in Scale Space Models"', '"ECCV 2002"', '["Critical Path", "Scale Space", "Critical Curve", "Catastrophe Theory", "Critical Curf"]', '"https://doi.org/10.1007/3-540-47969-4_13"', '"In order to investigate the deep structure of Gaussian scale space images, one needs to understand the behaviour of spatial critical points under the influence of blurring. We show how the mathematical framework of catastrophe theory can be used to describe the behaviour of critical point trajectories when various different types of generic events, viz. annihilations and creations of pairs of spatial critical points, (almost) coincide. Although such events are non-generic in mathematical sense, they are not unlikely to be encountered in practice. Furthermore the behaviour leads to the observation that fine-to-coarse tracking of critical points doesn\\u2019t suffice. We apply the theory to an artificial image and a simulated MR image and show the occurrence of the described behaviour."'),
('"The Role of Facial Regions in Evaluating Social Dimensions"', '"ECCV 2012"', '["Face Image", "Facial Region", "Trait Category", "Facial Zone", "Steerable Pyramid"]', '"https://doi.org/10.1007/978-3-642-33868-7_21"', '"Facial trait judgments are an important information cue for people. Recent works in the Psychology field have stated the basis of face evaluation, defining a set of traits that we evaluate from faces (e.g. dominance, trustworthiness, aggressiveness, attractiveness, threatening or intelligence among others). We rapidly infer information from others faces, usually after a short period of time (<\\u20091000ms) we perceive a certain degree of dominance or trustworthiness of another person from the face. Although these perceptions are not necessarily accurate, they influence many important social outcomes (such as the results of the elections or the court decisions). This topic has also attracted the attention of Computer Vision scientists, and recently a computational model to automatically predict trait evaluations from faces has been proposed. These systems try to mimic the human perception by means of applying machine learning classifiers to a set of labeled data. In this paper we perform an experimental study on the specific facial features that trigger the social inferences. Using previous results from the literature, we propose to use simple similarity maps to evaluate which regions of the face influence the most the trait inferences. The correlation analysis is performed using only appearance, and the results from the experiments suggest that each trait is correlated with specific facial characteristics."'),
('"The R\\u00f4le of Self-Calibration in Euclidean Reconstruction from Two Rotating and Zooming Cameras', '"ECCV 2000"', '["Focal Length", "Reconstruction Error", "Fundamental Matrix", "Intrinsic Parameter", "Optic Centre"', '"https://doi.org/10.1007/3-540-45053-X_31"', '"Reconstructing the scene from image sequences captured by moving cameras with varying intrinsic parameters is one of the major achievements of computer vision research in recent years. However, there remain gaps in the knowledge of what is reliably recoverable when the camera motion is constrained to move in particular ways. This paper considers the special case of multiple cameras whose optic centres are fixed in space, but which are allowed to rotate and zoom freely, an arrangement seen widely in practical applications. The analysis is restricted to two such cameras, although the methods are readily extended to more than two."'),
('"The Scale of Geometric Texture"', '"ECCV 2012"', '["Visibility Function", "Surface Orientation", "Scale Variability", "Photometric Stereo", "Query Tex', '"https://doi.org/10.1007/978-3-642-33718-5_5"', '"The most defining characteristic of texture is its underlying geometry. Although the appearance of texture is as dynamic as its illumination and viewing conditions, its geometry remains constant. In this work, we study the fundamental characteristic properties of texture geometry\\u2014self similarity and scale variability\\u2014and exploit them to perform surface normal estimation, and geometric texture classification. Textures, whether they are regular or stochastic, exhibit some form of repetition in their underlying geometry. We use this property to derive a photometric stereo method uniquely tailored to utilize the redundancy in geometric texture. Using basic observations about the scale variability of texture geometry, we derive a compact, rotation invariant, scale-space representation of geometric texture. To evaluate this representation we introduce an extensive new texture database that contains multiple distances as well as in-plane and out-of plane rotations. The high accuracy of the classification results indicate the descriptive yet compact nature of our texture representation, and demonstrates the importance of geometric texture analysis, pointing the way towards improvements in appearance modeling and synthesis."'),
('"The Semi-explicit Shape Model for Multi-object Detection and Classification"', '"ECCV 2010"', '["Training Image", "Interest Point", "Object Class", "Marginal Probability", "Code Word"]', '"https://doi.org/10.1007/978-3-642-15552-9_25"', '"We propose a model for classification and detection of object classes where the number of classes may be large and where multiple instances of object classes may be present in an image. The algorithm combines a bottom-up, low-level, procedure of a bag-of-words naive Bayes phase for winnowing out unlikely object classes with a high-level procedure for detection and classification. The high-level process is a hybrid of a voting method where votes are filtered using beliefs computed by a class-specific graphical model. In that sense, shape is both explicit (determining the voting pattern) and implicit (each object part votes independently) \\u2014 hence the term \\u201dsemi-explicit shape model\\u201d."'),
('"The Space of Multibody Fundamental Matrices: Rank, Geometry and Projection"', '"WDV 2006"', '["Translational Motion", "Null Space", "Fundamental Matrix", "Point Correspondence", "Dynamic Scene"', '"https://doi.org/10.1007/978-3-540-70932-9_1"', '"We study the rank and geometry of the multibody fundamental matrix, a geometric entity characterizing the two-view geometry of dynamic scenes consisting of multiple rigid-body motions. We derive an upper bound on the rank of the multibody fundamental matrix that depends on the number of independent translations. We also derive an algebraic characterization of the SVD of a multibody fundamental matrix in the case of two or odd number of rigid-body motions with a common rotation. This characterization allows us to project an arbitrary matrix onto the space of multibody fundamental matrices using linear algebraic techniques."'),
('"The State-of-the-Art in Human-Computer Interaction"', '"CVHCI 2004"', '["International Workshop", "Face Recognition", "Emotional Intelligence", "Human Computer Interaction', '"https://doi.org/10.1007/978-3-540-24837-8_1"', '"Human computer interaction (HCI) lies at the crossroads of many scientific areas including artificial intelligence, computer vision, face recognition, motion tracking, etc. In recent years there has been a growing interest in improving all aspects of the interaction between humans and computers. It is argued that to truly achieve effective human-computer intelligent interaction (HCII), there is a need for the computer to be able to interact naturally with the user, similar to the way human-human interaction takes place."'),
('"The Visual Object Tracking VOT2014 Challenge Results"', '"ECCV 2014"', '["Performance evaluation", "Short-term single-object trackers", "VOT"]', '"https://doi.org/10.1007/978-3-319-16181-5_14"', '"The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are presented. The number of tested trackers makes VOT 2014 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2014 challenge that go beyond its VOT2013 predecessor are introduced: (i) a new VOT2014 dataset with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2013 evaluation methodology, (iii) a new unit for tracking speed assessment less dependent on the hardware and (iv) the VOT2014 evaluation toolkit that significantly speeds up execution of experiments. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://votchallenge.net)."'),
('"Theory of Optimal View Interpolation with Depth Inaccuracy"', '"ECCV 2010"', '["Linear Interpolation", "Target Image", "Optimal Interpolation", "Real Scene", "Gain Term"]', '"https://doi.org/10.1007/978-3-642-15561-1_25"', '"Depth inaccuracy greatly affects the quality of free-viewpoint image synthesis. A theoretical framework for a simplified view interpolation setup to quantitatively analyze the effect of depth inaccuracy and provide a principled optimization scheme based on the mean squared error metric is proposed. The theory clarifies that if the probabilistic distribution of disparity errors is available, optimal view interpolation that outperforms conventional linear interpolation can be achieved. It is also revealed that under specific conditions, the optimal interpolation converges to linear interpolation. Furthermore, appropriate band-limitation combined with linear interpolation is also discussed, leading to an easy algorithm that achieves near-optimal quality. Experimental results using real scenes are also presented to confirm this theory."'),
('"Thinking Inside the Box: Using Appearance Models and Context Based on Room Geometry"', '"ECCV 2010"', '["Spatial Layout", "Indoor Scene", "Object Hypothesis", "Camera Height", "Scene Layout"]', '"https://doi.org/10.1007/978-3-642-15567-3_17"', '"In this paper we show that a geometric representation of an object occurring in indoor scenes, along with rich scene structure can be used to produce a detector for that object in a single image. Using perspective cues from the global scene geometry, we first develop a 3D based object detector. This detector is competitive with an image based detector built using state-of-the-art methods; however, combining the two produces a notably improved detector, because it unifies contextual and geometric information. We then use a probabilistic model that explicitly uses constraints imposed by spatial layout \\u2013 the locations of walls and floor in the image \\u2013 to refine the 3D object estimates. We use an existing approach to compute spatial layout [1], and use constraints such as objects are supported by floor and can not stick through the walls. The resulting detector (a) has significantly improved accuracy when compared to the state-of-the-art 2D detectors and (b) gives a 3D interpretation of the location of the object, derived from a 2D image. We evaluate the detector on beds, for which we give extensive quantitative results derived from images of real scenes."'),
('"Three Dimensional Curvilinear Structure Detection Using Optimally Oriented Flux"', '"ECCV 2008"', '["Image Noise", "Hessian Matrix", "Object Boundary", "Image Gradient", "Structure Direction"]', '"https://doi.org/10.1007/978-3-540-88693-8_27"', '"This paper proposes a novel curvilinear structure detector, called Optimally Oriented Flux (OOF). OOF finds an optimal axis on which image gradients are projected in order to compute the image gradient flux. The computation of OOF is localized at the boundaries of local spherical regions. It avoids considering closely located adjacent structures. The main advantage of OOF is its robustness against the disturbance induced by closely located adjacent objects. Moreover, the analytical formulation of OOF introduces no additional computation load as compared to the calculation of the Hessian matrix which is widely used for curvilinear structure detection. It is experimentally demonstrated that OOF delivers accurate and stable curvilinear structure detection responses under the interference of closely located adjacent structures as well as image noise."'),
('"Three Dimensional Tissue Classifications in MR Brain Images"', '"CVAMIA 2006"', '["Grey Matter", "Percentage Error", "Partial Volume Effect", "Active Contour Model", "Fuzzy Classifi', '"https://doi.org/10.1007/11889762_21"', '"This paper presents an algorithm for classifying different tissue types in T1-weighted MR brain images using fuzzy segmentation. The main aim in this study is to compensate for the blurring effect on tissue boundaries due to partial volume effects. This paper is organized as follows: first, an adaptive greedy contour model has been developed to separate the intracranial volume (ICV) from the scalp and skull. Second, in order to deal with the problem of the partial volume effect, an algorithm for fuzzy segmentation is presented which has integrated fuzzy spatial affinity with statistical distributions of image intensities for each of the three tissues \\u2013 cerebrospinal fluid, white matter and grey matter. This algorithm is tested on well-established simulated MR brain volumes to generate an extensive quantitative comparison with different noise levels and different slice thicknesses ranging from 1mm to 5mm. Finally, the results of this algorithm on clinical MR brain images are demonstrated."'),
('"Three-Dimensional Hand Pointing Recognition Using Two Cameras by Interpolation and Integration of C', '"ECCV 2014"', '["Hand detection", "Hand pose estimation", "Multi-class classification"]', '"https://doi.org/10.1007/978-3-319-16178-5_50"', '"In this paper, we propose a novel method of hand recognition for remote mid-air pointing operation. In the proposed method, classification scores are calculated in a sliding window for hand postures with different pointing directions. Detection of a pointing hand and estimation of the pointing direction is performed by interpolating the classification scores. Moreover, we introduce two cameras and improve the recognition accuracy by integrating the classification scores obtained from two camera images. In the experiment, the recognition rate was 73 % at around 1 FPPI when \\\\(\\\\pm 10^\\\\circ \\\\) error was allowed. Though this result was still insufficient for practical applications, we confirmed that integration of two camera information greatly improved the recognition performance."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Three-Dimensional Mass Reconstruction in Mammography"', '"MMBIA 2004"', '["Mass Region", "Malignant Mass", "Algebraic Reconstruction Technique", "Benign Mass", "Neighbouring', '"https://doi.org/10.1007/978-3-540-27816-0_21"', '"In this paper, we present a novel method for reconstructing the 3-D shapes of masses. We first use the Shape from Silhouettes technique to get an approximation to the 3-D shape of the mass. We calculate the centroid of the mass in the 2-D images and back-project them to get their intersection in 3-D. We then apply a novel iterative method, which is derived from ART (Algebraic Reconstruction Technique), to refine the 3-D shape of the mass. The thickness of the masses is calculated according to the h int representation. We use the thickness of the masses in the CC and MLO or LM views to refine the 3-D approximation to the reconstructed shape of the mass. We find that the mean deviation rate of the reconstruction of a pair of benign masses is much larger than that of malignant masses, which can be used as a criterion of classifying a mass into malignant or benign."'),
('"Three-Dimensional Object Reconstruction from Compton Scattered Gamma-Ray Data"', '"MMBIA 2004"', '["Root Mean Square Error", "Gamma Camera", "Scattered Radiation", "Scattered Photon", "Object Recons', '"https://doi.org/10.1007/978-3-540-27816-0_3"', '"A new imaging principle for object reconstruction is proposed in Single Photon Emission Computer Tomography (SPECT) which is widely used in nuclear medicine. The quality of SPECT images is largely affected by many adverse factors among which chiefly Compton scattering of gamma rays. Recently we have proposed to exploit Compton scattered radiation to generate new data necessary for object reconstruction, instead of discarding it as usually done. This has led us to a new underlying imaging principle based on the inversion of a generalized Radon transform. In this new three-dimensional reconstruction method both signal to noise ratio and image quality are improved. Remarkably the complete data, collected at various angles of scattering, can be obtained by a motionless data taking gamma camera. Examples of object reconstruction are presented as illustrations."'),
('"TIGER \\u2013 A New Model for Spatio-temporal Realignment of FMRI Data"', '"MMBIA 2004"', '["Motion correction", "slice-timing correction", "FMRI", "registration", "spatio-temporal re-alignme', '"https://doi.org/10.1007/978-3-540-27816-0_25"', '"We describe a new model which is able to model accurately the characteristics of subject motion, a dominant artefact in Functional Magnetic Resonance Images. Using the model, which is based on specific knowledge regarding the nature of the image acquisition, it is possible to correct for this motion which would otherwise render activation detection on the images invalid. We also present an initial implementation based on the model and are able to demonstrate that the corrections available under this new scheme are significantly more accurate than existing approaches to the problem of subject motion, enabling a far more accurate analysis of the patterns of brain activation which these images seek to capture."'),
('"Tighter Relaxations for Higher-Order Models Based on Generalized Roof Duality"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33885-4_28"', '"Many problems in computer vision can be turned into a large-scale boolean optimization problem, which is in general NP-hard. In this paper, we further develop one of the most successful approaches, namely roof duality, for approximately solving such problems for higher-order models. Two new methods that can be applied independently or in combination are investigated. The first one is based on constructing relaxations using generators of the submodular function cone. In the second method, it is shown that the roof dual bound can be applied in an iterated way in order to obtain a tighter relaxation. We also provide experimental results that demonstrate better performance with respect to the state-of-the-art, both in terms of improved bounds and the number of optimally assigned variables."'),
('"Time-Lapse Image Fusion"', '"ECCV 2012"', '["High Dynamic Range", "High Dynamic Range Image", "Laplacian Pyramid", "Motion Video", "Gaussian En', '"https://doi.org/10.1007/978-3-642-33868-7_44"', '"Exposure fusion is a well known technique for blending multiple, differently-exposed images to create a single frame with wider dynamic range. In this paper, we propose a method that applies and extends exposure fusion to blend visual elements from time sequences while preserving interesting structure. We introduce a time-dependent decay into the image blending process that determines the contribution of individual frames based on their relative position in the sequence, and show how this temporal component can be made dependent on visual appearance. Our time-lapse fusion method can simulate on video the kind visual effects that arise in long-exposure photography. It can also create very-long-exposure photographs impossible to capture with current digital sensor technologies."'),
('"Time-Recursive Velocity-Adapted Spatio-Temporal Scale-Space Filters"', '"ECCV 2002"', '["Galilean Transformation", "Invariant Texture", "Discrete Kernel", "Wide Baseline Stereo", "Discret', '"https://doi.org/10.1007/3-540-47969-4_4"', '"This paper presents a theory for constructing and computing velocity-adapted scale-space filters for spatio-temporal image data. Starting from basic criteria in terms of time-causality, time-recursivity, locality and adaptivity with respect to motion estimates, a family of spatio-temporal recursive filters is proposed and analysed. An important property of the proposed family of smoothing kernels is that the spatio-temporal covariance matrices of the discrete kernels obey similar transformation properties under Galilean transformations as for continuous smoothing kernels on continuous domains. Moreover, the proposed theory provides an efficient way to compute and generate non-separable scale-space representations without need for explicit external warping mechanisms or keeping extended temporal buffers of the past. The approach can thus be seen as a natural extension of recursive scale-space filters from pure temporal data to spatio-temporal domains."'),
('"To Track or To Detect? An Ensemble Framework for Optimal Selection"', '"ECCV 2012"', '["Optimal Selection", "Color Histogram", "Hungarian Algorithm", "Deformable Part Model", "Tracking C', '"https://doi.org/10.1007/978-3-642-33715-4_43"', '"This paper presents a novel approach for multi-target tracking using an ensemble framework that optimally chooses target tracking results from that of independent trackers and a detector at each time step. The ensemble model is designed to select the best candidate scored by a function integrating detection confidence, appearance affinity, and smoothness constraints imposed using geometry and motion information. Parameters of our association score function are discriminatively trained with a max-margin framework. Optimal selection is achieved through a hierarchical data association step that progressively associates candidates to targets. By introducing a second target classifier and using the ranking score from the pre-trained classifier as the detection confidence measure, we add additional robustness against unreliable detections. The proposed algorithm robustly tracks a large number of moving objects in complex scenes with occlusions. We evaluate our approach on a variety of public datasets and show promising improvements over state-of-the-art methods."'),
('"Tone Correction with Dynamic Objects for Seamless Image Mosaic"', '"ECCV 2010"', '["Panorama", "Tone correction", "Seamless image mosaic"]', '"https://doi.org/10.1007/978-3-642-35740-4_9"', '"This paper presents a tone compensation method between images to make a seamless panoramic image. Different camera settings of input images, including white-balance, exposure time, and f-stops, affect the overall color tone of a resultant panoramic image. Although numerous methods have been proposed to deal with such color variations for seamless image stitching, most of them do not properly consider the dynamic scene in which different scene contents exist in input images. In this paper, we propose an efficient method that takes dynamic scene contents into account for compensating color tone difference. The proposed approach consists of three steps. First, we compensate the color tone difference by using the linear color transform with robust local features. Second, we filter out dynamic objects (i.e., dynamic scene contents) by measuring similarity between the linear transformed image and the reference image. Finally, we precisely correct the color variation with detected consistent regions only. The qualitative evaluation shows superior or competitive results compared to commercially available products."'),
('"Top-Points as Interest Points for Image Matching"', '"ECCV 2006"', '["Feature Vector", "Receiver Operating Characteristic Curve", "Critical Path", "Interest Point", "Qu', '"https://doi.org/10.1007/11744023_33"', '"We consider the use of top-points for object retrieval. These points are based on scale-space and catastrophe theory, and are invariant under gray value scaling and offset as well as scale-Euclidean transformations. The differential properties and noise characteristics of these points are mathematically well understood. It is possible to retrieve the exact location of a top-point from any coarse estimation through a closed-form vector equation which only depends on local derivatives in the estimated point. All these properties make top-points highly suitable as anchor points for invariant matching schemes. By means of a set of repeatability experiments and receiver-operator-curves we demonstrate the performance of top-points and differential invariant features as image descriptors."'),
('"Total Moving Face Reconstruction"', '"ECCV 2014"', '["3D reconstruction", "faces", "non-rigid reconstruction"]', '"https://doi.org/10.1007/978-3-319-10593-2_52"', '"We present an approach that takes a single video of a person\\u2019s face and reconstructs a high detail 3D shape for each video frame. We target videos taken under uncontrolled and uncalibrated imaging conditions, such as youtube videos of celebrities. In the heart of this work is a new dense 3D flow estimation method coupled with shape from shading. Unlike related works we do not assume availability of a blend shape model, nor require the person to participate in a training/capturing process. Instead we leverage the large amounts of photos that are available per individual in personal or internet photo collections. We show results for a variety of video sequences that include various lighting conditions, head poses, and facial expressions."'),
('"Toward a Full Probability Model of Edges in Natural Images"', '"ECCV 2002"', '["Natural image statistics", "probability model of local geometry", "scale-space", "image features",', '"https://doi.org/10.1007/3-540-47969-4_22"', '"We investigate the statistics of local geometric structures in natural images. Previous studies [13,14] of high-contrast 3\\u00d73 natural image patches have shown that, in the state space of these patches, we have a concentration of data points along a low-dimensional non-linear manifold that corresponds to edge structures. In this paper we extend our analysis to a filter-based multiscale image representation, namely the local 3-jet of Gaussian scale-space representations. A new picture of natural image statistics seems to emerge, where primitives (such as edges, blobs, and bars) generate low-dimensional non-linear structures in the state space of image data."'),
('"Toward Accurate Segmentation of the LV Myocardium and Chamber for Volumes Estimation in Gated SPECT', '"ECCV 2004"', '["Active Contour", "Chamber Volume", "Simulated Image", "Segmentation Error", "Accurate Segmentation', '"https://doi.org/10.1007/978-3-540-24673-2_22"', '"The left ventricle myocardium and chamber segmentation in gated SPECT images is a challenging problem. Segmentation is however the first step to geometry reconstruction and quantitative measurements needed for clinical parameters extraction from the images. New algorithms for segmenting the heart left ventricle myocardium and chamber are proposed. The accuracy of the volumes measured from the geometrical models used for segmentation is evaluated using simulated images. The error on the computed ejection fraction is low enough for diagnosis assistance. Experiments on real images are shown."'),
('"Toward Global Minimum through Combined Local Minima"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88693-8_22"', '"There are many local and greedy algorithms for energy minimization over Markov Random Field (MRF) such as iterated condition mode (ICM) and various gradient descent methods. Local minima solutions can be obtained with simple implementations and usually require smaller computational time than global algorithms. Also, methods such as ICM can be readily implemented in a various difficult problems that may involve larger than pairwise clique MRFs. However, their short comings are evident in comparison to newer methods such as graph cut and belief propagation. The local minimum depends largely on the initial state, which is the fundamental problem of its kind. In this paper, disadvantages of local minima techniques are addressed by proposing ways to combine multiple local solutions. First, multiple ICM solutions are obtained using different initial states. The solutions are combined with random partitioning based greedy algorithm called Combined Local Minima (CLM). There are numerous MRF problems that cannot be efficiently implemented with graph cut and belief propagation, and so by introducing ways to effectively combine local solutions, we present a method to dramatically improve many of the pre-existing local minima algorithms. The proposed approach is shown to be effective on pairwise stereo MRF compared with graph cut and sequential tree re-weighted belief propagation (TRW-S). Additionally, we tested our algorithm against belief propagation (BP) over randomly generated 30 \\u00d730 MRF with 2 \\u00d72 clique potentials, and we experimentally illustrate CLM\\u2019s advantage over message passing algorithms in computation complexity and performance."'),
('"Toward Ubiquitous Acceptance of Biometric Authentication: Template Protection Techniques"', '"BioAW 2004"', '["Smart Card", "Authentication Scheme", "Secret Share Scheme", "Authentication Server", "Biometric S', '"https://doi.org/10.1007/978-3-540-25976-3_16"', '"The present paper provides a study of theoretical and practical security issues related to the deployment of generic reliable authentication mechanisms based on the use of biometrics and personal hardware tokens, like smart cards. The analysis covers various possible authentication infrastructures, but is mainly focused on the definition of basic requirements and constraints of a particular security scheme, namely client-side authentication. The deployment of such a scheme proves to be necessary when specific application deployment constraints are encountered, particularly when there is a conspicuous need to guarantee the privacy of the users. The paper suggests several solutions to this problem, and proposes a particular template protection technique based on a secure secret sharing scheme. The fundamental goal of this technique is to secure biometric systems sensitive to privacy issues and which rely, at some extent, on authentication performed at the client end of the application."'),
('"Towards a Robust Face Detector"', '"BioAW 2004"', '["Input Image", "Face Image", "Complex Background", "Phase Congruency", "Face Detection System"]', '"https://doi.org/10.1007/978-3-540-25976-3_6"', '"In this work we present the preliminary results of a face detection system based on an hybrid approach: it combines typical feature-based techniques with image-based analysis, in order to better exploit the main characteristics available in the input image. Different modules contribute to the face detection task: 1) a template-based approach initially proposed in [12], 2) an edge-extraction technique well suited to deal with illumination-changes, 3) a multiple-classifier specifically designed to discard false positives and 4) a novel method based on a featureless representation of the eye-patterns that further improves the face/non-face discrimination. The experimental results show that the system can localize faces in images with complex background, even in presence of strong illumination changes."'),
('"Towards Automatic Selection of the Regularization Parameters in Emission Tomgraphy by Fourier Synth', '"MMBIA 2004"', '["Single Photon Emission Compute Tomography", "Condition Number", "Regularization Parameter", "Itera', '"https://doi.org/10.1007/978-3-540-27816-0_6"', '"The problem of image reconstruction in emission tomography in an ill-posed inverse problem. The methodology FRECT (Fourier regularized computed tomography) allows not only for a priori analysis of the stability of the reconstruction process but also for an exact definition of the resolution in the slices. Its natural regularization parameter, namely the cutoff frequency \\u03bd of the filter underlying the definition of the FRECT solution, can be calibrated by estimating the condition number for a range of values of \\u03bd. We first outline the methodology FRECT. We then discuss the numerical strategies which can be implemented in order to estimate the condition numbers of large matrices. Finally, we present a few results obtained in the context of SPECT reconstructions, and discuss the possibility to determine automatically the best possible cutoff frequency from the analysis of the stability."'),
('"Towards Complete Free-Form Reconstruction of Complex 3D Scenes from an Unordered Set of Uncalibrate', '"SMVP 2004"', '["Point Cloud", "Image Pair", "Bundle Adjustment", "Epipolar Geometry", "Radial Distortion"]', '"https://doi.org/10.1007/978-3-540-30212-4_1"', '"This paper describes a method for accurate dense reconstruction of a complex scene from a small set of high-resolution unorganized still images taken by a hand-held digital camera. A fully automatic data processing pipeline is proposed. Highly discriminative features are first detected in all images. Correspondences are then found in all image pairs by wide-baseline stereo matching and used in a scene structure and camera reconstruction step that can cope with occlusion and outliers. Image pairs suitable for dense matching are automatically selected, rectified and used in dense binocular matching. The dense point cloud obtained as the union of all pairwise reconstructions is fused by local approximation using oriented geometric primitives. For texturing, every primitive is mapped on the image with the best resolution."'),
('"Towards Computational Models of the Visual Aesthetic Appeal of Consumer Videos"', '"ECCV 2010"', '["Support Vector Machine", "Ground Truth", "Motion Vector", "Mean Opinion Score", "Image Quality Ass', '"https://doi.org/10.1007/978-3-642-15555-0_1"', '"In this paper, we tackle the problem of characterizing the aesthetic appeal of consumer videos and automatically classifying them into high or low aesthetic appeal. First, we conduct a controlled user study to collect ratings on the aesthetic value of 160 consumer videos. Next, we propose and evaluate a set of low level features that are combined in a hierarchical way in order to model the aesthetic appeal of consumer videos. After selecting the 7 most discriminative features, we successfully classify aesthetically appealing vs. aesthetically unappealing videos with a 73% classification accuracy using a support vector machine."'),
('"Towards Exhaustive Pairwise Matching in Large Image Collections"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_22"', '"Exhaustive pairwise matching on large datasets presents serious practical challenges, and has mostly remained an unexplored domain. We make a step in this direction by demonstrating the feasibility of scalable indexing and fast retrieval of appearance and geometric information in images. We identify unification of database filtering and geometric verification steps as a key step for doing this. We devise a novel inverted indexing scheme, based on Bloom filters, to scalably index high order features extracted from pairs of nearby features. Unlike a conventional inverted index, we can adapt the size of the inverted index to maintain adequate sparsity of the posting lists. This ensures constant time query retrievals. We are thus able to implement an exhaustive pairwise matching scheme, with linear time complexity, using the \\u2018query each image in turn\\u2019 technique. We find the exhaustive nature of our approach to be very useful in mining small clusters of images, as demonstrated by a 73.2% recall on the UKBench dataset. In the Oxford Buildings dataset, we are able to discover all the query buildings. We also discover interesting overlapping images connecting distant images."'),
('"Towards Improved Observation Models for Visual Tracking: Selective Adaptation"', '"ECCV 2002"', '["Colour Model", "Observation Model", "Visual Tracking", "Likelihood Model", "Adaptation Algorithm"]', '"https://doi.org/10.1007/3-540-47969-4_43"', '"An important issue in tracking is how to incorporate an appropriate degree of adaptivity into the observation model. Without any adaptivity, tracking fails when object properties change, for example when illumination changes affect surface colour. Conversely, if an observation model adapts too readily then, during some transient failure of tracking, it is liable to adapt erroneously to some part of the background. The approach proposed here is to adapt selectively, allowing adaptation only during periods when two particular conditions are met: that the object should be both present and in motion. The proposed mechanism for adaptivity is tested here with a foreground colour and motion model. The experimental setting itself is novel in that it uses combined colour and motion observations from a fixed filter bank, with motion used also for initialisation via a Monte Carlo proposal distribution. Adaptation is performed using a stochastic EM algorithm, during periods that meet the conditions above. Tests verify the value of such adaptivity, in that immunity to distraction from clutter of similar colour to the object is considerably enhanced."'),
('"Towards Intelligent Mission Profiles of Micro Air Vehicles: Multiscale Viterbi Classification"', '"ECCV 2004"', '["Object Recognition", "Image Class", "Aerial Image", "Visual Context", "Conditional Probability Tab', '"https://doi.org/10.1007/978-3-540-24671-8_14"', '"In this paper, we present a vision system for object recognition in aerial images, which enables broader mission profiles for Micro Air Vehicles (MAVs). The most important factors that inform our design choices are: real-time constraints, robustness to video noise, and complexity of object appearances. As such, we first propose the HSI color space and the Complex Wavelet Transform (CWT) as a set of sufficiently discriminating features. For each feature, we then build tree-structured belief networks (TSBNs) as our underlying statistical models of object appearances. To perform object recognition, we develop the novel multiscale Viterbi classification (MSVC) algorithm, as an improvement to multiscale Bayesian classification (MSBC). Next, we show how to globally optimize MSVC with respect to the feature set, using an adaptive feature selection algorithm. Finally, we discuss context-based object recognition, where visual contexts help to disambiguate the identity of an object despite the relative poverty of scene detail in flight images, and obviate the need for an exhaustive search of objects over various scales and locations in the image. Experimental results show that the proposed system achieves smaller classification error and fewer false positives than systems using the MSBC paradigm on challenging real-world test images."'),
('"Towards More Efficient and Effective LP-Based Algorithms for MRF Optimization"', '"ECCV 2010"', '["Dual Variable", "Master Problem", "Stereo Match", "Dual Objective", "Pairwise Potential"]', '"https://doi.org/10.1007/978-3-642-15552-9_38"', '"This paper proposes a framework that provides significant speed-ups and also improves the effectiveness of general message passing algorithms based on dual LP relaxations. It is applicable to both pairwise and higher order MRFs, as well as to any type of dual relaxation. It relies on combining two ideas. The first one is inspired by algebraic multigrid approaches for linear systems, while the second one employs a novel decimation strategy that carefully fixes the labels for a growing subset of nodes during the course of a dual LP-based algorithm. Experimental results on a wide variety of vision problems demonstrate the great effectiveness of this framework."'),
('"Towards Optimal Design of Time and Color Multiplexing Codes"', '"ECCV 2012"', '["Multiplexing codes", "maximum SNR", "convex optimization"]', '"https://doi.org/10.1007/978-3-642-33783-3_35"', '"Multiplexed illumination has been proved to be valuable and beneficial, in terms of noise reduction, in wide applications of computer vision and graphics, provided that the limitations of photon noise and saturation are properly tackled. Existing optimal multiplexing codes, in the sense of maximum signal-to-noise ratio (SNR), are primarily designed for time multiplexing, but they only apply to a multiplexing system requiring the number of measurements (M) equal to the number of illumination sources (N). In this paper, we formulate a general code design problem, where M\\u2009\\u2265\\u2009N, for time and color multiplexing, and develop a sequential semi-definite programming to deal with the formulated optimization problem. The proposed formulation and method can be readily specialized to time multiplexing, thereby making such optimized codes have a much broader application. Computer simulations will discover the main merit of the method\\u2014 a significant boost of SNR as M increases. Experiments will also be presented to demonstrate the effectiveness and superiority of the method in object illumination."'),
('"Towards Optimal Naive Bayes Nearest Neighbor"', '"ECCV 2010"', '["Support Vector Machine", "Linear Support Vector Machine", "Locality Sensitive Hashing", "Hinge Los', '"https://doi.org/10.1007/978-3-642-15561-1_13"', '"Naive Bayes Nearest Neighbor (NBNN) is a feature-based image classifier that achieves impressive degree of accuracy [1] by exploiting \\u2018Image-to-Class\\u2019 distances and by avoiding quantization of local image descriptors. It is based on the hypothesis that each local descriptor is drawn from a class-dependent probability measure. The density of the latter is estimated by the non-parametric kernel estimator, which is further simplified under the assumption that the normalization factor is class-independent. While leading to significant simplification, the assumption underlying the original NBNN is too restrictive and considerably degrades its generalization ability. The goal of this paper is to address this issue."'),
('"Towards Optimal Non-rigid Surface Tracking"', '"ECCV 2012"', '["dense motion capture", "non-rigid surface alignment", "non-sequential tracking", "minimum spanning', '"https://doi.org/10.1007/978-3-642-33765-9_53"', '"This paper addresses the problem of optimal alignment of non-rigid surfaces from multi-view video observations to obtain a temporally consistent representation. Conventional non-rigid surface tracking performs frame-to-frame alignment which is subject to the accumulation of errors resulting in drift over time. Recently, non-sequential tracking approaches have been introduced which re-order the input data based on a dissimilarity measure. One or more input sequences are represented in a tree with reducing alignment path length. This limits drift and increases robustness to large non-rigid deformations. However, jumps may occur in the aligned mesh sequence where tree branches meet due to independent error accumulation. Optimisation of the tree for non-sequential tracking is proposed to minimise the errors in temporal consistency due to both the drift and jumps. A novel cluster tree enforces sequential tracking in local segments of the sequence while allowing global non-sequential traversal among these segments. This provides a mechanism to create a tree structure which reduces the number of jumps between branches and limits the length of branches. Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including faces, cloth and people. This demonstrates that the proposed cluster tree achieves better temporal consistency than the previous sequential and non-sequential tracking approaches. Quantitative ground-truth comparison on a synthetic facial performance shows reduced error with the cluster tree."'),
('"Towards Optimal Training of Cascaded Detectors"', '"ECCV 2006"', '["False Positive Rate", "Operating Point", "Object Detection", "Training Time", "Face Detection"]', '"https://doi.org/10.1007/11744023_26"', '"Cascades of boosted ensembles have become popular in the object detection community following their highly successful introduction in the face detector of Viola and Jones [1]. In this paper, we explore several aspects of this architecture that have not yet received adequate attention: decision points of cascade stages, faster ensemble learning, and stronger weak hypotheses. We present a novel strategy to determine the appropriate balance between false positive and detection rates in the individual stages of the cascade based on a probablistic model of the overall cascade\\u2019s performance. To improve the training time of individual stages, we explore the use of feature filtering before the application of Adaboost. Finally, we show that the use of stronger weak hypotheses based on CART can significantly improve upon the standard face detection results on the CMU-MIT data set."'),
('"Towards Person Identification and Re-identification with Attributes"', '"ECCV 2012"', '["Near Neighbour", "Rank Score", "Camera Network", "British Machine Vision", "Support Vector Machine', '"https://doi.org/10.1007/978-3-642-33863-2_40"', '"Visual identification of an individual in a crowded environment observed by a distributed camera network is critical to a variety of tasks including commercial space management, border control, and crime prevention. Automatic re-identification of a human from public space CCTV video is challenging due to spatiotemporal visual feature variations and strong visual similarity in people\\u2019s appearance, compounded by low-resolution and poor quality video data. Relying on re-identification using a probe image is limiting, as a linguistic description of an individual\\u2019s profile may often be the only available cues. In this work, we show how mid-level semantic attributes can be used synergistically with low-level features for both identification and re-identification. Specifically, we learn an attribute-centric representation to describe people, and a metric for comparing attribute profiles to disambiguate individuals. This differs from existing approaches to re-identification which rely purely on bottom-up statistics of low-level features: it allows improved robustness to view and lighting; and can be used for identification as well as re-identification. Experiments demonstrate the flexibility and effectiveness of our approach compared to existing feature representations when applied to benchmark datasets."'),
('"Towards Predicting Good Users for Biometric Recognition Based on Keystroke Dynamics"', '"ECCV 2014"', '["Keystroke", "Typing patterns", "Biometric", "Authentication", "Quality", "Performance prediction"]', '"https://doi.org/10.1007/978-3-319-16181-5_54"', '"This paper studies ways to detect good users for biometric recognition based on keystroke dynamics. Keystroke dynamics is an active research field for the biometric scientific community. Despite the great efforts made during the last decades, the performance of keystroke dynamics recognition systems is far from the performance achieved by traditional hard biometrics. This is very pronounced for some users, who generate many recognition errors even with the most sophisticate recognition algorithms. On the other hand, previous works have demonstrated that some other users behave particularly well even with the simplest recognition algorithms. Our purpose here is to study ways to distinguish such classes of users using only the genuine enrollment data. The experiments comprise a public database and two popular recognition algorithms. The results show the effectiveness of the Kullback-Leibler divergence as a quality measure to categorize users in comparison with other four statistical measures."'),
('"Towards Real-Time Cue Integration by Using Partial Results"', '"ECCV 2002"', '["Cue integration", "real-time vision", "meta-reasoning"]', '"https://doi.org/10.1007/3-540-47979-1_22"', '"Typical cue integration techniques work by combining estimates produced by computations associated with each visual cue. Most of these computations are iterative, leading to partial results that are available upon each iteration, culminating in complete results when the algorithm finally terminates. Combining partial results upon each iteration would be the preferred strategy for cue integration, as early cue integration strategies are inherently more stable and more efficient. Surprisingly, existing cue integration techniques cannot correctly use partial results, but must wait for all of the cue computations to finish. This is because the intrinsic error in partial results, which arises entirely from the fact that the algorithm has not yet terminated, is not represented. While cue integration methods do exist which attempt to use partial results (such as one based on an iterated extended Kalman Filter), they make critical errors."'),
('"Towards Safer, Faster Prenatal Genetic Tests: Novel Unsupervised, Automatic and Robust Methods of S', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_34"', '"In this paper we present two new methods of segmentation that we developed for nuclei and chromosomic probes \\u2013 core objects for cytometry medical imaging. Our nucleic segmentation method is mathematically grounded on a novel parametric model of an image histogram, which accounts at the same time for the background noise, the nucleic textures and the nuclei\\u2019s alterations to the background. We adapted an Expectation-Maximisation algorithm to adjust this model to the histograms of each image and subregion, in a coarse-to-fine approach. The probe segmentation uses a new dome-detection algorithm, insensitive to background and foreground noise, which detects probes of any intensity. We detail our two segmentation methods and our EM algorithm, and discuss the strengths of our techniques compared with state-of-the-art approaches. Both our segmentation methods are unsupervised, automatic, and require no training nor tuning: as a result, they are directly applicable to a wide range of medical images. We have used them as part of a large-scale project for the improvement of prenatal diagnostic of genetic diseases, and tested them on more than 2,100 images with nearly 14,000 nuclei. We report 99.3% accuracy for each of our segmentation methods, with a robustness to different laboratory conditions unreported before."'),
('"Towards Scalable Dataset Construction: An Active Learning Approach"', '"ECCV 2008"', '["Active Learning", "Online Learning", "Object Category", "Weak Learner", "Label Image"]', '"https://doi.org/10.1007/978-3-540-88682-2_8"', '"As computer vision research considers more object categories and greater variation within object categories, it is clear that larger and more exhaustive datasets are necessary. However, the process of collecting such datasets is laborious and monotonous. We consider the setting in which many images have been automatically collected for a visual category (typically by automatic internet search), and we must separate relevant images from noise. We present a discriminative learning process which employs active, online learning to quickly classify many images with minimal user input. The principle advantage of this work over previous endeavors is its scalability. We demonstrate precision which is often superior to the state-of-the-art, with scalability which exceeds previous work."'),
('"Towards Space-Time Semantics in Two Frames"', '"ECCV 2012"', '["Interest Point", "Image Patch", "Synthetic Image", "Support Region", "Depth Discontinuity"]', '"https://doi.org/10.1007/978-3-642-33885-4_13"', '"We present a novel, low-level scheme to analyze spatial and temporal change within a local support region. Assuming available region correspondences between two adjacent frames, we divide each region into a regular grid of patches. Depending on the change of an image function inside the patch over time, each patch is assigned weights for the following four labels: \\u201cC\\u201d for a constant patch, \\u201cO\\u201d when new information originates from outside the support region, \\u201cI\\u201d for \\u201cinner\\u201d changes, and \\u201cN\\u201d for information from neighboring patches. Our method goes beyond optical flow, as it provides an additional semantic level of understanding the changes in space-time. We demonstrate how our novel \\u201cCOIN\\u201d scheme can be used to categorize local space-time events in image pairs, including locally planar support regions, 3D discontinuities, and virtual vs. real crossings of 3D structures."'),
('"Towards Transparent Systems: Semantic Characterization of Failure Modes"', '"ECCV 2014"', '["Vision System", "Failure Mode", "Discriminative Function", "Semantic Attribute", "Failure Predicti', '"https://doi.org/10.1007/978-3-319-10599-4_24"', '"Today\\u2019s computer vision systems are not perfect. They fail frequently. Even worse, they fail abruptly and seemingly inexplicably. We argue that making our systems more transparent via an explicit human understandable characterization of their failure modes is desirable. We propose characterizing the failure modes of a vision system using semantic attributes. For example, a face recognition system may say \\u201cIf the test image is blurry, or the face is not frontal, or the person to be recognized is a young white woman with heavy make up, I am likely to fail.\\u201d This information can be used at training time by researchers to design better features, models or collect more focused training data. It can also be used by a downstream machine or human user at test time to know when to ignore the output of the system, in turn making it more reliable. To generate such a \\u201cspecification sheet\\u201d, we discriminatively cluster incorrectly classified images in the semantic attribute space using L1-regularized weighted logistic regression. We show that our specification sheets can predict oncoming failures for face and animal species recognition better than several strong baselines. We also show that lay people can easily follow our specification sheets."'),
('"Towards Unified Object Detection and Semantic Segmentation"', '"ECCV 2014"', '["Object Detection", "Semantic Segmentation", "Unified Approach"]', '"https://doi.org/10.1007/978-3-319-10602-1_20"', '"Object detection and semantic segmentation are two strongly correlated tasks, yet typically solved separately or sequentially with substantially different techniques. Motivated by the complementary effect observed from the typical failure cases of the two tasks, we propose a unified framework for joint object detection and semantic segmentation. By enforcing the consistency between final detection and segmentation results, our unified framework can effectively leverage the advantages of leading techniques for these two tasks. Furthermore, both local and global context information are integrated into the framework to better distinguish the ambiguous samples. By jointly optimizing the model parameters for all the components, the relative importance of different component is automatically learned for each category to guarantee the overall performance. Extensive experiments on the PASCAL VOC 2010 and 2012 datasets demonstrate encouraging performance of the proposed unified framework for both object detection and semantic segmentation tasks."'),
('"Trace Quotient Problems Revisited"', '"ECCV 2006"', '["Face Recognition", "Linear Discriminant Analysis", "Tangent Space", "Recognition Rate", "Grassmann', '"https://doi.org/10.1007/11744047_18"', '"The formulation of trace quotient is shared by many computer vision problems; however, it was conventionally approximated by an essentially different formulation of quotient trace, which can be solved with the generalized eigenvalue decomposition approach. In this paper, we present a direct solution to the former formulation. First, considering that the feasible solutions are constrained on a Grassmann manifold, we present a necessary condition for the optimal solution of the trace quotient problem, which then naturally elicits an iterative procedure for pursuing the optimal solution. The proposed algorithm, referred to as Optimal Projection Pursuing (OPP), has the following characteristics: 1) OPP directly optimizes the trace quotient, and is theoretically optimal; 2) OPP does not suffer from the solution uncertainty issue existing in the quotient trace formulation that the objective function value is invariant under any nonsingular linear transformation, and OPP is invariant only under orthogonal transformations, which does not affect final distance measurement; and 3) OPP reveals the underlying equivalence between the trace quotient problem and the corresponding trace difference problem. Extensive experiments on face recognition validate the superiority of OPP over the solution of the corresponding quotient trace problem in both objective function value and classification capability."'),
('"Tracking and Characterization of Highly Deformable Cloud Structures"', '"ECCV 2000"', '["Active Contour", "Markov Random Field", "Convective Activity", "Convective Cloud", "Speed Function', '"https://doi.org/10.1007/3-540-45053-X_28"', '"Tracking and characterizing convective clouds from meteorological satellite images enable to evaluate the potential occurring of strong precipitation. We propose an original two-step tracking method based on the Level Set approach which can efficiently cope with frequent splitting or merging phases undergone by such highly deformable structures. The first step exploits a 2D motion field, and acts as a prediction step. The second step can produce, by comparing local and global photometric information, appropriate expansion or contraction forces on the evolving contours to accurately locate the cloud cells of interest. The characterization of the tracked clouds relies on both 2D local motion divergence information and temporal variations of temperature. It is formulated as a contextual statistical labeling problem involving three classes \\u201cgrowing activity\\u201d, \\u201cdeclining activity\\u201d and \\u201cinactivity\\u201d."'),
('"Tracking and Labelling of Interacting Multiple Targets"', '"ECCV 2006"', '["Feature Vector", "Image Gradient", "Relative Depth", "Interaction Graph", "Foreground Pixel"]', '"https://doi.org/10.1007/11744078_48"', '"Successful multi-target tracking requires solving two problems \\u2013 localize the targets and label their identity. An isolated target\\u2019s identity can be unambiguously preserved from one frame to the next. However, for long sequences of many moving targets, like a football game, grouping scenarios will occur in which identity labellings cannot be maintained reliably by using continuity of motion or appearance. This paper describes how to match targets\\u2019 identities despite these interactions."'),
('"Tracking and Object Classification for Automated Surveillance"', '"ECCV 2002"', '["Background Subtraction", "Single Person", "Shadow Region", "Cast Shadow", "Foreground Region"]', '"https://doi.org/10.1007/3-540-47979-1_23"', '"In this paper we discuss the issues that need to be resolved before fully automated outdoor surveillance systems can be developed, and present solutions to some of these problems. Any outdoor surveillance system must be able to track objects moving in its field of view, classify these objects and detect some of their activities. We have developed a method to track and classify these objects in realistic scenarios. Object tracking in a single camera is performed using background subtraction, followed by region correspondence. This takes into account multiple cues including velocities, sizes and distances of bounding boxes. Objects can be classified based on the type of their motion. This property may be used to label objects as a single person, vehicle or group of persons. Our proposed method to classify objects is based upon detecting recurrent motion for each tracked object. We develop a specific feature vector called a \\u2018Recurrent Motion Image\\u2019 (RMI) to calculate repeated motion of objects. Different types of objects yield very different RMI\\u2019s and therefore can easily be classified into different categories on the basis of their RMI. The proposed approach is very efficient both in terms of computational and space criteria. RMI\\u2019s are further used to detect carried objects. We present results on a large number of real world sequences including the PETS 2001 sequences. Our surveillance system works in real time at approximately 15Hz for 320x240 resolution color images on a 1.7 GHz pentium-4 PC."'),
('"Tracking and Rendering Using Dynamic Textures on Geometric Structure from Motion"', '"ECCV 2002"', '["Training Image", "Geometric Error", "Static Texture", "Dynamic Texture", "Factorization Algorithm"', '"https://doi.org/10.1007/3-540-47967-8_28"', '"Estimating geometric structure from uncalibrated images accurately enough for high quality rendering is difficult. We present a method where only coarse geometric structure is tracked and estimated from a moving camera. Instead a precise model of the intensity image variation is obtained by overlaying a dynamic, time varying texture on the structure. This captures small scale variations (e.g. non-planarity of the rendered surfaces, small camera geometry distortions and tracking errors). The dynamic texture is estimated and coded much like in movie compression, but parameterized in 6D pose instead of time, hence allowing the interpolation and extrapolation of new poses in the rendering and animation phase. We show experiments tracking and re-animating natural scenes as well as evaluating the geometric and image intensity accuracy on constructed special test scenes."'),
('"Tracking Articulated Motion Using a Mixture of Autoregressive Models"', '"ECCV 2004"', '["Joint Angle", "Autoregressive Model", "Human Motion", "Tracking Framework", "Articulate Motion"]', '"https://doi.org/10.1007/978-3-540-24672-5_5"', '"We present a novel approach to modelling the non-linear and time-varying dynamics of human motion, using statistical methods to capture the characteristic motion patterns that exist in typical human activities. Our method is based on automatically clustering the body pose space into connected regions exhibiting similar dynamical characteristics, modelling the dynamics in each region as a Gaussian autoregressive process. Activities that would require large numbers of exemplars in example based methods are covered by comparatively few motion models. Different regions correspond roughly to different action-fragments and our class inference scheme allows for smooth transitions between these, thus making it useful for activity recognition tasks. The method is used to track activities including walking, running, etc., using a planar 2D body model. Its effectiveness is demonstrated by its success in tracking complicated motions like turns, without any key frames or 3D information."'),
('"Tracking Aspects of the Foreground against the Background"', '"ECCV 2004"', '["Discriminant Function", "Tracking Algorithm", "IEEE Conf", "Appearance Model", "Object Region"]', '"https://doi.org/10.1007/978-3-540-24671-8_35"', '"In object tracking, change of object aspect is a cause of failure due to significant changes of object appearances. The paper proposes an approach to this problem without a priori learning object views. The object identification relies on a discriminative model using both object and background appearances. The background is represented as a set of texture patterns. The tracking algorithm then maintains a set of discriminant functions each recognizing a pattern in the object region against the background patterns that are currently relevant. Object matching is then performed efficiently by maximization of the sum of the discriminant functions over all object patterns. As a result, the tracker searches for the region that matches the target object and it also avoids background patterns seen before. The results of the experiment show that the proposed tracker is robust to even severe aspect changes when unseen views of the object come into view."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Tracking Benchmark Databases for Video-Based Sign Language Recognition"', '"ECCV 2010"', '["Sign Language Recognition", "Tracking", "Benchmark", "Data- bases"]', '"https://doi.org/10.1007/978-3-642-35749-7_22"', '"A survey of video databases that can be used within a continuous sign language recognition scenario to measure the performance of head and hand tracking algorithms either w.r.t. a tracking error rate or w.r.t. a word error rate criterion is presented in this work."'),
('"Tracking Discontinuous Motion Using Bayesian Inference"', '"ECCV 2000"', '["Bayesian Inference", "Hand Position", "Gesture Recognition", "Belief Revision", "Hand Orientation"', '"https://doi.org/10.1007/3-540-45053-X_10"', '"Robustly tracking people in visual scenes is an important task for surveillance, human-computer interfaces and visually mediated interaction. Existing attempts at tracking a person\\u2019s head and hands deal with ambiguity, uncertainty and noise by intrinsically assuming a consistently continuous visual stream and/or exploiting depth information. We present a method for tracking the head and hands of a human subject from a single view with no constraints on the continuity of motion. Hence the tracker is appropriate for real-time applications in which the availability of visual data is constrained, and motion is discontinuous. Rather than relying on spatio-temporal continuity and complex 3D models of the human body, a Bayesian Belief Network deduces the body part positions by fusing colour, motion and coarse intensity measurements with contextual semantics."'),
('"Tracking Dynamic Near-Regular Texture Under Occlusion and Rapid Movements"', '"ECCV 2006"', '["Tracking Algorithm", "Markov Random Field", "Tracking Process", "Active Appearance Model", "Subdiv', '"https://doi.org/10.1007/11744047_4"', '"We present a dynamic near-regular texture (NRT) tracking algorithm nested in a lattice-based Markov-Random-Field (MRF) model of a 3D spatiotemporal space. One basic observation used in our work is that the lattice structure of a dynamic NRT remains invariant despite its drastic geometry or appearance variations. On the other hand, dynamic NRT imposes special computational challenges to the state of the art tracking algorithms: including highly ambiguous correspondences, occlusions, and drastic illumination and appearance variations. Our tracking algorithm takes advantage of the topological invariant property of the dynamic NRT by combining a global lattice structure that characterizes the topological constraint among multiple textons and an image observation model that handles local geometry and appearance variations. Without any assumptions on the types of motion, camera model or lighting conditions, our tracking algorithm can effectively capture the varying underlying lattice structure of a dynamic NRT in different real world examples, including moving cloth, underwater patterns and marching crowd."'),
('"Tracking Feature Points in Uncalibrated Images with Radial Distortion"', '"ECCV 2012"', '["Motion Model", "Feature Tracking", "Warping Function", "Image Alignment", "Radial Distortion"]', '"https://doi.org/10.1007/978-3-642-33765-9_1"', '"The appearance of moving features in the field-of-view (FoV) of the camera may substantially change due to different camera poses. Typical solutions for tracking image points involve the assumption of an image motion model and the estimation of the motion parameters using image alignment techniques. While for conventional cameras this suffices, the radial distortion that arises in cameras with wide FoV lenses makes the standard motion models inaccurate. In this paper, we propose a set of motion models that implicitly encompass the distortion effect arising in this type of imaging devices. The proposed motion models are included in a standard image alignment framework for performing feature tracking in cameras presenting significant distortion. Consolidation experiments in repeatability and structure-from-motion scenarios show that the proposed RD-KLT trackers significantly improve the tracking performance in images presenting radial distortion, with minimal computational overhead when compared with a state-of-the-art KLT tracker."'),
('"Tracking in Action Space"', '"ECCV 2010"', '["Action Space", "Action Recognition", "Action Tracking", "Observation Function", "Action Primitive"', '"https://doi.org/10.1007/978-3-642-35749-7_8"', '"The recognition of human actions such as pointing at objects (\\u201cGive me that...\\u201d) is difficult because they ought to be recognized independent of scene parameters such as viewing direction. Furthermore, the parameters of the action, such as pointing direction, are important pieces of information. One common way to achieve recognition is by using 3D human body tracking followed by action recognition based on the captured tracking data. General 3D body tracking is, however, still a difficult problem. In this paper, we are looking at human body tracking for action recognition from a context-driven perspective. Instead of the space of human body poses, we consider the space of possible actions of a given context and argue that 3D body tracking reduces to action tracking in the parameter space in which the actions live. This reduces the high-dimensional problem to a low-dimensional one. In our approach, we use parametric hidden Markov models to represent parametric movements; particle filtering is used to track in the space of action parameters. Our approach is content with monocular video data and we demonstrate its effectiveness on synthetic and on real image sequences. In the experiments we focus on human arm movements."'),
('"Tracking Interacting Objects Optimally Using Integer Programming"', '"ECCV 2014"', '["Ground Plane", "Conditional Random Field", "Temporal Frame", "Integrality Constraint", "Basketball', '"https://doi.org/10.1007/978-3-319-10590-1_2"', '"In this paper, we show that tracking different kinds of interacting objects can be formulated as a network-flow Mixed Integer Program. This is made possible by tracking all objects simultaneously and expressing the fact that one object can appear or disappear at locations where another is in terms of linear flow constraints. We demonstrate the power of our approach on scenes involving cars and pedestrians, bags being carried and dropped by people, and balls being passed from one player to the next in a basketball game. In particular, we show that by estimating jointly and globally the trajectories of different types of objects, the presence of the ones which were not initially detected based solely on image evidence can be inferred from the detections of the others."'),
('"Tracking Objects Across Cameras by Incrementally Learning Inter-camera Colour Calibration and Patte', '"ECCV 2006"', '["Appearance Model", "Colour Similarity", "Observation Likelihood", "Coarse Quantisation", "Valid Li', '"https://doi.org/10.1007/11744047_10"', '"This paper presents a scalable solution to the problem of tracking objects across spatially separated, uncalibrated, non-overlapping cameras. Unlike other approaches this technique uses an incremental learning method, to model both the colour variations and posterior probability distributions of spatio-temporal links between cameras. These operate in parallel and are then used with an appearance model of the object to track across spatially separated cameras. The approach requires no pre-calibration or batch preprocessing, is completely unsupervised, and becomes more accurate over time as evidence is accumulated."'),
('"Tracking of Abrupt Motion Using Wang-Landau Monte Carlo Estimation"', '"ECCV 2008"', '["Markov Chain Monte Carlo", "Tracking Method", "Acceptance Ratio", "Metropolis Hastings", "Proposal', '"https://doi.org/10.1007/978-3-540-88682-2_30"', '"We propose a novel tracking algorithm based on the Wang-Landau Monte Carlo sampling method which efficiently deals with the abrupt motions. Abrupt motions could cause conventional tracking methods to fail since they violate the motion smoothness constraint. To address this problem, we introduce the Wang-Landau algorithm that has been recently proposed in statistical physics, and integrate this algorithm into the Markov Chain Monte Carlo based tracking method. Our tracking method alleviates the motion smoothness constraint utilizing both the likelihood term and the density of states term, which is estimated by the Wang-Landau algorithm. The likelihood term helps to improve the accuracy in tracking smooth motions, while the density of states term captures abrupt motions robustly. Experimental results reveal that our approach efficiently samples the object\\u2019s states even in a whole state space without loss of time. Therefore, it tracks the object of which motion is drastically changing, accurately and robustly."'),
('"Tracking of Multiple Objects Using Optical Flow Based Multiscale Elastic Matching"', '"WDV 2006"', '["Control Point", "Multiple Object", "Object Tracking", "Contour Point", "Deformable Object"]', '"https://doi.org/10.1007/978-3-540-70932-9_16"', '"A novel hybrid region-based and contour-based multiple object tracking model using optical flow based elastic matching is proposed. The proposed elastic matching model is general in two significant ways. First, it is suitable for tracking of both, rigid and deformable objects. Second, it is suitable for tracking using both, fixed cameras and moving cameras since the model does not rely on background subtraction. The elastic matching algorithm exploits both, the spectral features and contour-based features of the tracked objects, making it more robust and general in the context of object tracking. The proposed elastic matching algorithm uses a multiscale optical flow technique to compute the velocity field. This prevents the multiscale elastic matching algorithm from being trapped in a local optimum unlike conventional elastic matching algorithms that use a heuristic search procedure in the matching process. The proposed elastic matching based tracking framework is combined with Kalman filter in our current experiments. The multiscale elastic matching algorithm is used to compute the velocity field which is then approximated using B-spline surfaces. The control points of the B-spline surfaces are used directly as the tracking variables in a Kalman filtering model. The B-spline approximation of the velocity field is used to update the spectral features of the tracked objects in the Kalman filter model. The dynamic nature of these spectral features are subsequently used to reason about occlusion. Experimental results on tracking of multiple objects in real-time video are presented."'),
('"Tracking People with a Sparse Network of Bearing Sensors"', '"ECCV 2004"', '["Ground Plane", "Sparse Network", "Bearing Sensor", "Camera Optical Axis", "Camera Optical Center"]', '"https://doi.org/10.1007/978-3-540-24673-2_41"', '"Recent techniques for multi-camera tracking have relied on either overlap between the fields of view of the cameras or on a visible ground plane. We show that if information about the dynamics of the target is available, we can estimate the trajectory of the target without visible ground planes or overlapping cameras."'),
('"Tracking Using Motion Patterns for Very Crowded Scenes"', '"ECCV 2012"', '["motion pattern", "tracking", "very crowded scenes"]', '"https://doi.org/10.1007/978-3-642-33709-3_23"', '"This paper proposes Motion Structure Tracker (MST) to solve the problem of tracking in very crowded structured scenes. It combines visual tracking, motion pattern learning and multi-target tracking. Tracking in crowded scenes is very challenging due to hundreds of similar objects, cluttered background, small object size, and occlusions. However, structured crowded scenes exhibit clear motion pattern(s), which provides rich prior information. In MST, tracking and detection are performed jointly, and motion pattern information is integrated in both steps to enforce scene structure constraint. MST is initially used to track a single target, and further extended to solve a simplified version of the multi-target tracking problem. Experiments are performed on real-world challenging sequences, and MST gives promising results. Our method significantly outperforms several state-of-the-art methods both in terms of track ratio and accuracy."'),
('"Tracking Using Multilevel Quantizations"', '"ECCV 2014"', '["Tracking", "Multilevel Quantizations", "Online Random Forests", "Non-rigid Object Tracking", "Cond', '"https://doi.org/10.1007/978-3-319-10599-4_11"', '"Most object tracking methods only exploit a single quantization of an image space: pixels, superpixels, or bounding boxes, each of which has advantages and disadvantages. It is highly unlikely that a common optimal quantization level, suitable for tracking all objects in all environments, exists. We therefore propose a hierarchical appearance representation model for tracking, based on a graphical model that exploits shared information across multiple quantization levels. The tracker aims to find the most possible position of the target by jointly classifying the pixels and superpixels and obtaining the best configuration across all levels. The motion of the bounding box is taken into consideration, while Online Random Forests are used to provide pixel- and superpixel-level quantizations and progressively updated on-the-fly. By appropriately considering the multilevel quantizations, our tracker exhibits not only excellent performance in non-rigid object deformation handling, but also its robustness to occlusions. A quantitative evaluation is conducted on two benchmark datasets: a non-rigid object tracking dataset (11 sequences) and the CVPR2013 tracking benchmark (50 sequences). Experimental results show that our tracker overcomes various tracking challenges and is superior to a number of other popular tracking methods."'),
('"Tracking with Dynamic Hidden-State Shape Models"', '"ECCV 2008"', '["Support Vector Machine", "Dynamic Programming", "Dynamic Time Warping", "Edge Point", "Dynamic Pro', '"https://doi.org/10.1007/978-3-540-88682-2_49"', '"Hidden State Shape Models (HSSMs) were previously proposed to represent and detect objects in images that exhibit not just deformation of their shape but also variation in their structure. In this paper, we introduce Dynamic Hidden-State Shape Models (DHSSMs) to track and recognize the non-rigid motion of such objects, for example, human hands. Our recursive Bayesian filtering method, called DP-Tracking, combines an exhaustive local search for a match between image features and model states with a dynamic programming approach to find a global registration between the model and the object in the image. Our contribution is a technique to exploit the hierarchical structure of the dynamic programming approach that on average considerably speeds up the search for matches. We also propose to embed an online learning approach into the tracking mechanism that updates the DHSSM dynamically. The learning approach ensures that the DHSSM accurately represents the tracked object and distinguishes any clutter potentially present in the image. Our experiments show that our method can recognize the digits of a hand while the fingers are being moved and curled to various degrees. The method is robust to various illumination conditions, the presence of clutter, occlusions, and some types of self-occlusions. The experiments demonstrate a significant improvement in both efficiency and accuracy of recognition compared to the non-recursive way of frame-by-frame detection."'),
('"Tracking with the EM Contour Algorithm"', '"ECCV 2002"', '["Ground Plane", "Active Contour", "Object Boundary", "Image Location", "Observation Model"]', '"https://doi.org/10.1007/3-540-47969-4_1"', '"A novel active-contour method is presented and applied to pose refinement and tracking. The main innovation is that no \\u201dfeatures\\u201d are detected at any stage: contours are simply assumed to remove statistical dependencies between pixels on opposite sides of the contour. This assumption, together with a simple model of shape variability of the geometric models, leads to the application of an EM method for maximizing the likelihood of pose parameters. In addition, a dynamical model of the system leads to the application of a Kalman filter. The method is demonstrated by tracking motor vehicles with 3-D models."'),
('"Tracklet Descriptors for Action Modeling and Video Analysis"', '"ECCV 2010"', '["Recognition Rate", "Base Region", "Action Recognition", "Interest Point", "Dynamic Time Warping"]', '"https://doi.org/10.1007/978-3-642-15549-9_42"', '"We present spatio-temporal feature descriptors that can be inferred from video and used as building blocks in action recognition systems. They capture the evolution of \\u201celementary action elements\\u201d under a set of assumptions on the image-formation model and are designed to be insensitive to nuisance variability (absolute position, contrast), while retaining discriminative statistics due to the fine-scale motion and the local shape in compact regions of the image. Despite their simplicity, these descriptors, used in conjunction with basic classifiers, attain state of the art performance in the recognition of actions in benchmark datasets."'),
('"Tractable and Reliable Registration of 2D Point Sets"', '"ECCV 2014"', '["Loss Function", "Break Point", "Time Resolve Fluorescence", "Rigid Transformation", "Pairwise Cons', '"https://doi.org/10.1007/978-3-319-10590-1_26"', '"This paper introduces two new methods of registering 2D point sets over rigid transformations when the registration error is based on a robust loss function. In contrast to previous work, our methods are guaranteed to compute the optimal transformation, and at the same time, the worst-case running times are bounded by a low-degree polynomial in the number of correspondences. In practical terms, this means that there is no need to resort to ad-hoc procedures such as random sampling or local descent methods that cannot guarantee the quality of their solutions."'),
('"Training Deformable Object Models for Human Detection Based on Alignment and Clustering"', '"ECCV 2014"', '["Object Detection", "Mixture Component", "Average Precision", "Query Image", "Spectral Cluster"]', '"https://doi.org/10.1007/978-3-319-10602-1_27"', '"We propose a clustering method that considers non-rigid alignment of samples. The motivation for such a clustering is training of object detectors that consist of multiple mixture components. In particular, we consider the deformable part model (DPM) of Felzenszwalb et al., where each mixture component includes a learned deformation model. We show that alignment based clustering distributes the data better to the mixture components of the DPM than previous methods. Moreover, the alignment helps the non-convex optimization of the DPM find a consistent placement of its parts and, thus, learn more accurate part filters."'),
('"Training Hierarchical Feed-Forward Visual Recognition Models Using Transfer Learning from Pseudo-Ta', '"ECCV 2008"', '["Unlabeled Data", "Convolutional Neural Network", "Visual Recognition", "Transfer Learning", "Gende', '"https://doi.org/10.1007/978-3-540-88690-7_6"', '"Building visual recognition models that adapt across different domains is a challenging task for computer vision. While feature-learning machines in the form of hierarchial feed-forward models (e.g., convolutional neural networks) showed promise in this direction, they are still difficult to train especially when few training examples are available. In this paper, we present a framework for training hierarchical feed-forward models for visual recognition, using transfer learning from pseudo tasks. These pseudo tasks are automatically constructed from data without supervision and comprise a set of simple pattern-matching operations. We show that these pseudo tasks induce an informative inverse-Wishart prior on the functional behavior of the network, offering an effective way to incorporate useful prior knowledge into the network training. In addition to being extremely simple to implement, and adaptable across different domains with little or no extra tuning, our approach achieves promising results on challenging visual recognition tasks, including object recognition, gender recognition, and ethnicity recognition."'),
('"Training Object Class Detectors from Eye Tracking Data"', '"ECCV 2014"', '["Target Object", "Gaussian Mixture Model", "Object Class", "Appearance Model", "Visual Search Task"', '"https://doi.org/10.1007/978-3-319-10602-1_24"', '"Training an object class detector typically requires a large set of images annotated with bounding-boxes, which is expensive and time consuming to create. We propose novel approach to annotate object locations which can substantially reduce annotation time. We first track the eye movements of annotators instructed to find the object and then propose a technique for deriving object bounding-boxes from these fixations. To validate our idea, we collected eye tracking data for the trainval part of 10 object classes of Pascal VOC 2012 (6,270 images, 5 observers). Our technique correctly produces bounding-boxes in 50%of the images, while reducing the total annotation time by factor 6.8\\u00d7 compared to drawing bounding-boxes. Any standard object class detector can be trained on the bounding-boxes predicted by our model. Our large scale eye tracking dataset is available at groups.inf.ed.ac.uk/calvin/eyetrackdataset/ ."'),
('"Training-Based Spectral Reconstruction from a Single RGB Image"', '"ECCV 2014"', '["Spectral Image", "Training Image", "Hyperspectral Image", "Radial Basis Function Network", "Color ', '"https://doi.org/10.1007/978-3-319-10584-0_13"', '"This paper focuses on a training-based method to reconstruct a scene\\u2019s spectral reflectance from a single RGB image captured by a camera with known spectral response. In particular, we explore a new strategy to use training images to model the mapping between camera-specific RGB values and scene reflectance spectra. Our method is based on a radial basis function network that leverages RGB white-balancing to normalize the scene illumination to recover the scene reflectance. We show that our method provides the best result against three state-of-art methods, especially when the tested illumination is not included in the training stage. In addition, we also show an effective approach to recover the spectral illumination from the reconstructed spectral reflectance and RGB image. As a part of this work, we present a newly captured, publicly available, data set of hyperspectral images that are useful for addressing problems pertaining to spectral imaging, analysis and processing."'),
('"Trajectory-Based Modeling of Human Actions with Motion Reference Points"', '"ECCV 2012"', '["Action Recognition", "Human Action Recognition", "Histogram Intersection", "Dense Trajectory", "Vi', '"https://doi.org/10.1007/978-3-642-33715-4_31"', '"Human action recognition in videos is a challenging problem with wide applications. State-of-the-art approaches often adopt the popular bag-of-features representation based on isolated local patches or temporal patch trajectories, where motion patterns like object relationships are mostly discarded. This paper proposes a simple representation specifically aimed at the modeling of such motion relationships. We adopt global and local reference points to characterize motion information, so that the final representation can be robust to camera movement. Our approach operates on top of visual codewords derived from local patch trajectories, and therefore does not require accurate foreground-background separation, which is typically a necessary step to model object relationships. Through an extensive experimental evaluation, we show that the proposed representation offers very competitive performance on challenging benchmark datasets, and combining it with the bag-of-features representation leads to substantial improvement. On Hollywood2, Olympic Sports, and HMDB51 datasets, we obtain 59.5%, 80.6% and 40.7% respectively, which are the best reported results to date."'),
('"Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation"', '"ECCV 2014"', '["Semantic Representation", "Canonical Correlation Analysis", "Target Class", "Semantic Space", "Lab', '"https://doi.org/10.1007/978-3-319-10605-2_38"', '"Most existing zero-shot learning approaches exploit transfer learning via an intermediate-level semantic representation such as visual attributes or semantic word vectors. Such a semantic representation is shared between an annotated auxiliary dataset and a target dataset with no annotation. A projection from a low-level feature space to the semantic space is learned from the auxiliary dataset and is applied without adaptation to the target dataset. In this paper we identify an inherent limitation with this approach. That is, due to having disjoint and potentially unrelated classes, the projection functions learned from the auxiliary dataset/domain are biased when applied directly to the target dataset/domain. We call this problem the projection domain shift problem and propose a novel framework, transductive multi-view embedding, to solve it. It is \\u2018transductive\\u2019 in that unlabelled target data points are explored for projection adaptation, and \\u2018multi-view\\u2019 in that both low-level feature (view) and multiple semantic representations (views) are embedded to rectify the projection shift. We demonstrate through extensive experiments that our framework (1) rectifies the projection shift between the auxiliary and target domains, (2) exploits the complementarity of multiple semantic representations, (3) achieves state-of-the-art recognition results on image and video benchmark datasets, and (4) enables novel cross-view annotation tasks."'),
('"Transfer Learning Based Visual Tracking with Gaussian Processes Regression"', '"ECCV 2014"', '["Object Tracking", "Target Sample", "Observation Model", "Visual Tracking", "Transfer Learn"]', '"https://doi.org/10.1007/978-3-319-10578-9_13"', '"Modeling the target appearance is critical in many modern visual tracking algorithms. Many tracking-by-detection algorithms formulate the probability of target appearance as exponentially related to the confidence of a classifier output. By contrast, in this paper we directly analyze this probability using Gaussian Processes Regression (GPR), and introduce a latent variable to assist the tracking decision. Our observation model for regression is learnt in a semi-supervised fashion by using both labeled samples from previous frames and the unlabeled samples that are tracking candidates extracted from the current frame. We further divide the labeled samples into two categories: auxiliary samples collected from the very early frames and target samples from most recent frames. The auxiliary samples are dynamically re-weighted by the regression, and the final tracking result is determined by fusing decisions from two individual trackers, one derived from the auxiliary samples and the other from the target samples. All these ingredients together enable our tracker, denoted as TGPR, to alleviate the drifting issue from various aspects. The effectiveness of TGPR is clearly demonstrated by its excellent performances on three recently proposed public benchmarks, involving 161 sequences in total, in comparison with state-of-the-arts."'),
('"Transformation-Invariant Embedding for Image Analysis"', '"ECCV 2004"', '["Face Image", "Principle Component Analysis", "Target Image", "Visual Data", "Local Linear Embed"]', '"https://doi.org/10.1007/978-3-540-24673-2_42"', '"Dimensionality reduction is an essential aspect of visual processing. Traditionally, linear dimensionality reduction techniques such as principle components analysis have been used to find low dimensional linear subspaces in visual data. However, sub-manifolds in natural data are rarely linear, and consequently many recent techniques have been developed for discovering non-linear manifolds. Prominent among these are Local Linear Embedding and Isomap. Unfortunately, such techniques currently use a naive appearance model that judges image similarity based solely on Euclidean distance. In visual data, Euclidean distances rarely correspond to a meaningful perceptual difference between nearby images. In this paper, we attempt to improve the quality of manifold inference techniques for visual data by modeling local neighborhoods in terms of natural transformations between images\\u2014for example, by allowing image operations that extend simple differences and linear combinations. We introduce the idea of modeling local tangent spaces of the manifold in terms of these richer transformations. Given a local tangent space representation, we then embed data in a lower dimensional coordinate system while preserving reconstruction weights. This leads to improved manifold discovery in natural image sets."'),
('"Transitions of the 3D Medial Axis under a One-Parameter Family of Deformations"', '"ECCV 2002"', '["Viscosity Solution", "Medial Axis", "Double Arrow", "Deformation Path", "Circle Tangent"]', '"https://doi.org/10.1007/3-540-47967-8_48"', '"The instabilities of the medial axis of a shape under deformations have long been recognized as a major obstacle to its use in recognition and other applications. These instabilities, or transitions, occur when the structure of the medial axis graph changes abruptly under deformations of shape. The recent classification of these transitions in 2D for the medial axis and for the shock graph, was a key factor both in the development of an object recognition system and an approach to perceptual organization. This paper classifies generic transitions of the 3D medial axis, by examining the order of contact of spheres with the surface, leading to an enumeration of possible transitions, which are then examined on a case by case basis. Some cases are ruled out as never occurring in any family of deformations, while others are shown to be non-generic in a one-parameter family of deformations. Finally, the remaining cases are shown to be viable by developing a specific example for each. We relate these transitions to a classification by Bogaevsky of singularities of the viscosity solutions of the Hamilton-Jacobi equation. We believe that the classification of these transitions is vital to the successful regularization of the medial axis and its use in real applications."'),
('"TreeCANN - k-d Tree Coherence Approximate Nearest Neighbor Algorithm"', '"ECCV 2012"', '["Approximate nearest neighbor search", "patch matching"]', '"https://doi.org/10.1007/978-3-642-33765-9_43"', '"TreeCANN is a fast algorithm for approximately matching all patches between two images. It does so by following the established convention of finding an initial set of matching patch candidates between the two images and then propagating good matches to neighboring patches in the image plane. TreeCANN accelerates each of these components substantially leading to an algorithm that is \\u00d73 to \\u00d75 faster than existing methods. Seed matching is achieved using a properly tuned k-d tree on a sparse grid of patches. In particular, we show that a sequence of key design decisions can make k-d trees run as fast as recently proposed state-of-the-art methods, and because of image coherency it is enough to consider only a sparse grid of patches across the image plane. We then develop a novel propagation step that is based on the integral image, which drastically reduces the computational load that is dominated by the need to repeatedly measure similarity between pairs of patches. As a by-product we give an optimal algorithm for exact matching that is based on the integral image. The proposed exact algorithm is faster than previously reported results and depends only on the size of the images and not on the size of the patches. We report results on large and varied data sets and show that TreeCANN is orders of magnitude faster than exact NN search yet produces matches that are within 1% error, compared to the exact NN search."'),
('"TriangleFlow: Optical Flow with Triangulation-Based Higher-Order Likelihoods"', '"ECCV 2010"', '["Optical Flow", "Delaunay Triangulation", "Regularization Term", "Conditional Random Field", "Regul', '"https://doi.org/10.1007/978-3-642-15558-1_20"', '"We use a simple yet powerful higher-order conditional random field (CRF) to model optical flow. It consists of a standard photo-consistency cost and a prior on affine motions both modeled in terms of higher-order potential functions. Reasoning jointly over a large set of unknown variables provides more reliable motion estimates and a robust matching criterion. One of the main contributions is that unlike previous region-based methods, we omit the assumption of constant flow. Instead, we consider local affine warps whose likelihood energy can be computed exactly without approximations. This results in a tractable, so-called, higher-order likelihood function. We realize this idea by employing triangulation meshes which immensely reduce the complexity of the problem. Optimization is performed by hierarchical fusion moves and an adaptive mesh refinement strategy. Experiments show that we achieve high-quality motion fields on several data sets including the Middlebury optical flow database."'),
('"Triangulation for Points on Lines"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744078_15"', '"Triangulation consists in finding a 3D point reprojecting the best as possible onto corresponding image points. It is classical to minimize the reprojection error, which, in the pinhole camera model case, is nonlinear in the 3D point coordinates. We study the triangulation of points lying on a 3D line, which is a typical problem for Structure-From-Motion in man-made environments. We show that the reprojection error can be minimized by finding the real roots of a polynomial in a single variable, which degree depends on the number of images. We use a set of transformations in 3D and in the images to make the degree of this polynomial as low as possible, and derive a practical reconstruction algorithm. Experimental comparisons with an algebraic approximation algorithm and minimization of the reprojection error using Gauss-Newton are reported for simulated and real data. Our algorithm finds the optimal solution with high accuracy in all cases, showing that the polynomial equation is very stable. It only computes the roots corresponding to feasible points, and can thus deal with a very large number of views \\u2013 triangulation from hundreds of views is performed in a few seconds. Reconstruction accuracy is shown to be greatly improved compared to standard triangulation methods that do not take the line constraint into account."'),
('"TriCoS: A Tri-level Class-Discriminative Co-segmentation Method for Image Classification"', '"ECCV 2012"', '["Ground Truth", "Training Image", "Foreground Object", "Multiple Kernel Learning", "Fisher Vector"]', '"https://doi.org/10.1007/978-3-642-33718-5_57"', '"The aim of this paper is to leverage foreground segmentation to improve classification performance on weakly annotated datasets \\u2013 those with no additional annotation other than class labels. We introduce TriCoS, a new co-segmentation algorithm that looks at all training images jointly and automatically segments out the most class-discriminative foregrounds for each image. Ultimately, those foreground segmentations are used to train a classification system."'),
('"Tubular Structure Filtering by Ranking Orientation Responses of Path Operators"', '"ECCV 2014"', '["mathematical morphology", "non-linear filtering", "path operators", "thin structures", "3D imaging', '"https://doi.org/10.1007/978-3-319-10605-2_14"', '"Thin objects in 3D volumes, for instance vascular networks in medical imaging or various kinds of fibres in materials science, have been of interest for some time to computer vision. Particularly, tubular objects are everywhere elongated in one principal direction \\u2013 which varies spatially \\u2013 and are thin in the other two perpendicular directions. Filters for detecting such structures use for instance an analysis of the three principal directions of the Hessian, which is a local feature. In this article, we present a low-level tubular structure detection filter. This filter relies on paths, which are semi-global features that avoid any blurring effect induced by scale-space convolution. More precisely, our filter is based on recently developed morphological path operators. These require sampling only in a few principal directions, are robust to noise and do not assume feature regularity. We show that by ranking the directional response of this operator, we are further able to efficiently distinguish between blob, thin planar and tubular structures. We validate this approach on several applications, both from a qualitative and a quantitative point of view, demonstrating noise robustness and an efficient response on tubular structures."'),
('"Two-Granularity Tracking: Mediating Trajectory and Detection Graphs for Tracking under Occlusions"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33715-4_40"', '"We propose a tracking framework that mediates grouping cues from two levels of tracking granularities, detection tracklets and point trajectories, for segmenting objects in crowded scenes. Detection tracklets capture objects when they are mostly visible. They may be sparse in time, may miss partially occluded or deformed objects, or contain false positives. Point trajectories are dense in space and time. Their affinities integrate long range motion and 3D disparity information, useful for segmentation. Affinities may leak though across similarly moving objects, since they lack model knowledge. We establish one trajectory and one detection tracklet graph, encoding grouping affinities in each space and associations across. Two-granularity tracking is cast as simultaneous detection tracklet classification and clustering (cl2) in the joint space of tracklets and trajectories. We solve cl2 by explicitly mediating contradictory affinities in the two graphs: Detection tracklet classification modifies trajectory affinities to reflect object specific dis-associations. Non-accidental grouping alignment between detection tracklets and trajectory clusters boosts or rejects corresponding detection tracklets, changing accordingly their classification.We show our model can track objects through sparse, inaccurate detections and persistent partial occlusions. It adapts to the changing visibility masks of the targets, in contrast to detection based bounding box trackers, by effectively switching between the two granularities according to object occlusions, deformations and background clutter."'),
('"Two-Phase Kernel Estimation for Robust Motion Deblurring"', '"ECCV 2010"', '["Latent Image", "Kernel Estimation", "Impulse Noise", "Motion Blur", "Blind Deconvolution"]', '"https://doi.org/10.1007/978-3-642-15549-9_12"', '"We discuss a few new motion deblurring problems that are significant to kernel estimation and non-blind deconvolution. We found that strong edges do not always profit kernel estimation, but instead under certain circumstance degrade it. This finding leads to a new metric to measure the usefulness of image edges in motion deblurring and a gradient selection process to mitigate their possible adverse effect. We also propose an efficient and high-quality kernel estimation method based on using the spatial prior and the iterative support detection (ISD) kernel refinement, which avoids hard threshold of the kernel elements to enforce sparsity. We employ the TV-\\u21131 deconvolution model, solved with a new variable substitution scheme to robustly suppress noise."'),
('"Two-View Underwater Structure and Motion for Cameras under Flat Refractive Interfaces"', '"ECCV 2012"', '["Trial Vector", "Reprojection Error", "Scene Point", "Uniform Noise", "Forward Projection"]', '"https://doi.org/10.1007/978-3-642-33765-9_22"', '"In an underwater imaging system, a refractive interface is introduced when a camera looks into the water-based environment, resulting in distorted images due to refraction. Simply ignoring the refraction effect or using the lens radial distortion model causes erroneous 3D reconstruction. This paper deals with a general underwater imaging setup using two cameras, of which each camera is placed in a separate waterproof housing with a flat window. The impact of refraction is explicitly modeled in the refractive camera model. Based on two new concepts, namely the Ellipse of Refrax (EoR) and Refractive Depth (RD) of a scene point, we show that provably optimal underwater structure and motion under L \\u2009\\u221e\\u2009-norm can be estimated given known rotation. The constraint of known rotation is further relaxed by incorporating two-view geometry estimation into a new hybrid optimization framework. The experimental results using both synthetic and real images demonstrate that the proposed method can significantly improve the accuracy of camera motion and 3D structure estimation for underwater applications."'),
('"Typical Sequences Extraction and Recognition"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_7"', '"This paper presented a temporal sequence analyzing method, aiming at the extraction of typical sequences from an unlabeled dataset. The extraction procedure is based on HMM training and hierarchical separation of WTOM (Weighted Transition Occurring Matrix). During the extraction, HMMs are built each for a kind of typical sequence. Then Threshold Model is used to segment and recognize continuous sequence. The method has been tested on unsupervised event analysis in video surveillance and model learning of athlete actions."'),
('"Ultra-wide Baseline Facade Matching for Geo-localization"', '"ECCV 2012"', '["Query Image", "Retrieval Accuracy", "Motif Scale", "Sift Feature", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-642-33863-2_18"', '"Matching street-level images to a database of airborne images is hard because of extreme viewpoint and illumination differences. Color/gradient distributions or local descriptors fail to match forcing us to rely on the structure of self-similarity of patterns on facades. We propose to capture this structure with a novel \\u201cscale-selective self-similarity\\u201d (S 4) descriptor which is computed at each point on the facade at its inherent scale. To achieve this, we introduce a new method for scale selection which enables the extraction and segmentation of facades as well. Matching is done with a Bayesian classification of the street-view query S 4 descriptors given all labeled descriptors in the bird\\u2019s-eye-view database. We show experimental results on retrieval accuracy on a challenging set of publicly available imagery and compare with standard SIFT-based techniques."'),
('"Ultrasound Stimulated Vibro-acoustography"', '"MMBIA 2004"', '["Acoustic Emission", "Audio Signal", "Radiation Force", "Pulse Interval", "Ultrasound Beam"]', '"https://doi.org/10.1007/978-3-540-27816-0_1"', '"Vibro-acoustography is a method of imaging and measurement that uses ultrasound to produce radiation force to vibrate objects. The radiation force is concentrated laterally by focusing the ultrasound beam. The radiation force is limited in depth by intersecting two beams at different frequencies so that there is interference between the beams at the difference frequency only at their intersection. This results in a radiation stress of limited spatial extent on or within the object of interest. The resulting harmonic displacement of the object is detected by acoustic emission, ultrasound Doppler, or laser interferometery. The displacement is a complicated function of the object material parameters. However, significant images and measurements can be made with this arrangement. Vibro-acoustography can produce high resolution speckle free im-ages of biologically relevant objects such as breast micro-calcification and vessel calcifications, heart valves, and normal arteries. Vibro-acoustography can also make spot measurements such as microbubble contrast agent concentration in vessels. Several examples of these results will be described."'),
('"Unbiased Errors-In-Variables Estimation Using Generalized Eigensystem Analysis"', '"SMVP 2004"', '["Random Matrix", "Singular Vector", "Total Little Square", "Parameter Estimation Problem", "Tensor ', '"https://doi.org/10.1007/978-3-540-30212-4_4"', '"Recent research provided several new and fast approaches for the class of parameter estimation problems that are common in computer vision. Incorporation of complex noise model (mostly in form of covariance matrices) into errors-in-variables or total least squares models led to a considerable improvement of existing algorithms."'),
('"Uncalibrated Factorization Using a Variable Symmetric Affine Camera"', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11744085_12"', '"In order to reconstruct 3-D Euclidean shape by the Tomasi-Kanade factorization, one needs to specify an affine camera model such as orthographic, weak perspective, and paraperspective. We present a new method that does not require any such specific models. We show that a minimal requirement for an affine camera to mimic perspective projection leads to a unique camera model, called symmetric affine camera, which has two free functions. We determine their values from input images by linear computation and demonstrate by experiments that an appropriate camera model is automatically selected."'),
('"Uncertainty Modeling Framework for Constraint-Based Elementary Scenario Detection in Vision Systems', '"ECCV 2014"', '["Uncertainty Modeling", "Ontology", "Event Detection", "Activities of Daily Living", "Older People"', '"https://doi.org/10.1007/978-3-319-16181-5_19"', '"Event detection has advanced significantly in the past decades relying on pixel- and feature-level representations of video-clips. Although effective those representations have difficulty on incorporating scene semantics. Ontology and description-based approaches can explicitly embed scene semantics, but their deterministic nature is susceptible to noise from underlying components of vision systems. We propose a probabilistic framework to handle uncertainty on a constraint-based ontology framework for event detection. This work focuses on elementary event (scenario) uncertainty and proposes probabilistic constraints to quantify the spatial relationship between person and contextual objects. The uncertainty modeling framework is demonstrated on the detection of activities of daily living of participants of an Alzheimer\\u2019s disease study, monitored by a vision system using a RGB-D sensor (Kinect, Microsoft) as input. Two evaluations were carried out: the first, a 3-fold cross-validation focusing on elementary scenario detection (n:10 participants); and the second devoted for complex scenario detection (semi-probabilistic approach, n:45). Results showed the uncertainty modeling improves the detection of elementary scenarios in recall (e.g., In zone phone: 84 to 100 %) and precision indices (e.g., In zone Reading: 54.5 to 85.7%), and the recall of Complex scenarios."'),
('"Understanding and Modeling the Evolution of Critical Points under Gaussian Blurring"', '"ECCV 2002"', '["Critical Path", "Scale Space", "Critical Curve", "Catastrophe Theory", "Gaussian Blur"]', '"https://doi.org/10.1007/3-540-47969-4_10"', '"In order to investigate the deep structure of Gaussian scale space images, one needs to understand the behaviour of critical points under the influence of parameter-driven blurring. During this evolution two different types of special points are encountered, the so-called scale space saddles and the catastrophe points, the latter describing the pairwise annihilation and creation of critical points. The mathematical framework of catastrophe theory is used to model non-generic events that might occur due to e.g. local symmetries in the image. It is shown how this knowledge can be exploited in conjunction with the scale space saddle points, yielding a scale space hierarchy tree that can be used for segmentation. Furthermore the relevance of creations of pairs of critical points with respect to the hierarchy is discussed. We clarify the theory with an artificial image and a simulated MR image."'),
('"Understanding Camera Trade-Offs through a Bayesian Analysis of Light Field Projections"', '"ECCV 2008"', '["Sensor Element", "Stereo Camera", "Depth Discontinuity", "Band Limited Signal", "Joint Reconstruct', '"https://doi.org/10.1007/978-3-540-88693-8_7"', '"Computer vision has traditionally focused on extracting structure, such as depth, from images acquired using thin-lens or pinhole optics. The development of computational imaging is broadening this scope; a variety of unconventional cameras do not directly capture a traditional image anymore, but instead require the joint reconstruction of structure and image information. For example, recent coded aperture designs have been optimized to facilitate the joint reconstruction of depth and intensity. The breadth of imaging designs requires new tools to understand the tradeoffs implied by different strategies."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Understanding Critical Factors in Appearance-Based Gender Categorization"', '"ECCV 2012"', '["Facial Expression", "Local Binary Pattern", "Gender Categorization", "Gabor Feature", "Gender Reco', '"https://doi.org/10.1007/978-3-642-33868-7_28"', '"Gender categorization, based on the analysis of facial appearance, can be useful in a large set of applications. In this paper we investigate the gender classification problem from a non-conventional perspective. In particular, the analysis will aim to determine the factors critically affecting the accuracy of available technologies, better explaining differences between face-based identification and gender categorization."'),
('"Understanding Iconic Image-Based Face Biometrics"', '"BioAW 2002"', '["Face Recognition", "Face Image", "Equal Error Rate", "Error Index", "Face Recognition System"]', '"https://doi.org/10.1007/3-540-47917-1_3"', '"In the last decade, many recognition and authentication systems based on biometric mesaurements have been proposed. Still algorithms based on face images are quite appealing for the possibility to easy adapt and taylor a system to many application domains."'),
('"Understanding Road Scenes Using Visual Cues and GPS Information"', '"ECCV 2012"', '["Adverse Weather Condition", "Digital Database", "Advanced Driver Assistance System", "Road Scene",', '"https://doi.org/10.1007/978-3-642-33885-4_70"', '"Understanding road scenes is important in computer vision with different applications to improve road safety (e.g., advanced driver assistance systems) and to develop autonomous driving systems (e.g., Google driver-less vehicle). Current vision\\u2013based approaches rely on the robust combination of different technologies including color and texture recognition, object detection, scene context understanding. However, the performance of these approaches drops\\u2013off in complex acquisition conditions with reduced visibility (e.g., dusk, dawn, night) or adverse weather conditions (e.g., rainy, snowy, foggy). In these adverse situations any prior information about the scene is relevant to constraint the process. Therefore, in this demo we show a novel approach to obtain on\\u2013line prior information about the road ahead a moving vehicle to improve road scene understanding algorithms. This combination exploits the robustness of digital databases and the adaptation of algorithms based on visual information acquired in real time. Experimental results in challenging road scenarios show the applicability of the algorithm to improve vision\\u2013based road scene understanding algorithms. Furthermore, the algorithm can also be applied to correct imprecise road information in the database."'),
('"Underwater Camera Calibration"', '"ECCV 2000"', '["Focal Length", "Camera Calibration", "Principal Point", "Principal Plane", "Underwater Camera"]', '"https://doi.org/10.1007/3-540-45053-X_42"', '"This article deals with optical laws that must be considered when using underwater cameras. Both theoretical and experimental point of views are described, and it is shown that relationships between air and water calibration can be found."'),
('"Undoing the Damage of Dataset Bias"', '"ECCV 2012"', '["Target Domain", "Domain Adaptation", "Transfer Learning", "Visual World", "Spatial Pyramid"]', '"https://doi.org/10.1007/978-3-642-33718-5_12"', '"The presence of bias in existing object recognition datasets is now well-known in the computer vision community. While it remains in question whether creating an unbiased dataset is possible given limited resources, in this work we propose a discriminative framework that directly exploits dataset bias during training. In particular, our model learns two sets of weights: (1) bias vectors associated with each individual dataset, and (2) visual world weights that are common to all datasets, which are learned by undoing the associated bias from each dataset. The visual world weights are expected to be our best possible approximation to the object model trained on an unbiased dataset, and thus tend to have good generalization ability. We demonstrate the effectiveness of our model by applying the learned weights to a novel, unseen dataset, and report superior results for both classification and detection tasks compared to a classical SVM that does not account for the presence of bias. Overall, we find that it is beneficial to explicitly account for bias when combining multiple datasets."'),
('"Unfolding an Indoor Origami World"', '"ECCV 2014"', '["Grid Cell", "Indoor Scene", "Local Evidence", "Cluttered Scene", "Concave Edge"]', '"https://doi.org/10.1007/978-3-319-10599-4_44"', '"In this work, we present a method for single-view reasoning about 3D surfaces and their relationships. We propose the use of mid-level constraints for 3D scene understanding in the form of convex and concave edges and introduce a generic framework capable of incorporating these and other constraints. Our method takes a variety of cues and uses them to infer a consistent interpretation of the scene. We demonstrate improvements over the state-of-the art and produce interpretations of the scene that link large planar surfaces."'),
('"Unified Crowd Segmentation"', '"ECCV 2008"', '["Partial Occlusion", "Color Constancy", "Crowded Scene", "Pairwise Likelihood", "IEEE Computer Visi', '"https://doi.org/10.1007/978-3-540-88693-8_51"', '"This paper presents a unified approach to crowd segmentation. A global solution is generated using an Expectation Maximization framework. Initially, a head and shoulder detector is used to nominate an exhaustive set of person locations and these form the person hypotheses. The image is then partitioned into a grid of small patches which are each assigned to one of the person hypotheses. A key idea of this paper is that while whole body monolithic person detectors can fail due to occlusion, a partial response to such a detector can be used to evaluate the likelihood of a single patch being assigned to a hypothesis. This captures local appearance information without having to learn specific appearance models. The likelihood of a pair of patches being assigned to a person hypothesis is evaluated based on low level image features such as uniform motion fields and color constancy. During the E-step, the single and pairwise likelihoods are used to compute a globally optimal set of assignments of patches to hypotheses. In the M-step, parameters which enforce global consistency of assignments are estimated. This can be viewed as a form of occlusion reasoning. The final assignment of patches to hypotheses constitutes a segmentation of the crowd. The resulting system provides a global solution that does not require background modeling and is robust with respect to clutter and partial occlusion."'),
('"Unified Frequency Domain Analysis of Lightfield Cameras"', '"ECCV 2008"', '["Frequency Domain", "Frequency Domain Analysis", "Bandlimited Signal", "Conventional Camera", "Came', '"https://doi.org/10.1007/978-3-540-88690-7_17"', '"This paper presents a theory that encompasses both \\u201cplenoptic\\u201d (microlens based) and \\u201cheterodyning\\u201d (mask based) cameras in a single frequency-domain mathematical formalism. Light-field capture has traditionally been analyzed using spatio-angular representation, with the exception of the frequency-domain \\u201cheterodyning\\u201d work. In this paper we interpret \\u201cheterodyning\\u201d as a general theory of multiplexing the radiance in the frequency domain. Using this interpretation, we derive a mathematical theory of recovering the 4D spatial and angular information from the multiplexed 2D frequency representation. The resulting method is applicable to all lightfield cameras, lens-based and mask-based. The generality of our approach suggests new designs for lightfield cameras. We present one such novel lightfield camera, based on a mask outside a conventional camera. Experimental results are presented for all cameras described."'),
('"Unifying Approaches and Removing Unrealistic Assumptions in Shape from Shading: Mathematics Can Hel', '"ECCV 2004"', '["Weak Solution", "Viscosity Solution", "State Constraint", "Eikonal Equation", "Perspective Project', '"https://doi.org/10.1007/978-3-540-24673-2_12"', '"This article proposes a solution of the Lambertian Shape From Shading (SFS) problem by designing a new mathematical framework based on the notion of viscosity solutions. The power of our approach is twofolds: 1) it defines a notion of weak solutions (in the viscosity sense) which does not necessarily require boundary data. Note that, in the previous SFS work of Rouy et al. [23,15], Falcone et al. [8], Prados et al. [22,20], the characterization of a viscosity solution and its computation require the knowledge of its values on the boundary of the image. This was quite unrealistic because in practice such values are not known. 2) it unifies the work of Rouy et al. [23,15], Falcone et al. [8], Prados et al. [22,20], based on the notion of viscosity solutions and the work of Dupuis and Oliensis [6] dealing with classical (C 1) solutions. Also, we generalize their work to the \\u201cperspective SFS\\u201d problem recently introduced by Prados and Faugeras 20."'),
('"Unique Signatures of Histograms for Local Surface Description"', '"ECCV 2010"', '["Feature Point", "Exponential Mapping", "Unique Signature", "Total Little Square", "Surface Match"]', '"https://doi.org/10.1007/978-3-642-15558-1_26"', '"This paper deals with local 3D descriptors for surface matching. First, we categorize existing methods into two classes: Signatures and Histograms. Then, by discussion and experiments alike, we point out the key issues of uniqueness and repeatability of the local reference frame. Based on these observations, we formulate a novel comprehensive proposal for surface representation, which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor. The latter lays at the intersection between Signatures and Histograms, so as to possibly achieve a better balance between descriptiveness and robustness. Experiments on publicly available datasets as well as on range scans obtained with Spacetime Stereo provide a thorough validation of our proposal."'),
('"Unlevel-Sets: Geometry and Prior-Based Segmentation"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24673-2_5"', '"We present a novel variational approach to top-down image segmentation, which accounts for significant projective transformations between a single prior image and the image to be segmented. The proposed segmentation process is coupled with reliable estimation of the transformation parameters, without using point correspondences. The prior shape is represented by a generalized cone that is based on the contour of the reference object. Its unlevel sections correspond to possible instances of the visible contour under perspective distortion and scaling. We extend the Chan-Vese energy functional by adding a shape term. This term measures the distance between the currently estimated section of the generalized cone and the region bounded by the zero-crossing of the evolving level set function. Promising segmentation results are obtained for images of rotated, translated, corrupted and partly occluded objects. The recovered transformation parameters are compatible with the ground truth."'),
('"Unsupervised Activity Analysis and Monitoring Algorithms for Effective Surveillance Systems"', '"ECCV 2012"', '["Anomaly Detection", "Metro Station", "Group Detection", "Advance Video", "Audio Event"]', '"https://doi.org/10.1007/978-3-642-33885-4_80"', '"In this demonstration, we will show the different modules related to the automatic surveillance prototype developed in the context of the EU VANAHEIM project. Several components will be demonstrated on real data from the Torino metro. First, different unsupervised activity modeling algorithms that capture recurrent activities from long recordings will be illustrated. A contrario, they provide unusuallness measures that can be used to select the most interesting streams to be displayed in control rooms. Second, different scene analysis algorithms will be demonstrated, ranging from left-luggage detection to the automatic identification of groups and their tracking. Third, a set of situationnal reporting methods (flow and count monitoring in escalators and at platforms as well as human presence at lift ) that provide a global view of the activity in the metro station and are displayed on maps or along with analyzed video streams. Finally, an offline activity discovery tool based on long term recordings. All algorithms are integrated into a Video Management Solution using an innovative VideoWall module that will be demonstrated as well."'),
('"Unsupervised and Supervised Visual Codes with Restricted Boltzmann Machines"', '"ECCV 2012"', '["Unsupervised Learning", "Sparse Code", "Feature Code", "Restricted Boltzmann Machine", "Visual Cod', '"https://doi.org/10.1007/978-3-642-33715-4_22"', '"Recently, the coding of local features (e.g. SIFT) for image categorization tasks has been extensively studied. Incorporated within the Bag of Words (BoW) framework, these techniques optimize the projection of local features into the visual codebook, leading to state-of-the-art performances in many benchmark datasets. In this work, we propose a novel visual codebook learning approach using the restricted Boltzmann machine (RBM) as our generative model. Our contribution is three-fold. Firstly, we steer the unsupervised RBM learning using a regularization scheme, which decomposes into a combined prior for the sparsity of each feature\\u2019s representation as well as the selectivity for each codeword. The codewords are then fine-tuned to be discriminative through the supervised learning from top-down labels. Secondly, we evaluate the proposed method with the Caltech-101 and 15-Scenes datasets, either matching or outperforming state-of-the-art results. The codebooks are compact and inference is fast. Finally, we introduce an original method to visualize the codebooks and decipher what each visual codeword encodes."'),
('"Unsupervised Classemes"', '"ECCV 2012"', '["Feature Vector", "Semantic Feature", "Multiple Kernel Learning", "Sift Descriptor", "Primitive Fea', '"https://doi.org/10.1007/978-3-642-33885-4_41"', '"In this paper we present a new model of semantic features that, unlike previously presented methods, does not rely on the presence of a labeled training data base, as the creation of the feature extraction function is done in an unsupervised manner."'),
('"Unsupervised Classification and Part Localization by Consistency Amplification"', '"ECCV 2008"', '["Class Image", "Part Localization", "Object Part", "Part Detector", "Detection Score"]', '"https://doi.org/10.1007/978-3-540-88688-4_24"', '"We present a novel method for unsupervised classification, including the discovery of a new category and precise object and part localization. Given a set of unlabelled images, some of which contain an object of an unknown category, with unknown location and unknown size relative to the background, the method automatically identifies the images that contain the objects, localizes them and their parts, and reliably learns their appearance and geometry for subsequent classification. Current unsupervised methods construct classifiers based on a fixed set of initial features. Instead, we propose a new approach which iteratively extracts new features and re-learns the induced classifier, improving class vs. non-class separation at each iteration. We develop two main tools that allow this iterative combined search. The first is a novel star-like model capable of learning a geometric class representation in the unsupervised setting. The second is learning of \\u201dpart specific features\\u201d that are optimized for parts detection, and which optimally combine different part appearances discovered in the training examples. These novel aspects lead to precise part localization and to improvement in overall classification performance compared with previous methods. We applied our method to multiple object classes from Caltech-101, UIUC and a sub-classification problem from PASCAL. The obtained results are comparable to state-of-the-art supervised classification techniques and superior to state-of-the-art unsupervised approaches previously applied to the same image sets."'),
('"Unsupervised Dense Object Discovery, Detection, Tracking and Reconstruction"', '"ECCV 2014"', '["Structure From Motion", "SLAM", "3D Tracking", "3D Reconstruction", "Dense Reconstruction", "Learn', '"https://doi.org/10.1007/978-3-319-10605-2_6"', '"In this paper, we present an unsupervised framework for discovering, detecting, tracking, and reconstructing dense objects from a video sequence. The system simultaneously localizes a moving camera, and discovers a set of shape and appearance models for multiple objects, including the scene background. Each object model is represented by both a 2D and 3D level-set. This representation is used to improve detection, 2D-tracking, 3D-registration and importantly subsequent updates to the level-set itself. This single framework performs dense simultaneous localization and mapping as well as unsupervised object discovery. At each iteration portions of the scene that fail to track, such as bulk outliers on moving rigid bodies, are used to either seed models for new objects or to update models of known objects. For the latter, once an object is successfully tracked in 2D with aid from a 2D level-set segmentation, the level-set is updated and then used to aid registration and evolution of a 3D level-set that captures shape information. For a known object either learned by our system or introduced from a third-party library, our framework can detect similar appearances and geometries in the scene. The system is tested using single and multiple object data sets. Results demonstrate an improved method for discovering and reconstructing 2D and 3D object models, which aid tracking even under significant occlusion or rapid motion."'),
('"Unsupervised Discovery of Mid-Level Discriminative Patches"', '"ECCV 2012"', '["Visual Word", "Image Patch", "Visual World", "Spatial Pyramid", "Unlabeled Image"]', '"https://doi.org/10.1007/978-3-642-33709-3_6"', '"The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation. The desired patches need to satisfy two requirements: 1) to be representative, they need to occur frequently enough in the visual world; 2) to be discriminative, they need to be different enough from the rest of the visual world. The patches could correspond to parts, objects, \\u201cvisual phrases\\u201d, etc. but are not restricted to be any one of them. We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches. We use an iterative procedure which alternates between clustering and training discriminative classifiers, while applying careful cross-validation at each step to prevent overfitting. The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation, suggesting that it could be used in place of visual words for many tasks. Furthermore, discriminative patches can also be used in a supervised regime, such as scene classification, where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset."'),
('"Unsupervised Learning of Discriminative Relative Visual Attributes"', '"ECCV 2012"', '["Relative Attribute", "Unsupervised Learn", "Visual Attribute", "Training Class", "Dimensionality R', '"https://doi.org/10.1007/978-3-642-33885-4_7"', '"Unsupervised learning of relative visual attributes is important because it is often infeasible for a human annotator to predefine and manually label all the relative attributes in large datasets. We propose a method for learning relative visual attributes given a set of images for each training class. The method is unsupervised in the sense that it does not require a set of predefined attributes. We formulate the learning as a mixed-integer programming problem and propose an efficient algorithm to solve it approximately. Experiments show that the learned attributes can provide good generalization and tend to be more discriminative than hand-labeled relative attributes. While in the unsupervised setting the learned attributes do not have explicit names, many are highly correlated with human annotated attributes and this demonstrates that our method is able to discover relative attributes automatically."'),
('"Unsupervised Learning of Functional Categories in Video Scenes"', '"ECCV 2010"', '["Feature Vector", "Functional Category", "Anomaly Detection", "Unsupervised Learn", "Semantic Label', '"https://doi.org/10.1007/978-3-642-15552-9_48"', '"Existing methods for video scene analysis are primarily concerned with learning motion patterns or models for anomaly detection. We present a novel form of video scene analysis where scene element categories such as roads, parking areas, sidewalks and entrances, can be segmented and categorized based on the behaviors of moving objects in and around them. We view the problem from the perspective of categorical object recognition, and present an approach for unsupervised learning of functional scene element categories. Our approach identifies functional regions with similar behaviors in the same scene and/or across scenes, by clustering histograms based on a trajectory-level, behavioral codebook. Experiments are conducted on two outdoor webcam video scenes with low frame rates and poor quality. Unsupervised classification results are presented for each scene independently, and also jointly where models learned on one scene are applied to the other."'),
('"Unsupervised Learning of Models for Recognition"', '"ECCV 2000"', '["Expectation Maximization", "Training Image", "Object Class", "Unsupervised Learn", "Expectation Ma', '"https://doi.org/10.1007/3-540-45054-8_2"', '"We present a method to learn object class models from unlabeled and unsegmented cluttered scenes for the purpose of visual object recognition. We focus on a particular type of model where objects are represented as flexible constellations of rigid parts (features). The variability within a class is represented by a joint probability density function (pdf) on the shape of the constellation and the output of part detectors. In a first stage, the method automatically identifies distinctive parts in the training set by applying a clustering algorithm to patterns selected by an interest operator. It then learns the statistical shape model using expectation maximization. The method achieves very good classification results on human faces and rear views of cars."'),
('"Unsupervised Learning of Skeletons from Motion"', '"ECCV 2008"', '["Feature Point", "Motion Capture", "Neural Information Processing System", "Structure Learning", "S', '"https://doi.org/10.1007/978-3-540-88690-7_42"', '"Humans demonstrate a remarkable ability to parse complicated motion sequences into their constituent structures and motions. We investigate this problem, attempting to learn the structure of one or more articulated objects, given a time-series of two-dimensional feature positions. We model the observed sequence in terms of \\u201cstick figure\\u201d objects, under the assumption that the relative joint angles between sticks can change over time, but their lengths and connectivities are fixed. We formulate the problem in a single probabilistic model that includes multiple sub-components: associating the features with particular sticks, determining the proper number of sticks, and finding which sticks are physically joined. We test the algorithm on challenging datasets of 2D projections of optical human motion capture and feature trajectories from real videos."'),
('"Unsupervised Patch-Based Image Regularization and Representation"', '"ECCV 2006"', '["Texture Synthesis", "Denoising Method", "Adaptive Neighborhood", "Image Regularization", "Small Im', '"https://doi.org/10.1007/11744085_43"', '"A novel adaptive and patch-based approach is proposed for image regularization and representation. The method is unsupervised and based on a pointwise selection of small image patches of fixed size in the variable neighborhood of each pixel. The main idea is to associate with each pixel the weighted sum of data points within an adaptive neighborhood and to use image patches to take into account complex spatial interactions in images. In this paper, we consider the problem of the adaptive neighborhood selection in a manner that it balances the accuracy of the estimator and the stochastic error, at each spatial position. Moreover, we propose a practical algorithm with no hidden parameter for image regularization that uses no library of image patches and no training algorithm. The method is applied to both artificially corrupted and real images and the performance is very close, and in some cases even surpasses, to that of the best published denoising methods."'),
('"Unsupervised Structure Learning: Hierarchical Recursive Composition, Suspicious Coincidence and Com', '"ECCV 2008"', '["Leaf Node", "Child Node", "Unsupervised Learning", "Competitive Exclusion", "Vertical Edge"]', '"https://doi.org/10.1007/978-3-540-88688-4_56"', '"We describe a new method for unsupervised structure learning of a hierarchical compositional model (HCM) for deformable objects. The learning is unsupervised in the sense that we are given a training dataset of images containing the object in cluttered backgrounds but we do not know the position or boundary of the object. The structure learning is performed by a bottom-up and top-down process. The bottom-up process is a novel form of hierarchical clustering which recursively composes proposals for simple structures to generate proposals for more complex structures. We combine standard clustering with the suspicious coincidence principle and the competitive exclusion principle to prune the number of proposals to a practical number and avoid an exponential explosion of possible structures. The hierarchical clustering stops automatically, when it fails to generate new proposals, and outputs a proposal for the object model. The top-down process validates the proposals and fills in missing elements. We tested our approach by using it to learn a hierarchical compositional model for parsing and segmenting horses on Weizmann dataset. We show that the resulting model is comparable with (or better than) alternative methods. The versatility of our approach is demonstrated by learning models for other objects (e.g., faces, pianos, butterflies, monitors, etc.). It is worth noting that the low-levels of the object hierarchies automatically learn generic image features while the higher levels learn object specific features."'),
('"Unsupervised Temporal Commonality Discovery"', '"ECCV 2012"', '["Temporal bag of words", "branch and bound", "temporal commonality discovery"]', '"https://doi.org/10.1007/978-3-642-33765-9_27"', '"Unsupervised discovery of commonalities in images has recently attracted much interest due to the need to find correspondences in large amounts of visual data. A natural extension, and a relatively unexplored problem, is how to discover common semantic temporal patterns in videos. That is, given two or more videos, find the subsequences that contain similar visual content in an unsupervised manner. We call this problem Temporal Commonality Discovery (TCD). The naive exhaustive search approach to solve the TCD problem has a computational complexity quadratic with the length of each sequence, making it impractical for regular-length sequences. This paper proposes an efficient branch and bound (B&B) algorithm to tackle the TCD problem. We derive tight bounds for classical distances between temporal bag of words of two segments, including \\u21131, intersection and \\u03c7 2. Using these bounds the B&B algorithm can efficiently find the global optimal solution. Our algorithm is general, and it can be applied to any feature that has been quantified into histograms. Experiments on finding common facial actions in video and human actions in motion capture data demonstrate the benefits of our approach. To the best of our knowledge, this is the first work that addresses unsupervised discovery of common events in videos."'),
('"Unsupervised Texture Segmentation with Nonparametric Neighborhood Statistics"', '"ECCV 2006"', '["Neighborhood Size", "Texture Region", "Texture Synthesis", "Entropy Minimization", "Threshold Dyna', '"https://doi.org/10.1007/11744047_38"', '"This paper presents a novel approach to unsupervised texture segmentation that relies on a very general nonparametric statistical model of image neighborhoods. The method models image neighborhoods directly, without the construction of intermediate features. It does not rely on using specific descriptors that work for certain kinds of textures, but is rather based on a more generic approach that tries to adaptively capture the core properties of textures. It exploits the fundamental description of textures as images derived from stationary random fields and models the associated higher-order statistics nonparametrically. This general formulation enables the method to easily adapt to various kinds of textures. The method minimizes an entropy-based metric on the probability density functions of image neighborhoods to give an optimal segmentation. The entropy minimization drives a very fast level-set scheme that uses threshold dynamics, which allows for a very rapid evolution towards the optimal segmentation during the initial iterations. The method does not rely on a training stage and, hence, is unsupervised. It automatically tunes its important internal parameters based on the information content of the data. The method generalizes in a straightforward manner from the two-region case to an arbitrary number of regions and incorporates an efficient multi-phase level-set framework. This paper presents numerous results, for both the two-texture and multiple-texture cases, using synthetic and real images that include electron-microscopy images."'),
('"Unsupervised Video Adaptation for Parsing Human Motion"', '"ECCV 2014"', '["Unsupervised Video Pose Estimation", "Image to Video Adaptation", "Unconstrained Internet Videos"]', '"https://doi.org/10.1007/978-3-319-10602-1_23"', '"In this paper, we propose a method to parse human motion in unconstrained Internet videos without labeling any videos for training. We use the training samples from a public image pose dataset to avoid the tediousness of labeling video streams. There are two main problems confronted. First, the distribution of images and videos are different. Second, no temporal information is available in the training images. To smooth the inconsistency between the labeled images and unlabeled videos, our algorithm iteratively incorporates the pose knowledge harvested from the testing videos into the image pose detector via an adjust-and-refine method. During this process, continuity and tracking constraints are imposed to leverage the spatio-temporal information only available in videos. For our experiments, we have collected two datasets from YouTube and experiments show that our method achieves good performance for parsing human motions. Furthermore, we found that our method achieves better performance by using unlabeled video than adding more labeled pose images into the training set."'),
('"Untangling Object-View Manifold for Multiview Recognition and Pose Estimation"', '"ECCV 2014"', '["Category Recognition", "Generic Object Recognition", "Visual Manifold", "Style Vector", "Pose Esti', '"https://doi.org/10.1007/978-3-319-10593-2_29"', '"The problem of multi-view/view-invariant recognition remains one of the most fundamental challenges to the progress of the computer vision. In this paper we consider the problem of modeling the combined object-viewpoint manifold. The shape and appearance of an object in a given image is a function of its category, style within category, viewpoint, and several other factors. The visual manifold (in any chosen feature representation space) given all these variability collectively is very hard and even impossible to model. We propose an efficient computational framework that can untangle such a complex manifold, and achieve a model that separates a view-invariant category representation, from category-invariant pose representation. We outperform the state of the art in the three widely used multiview dataset, for both category recognition, and pose estimation."'),
('"UPnP: An Optimal O(n) Solution to the Absolute Pose Problem with Universal Applicability"', '"ECCV 2014"', '["PnP", "Non-perspective PnP", "Generalized absolute pose", "linear complexity", "global optimality"', '"https://doi.org/10.1007/978-3-319-10590-1_9"', '"A large number of absolute pose algorithms have been presented in the literature. Common performance criteria are computational complexity, geometric optimality, global optimality, structural degeneracies, and the number of solutions. The ability to handle minimal sets of correspondences, resulting solution multiplicity, and generalized cameras are further desirable properties. This paper presents the first PnP solution that unifies all the above desirable properties within a single algorithm. We compare our result to state-of-the-art minimal, non-minimal, central, and non-central PnP algorithms, and demonstrate universal applicability, competitive noise resilience, and superior computational efficiency. Our algorithm is called Unified PnP (UPnP)."'),
('"User Assisted Separation of Reflections from a Single Image Using a Sparsity Prior"', '"ECCV 2004"', '["Input Image", "Gaussian Mixture Model", "Single Image", "Natural Image", "Natural Scene"]', '"https://doi.org/10.1007/978-3-540-24670-1_46"', '"When we take a picture through transparent glass the image we obtain is often a linear superposition of two images: the image of the scene beyond the glass plus the image of the scene reflected by the glass. Decomposing the single input image into two images is a massively ill-posed problem: in the absence of additional knowledge about the scene being viewed there are an infinite number of valid decompositions. In this paper we focus on an easier problem: user assisted separation in which the user interactively labels a small number of gradients as belonging to one of the layers."'),
('"Using 3D Line Segments for Robust and Efficient Change Detection from Multiple Noisy Images"', '"ECCV 2008"', '["Line Segment", "Change Detection", "Test Image", "Training Image", "Scene Geometry"]', '"https://doi.org/10.1007/978-3-540-88693-8_13"', '"In this paper, we propose a new approach to change detection that is based on the appearance or disappearance of 3D lines, which may be short, as seen in a new image. These 3D lines are estimated automatically and quickly from a set of previously-taken learning-images from arbitrary view points and under arbitrary lighting conditions. 3D change detection traditionally involves unsupervised estimation of scene geometry and the associated BRDF at each observable voxel in the scene, and the comparison of a new image with its prediction. If a significant number of pixels differ in the two aligned images, a change in the 3D scene is assumed to have occurred. The importance of our approach is that by comparing images of lines rather than of gray levels, we avoid the computationally intensive, and some-times impossible, tasks of estimating 3D surfaces and their associated BRDFs in the model-building stage. We estimate 3D lines instead where the lines are due to 3D ridges or BRDF ridges which are computationally much less costly and are more reliably detected. Our method is widely applicable as man-made structures consisting of 3D line segments are the main focus of most applications. The contributions of this paper are: change detection based on appropriate interpretation of line appearance and disappearance in a new image; unsupervised estimation of \\u201cshort\\u201d 3D lines from multiple images such that the required computation is manageable and the estimation accuracy is high."'),
('"Using 3D Models for Real-Time Facial Feature Tracking, Pose Estimation, and Expression Monitoring"', '"ECCV 2012"', '["Face Gesture Recognition", "Facial Action", "Variable Illumination", "Active Shape Model", "Model ', '"https://doi.org/10.1007/978-3-642-33885-4_74"', '"We present an application which uses 3D statistical shape models to track a subject in real time using a single fixed camera. The system can handle large pose variation; variable illumination; occlusion; glasses. Since the models are 3D, the application can report pose information which may be vital in a safety context such as driving attentiveness. Two models are used in tandem, one for identity and one for facial actions, enabling the system to also estimate the user\\u2019s behavioural state at a basic level. The system works directly on the captured images, with no pre-processing, and tracks the facial features using simple template matching and boundary detection. The parameters of the identity model adapt over time to the model subspace occupied by the subject, and this allows the second model to describe simple actions such as eye, brow, and mouth movement. The parameters of the actions model are then used to identify smiling, frowning, talking, and blinking using simple linear discriminants."'),
('"Using a Connected Filter for Structure Estimation in Perspective Systems"', '"WDV 2006"', '["Feature Point", "Process Noise", "Rigid Body Motion", "Error Covariance Matrix", "Perspective Proj', '"https://doi.org/10.1007/978-3-540-70932-9_21"', '"Three-dimensional structure information can be estimated from two-dimensional perspective images using recursive estimation methods. This paper investigates possibilities to improve structure filter performance for a certain class of stochastic perspective systems by utilizing mutual information, in particular when each observed point on a rigid object is affected by the same process noise. After presenting the dynamic system of interest, the method is applied, using an extended Kalman filter for the estimation, to a simulated time-varying multiple point vision system. The performance of a connected filter is compared, using Monte Carlo methods, to that of a set of independent filters. The idea is then further illustrated and analyzed by means of a simple linear system. Finally more formal stochastic differential equation aspects, especially the impact of transformations in the It\\u00f4 sense, are discussed and related to physically realistic noise models in vision systems."'),
('"Using Dirichlet Free Form Deformation to Fit Deformable Models to Noisy 3-D Data"', '"ECCV 2002"', '["Control Point", "Computer Animation", "Natural Neighbor", "Gradient Vector Flow", "Surface Triangu', '"https://doi.org/10.1007/3-540-47967-8_47"', '"Free-form deformations (FFD) constitute an important geometric shape modification method that has been extensively investigated for computer animation and geometric modelling. In this work, we show that FFDs are also very effective to fit deformable models to the kind of noisy 3-D data that vision algorithms such as stereo tend to produce."'),
('"Using Inter-feature-Line Consistencies for Sequence-Based Object Recognition"', '"ECCV 2004"', '["Source Node", "Face Recognition", "Object Recognition", "Sink Node", "Feature Line"]', '"https://doi.org/10.1007/978-3-540-24670-1_9"', '"An image sequence-based framework for appearance-based object recognition is proposed in this paper. Compared with the methods of using a single view for object recognition, inter-frame consistencies can be exploited in a sequence-based method, so that a better recognition performance can be achieved. We use the nearest feature line method (NFL) [8] to model each object. The NFL method is extended in this paper by further integrating motion-continuity information between features lines in a probabilistic framework. The associated recognition task is formulated as maximizing an a posteriori probability measure. The recognition problem is then further transformed to a shortest-path searching problem, and a dynamic-programming technique is used to solve it."'),
('"Using Isometry to Classify Correct/Incorrect 3D-2D Correspondences"', '"ECCV 2014"', '["Input Image", "Deformable Model", "Pairwise Constraint", "Deformable Surface", "Correct Correspond', '"https://doi.org/10.1007/978-3-319-10593-2_22"', '"Template-based methods have been successfully used for surface detection and 3D reconstruction from a 2D input image, especially when the surface is known to deform isometrically. However, almost all such methods require that keypoint correspondences be first matched between the template and the input image. Matching thus exists as a current limitation because existing methods are either slow or tend to perform poorly for discontinuous or unsmooth surfaces or deformations. This is partly because the 3D isometric deformation constraint cannot be easily used in the 2D image directly. We propose to resolve that difficulty by detecting incorrect correspondences using the isometry constraint directly in 3D. We do this by embedding a set of putative correspondences in 3D space, by estimating their depth and local 3D orientation in the input image, from local image warps computed quickly and accurately by means of Inverse Composition. We then relax isometry to inextensibility to get a first correct/incorrect classification using simple pairwise constraints. This classification is then efficiently refined using higher-order constraints, which we formulate as the consistency between the correspondences\\u2019 local 3D geometry. Our algorithm is fast and has only one free parameter governing the precision/recall trade-off. We show experimentally that it significantly outperforms state-of-the-art."'),
('"Using Linking Features in Learning Non-parametric Part Models"', '"ECCV 2012"', '["linking features", "parts detection", "FLPM model"]', '"https://doi.org/10.1007/978-3-642-33712-3_24"', '"We present an approach to the detection of parts of highly deformable objects, such as the human body. Instead of using kinematic constraints on relative angles used by most existing approaches for modeling part-to-part relations, we learn and use special observed \\u2018linking\\u2019 features that support particular pairwise part configurations. In addition to modeling the appearance of individual parts, the current approach adds modeling of the appearance of part-linking, which is shown to provide useful information. For example, configurations of the lower and upper arms are supported by observing corresponding appearances of the elbow or other relevant features. The proposed model combines the support from all the linking features observed in a test image to infer the most likely joint configuration of all the parts of interest. The approach is trained using images with annotated parts, but no a-priori known part connections or connection parameters are assumed, and the linking features are discovered automatically during training. We evaluate the performance of the proposed approach on two challenging human body parts detection datasets, and obtain performance comparable, and in some cases superior, to the state-of-the-art. In addition, the approach generality is shown by applying it without modification to part detection on datasets of animal parts and of facial fiducial points."'),
('"Using Multiple Hypotheses to Improve Depth-Maps for Multi-View Stereo"', '"ECCV 2008"', '["Markov Random Field", "IEEE Conf", "Unknown State", "Depth Estimate", "Normalise Cross Correlation', '"https://doi.org/10.1007/978-3-540-88682-2_58"', '"We propose an algorithm to improve the quality of depth-maps used for Multi-View Stereo (MVS). Many existing MVS techniques make use of a two stage approach which estimates depth-maps from neighbouring images and then merges them to extract a final surface. Often the depth-maps used for the merging stage will contain outliers due to errors in the matching process. Traditional systems exploit redundancy in the image sequence (the surface is seen in many views), in order to make the final surface estimate robust to these outliers. In the case of sparse data sets there is often insufficient redundancy and thus performance degrades as the number of images decreases. In order to improve performance in these circumstances it is necessary to remove the outliers from the depth-maps. We identify the two main sources of outliers in a top performing algorithm: (1) spurious matches due to repeated texture and (2) matching failure due to occlusion, distortion and lack of texture. We propose two contributions to tackle these failure modes. Firstly, we store multiple depth hypotheses and use a spatial consistency constraint to extract the true depth. Secondly, we allow the algorithm to return an unknown state when the a true depth estimate cannot be found. By combining these in a discrete label MRF optimisation we are able to obtain high accuracy depth-maps with low numbers of outliers. We evaluate our algorithm in a multi-view stereo framework and find it to confer state-of-the-art performance with the leading techniques, in particular on the standard evaluation sparse data sets."'),
('"Using Partial Edge Contour Matches for Efficient Object Category Localization"', '"ECCV 2010"', '["Object Detection", "Interest Point", "Query Image", "Partial Match", "Reference Template"]', '"https://doi.org/10.1007/978-3-642-15555-0_3"', '"We propose a method for object category localization by partially matching edge contours to a single shape prototype of the category. Previous work in this area either relies on piecewise contour approximations, requires meaningful supervised decompositions, or matches coarse shape-based descriptions at local interest points. Our method avoids error-prone pre-processing steps by using all obtained edges in a partial contour matching setting. The matched fragments are efficiently summarized and aggregated to form location hypotheses. The efficiency and accuracy of our edge fragment based voting step yields high quality hypotheses in low computation time. The experimental evaluation achieves excellent performance in the hypotheses voting stage and yields competitive results on challenging datasets like ETHZ and INRIA horses."'),
('"Using Robust Estimation Algorithms for Tracking Explicit Curves"', '"ECCV 2002"', '["Covariance Matrix", "Gaussian Case", "Thin Black Line", "Inverse Covariance Matrix", "Measure Rand', '"https://doi.org/10.1007/3-540-47969-4_33"', '"The context of this work is lateral vehicle control using a camera as a sensor. A natural tool for controlling a vehicle is recursive filtering. The well-known Kalman filtering theory relies on Gaussian assumptions on both the state and measure random variables. However, image processing algorithms yield measurements that, most of the time, are far from Gaussian, as experimentally shown on real data in our application. It is therefore necessary to make the approach more robust, leading to the so-called robust Kalman filtering. In this paper, we review this approach from a very global point of view, adopting a constrained least squares approach, which is very similar to the half-quadratic theory, and justifies the use of iterative reweighted least squares algorithms. A key issue in robust Kalman filtering is the choice of the prediction error covariance matrix. Unlike in the Gaussian case, its computation is not straightforward in the robust case, due to the nonlinearity of the involved expectation. We review the classical alternatives and propose new ones. A theoretical study of these approximations is out of the scope of this paper, however we do provide an experimental comparison on synthetic data perturbed with Cauchy-distributed noise."'),
('"Utilization of False Color Images in Shadow Detection"', '"ECCV 2012"', '["Support Vector Machine", "False Color", "Shadow Detection", "Boost Decision Tree", "Dark Object"]', '"https://doi.org/10.1007/978-3-642-33868-7_47"', '"Shadows are illuminated as a result of Rayleigh scattering phenomenon, which happens to be more effective for small wavelengths of light. We propose utilization of false color images for shadow detection, since the transformation eliminates high frequency blue component and introduces low frequency near-infrared channel. Effectiveness of the approach is tested by using several shadow-variant texture and color-related cues proposed in the literature. Performances of these cues in regular and false color images are compared and analyzed within a supervised system by using a support vector machine classifier."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Uzawa Block Relaxation Methods for Color Image Restoration"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33868-7_49"', '"In this paper we propose to investigate the use of a vectorial total variation model with spatially varying regularization and data terms for color image denoising and restoration. We pay attention to two main minimization problems: the minimization of a weighted vectorial total variation term TV g , which acts as a regularization term, using the L 2 norm as data term or the minimization of the vectorial total variation with a spatially varying \\\\(L^1_g\\\\) norm. The optimization process takes benefit of convex optimization tools by introducing an augmented Lagrangian formulation. This formulation leads us to simple and efficient algorithms based on Uzawa block relaxation schemes that are also robust towards the choice of the penalty parameter. In this paper, We propose to study more particularly the impact of spatially varying terms (total variation term or data terms) for color image restoration. A new weighted total variation term is proposed for old parchments restoration and we also compare the use of a weighted total variation term with a spatially varying data term for impulse noise removal in color images."'),
('"V1-Inspired Features Induce a Weighted Margin in SVMs"', '"ECCV 2012"', '["Neural Information Processing System", "Linear Support Vector Machine", "Rank Reduction", "Kernel ', '"https://doi.org/10.1007/978-3-642-33709-3_5"', '"Image representations derived from simplified models of the primary visual cortex (V1), such as HOG and SIFT, elicit good performance in a myriad of visual classification tasks including object recognition/detection, pedestrian detection and facial expression classification. A central question in the vision, learning and neuroscience communities regards why these architectures perform so well. In this paper, we offer a unique perspective to this question by subsuming the role of V1-inspired features directly within a linear support vector machine (SVM). We demonstrate that a specific class of such features in conjunction with a linear SVM can be reinterpreted as inducing a weighted margin on the Kronecker basis expansion of an image. This new viewpoint on the role of V1-inspired features allows us to answer fundamental questions on the uniqueness and redundancies of these features, and offer substantial improvements in terms of computational and storage efficiency."'),
('"Vanishing Point Detection by Segment Clustering on the Projective Space"', '"ECCV 2010"', '["Vanishing point detection", "Segment clustering", "3D reconstruction"]', '"https://doi.org/10.1007/978-3-642-35740-4_25"', '"The analysis of vanishing points on digital images provides strong cues for inferring the 3D structure of the depicted scene and can be exploited in a variety of computer vision applications. In this paper, we propose a method for estimating vanishing points in images of architectural environments that can be used for camera calibration and pose estimation, important tasks in large-scale 3D reconstruction. Our method performs automatic segment clustering in projective space \\u2013 a direct transformation from the image space \\u2013 instead of the traditional bounded accumulator space. Since it works in projective space, it handles finite and infinite vanishing points, without any special condition or threshold tuning. Experiments on real images show the effectiveness of the proposed method. We identify three orthogonal vanishing points and compute the estimation error based on their relation with the Image of the Absolute Conic (IAC) and based on the computation of the camera focal length."'),
('"Variational Motion Segmentation with Level Sets"', '"ECCV 2006"', '["Computer Vision", "Ground Truth", "Motion Estimation", "Active Contour", "Coarse Scale"]', '"https://doi.org/10.1007/11744023_37"', '"We suggest a variational method for the joint estimation of optic flow and the segmentation of the image into regions of similar motion. It makes use of the level set framework following the idea of motion competition, which is extended to non-parametric motion. Moreover, we automatically determine an appropriate initialization and the number of regions by means of recursive two-phase splits with higher order region models. The method is further extended to the spatiotemporal setting and the use of additional cues like the gray value or color for the segmentation. It need not fear a quantitative comparison to pure optic flow estimation techniques: For the popular Yosemite sequence with clouds we obtain the currently most accurate result. We further uncover a mistake in the ground truth. Coarsely correcting this, we get an average angular error below 1 degree."'),
('"Variational Pairing of Image Segmentation and Blind Restoration"', '"ECCV 2004"', '["Image Segmentation", "Image Restoration", "Variational Pairing", "Blur Kernel", "Isotropic Gaussia', '"https://doi.org/10.1007/978-3-540-24671-8_13"', '"Segmentation and blind restoration are both classical problems, that are known to be difficult and have attracted major research efforts. This paper shows that the two problems are tightly coupled and can be successfully solved together. Mutual support of the segmentation and blind restoration processes within a joint variational framework is theoretically motivated, and validated by successful experimental results. The proposed variational method integrates Mumford-Shah segmentation with parametric blur-kernel recovery and image deconvolution. The functional is formulated using the \\u0393-convergence approximation and is iteratively optimized via the alternate minimization method. While the major novelty of this work is in the unified solution of the segmentation and blind restoration problems, the important special case of known blur is also considered and promising results are obtained."'),
('"Variational Shape and Reflectance Estimation Under Changing Light and Viewpoints"', '"ECCV 2006"', '["Light Variation", "Photometric Stereo", "Specular Surface", "Chess Game", "Deformable Mesh"]', '"https://doi.org/10.1007/11744023_42"', '"Fitting parameterized 3D shape and general reflectance models to 2D image data is challenging due to the high dimensionality of the problem. The proposed method combines the capabilities of classical and photometric stereo, allowing for accurate reconstruction of both textured and non-textured surfaces. In particular, we present a variational method implemented as a PDE-driven surface evolution interleaved with reflectance estimation. The surface is represented on an adaptive mesh allowing topological change. To provide the input data, we have designed a capture setup that simultaneously acquires both viewpoint and light variation while minimizing self-shadowing. Our capture method is feasible for real-world application as it requires a moderate amount of input data and processing time. In experiments, models of people and everyday objects were captured from a few dozen images taken with a consumer digital camera. The capture process recovers a photo-consistent model of spatially varying Lambertian and specular reflectance and a highly accurate geometry."'),
('"VCDB: A Large-Scale Database for Partial Copy Detection in Videos"', '"ECCV 2014"', '["Video copy detection", "benchmark dataset", "frame matching", "temporal alignment"]', '"https://doi.org/10.1007/978-3-319-10593-2_24"', '"The task of partial copy detection in videos aims at finding if one or more segments of a query video have (transformed) copies in a large dataset. Since collecting and annotating large datasets of real partial copies are extremely time-consuming, previous video copy detection research used either small-scale datasets or large datasets with simulated partial copies by imposing several pre-defined transformations (e.g., photometric or geometric changes). While the simulated datasets were useful for research, it is unknown how well the techniques developed on such data work on real copies, which are often too complex to be simulated. In this paper, we introduce a large-scale video copy database (VCDB) with over 100,000 Web videos, containing more than 9,000 copied segment pairs found through careful manual annotation. We further benchmark a baseline system on VCDB, which has demonstrated state-of-the-art results in recent copy detection research. Our evaluation suggests that existing techniques\\u2014which have shown near-perfect results on the simulated benchmarks\\u2014are far from satisfactory in detecting complex real copies. We believe that the release of VCDB will largely advance the research around this challenging problem."'),
('"Velocity-Dependent Shutter Sequences for Motion Deblurring"', '"ECCV 2010"', '["Spatial Frequency", "Point Spread Function", "Motion Blur", "Sharp Image", "Object Velocity"]', '"https://doi.org/10.1007/978-3-642-15567-3_23"', '"We address the problem of high-quality image capture of fast-moving objects in moderate light environments. In such cases, the use of a traditional shutter is known to yield non-invertible motion blur due to the loss of certain spatial frequencies. We extend the flutter shutter method of Raskar et al. to fast-moving objects by first demonstrating that no coded exposure sequence yields an invertible point spread function for all velocities. Based on this, we argue that the shutter sequence must be dependent on object velocity, and propose a method for computing such velocity-dependent sequences. We demonstrate improved image quality from velocity-dependent sequences on fast-moving objects, as compared to sequences found using the existing sampling method."'),
('"Velocity-Guided Tracking of Deformable Contours in Three Dimensional Space"', '"ECCV 2000"', '[]', '"https://doi.org/10.1007/3-540-45054-8_17"', '"This paper presents a 3D active contour model for boundary tracking, motion analysis and position prediction of non-rigid objects, which applies stereo vision and velocity control to the class of deformable contour models, known as snakes. The proposed contour evolves in three dimensional space in reaction to a 3D potential function, which is derived by projecting the contour onto the 2D stereo images. The potential function is augmented by a velocity term, which is related to the three dimensional velocity field along the contour, and is used to guide the contour displacement between subsequent images. This leads to improved spatio-temporal tracking performance, which is demonstrated through experimental results with real and synthetic images. Good tracking performance is obtained with as little as one iteration per frame, which provides a considerable advantage for real time operation."'),
('"Very Fast Template Matching"', '"ECCV 2002"', '["Normalize Correlation", "Polynomial Approximation", "Template Match", "Centralize Moment", "Integr', '"https://doi.org/10.1007/3-540-47979-1_24"', '"Template matching by normalized correlations is a common technique for determine the existence and compute the location of a shape within an image. In many cases the run time of computer vision applications is dominated by repeated computation of template matching, applied to locate multiple templates in varying scale and orientation. A straightforward implementation of template matching for an image size n and a template size k requires order of kn operations. There are fast algorithms that require order of n log n operations. We describe a new approximation scheme that requires order n operations. It is based on the idea of \\u201cIntegral-Images\\u201d, recently introduced by Viola and Jones."'),
('"Video Action Detection with Relational Dynamic-Poselets"', '"ECCV 2014"', '["Action detection", "dynamic-poselet", "sequential skeleton model"]', '"https://doi.org/10.1007/978-3-319-10602-1_37"', '"Action detection is of great importance in understanding human motion from video. Compared with action recognition, it not only recognizes action type, but also localizes its spatiotemporal extent. This paper presents a relational model for action detection, which first decomposes human action into temporal \\u201ckey poses\\u201d and then further into spatial \\u201caction parts\\u201d. Specifically, we start by clustering cuboids around each human joint into dynamic-poselets using a new descriptor. The cuboids from the same cluster share consistent geometric and dynamic structure, and each cluster acts as a mixture of body parts. We then propose a sequential skeleton model to capture the relations among dynamic-poselets. This model unifies the tasks of learning the composites of mixture dynamic-poselets, the spatiotemporal structures of action parts, and the local model for each action part in a single framework. Our model not only allows to localize the action in a video stream, but also enables a detailed pose estimation of an actor. We formulate the model learning problem in a structured SVM framework and speed up model inference by dynamic programming. We conduct experiments on three challenging action detection datasets: the MSR-II dataset, the UCF Sports dataset, and the JHMDB dataset. The results show that our method achieves superior performance to the state-of-the-art methods on these datasets."'),
('"Video and Image Bayesian Demosaicing with a Two Color Image Prior"', '"ECCV 2006"', '["Color Image", "Color Channel", "Multiple Image", "Bayer Color", "Bilinear Interpolation"]', '"https://doi.org/10.1007/11744023_40"', '"The demosaicing process converts single-CCD color representations of one color channel per pixel into full per-pixel RGB. We introduce a Bayesian technique for demosaicing Bayer color filter array patterns that is based on a statistically-obtained two color per-pixel image prior. By modeling all local color behavior as a linear combination of two fully specified RGB triples, we avoid color fringing artifacts while preserving sharp edges. Our grid-less, floating-point pixel location architecture can process both single images and multiple images from video within the same framework, with multiple images providing denser color samples and therefore better color reproduction with reduced aliasing. An initial clustering is performed to determine the underlying local two color model surrounding each pixel. Using a product of Gaussians statistical model, the underlying linear blending ratio of the two representative colors at each pixel is estimated, while simultaneously providing noise reduction. Finally, we show that by sampling the image model at a finer resolution than the source images during reconstruction, our continuous demosaicing technique can super-resolve in a single step."'),
('"Video Compass"', '"ECCV 2002"', '["Vanishing point estimation", "relative orientation", "calibration using vanishing points", "vision', '"https://doi.org/10.1007/3-540-47979-1_32"', '"In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera with respect to the scene."'),
('"Video Matting Using Multi-frame Nonlocal Matting Laplacian"', '"ECCV 2012"', '["Video Frame", "Soft Constraint", "Temporal Coherence", "Alpha Matte", "Video Block"]', '"https://doi.org/10.1007/978-3-642-33783-3_39"', '"We present an algorithm for extracting high quality temporally coherent alpha mattes of objects from a video. Our approach extends the conventional image matting approach, i.e. closed-form matting, to video by using multi-frame nonlocal matting Laplacian. Our multi-frame nonlocal matting Laplacian is defined over a nonlocal neighborhood in spatial temporal domain, and it solves the alpha mattes of several video frames all together simultaneously. To speed up computation and to reduce memory requirement for solving the multi-frame nonlocal matting Laplacian, we use the approximate nearest neighbor(ANN) to find the nonlocal neighborhood and the k-d tree implementation to divide the nonlocal matting Laplacian into several smaller linear systems. Finally, we adopt the nonlocal mean regularization to enhance temporal coherence of the estimated alpha mattes and to correct alpha matte errors at low contrast regions. We demonstrate the effectiveness of our approach on various examples with qualitative comparisons to the results from previous matting algorithms."'),
('"Video Mensuration Using a Stationary Camera"', '"ECCV 2006"', '["Line Segment", "Reference Plane", "World System", "Reference Length", "Stationary Camera"]', '"https://doi.org/10.1007/11744078_13"', '"This paper presents a method for video mensuration using a single stationary camera. The problem we address is simple, i.e., the mensuration of any arbitrary line segment on the reference plane using multiple frames with minimal calibration. Unlike previous solutions that are based on planar rectification, our approach is based on fitting the image of multiple concentric circles on the plane. Further, the proposed method aims to minimize the error in mensuration. Hence we can calculate the mensuration of the line segments not lying on the reference plane. Using an algorithm for detecting and tracking wheels of an automobile, we have implemented a fully automatic system for wheel base mensuration. The mensuration results are accurate enough that they can be used to determine the vehicle classes. Furthermore, we measure the line segment between any two points on the vehicle and plot them in top and side views."'),
('"Video Object Co-segmentation by Regulated Maximum Weight Cliques"', '"ECCV 2014"', '["Video Segmentation", "Cosegmentation"]', '"https://doi.org/10.1007/978-3-319-10584-0_36"', '"In this paper, we propose a novel approach for object co-segmentation in arbitrary videos by sampling, tracking and matching object proposals via a Regulated Maximum Weight Clique (RMWC) extraction scheme. The proposed approach is able to achieve good segmentation results by pruning away noisy segments in each video through selection of object proposal tracklets that are spatially salient and temporally consistent, and by iteratively extracting weighted groupings of objects with similar shape and appearance (with-in and across videos). The object regions obtained from the video sets are used to initialize per-pixel segmentation to get the final co-segmentation results. Our approach is general in the sense that it can handle multiple objects, temporary occlusions, and objects going in and out of view. Additionally, it makes no prior assumption on the commonality of objects in the video collection. The proposed method is evaluated on publicly available multi-class video object co-segmentation dataset and demonstrates improved performance compared to the state-of-the-art methods."'),
('"Video Object Discovery and Co-segmentation with Extremely Weak Supervision"', '"ECCV 2014"', '["video object discovery", "video object co-segmentation", "spatiotemporal auto-context model", "Spa', '"https://doi.org/10.1007/978-3-319-10593-2_42"', '"Video object co-segmentation refers to the problem of simultaneously segmenting a common category of objects from multiple videos. Most existing video co-segmentation methods assume that all frames from all videos contain the target objects. Unfortunately, this assumption is rarely true in practice, particularly for large video sets, and existing methods perform poorly when the assumption is violated. Hence, any practical video object co-segmentation algorithm needs to identify the relevant frames containing the target object from all videos, and then co-segment the object only from these relevant frames. We present a spatiotemporal energy minimization formulation for simultaneous video object discovery and co-segmentation across multiple videos. Our formulation incorporates a spatiotemporal auto-context model, which is combined with appearance modeling for superpixel labeling. The superpixel-level labels are propagated to the frame level through a multiple instance boosting algorithm with spatial reasoning (Spatial-MILBoosting), based on which frames containing the video object are identified. Our method only needs to be bootstrapped with the frame-level labels for a few video frames (e.g., usually 1 to 3) to indicate if they contain the target objects or not. Experiments on three datasets validate the efficacy of our proposed method, which compares favorably with the state-of-the-art."'),
('"Video Pop-up: Monocular 3D Reconstruction of Dynamic Scenes"', '"ECCV 2014"', '["Minimum Description Length", "Dynamic Scene", "Bundle Adjustment", "Fundamental Matrice", "Motion ', '"https://doi.org/10.1007/978-3-319-10584-0_38"', '"Consider a video sequence captured by a single camera observing a complex dynamic scene containing an unknown mixture of multiple moving and possibly deforming objects. In this paper we propose an unsupervised approach to the challenging problem of simultaneously segmenting the scene into its constituent objects and reconstructing a 3D model of the scene. The strength of our approach comes from the ability to deal with real-world dynamic scenes and to handle seamlessly different types of motion: rigid, articulated and non-rigid. We formulate the problem as hierarchical graph-cut based segmentation where we decompose the whole scene into background and foreground objects and model the complex motion of non-rigid or articulated objects as a set of overlapping rigid parts. We evaluate the motion segmentation functionality of our approach on the Berkeley Motion Segmentation Dataset. In addition, to validate the capability of our approach to deal with real-world scenes we provide 3D reconstructions of some challenging videos from the YouTube-Objects dataset."'),
('"Video Registration to SfM Models"', '"ECCV 2014"', '["Root Mean Square", "Kalman Filter", "Augmented Reality", "Positional Error", "Kernel Regression"]', '"https://doi.org/10.1007/978-3-319-10602-1_1"', '"Registering image data to Structure from Motion (SfM) point clouds is widely used to find precise camera location and orientation with respect to a world model. In case of videos one constraint has previously been unexploited: temporal smoothness. Without temporal smoothness the magnitude of the pose error in each frame of a video will often dominate the magnitude of frame-to-frame pose change. This hinders application of methods requiring stable poses estimates (e.g. tracking, augmented reality). We incorporate temporal constraints into the image-based registration setting and solve the problem by pose regularization with model fitting and smoothing methods. This leads to accurate, gap-free and smooth poses for all frames. We evaluate different methods on challenging synthetic and real street-view SfM data for varying scenarios of motion speed, outlier contamination, pose estimation failures and 2D-3D correspondence noise. For all test cases a 2 to 60-fold reduction in root mean squared (RMS) positional error is observed, depending on pose estimation difficulty. For varying scenarios, different methods perform best. We give guidance which methods should be preferred depending on circumstances and requirements."'),
('"Video Registration Using Dynamic Textures"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_38"', '"We propose a dynamic texture feature-based algorithm for registering two video sequences of a rigid or nonrigid scene taken from two synchronous or asynchronous cameras. We model each video sequence as the output of a linear dynamical system, and transform the task of registering frames of the two sequences to that of registering the parameters of the corresponding models. This allows us to perform registration using the more classical image-based features as opposed to space-time features, such as space-time volumes or feature trajectories. As the model parameters are not uniquely defined, we propose a generic method to resolve these ambiguities by jointly identifying the parameters from multiple video sequences. We finally test our algorithm on a wide variety of challenging video sequences and show that it matches the performance of significantly more computationally expensive existing methods."'),
('"Video Summaries through Mosaic-Based Shot and Scene Clustering"', '"ECCV 2002"', '["Video Sequence", "Physical Setting", "Video Summarization", "Sport Video", "Video Summary"]', '"https://doi.org/10.1007/3-540-47979-1_26"', '"We present an approach for compact video summaries that allows fast and direct access to video data. The video is segmented into shots and, in appropriate video genres, into scenes, using previously proposed methods. A new concept that supports the hierarchical representation of video is presented, and is based on physical setting and camera locations. We use mosaics to represent and cluster shots, and detect appropriate mosaics to represent scenes. In contrast to approaches to video indexing which are based on key-frames, our efficient mosaic-based scene representation allows fast clustering of scenes into physical settings, as well as further comparison of physical settings across videos. This enables us to detect plots of different episodes in situation comedies and serves as a basis for indexing whole video sequences. In sports videos where settings are not as well defined, our approach allows classifying shots for characteristic event detection. We use a novel method for mosaic comparison and create a highly compact non-temporal representation of video. This representation allows accurate comparison of scenes across different videos and serves as a basis for indexing video libraries."'),
('"Video Synchronization Using Temporal Signals from Epipolar Lines"', '"ECCV 2010"', '["Video Sequence", "Time Shift", "Temporal Signal", "Time Synchronization", "Fundamental Matrix"]', '"https://doi.org/10.1007/978-3-642-15558-1_2"', '"Time synchronization of video sequences in a multi-camera system is necessary for successfully analyzing the acquired visual information. Even if synchronization is established, its quality may deteriorate over time due to a variety of reasons, most notably frame dropping. Consequently, synchronization must be actively maintained. This paper presents a method for online synchronization that relies only on the video sequences. We introduce a novel definition of low level temporal signals computed from epipolar lines. The spatial matching of two such temporal signals is given by the fundamental matrix. Thus, no pixel correspondence is required, bypassing the problem of correspondence changes in the presence of motion. The synchronization is determined from registration of the temporal signals. We consider general video data with substantial movement in the scene, for which high level information may be hard to extract from each individual camera (e.g., computing trajectories in crowded scenes). Furthermore, a trivial correspondence between the sequences is not assumed to exist. The method is online and can be used to resynchronize video sequences every few seconds, with only a small delay. Experiments on indoor and outdoor sequences demonstrate the effectiveness of the method."'),
('"Video-Based Action Detection Using Multiple Wearable Cameras"', '"ECCV 2014"', '["Action detection", "Multi-view videos", "Focal character", "Wearable cameras"]', '"https://doi.org/10.1007/978-3-319-16178-5_51"', '"This paper is focused on developing a new approach for video-based action detection where a set of temporally synchronized videos are taken by multiple wearable cameras from different and varying views and our goal is to accurately localize the starting and ending time of each instance of the actions of interest in such videos. Compared with traditional approaches based on fixed-camera videos, this new approach incorporates the visual attention of the camera wearers and allows for the action detection in a larger area, although it brings in new challenges such as unconstrained motion of cameras. In this approach, we leverage the multi-view information and the temporal synchronization of the input videos for more reliable action detection. Specifically, we detect and track the focal character in each video and conduct action recognition only for the focal character in each temporal sliding window. To more accurately localize the starting and ending time of actions, we develop a strategy that may merge temporally adjacent sliding windows when detecting durative actions, and non-maximally suppress temporally adjacent sliding windows when detecting momentary actions. Finally we propose a voting scheme to integrate the detection results from multiple videos for more accurate action detection. For the experiments, we collect a new dataset of multiple wearable-camera videos that reflect the complex scenarios in practice."'),
('"VideoCut: Removing Irrelevant Frames by Discovering the Object of Interest"', '"ECCV 2008"', '["Visual Word", "Image Patch", "Scale Invariant Feature Transform", "Neural Information Processing S', '"https://doi.org/10.1007/978-3-540-88682-2_34"', '"We propose a novel method for removing irrelevant frames from a video given user-provided frame-level labeling for a very small number of frames. We first hypothesize a number of candidate areas which possibly contain the object of interest, and then figure out which area(s) truly contain the object of interest. Our method enjoys several favorable properties. First, compared to approaches where a single descriptor is used to describe a whole frame, each area\\u2019s feature descriptor has the chance of genuinely describing the object of interest, hence it is less affected by background clutter. Second, by considering the temporal continuity of a video instead of treating the frames as independent, we can hypothesize the location of the candidate areas more accurately. Third, by infusing prior knowledge into the topic-motion model, we can precisely follow the trajectory of the object of interest. This allows us to largely reduce the number of candidate areas and hence reduce the chance of overfitting the data during learning. We demonstrate the effectiveness of the method by comparing it to several other semi-supervised learning approaches on challenging video clips."'),
('"View and Style-Independent Action Manifolds for Human Activity Recognition"', '"ECCV 2010"', '["Action Recognition", "Human Action Recognition", "Action Descriptor", "Visual Hull", "Nonlinear Di', '"https://doi.org/10.1007/978-3-642-15567-3_40"', '"We introduce a novel approach to automatically learn intuitive and compact descriptors of human body motions for activity recognition. Each action descriptor is produced, first, by applying Temporal Laplacian Eigenmaps to view-dependent videos in order to produce a stylistic invariant embedded manifold for each view separately. Then, all view-dependent manifolds are automatically combined to discover a unified representation which model in a single three dimensional space an action independently from style and viewpoint. In addition, a bidirectional nonlinear mapping function is incorporated to allow projecting actions between original and embedded spaces. The proposed framework is evaluated on a real and challenging dataset (IXMAS), which is composed of a variety of actions seen from arbitrary viewpoints. Experimental results demonstrate robustness against style and view variation and match the most accurate action recognition method."'),
('"View Planning Approach for Automatic 3D Digitization of Unknown Objects"', '"ECCV 2012"', '["3D Digitization", "Automation", "Automatic Scanning", "View Planning", "Next Best View", "Non-Mode', '"https://doi.org/10.1007/978-3-642-33885-4_50"', '"This paper addresses the view planning problem for the digitization of 3D objects without prior knowledge on their shape and presents a novel surface approach for the Next Best View (NBV) computation. The proposed method uses the concept of Mass Vector Chains (MVC) to define the global orientation of the scanned part. All of the viewpoints satisfying an orientation constraint are clustered using the Mean Shift technique to construct a first set of candidates for the NBV. Then, a weight is assigned to each mode according to the elementary orientations of its different descriptors. The NBV is chosen among the modes with the highest weights and which comply with the robotics constraints. Eventually, our method is generic since it is applicable to all kinds of scanners. Experiments applying a digitization cell demonstrate the feasibility and the efficiency of the approach which leads to an intuitive and fast 3D acquisition while moving efficiently the ranging device."'),
('"View Point Tracking of Rigid Objects Based on Shape Sub-manifolds"', '"ECCV 2008"', '["Active Contour", "Locally Linear Embedding", "Rigid Object", "Object Contour", "Invariant Shape"]', '"https://doi.org/10.1007/978-3-540-88690-7_19"', '"We study the task to infer and to track the viewpoint onto a 3D rigid object by observing its image contours in a sequence of images. To this end, we consider the manifold of invariant planar contours and learn the low-dimensional submanifold corresponding to the object contours by observing the object off-line from a number of different viewpoints. This submanifold of object contours can be parametrized by the view sphere and, in turn, be used for keeping track of the object orientation relative to the observer, through interpolating samples on the submanifold in a geometrically proper way. Our approach replaces explicit 3D object models by the corresponding invariant shape submanifolds that are learnt from a sufficiently large number of image contours, and is applicable to arbitrary objects."'),
('"View Synthesis for Recognizing Unseen Poses of Object Classes"', '"ECCV 2008"', '["Object Recognition", "Object Class", "Linkage Structure", "View Versus", "View Synthesis"]', '"https://doi.org/10.1007/978-3-540-88690-7_45"', '"An important task in object recognition is to enable algorithms to categorize objects under arbitrary poses in a cluttered 3D world. A recent paper by Savarese & Fei-Fei [1] has proposed a novel representation to model 3D object classes. In this representation stable parts of objects from one class are linked together to capture both the appearance and shape properties of the object class. We propose to extend this framework and improve the ability of the model to recognize poses that have not been seen in training. Inspired by works in single object view synthesis (e.g., Seitz & Dyer [2]), our new representation allows the model to synthesize novel views of an object class at recognition time. This mechanism is incorporated in a novel two-step algorithm that is able to classify objects under arbitrary and/or unseen poses. We compare our results on pose categorization with the model and dataset presented in [1]. In a second experiment, we collect a new, more challenging dataset of 8 object classes from crawling the web. In both experiments, our model shows competitive performances compared to [1] for classifying objects in unseen poses."'),
('"View Synthesis with Occlusion Reasoning Using Quasi-Sparse Feature Correspondences"', '"ECCV 2002"', '["Structure From Motion", "Surface Geometry", "Image Based Rendering"]', '"https://doi.org/10.1007/3-540-47967-8_31"', '"The goal of most image based rendering systems can be stated as follows: given a set of pictures taken from various vantage points, synthesize the image that would be obtained from a novel viewpoint. In this paper we present a novel approach to view synthesis which hinges on the observation that human viewers tend to be quite sensitive to the motion of features in the image corresponding to intensity discontinuities or edges. Our system focuses its efforts on recovering the 3D position of these features so that their motions can be synthesized correctly. In the current implementation these feature points are recovered from image sequences by employing the epipolar plane image (EPI) analysis techniques proposed by Bolles, Baker, and Marimont. The output of this procedure resembles the output of an edge extraction system where the edgels are augmented with accurate depth information. This method has the advantage of producing accurate depth estimates for most of the salient features in the scene including those corresponding to occluding contours. We will demonstrate that it is possible to produce compelling novel views based on this information."'),
('"View-Consistent 3D Scene Flow Estimation over Multiple Frames"', '"ECCV 2014"', '["Rigid Motion", "Data Term", "Multiple Frame", "Reference View", "Canonical View"]', '"https://doi.org/10.1007/978-3-319-10593-2_18"', '"We propose a method to recover dense 3D scene flow from stereo video. The method estimates the depth and 3D motion field of a dynamic scene from multiple consecutive frames in a sliding temporal window, such that the estimate is consistent across both viewpoints of all frames within the window. The observed scene is modeled as a collection of planar patches that are consistent across views, each undergoing a rigid motion that is approximately constant over time. Finding the patches and their motions is cast as minimization of an energy function over the continuous plane and motion parameters and the discrete pixel-to-plane assignment. We show that such a view-consistent multi-frame scheme greatly improves scene flow computation in the presence of occlusions, and increases its robustness against adverse imaging conditions, such as specularities. Our method currently achieves leading performance on the KITTI benchmark, for both flow and stereo."'),
('"View-Invariant Action Recognition Using Latent Kernelized Structural SVM"', '"ECCV 2012"', '["View-invariant action recognition", "latent kernelized structural SVM", "correlation feature", "mu', '"https://doi.org/10.1007/978-3-642-33715-4_30"', '"This paper goes beyond recognizing human actions from a fixed view and focuses on action recognition from an arbitrary view. A novel learning algorithm, called latent kernelized structural SVM, is proposed for the view-invariant action recognition, which extends the kernelized structural SVM framework to include latent variables. Due to the changing and frequently unknown positions of the camera, we regard the view label of action as a latent variable and implicitly infer it during both learning and inference. Motivated by the geometric correlation between different views and semantic correlation between different action classes, we additionally propose a mid-level correlation feature which describes an action video by a set of decision values from the pre-learned classifiers of all the action classes from all the views. Each decision value captures both geometric and semantic correlations between the action video and the corresponding action class from the corresponding view. After that, we combine the low-level visual cue, mid-level correlation description, and high-level label information into a novel nonlinear kernel under the latent kernelized structural SVM framework. Extensive experiments on multi-view IXMAS and MuHAVi action datasets demonstrate that our method generally achieves higher recognition accuracy than other state-of-the-art methods."'),
('"View-invariant Estimation of Height and Stride for Gait Recognition"', '"BioAW 2002"', '["Stride Length", "Gait Feature", "Human Walking", "Gait Recognition", "Gait Dynamic"]', '"https://doi.org/10.1007/3-540-47917-1_16"', '"We present a parametric method to automatically identify people in monocular low-resolution video by estimating the height and stride parameters of their walking gait. Stride parameters (stride length and cadence) are functions of body height, weight, and gender. Previous work has demonstrated effective use of these biometrics for identification and verification of people. In this paper, we show that performance is significantly improved by using height as an additional discriminant feature. Height is estimated by robustly segmenting the person from the background and fitting their apparent height to a time-dependent model. This method is correspondence-free and works with low-resolution images of people. It is also view-invariant, albeit performance is optimal in near fronto-parallel configurations. Identification accuracy is estimated at 47% for fronto-parallel sequences of 41 people, and 65% for non-fronto-parallel sequences of 17 people, compared with 18% and 51%, respectively, when only stride and cadence are used."'),
('"View-Invariant Modeling and Recognition of Human Actions Using Grammars"', '"WDV 2006"', '[]', '"https://doi.org/10.1007/978-3-540-70932-9_9"', '"In this paper, we represent human actions as sentences generated by a language built on atomic body poses or phonemes. The knowledge of body pose is stored only implicitly as a set of silhouettes seen from multiple viewpoints; no explicit 3D poses or body models are used, and individual body parts are not identified. Actions and their constituent atomic poses are extracted from a set of multiview multiperson video sequences by an automatic keyframe selection process, and are used to automatically construct a probabilistic context-free grammar (PCFG), which encodes the syntax of the actions. Given a new single viewpoint video, we can parse it to recognize actions and changes in viewpoint simultaneously. Experimental results are provided."'),
('"View-Invariant Recognition Using Corresponding Object Fragments"', '"ECCV 2004"', '["Mutual Information", "Face Recognition", "Recognition Performance", "Object Part", "Active Appeara', '"https://doi.org/10.1007/978-3-540-24671-8_12"', '"We develop a novel approach to view-invariant recognition and apply it to the task of recognizing face images under widely separated viewing directions. Our main contribution is a novel object representation scheme using \\u2018extended fragments\\u2019 that enables us to achieve a high level of recognition performance and generalization across a wide range of viewing conditions. Extended fragments are equivalence classes of image fragments that represent informative object parts under different viewing conditions. They are extracted automatically from short video sequences during learning. Using this representation, the scheme is unique in its ability to generalize from a single view of a novel object and compensate for a significant change in viewing direction without using 3D information. As a result, novel objects can be recognized from viewing directions from which they were not seen in the past. Experiments demonstrate that the scheme achieves significantly better generalization and recognition performance than previously used methods."'),
('"Viewpoint Induced Deformation Statistics and the Design of Viewpoint Invariant Features: Singularit', '"ECCV 2006"', '["Salient Region", "Region Descriptor", "Joint Histogram", "Domain Deformation", "Occlude Boundary"]', '"https://doi.org/10.1007/11744047_28"', '"We study the set of domain deformations induced on images of three-dimensional scenes by changes of the vantage point. We parametrize such deformations and derive empirical statistics on the parameters, that show a kurtotic behavior similar to that of natural image and range statistics. Such a behavior would suggest that most deformations are locally smooth, and therefore could be captured by simple parametric maps, such as affine ones. However, we show that deformations induced by singularities and occluding boundaries, although rare, are highly salient, thus warranting the development of dedicated descriptors. We therefore illustrate the development of viewpoint invariant descriptors for singularities, as well as for occluding boundaries. We test their performance on scenes where the current state of the art based on affine-invariant region descriptors fail to establish correspondence, highlighting the features and shortcomings of our approach."'),
('"Viewpoint Invariant Collective Activity Recognition with Relative Action Context"', '"ECCV 2012"', '["Collective Activity", "Quantization Error", "Post Process", "Sparse Code", "Action Context"]', '"https://doi.org/10.1007/978-3-642-33885-4_26"', '"This paper presents an approach for collective activity recognition. Collective activities are activities performed by multiple persons, such as queueing in a line and talking together. To recognize them, the action context (AC) descriptor [1] encodes the \\u201capparent\\u201d relation (e.g. a group crossing and facing \\u201cright\\u201d), however this representation is sensitive to viewpoint change. We instead propose a novel feature representation called the relative action context (RAC) descriptor that encodes the \\u201crelative\\u201d relation (e.g. a group crossing and facing the \\u201csame\\u201d direction). This representation is viewpoint invariant and complementary to AC; hence we employ a simplified combinational classifier. This paper also introduces two methods to accelerate performance. First, to make the contexts robust to various situations, we apply post processes. Second, to reduce local classification failures, we regularize the classification using fully connected CRFs. Experimental results show that our method is applicable to various scenes and outperforms state-of-the art methods."'),
('"Viewpoint Invariant Matching via Developable Surfaces"', '"ECCV 2012"', '["Developable Surface", "Viewpoint Change", "Harris Corner", "Loop Detection", "Real World Scene"]', '"https://doi.org/10.1007/978-3-642-33868-7_7"', '"Stereo systems, time-of-flight cameras, laser range sensors and consumer depth cameras nowadays produce a wealth of image data with depth information (RGBD), yet the number of approaches that can take advantage of color and geometry data at the same time is quite limited. We address the topic of wide baseline matching between two RGBD images, i.e. finding correspondences from largely different viewpoints for recognition, model fusion or loop detection. Here we normalize local image features with respect to the underlying geometry and show a significantly increased number of correspondences. Rather than moving a virtual camera to some position in front of a dominant scene plane, we propose to unroll developable scene surfaces and detect features directly in the \\u201cwall paper\\u201d of the scene. This allows viewpoint invariant matching also in scenes with curved architectural elements or with objects like bottles, cans or (partial) cones and others. We prove the usefulness of our approach using several real world scenes with different objects."'),
('"Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features"', '"ECCV 2008"', '["Computer Vision", "IEEE Computer Society", "Appearance Model", "Feature Channel", "Correct Match"]', '"https://doi.org/10.1007/978-3-540-88682-2_21"', '"Viewpoint invariant pedestrian recognition is an important yet under-addressed problem in computer vision. This is likely due to the difficulty in matching two objects with unknown viewpoint and pose. This paper presents a method of performing viewpoint invariant pedestrian recognition using an efficiently and intelligently designed object representation, the ensemble of localized features (ELF). Instead of designing a specific feature by hand to solve the problem, we define a feature space using our intuition about the problem and let a machine learning algorithm find the best representation. We show how both an object class specific representation and a discriminative recognition model can be learned using the AdaBoost algorithm. This approach allows many different kinds of simple features to be combined into a single similarity function. The method is evaluated using a viewpoint invariant pedestrian recognition dataset and the results are shown to be superior to all previous benchmarks for both recognition and reacquisition of pedestrians."'),
('"Virtual Touch Screen for Mixed Reality"', '"CVHCI 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24837-8_6"', '"Mixed Reality (MR) opens a new dimension for Human Computer Interaction (HCI). Combined with computer vision (CV) techniques, it is possible to create advanced input devices. This paper describes a novel form of HCI for the MR environment that combines CV with MR to allow a MR user to interact with a floating virtual touch screen using their bare hands. The system allows the visualisation of the virtual interfaces and touch screen through a Head Mounted Display (HMD). Visual tracking and interpretation of the user\\u2019s hand and finger motion allows the detection of key presses on the virtual touch screen. We describe an implementation of this type of interfaces and demonstrate the results through a virtual keypad application."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Virtual Visual Hulls: Example-Based 3D Shape Inference from Silhouettes"', '"SMVP 2004"', '["Object Class", "Single View", "Virtual View", "Virtual Camera", "Visual Hull"]', '"https://doi.org/10.1007/978-3-540-30212-4_3"', '"We present a method for estimating the 3D visual hull of an object from a known class given a single silhouette or sequence of silhouettes observed from an unknown viewpoint. A non-parametric density model of object shape is learned for the given object class by collecting multi-view silhouette examples from calibrated, though possibly varied, camera rigs. To infer a 3D shape from a single input silhouette, we search for 3D shapes which maximize the posterior given the observed contour. The input is matched to component single views of the multi-view training examples. A set of viewpoint-aligned virtual views are generated from the visual hulls corresponding to these examples. The most likely visual hull for the input is then found by interpolating between the contours of these aligned views. When the underlying shape is ambiguous given a single view silhouette, we produce multiple visual hull hypotheses; if a sequence of input images is available, a dynamic programming approach is applied to find the maximum likelihood path through the feasible hypotheses over time. We show results of our algorithm on real and synthetic images of people."'),
('"Visibility Analysis and Sensor Planning in Dynamic Environments"', '"ECCV 2004"', '[]', '"https://doi.org/10.1007/978-3-540-24670-1_14"', '"We analyze visibility from static sensors in a dynamic scene with moving obstacles (people). Such analysis is considered in a probabilistic sense in the context of multiple sensors, so that visibility from even one sensor might be sufficient. Additionally, we analyze worst-case scenarios for high-security areas where targets are non-cooperative. Such visibility analysis provides important performance characterization of multi-camera systems. Furthermore, maximization of visibility in a given region of interest yields the optimum number and placement of cameras in the scene. Our analysis has applications in surveillance \\u2013 manual or automated \\u2013 and can be utilized for sensor planning in places like museums, shopping malls, subway stations and parking lots. We present several example scenes \\u2013 simulated and real \\u2013 for which interesting camera configurations were obtained using the formal analysis developed in the paper."'),
('"Visibility Maps for Improving Seam Carving"', '"ECCV 2010"', '["Input Image", "Output Image", "Contact Term", "Unary Term", "Image Editing"]', '"https://doi.org/10.1007/978-3-642-35740-4_11"', '"In this paper, we present a new, improved seam carving algorithm. Seam carving efficiently removes pixels from an image to produce a retargeted image. It has proved popular with users and has been used as a component in many retargeting algorithms. We introduce the visibility map, a new framework for pixel removing image editing methods. This allows us to cast retargeting as a binary graph labelling problem. We derive a general algorithm which uses seam carving operations for efficient greedy optimization of a well defined energy, and compare this with forward energy seam carving and shift map image editing. We test this method with varying parameters on a large number of images, and present an improved seam carving algorithm which can demonstrably produce better results. We draw general conclusions about pixel removing methods for retargeting and motivate future directions of research."'),
('"Visibility Probability Structure from SfM Datasets and Applications"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33715-4_10"', '"Large scale reconstructions of camera matrices and point clouds have been created using structure from motion from community photo collections. Such a dataset is rich in information; it represents a sampling of the geometry and appearance of the underlying space. In this paper, we encode the visibility information between and among points and cameras as visibility probabilities. The conditional visibility probability of a set of points on a point (or a set of cameras on a camera) can rank points (or cameras) based on their mutual dependence. We combine the conditional probability with a distance measure to prioritize points for fast guided search for the image localization problem. We define dual problem of feature triangulation as finding the 3D coordinates of a given image feature point. We use conditional visibility probability to quickly identify a subset of cameras in which a feature is visible."'),
('"Visibility Subspaces: Uncalibrated Photometric Stereo with Shadows"', '"ECCV 2010"', '["Subspace Cluster", "Cast Shadow", "Motion Segmentation", "Photometric Stereo", "Shadow Detection"]', '"https://doi.org/10.1007/978-3-642-15552-9_19"', '"Photometric stereo relies on inverting the image formation process, and doing this accurately requires reasoning about the visibility of light sources with respect to each image point. While simple heuristics for shadow detection suffice in some cases, they are susceptible to error. This paper presents an alternative approach for handling visibility in photometric stereo, one that is suitable for uncalibrated settings where the light directions are not known. A surface imaged under a finite set of light sources can be divided into regions having uniform visibility, and when the surface is Lambertian, these regions generally map to distinct three-dimensional illumination subspaces. We show that by identifying these subspaces, we can locate the regions and their visibilities, and in the process identify shadows. The result is an automatic method for uncalibrated Lambertian photometric stereo in the presence of shadows, both cast and attached."'),
('"Vision Correcting Displays Based on Inverse Blurring and Aberration Compensation"', '"ECCV 2014"', '["Aberrations", "Visual correction", "Multilayer display", "Deconvolution", "Transparent LCDs", "Lig', '"https://doi.org/10.1007/978-3-319-16199-0_37"', '"The concept of a vision correcting display involves digitally modifying the content of a display using measurements of the optical aberrations of the viewer\\u2019s eye so that the display can be seen in sharp focus by the user without requiring the use of eyeglasses or contact lenses. Our first approach inversely blurs the image content on a single layer. After identifying fundamental limitations of this approach, we propose the multilayer concept. We then develop a fractional frequency separation method to enhance the image contrast and build a multilayer prototype comprising transparent LCDs. Finally, we combine our viewer-adaptive inverse blurring with off-the-shelf lenslets or parallax barriers and demonstrate that the resulting vision-correcting computational display system facilitates significantly higher contrast and resolution as compared to previous solutions. We also demonstrate the capability to correct higher order aberrations."'),
('"Vision-Based Guidance and Control of Robots in Projective Space"', '"ECCV 2000"', '["Projective Space", "Rigid Motion", "Stereo Vision", "Visual Servoing", "Primitive Motion"]', '"https://doi.org/10.1007/3-540-45053-X_4"', '"In this paper, we propose a method using stereo vision for visually guiding and controlling a robot in projective three-space. Our formulation is entirely projective. Metric models are not required and are replaced with projective models of both the stereo geometry and the robot\\u2019s \\u201cprojective kinematics\\u201d. Such models are preferable since they can be identified from the vision data without any a-priori knowledge. More precisely, we present constraints on projective space that reflect the visibility and mobility underlying a given task. Using interaction matrix that relates articulation space to projective space, we decompose the task into three elementary components: a translation and two rotations. This allows us to define trajectories that are both visually and globally feasible, i.e. problems like self-occlusion, local minima, and divergent control no longer exist. In this paper, we will not adopt a straight-foward image-based trajectory tracking. Instead, a directly computed control that combines a feed-forward steering loop with a feed-back control loop, based on the Cartesian error of each of the task\\u2019s components."'),
('"Vision-Based Interpretation of Hand Gestures for Remote Control of a Computer Mouse"', '"ECCV 2006"', '["Hand Gesture", "Iterative Close Point", "Stereo Pair", "Mouse Button", "Iterative Close Point"]', '"https://doi.org/10.1007/11754336_5"', '"This paper presents a vision-based interface for controlling a computer mouse via 2D and 3D hand gestures. The proposed interface builds upon our previous work that permits the detection and tracking of multiple hands that can move freely in the field of view of a potentially moving camera system. Dependable hand tracking, combined with fingertip detection, facilitates the definition of simple and, therefore, robustly interpretable vocabularies of hand gestures that are subsequently used to enable a human operator convey control information to a computer system. Two such vocabularies are defined, implemented and validated. The first one depends only on 2D hand tracking results while the second also makes use of 3D information. As confirmed by several experiments, the proposed interface achieves accurate mouse positioning, smooth cursor movement and reliable recognition of gestures activating button events. Owing to these properties, our interface can be used as a virtual mouse for controlling any Windows application."'),
('"Vision-Based Multiple Interacting Targets Tracking via On-Line Supervised Learning"', '"ECCV 2008"', '["Target Tracking", "Image Patch", "Data Association", "Tracking Result", "Continuous Frame"]', '"https://doi.org/10.1007/978-3-540-88690-7_48"', '"Successful multi-target tracking requires locating the targets and labeling their identities. This mission becomes significantly more challenging when many targets frequently interact with each other (present partial or complete occlusions). This paper presents an on-line supervised learning based method for tracking multiple interacting targets. When the targets do not interact with each other, multiple independent trackers are employed for training a classifier for each target. When the targets are in close proximity or present occlusions, the learned classifiers are used to assist in tracking. The tracking and learning supplement each other in the proposed method, which not only deals with tough problems encountered in multi-target tracking, but also ensures the entire process to be completely on-line. Various evaluations have demonstrated that this method performs better than previous methods when the interactions occur, and can maintain the correct tracking under various complex tracking situations, including crossovers, collisions and occlusions."'),
('"Vision-Based SLAM and Moving Objects Tracking for the Perceptual Support of a Smart Walker Platform', '"ECCV 2014"', '["Visual tracking", "Human detection and tracking", "Mapping", "Egomotion estimation", "SLAMMOT", "S', '"https://doi.org/10.1007/978-3-319-16199-0_29"', '"The problems of vision-based detection and tracking of independently moving objects, localization and map construction are highly interrelated, in the sense that the solution of any of them provides valuable information to the solution of the others. In this paper, rather than trying to solve each of them in isolation, we propose a method that treats all of them simultaneously. More specifically, given visual input acquired by a moving RGBD camera, the method detects independently moving objects and tracks them in time. Additionally, the method estimates the camera (ego)motion and the motion of the tracked objects in a coordinate system that is attached to the static environment, a map of which is progressively built from scratch. The loose assumptions that the method adopts with respect to the problem parameters make it a valuable component for any robotic platform that moves in a dynamic environment and requires simultaneous tracking of moving objects, egomotion estimation and map construction. The usability of the method is further enhanced by its robustness and its low computational requirements that permit real time execution even on low-end CPUs."'),
('"Vision-Based Vehicle Localization Using a Visual Street Map with Embedded SURF Scale"', '"ECCV 2014"', '["Ego-localization", "Monocular vision", "Dynamic time warping", "SURF", "Vehicle navigation"]', '"https://doi.org/10.1007/978-3-319-16178-5_11"', '"Accurate vehicle positioning is important not only for in-car navigation systems but is also a requirement for emerging autonomous driving methods. Consumer level GPS are inaccurate in a number of driving environments such as in tunnels or areas where tall buildings cause satellite shadowing. Current vision-based methods typically rely on the integration of multiple sensors or fundamental matrix calculation which can be unstable when the baseline is small."'),
('"Visual Code-Sentences: A New Video Representation Based on Image Descriptor Sequences"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33863-2_32"', '"We present a new descriptor-sequence model for action recognition that enhances discriminative power in the spatio-temporal context, while maintaining robustness against background clutter as well as variability in inter-/intra-person behavior. We extend the framework of Dense Trajectories based activity recognition (Wang et al., 2011) and introduce a pool of dynamic Bayesian networks (e.g., multiple HMMs) with histogram descriptors as codebooks of composite action categories represented at respective key points. The entire codebooks bound with spatio-temporal interest points constitute intermediate feature representation as basis for generic action categories. This representation scheme is intended to serve as visual code-sentences which subsume a rich vocabulary of basis action categories. Through extensive experiments using KTH, UCF Sports, and Hollywood2 datasets, we demonstrate some improvements over the state-of-the-art methods."'),
('"Visual Cortex as a General-Purpose Information-Processing Device"', '"ECCV 2012"', '["computational neuroscience", "development", "self-organization"]', '"https://doi.org/10.1007/978-3-642-33863-2_49"', '"Experiments on the primary visual cortex (V1) of monkeys have established that (1) V1 neurons respond to certain low-level visual features like orientation and color at specific locations, (2) this selectivity is preserved over wide ranges in contrast, (3) preferences are each mapped smoothly across the V1 surface, and (4) surround modulation effects and visual illusions result from complex patterns of interaction between these neurons. Although these properties are specific to vision, this paper describes how each can arise from a generic cortical architecture and local learning rules. In this approach, initially unspecific model neurons automatically become specialized for typical patterns of incoming neural activity, forming detailed representations of visual properties through self-organization. The resulting computational model suggests that it may be possible to devise a relatively simple, general, high-performance system for processing visual and other real-world data."'),
('"Visual Data Fusion for Objects Localization by Active Vision"', '"ECCV 2002"', '["Object Localization", "Camera Motion", "Visual Data", "Active Vision", "Visual Servoing"]', '"https://doi.org/10.1007/3-540-47979-1_21"', '"Visual sensors provide exclusively uncertain and partial knowledge of a scene. In this article, we present a suitable scene knowledge representation that makes integration and fusion of new, uncertain and partial sensor measures possible. It is based on a mixture of stochastic and set membership models. We consider that, for a large class of applications, an approximated representation is sufficient to build a preliminary map of the scene. Our approximation mainly results in ellipsoidal calculus by means of a normal assumption for stochastic laws and ellipsoidal over or inner bounding for uniform laws. These approximations allow us to build an efficient estimation process integrating visual data on line. Based on this estimation scheme, optimal exploratory motions of the camera can be automatically determined. Real time experimental results validating our approach are finally given."'),
('"Visual Dictionary Learning for Joint Object Categorization and Segmentation"', '"ECCV 2012"', '[]', '"https://doi.org/10.1007/978-3-642-33715-4_52"', '"Representing objects using elements from a visual dictionary is widely used in object detection and categorization. Prior work on dictionary learning has shown improvements in the accuracy of object detection and categorization by learning discriminative dictionaries. However none of these dictionaries are learnt for joint object categorization and segmentation. Moreover, dictionary learning is often done separately from classifier training, which reduces the discriminative power of the model. In this paper, we formulate the semantic segmentation problem as a joint categorization, segmentation and dictionary learning problem. To that end, we propose a latent conditional random field (CRF) model in which the observed variables are pixel category labels and the latent variables are visual word assignments. The CRF energy consists of a bottom-up segmentation cost, a top-down bag of (latent) words categorization cost, and a dictionary learning cost. Together, these costs capture relationships between image features and visual words, relationships between visual words and object categories, and spatial relationships among visual words. The segmentation, categorization, and dictionary learning parameters are learnt jointly using latent structural SVMs, and the segmentation and visual words assignments are inferred jointly using energy minimization techniques. Experiments on the Graz02 and CamVid datasets demonstrate the performance of our approach."'),
('"Visual Encoding of Tilt from Optic Flow: Psychophysics and Computational Modelling"', '"ECCV 2000"', '["Optic Flow", "Large Field", "Small Field", "Perspective Projection", "Orthographic Projection"]', '"https://doi.org/10.1007/3-540-45053-X_51"', '"Many computational models indicate ambiguities in the recovery of plane orientation from optic flow. Here we questioned whether psychophysical responses agree with these models. We measured the perceived tilt of a plane rotating in depth with two-view stimuli for 9 human observers. Response accuracy was higher under wide-field perspective projection (60\\u00b0) than in small field (8\\u00b0). Also, it decreased when the tilt and frontal translation were orthogonal rather than parallel. This effect was stronger in small field than in large field. Different computational models focusing on the recovery of plane orientation from optic flow can account for our results when associated with a hypothesis of minimal translation in depth. However, the twofold ambiguity predicted by these models is usually not found. Rather, most responses show a shift of the reported tilts toward the spurious solution with concomitant increase in response variability. Such findings point to the need for further simulations of the computational models."'),
('"Visual Interaction Including Biometrics Information for a Socially Assistive Robotic Platform"', '"ECCV 2014"', '["Linear Discriminant Analysis", "Local Binary Pattern", "Humanoid Robot", "Face Detection", "Active', '"https://doi.org/10.1007/978-3-319-16199-0_28"', '"This work introduces biometrics as a way to improve human-robot interaction. In particular, gender and age estimation algorithms are used to provide awareness of the user biometrics to a humanoid robot (Aldebaran NAO), in order to properly react with a specific gender/age behavior. The system can also manage multiple persons at the same time, recognizing the age and gender of each participant. All the estimation algorithms employed have been validated through a k-fold test and successively practically tested in a real human-robot interaction environment, allowing for a better natural interaction. Our system is able to work at a frame rate of 13 fps with 640\\\\(\\\\times \\\\)480 images taken from NAO\\u2019s embedded camera. The proposed application is well-suited for all assisted environments that consider the presence of a socially assistive robot like therapy with disable people, dementia, post-stroke rehabilitation, Alzheimer disease or autism."'),
('"Visual Object Tracking for the Extraction of Multiple Interacting Plant Root Systems"', '"ECCV 2014"', '["Multiple object tracking", "Root system recovery", "Plant interaction", "X-ray micro computed tomo', '"https://doi.org/10.1007/978-3-319-16220-1_7"', '"We propose a visual object tracking framework for the extraction of multiple interacting plant root systems from three-dimensional X-ray micro computed tomography images of plants grown in soil. Our method is based on a level set framework guided by a greyscale intensity distribution model to identify object boundaries in image cross-sections. Root objects are followed through the data volume, while updating the tracker\\u2019s appearance models to adapt to changing intensity values. In the presence of multiple root systems, multiple trackers can be used, but need to distinguish target objects from one another in order to correctly associate roots with their originating plants. Since root objects are expected to exhibit similar greyscale intensity distributions, shape information is used to constrain the evolving level set interfaces in order to lock trackers to their correct targets. The proposed method is tested on root systems of wheat plants grown in soil."'),
('"Visual Recognition Using Local Quantized Patterns"', '"ECCV 2012"', '["Face Recognition", "Object Detection", "Local Binary Pattern", "Local Pattern", "Average Precision', '"https://doi.org/10.1007/978-3-642-33709-3_51"', '"Features such as Local Binary Patterns (LBP) and Local Ternary Patterns (LTP) have been very successful in a number of areas including texture analysis, face recognition and object detection. They are based on the idea that small patterns of qualitative local gray-level differences contain a great deal of information about higher-level image content. Current local pattern features use hand-specified codings that are limited to small spatial supports and coarse graylevel comparisons. We introduce Local Quantized Patterns (LQP), a generalization that uses lookup-table-based vector quantization to code larger or deeper patterns. LQP inherits some of the flexibility and power of visual word representations without sacrificing the run-time speed and simplicity of local pattern ones. We show that it outperforms well-established features including HOG, LBP and LTP and their combinations on a range of challenging object detection and texture classification problems."'),
('"Visual Recognition with Humans in the Loop"', '"ECCV 2010"', '["Computer Vision", "Bird Species", "Object Recognition", "Visual Recognition", "User Response"]', '"https://doi.org/10.1007/978-3-642-15561-1_32"', '"We present an interactive, hybrid human-computer method for object classification. The method applies to classes of objects that are recognizable by people with appropriate expertise (e.g., animal species or airplane model), but not (in general) by people without such expertise. It can be seen as a visual version of the 20 questions game, where questions based on simple visual attributes are posed interactively. The goal is to identify the true class while minimizing the number of questions asked, using the visual content of the image. We introduce a general framework for incorporating almost any off-the-shelf multi-class object recognition algorithm into the visual 20 questions game, and provide methodologies to account for imperfect user responses and unreliable computer vision algorithms. We evaluate our methods on Birds-200, a difficult dataset of 200 tightly-related bird species, and on the Animals With Attributes dataset. Our results demonstrate that incorporating user input drives up recognition accuracy to levels that are good enough for practical applications, while at the same time, computer vision reduces the amount of human interaction required."'),
('"Visual Tracking by Sampling Tree-Structured Graphical Models"', '"ECCV 2014"', '["Visual tracking", "tree-structured graphical model", "Markov Chain Monte Carlo (MCMC)", "manifold ', '"https://doi.org/10.1007/978-3-319-10590-1_1"', '"Probabilistic tracking algorithms typically rely on graphical models based on the first-order Markov assumption. Although such linear structure models are simple and reasonable, it is not appropriate for persistent tracking since temporal failures by short-term occlusion, shot changes, and appearance changes may impair the remaining frames significantly. More general graphical models may be useful to exploit the intrinsic structure of input video and improve tracking performance. Hence, we propose a novel offline tracking algorithm by identifying a tree-structured graphical model, where we formulate a unified framework to optimize tree structure and track a target in a principled way, based on MCMC sampling. To reduce computational cost, we also introduce a technique to find the optimal tree for a small number of key frames first and employ a semi-supervised manifold alignment technique of tree construction for all frames. We evaluated our algorithm in many challenging videos and obtained outstanding results compared to the state-of-the-art techniques quantitatively and qualitatively."'),
('"Visual Tracking Using a Pixelwise Spatiotemporal Oriented Energy Representation"', '"ECCV 2010"', '["Motion Estimation", "Feature Representation", "Visual Tracking", "Illumination Change", "Appearanc', '"https://doi.org/10.1007/978-3-642-15561-1_37"', '"This paper presents a novel pixelwise representation for visual tracking that models both the spatial structure and dynamics of a target in a unified fashion. The representation is derived from spatiotemporal energy measurements that capture underlying local spacetime orientation structure at multiple scales. For interframe motion estimation, the feature representation is instantiated within a pixelwise template warping framework; thus, the spatial arrangement of the pixelwise energy measurements remains intact. The proposed target representation is extremely rich, including appearance and motion information as well as information about how these descriptors are spatially arranged. Qualitative and quantitative empirical evaluation on challenging sequences demonstrates that the resulting tracker outperforms several alternative state-of-the-art systems."'),
('"Visual Tracking via Adaptive Tracker Selection with Multiple Features"', '"ECCV 2012"', '["Visual tracking", "multiple features", "transition probability matrix", "robust likelihood functio', '"https://doi.org/10.1007/978-3-642-33765-9_3"', '"In this paper, a robust visual tracking method is proposed to track an object in dynamic conditions that include motion blur, illumination changes, pose variations, and occlusions. To cope with these challenges, multiple trackers with different feature descriptors are utilized, and each of which shows different level of robustness to certain changes in an object\\u2019s appearance. To fuse these independent trackers, we propose two configurations, tracker selection and interaction. The tracker interaction is achieved based on a transition probability matrix (TPM) in a probabilistic manner. The tracker selection extracts one tracking result from among multiple tracker outputs by choosing the tracker that has the highest tracker probability. According to various changes in an object\\u2019s appearance, the TPM and tracker probability are updated in a recursive Bayesian form by evaluating each tracker\\u2019s reliability, which is measured by a robust tracker likelihood function (TLF). When the tracking in each frame is completed, the estimated object\\u2019s state is obtained and fed into the reference update via the proposed learning strategy, which retains the robustness and adaptability of the TLF and multiple trackers. The experimental results demonstrate that our proposed method is robust in various benchmark scenarios."'),
('"Visualization of Temperature Change Using RGB-D Camera and Thermal Camera"', '"ECCV 2014"', '["Thermal Image", "Iterative Close Point", "Thermal Camera", "Reprojection Error", "Online Phase"]', '"https://doi.org/10.1007/978-3-319-16178-5_27"', '"In this paper, we present a system for visualizing temperature changes in a scene using an RGB-D camera coupled with a thermal camera. This system has applications in the context of maintenance of power equipments. We propose a two-stage approach made of with an offline and an online phases. During the first stage, after the calibration, we generate a 3D reconstruction of the scene with the color and the thermal data. We then apply the Viewpoint Generative Learning (VGL) method on the colored 3D model for creating a database of descriptors obtained from features robust to strong viewpoint changes. During the second online phase we compare the descriptors extracted from the current view against the ones in the database for estimating the pose of the camera. In this situation, we can display the current thermal data and compare it with the data saved during the offline phase."'),
('"Visualizing and Understanding Convolutional Networks"', '"ECCV 2014"', '["Input Image", "Training Image", "Convolutional Neural Network", "Stochastic Gradient Descent", "Pi', '"https://doi.org/10.1007/978-3-319-10590-1_53"', '"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets."'),
('"VocMatch: Efficient Multiview Correspondence for Structure from Motion"', '"ECCV 2014"', '["Feature matching", "Image clustering", "Structure from motion"]', '"https://doi.org/10.1007/978-3-319-10578-9_4"', '"Feature matching between pairs of images is a main bottleneck of structure-from-motion computation from large, unordered image sets. We propose an efficient way to establish point correspondences between all pairs of images in a dataset, without having to test each individual pair. The principal message of this paper is that, given a sufficiently large visual vocabulary, feature matching can be cast as image indexing, subject to the additional constraints that index words must be rare in the database and unique in each image. We demonstrate that the proposed matching method, in conjunction with a standard inverted file, is 2-3 orders of magnitude faster than conventional pairwise matching. The proposed vocabulary-based matching has been integrated into a standard SfM pipeline, and delivers results similar to those of the conventional method in much less time."'),
('"Voice Activity Detection Using Wavelet-Based Multiresolution Spectrum and Support Vector Machines a', '"ECCV 2006"', '[]', '"https://doi.org/10.1007/11754336_8"', '"This paper presents a Voice Activity Detection (VAD) algorithm and efficient speech mixing algorithm for a multimedia conference. The proposed VAD uses MFCC of multiresolution spectrum based on wavelets and two classical audio parameters as audio feature, and prejudges silence by detection of multi-gate zero cross ratio, and classify noise and voice by Support Vector Machines (SVM). New speech mixing algorithm used in Multipoint Control Unit (MCU) of conferences imposes short-time power of each audio stream as mixing weight vector, and is designed for parallel processing in program. Various experiments show, proposed VAD algorithm achieves overall better performance in all SNRs than VAD of G.729b and other VAD, output audio of new speech mixing algorithm has excellent hearing perceptibility, and its computational time delay are small enough to satisfy the needs of real-time transmission, and MCU computation is lower than that based on G.729b VAD."'),
('"Volterra Filtering of Noisy Images of Curves"', '"ECCV 2002"', '["Markov Process", "Noisy Image", "Direction Process", "Illusory Contour", "Good Continuation"]', '"https://doi.org/10.1007/3-540-47977-5_40"', '"How should one filter very noisy images of curves? While blurring with a Gaussian reduces noise, it also reduces contour contrast. Both non-homogeneous and anisotropic diffusion smooth images while preserving contours, but these methods assume a single local orientation and therefore they can merge or distort nearby or crossing contours. To avoid these difficulties, we view curve enhancement as a statistical estimation problem in the three-dimensional (x, y, \\u03b8)-space of positions and directions, where our prior is a probabilistic model of an ideal edge/line map known as the curve indicator random field (cirf). Technically, this random field is a superposition of local times of Markov processes that model the individual curves; intuitively, it is an idealized artist\\u2019s sketch, where the value of the field is the amount of ink deposited by the artist\\u2019s pen. After reviewing the cirf framework and our earlier formulas for the CIRF cumulants, we compute the minimum mean squared error (mmse) estimate of the cirf embedded in large amounts of Gaussian white noise. The derivation involves a perturbation expansion in an infinite noise limit, and results in linear, quadratic, and cubic (Volterra) cirf filters for enhancing images of contours. The self-avoidingness of smooth curves in (x, y, \\u03b8) simplified our analysis and the resulting algorithms, which run in O(n log n) time, where n is the size of the input. This suggests that the Gestalt principle of good continuation may not only express the likely smoothness of contours, but it may have a computational basis as well."'),
('"Voting by Grouping Dependent Parts"', '"ECCV 2010"', '["Training Image", "Object Detection", "Query Image", "Correspondence Problem", "Query Feature"]', '"https://doi.org/10.1007/978-3-642-15555-0_15"', '"Hough voting methods efficiently handle the high complexity of multi-scale, category-level object detection in cluttered scenes. The primary weakness of this approach is however that mutually dependent local observations are independently voting for intrinsically global object properties such as object scale. All the votes are added up to obtain object hypotheses. The assumption is thus that object hypotheses are a sum of independent part votes. Popular representation schemes are, however, based on an overlapping sampling of semi-local image features with large spatial support (e.g. SIFT or geometric blur). Features are thus mutually dependent and we incorporate these dependences into probabilistic Hough voting by presenting an objective function that combines three intimately related problems: i) grouping of mutually dependent parts, ii) solving the correspondence problem conjointly for dependent parts, and iii) finding concerted object hypotheses using extended groups rather than based on local observations alone. Experiments successfully demonstrate that state-of-the-art Hough voting and even sliding windows are significantly improved by utilizing part dependences and jointly optimizing groups, correspondences, and votes."'),
('"Wavelet-Based Correlation for Stereopsis"', '"ECCV 2002"', '["Correlation Measure", "Stereo Pair", "Gabor Wavelet", "Stereoscopic Image", "Stereo Image Pair"]', '"https://doi.org/10.1007/3-540-47967-8_33"', '"Position disparity between two stereoscopic images, combined with camera calibration information, allow depth recovery. The measurement of position disparity is known to be ambiguous when the scene reflectance displays repetitive patterns. This problem is reduced if one analyzes scale disparity, as in shape from texture, which relies on the deformations of repetitive patterns to recover scene geometry from a single view."'),
('"Wavelet-Based Super-Resolution Reconstruction: Theory and Algorithm"', '"ECCV 2006"', '["Iterative Reconstruction", "Tikhonov Regularization", "Total Variation Regularization", "Wavelet D', '"https://doi.org/10.1007/11744085_23"', '"We present a theoretical analysis and a new algorithm for the problem of super-resolution imaging: the reconstruction of HR (high-resolution) images from a sequence of LR (low-resolution) images. Super-resolution imaging entails solutions to two problems. One is the alignment of image frames. The other is the reconstruction of a HR image from multiple aligned LR images. Our analysis of the latter problem reveals insights into the theoretical limits of super-resolution reconstruction. We find that at best we can reconstruct a HR image blurred by a specific low-pass filter. Based on the analysis we present a new wavelet-based iterative reconstruction algorithm which is very robust to noise. Furthermore, it has a computationally efficient built-in denoising scheme with a nearly optimal risk bound. Roughly speaking, our method could be described as a better-conditioned iterative back-projection scheme with a fast and optimal regularization criteria in each iteration step. Experiments with both simulated and real data demonstrate that our approach has significantly better performance than existing super-resolution methods. It has the ability to remove even large amounts of mixed noise without creating smoothing artifacts."'),
('"Way to Go! Detecting Open Areas Ahead of a Walking Person"', '"ECCV 2014"', '["Depth Information", "Inertial Measurement Unit", "Obstacle Detection", "Impaired People", "Urban S', '"https://doi.org/10.1007/978-3-319-16199-0_25"', '"We determine the region in front of a walking person that is not blocked by obstacles. This is an important task when trying to assist visually impaired people or navigate autonomous robots in urban environments. We use conditional random fields to learn how to interpret texture and depth information for their accessibility. We demonstrate the effectiveness of the proposed approach on a novel dataset, which consists of urban outdoor and indoor scenes that were recorded with a handheld stereo camera."'),
('"We Are Family: Joint Pose Estimation of Multiple Persons"', '"ECCV 2010"', '["Body Part", "Group Photo", "Joint Model", "Appearance Model", "Detection Window"]', '"https://doi.org/10.1007/978-3-642-15549-9_17"', '"We present a novel multi-person pose estimation framework, which extends pictorial structures (PS) to explicitly model interactions between people and to estimate their poses jointly. Interactions are modeled as occlusions between people. First, we propose an occlusion probability predictor, based on the location of persons automatically detected in the image, and incorporate the predictions as occlusion priors into our multi-person PS model. Moreover, our model includes an inter-people exclusion penalty, preventing body parts from different people from occupying the same image region. Thanks to these elements, our model has a global view of the scene, resulting in better pose estimates in group photos, where several persons stand nearby and occlude each other. In a comprehensive evaluation on a new, challenging group photo datasets we demonstrate the benefits of our multi-person model over a state-of-the-art single-person pose estimator which treats each person independently."'),
('"Weak Hypotheses and Boosting for Generic Object Detection and Recognition"', '"ECCV 2004"', '["Object Recognition", "Training Image", "Object Detection", "Interest Point", "Local Descriptor"]', '"https://doi.org/10.1007/978-3-540-24671-8_6"', '"In this paper we describe the first stage of a new learning system for object detection and recognition. For our system we propose Boosting [5] as the underlying learning technique. This allows the use of very diverse sets of visual features in the learning process within a common framework: Boosting \\u2014 together with a weak hypotheses finder \\u2014 may choose very inhomogeneous features as most relevant for combination into a final hypothesis. As another advantage the weak hypotheses finder may search the weak hypotheses space without explicit calculation of all available hypotheses, reducing computation time. This contrasts the related work of Agarwal and Roth [1] where Winnow was used as learning algorithm and all weak hypotheses were calculated explicitly. In our first empirical evaluation we use four types of local descriptors: two basic ones consisting of a set of grayvalues and intensity moments and two high level descriptors: moment invariants [8] and SIFTs [12]. The descriptors are calculated from local patches detected by an interest point operator. The weak hypotheses finder selects one of the local patches and one type of local descriptor and efficiently searches for the most discriminative similarity threshold. This differs from other work on Boosting for object recognition where simple rectangular hypotheses [22] or complex classifiers [20] have been used. In relatively simple images, where the objects are prominent, our approach yields results comparable to the state-of-the-art [3]. But we also obtain very good results on more complex images, where the objects are located in arbitrary positions, poses, and scales in the images. These results indicate that our flexible approach, which also allows the inclusion of features from segmented regions and even spatial relationships, leads us a significant step towards generic object recognition."'),
('"Weakly Supervised Action Labeling in Videos under Ordering Constraints"', '"ECCV 2014"', '["Video Clip", "Action Recognition", "Temporal Constraint", "Dynamic Time Warping", "Convex Relaxati', '"https://doi.org/10.1007/978-3-319-10602-1_41"', '"We are given a set of video clips, each one annotated with an ordered list of actions, such as \\u201cwalk\\u201d then \\u201csit\\u201d then \\u201canswer phone\\u201d extracted from, for example, the associated text script. We seek to temporally localize the individual actions in each clip as well as to learn a discriminative classifier for each action. We formulate the problem as a weakly supervised temporal assignment with ordering constraints. Each video clip is divided into small time intervals and each time interval of each video clip is assigned one action label, while respecting the order in which the action labels appear in the given annotations. We show that the action label assignment can be determined together with learning a classifier for each action in a discriminative manner. We evaluate the proposed model on a new and challenging dataset of 937 video clips with a total of 787720 frames containing sequences of 16 different actions from 69 Hollywood movies."'),
('"Weakly Supervised Classification of Objects in Images Using Soft Random Forests"', '"ECCV 2010"', '["Training Sample", "Random Forest", "Iterative Procedure", "Training Dataset", "Supervise Learning"', '"https://doi.org/10.1007/978-3-642-15561-1_14"', '"The development of robust classification model is among the important issues in computer vision. This paper deals with weakly supervised learning that generalizes the supervised and semi-supervised learning. In weakly supervised learning training data are given as the priors of each class for each sample. We first propose a weakly supervised strategy for learning soft decision trees. Besides, the introduction of class priors for training samples instead of hard class labels makes natural the formulation of an iterative learning procedure. We report experiments for UCI object recognition datasets. These experiments show that recognition performance close to the supervised learning can be expected using the propose framework. Besides, an application to semi-supervised learning, which can be regarded as a particular case of weakly supervised learning, further demonstrates the pertinence of the contribution. We further discuss the relevance of weakly supervised learning for computer vision applications."'),
('"Weakly Supervised Learning of Object Segmentations from Web-Scale Video"', '"ECCV 2012"', '["Object Segmentation", "Visual Concept", "Video Segmentation", "Multiple Instance Learn", "Video St', '"https://doi.org/10.1007/978-3-642-33863-2_20"', '"We propose to learn pixel-level segmentations of objects from weakly labeled (tagged) internet videos. Specifically, given a large collection of raw YouTube content, along with potentially noisy tags, our goal is to automatically generate spatiotemporal masks for each object, such as \\u201cdog\\u201d, without employing any pre-trained object detectors. We formulate this problem as learning weakly supervised classifiers for a set of independent spatio-temporal segments. The object seeds obtained using segment-level classifiers are further refined using graphcuts to generate high-precision object masks. Our results, obtained by training on a dataset of 20,000 YouTube videos weakly tagged into 15 classes, demonstrate automatic extraction of pixel-level object masks. Evaluated against a ground-truthed subset of 50,000 frames with pixel-level annotations, we confirm that our proposed methods can learn good object masks just by watching YouTube."'),
('"Weakly Supervised Learning of Objects, Attributes and Their Associations"', '"ECCV 2014"', '["Weakly supervised learning", "object attribute associations"]', '"https://doi.org/10.1007/978-3-319-10605-2_31"', '"When humans describe images they tend to use combinations of nouns and adjectives, corresponding to objects and their associated attributes respectively. To generate such a description automatically, one needs to model objects, attributes and their associations. Conventional methods require strong annotation of object and attribute locations, making them less scalable. In this paper, we model object-attribute associations from weakly labelled images, such as those widely available on media sharing sites (e.g. Flickr), where only image-level labels (either object or attributes) are given, without their locations and associations. This is achieved by introducing a novel weakly supervised non-parametric Bayesian model. Once learned, given a new image, our model can describe the image, including objects, attributes and their associations, as well as their locations and segmentation. Extensive experiments on benchmark datasets demonstrate that our weakly supervised model performs at par with strongly supervised models on tasks such as image description and retrieval based on object-attribute associations."'),
('"Weakly Supervised Learning of Part-Based Spatial Models for Visual Object Recognition"', '"ECCV 2006"', '["Spatial Model", "Maximal Clique", "Appearance Model", "Reference Node", "Visual Object Recognition', '"https://doi.org/10.1007/11744023_2"', '"In this paper we investigate a new method of learning part-based models for visual object recognition, from training data that only provides information about class membership (and not object location or configuration). This method learns both a model of local part appearance and a model of the spatial relations between those parts. In contrast, other work using such a weakly supervised learning paradigm has not considered the problem of simultaneously learning appearance and spatial models. Some of these methods use a \\u201cbag\\u201d model where only part appearance is considered whereas other methods learn spatial models but only given the output of a particular feature detector. Previous techniques for learning both part appearance and spatial relations have instead used a highly supervised learning process that provides substantial information about object part location. We show that our weakly supervised technique produces better results than these previous highly supervised methods. Moreover, we investigate the degree to which both richer spatial models and richer appearance models are helpful in improving recognition performance. Our results show that while both spatial and appearance information can be useful, the effect on performance depends substantially on the particular object class and on the difficulty of the test dataset."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"Weakly Supervised Object Localization with Latent Category Learning"', '"ECCV 2014"', '["weakly supervised learning", "object localization", "category learning", "latent semantic analysis', '"https://doi.org/10.1007/978-3-319-10599-4_28"', '"Localizing objects in cluttered backgrounds is a challenging task in weakly supervised localization. Due to large object variations in cluttered images, objects have large ambiguity with backgrounds. However, backgrounds contain useful latent information, e.g., the sky for aeroplanes. If we can learn this latent information, object-background ambiguity can be reduced to suppress the background. In this paper, we propose the latent category learning (LCL), which is an unsupervised learning problem given only image-level class labels. Firstly, inspired by the latent semantic discovery, we use the typical probabilistic Latent Semantic Analysis (pLSA) to learn the latent categories, which can represent objects, object parts or backgrounds. Secondly, to determine which category contains the target object, we propose a category selection method evaluating each category\\u2019s discrimination. We evaluate the method on the PASCAL VOC 2007 database and ILSVRC 2013 detection challenge. On VOC 2007, the proposed method yields the annotation accuracy of 48%, which outperforms previous results by 10%. More importantly, we achieve the detection average precision of 30.9%, which improves previous results by 8% and can be competitive with the supervised deformable part model (DPM) 5.0 baseline 33.7%. On ILSVRC 2013 detection, the method yields the precision of 6.0%, which is also competitive with the DPM 5.0."'),
('"Weakly Supervised Object Localization with Stable Segmentations"', '"ECCV 2008"', '["Image Categorization", "Object Categorization", "Salient Region", "Average Categorization Accuracy', '"https://doi.org/10.1007/978-3-540-88682-2_16"', '"Multiple Instance Learning (MIL) provides a framework for training a discriminative classifier from data with ambiguous labels. This framework is well suited for the task of learning object classifiers from weakly labeled image data, where only the presence of an object in an image is known, but not its location. Some recent work has explored the application of MIL algorithms to the tasks of image categorization and natural scene classification. In this paper we extend these ideas in a framework that uses MIL to recognize and localize objects in images. To achieve this we employ state of the art image descriptors and multiple stable segmentations. These components, combined with a powerful MIL algorithm, form our object recognition system called MILSS. We show highly competitive object categorization results on the Caltech dataset. To evaluate the performance of our algorithm further, we introduce the challenging Landmarks-18 dataset, a collection of photographs of famous landmarks from around the world. The results on this new dataset show the great potential of our proposed algorithm."'),
('"Weakly Supervised Shape Based Object Detection with Particle Filter"', '"ECCV 2010"', '["Particle Filter", "Object Detection", "Area Under Curve", "Spatial Layout", "Shape Class"]', '"https://doi.org/10.1007/978-3-642-15555-0_55"', '"We describe an efficient approach to construct shape models composed of contour parts with partially-supervised learning. The proposed approach can easily transfer parts structure to different object classes as long as they have similar shape. The spatial layout between parts is described by a non-parametric density, which is more flexible and easier to learn than commonly used Gaussian or other parametric distributions. We express object detection as state estimation inference executed using a novel Particle Filters (PF) framework with static observations, which is quite different from previous PF methods. Although the underlying graph structure of our model is given by a fully connected graph, the proposed PF algorithm efficiently linearizes it by exploring the conditional dependencies of the nodes representing contour parts. Experimental results demonstrate that the proposed approach can not only yield very good detection results but also accurately locates contours of target objects in cluttered images."'),
('"Weakly-Paired Maximum Covariance Analysis for Multimodal Dimensionality Reduction and Transfer Lear', '"ECCV 2010"', '["Dimensionality Reduction", "Linear Discriminant Analysis", "Local Binary Pattern", "Canonical Corr', '"https://doi.org/10.1007/978-3-642-15552-9_41"', '"We study the problem of multimodal dimensionality reduction assuming that data samples can be missing at training time, and not all data modalities may be present at application time. Maximum covariance analysis, as a generalization of PCA, has many desirable properties, but its application to practical problems is limited by its need for perfectly paired data. We overcome this limitation by a latent variable approach that allows working with weakly paired data and is still able to efficiently process large datasets using standard numerical routines. The resulting weakly paired maximum covariance analysis often finds better representations than alternative methods, as we show in two exemplary tasks: texture discrimination and transfer learning."'),
('"Wearable RGBD Indoor Navigation System for the Blind"', '"ECCV 2014"', '["Navigation System", "Mobility Performance", "Blind Subject", "Visual Odometry", "Navigation Algori', '"https://doi.org/10.1007/978-3-319-16199-0_35"', '"In this paper, we present a novel wearable RGBD camera based navigation system for the visually impaired. The system is composed of a smartphone user interface, a glass-mounted RGBD camera device, a real-time navigation algorithm, and haptic feedback system. A smartphone interface provides an effective way to communicate to the system using audio and haptic feedback. In order to extract orientational information of the blind users, the navigation algorithm performs real-time 6-DOF feature based visual odometry using a glass-mounted RGBD camera as an input device. The navigation algorithm also builds a 3D voxel map of the environment and analyzes 3D traversability. A path planner of the navigation algorithm integrates information from the egomotion estimation and mapping and generates a safe and an efficient path to a waypoint delivered to the haptic feedback system. The haptic feedback system consisting of four micro-vibration motors is designed to guide the visually impaired user along the computed path and to minimize cognitive loads. The proposed system achieves real-time performance at \\\\(28.4\\\\)Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the mobility performance in a cluttered environment. The experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices."'),
('"Webpage Saliency"', '"ECCV 2014"', '["Web Viewing", "Visual Attention", "Multiple Kernel Learning"]', '"https://doi.org/10.1007/978-3-319-10584-0_3"', '"Webpage is becoming a more and more important visual input to us. While there are few studies on saliency in webpage, we in this work make a focused study on how humans deploy their attention when viewing webpages and for the first time propose a computational model that is designed to predict webpage saliency. A dataset is built with 149 webpages and eye tracking data from 11 subjects who free-view the webpages. Inspired by the viewing patterns on webpages, multi-scale feature maps that contain object blob representation and text representation are integrated with explicit face maps and positional bias. We propose to use multiple kernel learning (MKL) to achieve a robust integration of various feature maps. Experimental results show that the proposed model outperforms its counterparts in predicting webpage saliency."'),
('"Wehrli 2.0: An Algorithm for \\u201cTidying up Art\\u201d"', '"ECCV 2012"', '["Tidying up Art", "Image Segmentation", "Label Cost Prior", "Convex Relaxation", "Convex Optimizati', '"https://doi.org/10.1007/978-3-642-33863-2_55"', '"We propose an algorithm for automatizing the task of \\u201cTidying up Art\\u201d introduced by the comedian Wehrli [1]. Driven by a strong sense of order and tidyness, Wehrli systematically dissects famous artworks into their constituents and rearranges them according to certain ordering principles. The proposed algorithmic solution to this problem builds up on a number of recent advances in image segmentation and grouping. It has two important advantages: Firstly, the computerized tidying up of art is substantially faster than manual labor requiring only a few seconds on state-of-the-art GPUs compared to many hours of manual labor. Secondly, the computed part decomposition and reordering is fully reproducible. In particular, the arrangement of parts is determined based on mathematically transparent criteria rather than the invariably subjective and irreproducible human sense of order."'),
('"Weight-Optimal Local Binary Patterns"', '"ECCV 2014"', '["Local binary patterns (LBP)", "Weight-optimal local binary patterns (WoLBP)"]', '"https://doi.org/10.1007/978-3-319-16181-5_11"', '"In this work, we have proposed a learning paradigm for obtaining weight-optimal local binary patterns (WoLBP). We first re-formulate the LBP problem into matrix multiplication with all the bitmaps flattened and then resort to the Fisher ratio criterion for obtaining the optimal weight matrix for LBP encoding. The solution is closed form and can be easily solved using one eigen-decomposition. The experimental results on the FRGC ver2.0 database have shown that the WoLBP gains significant performance improvement over traditional LBP, and such WoLBP learning procedure can be directly ported to many other LBP variants to further improve their performances."'),
('"Weighted Block-Sparse Low Rank Representation for Face Clustering in Videos"', '"ECCV 2014"', '["low rank representation", "block-sparsity", "subspace clustering", "face clustering"]', '"https://doi.org/10.1007/978-3-319-10599-4_9"', '"In this paper, we study the problem of face clustering in videos. Specifically, given automatically extracted faces from videos and two kinds of prior knowledge (the face track that each face belongs to, and the pairs of faces that appear in the same frame), the task is to partition the faces into a given number of disjoint groups, such that each group is associated with one subject. To deal with this problem, we propose a new method called weighted block-sparse low rank representation (WBSLRR) which considers the available prior knowledge while learning a low rank data representation, and also develop a simple but effective approach to obtain the clustering result of faces. Moreover, after using several acceleration techniques, our proposed method is suitable for solving large-scale problems. The experimental results on two benchmark datasets demonstrate the effectiveness of our approach."'),
('"Weighted Minimal Hypersurfaces and Their Applications in Computer Vision"', '"ECCV 2004"', '["Minimal Surface", "Surface Point", "Active Contour Model", "Minimal Hypersurface", "Visual Hull"]', '"https://doi.org/10.1007/978-3-540-24671-8_29"', '"Many interesting problems in computer vision can be formulated as a minimization problem for an energy functional. If this functional is given as an integral of a scalar-valued weight function over an unknown hypersurface, then the minimal surface we are looking for can be determined as a solution of the functional\\u2019s Euler-Lagrange equation. This paper deals with a general class of weight functions that may depend on the surface point and normal. By making use of a mathematical tool called the method of the moving frame, we are able to derive the Euler-Lagrange equation in arbitrary-dimensional space and without the need for any surface parameterization. Our work generalizes existing proofs, and we demonstrate that it yields the correct evolution equations for a variety of previous computer vision techniques which can be expressed in terms of our theoretical framework. In addition, problems involving minimal hypersurfaces in dimensions higher than three, which were previously impossible to solve in practice, can now be introduced and handled by generalized versions of existing algorithms. As one example, we sketch a novel idea how to reconstruct temporally coherent geometry from multiple video streams."'),
('"Weighted Update and Comparison for Channel-Based Distribution Field Tracking"', '"ECCV 2014"', '["Target Model", "Population Code", "Normalize Cross Correlation", "Channel Vector", "Baseline Exper', '"https://doi.org/10.1007/978-3-319-16181-5_15"', '"There are three major issues for visual object trackers: model representation, search and model update. In this paper we address the last two issues for a specific model representation, grid based distribution models by means of channel-based distribution fields. Particularly we address the comparison part of searching. Previous work in the area has used standard methods for comparison and update, not exploiting all the possibilities of the representation. In this work we propose two comparison schemes and one update scheme adapted to the distribution model. The proposed schemes significantly improve the accuracy and robustness on the Visual Object Tracking (VOT) 2014 Challenge dataset."'),
('"Well Begun Is Half Done: Generating High-Quality Seeds for Automatic Image Dataset Construction fro', '"ECCV 2014"', '["Synthetic Dataset", "Convolutional Neural Network", "Seed Image", "Deep Neural Network", "Adaptive', '"https://doi.org/10.1007/978-3-319-10593-2_26"', '"We present a fully automatic approach to construct a large-scale, high-precision dataset from noisy web images. Within the entire pipeline, we focus on generating high quality seed images for subsequent dataset growing. High quality seeds are essential as we revealed, but they have received relatively less attention in previous works with respect to how to automatically generate them. In this work, we propose a density score based on rank-order distance to identify positive seed images. The basic idea is images relevant to a concept typically are tightly clustered, while the outliers are widely scattered. Through adaptive thresholding, we guarantee the selected seeds as numerous and accurate as possible. Starting with the high quality seeds, we grow a high quality dataset by dividing seeds and conducting iterative negative and positive mining. Our system can automatically collect thousands of images for one concept/class, with a precision rate of 95% or more. Comparisons with recent state-of-the-arts also demonstrate our method\\u2019s superior performance."'),
('"What Are Textons?"', '"ECCV 2002"', '["Independent Component Analysis", "Cluster Center", "Natural Image", "Independent Component Analysi', '"https://doi.org/10.1007/3-540-47979-1_53"', '"Textons refer to fundamental micro-structures in generic natural images and thus constitute the basic elements in early (pre-attentive) visual perception. However, the word \\u201ctexton\\u201d remains a vague concept in the literature of computer vision and visual perception, and a precise mathematical definition has yet to be found. In this article, we argue that the definition of texton should be governed by a sound mathematical model of images, and the set of textons must be learned from, or best tuned to, an image ensemble. We adopt a generative image model that an image is a superposition of bases from an over-complete dictionary, then a texton is defined as a mini-template that consists of a varying number of image bases with some geometric and photometric configurations. By analogy to physics, if image bases are like protons, neutrons and electrons, then textons are like atoms. Then a small number of textons can be learned from training images as repeating micro-structures. We report four experiments for comparison. The first experiment computes clusters in feature space of filter responses. The second use transformed component analysis in both feature space and image patches. The third adopts a two-layer generative model where an image is generated by image bases and image bases are generated by textons. The fourth experiment shows textons from motion image sequences, which we call movetons."'),
('"What Can Be Known about the Radiometric Response from Images?"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47979-1_13"', '"Brightness values of pixels in an image are related to image irradiance by a non-linear function, called the radiometric response function. Recovery of this function is important since many algorithms in computer vision and image processing use image irradiance. Several investigators have described methods for recovery of the radiometric response, without using charts, from multiple exposures of the same scene. All these recovery methods are based solely on the correspondence of gray-levels in one exposure to gray-levels in another exposure. This correspondence can be described by a function we call the brightness transfer function. We show that brightness transfer functions, and thus images themselves, do not uniquely determine the radiometric response function, nor the ratios of exposure between the images. We completely determine the ambiguity associated with the recovery of the response function and the exposure ratios. We show that all previous methods break these ambiguities only by making assumptions on the form of the response function. While iterative schemes which may not converge were used previously to find the exposure ratio, we show when it can be recovered directly from the brightness transfer function. We present a novel method to recover the brightness transfer function between images from only their brightness histograms. This allows us to determine the brightness transfer function between images of different scenes whenever the change in the distribution of scene radiances is small enough. We show an example of recovery of the response function from an image sequence with scene motion by constraining the form of the response function to break the ambiguities."'),
('"What Do Four Points in Two Calibrated Images Tell Us about the Epipoles?"', '"ECCV 2004"', '["Image Point", "Geometric Construction", "Point Pair", "Point Correspondence", "Curve Branch"]', '"https://doi.org/10.1007/978-3-540-24671-8_4"', '"Suppose that two perspective views of four world points are given, that the intrinsic parameters are known, but the camera poses and the world point positions are not. We prove that the epipole in each view is then constrained to lie on a curve of degree ten. We give the equation for the curve and establish many of the curve\\u2019s properties. For example, we show that the curve has four branches through each of the image points and that it has four additional points on each conic of the pencil of conics through the four image points. We show how to compute the four curve points on each conic in closed form. We show that orientation constraints allow only parts of the curve and find that there are impossible configurations of four corresponding point pairs. We give a novel algorithm that solves for the essential matrix given three corresponding points and one epipole. We then use the theory to describe a solution, using a 1-parameter search, to the notoriously difficult problem of solving for the pose of three views given four corresponding points."'),
('"What Do I See? Modeling Human Visual Perception for Multi-person Tracking"', '"ECCV 2014"', '["View Image", "Human Detection", "Virtual Scene", "Crowd Simulation", "Attentive Vision Model"]', '"https://doi.org/10.1007/978-3-319-10605-2_21"', '"This paper presents a novel approach for multi-person tracking utilizing a model motivated by the human vision system. The model predicts human motion based on modeling of perceived information. An attention map is designed to mimic human reasoning that integrates both spatial and temporal information. The spatial component addresses human attention allocation to different areas in a scene and is represented using a retinal mapping based on the log-polar transformation while the temporal component denotes the human attention allocation to subjects with different motion velocity and is modeled as a static-dynamic attention map. With the static-dynamic attention map and retinal mapping, attention driven motion of the tracked target is estimated with a center-surround search mechanism. This perception based motion model is integrated into a data association tracking framework with appearance and motion features. The proposed algorithm tracks a large number of subjects in complex scenes and the evaluation on public datasets show promising improvements over state-of-the-art methods."'),
('"What Does Classifying More Than 10,000 Image Categories Tell Us?"', '"ECCV 2010"', '["Image Category", "Query Image", "Semantic Space", "Spatial Pyramid", "Lower Common Ancestor"]', '"https://doi.org/10.1007/978-3-642-15555-0_6"', '"Image classification is a critical task for both humans and computers. One of the challenges lies in the large scale of the semantic space. In particular, humans can recognize tens of thousands of object classes and scenes. No computer vision algorithm today has been tested at this scale. This paper presents a study of large scale categorization including a series of challenging experiments on classification with more than 10,000 image classes. We find that a) computational issues become crucial in algorithm design; b) conventional wisdom from a couple of hundred image categories on relative performance of different classifiers does not necessarily hold when the number of categories increases; c) there is a surprisingly strong relationship between the structure of WordNet (developed for studying language) and the difficulty of visual categorization; d) classification can be improved by exploiting the semantic hierarchy. Toward the future goal of developing automatic vision algorithms to recognize tens of thousands or even millions of image categories, we make a series of observations and arguments about dataset scale, category density, and image hierarchy."'),
('"What Does the Scene Look Like from a Scene Point?"', '"ECCV 2002"', '["Novel-view synthesis", "Synthesis without structure or motion"]', '"https://doi.org/10.1007/3-540-47967-8_59"', '"In this paper we examine the problem of synthesizing virtual views from scene points within the scene, i.e., from scene points which are imaged by the real cameras. On one hand this provides a simple way of defining the position of the virtual camera in an uncalibrated setting. On the other hand, it implies extreme changes in viewpoint between the virtual and real cameras. Such extreme changes in viewpoint are not typical of most New-View-Synthesis (NVS) problems."'),
('"What Does the Sky Tell Us about the Camera?"', '"ECCV 2008"', '["Azimuth Angle", "Camera Parameter", "Cloud Layer", "Horizon Line", "Camera Response Function"]', '"https://doi.org/10.1007/978-3-540-88693-8_26"', '"As the main observed illuminant outdoors, the sky is a rich source of information about the scene. However, it is yet to be fully explored in computer vision because its appearance depends on the sun position, weather conditions, photometric and geometric parameters of the camera, and the location of capture. In this paper, we propose the use of a physically-based sky model to analyze the information available within the visible portion of the sky, observed over time. By fitting this model to an image sequence, we show how to extract camera parameters such as the focal length, and the zenith and azimuth angles. In short, the sky serves as a geometric calibration target. Once the camera parameters are recovered, we show how to use the same model in two applications: 1) segmentation of the sky and cloud layers, and 2) data-driven sky matching across different image sequences based on a novel similarity measure defined on sky parameters. This measure, combined with a rich appearance database, allows us to model a wide range of sky conditions."'),
('"What Energy Functions Can Be Minimized via Graph Cuts?"', '"ECCV 2002"', '[]', '"https://doi.org/10.1007/3-540-47977-5_5"', '"In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper we characterize the energy functions that can be minimized by graph cuts. Our results are restricted to energy functions with binary variables. However, our work generalizes many previous constructions, and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration and scene reconstruction. We present three main results: a necessary condition for any energy function that can be minimized by graph cuts; a sufficient condition for energy functions that can be written as a sum of functions of up to three variables at a time; and a general-purpose construction to minimize such an energy function. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible, and then follow our construction to create the appropriate graph."'),
('"What Is a Good Image Segment? A Unified Approach to Segment Extraction"', '"ECCV 2008"', '["Image Segmentation", "Segmentation Algorithm", "Query Image", "Image Segment", "Segment Boundary"]', '"https://doi.org/10.1007/978-3-540-88693-8_3"', '"There is a huge diversity of definitions of \\u201cvisually meaningful\\u201d image segments, ranging from simple uniformly colored segments, textured segments, through symmetric patterns, and up to complex semantically meaningful objects. This diversity has led to a wide range of different approaches for image segmentation. In this paper we present a single unified framework for addressing this problem \\u2013 \\u201cSegmentation by Composition\\u201d. We define a good image segment as one which can be easily composed using its own pieces, but is difficult to compose using pieces from other parts of the image. This non-parametric approach captures a large diversity of segment types, yet requires no pre-definition or modelling of segment types, nor prior training. Based on this definition, we develop a segment extraction algorithm \\u2013 i.e., given a single point-of-interest, provide the \\u201cbest\\u201d image segment containing that point. This induces a figure-ground image segmentation, which applies to a range of different segmentation tasks: single image segmentation, simultaneous co-segmentation of several images, and class-based segmentations."'),
('"What Is a Good Nearest Neighbors Algorithm for Finding Similar Patches in Images?"', '"ECCV 2008"', '[]', '"https://doi.org/10.1007/978-3-540-88688-4_27"', '"Many computer vision algorithms require searching a set of images for similar patches, which is a very expensive operation. In this work, we compare and evaluate a number of nearest neighbors algorithms for speeding up this task. Since image patches follow very different distributions from the uniform and Gaussian distributions that are typically used to evaluate nearest neighbors methods, we determine the method with the best performance via extensive experimentation on real images. Furthermore, we take advantage of the inherent structure and properties of images to achieve highly efficient implementations of these algorithms. Our results indicate that vantage point trees, which are not well known in the vision community, generally offer the best performance."'),
('"What Is the Chance of Happening: A New Way to Predict Where People Look"', '"ECCV 2010"', '["Visual Attention", "Human Visual System", "Natural Image", "Bayesian Framework", "Independent Comp', '"https://doi.org/10.1007/978-3-642-15555-0_46"', '"Visual attention is an important issue in image and video analysis and keeps being an open problem in the computer vision field. Motivated by the famous Helmholtz principle, a new approach of visual attention analysis is proposed in this paper based on the low level feature statistics of natural images and the Bayesian framework. Firstly, two priors, i.e., Surrounding Feature Prior (SFP) and Single Feature Probability Distribution (SFPD) are learned and integrated by a Bayesian framework to compute the chance of happening (CoH) of each pixel in an image. Then another prior, i.e., Center Bias Prior (CBP), is learned and applied to the CoH to compute the saliency map of the image. The experimental results demonstrate that the proposed approach is both effective and efficient by providing more accurate and quick visual attention location. We make three major contributions in this paper: (1) A set of simple but powerful priors, SFP, SFPD and CBP, are presented in an intuitive way; (2) A computational model of CoH based on Bayesian framework is given to integrate SFP and SFPD together; (3) A computationally plausible way to obtain the saliency map of natural images based on CoH and CBP."'),
('"What Is the Range of Surface Reconstructions from a Gradient Field?"', '"ECCV 2006"', '["Span Tree", "Surface Reconstruction", "Laplacian Matrix", "Photometric Stereo", "Correction Gradie', '"https://doi.org/10.1007/11744023_45"', '"We propose a generalized equation to represent a continuum of surface reconstruction solutions of a given non-integrable gradient field. We show that common approaches such as Poisson solver and Frankot-Chellappa algorithm are special cases of this generalized equation. For a N \\u00d7 N pixel grid, the subspace of all integrable gradient fields is of dimension N 2 \\u2013 1. Our framework can be applied to derive a range of meaningful surface reconstructions from this high dimensional space. The key observation is that the range of solutions is related to the degree of anisotropy in applying weights to the gradients in the integration process. While common approaches use isotropic weights, we show that by using a progression of spatially varying anisotropic weights, we can achieve significant improvement in reconstructions. We propose (a) \\u03b1-surfaces using binary weights, where the parameter \\u03b1 allows trade off between smoothness and robustness, (b) M-estimators and edge preserving regularization using continuous weights and (c) Diffusion using affine transformation of gradients. We provide results on photometric stereo, compare with previous approaches and show that anisotropic treatment discounts noise while recovering salient features in reconstructions."'),
('"What Is the Role of Independence for Visual Recognition?"', '"ECCV 2002"', '["Feature Space", "Discrete Cosine Transform", "Independent Component Analysis", "Recognition Accura', '"https://doi.org/10.1007/3-540-47969-4_20"', '"Independent representations have recently attracted significant attention from the biological vision and cognitive science communities. It has been 1) argued that properties such as sparseness and independence play a major role in visual perception, and 2) shown that imposing such properties on visual representations originates receptive fields similar to those found in human vision. We present a study of the impact of feature independence in the performance of visual recognition architectures. The contributions of this study are of both theoretical and empirical natures, and support two main conclusions. The first is that the intrinsic complexity of the recognition problem (Bayes error) is higher for independent representations. The increase can be significant, close to 10% in the databases we considered. The second is that criteria commonly used in independent component analysis are not sufficient to eliminate all the dependencies that impact recognition. In fact, \\u201cindependent components\\u201d can be less independent than previous representations, such as principal components or wavelet bases."'),
('"What Makes a Good Detector? \\u2013 Structured Priors for Learning from Few Examples"', '"ECCV 2012"', '["Training Image", "Semantic Relatedness", "Object Class", "Target Class", "Transfer Learning"]', '"https://doi.org/10.1007/978-3-642-33715-4_26"', '"Transfer learning can counter the heavy-tailed nature of the distribution of training examples over object classes. Here, we study transfer learning for object class detection. Starting from the intuition that \\u201cwhat makes a good detector\\u201d should manifest itself in the form of repeatable statistics over existing \\u201cgood\\u201d detectors, we design a low-level feature model that can be used as a prior for learning new object class models from scarce training data. Our priors are structured, capturing dependencies both on the level of individual features and spatially neighboring pairs of features. We confirm experimentally the connection between the information captured by our priors and \\u201cgood\\u201d detectors as well as the connection to transfer learning from sources of different quality. We give an in-depth analysis of our priors on a subset of the challenging PASCAL VOC 2007 data set and demonstrate improved average performance over all 20 classes, achieved without manual intervention."'),
('"What, Where and How Many? Combining Object Detectors and CRFs"', '"ECCV 2010"', '["Object Detection", "Conditional Random Field", "False Positive Detection", "Conditional Random Fie', '"https://doi.org/10.1007/978-3-642-15561-1_31"', '"Computer vision algorithms for individual tasks such as object recognition, detection and segmentation have shown impressive results in the recent past. The next challenge is to integrate all these algorithms and address the problem of scene understanding. This paper is a step towards this goal. We present a probabilistic framework for reasoning about regions, objects, and their attributes such as object class, location, and spatial extent. Our model is a Conditional Random Field defined on pixels, segments and objects. We define a global energy function for the model, which combines results from sliding window detectors, and low-level pixel-based unary and pairwise relations. One of our primary contributions is to show that this energy function can be solved efficiently. Experimental results show that our model achieves significant improvement over the baseline methods on CamVid and pascal voc datasets."'),
('"Which Looks Like Which: Exploring Inter-class Relationships in Fine-Grained Visual Categorization"', '"ECCV 2014"', '["Fine-grained visual categorization", "inter-class relationship", "multiple task learning"]', '"https://doi.org/10.1007/978-3-319-10578-9_28"', '"Fine-grained visual categorization aims at classifying visual data at a subordinate level, e.g., identifying different species of birds. It is a highly challenging topic receiving significant research attention recently. Most existing works focused on the design of more discriminative feature representations to capture the subtle visual differences among categories. Very limited efforts were spent on the design of robust model learning algorithms. In this paper, we treat the training of each category classifier as a single learning task, and formulate a generic multiple task learning (MTL) framework to train multiple classifiers simultaneously. Different from the existing MTL methods, the proposed generic MTL algorithm enforces no structure assumptions and thus is more flexible in handling complex inter-class relationships. In particular, it is able to automatically discover both clusters of similar categories and outliers. We show that the objective of our generic MTL formulation can be solved using an iterative reweighted \\u21132 method. Through an extensive experimental validation, we demonstrate that our method outperforms several state-of-the-art approaches."'),
('"Whitening for Photometric Comparison of Smooth Surfaces under Varying Illumination"', '"ECCV 2004"', '["Reference Image", "Query Image", "Synthetic Image", "Lighting Direction", "Lighting Change"]', '"https://doi.org/10.1007/978-3-540-24673-2_18"', '"We consider the problem of image comparison in order to match smooth surfaces under varying illumination. In a smooth surface nearby surface normals are highly correlated. We model such surfaces as Gaussian processes and derive the resulting statistical characterization of the corresponding images. Supported by this model, we treat the difference between two images, associated with the same surface and different lighting, as colored Gaussian noise, and use the whitening tool from signal detection theory to construct a measure of difference between such images. This also improves comparisons by accentuating the differences between images of different surfaces. At the same time, we prove that no linear filter, including ours, can produce lighting insensitive image comparisons. While our Gaussian assumption is a simplification, the resulting measure functions well for both synthetic and real smooth objects. Thus we improve upon methods for matching images of smooth objects, while providing insight into the performance of such methods. Much prior work has focused on image comparison methods appropriate for highly curved surfaces. We combine our method with one of these, and demonstrate high performance on rough and smooth objects."'),
('"Why Did the Person Cross the Road (There)? Scene Understanding Using Probabilistic Logic Models and', '"ECCV 2010"', '["Logic Rule", "Proximal Zone", "Horizon Line", "Markov Logic Network", "Common Sense Knowledge"]', '"https://doi.org/10.1007/978-3-642-15552-9_50"', '"We develop a video understanding system for scene elements, such as bus stops, crosswalks, and intersections, that are characterized more by qualitative activities and geometry than by intrinsic appearance. The domain models for scene elements are not learned from a corpus of video, but instead, naturally elicited by humans, and represented as probabilistic logic rules within a Markov Logic Network framework. Human elicited models, however, represent object interactions as they occur in the 3D world rather than describing their appearance projection in some specific 2D image plane. We bridge this gap by recovering qualitative scene geometry to analyze object interactions in the 3D world and then reasoning about scene geometry, occlusions and common sense domain knowledge using a set of meta-rules. The effectiveness of this approach is demonstrated on a set of videos of public spaces."'),
('"Wide Baseline Point Matching Using Affine Invariants Computed from Intensity Profiles"', '"ECCV 2000"', '["Feature Vector", "Interest Point", "Query Image", "Visual Servoing", "Point Correspondence"]', '"https://doi.org/10.1007/3-540-45054-8_53"', '"The problem of establishing correspondences between images taken from different viewpoints is fundamental in computer vision. We propose an algorithm which is capable of handling larger changes in viewpoint than classical correlation based techniques. Optimal performance for the algorithm is achieved for textured objects which are locally planar in at least one direction. The algorithm works by computing affinely invariant fourier features from intensity profiles in each image. The intensity profiles are extracted from the image data between randomly selected pairs of image interest points. Using a voting scheme, pairs of interest points are matched across images by comparing vectors of fourier features. Outliers among the matches are rejected in two stages, a fast stage using novel view consistency constraints, and a second, slower stage using RANSAC and fundamental matrix computation. In order to demonstrate the quality of the results, the algorithm is tested on several different image pairs."'),
('"Window Annealing over Square Lattice Markov Random Field"', '"ECCV 2008"', '["Simulated Annealing", "Markov Random Field", "Global Constraint", "Sequential Monte Carlo", "Detai', '"https://doi.org/10.1007/978-3-540-88688-4_23"', '"Monte Carlo methods and their subsequent simulated annealing are able to minimize general energy functions. However, the slow convergence of simulated annealing compared with more recent deterministic algorithms such as graph cuts and belief propagation hinders its popularity over the large dimensional Markov Random Field (MRF). In this paper, we propose a new efficient sampling-based optimization algorithm called WA (Window Annealing) over squared lattice MRF, in which cluster sampling and annealing concepts are combined together. Unlike the conventional annealing process in which only the temperature variable is scheduled, we design a series of artificial \\u201dguiding\\u201d (auxiliary) probability distributions based on the general sequential Monte Carlo framework. These auxiliary distributions lead to the maximum a posteriori (MAP) state by scheduling both the temperature and the proposed maximum size of the windows (rectangular cluster) variable. This new annealing scheme greatly enhances the mixing rate and consequently reduces convergence time. Moreover, by adopting the integral image technique for computation of the proposal probability of a sampled window, we can achieve a dramatic reduction in overall computations. The proposed WA is compared with several existing Monte Carlo based optimization techniques as well as state-of-the-art deterministic methods including Graph Cut (GC) and sequential tree re-weighted belief propagation (TRW-S) in the pairwise MRF stereo problem. The experimental results demonstrate that the proposed WA method is comparable with GC in both speed and obtained energy level."'),
('"Word Spotting in the Wild"', '"ECCV 2010"', '["Word Recognition", "Image Text", "Optical Character Recognition", "Multiple Kernel Learn", "Street', '"https://doi.org/10.1007/978-3-642-15549-9_43"', '"We present a method for spotting words in the wild, i.e., in real images taken in unconstrained environments. Text found in the wild has a surprising range of difficulty. At one end of the spectrum, Optical Character Recognition (OCR) applied to scanned pages of well formatted printed text is one of the most successful applications of computer vision to date. At the other extreme lie visual CAPTCHAs \\u2013 text that is constructed explicitly to fool computer vision algorithms. Both tasks involve recognizing text, yet one is nearly solved while the other remains extremely challenging. In this work, we argue that the appearance of words in the wild spans this range of difficulties and propose a new word recognition approach based on state-of-the-art methods from generic object recognition, in which we consider object categories to be the words themselves. We compare performance of leading OCR engines \\u2013 one open source and one proprietary \\u2013 with our new approach on the ICDAR Robust Reading data set and a new word spotting data set we introduce in this paper: the Street View Text data set. We show improvements of up to 16% on the data sets, demonstrating the feasibility of a new approach to a seemingly old problem."'),
('"Worldwide Pose Estimation Using 3D Point Clouds"', '"ECCV 2012"', '["Point Cloud", "Database Image", "Query Image", "Average Descriptor", "Sift Descriptor"]', '"https://doi.org/10.1007/978-3-642-33718-5_2"', '"We address the problem of determining where a photo was taken by estimating a full 6-DOF-plus-intrincs camera pose with respect to a large geo-registered 3D point cloud, bringing together research on image localization, landmark recognition, and 3D pose estimation. Our method scales to datasets with hundreds of thousands of images and tens of millions of 3D points through the use of two new techniques: a co-occurrence prior for RANSAC and bidirectional matching of image features with 3D points. We evaluate our method on several large data sets, and show state-of-the-art results on landmark recognition as well as the ability to locate cameras to within meters, requiring only seconds per query."'),
('"Writer Identification Using Finger-Bend in Writing Signature"', '"BioAW 2004"', '["Reference Pattern", "Bend Angle", "Signature Verification", "Data Glove", "Online Signature"]', '"https://doi.org/10.1007/978-3-540-25976-3_21"', '"In sign authentication, some feature parameters such as pen position, pen pressure and pen inclination are combined to get better performance. Considering that these features are results of individual contraction of muscles with different shape of skeletons, it is meaningful to measure time sequences of finger-bends for authenticating individuals. In this paper, we show the individuality in the bend of joints of fingers in signing, and we discuss the possibility of writer authentication."'),
('"W\\u03b1SH: Weighted \\u03b1-Shapes for Local Feature Detection"', '"ECCV 2012"', '["Scale Invariant Feature Transform", "Query Time", "Weighted Point", "Regular Triangulation", "Maxi', '"https://doi.org/10.1007/978-3-642-33709-3_56"', '"Depending on the application, local feature detectors should comply with properties that are often contradictory, e.g. distinctiveness vs. robustness. Providing a good balance is a standing problem in the field. In this direction, we propose a novel approach for local feature detection starting from sampled edges. The detector is based on shape stability measures across the weighted \\u03b1-filtration, a computational geometry construction that captures the shape of a non-uniform set of points. The extracted features are blob-like and include non-extremal regions as well as regions determined by cavities of boundary shape. Overall, the approach provides distinctive regions, while achieving high robustness in terms of repeatability and matching score, as well as competitive performance in a large scale image retrieval application."'),
('"Yet Another Survey on Image Segmentation: Region and Boundary Information Integration"', '"ECCV 2002"', '["grouping and segmentation", "region based segmentation", "boundary based segmentation", "cooperati', '"https://doi.org/10.1007/3-540-47977-5_27"', '"Image segmentation has been, and still is, a relevant research area in Computer Vision, and hundreds of segmentation algorithms have been proposed in the last 30 years. However, it is well known that elemental segmentation techniques based on boundary or region information often fail to produce accurate segmentation results. Hence, in the last few years, there has been a tendency towards algorithms which take advantage of the complementary nature of such information. This paper reviews different segmentation proposals which integrate edge and region information and highlights 7 different strategies and methods to fuse such information. In contrast with other surveys which only describe and compare qualitatively different approaches, this survey deals with a real quantitative comparison. In this sense, key methods have been programmed and their accuracy analyzed and compared using synthetic and real images. A discussion justified with experimental results is given and the code is available on Internet."'),
('"Zero-Shot Learning via Visual Abstraction"', '"ECCV 2014"', '["zero-shot learning", "visual abstraction", "synthetic data", "pose"]', '"https://doi.org/10.1007/978-3-319-10593-2_27"', '"One of the main challenges in learning fine-grained visual categories is gathering training images. Recent work in Zero-Shot Learning (ZSL) circumvents this challenge by describing categories via attributes or text. However, not all visual concepts, e.g., two people dancing, are easily amenable to such descriptions. In this paper, we propose a new modality for ZSL using visual abstraction to learn difficult-to-describe concepts. Specifically, we explore concepts related to people and their interactions with others. Our proposed modality allows one to provide training data by manipulating abstract visualizations, e.g., one can illustrate interactions between two clipart people by manipulating each person\\u2019s pose, expression, gaze, and gender. The feasibility of our approach is shown on a human pose dataset and a new dataset containing complex interactions between two people, where we outperform several baselines. To better match across the two domains, we learn an explicit mapping between the abstract and real worlds."');
INSERT INTO `complexpapers` (`name`, `year`, `keyword`, `link`, `digest`) VALUES
('"\\u2018Dynamism of a Dog on a Leash\\u2019 or Behavior Classification by Eigen-Decomposition of Perio', '"ECCV 2002"', '["Feature Point", "Periodic Motion", "Parameterized Representation", "Implicit Representation", "Mot', '"https://doi.org/10.1007/3-540-47969-4_31"', '"Following Futurism, we show how periodic motions can be represented by a small number of eigen-shapes that capture the whole dynamic mechanism of periodic motions. Spectral decomposition of a silhouette of an object in motion serves as a basis for behavior classification by principle component analysis. The boundary contour of the walking dog, for example, is first computed efficiently and accurately. After normalization, the implicit representation of a sequence of silhouette contours given by their corresponding binary images, is used for generating eigen-shapes for the given motion. Singular value decomposition produces these eigen-shapes that are then used to analyze the sequence. We show examples of object as well as behavior classification based on the eigen-decomposition of the binary silhouette sequence."'),
('"\\u201cClustering by Composition\\u201d \\u2013 Unsupervised Discovery of Image Categories"', '"ECCV 2012"', '["Image Category", "Query Image", "Multiple Image", "Good Region", "Image Collection"]', '"https://doi.org/10.1007/978-3-642-33786-4_35"', '"We define a \\u201cgood image cluster\\u201d as one in which images can be easily composed (like a puzzle) using pieces from each other, while are difficult to compose from images outside the cluster. The larger and more statistically significant the pieces are, the stronger the affinity between the images. This gives rise to unsupervised discovery of very challenging image categories. We further show how multiple images can be composed from each other simultaneously and efficiently using a collaborative randomized search algorithm. This collaborative process exploits the \\u201cwisdom of crowds of images\\u201d, to obtain a sparse yet meaningful set of image affinities, and in time which is almost linear in the size of the image collection. \\u201cClustering-by-Composition\\u201d can be applied to very few images (where a \\u2018cluster model\\u2019 cannot be \\u2018learned\\u2019), as well as on benchmark evaluation datasets, and yields state-of-the-art results."'),
('"\\\\(\\\\mathcal {ALC({\\\\mathbf {F}}})\\\\): A New Description Logic for Spatial Reasoning in Images"', '"ECCV 2014"', '["Spatial reasoning", "Ontology-based image understanding", "Description logics"]', '"https://doi.org/10.1007/978-3-319-16181-5_26"', '"In image interpretation and computer vision, spatial relations between objects and spatial reasoning are of prime importance for recognition and interpretation tasks. Quantitative representations of spatial knowledge have been proposed in the literature. In the Artificial Intelligence community, logical formalisms such as ontologies have also been proposed for spatial knowledge representation and reasoning, and a challenging and open problem consists in bridging the gap between these ontological representations and the quantitative ones used in image interpretation. In this paper, we propose a new description logic, named \\\\(\\\\mathcal {ALC({\\\\mathbf {F}})}\\\\), dedicated to spatial reasoning for image understanding. Our logic relies on the family of description logics equipped with concrete domains, a widely accepted way to integrate quantitative and qualitative qualities of real world objects in the conceptual domain, in which we have integrated mathematical morphological operators as predicates. Merging description logics with mathematical morphology enables us to provide new mechanisms to derive useful concrete representations of spatial concepts and new qualitative and quantitative spatial reasoning tools. It also enables imprecision and uncertainty of spatial knowledge to be taken into account through the fuzzy representation of spatial relations. We illustrate the benefits of our formalism on a model-guided cerebral image interpretation task."');

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
