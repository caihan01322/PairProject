/*
 Navicat Premium Data Transfer

 Source Server         : local
 Source Server Type    : MySQL
 Source Server Version : 50729
 Source Host           : localhost:3306
 Source Schema         : paperdb

 Target Server Type    : MySQL
 Target Server Version : 50729
 File Encoding         : 65001

 Date: 25/03/2021 19:35:55
*/

SET NAMES utf8mb4;
SET FOREIGN_KEY_CHECKS = 0;

-- ----------------------------
-- Table structure for collection
-- ----------------------------
DROP TABLE IF EXISTS `collection`;
CREATE TABLE `collection`  (
  `userid` int(6) NOT NULL,
  `folder` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`userid`, `folder`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of collection
-- ----------------------------
INSERT INTO `collection` VALUES (1, '默认收藏夹');
INSERT INTO `collection` VALUES (2, '默认收藏夹');

-- ----------------------------
-- Table structure for paper
-- ----------------------------
DROP TABLE IF EXISTS `paper`;
CREATE TABLE `paper`  (
  `pid` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `key1` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `key2` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `key3` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `key4` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `key5` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `summary` varchar(10000) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `meeting` varchar(8) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `year` varchar(4) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `ptime` varchar(50) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `link` varchar(500) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`pid`) USING BTREE
) ENGINE = InnoDB AUTO_INCREMENT = 13032 CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of paper
-- ----------------------------
INSERT INTO `paper` VALUES (10001, 'A Discriminative Model with Multiple Temporal Scales for Action Prediction', 'Action Prediction', 'Structured SVM', 'Sequential Data', '', '', 'The speed with which intelligent systems can react to an action depends on how soon it can be recognized. The ability to recognize ongoing actions is critical in many applications, for example, spotting criminal activity. It is challenging, since decisions have to be made based on partial videos of temporally incomplete action executions. In this paper, we propose a novel discriminative multi-scale model for predicting the action class from a partially observed video. The proposed model captures temporal dynamics of human actions by explicitly considering all the history of observed features as well as features in smaller temporal segments. We develop a new learning formulation, which elegantly captures the temporal evolution over time, and enforces the label consistency between segments and corresponding partial videos. Experimental results on two public datasets show that the proposed approach outperforms state-of-the-art action prediction methods.', 'ECCV', '2014', '17 September 2016', 'https://doi.org/10.1007/978-3-319-10602-1_39');
INSERT INTO `paper` VALUES (10002, '3D Image Reconstruction from X-Ray Measurements with Overlap', 'Image reconstruction', 'Medical imaging', 'X-ray', 'Overlap', '', '3D image reconstruction from a set of X-ray projections is an important image reconstruction problem, with applications in medical imaging, industrial inspection and airport security. The innovation of X-ray emitter arrays allows for a novel type of X-ray scanners with multiple simultaneously emitting sources. However, two or more sources emitting at the same time can yield measurements from overlapping rays, imposing a new type of image reconstruction problem based on nonlinear constraints. Using traditional linear reconstruction methods, respective scanner geometries have to be implemented such that no rays overlap, which severely restricts the scanner design. We derive a new type of 3D image reconstruction model with nonlinear constraints, based on measurements with overlapping X-rays. Further, we show that the arising optimization problem is partially convex, and present an algorithm to solve it. Experiments show highly improved image reconstruction results from both simulated and real-world measurements.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_2');
INSERT INTO `paper` VALUES (10003, '4D Match Trees for Non-rigid Surface Alignment', 'Non-sequential tracking', 'Surface alignment', 'Temporal coherence', 'Dynamic scene reconstruction', '4D modeling', 'This paper presents a method for dense 4D temporal alignment of partial reconstructions of non-rigid surfaces observed from single or multiple moving cameras of complex scenes. 4D Match Trees are introduced for robust global alignment of non-rigid shape based on the similarity between images across sequences and views. Wide-timeframe sparse correspondence between arbitrary pairs of images is established using a segmentation-based feature detector (SFD) which is demonstrated to give improved matching of non-rigid shape. Sparse SFD correspondence allows the similarity between any pair of image frames to be estimated for moving cameras and multiple views. This enables the 4D Match Tree to be constructed which minimises the observed change in non-rigid shape for global alignment across all images. Dense 4D temporal correspondence across all frames is then estimated by traversing the 4D Match tree using optical flow initialised from the sparse feature matches. The approach is evaluated on single and multiple view images sequences for alignment of partial surface reconstructions of dynamic objects in complex indoor and outdoor scenes to obtain a temporally consistent 4D representation. Comparison to previous 2D and 3D scene flow demonstrates that 4D Match Trees achieve reduced errors due to drift and improved robustness to large non-rigid deformations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_13');
INSERT INTO `paper` VALUES (10004, 'A 3D Morphable Eye Region Model for Gaze Estimation', 'Morphable model', 'Gaze estimation', 'Analysis-by-synthesis', '', '', 'Morphable face models are a powerful tool, but have previously failed to model the eye accurately due to complexities in its material and motion. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model. It is the first morphable model that accurately captures eye region shape, since it was built from high-quality head scans. It is also the first to allow independent eyeball movement, since we treat it as a separate part. To showcase our model we present a new method for illumination- and head-pose–invariant gaze estimation from a single RGB image. We fit our model to an image through analysis-by-synthesis, solving for eye region shape, texture, eyeball pose, and illumination simultaneously. The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images, and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of \\(9.44^\\circ \\) in a challenging user-independent scenario.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_18');
INSERT INTO `paper` VALUES (10005, 'A 4D Light-Field Dataset and CNN Architectures for Material Recognition', 'Light-field', 'Material recognition', 'Convolutional neural network', '', '', 'We introduce a new light-field dataset of materials, and take advantage of the recent success of deep learning to perform material recognition on the 4D light-field. Our dataset contains 12 material categories, each with 100 images taken with a Lytro Illum, from which we extract about 30,000 patches in total. To the best of our knowledge, this is the first mid-size dataset for light-field images. Our main goal is to investigate whether the additional information in a light-field (such as multiple sub-aperture views and view-dependent reflectance effects) can aid material recognition. Since recognition networks have not been trained on 4D images before, we propose and compare several novel CNN architectures to train on light-field images. In our experiments, the best performing CNN architecture achieves a 7 % boost compared with 2D image classification (\\(70\\,\\%\\rightarrow 77\\,\\%\\)). These results constitute important baselines that can spur further research in the use of CNNs for light-field applications. Upon publication, our dataset also enables other novel applications of light-fields, including object detection, image segmentation and view interpolation.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_8');
INSERT INTO `paper` VALUES (10006, 'A Benchmark and Simulator for UAV Tracking', 'UAV tracking', 'UAV simulator', 'Aerial object tracking', '', '', 'In this paper, we propose a new aerial video dataset and benchmark for low altitude UAV target tracking, as well as, a photo-realistic UAV simulator that can be coupled with tracking methods. Our benchmark provides the first evaluation of many state-of-the-art and popular trackers on 123 new and fully annotated HD video sequences captured from a low-altitude aerial perspective. Among the compared trackers, we determine which ones are the most suitable for UAV tracking both in terms of tracking accuracy and run-time. The simulator can be used to evaluate tracking algorithms in real-time scenarios before they are deployed on a UAV “in the field”, as well as, generate synthetic but photo-realistic tracking datasets with automatic ground truth annotations to easily extend existing real-world datasets. Both the benchmark and simulator are made publicly available to the vision community on our website to further research in the area of object tracking from UAVs. (https://ivul.kaust.edu.sa/Pages/pub-benchmark-simulator-uav.aspx.).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_27');
INSERT INTO `paper` VALUES (10007, 'A Benchmark for Automatic Visual Classification of Clinical Skin Disease Images', 'Skin disease image', 'Computer aided diagnosis', 'Image classification', 'CNNs', 'Hand-crafted features', 'Skin disease is one of the most common human illnesses. It pervades all cultures, occurs at all ages, and affects between 30 % and 70 % of individuals, with even higher rates in at-risk. However, diagnosis of skin diseases by observing is a very difficult job for both doctors and patients, where an intelligent system can be helpful. In this paper, we mainly introduce a benchmark dataset for clinical skin diseases to address this problem. To the best of our knowledge, this dataset is currently the largest for visual recognition of skin diseases. It contains 6,584 images from 198 classes, varying according to scale, color, shape and structure. We hope that this benchmark dataset will encourage further research on visual skin disease classification. Moreover, the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks(CNNs), we also perform extensive analyses on this dataset using the state of the art methods including CNNs.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_13');
INSERT INTO `paper` VALUES (10008, 'A Cluster Sampling Method for Image Matting via Sparse Coding', 'Image matting', 'Sampling', 'Clustering', 'Sparse coding', 'Foreground extraction', 'In this paper, we present a new image matting algorithm which solves two major problems encountered by previous sampling-based algorithms. The first is that existing sampling-based approaches typically rely on certain spatial assumptions in collecting samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. Here, we propose a method that a more representative set of samples is collected so as not to miss out true samples. This is accomplished by clustering the foreground and background pixels and collecting samples from each of the clusters. The second problem is that the quality of matting result is determined by the goodness of a single sample pair which causes errors when sampling-based methods fail to select the best pairs. In this paper, we derive a new objective function for directly obtaining the estimation of the alpha matte from a bunch of samples. Comparison on a standard benchmark dataset demonstrates that the proposed approach generates more robust and accurate alpha matte than state-of-the-art methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_13');
INSERT INTO `paper` VALUES (10009, 'A Convex Solution to Spatially-Regularized Correspondence Problems', 'Correspondence Problem', 'Geometric Measure Theory', 'Spatial Regularity', 'Efficient Primal-dual Algorithm', 'Minimal Surface Problem', 'We propose a convex formulation of the correspondence problem between two images with respect to an energy function measuring data consistency and spatial regularity. To this end, we formulate the general correspondence problem as the search for a minimal two-dimensional surface in \\(\\mathbb {R}^4\\). We then use tools from geometric measure theory and introduce 2-vector fields as a representation of two-dimensional surfaces in \\(\\mathbb {R}^4\\). We propose a discretization of this surface formulation that gives rise to a convex minimization problem and compute a globally optimal solution using an efficient primal-dual algorithm.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_52');
INSERT INTO `paper` VALUES (10010, 'A Deep Learning-Based Approach to Progressive Vehicle Re-identification for Urban Surveillance', 'Vehicle re-identification', 'Progressive search', 'Deep learning', 'License plate verification', 'Spatiotemporal relation', 'While re-identification (Re-Id) of persons has attracted intensive attention, vehicle, which is a significant object class in urban video surveillance, is often overlooked by vision community. Most existing methods for vehicle Re-Id only achieve limited performance, as they predominantly focus on the generic appearance of vehicle while neglecting some unique identities of vehicle (e.g., license plate). In this paper, we propose a novel deep learning-based approach to PROgressive Vehicle re-ID, called “PROVID”. Our approach treats vehicle Re-Id as two specific progressive search processes: coarse-to-fine search in the feature space, and near-to-distant search in the real world surveillance environment. The first search process employs the appearance attributes of vehicle for a coarse filtering, and then exploits the Siamese Neural Network for license plate verification to accurately identify vehicles. The near-to-distant search process retrieves vehicles in a manner like human beings, by searching from near to faraway cameras and from close to distant time. Moreover, to facilitate progressive vehicle Re-Id research, we collect to-date the largest dataset named VeRi-776 from large-scale urban surveillance videos, which contains not only massive vehicles with diverse attributes and high recurrence rate, but also sufficient license plates and spatiotemporal labels. A comprehensive evaluation on the VeRi-776 shows that our approach outperforms the state-of-the-art methods by 9.28 % improvements in term of mAP.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_53');
INSERT INTO `paper` VALUES (10011, 'A Diagram is Worth a Dozen Images', 'Random Forest', 'Natural Image', 'Convolutional Neural Network', 'Question Answering', 'Random Forest Model', 'Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for about 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_15');
INSERT INTO `paper` VALUES (10012, 'A Discriminative Framework for Anomaly Detection in Large Videos', 'Anomaly detection', 'Discriminative', 'Unsupervised', 'Context', 'Surveillance', 'We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering. Current algorithms in anomaly detection are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer, more complex videos. By defining anomalies as examples that can be distinguished from other examples in the same video, our definition inspires a shift in approaches from classical density estimation to simple discriminative learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and (2) unsupervised, requiring no separate training sequences. We show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_21');
INSERT INTO `paper` VALUES (10013, 'A Distance for HMMs Based on Aggregated Wasserstein Metric and State Registration', 'Hidden Markov Model', 'Gaussian Mixture Model', 'Wasserstein distance', '', '', 'We propose a framework, named Aggregated Wasserstein, for computing a dissimilarity measure or distance between two Hidden Markov Models with state conditional distributions being Gaussian. For such HMMs, the marginal distribution at any time spot follows a Gaussian mixture distribution, a fact exploited to softly match, aka register, the states in two HMMs. We refer to such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of states is inspired by the intrinsic relationship of optimal transport and the Wasserstein metric between distributions. Specifically, the components of the marginal GMMs are matched by solving an optimal transport problem where the cost between components is the Wasserstein metric for Gaussian distributions. The solution of the optimization problem is a fast approximation to the Wasserstein metric between two GMMs. The new Aggregated Wasserstein distance is a semi-metric and can be computed without generating Monte Carlo samples. It is invariant to relabeling or permutation of the states. This distance quantifies the dissimilarity of GMM-HMMs by measuring both the difference between the two marginal GMMs and the difference between the two transition matrices. Our new distance is tested on the tasks of retrieval and classification of time series. Experiments on both synthetic data and real data have demonstrated its advantages in terms of accuracy as well as efficiency in comparison with existing distances based on the Kullback-Leibler divergence.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_27');
INSERT INTO `paper` VALUES (10014, 'A Geometric Approach to Image Labeling', 'Image labeling', 'Assignment manifold', 'Fisher-Rao metric', 'Riemannian gradient flow', '', 'We introduce a smooth non-convex approach in a novel geometric framework which complements established convex and non-convex approaches to image labeling. The major underlying concept is a smooth manifold of probabilistic assignments of a prespecified set of prior data (the “labels”) to given image data. The Riemannian gradient flow with respect to a corresponding objective function evolves on the manifold and terminates, for any \\(\\delta > 0\\), within a \\(\\delta \\)-neighborhood of an unique assignment (labeling). As a consequence, unlike with convex outer relaxation approaches to (non-submodular) image labeling problems, no post-processing step is needed for the rounding of fractional solutions. Our approach is numerically implemented with sparse, highly-parallel interior-point updates that efficiently converge, largely independent from the number of labels. Experiments with noisy labeling and inpainting problems demonstrate competitive performance.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_9');
INSERT INTO `paper` VALUES (10015, 'A Large Contextual Dataset for Classification, Detection and Counting of Cars with Deep Learning', 'Deep', 'Learning', 'CNN', 'COWC', 'Context', 'We have created a large diverse set of cars from overhead images (Data sets, annotations, networks and scripts are available from http://gdo-datasci.ucllnl.org/cowc/), which are useful for training a deep learner to binary classify, detect and count them. The dataset and all related material will be made publically available. The set contains contextual matter to aid in identification of difficult targets. We demonstrate classification and detection on this dataset using a neural network we call ResCeption. This network combines residual learning with Inception-style layers and is used to count cars in one look. This is a new way to count objects rather than by localization or density estimation. It is fairly accurate, fast and easy to implement. Additionally, the counting method is not car or scene specific. It would be easy to train this method to count other kinds of objects and counting over new scenes requires no extra set up or assumptions about object locations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_48');
INSERT INTO `paper` VALUES (10016, 'A Minimal Solution for Non-perspective Pose Estimation from Line Correspondences', 'Pose estimation', 'Plücker lines', 'Non-perspective', 'Gröbner basis', 'Line correspondences', 'In this paper, we study and propose solutions to the relatively un-investigated non-perspective pose estimation problem from line correspondences. Specifically, we represent the 2D and 3D line correspondences as Plücker lines and derive the minimal solution for the minimal problem of three line correspondences with Gröbner basis. Our minimal 3-Line algorithm that gives up to eight solutions is well-suited for robust estimation with RANSAC. We show that our algorithm works as a least-squares that takes in more than three line correspondences without any reformulation. In addition, our algorithm does not require initialization in both the minimal 3-Line and least-squares n-Line cases. Furthermore, our algorithm works without a need for reformulation under the special case of perspective pose estimation when all line correspondences are observed from one single camera. We verify our algorithms with both simulated and real-world data.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_11');
INSERT INTO `paper` VALUES (10017, 'A Multi-scale CNN for Affordance Segmentation in RGB Images', 'Object affordance', 'Mid-level cues', 'Deep learning', '', '', 'Given a single RGB image our goal is to label every pixel with an affordance type. By affordance, we mean an object’s capability to readily support a certain human action, without requiring precursor actions. We focus on segmenting the following five affordance types in indoor scenes: ‘walkable’, ‘sittable’, ‘lyable’, ‘reachable’, and ‘movable’. Our approach uses a deep architecture, consisting of a number of multi-scale convolutional neural networks, for extracting mid-level visual cues and combining them toward affordance segmentation. The mid-level cues include depth map, surface normals, and segmentation of four types of surfaces – namely, floor, structure, furniture and props. For evaluation, we augmented the NYUv2 dataset with new ground-truth annotations of the five affordance types. We are not aware of prior work which starts from pixels, infers mid-level cues, and combines them in a feed-forward fashion for predicting dense affordance maps of a single RGB image.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_12');
INSERT INTO `paper` VALUES (10018, 'A Neural Approach to Blind Motion Deblurring', 'Blind deconvolution', 'Motion deblurring', 'Deep learning', '', '', 'We present a new method for blind motion deblurring that uses a neural network trained to compute estimates of sharp image patches from observations that are blurred by an unknown motion kernel. Instead of regressing directly to patch intensities, this network learns to predict the complex Fourier coefficients of a deconvolution filter to be applied to the input patch for restoration. For inference, we apply the network independently to all overlapping patches in the observed image, and average its outputs to form an initial estimate of the sharp image. We then explicitly estimate a single global blur kernel by relating this estimate to the observed image, and finally perform non-blind deconvolution with this kernel. Our method exhibits accuracy and robustness close to state-of-the-art iterative methods, while being much faster when parallelized on GPU hardware.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_14');
INSERT INTO `paper` VALUES (10019, 'A Novel Tiny Object Recognition Algorithm Based on Unit Statistical Curvature Feature', 'Object recognition', 'Tiny object', 'Feature descriptor', 'Unit Statistical Curvature Feature', '', 'To recognize tiny objects whose sizes are in the range of 15\\(\\times \\)15 to 40\\(\\times \\)40 pixels, a novel image feature descriptor, unit statistical curvature feature (USCF), is proposed based on the statistics of unit curvature distribution. USCF can represent the local general invariant features of the image texture. Due to the curvature features are independent of image sizes, USCF algorithm had high recognition rate for object images in any size including tiny object images. USCF is invariant to rotation and linear illumination variation, and is partially invariant to viewpoint variation. Experimental results showed that the recognition rate of USCF algorithm was the highest for tiny object recognition compared to other nine typical object recognition algorithms under complex test conditions with simultaneous rotation, illumination, viewpoint variation and background interference.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_46');
INSERT INTO `paper` VALUES (10020, 'A Recurrent Encoder-Decoder Network for Sequential Face Alignment', 'Recurrent learning', 'Encoder-decoder', 'Face alignment', '', '', 'We propose a novel recurrent encoder-decoder network model for real-time video-based face alignment. Our proposed model predicts 2D facial point maps regularized by a regression loss, while uniquely exploiting recurrent learning at both spatial and temporal dimensions. At the spatial level, we add a feedback loop connection between the combined output response map and the input, in order to enable iterative coarse-to-fine face alignment using a single network model. At the temporal level, we first decouple the features in the bottleneck of the network into temporal-variant factors, such as pose and expression, and temporal-invariant factors, such as identity information. Temporal recurrent learning is then applied to the decoupled temporal-variant features, yielding better generalization and significantly more accurate results at test time. We perform a comprehensive experimental analysis, showing the importance of each component of our proposed model, as well as superior results over the state-of-the-art in standard datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_3');
INSERT INTO `paper` VALUES (10021, 'A Sequential Approach to 3D Human Pose Estimation: Separation of Localization and Identification of Body Joints', 'Depth camera', 'Human pose', 'Regression forest', '', '', 'In this paper, we propose a new approach to 3D human pose estimation from a single depth image. Conventionally, 3D human pose estimation is formulated as a detection problem of the desired list of body joints. Most of the previous methods attempted to simultaneously localize and identify body joints, with the expectation that the accomplishment of one task would facilitate the accomplishment of the other. However, we believe that identification hampers localization; therefore, the two tasks should be solved separately for enhanced pose estimation performance. We propose a two-stage framework that initially estimates all the locations of joints and subsequently identifies the estimated joints for a specific pose. The locations of joints are estimated by regressing K closest joints from every pixel with the use of a random tree. The identification of joints are realized by transferring labels from a retrieved nearest exemplar model. Once the 3D configuration of all the joints is derived, identification becomes much easier than when it is done simultaneously with localization, exploiting the reduced solution space. Our proposed method achieves significant performance gain on pose estimation accuracy, thereby improving both localization and identification. Experimental results show that the proposed method exhibits an accuracy significantly higher than those of previous approaches that simultaneously localize and identify the body parts.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_45');
INSERT INTO `paper` VALUES (10022, 'A Shape-Based Approach for Salient Object Detection Using Deep Learning', 'Salient object detection', 'Deep learning', 'Convolutional neural networks', '', '', 'Salient object detection is a key step in many image analysis tasks as it not only identifies relevant parts of a visual scene but may also reduce computational complexity by filtering out irrelevant segments of the scene. In this paper, we propose a novel salient object detection method that combines a shape prediction driven by a convolutional neural network with the mid and low-region preserving image information. Our model learns a shape of a salient object using a CNN model for a target region and estimates the full but coarse saliency map of the target image. The map is then refined using image specific low-to-mid level information. Experimental results show that the proposed method outperforms previous state-of-the-arts methods in salient object detection.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_28');
INSERT INTO `paper` VALUES (10023, 'A Simple Hierarchical Pooling Data Structure for Loop Closure', 'Loop closure', 'Hierarchical pooling', 'Bag-of-words', 'Descriptor aggregation', '', 'We propose a data structure obtained by hierarchically pooling Bag-of-Words (BoW) descriptors during a sequence of views that achieves average speedups in large-scale loop closure applications ranging from 2 to 20 times on benchmark datasets. Although simple, the method works as well as sophisticated agglomerative schemes at a fraction of the cost with minimal loss of performance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_20');
INSERT INTO `paper` VALUES (10024, 'A Software Platform for Manipulating the Camera Imaging Pipeline', 'Camera processing pipeline', 'Computational photography', 'Color processing', '', '', 'There are a number of processing steps applied onboard a digital camera that collectively make up the camera imaging pipeline. Unfortunately, the imaging pipeline is typically embedded in a camera’s hardware making it difficult for researchers working on individual components to do so within the proper context of the full pipeline. This not only hinders research, it makes evaluating the effects from modifying an individual pipeline component on the final camera output challenging, if not impossible. This paper presents a new software platform that allows easy access to each stage of the camera imaging pipeline. The platform allows modification of the parameters for individual components as well as the ability to access and manipulate the intermediate images as they pass through different stages. We detail our platform design and demonstrate its usefulness on a number of examples.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_26');
INSERT INTO `paper` VALUES (10025, 'A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection', 'Object detection', 'Multi-scale', 'Unified neural network', '', '', 'A unified deep neural network, denoted the multi-scale CNN (MS-CNN), is proposed for fast multi-scale object detection. The MS-CNN consists of a proposal sub-network and a detection sub-network. In the proposal sub-network, detection is performed at multiple output layers, so that receptive fields match objects of different scales. These complementary scale-specific detectors are combined to produce a strong multi-scale object detector. The unified network is learned end-to-end, by optimizing a multi-task loss. Feature upsampling by deconvolution is also explored, as an alternative to input upsampling, to reduce the memory and computation costs. State-of-the-art object detection performance, at up to 15 fps, is reported on datasets, such as KITTI and Caltech, containing a substantial number of small objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_22');
INSERT INTO `paper` VALUES (10026, 'A Versatile Approach for Solving PnP, PnPf, and PnPfr Problems', 'Absolute camera pose estimation', 'PnP Problem', 'Focal Length', 'Radial distortion', '', 'This paper proposes a versatile approach for solving three kinds of absolute camera pose estimation problem: PnP problem for calibrated cameras, PnPf problem for cameras with unknown focal length, and PnPfr problem for cameras with unknown focal length and unknown radial distortion. This is not only the first least squares solution to PnPfr problem, but also the first approach formulating three problems in the same theoretical manner. We show that all problems have a common subproblem represented as multivariate polynomial equations. Solving these equations by Gröbner basis method, we derive a linear form for the remaining parameters of each problem. Finally, we apply root polishing to strictly satisfy the original KKT condition. The proposed PnP and PnPf solvers have comparable performance to the state-of-the-art methods on synthetic distortion-free data. Moreover, the novel PnPfr solver gives the best result on distorted point data and demonstrates real image rectification against significant distortion.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_21');
INSERT INTO `paper` VALUES (10027, 'Abundant Inverse Regression Using Sufficient Reduction and Its Applications', 'Inverse regression', 'Kernel regression', 'Abundant regression', 'Temperature prediction', 'Alzheimer’s disease', 'Statistical models such as linear regression drive numerous applications in computer vision and machine learning. The landscape of practical deployments of these formulations is dominated by forward regression models that estimate the parameters of a function mapping a set of p covariates, \\(\\varvec{x}\\), to a response variable, y. The less known alternative, Inverse Regression, offers various benefits that are much less explored in vision problems. The goal of this paper is to show how Inverse Regression in the “abundant” feature setting (i.e., many subsets of features are associated with the target label or response, as is the case for images), together with a statistical construction called Sufficient Reduction, yields highly flexible models that are a natural fit for model estimation tasks in vision. Specifically, we obtain formulations that provide relevance of individual covariates used in prediction, at the level of specific examples/samples — in a sense, explaining why a particular prediction was made. With no compromise in performance relative to other methods, an ability to interpret why a learning algorithm is behaving in a specific way for each prediction, adds significant value in numerous applications. We illustrate these properties and the benefits of Abundant Inverse Regression on three distinct applications.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_35');
INSERT INTO `paper` VALUES (10028, 'Accelerating the Super-Resolution Convolutional Neural Network', 'Mapping Layer', 'Deep Model', 'Interpolation Kernel', 'Convolution Filter', 'Convolution Layer', 'As a successful deep model applied in image super-resolution (SR), the Super-Resolution Convolutional Neural Network (SRCNN) [1, 2] has demonstrated superior performance to the previous hand-crafted models either in speed and restoration quality. However, the high computational cost still hinders it from practical usage that demands real-time performance (24 fps). In this paper, we aim at accelerating the current SRCNN, and propose a compact hourglass-shape CNN structure for faster and better SR. We re-design the SRCNN structure mainly in three aspects. First, we introduce a deconvolution layer at the end of the network, then the mapping is learned directly from the original low-resolution image (without interpolation) to the high-resolution one. Second, we reformulate the mapping layer by shrinking the input feature dimension before mapping and expanding back afterwards. Third, we adopt smaller filter sizes but more mapping layers. The proposed model achieves a speed up of more than 40 times with even superior restoration quality. Further, we present the parameter settings that can achieve real-time performance on a generic CPU while still maintaining good performance. A corresponding transfer strategy is also proposed for fast training and testing across different upscaling factors.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_25');
INSERT INTO `paper` VALUES (10029, 'ActionSnapping: Motion-Based Video Synchronization', 'Action Recognition', 'Cost Matrix', 'Input Video', 'Point Trajectory', 'Video Annotation', 'Video synchronization is a fundamental step for many applications in computer vision, ranging from video morphing to motion analysis. We present a novel method for synchronizing action videos where a similar action is performed by different people at different times and different locations with different local speed changes, e.g., as in sports like weightlifting, baseball pitch, or dance. Our approach extends the popular “snapping” tool of video editing software and allows users to automatically snap action videos together in a timeline based on their content. Since the action can take place at different locations, existing appearance-based methods are not appropriate. Our approach leverages motion information, and computes a nonlinear synchronization of the input videos to establish frame-to-frame temporal correspondences. We demonstrate our approach can be applied for video synchronization, video annotation, and action snapshots. Our approach has been successfully evaluated with ground truth data and a user study.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_10');
INSERT INTO `paper` VALUES (10030, 'Adaptive Signal Recovery on Graphs via Harmonic Analysis for Experimental Design in Neuroimaging', 'Fractional Anisotropy', 'Mother Wavelet', 'Graph Vertex', 'Full Cohort', 'Matrix Completion', 'Consider an experimental design of a neuroimaging study, where we need to obtain p measurements for each participant in a setting where \\(p^\\prime (< p)\\) are cheaper and easier to acquire while the remaining \\((p-p^\\prime )\\) are expensive. For example, the \\(p^{\\prime }\\) measurements may include demographics, cognitive scores or routinely offered imaging scans while the \\((p-p^{\\prime })\\) measurements may correspond to more expensive types of brain image scans with a higher participant burden. In this scenario, it seems reasonable to seek an “adaptive” design for data acquisition so as to minimize the cost of the study without compromising statistical power. We show how this problem can be solved via harmonic analysis of a band-limited graph whose vertices correspond to participants and our goal is to fully recover a multi-variate signal on the nodes, given the full set of cheaper features and a partial set of more expensive measurements. This is accomplished using an adaptive query strategy derived from probing the properties of the graph in the frequency space. To demonstrate the benefits that this framework can provide, we present experimental evaluations on two independent neuroimaging studies and show that our proposed method can reliably recover the true signal with only partial observations directly yielding substantial financial savings.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_12');
INSERT INTO `paper` VALUES (10031, 'All-Around Depth from Small Motion with a Spherical Panoramic Camera', 'Structure from Motion (SfM)', 'Small motion', 'Stereoscopic panorama', 'Spherical Panoramic Camera', '', 'With the growing use of head-mounted displays for virtual reality (VR), generating 3D contents for these devices becomes an important topic in computer vision. For capturing full 360 degree panoramas in a single shot, the Spherical Panoramic Camera (SPC) are gaining in popularity. However, estimating depth from a SPC remains a challenging problem. In this paper, we propose a practical method that generates all-around dense depth map using a narrow-baseline video clip captured by a SPC. While existing methods for depth from small motion rely on perspective cameras, we introduce a new bundle adjustment approach tailored for SPC that minimizes the re-projection error directly on the unit sphere. It enables to estimate approximate metric camera poses and 3D points. Additionally, we present a novel dense matching method called sphere sweeping algorithm. This allows us to take advantage of the overlapping regions between the cameras. To validate the effectiveness of the proposed method, we evaluate our approach on both synthetic and real-world data. As an example of the applications, we also present stereoscopic panorama images generated from our depth results.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_10');
INSERT INTO `paper` VALUES (10032, 'Ambient Sound Provides Supervision for Visual Learning', 'Sound', 'Convolutional networks', 'Unsupervised learning', '', '', 'The sound of crashing waves, the roar of fast-moving cars – sound conveys important information about the objects in our surroundings. In this work, we show that ambient sounds can be used as a supervisory signal for learning visual models. To demonstrate this, we train a convolutional neural network to predict a statistical summary of the sound associated with a video frame. We show that, through this process, the network learns a representation that conveys information about objects and scenes. We evaluate this representation on several recognition tasks, finding that its performance is comparable to that of other state-of-the-art unsupervised learning methods. Finally, we show through visualizations that the network learns units that are selective to objects that are often associated with characteristic sounds.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_48');
INSERT INTO `paper` VALUES (10033, 'Amodal Instance Segmentation', 'Instance segmentation', 'Amodal completions', 'Occlusion reasoning', '', '', 'We consider the problem of amodal instance segmentation, the objective of which is to predict the region encompassing both visible and occluded parts of each object. Thus far, the lack of publicly available amodal segmentation annotations has stymied the development of amodal segmentation methods. In this paper, we sidestep this issue by relying solely on standard modal instance segmentation annotations to train our model. The result is a new method for amodal instance segmentation, which represents the first such method to the best of our knowledge. We demonstrate the proposed method’s effectiveness both qualitatively and quantitatively.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_42');
INSERT INTO `paper` VALUES (10034, 'An Efficient Fusion Move Algorithm for the Minimum Cost Lifted Multicut Problem', 'Multicut Problem', 'Fusion Move', 'Biological Image Analysis', 'Superpixels', 'Region Adjacency Graph (RAG)', 'Many computer vision problems can be cast as an optimization problem whose feasible solutions are decompositions of a graph. The minimum cost lifted multicut problem is such an optimization problem. Its objective function can penalize or reward all decompositions for which any given pair of nodes are in distinct components. While this property has many potential applications, such applications are hampered by the fact that the problem is NP-hard. We propose a fusion move algorithm for computing feasible solutions, better and more efficiently than existing algorithms. We demonstrate this and applications to image segmentation, obtaining a new state of the art for a problem in biological image analysis.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_44');
INSERT INTO `paper` VALUES (10035, 'An Empirical Study and Analysis of Generalized Zero-Shot Learning for Object Recognition in the Wild', 'Zero-shot Learning (ZSL)', 'Unseen Classes', 'Semantic Embedding', 'Unseen Ones', 'ZSL Approaches', 'We investigate the problem of generalized zero-shot learning (GZSL). GZSL relaxes the unrealistic assumption in conventional zero-shot learning (ZSL) that test data belong only to unseen novel classes. In GZSL, test data might also come from seen classes and the labeling space is the union of both types of classes. We show empirically that a straightforward application of classifiers provided by existing ZSL approaches does not perform well in the setting of GZSL. Motivated by this, we propose a surprisingly simple but effective method to adapt ZSL approaches for GZSL. The main idea is to introduce a calibration factor to calibrate the classifiers for both seen and unseen classes so as to balance two conflicting forces: recognizing data from seen classes and those from unseen ones. We develop a new performance metric called the Area Under Seen-Unseen accuracy Curve to characterize this trade-off. We demonstrate the utility of this metric by analyzing existing ZSL approaches applied to the generalized setting. Extensive empirical studies reveal strengths and weaknesses of those approaches on three well-studied benchmark datasets, including the large-scale ImageNet with more than 20,000 unseen categories. We complement our comparative studies in learning methods by further establishing an upper bound on the performance limit of GZSL. In particular, our idea is to use class-representative visual features as the idealized semantic embeddings. We show that there is a large gap between the performance of existing approaches and the performance limit, suggesting that improving the quality of class semantic embeddings is vital to improving ZSL.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_4');
INSERT INTO `paper` VALUES (10036, 'An Evaluation of Computational Imaging Techniques for Heterogeneous Inverse Scattering', 'Inverse scattering', 'Computational imaging', '', '', '', 'Inferring internal scattering parameters for general, heterogeneous materials, remains a challenging inverse problem. Its difficulty arises from the complex way in which scattering materials interact with light, as well as the very high dimensionality of the material space implied by heterogeneity. The recent emergence of diverse computational imaging techniques, together with the widespread availability of computing power, present a renewed opportunity for tackling this problem. We take first steps in this direction, by deriving theoretical results, developing an algorithmic framework, and performing quantitative evaluations for the problem of heterogeneous inverse scattering from simulated measurements of different computational imaging configurations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_42');
INSERT INTO `paper` VALUES (10037, 'An Occlusion-Resistant Ellipse Detection Method by Joining Coelliptic Arcs', 'Ellipse detection', 'Arc detection', 'Feature extraction', 'Hough transform', '', 'In this study, we propose an ellipse detection method which gives prospering results on occlusive cases. The method starts with detection of edge segments. Then we extract elliptical arcs by computing corners and fitting ellipse to the pixels between two consecutive corners. Once the elliptical arcs are extracted, we aim to test all possible arc subsets. However, this requires exponential complexity and runtime diverges as the number of arcs increases. To accelerate the process, arc pairing strategy is deployed by using conic properties of arcs. If any pair found to be non-coelliptic, then arc combinations including that pair are eliminated. Therefore the number of possible arcs subsets is reduced and computation time is improved. In the end, ellipse fitting is applied to remaining arc combinations to decide on final ellipses. Performance of the proposed algorithm is tested on real datasets, and better results have been obtained compare to state-of-the-art algorithms.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_31');
INSERT INTO `paper` VALUES (10038, 'Angry Crowds: Detecting Violent Events in Videos', 'Violent events', 'Social force model', 'Behavioral heuristics', '', '', 'Approaches inspired by Newtonian mechanics have been successfully applied for detecting abnormal behaviors in crowd scenarios, being the most notable example the Social Force Model (SFM). This class of approaches describes the movements and local interactions among individuals in crowds by means of repulsive and attractive forces. Despite their promising performance, recent socio-psychology studies have shown that current SFM-based methods may not be capable of explaining behaviors in complex crowd scenarios. An alternative approach consists in describing the cognitive processes that gives rise to the behavioral patterns observed in crowd using heuristics. Inspired by these studies, we propose a new hybrid framework to detect violent events in crowd videos. More specifically, (i) we define a set of simple behavioral heuristics to describe people behaviors in crowd, and (ii) we implement these heuristics into physical equations, being able to model and classify such behaviors in the videos. The resulting heuristic maps are used to extract video features to distinguish violence from normal events. Our violence detection results set the new state of the art on several standard benchmarks and demonstrate the superiority of our method compared to standard motion descriptors, previous physics-inspired models used for crowd analysis and pre-trained ConvNet for crowd behavior analysis.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46478-7_1');
INSERT INTO `paper` VALUES (10039, 'ATGV-Net: Accurate Depth Super-Resolution', 'Deep networks', 'Variational methods', 'Depth super-resolution', '', '', 'In this work we present a novel approach for single depth map super-resolution. Modern consumer depth sensors, especially Time-of-Flight sensors, produce dense depth measurements, but are affected by noise and have a low lateral resolution. We propose a method that combines the benefits of recent advances in machine learning based single image super-resolution, i.e. deep convolutional networks, with a variational method to recover accurate high-resolution depth maps. In particular, we integrate a variational method that models the piecewise affine structures apparent in depth data via an anisotropic total generalized variation regularization term on top of a deep network. We call our method ATGV-Net and train it end-to-end by unrolling the optimization procedure of the variational method. To train deep networks, a large corpus of training data with accurate ground-truth is required. We demonstrate that it is feasible to train our method solely on synthetic data that we generate in large quantities for this task. Our evaluations show that we achieve state-of-the-art results on three different benchmarks, as well as on a challenging Time-of-Flight dataset, all without utilizing an additional intensity image as guidance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_17');
INSERT INTO `paper` VALUES (10040, 'Attribute2Image: Conditional Image Generation from Visual Attributes', 'Face Image', 'Image Generation', 'Convolutional Neural Network', 'Deep Neural Network', 'Recognition Model', 'This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_47');
INSERT INTO `paper` VALUES (10041, 'Automatic Attribute Discovery with Neural Activations', 'Concept discovery', 'Attribute discovery', 'Saliency detection', '', '', 'How can a machine learn to recognize visual attributes emerging out of online community without a definitive supervised dataset? This paper proposes an automatic approach to discover and analyze visual attributes from a noisy collection of image-text data on the Web. Our approach is based on the relationship between attributes and neural activations in the deep network. We characterize the visual property of the attribute word as a divergence within weakly-annotated set of images. We show that the neural activations are useful for discovering and learning a classifier that well agrees with human perception from the noisy real-world Web data. The empirical study suggests the layered structure of the deep neural networks also gives us insights into the perceptual depth of the given word. Finally, we demonstrate that we can utilize highly-activating neurons for finding semantically relevant regions.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_16');
INSERT INTO `paper` VALUES (10042, 'Automatically Selecting Inference Algorithms for Discrete Energy Minimisation', 'Problem Instance', 'Problem Class', 'Inference Algorithm', 'Stereo Match', 'Semantic Segmentation', 'Minimisation of discrete energies defined over factors is an important problem in computer vision, and a vast number of MAP inference algorithms have been proposed. Different inference algorithms perform better on factor graph models (GMs) from different underlying problem classes, and in general it is difficult to know which algorithm will yield the lowest energy for a given GM. To mitigate this difficulty, survey papers [1, 2, 3] advise the practitioner on what algorithms perform well on what classes of models. We take the next step forward, and present a technique to automatically select the best inference algorithm for an input GM. We validate our method experimentally on an extended version of the OpenGM2 benchmark [3], containing a diverse set of vision problems. On average, our method selects an inference algorithm yielding labellings with 96 % of variables the same as the best available algorithm.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_15');
INSERT INTO `paper` VALUES (10043, 'Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking', 'Training Sample', 'Fourier Coefficient', 'Object Tracking', 'Convolution Operator', 'Visual Tracking', 'Discriminative Correlation Filters (DCF) have demonstrated excellent performance for visual object tracking. The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample. However, the underlying DCF formulation is restricted to single-resolution feature maps, significantly limiting its potential. In this paper, we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters. We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain. Our proposed formulation enables efficient integration of multi-resolution deep feature maps, leading to superior results on three object tracking benchmarks: OTB-2015 (\\(+5.1\\,\\%\\) in mean OP), Temple-Color (\\(+4.6\\,\\%\\) in mean OP), and VOT2015 (\\(20\\,\\%\\) relative reduction in failure rate). Additionally, our approach is capable of sub-pixel localization, crucial for the task of accurate feature point tracking. We also demonstrate the effectiveness of our learning formulation in extensive feature point tracking experiments.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_29');
INSERT INTO `paper` VALUES (10044, 'Biconvex Relaxation for Semidefinite Programming in Computer Vision', 'Inequality Constraint', 'Interior Point Method', 'Semidefinite Program', 'Photometric Stereo', 'Semidefinite Relaxation', 'Semidefinite programming (SDP) is an indispensable tool in computer vision, but general-purpose solvers for SDPs are often too slow and memory intensive for large-scale problems. Our framework, referred to as biconvex relaxation (BCR), transforms an SDP consisting of PSD constraint matrices into a specific biconvex optimization problem, which can then be approximately solved in the original, low-dimensional variable space at low complexity. The resulting problem is solved using an efficient alternating minimization (AM) procedure. Since AM has the potential to get stuck in local minima, we propose a general initialization scheme that enables BCR to start close to a global optimum—this is key for BCR to quickly converge to optimal or near-optimal solutions. We showcase the efficacy of our approach on three applications in computer vision, namely segmentation, co-segmentation, and manifold metric learning. BCR achieves solution quality comparable to state-of-the-art SDP methods with speedups between \\(4\\times \\) and \\(35\\times \\).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_43');
INSERT INTO `paper` VALUES (10045, 'Binary Hashing with Semidefinite Relaxation and Augmented Lagrangian', 'Two-step hashing', 'Semidefinite programming', 'Augmented Lagrangian', '', '', 'This paper proposes two approaches for inferencing binary codes in two-step (supervised, unsupervised) hashing. We first introduce an unified formulation for both supervised and unsupervised hashing. Then, we cast the learning of one bit as a Binary Quadratic Problem (BQP). We propose two approaches to solve BQP. In the first approach, we relax BQP as a semidefinite programming problem which its global optimum can be achieved. We theoretically prove that the objective value of the binary solution achieved by this approach is well bounded. In the second approach, we propose an augmented Lagrangian based approach to solve BQP directly without relaxing the binary constraint. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_49');
INSERT INTO `paper` VALUES (10046, 'Branching Path Following for Graph Matching', 'Graph matching', 'Path following', 'Numerical continuation', 'Singular point', 'Branch switching', 'Recently, graph matching algorithms utilizing the path following strategy have exhibited state-of-the-art performances. However, the paths computed in these algorithms often contain singular points, which usually hurt the matching performance. To deal with this issue, in this paper we propose a novel path following strategy, named branching path following (BPF), which consequently improves graph matching performance. In particular, we first propose a singular point detector by solving an KKT system, and then design a branch switching method to seek for better paths at singular points. Using BPF, a new graph matching algorithm named BPF-G is developed by applying BPF to a recently proposed path following algorithm named GNCCP (Liu&Qiao 2014). For evaluation, we compare BPF-G with several recently proposed graph matching algorithms on a synthetic dataset and four public benchmark datasets. Experimental results show that our approach achieves remarkable improvement in matching accuracy and outperforms other algorithms.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_32');
INSERT INTO `paper` VALUES (10047, 'Building Dual-Domain Representations for Compression Artifacts Reduction', 'Compression artifacts reduction', 'Dual-domain representation', 'Very deep convolutional network', '', '', 'We propose a highly accurate approach to remove artifacts of JPEG-compressed images. Our approach jointly learns a very deep convolutional network in both DCT and pixel domains. The dual-domain representation can make full use of DCT-domain prior knowledge of JPEG compression, which is usually lacking in traditional network-based approaches. At the same time, it can also benefit from the prowess and the efficiency of the deep feed-forward architecture, in comparison to capacity-limited sparse-coding-based approaches. Two simple strategies, i.e., Adam and residual learning, are adopted to train the very deep network and later proved to be a success. Extensive experiments demonstrate the large improvements of our approach over the state of the arts.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_38');
INSERT INTO `paper` VALUES (10048, 'Building Scene Models by Completing and Hallucinating Depth and Semantics', 'Scene Model', 'Input Depth Map', 'Completion Depth', 'Foreground Objects', 'Foreground Mask', 'Building 3D scene models has been a longstanding goal of computer vision. The great progress in depth sensors brings us one step closer to achieving this in a single shot. However, depth sensors still produce imperfect measurements that are sparse and contain holes. While depth completion aims at tackling this issue, it ignores the fact that some regions of the scene are occluded by the foreground objects. Building a scene model would therefore require to hallucinate the depth behind these objects. In contrast with existing methods that either rely on manual input, or focus on the indoor scenario, we introduce a fully-automatic method to jointly complete and hallucinate depth and semantics in challenging outdoor scenes. To this end, we develop a two-layer model representing both the visible information and the hidden one. At the heart of our approach lies a formulation based on the Mumford-Shah functional, for which we derive an effective optimization strategy. Our experiments evidence that our approach can accurately fill the large holes in the input depth maps, segment the different kinds of objects in the scene, and hallucinate the depth and semantics behind the foreground objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_16');
INSERT INTO `paper` VALUES (10049, 'Can We Jointly Register and Reconstruct Creased Surfaces by Shape-from-Template Accurately?', '3D reconstruction', 'Shape-from-Template', 'Isometry', 'Boundaries', 'Bending energy', 'Shape-from-Template (SfT) aims to reconstruct a deformable object from a single image using a texture-mapped 3D model of the object in a reference position. Most existing SfT methods require well-textured surfaces that deform smoothly, which is a significant limitation. Due to the sparsity of correspondence constraint and strong regularizations, they usually fail to reconstruct strong changes of surface curvature such as surface creases. We investigate new ways to solve SfT for creased surfaces. Our main idea is to implicitly model creases with a dense mesh-based surface representation with an associated robust bending energy term, which deactivates curvature smoothing automatically where needed. Crucially, the crease locations are not required a priori since they emerge as the lowest-energy state during optimization. We show with real data that by combining this model with correspondence and surface boundary constraints we can successfully reconstruct creases while also preserving smooth regions.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_7');
INSERT INTO `paper` VALUES (10050, 'CDT: Cooperative Detection and Tracking for Tracing Multiple Objects in Video Sequences', 'Joint detection and tracking', 'Multiple object tracking', 'Object detection', 'Model-free tracking', 'Online multi-object tracking', 'A cooperative detection and model-free tracking algorithm, referred to as CDT, for multiple object tracking is proposed in this work. The proposed CDT algorithm has three components: object detector, forward tracker, and backward tracker. First, the object detector detects targets with high confidence levels only to reduce spurious detection and achieve a high precision rate. Then, each detected target is traced by the forward tracker and then by the backward tracker to restore undetected states. In the tracking processes, the object detector cooperates with the trackers to handle appearing or disappearing targets and to refine inaccurate state estimates. With this detection guidance, the model-free tracking can trace multiple objects reliably and accurately. Experimental results show that the proposed CDT algorithm provides excellent performance on a recent benchmark. Furthermore, an online version of the proposed algorithm also excels in the benchmark.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_51');
INSERT INTO `paper` VALUES (10051, 'Chained Predictions Using Convolutional Neural Networks', 'Structured tasks', 'Chain model', 'Human pose estimation', '', '', 'In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing results on human pose estimation from images and videos.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_44');
INSERT INTO `paper` VALUES (10052, 'Cluster Sparsity Field for Hyperspectral Imagery Denoising', 'Hyperspectral', 'Denoising', 'Structured sparsity', 'Spatial similarity', '', 'Hyperspectral images (HSIs) can facilitate extensive computer vision applications with the extra spectra information. However, HSIs often suffer from noise corruption during the practical imaging procedure. Though it has been testified that intrinsic correlation across spectrum and spatial similarity (i.e., local similarity in locally smooth areas and non-local similarity among recurrent patterns) in HSIs are useful for denoising, how to fully exploit them together to obtain a good denoising model is seldom studied. In this study, we present an effective cluster sparsity field based HSIs denoising (CSFHD) method by exploiting those two characteristics simultaneously. Firstly, a novel Markov random field prior, named cluster sparsity field (CSF), is proposed for the sparse representation of an HSI. By grouping pixels into several clusters with spectral similarity, the CSF prior defines both a structured sparsity potential and a graph structure potential on each cluster to model the correlation across spectrum and spatial similarity in the HSI, respectively. Then, the CSF prior learning and the image denoising are unified into a variational framework for optimization, where all unknown variables are learned directly from the noisy observation. This guarantees to learn a data-dependent image model, thus producing satisfying denoising results. Plenty experiments on denoising synthetic and real noisy HSIs validated that the proposed CSFHD outperforms several state-of-the-art methods.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_38');
INSERT INTO `paper` VALUES (10053, 'CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples', 'CNN fine-tuning', 'Unsupervised learning', 'Image retrieval', '', '', 'Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_1');
INSERT INTO `paper` VALUES (10054, 'Coarse-to-fine Planar Regularization for Dense Monocular Depth Estimation', 'SLAM', 'Monocular odometry', 'Dense tracking and mapping', '', '', 'Simultaneous localization and mapping (SLAM) using the whole image data is an appealing framework to address shortcoming of sparse feature-based methods – in particular frequent failures in textureless environments. Hence, direct methods bypassing the need of feature extraction and matching became recently popular. Many of these methods operate by alternating between pose estimation and computing (semi-)dense depth maps, and are therefore not fully exploiting the advantages of joint optimization with respect to depth and pose. In this work, we propose a framework for monocular SLAM, and its local model in particular, which optimizes simultaneously over depth and pose. In addition to a planarity enforcing smoothness regularizer for the depth we also constrain the complexity of depth map updates, which provides a natural way to avoid poor local minima and reduces unknowns in the optimization. Starting from a holistic objective we develop a method suitable for online and real-time monocular SLAM. We evaluate our method quantitatively in pose and depth on the TUM dataset, and qualitatively on our own video sequences.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_29');
INSERT INTO `paper` VALUES (10055, 'COCO Attributes: Attributes for People, Animals, and Objects', 'Dataset creation', 'Attributes', 'Crowdsourcing', 'Multilabel recognition', '', 'In this paper, we discover and annotate visual attributes for the COCO dataset. With the goal of enabling deeper object understanding, we deliver the largest attribute dataset to date. Using our COCO Attributes dataset, a fine-tuned classification system can do more than recognize object categories – for example, rendering multi-label classifications such as “sleeping spotted curled-up cat” instead of simply “cat”. To overcome the expense of annotating thousands of COCO object instances with hundreds of attributes, we present an Economic Labeling Algorithm (ELA) which intelligently generates crowd labeling tasks based on correlations between attributes. The ELA offers a substantial reduction in labeling cost while largely maintaining attribute density and variety. Currently, we have collected 3.5 million object-attribute pair annotations describing 180 thousand different objects. We demonstrate that our efficiently labeled training data can be used to produce classifiers of similar discriminative ability as classifiers created using exhaustively labeled ground truth. Finally, we provide baseline performance analysis for object attribute recognition.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_6');
INSERT INTO `paper` VALUES (10056, 'Colorful Image Colorization', 'Colorization', 'Vision for graphics', 'CNNs', 'Self-supervised learning', '', 'Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32 % of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_40');
INSERT INTO `paper` VALUES (10057, 'Complexity of Discrete Energy Minimization Problems', 'Energy minimization', 'Complexity', 'NP-hard', 'APX', 'exp-APX', 'Discrete energy minimization is widely-used in computer vision and machine learning for problems such as MAP inference in graphical models. The problem, in general, is notoriously intractable, and finding the global optimal solution is known to be NP-hard. However, is it possible to approximate this problem with a reasonable ratio bound on the solution quality in polynomial time? We show in this paper that the answer is no. Specifically, we show that general energy minimization, even in the 2-label pairwise case, and planar energy minimization with three or more labels are exp-APX-complete. This finding rules out the existence of any approximation algorithm with a sub-exponential approximation ratio in the input size for these two problems, including constant factor approximations. Moreover, we collect and review the computational complexity of several subclass problems and arrange them on a complexity scale consisting of three major complexity classes – PO, APX, and exp-APX, corresponding to problems that are solvable, approximable, and inapproximable in polynomial time. Problems in the first two complexity classes can serve as alternative tractable formulations to the inapproximable ones. This paper can help vision researchers to select an appropriate model for an application or guide them in designing new algorithms.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_51');
INSERT INTO `paper` VALUES (10058, 'Connectionist Temporal Modeling for Weakly Supervised Action Labeling', 'Action Labels', 'Connectionist Temporal Classification (CTC)', 'Visual Similarity Function', 'Temperature Supervision', 'Frame Accuracy', 'We propose a weakly-supervised framework for action labeling in video, where only the order of occurring actions is required during training time. The key challenge is that the per-frame alignments between the input (video) and label (action) sequences are unknown during training. We address this by introducing the Extended Connectionist Temporal Classification (ECTC) framework to efficiently evaluate all possible alignments via dynamic programming and explicitly enforce their consistency with frame-to-frame visual similarities. This protects the model from distractions of visually inconsistent or degenerated alignments without the need of temporal supervision. We further extend our framework to the semi-supervised case when a few frames are sparsely annotated in a video. With less than 1 % of labeled frames per video, our method is able to outperform existing semi-supervised approaches and achieve comparable performance to that of fully supervised approaches.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_9');
INSERT INTO `paper` VALUES (10059, 'ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization', 'Object recognition', 'Object detection', 'Weakly supervised object localization', 'Context', 'Convolutional neural networks', 'We aim to localize objects in images using image-level supervision only. Previous approaches to this problem mainly focus on discriminative object regions and often fail to locate precise object boundaries. We address this problem by introducing two types of context-aware guidance models, additive and contrastive models, that leverage their surrounding context regions to improve localization. The additive model encourages the predicted object region to be supported by its surrounding context region. The contrastive model encourages the predicted object region to be outstanding from its surrounding context region. Our approach benefits from the recent success of convolutional neural networks for object recognition and extends Fast R-CNN to weakly supervised object localization. Extensive experimental evaluation on the PASCAL VOC 2007 and 2012 benchmarks shows that our context-aware approach significantly improves weakly supervised localization and detection.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_22');
INSERT INTO `paper` VALUES (10060, 'Contextual Priming and Feedback for Faster R-CNN', 'Object Detection', 'Joint Model', 'Segmentation Signal', 'Segmentation Module', 'Contextual Priming', 'The field of object detection has seen dramatic performance improvements in the last few years. Most of these gains are attributed to bottom-up, feedforward ConvNet frameworks. However, in case of humans, top-down information, context and feedback play an important role in doing object detection. This paper investigates how we can incorporate top-down information and feedback in the state-of-the-art Faster R-CNN framework. Specifically, we propose to: (a) augment Faster R-CNN with a semantic segmentation network; (b) use segmentation for top-down contextual priming; (c) use segmentation to provide top-down iterative feedback using two stage training. Our results indicate that all three contributions improve the performance on object detection, semantic segmentation and region proposal generation.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_20');
INSERT INTO `paper` VALUES (10061, 'Convolutional Oriented Boundaries', 'Contour detection', 'Contour orientation estimation', 'Hierarchical image segmentation', 'Object proposals', '', 'We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments on BSDS, PASCAL Context, PASCAL Segmentation, and MS-COCO, showing that COB provides state-of-the-art contours, region hierarchies, and object proposals in all datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_35');
INSERT INTO `paper` VALUES (10062, 'Cross-Modal Supervision for Learning Active Speaker Detection in Video', 'Active speaker detection', 'Cross-modal supervision', 'Weakly supervised learning', 'Online learning', '', 'In this paper, we show how to use audio to supervise the learning of active speaker detection in video. Voice Activity Detection (VAD) guides the learning of the vision-based classifier in a weakly supervised manner. The classifier uses spatio-temporal features to encode upper body motion - facial expressions and gesticulations associated with speaking. We further improve a generic model for active speaker detection by learning person specific models. Finally, we demonstrate the online adaptation of generic models learnt on one dataset, to previously unseen people in a new dataset, again using audio (VAD) for weak supervision. The use of temporal continuity overcomes the lack of clean training data. We are the first to present an active speaker detection system that learns on one audio-visual dataset and automatically adapts to speakers in a new dataset. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_18');
INSERT INTO `paper` VALUES (10063, 'DAPs: Deep Action Proposals for Action Understanding', 'Action proposals', 'Action detection', 'Long-short term memory', '', '', 'Object proposals have contributed significantly to recent advances in object understanding in images. Inspired by the success of this approach, we introduce Deep Action Proposals (DAPs), an effective and efficient algorithm for generating temporal action proposals from long videos. We show how to take advantage of the vast capacity of deep learning models and memory cells to retrieve from untrimmed videos temporal segments, which are likely to contain actions. A comprehensive evaluation indicates that our approach outperforms previous work on a large scale action benchmark, runs at 134 FPS making it practical for large-scale scenarios, and exhibits an appealing ability to generalize, i.e. to retrieve good quality temporal proposals of actions unseen in training.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_47');
INSERT INTO `paper` VALUES (10064, 'DAVE: A Unified Framework for Fast Vehicle Detection and Annotation', 'Vehicle detection', 'Attributes annotation', 'Latent knowledge guidance', 'Joint learning', 'Deep networks', 'Vehicle detection and annotation for streaming video data with complex scenes is an interesting but challenging task for urban traffic surveillance. In this paper, we present a fast framework of Detection and Annotation for Vehicles (DAVE), which effectively combines vehicle detection and attributes annotation. DAVE consists of two convolutional neural networks (CNNs): a fast vehicle proposal network (FVPN) for vehicle-like objects extraction and an attributes learning network (ALN) aiming to verify each proposal and infer each vehicle’s pose, color and type simultaneously. These two nets are jointly optimized so that abundant latent knowledge learned from the ALN can be exploited to guide FVPN training. Once the system is trained, it can achieve efficient vehicle detection and annotation for real-world traffic surveillance data. We evaluate DAVE on a new self-collected UTS dataset and the public PASCAL VOC2007 car and LISA 2010 datasets, with consistent improvements over existing algorithms.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_18');
INSERT INTO `paper` VALUES (10065, 'Deep Attributes Driven Multi-camera Person Re-identification', 'Deep attributes', 'Re-identification', '', '', '', 'The visual appearance of a person is easily affected by many factors like pose variations, viewpoint changes and camera parameter differences. This makes person Re-Identification (ReID) among multiple cameras a very challenging task. This work is motivated to learn mid-level human attributes which are robust to such visual appearance variations. And we propose a semi-supervised attribute learning framework which progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely deep attributes exhibit superior generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained surprisingly good accuracy on four person ReID datasets. Experiments also show that a simple distance metric learning modular further boosts our method, making it significantly outperform many recent works.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_30');
INSERT INTO `paper` VALUES (10066, 'Deep Automatic Portrait Matting', 'Portrait', 'Matting', 'Automatic method', 'Neural network', '', 'We propose an automatic image matting method for portrait images. This method does not need user interaction, which was however essential in most previous approaches. In order to accomplish this goal, a new end-to-end convolutional neural network (CNN) based framework is proposed taking the input of a portrait image. It outputs the matte result. Our method considers not only image semantic prediction but also pixel-level image matte optimization. A new portrait image dataset is constructed with our labeled matting ground truth. Our automatic method achieves comparable results with state-of-the-art methods that require specified foreground and background regions or pixels. Many applications are enabled given the automatic nature of our system.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_6');
INSERT INTO `paper` VALUES (10067, 'Deep Cascaded Bi-Network for Face Hallucination', 'Facial Image', 'Dense Field', 'Convolutional Neural Network', 'Super Resolution', 'Facial Landmark', 'We present a novel framework for hallucinating faces of unconstrained poses and with very low resolution (face size as small as 5pxIOD). In contrast to existing studies that mostly ignore or assume pre-aligned face spatial configuration (e.g. facial landmarks localization or dense correspondence field), we alternatingly optimize two complementary tasks, namely face hallucination and dense correspondence field estimation, in a unified framework. In addition, we propose a new gated deep bi-network that contains two functionality-specialized branches to recover different levels of texture details. Extensive experiments demonstrate that such formulation allows exceptional hallucination quality on in-the-wild low-res faces with significant pose and illumination variations.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_37');
INSERT INTO `paper` VALUES (10068, 'Deep Deformation Network for Object Landmark Localization', 'Landmark localization', 'Convolutional Neural Network', 'Non-rigid shape analysis', '', '', 'We propose a novel cascaded framework, namely deep deformation network (DDN), for localizing landmarks in non-rigid objects. The hallmarks of DDN are its incorporation of geometric constraints within a convolutional neural network (CNN) framework, ease and efficiency of training, as well as generality of application. A novel shape basis network (SBN) forms the first stage of the cascade, whereby landmarks are initialized by combining the benefits of CNN features and a learned shape basis to reduce the complexity of the highly nonlinear pose manifold. In the second stage, a point transformer network (PTN) estimates local deformation parameterized as thin-plate spline transformation for a finer refinement. Our framework does not incorporate either handcrafted features or part connectivity, which enables an end-to-end shape prediction pipeline during both training and testing. In contrast to prior cascaded networks for landmark localization that learn a mapping from feature space to landmark locations, we demonstrate that the regularization induced through geometric priors in the DDN makes it easier to train, yet produces superior results. The efficacy and generality of the architecture is demonstrated through state-of-the-art performances on several benchmarks for multiple tasks such as facial landmark localization, human body pose estimation and bird part localization.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_4');
INSERT INTO `paper` VALUES (10069, 'Deep Image Retrieval: Learning Global Representations for Image Search', 'Deep learning', 'Instance-level retrieval', '', '', '', 'We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at www.xrce.xerox.com/Deep-Image-Retrieval.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_15');
INSERT INTO `paper` VALUES (10070, 'Deep Joint Image Filtering', 'Joint filtering', 'Deep convolutional neural networks', '', '', '', 'Joint image filters can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods rely on various kinds of explicit filter construction or hand-designed objective functions. It is thus difficult to understand, improve, and accelerate them in a coherent framework. In this paper, we propose a learning-based approach to construct a joint filter based on Convolutional Neural Networks. In contrast to existing methods that consider only the guidance image, our method can selectively transfer salient structures that are consistent in both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well for other modalities, e.g., Flash/Non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive comparisons with state-of-the-art methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_10');
INSERT INTO `paper` VALUES (10071, 'Deep Learning 3D Shape Surfaces Using Geometry Images', 'Deep learning', '3D Shape', 'Surfaces', 'CNN', 'Geometry images', 'Surfaces serve as a natural parametrization to 3D shapes. Learning surfaces using convolutional neural networks (CNNs) is a challenging task. Current paradigms to tackle this challenge are to either adapt the convolutional filters to operate on surfaces, learn spectral descriptors defined by the Laplace-Beltrami operator, or to drop surfaces altogether in lieu of voxelized inputs. Here we adopt an approach of converting the 3D shape into a ‘geometry image’ so that standard CNNs can directly be used to learn 3D shapes. We qualitatively and quantitatively validate that creating geometry images using authalic parametrization on a spherical domain is suitable for robust learning of 3D shape surfaces. This spherically parameterized shape is then projected and cut to convert the original 3D shape into a flat and regular geometry image. We propose a way to implicitly learn the topology and structure of 3D shapes using geometry images encoded with suitable features. We show the efficacy of our approach to learn 3D shape surfaces for classification and retrieval tasks on non-rigid and rigid shape datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_14');
INSERT INTO `paper` VALUES (10072, 'Deep Learning of Local RGB-D Patches for 3D Object Detection and 6D Pose Estimation', 'Convolutional Neural Network', 'Local Patch', 'Vote Casting', 'Local Vote', 'Vote Space', 'We present a 3D object detection method that uses regressed descriptors of locally-sampled RGB-D patches for 6D vote casting. For regression, we employ a convolutional auto-encoder that has been trained on a large collection of random local patches. During testing, scene patch descriptors are matched against a database of synthetic model view patches and cast 6D object votes which are subsequently filtered to refined hypotheses. We evaluate on three datasets to show that our method generalizes well to previously unseen input data, delivers robust detection results that compete with and surpass the state-of-the-art while being scalable in the number of objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_13');
INSERT INTO `paper` VALUES (10073, 'Deep Learning the City: Quantifying Urban Perception at a Global Scale', 'Perception', 'Attributes', 'Street view', 'Crowdsourcing', '', 'Computer vision methods that quantify the perception of urban environment are increasingly being used to study the relationship between a city’s physical appearance and the behavior and health of its residents. Yet, the throughput of current methods is too limited to quantify the perception of cities across the world. To tackle this challenge, we introduce a new crowdsourced dataset containing 110,988 images from 56 cities, and 1,170,000 pairwise comparisons provided by 81,630 online volunteers along six perceptual attributes: safe, lively, boring, wealthy, depressing, and beautiful. Using this data, we train a Siamese-like convolutional neural architecture, which learns from a joint classification and ranking loss, to predict human judgments of pairwise image comparisons. Our results show that crowdsourcing combined with neural networks can produce urban perception data at the global scale.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_12');
INSERT INTO `paper` VALUES (10074, 'Deep Networks with Stochastic Depth', 'Training Time', 'Test Error', 'Constant Depth', 'Validation Error', 'Early Layer', 'Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_39');
INSERT INTO `paper` VALUES (10075, 'Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation', 'Domain adaptation', 'Object recognition', 'Deep learning', 'Convolutional networks', 'Transfer learning', 'In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: (i) supervised classification of labeled source data, and (ii) unsupervised reconstruction of unlabeled target data. In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_36');
INSERT INTO `paper` VALUES (10076, 'Deep Robust Encoder Through Locality Preserving Low-Rank Dictionary', 'Auto-encoder', 'Low-rank dictionary', 'Graph regularizer', '', '', 'Deep learning has attracted increasing attentions recently due to its appealing performance in various tasks. As a principal way of deep feature learning, deep auto-encoder has been widely discussed in such problems as dimensionality reduction and model pre-training. Conventional auto-encoder and its variants usually involve additive noises (e.g., Gaussian, masking) for training data to learn robust features, which, however, did not consider the already corrupted data. In this paper, we propose a novel Deep Robust Encoder (DRE) through locality preserving low-rank dictionary to extract robust and discriminative features from corrupted data, where a low-rank dictionary and a regularized deep auto-encoder are jointly optimized. First, we propose a novel loss function in the output layer with a learned low-rank clean dictionary and corresponding weights with locality information, which ensures that the reconstruction is noise free. Second, discriminant graph regularizers that preserve the local geometric structure for the data are developed to guide the deep feature learning in each encoding layer. Experimental results on several benchmarks including object and face images verify the effectiveness of our algorithm by comparing with the state-of-the-art approaches.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_34');
INSERT INTO `paper` VALUES (10077, 'Deep Specialized Network for Illuminant Estimation', 'Illuminant Estimation', 'Color Constancy', 'Selection Hypothesis', 'Large Hypothesis Space', 'Network Hypothesis', 'Illuminant estimation to achieve color constancy is an ill-posed problem. Searching the large hypothesis space for an accurate illuminant estimation is hard due to the ambiguities of unknown reflections and local patch appearances. In this work, we propose a novel Deep Specialized Network (DS-Net) that is adaptive to diverse local regions for estimating robust local illuminants. This is achieved through a new convolutional network architecture with two interacting sub-networks, i.e. an hypotheses network (HypNet) and a selection network (SelNet). In particular, HypNet generates multiple illuminant hypotheses that inherently capture different modes of illuminants with its unique two-branch structure. SelNet then adaptively picks for confident estimations from these plausible hypotheses. Extensive experiments on the two largest color constancy benchmark datasets show that the proposed ‘hypothesis selection’ approach is effective to overcome erroneous estimation. Through the synergy of HypNet and SelNet, our approach outperforms state-of-the-art methods such as [1, 2, 3].', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_23');
INSERT INTO `paper` VALUES (10078, 'Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks', 'Monocular stereo reconstruction', 'Deep convolutional neural networks', '', '', '', 'As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2D videos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained end-to-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_51');
INSERT INTO `paper` VALUES (10079, 'DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model', 'Body Part', 'Integer Linear Programming', 'Area Under Curve', 'Part Detector', 'Conv4 Bank', 'The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_3');
INSERT INTO `paper` VALUES (10080, 'DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation', 'Gaze correction', 'Warping', 'Spatial transformers', 'Supervised learning', 'Deep learning', 'In this work, we consider the task of generating highly-realistic images of a given face with a redirected gaze. We treat this problem as a specific instance of conditional image generation and suggest a new deep architecture that can handle this task very well as revealed by numerical comparison with prior art and a user study. Our deep architecture performs coarse-to-fine warping with an additional intensity correction of individual pixels. All these operations are performed in a feed-forward manner, and the parameters associated with different operations are learned jointly in the end-to-end fashion. After learning, the resulting neural network can synthesize images with manipulated gaze, while the redirection angle can be selected arbitrarily from a certain range and provided as an input to the network.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_20');
INSERT INTO `paper` VALUES (10081, 'Degeneracies in Rolling Shutter SfM', 'Structure from motion', 'Rolling shutter', 'Degeneracy', 'Non-perspective cameras', '', 'We address the problem of Structure from Motion (SfM) with rolling shutter cameras. We first show that many common camera configurations, e.g. cameras with parallel readout directions, become critical and allow for a large class of ambiguities in multi-view reconstruction. We provide mathematical analysis for one, two and some multi-view cases and verify it by synthetic experiments. Next, we demonstrate that bundle adjustment with rolling shutter cameras, which are close to critical configurations, may still produce drastically deformed reconstructions. Finally, we provide practical recipes how to photograph with rolling shutter cameras to avoid scene deformations in SfM. We evaluate the recipes and provide a quantitative analysis of their performance in real experiments. Our results show how to reconstruct correct 3D models with rolling shutter cameras.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_3');
INSERT INTO `paper` VALUES (10082, 'Depth Map Super-Resolution by Deep Multi-Scale Guidance', 'Sparse Code', 'Convolutional Neural Network', 'Super Resolution', 'Joint Bilateral Filter', 'Image Super Resolution', 'Depth boundaries often lose sharpness when upsampling from low-resolution (LR) depth maps especially at large upscaling factors. We present a new method to address the problem of depth map super resolution in which a high-resolution (HR) depth map is inferred from a LR depth map and an additional HR intensity image of the same scene. We propose a Multi-Scale Guided convolutional network (MSG-Net) for depth map super resolution. MSG-Net complements LR depth features with HR intensity features using a multi-scale fusion strategy. Such a multi-scale guidance allows the network to better adapt for upsampling of both fine- and large-scale structures. Specifically, the rich hierarchical HR intensity features at different levels progressively resolve ambiguity in depth map upsampling. Moreover, we employ a high-frequency domain training method to not only reduce training time but also facilitate the fusion of depth and intensity features. With the multi-scale guidance, MSG-Net achieves state-of-art performance for depth map upsampling.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_22');
INSERT INTO `paper` VALUES (10083, 'Detecting Engagement in Egocentric Video', 'Ground Truth', 'Random Forest', 'Optical Flow', 'Inertial Sensor', 'Random Forest Classifier', 'In a wearable camera video, we see what the camera wearer sees. While this makes it easy to know roughly Open image in new window , it does not immediately reveal Open image in new window . Specifically, at what moments did his focus linger, as he paused to gather more information about something he saw? Knowing this answer would benefit various applications in video summarization and augmented reality, yet prior work focuses solely on the “what” question (estimating saliency, gaze) without considering the “when” (engagement). We propose a learning-based approach that uses long-term egomotion cues to detect engagement, specifically in browsing scenarios where one frequently takes in new visual information (e.g., shopping, touring). We introduce a large, richly annotated dataset for ego-engagement that is the first of its kind. Our approach outperforms a wide array of existing methods. We show engagement can be detected well independent of both scene appearance and the camera wearer’s identity.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_28');
INSERT INTO `paper` VALUES (10084, 'Distinct Class-Specific Saliency Maps for Weakly Supervised Semantic Segmentation', 'Semantic segmentation', 'Weakly supervised segmentation', 'Fully convolutional neural network', 'Fully connected CRF', '', 'In this paper, we deal with a weakly supervised semantic segmentation problem where only training images with image-level labels are available. We propose a weakly supervised semantic segmentation method which is based on CNN-based class-specific saliency maps and fully-connected CRF. To obtain distinct class-specific saliency maps which can be used as unary potentials of CRF, we propose a novel method to estimate class saliency maps which improves the method proposed by Simonyan et al. (2014) significantly by the following improvements: (1) using CNN derivatives with respect to feature maps of the intermediate convolutional layers with up-sampling instead of an input image; (2) subtracting the saliency maps of the other classes from the saliency maps of the target class to differentiate target objects from other objects; (3) aggregating multiple-scale class saliency maps to compensate lower resolution of the feature maps. After obtaining distinct class saliency maps, we apply fully-connected CRF by using the class maps as unary potentials. By the experiments, we show that the proposed method has outperformed state-of-the-art results with the PASCAL VOC 2012 dataset under the weakly-supervised setting.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_14');
INSERT INTO `paper` VALUES (10085, 'Distractor-Supported Single Target Tracking in Extremely Cluttered Scenes', 'Single Target Tracking', 'Proposed Tracker', 'Estimated Target Location', 'Foreground Samples', 'Juggling Sequence', 'This paper presents a novel method for single target tracking in RGB images under conditions of extreme clutter and camouflage, including frequent occlusions by objects with similar appearance as the target. In contrast to conventional single target trackers, which only maintain the estimated target status, we propose a multi-level clustering-based robust estimation for online detection and learning of multiple target-like regions, called distractors, when they appear near to the true target. To distinguish the target from these distractors, we exploit a global dynamic constraint (derived from the target and the distractors) in a feedback loop to improve single target tracking performance in situations where the target is camouflaged in highly cluttered scenes. Our proposed method successfully prevents the estimated target location from erroneously jumping to a distractor during occlusion or extreme camouflage interactions. To gain an insightful understanding of the evaluated trackers, we have augmented publicly available benchmark videos, by proposing a new set of clutter and camouflage sub-attributes, and annotating these sub-attributes for all frames in all sequences. Using this dataset, we first evaluate the effect of each key component of the tracker on the overall performance. Then, the proposed tracker is compared to other highly ranked single target tracking algorithms in the literature. The experimental results show that applying the proposed global dynamic constraint in a feedback loop can improve single target tracker performance, and demonstrate that the overall algorithm significantly outperforms other state-of-the-art single target trackers in highly cluttered scenes.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_8');
INSERT INTO `paper` VALUES (10086, 'Do We Really Need to Collect Millions of Faces for Effective Face Recognition?', 'Face Recognition', 'Face Image', 'Training Image', 'Convolutional Neural Network', 'Data Augmentation', 'Face recognition capabilities have recently made extraordinary leaps. Though this progress is at least partially due to ballooning training set sizes – huge numbers of face images downloaded and labeled for identity – it is not clear if the formidable task of collecting so many images is truly necessary. We propose a far more accessible means of increasing training data sizes for face recognition systems: Domain specific data augmentation. We describe novel methods of enriching an existing dataset with important facial appearance variations by manipulating the faces it contains. This synthesis is also used when matching query images represented by standard convolutional neural networks. The effect of training and testing with synthesized images is tested on the LFW and IJB-A (verification and identification) benchmarks and Janus CS2. The performances obtained by our approach match state of the art results reported by systems trained on millions of downloaded images.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_35');
INSERT INTO `paper` VALUES (10087, 'DOC: Deep OCclusion Estimation from a Single Image', 'Object Boundary', 'Markov Random Field', 'Boundary Detection', 'Deep Network', 'Semantic Segmentation', 'In this paper, we propose a deep convolutional network architecture, called DOC, which detects object boundaries and estimates the occlusion relationships (i.e. which side of the boundary is foreground and which is background). Specifically, we first represent occlusion relations by a binary edge indicator, to indicate the object boundary, and an occlusion orientation variable whose direction specifies the occlusion relationships by a left-hand rule, see Fig. 1. Then, our DOC networks exploit local and non-local image cues to learn and estimate this representation and hence recover occlusion relations. To train and test DOC, we construct a large-scale instance occlusion boundary dataset using PASCAL VOC images, which we call the PASCAL instance occlusion dataset (PIOD). It contains 10,000 images and hence is two orders of magnitude larger than existing occlusion datasets for outdoor images. We test two variants of DOC on PIOD and on the BSDS ownership dataset and show they outperform state-of-the-art methods typically by more than 5AP. Finally, we perform numerous experiments investigating multiple settings of DOC and transfer between BSDS and PIOD, which provides more insights for further study of occlusion estimation.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_33');
INSERT INTO `paper` VALUES (10088, 'Domain Adaptive Fisher Vector for Visual Recognition', 'Domain adaptation', 'Fisher vector', '', '', '', 'In this paper, we consider Fisher vector in the context of domain adaptation, which has rarely been discussed by the existing domain adaptation methods. Particularly, in many real scenarios, the distributions of Fisher vectors of the training samples (i.e., source domain) and test samples (i.e., target domain) are considerably different, which may degrade the classification performance on the target domain by using the classifiers/regressors learnt based on the training samples from the source domain. To address the domain shift issue, we propose a Domain Adaptive Fisher Vector (DAFV) method, which learns a transformation matrix to select the domain invariant components of Fisher vectors and simultaneously solves a regression problem for visual recognition tasks based on the transformed features. Specifically, we employ a group lasso based regularizer on the transformation matrix to select the components of Fisher vectors, and use a regularizer based on the Maximum Mean Discrepancy (MMD) criterion to reduce the data distribution mismatch of transformed features between the source domain and the target domain. Comprehensive experiments demonstrate the effectiveness of our DAFV method on two benchmark datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_33');
INSERT INTO `paper` VALUES (10089, 'Double-Opponent Vectorial Total Variation', 'Color Image', 'Color Space', 'Color Channel', 'Color Edge', 'Artificial Color', 'We present a new vectorial total variation (VTV) method that addresses the problem of color consistent image filtering. Our approach combines insights based on the double-opponent cell representation in the visual cortex with state-of-the-art variational modelling using VTV regularization. Existing methods of vectorial total variation regularizers have insufficient (even no) coupling between the color channels and thus may introduce color artifacts. We address this problem by introducing a novel color channel coupling inspired from a pullback-metric from an opponent space to the observation space. We show existence and uniqueness of a solution in the space of vectorial functions of bounded variation. In experiments, we demonstrate that our novel approach compares favorably to state-of-the-art methods w.r.t. to structure coherence and color consistency.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_40');
INSERT INTO `paper` VALUES (10090, 'Dual Structured Light 3D Using a 1D Sensor', 'Structured light', 'Dual photography', '', '', '', 'Structured light-based 3D reconstruction methods often illuminate a scene using patterns with 1D translational symmetry such as stripes, Gray codes or sinusoidal phase shifting patterns. These patterns are decoded using images captured by a traditional 2D sensor. In this work, we present a novel structured light approach that uses a 1D sensor with simple optics and no moving parts to reconstruct scenes with the same acquisition speed as a traditional 2D sensor. While traditional methods compute correspondences between columns of the projector and 2D camera pixels, our ‘dual’ approach computes correspondences between columns of the 1D camera and 2D projector pixels. The use of a 1D sensor provides significant advantages in many applications that operate in short-wave infrared range (0.9–2.5 microns) or require dynamic vision sensors (DVS), where a 2D sensor is prohibitively expensive and difficult to manufacture. We analyze the proposed design, explore hardware alternatives and discuss the performance in the presence of ambient light and global illumination.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_23');
INSERT INTO `paper` VALUES (10091, 'Efficient Continuous Relaxations for Dense CRF', 'Energy minimisation', 'Dense CRF', 'Inference', 'Linear programming', 'Quadratic programming', 'Dense conditional random fields (CRF) with Gaussian pairwise potentials have emerged as a popular framework for several computer vision applications such as stereo correspondence and semantic segmentation. By modeling long-range interactions, dense CRFs provide a more detailed labelling compared to their sparse counterparts. Variational inference in these dense models is performed using a filtering-based mean-field algorithm in order to obtain a fully-factorized distribution minimising the Kullback-Leibler divergence to the true distribution. In contrast to the continuous relaxation-based energy minimisation algorithms used for sparse CRFs, the mean-field algorithm fails to provide strong theoretical guarantees on the quality of its solutions. To address this deficiency, we show that it is possible to use the same filtering approach to speed-up the optimisation of several continuous relaxations. Specifically, we solve a convex quadratic programming (QP) relaxation using the efficient Frank-Wolfe algorithm. This also allows us to solve difference-of-convex relaxations via the iterative concave-convex procedure where each iteration requires solving a convex QP. Finally, we develop a novel divide-and-conquer method to compute the subgradients of a linear programming relaxation that provides the best theoretical bounds for energy minimisation. We demonstrate the advantage of continuous relaxations over the widely used mean-field algorithm on publicly available datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_50');
INSERT INTO `paper` VALUES (10092, 'Efficient Large Scale Image Classification via Prediction Score Decomposition', 'Large scale classification', 'Label tree', 'Matrix decomposition', '', '', 'There has been growing interest in reducing the test time complexity of multi-class classification problems with large numbers of classes. The key idea to solve it is to reduce the number of classifier evaluations used to predict labels. The state-of-the-art methods usually employ the label tree approach that usually suffers the well-know error propagation problem and it is difficult for parallelization for further speedup. We propose another practical approach, with the same goal of using a small number of classifiers to achieve a good trade-off between testing efficiency and classification accuracy. The proposed method analyzes the correlation among classes, suppresses redundancy, and generates a small number of classifiers that best approximate the prediction scores of the original large number of classes. Different from label-tree methods in which each test example follows a different traversing path from the root to a leaf node and results in a different set of classifiers each time, the proposed method applies the same set of classifiers to all test examples. As a result, it is much more efficient in practice, even in the case of using the same number of classifier evaluations as the label-tree methods. Experiments on several large datasets including ILSVRC2010-1K, SUN-397, and Caltech-256 show the efficiency of our method.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_46');
INSERT INTO `paper` VALUES (10093, 'Efficient Multi-frequency Phase Unwrapping Using Kernel Density Estimation', 'Time-of-flight', 'Kinect v2', 'Kernel-density-estimation', '', '', 'In this paper we introduce an efficient method to unwrap multi-frequency phase estimates for time-of-flight ranging. The algorithm generates multiple depth hypotheses and uses a spatial kernel density estimate (KDE) to rank them. The confidence produced by the KDE is also an effective means to detect outliers. We also introduce a new closed-form expression for phase noise prediction, that better fits real data. The method is applied to depth decoding for the Kinect v2 sensor, and compared to the Microsoft Kinect SDK and to the open source driver libfreenect2. The intended Kinect v2 use case is scenes with less than 8 m range, and for such cases we observe consistent improvements, while maintaining real-time performance. When extending the depth range to the maximal value of 18.75 m, we get about \\(52\\,\\%\\) more valid measurements than libfreenect2. The effect is that the sensor can now be used in large depth scenes, where it was previously not a good choice.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_11');
INSERT INTO `paper` VALUES (10094, 'Efficient Multi-view Surface Refinement with Adaptive Resolution Control', 'Performance Gain', 'Triangular Mesh', 'Normalize Cross Correlation', 'Refinement Algorithm', 'Reprojection Error', 'The existing stereo refinement methods optimize a surface representation using a multi-view photo-consistency functional. Such optimization is iterative and requires repeated computation of gradients over all surface regions, which is the bottleneck affecting adversely the computational efficiency of the refinement. In this paper, we present a flexible and efficient framework for mesh surface refinement in multi-view stereo. The newly proposed Adaptive Resolution Control (ARC) evaluates an optimal trade-off between the geometry accuracy and the performance via curve analysis. Then, it classifies the regions into the significant and insignificant ones using a graph-cut optimization. After that, each region is subdivided and simplified accordingly in the remaining refinement process, producing a triangular mesh in adaptive resolutions. Consequently, the ARC accelerates the stereo refinement by severalfold by culling out most insignificant regions, while still maintaining a similar level of geometry details that the state-of-the-art methods could achieve. We have implemented the ARC and demonstrated intensively on both public benchmarks and private datasets, which all confirm the effectiveness and the robustness of the ARC.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_21');
INSERT INTO `paper` VALUES (10095, 'Ego2Top: Matching Viewers in Egocentric and Top-View Videos', 'Egocentric vision', 'Surveillance', 'Spectral graph matching', 'Gist', 'Cross-domain image understanding', 'Egocentric cameras are becoming increasingly popular and provide us with large amounts of videos, captured from the first person perspective. At the same time, surveillance cameras and drones offer an abundance of visual information, often captured from top-view. Although these two sources of information have been separately studied in the past, they have not been collectively studied and related. Having a set of egocentric cameras and a top-view camera capturing the same area, we propose a framework to identify the egocentric viewers in the top-view video. We utilize two types of features for our assignment procedure. Unary features encode what a viewer (seen from top-view or recording an egocentric video) visually experiences over time. Pairwise features encode the relationship between the visual content of a pair of viewers. Modeling each view (egocentric or top) by a graph, the assignment process is formulated as spectral graph matching. Evaluating our method over a dataset of 50 top-view and 188 egocentric videos taken in different scenarios demonstrates the efficiency of the proposed approach in assigning egocentric viewers to identities present in top-view camera. We also study the effect of different parameters such as the number of egocentric viewers and visual features.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_16');
INSERT INTO `paper` VALUES (10096, 'Eigen Appearance Maps of Dynamic Shapes', 'Optical Flow', 'Input Texture', 'Appearance Variation', 'Texture Space', 'Projection Coefficient', 'We address the problem of building efficient appearance representations of shapes observed from multiple viewpoints and in several movements. Multi-view systems now allow the acquisition of spatio-temporal models of such moving objects. While efficient geometric representations for these models have been widely studied, appearance information, as provided by the observed images, is mainly considered on a per frame basis, and no global strategy yet addresses the case where several temporal sequences of a shape are available. We propose a per subject representation that builds on PCA to identify the underlying manifold structure of the appearance information relative to a shape. The resulting eigen representation encodes shape appearance variabilities due to viewpoint and motion, with Eigen textures, and due to local inaccuracies in the geometric model, with Eigen warps. In addition to providing compact representations, such decompositions also allow for appearance interpolation and appearance completion. We evaluate their performances over different characters and with respect to their ability to reproduce compelling appearances in a compact way.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_14');
INSERT INTO `paper` VALUES (10097, 'Embedding Deep Metric for Person Re-identification: A Study Against Large Variations', 'Person re-identification', 'Deep learning', 'CNN', '', '', 'Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)’s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive (i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_44');
INSERT INTO `paper` VALUES (10098, 'End-to-End Localization and Ranking for Relative Attributes', 'Relative attributes', 'Ranking', 'Localization', 'Discovery', '', 'We propose an end-to-end deep convolutional network to simultaneously localize and rank relative visual attributes, given only weakly-supervised pairwise image comparisons. Unlike previous methods, our network jointly learns the attribute’s features, localization, and ranker. The localization module of our network discovers the most informative image region for the attribute, which is then used by the ranking module to learn a ranking model of the attribute. Our end-to-end framework also significantly speeds up processing and is much faster than previous methods. We show state-of-the-art ranking results on various relative attribute datasets, and our qualitative localization results clearly demonstrate our network’s ability to learn meaningful image patches.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_45');
INSERT INTO `paper` VALUES (10099, 'Estimation of Human Body Shape in Motion with Wide Clothing', 'Human body modeling', 'Shape and motion estimation', 'Statistical shape space', '', '', 'Estimating 3D human body shape in motion from a sequence of unstructured oriented 3D point clouds is important for many applications. We propose the first automatic method to solve this problem that works in the presence of loose clothing. The problem is formulated as an optimization problem that solves for identity and posture parameters in a shape space capturing likely body shape variations. The automation is achieved by leveraging a recent robust pose detection method [1]. To account for clothing, we take advantage of motion cues by encouraging the estimated body shape to be inside the observations. The method is evaluated on a new benchmark containing different subjects, motions, and clothing styles that allows to quantitatively measure the accuracy of body shape estimates. Furthermore, we compare our results to existing methods that require manual input and demonstrate that results of similar visual quality can be obtained.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_27');
INSERT INTO `paper` VALUES (10100, 'Evaluation of LBP and Deep Texture Descriptors with a New Robustness Benchmark', 'Local binary pattern', 'Deep learning', 'Performance evaluation', 'Texture classification', '', 'In recent years, a wide variety of different texture descriptors has been proposed, including many LBP variants. New types of descriptors based on multistage convolutional networks and deep learning have also emerged. In different papers the performance comparison of the proposed methods to earlier approaches is mainly done with some well-known texture datasets, with differing classifiers and testing protocols, and often not using the best sets of parameter values and multiple scales for the comparative methods. Very important aspects such as computational complexity and effects of poor image quality are often neglected.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_5');
INSERT INTO `paper` VALUES (10101, 'Exploiting Semantic Information and Deep Matching for Optical Flow', 'Optical flow', 'Low-level vision', 'Deep learning', 'Autonomous driving', '', 'We tackle the problem of estimating optical flow from a monocular camera in the context of autonomous driving. We build on the observation that the scene is typically composed of a static background, as well as a relatively small number of traffic participants which move rigidly in 3D. We propose to estimate the traffic participants using instance-level segmentation. For each traffic participant, we use the epipolar constraints that govern each independent motion for faster and more accurate estimation. Our second contribution is a new convolutional net that learns to perform flow matching, and is able to estimate the uncertainty of its matches. This is a core element of our flow estimation pipeline. We demonstrate the effectiveness of our approach in the challenging KITTI 2015 flow benchmark, and show that our approach outperforms published approaches by a large margin.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_10');
INSERT INTO `paper` VALUES (10102, 'Face Detection with End-to-End Integration of a ConvNet and a 3D Model', 'Face detection', 'Face 3D model', 'ConvNet', 'Deep learning', 'Multi-task learning', 'This paper presents a method for face detection in the wild, which integrates a ConvNet and a 3D mean face model in an end-to-end multi-task discriminative learning framework. The 3D mean face model is predefined and fixed (e.g., we used the one provided in the AFLW dataset). The ConvNet consists of two components: (i) The face proposal component computes face bounding box proposals via estimating facial key-points and the 3D transformation (rotation and translation) parameters for each predicted key-point w.r.t. the 3D mean face model. (ii) The face verification component computes detection results by pruning and refining proposals based on facial key-points based configuration pooling. The proposed method addresses two issues in adapting state-of-the-art generic object detection ConvNets (e.g., faster R-CNN) for face detection: (i) One is to eliminate the heuristic design of predefined anchor boxes in the region proposals network (RPN) by exploiting a 3D mean face model. (ii) The other is to replace the generic RoI (Region-of-Interest) pooling layer with a configuration pooling layer to respect underlying object structures. The multi-task loss consists of three terms: the classification Softmax loss and the location smooth \\(l_1\\)-losses of both the facial key-points and the face bounding boxes. In experiments, our ConvNet is trained on the AFLW dataset only and tested on the FDDB benchmark with fine-tuning and on the AFW benchmark without fine-tuning. The proposed method obtains very competitive state-of-the-art performance in the two benchmarks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_26');
INSERT INTO `paper` VALUES (10103, 'Faceless Person Recognition: Privacy Implications in Social Media', 'Privacy', 'Person recognition', 'Social media', '', '', 'As we shift more of our lives into the virtual domain, the volume of data shared on the web keeps increasing and presents a threat to our privacy. This works contributes to the understanding of privacy implications of such data sharing by analysing how well people are recognisable in social media data. To facilitate a systematic study we define a number of scenarios considering factors such as how many heads of a person are tagged and if those heads are obfuscated or not. We propose a robust person recognition system that can handle large variations in pose and clothing, and can be trained with few training samples. Our results indicate that a handful of images is enough to threaten users’ privacy, even in the presence of obfuscation. We show detailed experimental results, and discuss their implications.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_2');
INSERT INTO `paper` VALUES (10104, 'Facilitating and Exploring Planar Homogeneous Texture for Indoor Scene Understanding', 'Homogeneous texture', 'Shape from texture', 'Planar rectification', 'Invariant detection', 'Indoor scene understanding', 'Indoor scenes tend to be abundant with planar homogeneous texture, manifesting as regularly repeating scene elements along a plane. In this work, we propose to exploit such structure to facilitate high-level scene understanding. By robustly fitting a texture projection model to optimal dominant frequency estimates in image patches, we arrive at a projective-invariant method to localize such semantically meaningful regions in multi-planar scenes. The recovered projective parameters also allow an affine-ambiguous rectification in real-world images marred with outliers, room clutter, and photometric severities. Qualitative and quantitative results show our method outperforms existing representative work for both rectification and detection. We then explore the potential of homogeneous texture for two indoor scene understanding tasks. In scenes where vanishing points cannot be reliably detected, or the Manhattan assumption is not satisfied, homogeneous texture detected by the proposed approach provides alternative cues to obtain an indoor scene geometric layout. Second, low-level feature descriptors extracted upon affine rectification of detected texture are found to be not only class-discriminative but also complementary to features without rectification, improving recognition performance on the MIT Indoor67 benchmark.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_3');
INSERT INTO `paper` VALUES (10105, 'Fashion Landmark Detection in the Wild', 'Clothes landmark detection', 'Cascaded deep convolutional neural networks', 'Attribute prediction', 'Clothes retrieval', '', 'Visual fashion analysis has attracted many attentions in the recent years. Previous work represented clothing regions by either bounding boxes or human joints. This work presents fashion landmark detection or fashion alignment, which is to predict the positions of functional key points defined on the fashion items, such as the corners of neckline, hemline, and cuff. To encourage future studies, we introduce a fashion landmark dataset (The dataset is available at http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html.) with over 120K images, where each image is labeled with eight landmarks. With this dataset, we study fashion alignment by cascading multiple convolutional neural networks in three stages. These stages gradually improve the accuracies of landmark predictions. Extensive experiments demonstrate the effectiveness of the proposed method, as well as its generalization ability to pose estimation. Fashion landmark is also compared to clothing bounding boxes and human joints in two applications, fashion attribute prediction and clothes retrieval, showing that fashion landmark is a more discriminative representation to understand fashion images.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_15');
INSERT INTO `paper` VALUES (10106, 'Fast 6D Pose Estimation from a Monocular Image Using Hierarchical Pose Trees', '6D pose estimation', 'Texture-less objects', 'Template matching', '', '', 'It has been shown that the template based approaches could quickly estimate 6D pose of texture-less objects from a monocular image. However, they tend to be slow when the number of templates amounts to tens of thousands for handling a wider range of 3D object pose. To alleviate this problem, we propose a novel image feature and a tree-structured model. Our proposed perspectively cumulated orientation feature (PCOF) is based on the orientation histograms extracted from randomly generated 2D projection images using 3D CAD data, and the template using PCOF explicitly handle a certain range of 3D object pose. The hierarchical pose trees (HPT) is built by clustering 3D object pose and reducing the resolutions of templates, and HPT accelerates 6D pose estimation based on a coarse-to-fine strategy with an image pyramid. In the experimental evaluation on our texture-less object dataset, the combination of PCOF and HPT showed higher accuracy and faster speed in comparison with state-of-the-art techniques.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_24');
INSERT INTO `paper` VALUES (10107, 'Fast Global Registration', 'Global Registration Algorithm', 'Local Reﬁnement Algorithm', 'Local Refinement Method', 'Synthetic Range Data', 'Candidate Correspondences', 'We present an algorithm for fast global registration of partially overlapping 3D surfaces. The algorithm operates on candidate matches that cover the surfaces. A single objective is optimized to align the surfaces and disable false matches. The objective is defined densely over the surfaces and the optimization achieves tight alignment with no initialization. No correspondence updates or closest-point queries are performed in the inner loop. An extension of the algorithm can perform joint global registration of many partially overlapping surfaces. Extensive experiments demonstrate that the presented approach matches or exceeds the accuracy of state-of-the-art global registration pipelines, while being at least an order of magnitude faster. Remarkably, the presented approach is also faster than local refinement algorithms such as ICP. It provides the accuracy achieved by well-initialized local refinement algorithms, without requiring an initialization and at lower computational cost.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_47');
INSERT INTO `paper` VALUES (10108, 'Fast Guided Global Interpolation for Depth and Motion', 'Image-guided interpolation', 'Depth upsampling', 'Optical flow', '', '', 'We study the problems of upsampling a low-resolution depth map and interpolating an initial set of sparse motion matches, with the guidance from a corresponding high-resolution color image. The common objective for both tasks is to densify a set of sparse data points, either regularly distributed or scattered, to a full image grid through a 2D guided interpolation process. We propose a unified approach that casts the fundamental guided interpolation problem into a hierarchical, global optimization framework. Built on a weighted least squares (WLS) formulation with its recent fast solver – fast global smoothing (FGS) technique, our method progressively densifies the input data set by efficiently performing the cascaded, global interpolation (or smoothing) with alternating guidances. Our cascaded scheme effectively addresses the potential structure inconsistency between the sparse input data and the guidance image, while preserving depth or motion boundaries. To prevent new data points of low confidence from contaminating the next interpolation process, we also prudently evaluate the consensus of the interpolated intermediate data. Experiments show that our general interpolation approach successfully tackles several notorious challenges. Our method achieves quantitatively competitive results on various benchmark evaluations, while running much faster than other competing methods designed specifically for either depth upsampling or motion interpolation.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_44');
INSERT INTO `paper` VALUES (10109, 'Fast Optical Flow Using Dense Inverse Search', 'Optical Flow', 'Large Displacement', 'Coarse Scale', 'Disk Access', 'Image Pyramid', 'Most recent works in optical flow extraction focus on the accuracy and neglect the time complexity. However, in real-life visual applications, such as tracking, activity detection and recognition, the time complexity is critical. We propose a solution with very low time complexity and competitive accuracy for the computation of dense optical flow. It consists of three parts: (1) inverse search for patch correspondences; (2) dense displacement field creation through patch aggregation along multiple scales; (3) variational refinement. At the core of our Dense Inverse Search-based method (DIS) is the efficient search of correspondences inspired by the inverse compositional image alignment proposed by Baker and Matthews (2001, 2004). DIS is competitive on standard optical flow benchmarks. DIS runs at 300 Hz up to 600 Hz on a single CPU core (1024 \\(\\times \\) 436 resolution. 42 Hz/46 Hz when including preprocessing: disk access, image re-scaling, gradient computation. More details in Sect. 3.1.), reaching the temporal resolution of human’s biological vision system. It is order(s) of magnitude faster than state-of-the-art methods in the same range of accuracy, making DIS ideal for real-time applications.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_29');
INSERT INTO `paper` VALUES (10110, 'Fine-Grained Material Classification Using Micro-geometry and Reflectance', 'Material classification', 'Micro-geometry', 'Reflectance', 'Photometric stereo', '', 'In this paper we focus on an understudied computer vision problem, particularly how the micro-geometry and the reflectance of a surface can be used to infer its material. To this end, we introduce a new, publicly available database for fine-grained material classification, consisting of over 2000 surfaces of fabrics (http://ibug.doc.ic.ac.uk/resources/fabrics.). The database has been collected using a custom-made portable but cheap and easy to assemble photometric stereo sensor. We use the normal map and the albedo of each surface to recognize its material via the use of handcrafted and learned features and various feature encodings. We also perform garment classification using the same approach. We show that the fusion of normals and albedo information outperforms standard methods which rely only on the use of texture information. Our methodologies, both for data collection, as well as for material classification can be applied easily to many real-word scenarios including design of new robots able to sense materials and industrial inspection.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_47');
INSERT INTO `paper` VALUES (10111, 'Fine-Scale Surface Normal Estimation Using a Single NIR Image', 'Shape from shading', 'Near infrared image', 'Generative adversarial network', '', '', 'We present surface normal estimation using a single near infrared (NIR) image. We are focusing on reconstructing fine-scale surface geometry using an image captured with an uncalibrated light source. To tackle this ill-posed problem, we adopt a generative adversarial network, which is effective in recovering sharp outputs essential for fine-scale surface normal estimation. We incorporate the angular error and an integrability constraint into the objective function of the network to make the estimated normals incorporate physical characteristics. We train and validate our network on a recent NIR dataset, and also evaluate the generality of our trained model by using new external datasets that are captured with a different camera under different environments.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_30');
INSERT INTO `paper` VALUES (10112, 'Focal Flow: Measuring Distance and Velocity with Defocus and Differential Motion', 'Optical Flow', 'Image Patch', 'Pinhole Camera', 'Differential Motion', 'Sensor Distance', 'We present the focal flow sensor. It is an unactuated, monocular camera that simultaneously exploits defocus and differential motion to measure a depth map and a 3D scene velocity field. It does so using an optical-flow-like, per-pixel linear constraint that relates image derivatives to depth and velocity. We derive this constraint, prove its invariance to scene texture, and prove that it is exactly satisfied only when the sensor’s blur kernels are Gaussian. We analyze the inherent sensitivity of the ideal focal flow sensor, and we build and test a prototype. Experiments produce useful depth and velocity information for a broader set of aperture configurations, including a simple lens with a pillbox aperture.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_41');
INSERT INTO `paper` VALUES (10113, 'Foreground Segmentation via Dynamic Tree-Structured Sparse RPCA', 'Background Model', 'Foreground Object', 'Foreground Region', 'Robust Principal Component Analysis', 'Foreground Segmentation', 'Video analysis often begins with background subtraction which consists of creation of a background model, followed by a regularization scheme. Recent evaluation of representative background subtraction techniques demonstrated that there are still considerable challenges facing these methods. We present a new method in which we regard the image sequence as being made up of the sum of a low-rank background matrix and a dynamic tree-structured sparse outlier matrix and solve the decomposition using our approximated Robust Principal Component Analysis method extended to handle camera motion. Our contribution lies in dynamically estimating the support of the foreground regions via a superpixel generation step, so as to impose spatial coherence on these regions, and to obtain crisp and meaningful foreground regions. These advantages enable our method to outperform state-of-the-art alternatives in three benchmark datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_19');
INSERT INTO `paper` VALUES (10114, 'Friction from Reflectance: Deep Reflectance Codes for Predicting Physical Surface Properties from One-Shot In-Field Reflectance', 'Surface friction', 'Reflectance', 'Material recognition', 'Binary embedding', '', 'Images are the standard input for vision algorithms, but one-shot in-field reflectance measurements are creating new opportunities for recognition and scene understanding. In this work, we address the question of what reflectance can reveal about materials in an efficient manner. We go beyond the question of recognition and labeling and ask the question: What intrinsic physical properties of the surface can be estimated using reflectance? We introduce a framework that enables prediction of actual friction values for surfaces using one-shot reflectance measurements. This work is a first of its kind vision-based friction estimation. We develop a novel representation for reflectance disks that capture partial BRDF measurements instantaneously. Our method of deep reflectance codes combines CNN features and fisher vector pooling with optimal binary embedding to create codes that have sufficient discriminatory power and have important properties of illumination and spatial invariance. The experimental results demonstrate that reflectance can play a new role in deciphering the underlying physical properties of real-world scenes.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_49');
INSERT INTO `paper` VALUES (10115, 'From Multiview Image Curves to 3D Drawings', 'Multiview stereo', '3D reconstruction', '3D curve networks', 'Junctions', '', 'Reconstructing 3D scenes from multiple views has made impressive strides in recent years, chiefly by correlating isolated feature points, intensity patterns, or curvilinear structures. In the general setting – without controlled acquisition, abundant texture, curves and surfaces following specific models or limiting scene complexity – most methods produce unorganized point clouds, meshes, or voxel representations, with some exceptions producing unorganized clouds of 3D curve fragments. Ideally, many applications require structured representations of curves, surfaces and their spatial relationships. This paper presents a step in this direction by formulating an approach that combines 2D image curves into a collection of 3D curves, with topological connectivity between them represented as a 3D graph. This results in a 3D drawing, which is complementary to surface representations in the same sense as a 3D scaffold complements a tent taut over it. We evaluate our results against truth on synthetic and real datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_5');
INSERT INTO `paper` VALUES (10116, 'Fundamental Matrices from Moving Objects Using Line Motion Barcodes', 'Fundamental matrix', 'Epipolar geometry', 'Motion barcodes', 'Epipolar lines', 'Multi-camera calibration', 'Computing the epipolar geometry between cameras with very different viewpoints is often very difficult. The appearance of objects can vary greatly, and it is difficult to find corresponding feature points. Prior methods searched for corresponding epipolar lines using points on the convex hull of the silhouette of a single moving object. These methods fail when the scene includes multiple moving objects. This paper extends previous work to scenes having multiple moving objects by using the “Motion Barcodes”, a temporal signature of lines. Corresponding epipolar lines have similar motion barcodes, and candidate pairs of corresponding epipoar lines are found by the similarity of their motion barcodes. As in previous methods we assume that cameras are relatively stationary and that moving objects have already been extracted using background subtraction.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_14');
INSERT INTO `paper` VALUES (10117, 'Gaussian Process Density Counting from Weak Supervision', 'Random Forest', 'Gaussian Process', 'Latent Function', 'Count Density', 'Multiple Instance Learning', 'As a novel learning setup, we introduce learning to count objects within an image from only region-level count information. This level of supervision is weaker than earlier approaches that require segmenting, drawing bounding boxes, or putting dots on centroids of all objects within training images. We devise a weakly supervised kernel learner that achieves higher count accuracies than previous counting models. We achieve this by placing a Gaussian process prior on a latent function the square of which is the count density. We impose non-negativeness and smooth the GP response as an intermediary step in model inference. We illustrate the effectiveness of our model on two benchmark applications: (i) synthetic cell and (ii) pedestrian counting, and one novel application: (iii) erythrocyte counting on blood samples of malaria patients.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_22');
INSERT INTO `paper` VALUES (10118, 'General Automatic Human Shape and Motion Capture Using Volumetric Contour Cues', 'Motion Capture', 'Body Model', 'Bone Length', 'Mesh Vertex', 'Joint Location', 'Markerless motion capture algorithms require a 3D body with properly personalized skeleton dimension and/or body shape and appearance to successfully track a person. Unfortunately, many tracking methods consider model personalization a different problem and use manual or semi-automatic model initialization, which greatly reduces applicability. In this paper, we propose a fully automatic algorithm that jointly creates a rigged actor model commonly used for animation – skeleton, volumetric shape, appearance, and optionally a body surface – and estimates the actor’s motion from multi-view video input only. The approach is rigorously designed to work on footage of general outdoor scenes recorded with very few cameras and without background subtraction. Our method uses a new image formation model with analytic visibility and analytically differentiable alignment energy. For reconstruction, 3D body shape is approximated as a Gaussian density field. For pose and shape estimation, we minimize a new edge-based alignment energy inspired by volume ray casting in an absorbing medium. We further propose a new statistical human body model that represents the body surface, volumetric Gaussian density, and variability in skeleton shape. Given any multi-view sequence, our method jointly optimizes the pose and shape parameters of this model fully automatically in a spatiotemporal way.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_31');
INSERT INTO `paper` VALUES (10119, 'Generating Visual Explanations', 'Visual explanation', 'Image description', 'Language and vision', '', '', 'Clearly explaining a rationale for a classification decision to an end user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. Through a novel loss function based on sampling and reinforcement learning, our model learns to generate sentences that realize a global sentence property, such as class specificity. Our results on the CUB dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_1');
INSERT INTO `paper` VALUES (10120, 'Generative Image Modeling Using Style and Structure Adversarial Networks', 'Object Detection', 'Unsupervised Learning', 'Scene Classification', 'Restrict Boltzmann Machine', 'Indoor Scene', 'Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (\\({\\text {S}^2}\\)-GAN). Our \\({\\text {S}^2}\\)-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our \\({\\text {S}^2}\\)-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_20');
INSERT INTO `paper` VALUES (10121, 'Generative Visual Manipulation on the Natural Image Manifold', 'Reconstruction Error', 'Natural Image', 'Deep Neural Network', 'Editing Operation', 'Image Editing', 'Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to “fall off” the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user’s scribbles.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_36');
INSERT INTO `paper` VALUES (10122, 'Generic 3D Representation via Pose Estimation and Matching', 'Generic vision', 'Representation', 'Descriptor learning', 'Pose estimation', 'Wide-baseline matching', 'Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross modality pose estimation).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_33');
INSERT INTO `paper` VALUES (10123, 'Geometric Neural Phrase Pooling: Modeling the Spatial Co-occurrence of Neurons', 'Image classification', 'Convolutional Neural Networks', 'Spatial co-occurrence of neurons', 'Geometric Neural Phrase Pooling', '', 'Deep Convolutional Neural Networks (CNNs) are playing important roles in state-of-the-art visual recognition. This paper focuses on modeling the spatial co-occurrence of neuron responses, which is less studied in the previous work. For this, we consider the neurons in the hidden layer as neural words, and construct a set of geometric neural phrases on top of them. The idea that grouping neural words into neural phrases is borrowed from the Bag-of-Visual-Words (BoVW) model. Next, the Geometric Neural Phrase Pooling (GNPP) algorithm is proposed to efficiently encode these neural phrases. GNPP acts as a new type of hidden layer, which punishes the isolated neuron responses after convolution, and can be inserted into a CNN model with little extra computational overhead. Experimental results show that GNPP produces significant and consistent accuracy gain in image classification.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_39');
INSERT INTO `paper` VALUES (10124, 'Global Registration of 3D Point Sets via LRS Decomposition', 'Point-set registration', 'Motion synchronization', 'Matrix completion', 'Low-rank and sparse matrix decomposition', '', 'This paper casts the global registration of multiple 3D point-sets into a low-rank and sparse decomposition problem. This neat mathematical formulation caters for missing data, outliers and noise, and it benefits from a wealth of available decomposition algorithms that can be plugged-in. Experimental results show that this approach compares favourably to the state of the art in terms of precision and speed, and it outperforms all the analysed techniques as for robustness to outliers.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_30');
INSERT INTO `paper` VALUES (10125, 'Globally Continuous and Non-Markovian Crowd Activity Analysis from Videos', 'Gaussian Mixture Model', 'Gibbs Sampling', 'Anomaly Detection', 'Time Topic', 'Spatial Activity', 'Automatically recognizing activities in video is a classic problem in vision and helps to understand behaviors, describe scenes and detect anomalies. We propose an unsupervised method for such purposes. Given video data, we discover recurring activity patterns that appear, peak, wane and disappear over time. By using non-parametric Bayesian methods, we learn coupled spatial and temporal patterns with minimum prior knowledge. To model the temporal changes of patterns, previous works compute Markovian progressions or locally continuous motifs whereas we model time in a globally continuous and non-Markovian way. Visually, the patterns depict flows of major activities. Temporally, each pattern has its own unique appearance-disappearance cycles. To compute compact pattern representations, we also propose a hybrid sampling method. By combining these patterns with detailed environment information, we interpret the semantics of activities and report anomalies. Also, our method fits data better and detects anomalies that were difficult to detect previously.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_32');
INSERT INTO `paper` VALUES (10126, 'Going Further with Point Pair Features', 'Feature Point Pairs (PPFs)', 'Sensor Noise', 'Background Clutter', 'Scene Point', 'Occlusion Dataset', 'Point Pair Features is a widely used method to detect 3D objects in point clouds, however they are prone to fail in presence of sensor noise and background clutter. We introduce novel sampling and voting schemes that significantly reduces the influence of clutter and sensor noise. Our experiments show that with our improvements, PPFs become competitive against state-of-the-art methods as it outperforms them on several objects from challenging benchmarks, at a low computational cost.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_51');
INSERT INTO `paper` VALUES (10127, 'Graph-Based Consistent Matching for Structure-from-Motion', 'Structure-from-Motion', 'Image matching', 'Loop consistency', '', '', 'Pairwise image matching of unordered image collections greatly affects the efficiency and accuracy of Structure-from-Motion (SfM). Insufficient match pairs may result in disconnected structures or incomplete components, while costly redundant pairs containing erroneous ones may lead to folded and superimposed structures. This paper presents a graph-based image matching method that tackles the issues of completeness, efficiency and consistency in a unified framework. Our approach starts by chaining all but singleton images using a visual-similarity-based minimum spanning tree. Then the minimum spanning tree is incrementally expanded to form locally consistent strong triplets. Finally, a global community-based graph algorithm is introduced to strengthen the global consistency by reinforcing potentially large connected components. We demonstrate the superior performance of our method in terms of accuracy and efficiency on both benchmark and Internet datasets. Our method also performs remarkably well on the challenging datasets of highly ambiguous and duplicated scenes.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_9');
INSERT INTO `paper` VALUES (10128, 'Grid Loss: Detecting Occluded Faces', 'Object detection', 'CNN', 'Face detection', '', '', 'Detection of partially occluded objects is a challenging computer vision problem. Standard Convolutional Neural Network (CNN) detectors fail if parts of the detection window are occluded, since not every sub-part of the window is discriminative on its own. To address this issue, we propose a novel loss layer for CNNs, named grid loss, which minimizes the error rate on sub-blocks of a convolution layer independently rather than over the whole feature map. This results in parts being more discriminative on their own, enabling the detector to recover if the detection window is partially occluded. By mapping our loss layer back to a regular fully connected layer, no additional computational cost is incurred at runtime compared to standard CNNs. We demonstrate our method for face detection on several public face detection benchmarks and show that our method outperforms regular CNNs, is suitable for realtime applications and achieves state-of-the-art performance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_24');
INSERT INTO `paper` VALUES (10129, 'Grounding of Textual Phrases in Images by Reconstruction', 'Text Phrases', 'Ground Supervision', 'Box Proposals', 'Intersection Over Union (IOU)', 'Ground Truth Box', 'Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_49');
INSERT INTO `paper` VALUES (10130, 'Hand Pose Estimation from Local Surface Normals', 'Point Cloud', 'Random Forest', 'Rigid Transformation', 'Finger Joint', 'Viewpoint Change', 'We present a hierarchical regression framework for estimating hand joint positions from single depth images based on local surface normals. The hierarchical regression follows the tree structured topology of hand from wrist to finger tips. We propose a conditional regression forest, i.e. the Frame Conditioned Regression Forest (FCRF) which uses a new normal difference feature. At each stage of the regression, the frame of reference is established from either the local surface normal or previously estimated hand joints. By making the regression with respect to the local frame, the pose estimation is more robust to rigid transformations. We also introduce a new efficient approximation to estimate surface normals. We verify the effectiveness of our method by conducting experiments on two challenging real-world datasets and show consistent improvements over previous discriminative pose estimation methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_34');
INSERT INTO `paper` VALUES (10131, 'Head Reconstruction from Internet Photos', 'Internet photo collections', 'Head modeling', 'In the wild', 'Unconstrained 3D reconstruction', 'Uncalibrated', '3D face reconstruction from Internet photos has recently produced exciting results. A person’s face, e.g., Tom Hanks, can be modeled and animated in 3D from a completely uncalibrated photo collection. Most methods, however, focus solely on face area and mask out the rest of the head. This paper proposes that head modeling from the Internet is a problem we can solve. We target reconstruction of the rough shape of the head. Our method is to gradually “grow” the head mesh starting from the frontal face and extending to the rest of views using photometric stereo constraints. We call our method boundary-value growing algorithm. Results on photos of celebrities downloaded from the Internet are presented.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_23');
INSERT INTO `paper` VALUES (10132, 'HFS: Hierarchical Feature Selection for Efficient Image Segmentation', 'Image segmentation', 'Superpixel', 'Grouping', '', '', 'In this paper, we propose a real-time system, Hierarchical Feature Selection (HFS), that performs image segmentation at a speed of 50 frames-per-second. We make an attempt to improve the performance of previous image segmentation systems by focusing on two aspects: (1) a careful system implementation on modern GPUs for efficient feature computation; and (2) an effective hierarchical feature selection and fusion strategy with learning. Compared with classic segmentation algorithms, our system demonstrates its particular advantage in speed, with comparable results in segmentation quality. Adopting HFS in applications like salient object detection and object proposal generation results in a significant performance boost. Our proposed HFS system (will be open-sourced) can be used in a variety computer vision tasks that are built on top of image segmentation and superpixel extraction.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_53');
INSERT INTO `paper` VALUES (10133, 'Hierarchical Beta Process with Gaussian Process Prior for Hyperspectral Image Super Resolution', 'Hyperspectral', 'Super-resolution', 'Beta/Gaussian Process', '', '', 'Hyperspectral cameras acquire precise spectral information, however, their resolution is very low due to hardware constraints. We propose an image fusion based hyperspectral super resolution approach that employes a Bayesian representation model. The proposed model accounts for spectral smoothness and spatial consistency of the representation by using Gaussian Processes and a spatial kernel in a hierarchical formulation of the Beta Process. The model is employed by our approach to first infer Gaussian Processes for the spectra present in the hyperspectral image. Then, it is used to estimate the activity level of the inferred processes in a sparse representation of a high resolution image of the same scene. Finally, we use the model to compute multiple sparse codes of the high resolution image, that are merged with the samples of the Gaussian Processes for an accurate estimate of the high resolution hyperspectral image. We perform experiments with remotely sensed and ground-based hyperspectral images to establish the effectiveness of our approach.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_7');
INSERT INTO `paper` VALUES (10134, 'Hierarchical Dynamic Parsing and Encoding for Action Recognition', 'Action recognition', 'Hierarchical modeling', 'Dynamic encoding', '', '', 'A video action generally exhibits quite complex rhythms and non-stationary dynamics. To model such non-uniform dynamics, this paper describes a novel hierarchical dynamic encoding method to capture both the locally smooth dynamics and globally drastic dynamic changes. It provides a multi-layer joint representation for temporal modeling for action recognition. At the first layer, the action sequence is parsed in an unsupervised manner into several smooth-changing stages corresponding to different key poses or temporal structures. The dynamics within each stage are encoded by mean-pooling or learning to rank based encoding. At the second layer, the temporal information of the ordered dynamics extracted from the previous layer is encoded again to form the overall representation. Extensive experiments on a gesture action dataset (Chalearn) and several generic action datasets (Olympic Sports and Hollywood2) have demonstrated the effectiveness of the proposed method.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_13');
INSERT INTO `paper` VALUES (10135, 'Higher Order Conditional Random Fields in Deep Neural Networks', 'Semantic segmentation', 'Conditional random fields', 'Deep learning', 'Convolutional Neural Networks', '', 'We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image’s visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_33');
INSERT INTO `paper` VALUES (10136, 'Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding', 'Pointwise Mutual Information', 'Video Dataset', 'Amazon Mechanical Turk', 'Video Description', 'Script Generation', 'Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 s, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_31');
INSERT INTO `paper` VALUES (10137, 'HouseCraft: Building Houses from Rental Ads and Street Views', '3D reconstruction', '3D scene understanding', 'Localization', '', '', 'In this paper, we utilize rental ads to create realistic textured 3D models of building exteriors. In particular, we exploit the address of the property and its floorplan, which are typically available in the ad. The address allows us to extract Google StreetView images around the building, while the building’s floorplan allows for an efficient parametrization of the building in 3D via a small set of random variables. We propose an energy minimization framework which jointly reasons about the height of each floor, the vertical positions of windows and doors, as well as the precise location of the building in the world’s map, by exploiting several geometric and semantic cues from the StreetView imagery. To demonstrate the effectiveness of our approach, we collected a new dataset with 174 houses by crawling a popular rental website. Our experiments show that our approach is able to precisely estimate the geometry and location of the property, and can create realistic 3D building models.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_30');
INSERT INTO `paper` VALUES (10138, 'Human Attribute Recognition by Deep Hierarchical Contexts', 'Average Precision', 'Convolutional Neural Network', 'Human Attribute', 'Target Person', 'Scene Classification', 'We present an approach for recognizing human attributes in unconstrained settings. We train a Convolutional Neural Network (CNN) to select the most attribute-descriptive human parts from all poselet detections, and combine them with the whole body as a pose-normalized deep representation. We further improve by using deep hierarchical contexts ranging from human-centric level to scene level. Human-centric context captures human relations, which we compute from the nearest neighbor parts of other people on a pyramid of CNN feature maps. The matched parts are then average pooled and they act as a similarity regularization. To utilize the scene context, we re-score human-centric predictions by the global scene classification score jointly learned in our CNN, yielding final scene-aware predictions. To facilitate our study, a large-scale WIDER Attribute dataset(Dataset URL: http://mmlab.ie.cuhk.edu.hk/projects/WIDERAttribute) is introduced with human attribute and image event annotations, and our method surpasses competitive baselines on this dataset and other popular ones.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_41');
INSERT INTO `paper` VALUES (10139, 'Human Pose Estimation Using Deep Consensus Voting', 'Image Patch', 'Vote Scheme', 'Convolutional Layer', 'Patch Center', 'Pictorial Structure', 'In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_16');
INSERT INTO `paper` VALUES (10140, 'Human Re-identification in Crowd Videos Using Personal, Social and Environmental Constraints', 'Video surveillance', 'Re-identification', 'Dense crowds', 'Social constraints', 'Multiple cameras', 'This paper addresses the problem of human re-identification in videos of dense crowds. Re-identification in crowded scenes is a challenging problem due to large number of people and frequent occlusions, coupled with changes in their appearance due to different properties and exposure of cameras. To solve this problem, we model multiple Personal, Social and Environmental (PSE) constraints on human motion across cameras in crowded scenes. The personal constraints include appearance and preferred speed of each individual, while the social influences are modeled by grouping and collision avoidance. Finally, the environmental constraints model the transition probabilities between gates (entrances/exits). We incorporate these constraints into an energy minimization for solving human re-identification. Assigning 1–1 correspondence while modeling PSE constraints is NP-hard. We optimize using a greedy local neighborhood search algorithm to restrict the search space of hypotheses. We evaluated the proposed approach on several thousand frames of PRID and Grand Central datasets, and obtained significantly better results compared to existing methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_8');
INSERT INTO `paper` VALUES (10141, 'Human-in-the-Loop Person Re-identification', 'Person re-identification', 'Incremental learning', 'Human-in-the-loop', 'Metric ensemble', '', 'Current person re-identification (re-id) methods assume that (1) pre-labelled training data are available for every camera pair, (2) the gallery size for re-identification is moderate. Both assumptions scale poorly to real-world applications when camera network size increases and gallery size becomes large. Human verification of automatic model ranked re-id results becomes inevitable. In this work, a novel human-in-the-loop re-id model based on Human Verification Incremental Learning (HVIL) is formulated which does not require any pre-labelled training data to learn a model, therefore readily scalable to new camera pairs. This HVIL model learns cumulatively from human feedback to provide instant improvement to re-id ranking of each probe on-the-fly enabling the model scalable to large gallery sizes. We further formulate a Regularised Metric Ensemble Learning (RMEL) model to combine a series of incrementally learned HVIL models into a single ensemble model to be used when human feedback becomes unavailable.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_25');
INSERT INTO `paper` VALUES (10142, 'Identity Mappings in Deep Residual Networks', 'Identity Mapping', 'Training Error', 'Residual Function', 'Grey Arrow', 'Residual Unit', 'Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_38');
INSERT INTO `paper` VALUES (10143, 'Image Co-localization by Mimicking a Good Detector’s Confidence Score Distribution', 'Image co-localization', 'Unsupervised object discovery', '', '', '', 'Given a set of images containing objects from the same category, the task of image co-localization is to identify and localize each instance. This paper shows that this problem can be solved by a simple but intriguing idea, that is, a common object detector can be learnt by making its detection confidence scores distributed like those of a strongly supervised detector. More specifically, we observe that given a set of object proposals extracted from an image that contains the object of interest, an accurate strongly supervised object detector should give high scores to only a small minority of proposals, and low scores to most of them. Thus, we devise an entropy-based objective function to enforce the above property when learning the common object detector. Once the detector is learnt, we resort to a segmentation approach to refine the localization. We show that despite its simplicity, our approach outperforms state-of-the-arts.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_2');
INSERT INTO `paper` VALUES (10144, 'Image Co-segmentation Using Maximum Common Subgraph Matching and Region Co-growing', 'Maximum common subgraph', 'Region co-growing', '', '', '', 'We propose a computationally efficient graph based image co-segmentation algorithm where we extract objects with similar features from an image pair or a set of images. First we build a region adjacency graph (RAG) for each image by representing image superpixels as nodes. Then we compute the maximum common subgraph (MCS) between the RAGs using the minimum vertex cover of a product graph obtained from the RAG. Next using MCS outputs as the seeds, we iteratively co-grow the matched regions obtained from the MCS in each of the constituent images by using a weighted measure of inter-image feature similarities among the already matched regions and their neighbors that have not been matched yet. Upon convergence, we obtain the co-segmented objects. The MCS based algorithm allows multiple, similar objects to be co-segmented and the region co-growing stage helps to extract different sized, similar objects. Superiority of the proposed method is demonstrated by processing images containing different sized objects and multiple objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_44');
INSERT INTO `paper` VALUES (10145, 'Image Quality Assessment Using Similar Scene as Reference', 'Image Quality Assessment', 'Similar scene referenced image', 'Structural similarity', '“Naturalness”', 'Dual-path Deep Convolution Neural Network', 'Most of Image Quality Assessment (IQA) methods require the reference image to be pixel-wise aligned with the distorted image, and thus limiting the application of reference image based IQA methods. In this paper, we show that non-aligned image with similar scene could be well used for reference, using a proposed Dual-path deep Convolutional Neural Network (DCNN). Analysis indicates that the model captures the scene structural information and non-structural information “naturalness” between the pair for quality assessment. As shown in the experiments, our proposed DCNN model handles the IQA problem well. With an aligned reference image, our predictions outperform many state-of-the-art methods. And in more general case where the reference image contains the similar scene but is not aligned with the distorted one, DCNN could still achieve superior consistency with subjective evaluation than many existing methods that even use aligned reference images.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_1');
INSERT INTO `paper` VALUES (10146, 'Improving Multi-label Learning with Missing Labels by Structured Semantic Correlations', 'Semantic Representation', 'Semantic Concept', 'Semantic Space', 'Semantic Descriptor', 'Matrix Completion', 'Multi-label learning has attracted significant interests in computer vision recently, finding applications in many vision tasks such as multiple object recognition and automatic image annotation. Associating multiple labels to a complex image is very difficult, not only due to the intricacy of describing the image, but also because of the incompleteness nature of the observed labels. Existing works on the problem either ignore the label-label and instance-instance correlations or just assume these correlations are linear and unstructured. Considering that semantic correlations between images are actually structured, in this paper we propose to incorporate structured semantic correlations to solve the missing label problem of multi-label learning. Specifically, we project images to the semantic space with an effective semantic descriptor. A semantic graph is then constructed on these images to capture the structured correlations between them. We utilize the semantic graph Laplacian as a smooth term in the multi-label learning formulation to incorporate the structured semantic correlations. Experimental results demonstrate the effectiveness of the proposed semantic descriptor and the usefulness of incorporating the structured semantic correlations. We achieve better results than state-of-the-art multi-label learning methods on four benchmark datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_50');
INSERT INTO `paper` VALUES (10147, 'Improving Semantic Embedding Consistency by Metric Learning for Zero-Shot Classiffication', 'Zero-shot learning', 'Attributes', 'Semantic embedding', '', '', 'This paper addresses the task of zero-shot image classification. The key contribution of the proposed approach is to control the semantic embedding of images – one of the main ingredients of zero-shot learning – by formulating it as a metric learning problem. The optimized empirical criterion associates two types of sub-task constraints: metric discriminating capacity and accurate attribute prediction. This results in a novel expression of zero-shot learning not requiring the notion of class in the training phase: only pairs of image/attributes, augmented with a consistency indicator, are given as ground truth. At test time, the learned model can predict the consistency of a test image with a given set of attributes, allowing flexible ways to produce recognition inferences. Despite its simplicity, the proposed approach gives state-of-the-art results on four challenging datasets used for zero-shot recognition evaluation.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_44');
INSERT INTO `paper` VALUES (10148, 'Individualness and Determinantal Point Processes for Pedestrian Detection', 'Determinantal point process', 'Individualness', 'Object detection', 'Pedestrian detection', '', 'In this paper, we introduce individualness of detection candidates as a complement to objectness for pedestrian detection. The individualness assigns a single detection for each object out of raw detection candidates given by either object proposals or sliding windows. We show that conventional approaches, such as non-maximum suppression, are sub-optimal since they suppress nearby detections using only detection scores. We use a determinantal point process combined with the individualness to optimally select final detections. It models each detection using its quality and similarity to other detections based on the individualness. Then, detections with high detection scores and low correlations are selected by measuring their probability using a determinant of a matrix, which is composed of quality terms on the diagonal entries and similarities on the off-diagonal entries. For concreteness, we focus on the pedestrian detection problem as it is one of the most challenging problems due to frequent occlusions and unpredictable human motions. Experimental results demonstrate that the proposed algorithm works favorably against existing methods, including non-maximal suppression and a quadratic unconstrained binary optimization based method.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_20');
INSERT INTO `paper` VALUES (10149, 'Indoor-Outdoor 3D Reconstruction Alignment', 'Window Detection', 'Outdoor Scene', 'Model Alignment', 'Voxel Grid', 'Common Reference Frame', 'Structure-from-Motion can achieve accurate reconstructions of urban scenes. However, reconstructing the inside and the outside of a building into a single model is very challenging due to the lack of visual overlap and the change of lighting conditions between the two scenes. We propose a solution to align disconnected indoor and outdoor models of the same building into a single 3D model. Our approach leverages semantic information, specifically window detections, in multiple scenes to obtain candidate matches from which an alignment hypothesis can be computed. To determine the best alignment, we propose a novel cost function that takes both the number of window matches and the intersection of the aligned models into account. We evaluate our solution on multiple challenging datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_18');
INSERT INTO `paper` VALUES (10150, 'Instance-Sensitive Fully Convolutional Networks', 'Object Instance', 'Convolutional Layer', 'Local Coherence', 'Output Pixel', 'Image Coordinate System', 'Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_32');
INSERT INTO `paper` VALUES (10151, 'Interpreting the Ratio Criterion for Matching SIFT Descriptors', 'SIFT', 'Matching', 'a contrario', '', '', 'Matching keypoints by minimizing the Euclidean distance between their SIFT descriptors is an effective and extremely popular technique. Using the ratio between distances, as suggested by Lowe, is even more effective and leads to excellent matching accuracy. Probabilistic approaches that model the distribution of the distances were found effective as well. This work focuses, for the first time, on analyzing Lowe’s ratio criterion using a probabilistic approach. We provide two alternative interpretations of this criterion, which show that it is not only an effective heuristic but can also be formally justified. The first interpretation shows that Lowe’s ratio corresponds to a conditional probability that the match is incorrect. The second shows that the ratio corresponds to the Markov bound on this probability. The interpretations make it possible to slightly increase the effectiveness of the ratio criterion, and to obtain matching performance that exceeds all previous (non-learning based) results.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_42');
INSERT INTO `paper` VALUES (10152, 'Is Faster R-CNN Doing Well for Pedestrian Detection?', 'Pedestrian detection', 'Convolutional neural networks', 'Boosted forests', 'Hard-negative mining', '', 'Detecting pedestrian has been arguably addressed as a special topic beyond general object detection. Although recent deep learning object detectors such as Fast/Faster R-CNN have shown excellent performance for general object detection, they have limited success for detecting pedestrian, and previous leading pedestrian detectors were in general hybrid methods combining hand-crafted and deep convolutional features. In this paper, we investigate issues involving Faster R-CNN for pedestrian detection. We discover that the Region Proposal Network (RPN) in Faster R-CNN indeed performs well as a stand-alone pedestrian detector, but surprisingly, the downstream classifier degrades the results. We argue that two reasons account for the unsatisfactory accuracy: (i) insufficient resolution of feature maps for handling small instances, and (ii) lack of any bootstrapping strategy for mining hard negative examples. Driven by these observations, we propose a very simple but effective baseline for pedestrian detection, using an RPN followed by boosted forests on shared, high-resolution convolutional feature maps. We comprehensively evaluate this method on several benchmarks (Caltech, INRIA, ETH, and KITTI), presenting competitive accuracy and good speed. Code will be made publicly available.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_28');
INSERT INTO `paper` VALUES (10153, 'Joint Face Alignment and 3D Face Reconstruction', 'Face alignment', '3D face reconstruction', 'Cascaded regression', '', '', 'We present an approach to simultaneously solve the two problems of face alignment and 3D face reconstruction from an input 2D face image of arbitrary poses and expressions. The proposed method iteratively and alternately applies two sets of cascaded regressors, one for updating 2D landmarks and the other for updating reconstructed pose-expression-normalized (PEN) 3D face shape. The 3D face shape and the landmarks are correlated via a 3D-to-2D mapping matrix. In each iteration, adjustment to the landmarks is firstly estimated via a landmark regressor, and this landmark adjustment is also used to estimate 3D face shape adjustment via a shape regressor. The 3D-to-2D mapping is then computed based on the adjusted 3D face shape and 2D landmarks, and it further refines the 2D landmarks. An effective algorithm is devised to learn these regressors based on a training dataset of pairing annotated 3D face shapes and 2D face images. Compared with existing methods, the proposed method can fully automatically generate PEN 3D face shapes in real time from a single 2D face image and locate both visible and invisible 2D landmarks. Extensive experiments show that the proposed method can achieve the state-of-the-art accuracy in both face alignment and 3D face reconstruction, and benefit face recognition owing to its reconstructed PEN 3D face shapes.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_33');
INSERT INTO `paper` VALUES (10154, 'Joint Face Representation Adaptation and Clustering in Videos', 'Convolutional network', 'Transfer learning', 'Face clustering', 'Face recognition', '', 'Clustering faces in movies or videos is extremely challenging since characters’ appearance can vary drastically under different scenes. In addition, the various cinematic styles make it difficult to learn a universal face representation for all videos. Unlike previous methods that assume fixed handcrafted features for face clustering, in this work, we formulate a joint face representation adaptation and clustering approach in a deep learning framework. The proposed method allows face representation to gradually adapt from an external source domain to a target video domain. The adaptation of deep representation is achieved without any strong supervision but through iteratively discovered weak pairwise identity constraints derived from potentially noisy face clustering result. Experiments on three benchmark video datasets demonstrate that our approach generates character clusters with high purity compared to existing video face clustering methods, which are either based on deep face representation (without adaptation) or carefully engineered features.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_15');
INSERT INTO `paper` VALUES (10155, 'Joint Learning of Semantic and Latent Attributes', 'Attribute learning', 'Latent attributes', 'Person re-identification', 'Zero-shot learning', 'Dictionary learning', 'As mid-level semantic properties shared across object categories, attributes have been studied extensively. Recent approaches have attempted joint modelling of multiple attributes together with class labels so as to exploit their correlations for better attribute prediction and object recognition. However, they often ignore the fact that there exist some shared properties other than nameable/semantic attributes, which we call latent attributes. Basically, they can be further divided into discriminative and non-discriminative parts depending on whether they can contribute to an object recognition task. We argue that learning the latent attributes jointly with user-defined semantic attributes not only leads to better representation for object recognition but also helps with semantic attribute prediction. A novel dictionary learning model is proposed which decomposes the dictionary space into three parts corresponding to semantic, latent discriminative and latent background attributes respectively. An efficient algorithm is then formulated to solve the resultant optimization problem. Extensive experiments show that the proposed attribute learning method produces state-of-the-art results on both attribute prediction and attribute-based person re-identification.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_21');
INSERT INTO `paper` VALUES (10156, 'Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image', '3D body shape', 'Human pose', '2D to 3D', 'CNN', '', 'We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_34');
INSERT INTO `paper` VALUES (10157, 'Knowledge Transfer for Scene-Specific Motion Prediction', 'Knowledge Transfer', 'Dynamic Bayesian Network', 'Semantic Context', 'Context Descriptor', 'Input Scene', 'When given a single frame of the video, humans can not only interpret the content of the scene, but also they are able to forecast the near future. This ability is mostly driven by their rich prior knowledge about the visual world, both in terms of (i) the dynamics of moving agents, as well as (ii) the semantic of the scene. In this work we exploit the interplay between these two key elements to predict scene-specific motion patterns. First, we extract patch descriptors encoding the probability of moving to the adjacent patches, and the probability of being in that particular patch or changing behavior. Then, we introduce a Dynamic Bayesian Network which exploits this scene specific knowledge for trajectory prediction. Experimental results demonstrate that our method is able to accurately predict trajectories and transfer predictions to a novel scene characterized by similar elements.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_42');
INSERT INTO `paper` VALUES (10158, 'Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation', 'Semantic segmentation', 'Convolutional neural networks', '', '', '', 'CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_32');
INSERT INTO `paper` VALUES (10159, 'Large Scale Asset Extraction for Urban Images', '', '', '', '', '', 'Object proposals are currently used for increasing the computational efficiency of object detection. We propose a novel adaptive pipeline for interleaving object proposals with object classification and use it as a formulation for asset detection. We first preprocess the images using a novel and efficient rectification technique. We then employ a particle filter approach to keep track of three priors, which guide proposed samples and get updated using classifier output. Tests performed on over 1000 urban images demonstrate that our rectification method is faster than existing methods without loss in quality, and that our interleaved proposal method outperforms current state-of-the-art. We further demonstrate that other methods can be improved by incorporating our interleaved proposals.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_27');
INSERT INTO `paper` VALUES (10160, 'Large-Scale R-CNN with Classifier Adaptive Quantization', 'Image retrieval', 'Object detection', 'Image indexing', '', '', 'This paper extends R-CNN, a state-of-the-art object detection method, to larger scales. To apply R-CNN to a large database storing thousands to millions of images, the SVM classification of millions to billions of DCNN features extracted from object proposals is indispensable, which imposes unrealistic computational and memory costs. Our method dramatically narrows down the number of object proposals by using an inverted index and efficiently searches by using residual vector quantization (RVQ). Instead of k-means that has been used in inverted indices, we present a novel quantization method designed for linear classification wherein the quantization error is re-defined for linear classification. It approximates the error as the empirical error with pre-defined multiple exemplar classifiers and captures the variance and common attributes of object category classifiers effectively. Experimental results show that our method achieves comparable performance to that of applying R-CNN to all images while achieving a 250 times speed-up and 180 times memory reduction. Moreover, our approach significantly outperforms the state-of-the-art large-scale category detection method, with about a 40\\(\\sim \\)58 % increase in top-K precision. Scalability is also validated, and we demonstrate that our method can process 100 K images in 0.13 s while retaining precision.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_25');
INSERT INTO `paper` VALUES (10161, 'Large-Scale Training of Shadow Detectors with Noisily-Annotated Shadow Examples', 'Shadow detection', 'Large scale shadow dataset', 'Noisy labels', '', '', 'This paper introduces training of shadow detectors under the large-scale dataset paradigm. This was previously impossible due to the high cost of precise shadow annotation. Instead, we advocate the use of quickly but imperfectly labeled images. Our novel label recovery method automatically corrects a portion of the erroneous annotations such that the trained classifiers perform at state-of-the-art level. We apply our method to improve the accuracy of the labels of a new dataset that is 20 times larger than existing datasets and contains a large variety of scenes and image types. Naturally, such a large dataset is appropriate for training deep learning methods. Thus, we propose a semantic-aware patch level Convolutional Neural Network architecture that efficiently trains on patch level shadow examples while incorporating image level semantic information. This means that the detected shadow patches are refined based on image semantics. Our proposed pipeline can be a useful baseline for future advances in shadow detection.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_49');
INSERT INTO `paper` VALUES (10162, 'Learnable Histogram: Statistical Context Features for Deep Neural Networks', 'Histogram', 'Deep learning', 'Semantic segmentation', 'Object detection', '', 'Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher Vector, were commonly used with hand-crafted features in conventional classification methods, but attract less attention since the popularity of deep learning methods. In this paper, we propose a learnable histogram layer, which learns histogram features within deep neural networks in end-to-end training. Such a layer is able to back-propagate (BP) errors, learn optimal bin centers and bin widths, and be jointly optimized with other layers in deep networks during training. Two vision problems, semantic segmentation and object detection, are explored by integrating the learnable histogram layer into deep networks, which show that the proposed layer could be well generalized to different applications. In-depth investigations are conducted to provide insights on the newly introduced layer.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_15');
INSERT INTO `paper` VALUES (10163, 'Learning a Predictable and Generative Vector Representation for Objects', 'Natural Image', 'Cosine Distance', 'Image Network', 'Convolutional Layer', 'Prediction Loss', 'What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_29');
INSERT INTO `paper` VALUES (10164, 'Learning Common and Specific Features for RGB-D Semantic Segmentation with Deconvolutional Networks', 'Semantic segmentation', 'Deep learning', 'Common feature', 'Specific feature', '', 'In this paper, we tackle the problem of RGB-D semantic segmentation of indoor images. We take advantage of deconvolutional networks which can predict pixel-wise class labels, and develop a new structure for deconvolution of multiple modalities. We propose a novel feature transformation network to bridge the convolutional networks and deconvolutional networks. In the feature transformation network, we correlate the two modalities by discovering common features between them, as well as characterize each modality by discovering modality specific features. With the common features, we not only closely correlate the two modalities, but also allow them to borrow features from each other to enhance the representation of shared information. With specific features, we capture the visual patterns that are only visible in one modality. The proposed network achieves competitive segmentation accuracy on NYU depth dataset V1 and V2.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_40');
INSERT INTO `paper` VALUES (10165, 'Learning Diverse Models: The Coulomb Structured Support Vector Machine', 'Structured output learning', 'Diverse predictions', 'Multiple output learning', 'Structured support vector machine', '', 'In structured prediction, it is standard procedure to discriminatively train a single model that is then used to make a single prediction for each input. This practice is simple but risky in many ways. For instance, models are often designed with tractability rather than faithfulness in mind. To hedge against such model misspecification, it may be useful to train multiple models that all are a reasonable fit to the training data, but at least one of which may hopefully make more valid predictions than the single model in standard procedure. We propose the Coulomb Structured SVM (CSSVM) as a means to obtain at training time a full ensemble of different models. At test time, these models can run in parallel and independently to make diverse predictions. We demonstrate on challenging tasks from computer vision that some of these diverse predictions have significantly lower task loss than that of a single model, and improve over state-of-the-art diversity encouraging approaches.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_36');
INSERT INTO `paper` VALUES (10166, 'Learning Dynamic Hierarchical Models for Anytime Scene Labeling', 'Leaf Node', 'Hierarchical Model', 'Markov Decision Process', 'Feature Selection Method', 'Reward Function', 'With increasing demand for efficient image and video analysis, test-time cost of scene parsing becomes critical for many large-scale or time-sensitive vision applications. We propose a dynamic hierarchical model for anytime scene labeling that allows us to achieve flexible trade-offs between efficiency and accuracy in pixel-level prediction. In particular, our approach incorporates the cost of feature computation and model inference, and optimizes the model performance for any given test-time budget by learning a sequence of image-adaptive hierarchical models. We formulate this anytime representation learning as a Markov Decision Process with a discrete-continuous state-action space. A high-quality policy of feature and model selection is learned based on an approximate policy iteration method with action proposal mechanism. We demonstrate the advantages of our dynamic non-myopic anytime scene parsing on three semantic segmentation datasets, which achieves \\(90\\,\\%\\) of the state-of-the-art performances by using \\(15\\,\\%\\) of their overall costs.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_39');
INSERT INTO `paper` VALUES (10167, 'Learning High-Order Filters for Efficient Blind Deconvolution of Document Photographs', 'Text document', 'Camera motion', 'Blind deblurring', 'High-order filters', '', 'Photographs of text documents taken by hand-held cameras can be easily degraded by camera motion during exposure. In this paper, we propose a new method for blind deconvolution of document images. Observing that document images are usually dominated by small-scale high-order structures, we propose to learn a multi-scale, interleaved cascade of shrinkage fields model, which contains a series of high-order filters to facilitate joint recovery of blur kernel and latent image. With extensive experiments, we show that our method produces high quality results and is highly efficient at the same time, making it a practical choice for deblurring high resolution text images captured by modern mobile devices.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_45');
INSERT INTO `paper` VALUES (10168, 'Learning Image Matching by Simply Watching Video', 'Image matching', 'Unsupervised learning', 'Analysis by synthesis', 'Temporal coherence', 'Convolutional neural network', 'This work presents an unsupervised learning based approach to the ubiquitous computer vision problem of image matching. We start from the insight that the problem of frame interpolation implicitly solves for inter-frame correspondences. This permits the application of analysis-by-synthesis: we first train and apply a Convolutional Neural Network for frame interpolation, then obtain correspondences by inverting the learned CNN. The key benefit behind this strategy is that the CNN for frame interpolation can be trained in an unsupervised manner by exploiting the temporal coherence that is naturally contained in real-world video sequences. The present model therefore learns image matching by simply “watching videos”. Besides a promise to be more generally applicable, the presented approach achieves surprising performance comparable to traditional empirically designed methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_26');
INSERT INTO `paper` VALUES (10169, 'Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering', 'Activity prediction', 'Deep networks', 'Visual Question Answering', '', '', 'This paper proposes deep convolutional network models that utilize local and global context to make human activity label predictions in still images, achieving state-of-the-art performance on two recent datasets with hundreds of labels each. We use multiple instance learning to handle the lack of supervision on the level of individual person instances, and weighted loss to handle unbalanced training data. Further, we show how specialized features trained on these datasets can be used to improve accuracy on the Visual Question Answering (VQA) task, in the form of multiple choice fill-in-the-blank questions (Visual Madlibs). Specifically, we tackle two types of questions on person activity and person-object relationship and show improvements over generic features trained on the ImageNet classification task', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_25');
INSERT INTO `paper` VALUES (10170, 'Learning Recursive Filters for Low-Level Vision via a Hybrid Neural Network', 'Finite Impulse Response', 'Recurrent Neural Network', 'Weight Less Square', 'Convolutional Neural Network', 'Image Denoising', 'In this paper, we consider numerous low-level vision problems (e.g., edge-preserving filtering and denoising) as recursive image filtering via a hybrid neural network. The network contains several spatially variant recurrent neural networks (RNN) as equivalents of a group of distinct recursive filters for each pixel, and a deep convolutional neural network (CNN) that learns the weights of RNNs. The deep CNN can learn regulations of recurrent propagation for various tasks and effectively guides recurrent propagation over an entire image. The proposed model does not need a large number of convolutional channels nor big kernels to learn features for low-level vision filters. It is significantly smaller and faster in comparison with a deep CNN based image filter. Experimental results show that many low-level vision tasks can be effectively learned and carried out in real-time by the proposed algorithm.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_34');
INSERT INTO `paper` VALUES (10171, 'Learning Representations for Automatic Colorization', 'Color Space', 'Reference Image', 'Color Histogram', 'Deep Convolutional Neural Network', 'Semantic Segmentation', 'We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_35');
INSERT INTO `paper` VALUES (10172, 'Learning Semantic Deformation Flows with 3D Convolutional Networks', 'Convolutional Neural Network', 'Input Shape', 'Training Pair', 'Mesh Representation', 'Deformation Flow', 'Shape deformation requires expert user manipulation even when the object under consideration is in a high fidelity format such as a 3D mesh. It becomes even more complicated if the data is represented as a point set or a depth scan with significant self occlusions. We introduce an end-to-end solution to this tedious process using a volumetric Convolutional Neural Network (CNN) that learns deformation flows in 3D. Our network architectures take the voxelized representation of the shape and a semantic deformation intention (e.g., make more sporty) as input and generate a deformation flow at the output. We show that such deformation flows can be trivially applied to the input shape, resulting in a novel deformed version of the input without losing detail information. Our experiments show that the CNN approach achieves comparable results with state of the art methods when applied to CAD models. When applied to single frame depth scans, and partial/noisy CAD models we achieve \\({\\sim }60\\,\\%\\) less error compared to the state-of-the-art.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_18');
INSERT INTO `paper` VALUES (10173, 'Learning to Count with CNN Boosting', 'Counting', 'Convolutional Neural Networks', 'Gradient boosting', 'Sample selection', '', 'In this paper, we address the task of object counting in images. We follow modern learning approaches in which a density map is estimated directly from the input image. We employ CNNs and incorporate two significant improvements to the state of the art methods: layered boosting and selective sampling. As a result, we manage both to increase the counting accuracy and to reduce processing time. Moreover, we show that the proposed method is effective, even in the presence of labeling errors. Extensive experiments on five different datasets demonstrate the efficacy and robustness of our approach. Mean Absolute error was reduced by 20 % to 35 %. At the same time, the training time of each CNN has been reduced by 50 %.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_41');
INSERT INTO `paper` VALUES (10174, 'Learning to Hash with Binary Deep Neural Network', 'Learning to hash', 'Neural network', 'Discrete optimizatization', '', '', 'This work proposes deep network models and learning algorithms for unsupervised and supervised binary hashing. Our novel network design constrains one hidden layer to directly output the binary codes. This addresses a challenging issue in some previous works: optimizing non-smooth objective functions due to binarization. Moreover, we incorporate independence and balance properties in the direct and strict forms in the learning. Furthermore, we include similarity preserving property in our objective function. Our resulting optimization with these binary, independence, and balance constraints is difficult to solve. We propose to attack it with alternating optimization and careful relaxation. Experimental results on three benchmark datasets show that our proposed methods compare favorably with the state of the art.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_14');
INSERT INTO `paper` VALUES (10175, 'Learning to Learn: Model Regression Networks for Easy Small Sample Learning', 'Small sample learning', 'Transfer learning', 'Object recognition', 'Model transformation', 'Deep regression networks', 'We develop a conceptually simple but powerful approach that can learn novel categories from few annotated examples. In this approach, the experience with already learned categories is used to facilitate the learning of novel classes. Our insight is two-fold: (1) there exists a generic, category agnostic transformation from models learned from few samples to models learned from large enough sample sets, and (2) such a transformation could be effectively learned by high-capacity regressors. In particular, we automatically learn the transformation with a deep model regression network on a large collection of model pairs. Experiments demonstrate that encoding this transformation as prior knowledge greatly facilitates the recognition in the small sample size regime on a broad range of tasks, including domain adaptation, fine-grained recognition, action recognition, and scene classification.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_37');
INSERT INTO `paper` VALUES (10176, 'Learning to Refine Object Segments', 'Object Segmentation', 'Average Recall', 'Convolutional Layer', 'Segmentation Mask', 'Object Mask', 'Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse ‘mask encoding’ in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10–20% in average recall for various setups. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50 % faster than the original DeepMask network (under .8 s per image).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_5');
INSERT INTO `paper` VALUES (10177, 'Learning to Track at 100 FPS with Deep Regression Networks', 'Tracking', 'Deep learning', 'Neural networks', 'Machine learning', '', 'Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker’s state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker (Our tracker is available at http://davheld.github.io/GOTURN/GOTURN.html) is the first neural-network tracker that learns to track generic objects at 100 fps.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_45');
INSERT INTO `paper` VALUES (10178, 'Learning Visual Storylines with Skipping Recurrent Neural Networks', 'Visual Concept', 'Representative Event', 'Photo Album', 'Amazon Mechanical Turk', 'Future Image', 'What does a typical visit to Paris look like? Do people first take photos of the Louvre and then the Eiffel Tower? Can we visually model a temporal event like “Paris Vacation” using current frameworks? In this paper, we explore how we can automatically learn the temporal aspects, or storylines of visual concepts from web data. Previous attempts focus on consecutive image-to-image transitions and are unsuccessful at recovering the long-term underlying story. Our novel Skipping Recurrent Neural Network (S-RNN) model does not attempt to predict each and every data point in the sequence, like classic RNNs. Rather, S-RNN uses a framework that skips through the images in the photo stream to explore the space of all ordered subsets of the albums via an efficient sampling procedure. This approach reduces the negative impact of strong short-term correlations, and recovers the latent story more accurately. We show how our learned storylines can be used to analyze, predict, and summarize photo albums from Flickr. Our experimental results provide strong qualitative and quantitative evidence that S-RNN is significantly better than other candidate methods such as LSTMs on learning long-term correlations and recovering latent storylines. Moreover, we show how storylines can help machines better understand and summarize photo streams by inferring a brief personalized story of each individual album.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_5');
INSERT INTO `paper` VALUES (10179, 'Learning Without Forgetting', 'Convolutional neural networks', 'Transfer learning', 'Multi-task learning', 'Deep learning', 'Visual recognition', 'When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning as standard practice for improved new task performance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_37');
INSERT INTO `paper` VALUES (10180, 'Less Is More: Towards Compact CNNs', 'Convolutional neural network', 'Neuron reduction', 'Sparsity', '', '', 'To attain a favorable performance on large-scale datasets, convolutional neural networks (CNNs) are usually designed to have very high capacity involving millions of parameters. In this work, we aim at optimizing the number of neurons in a network, thus the number of parameters. We show that, by incorporating sparse constraints into the objective function, it is possible to decimate the number of neurons during the training stage. As a result, the number of parameters and the memory footprint of the neural network are also reduced, which is also desirable at the test time. We evaluated our method on several well-known CNN structures including AlexNet, and VGG over different datasets including ImageNet. Extensive experimental results demonstrate that our method leads to compact networks. Taking first fully connected layer as an example, our compact CNN contains only \\(30\\,\\%\\) of the original neurons without any degradation of the top-1 classification accuracy.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_40');
INSERT INTO `paper` VALUES (10181, 'Leveraging Visual Question Answering for Image-Caption Ranking', 'Visual question answering', 'Image-caption ranking', 'Mid-level concepts', '', '', 'Visual Question Answering (VQA) is the task of taking as input an image and a free-form natural language question about the image, and producing an accurate answer. In this work we view VQA as a “feature extraction” module to extract image and caption representations. We employ these representations for the task of image-caption ranking. Each feature dimension captures (imagines) whether a fact (question-answer pair) could plausibly be true for the image and caption. This allows the model to interpret images and captions from a wide variety of perspectives. We propose score-level and representation-level fusion models to incorporate VQA knowledge in an existing state-of-the-art VQA-agnostic image-caption ranking model. We find that incorporating and reasoning about consistency between images and captions significantly improves performance. Concretely, our model improves state-of-the-art on caption retrieval by 7.1 % and on image retrieval by 4.4 % on the MSCOCO dataset.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_17');
INSERT INTO `paper` VALUES (10182, 'LIFT: Learned Invariant Feature Transform', 'Local features', 'Feature descriptors', 'Deep Learning', '', '', 'We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_28');
INSERT INTO `paper` VALUES (10183, 'Localizing and Orienting Street Views Using Overhead Imagery', 'Image geolocalization', 'Image matching', 'Deep learning', 'Siamese network', 'Triplet network', 'In this paper we aim to determine the location and orientation of a ground-level query image by matching to a reference database of overhead (e.g. satellite) images. For this task we collect a new dataset with one million pairs of street view and overhead images sampled from eleven U.S. cities. We explore several deep CNN architectures for cross-domain matching – Classification, Hybrid, Siamese, and Triplet networks. Classification and Hybrid architectures are accurate but slow since they allow only partial feature precomputation. We propose a new loss function which significantly improves the accuracy of Siamese and Triplet embedding networks while maintaining their applicability to large-scale retrieval tasks like image geolocalization. This image matching task is challenging not just because of the dramatic viewpoint difference between ground-level and overhead imagery but because the orientation (i.e. azimuth) of the street views is unknown making correspondence even more difficult. We examine several mechanisms to match in spite of this – training for rotation invariance, sampling possible rotations at query time, and explicitly predicting relative rotation of ground and overhead images with our deep networks. It turns out that explicit orientation supervision also improves location prediction accuracy. Our best performing architectures are roughly 2.5 times as accurate as the commonly used Siamese network baseline.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_30');
INSERT INTO `paper` VALUES (10184, 'Look-Ahead Before You Leap: End-to-End Active Recognition by Forecasting the Effect of Motion', 'Active Recognition', 'Camera Motion', 'Active Vision', 'Partially Observable Markov Decision Process', 'Object Instance', 'Visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data, but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data. In this work, we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this “active recognition” setting. Further, we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world. To verify this hypothesis, we attempt to induce this capacity in our active recognition pipeline, by simultaneously learning to forecast the effects of the agent’s motions on its internal representation of the environment conditional on all past views. Results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition, and that “learning to look ahead” further boosts recognition performance.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_30');
INSERT INTO `paper` VALUES (10185, 'LSTM-CF: Unifying Context Modeling and Fusion with LSTMs for RGB-D Scene Labeling', 'RGB-D scene labeling', 'Image context modeling', 'Long short-term memory', 'Depth and photometric data fusion', '', 'Semantic labeling of RGB-D scenes is crucial to many intelligent applications including perceptual robotics. It generates pixelwise and fine-grained label maps from simultaneously sensed photometric (RGB) and depth channels. This paper addresses this problem by (i) developing a novel Long Short-Term Memorized Context Fusion (LSTM-CF) Model that captures and fuses contextual information from multiple channels of photometric and depth data, and (ii) incorporating this model into deep convolutional neural networks (CNNs) for end-to-end training. Specifically, contexts in photometric and depth channels are, respectively, captured by stacking several convolutional layers and a long short-term memory layer; the memory layer encodes both short-range and long-range spatial dependencies in an image along the vertical direction. Another long short-term memorized fusion layer is set up to integrate the contexts along the vertical direction from different channels, and perform bi-directional propagation of the fused vertical contexts along the horizontal direction to obtain true 2D global contexts. At last, the fused contextual representation is concatenated with the convolutional features extracted from the photometric channels in order to improve the accuracy of fine-scale semantic labeling. Our proposed model has set a new state of the art, i.e., \\({\\mathbf{48.1}}\\%\\) and \\({\\mathbf{49.4}}\\%\\) average class accuracy over 37 categories (\\({\\mathbf{2.2}}\\%\\) and \\({\\mathbf{5.4}}\\%\\) improvement) on the large-scale SUNRGBD dataset and the NYUDv2 dataset, respectively.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_34');
INSERT INTO `paper` VALUES (10186, 'MADMM: A Generic Algorithm for Non-smooth Optimization on Manifolds', 'Matrix Completion', 'Manifold Learning', 'Euclidean Distance Matrix', 'Manifold Constraint', 'Sparse Principal Component Analysis', 'Numerous problems in computer vision, pattern recognition, and machine learning are formulated as optimization with manifold constraints. In this paper, we propose the Manifold Alternating Directions Method of Multipliers (MADMM), an extension of the classical ADMM scheme for manifold-constrained non-smooth optimization problems. To our knowledge, MADMM is the first generic non-smooth manifold optimization method. We showcase our method on several challenging problems in dimensionality reduction, non-rigid correspondence, multi-modal clustering, and multidimensional scaling.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_41');
INSERT INTO `paper` VALUES (10187, 'Manhattan-World Urban Reconstruction from Point Clouds', 'Urban reconstruction', 'Manhattan-world scenes', 'Reconstruction', 'Box fitting', '', 'Manhattan-world urban scenes are common in the real world. We propose a fully automatic approach for reconstructing such scenes from 3D point samples. Our key idea is to represent the geometry of the buildings in the scene using a set of well-aligned boxes. We first extract plane hypothesis from the points followed by an iterative refinement step. Then, candidate boxes are obtained by partitioning the space of the point cloud into a non-uniform grid. After that, we choose an optimal subset of the candidate boxes to approximate the geometry of the buildings. The contribution of our work is that we transform scene reconstruction into a labeling problem that is solved based on a novel Markov Random Field formulation. Unlike previous methods designed for particular types of input point clouds, our method can obtain faithful reconstructions from a variety of data sources. Experiments demonstrate that our method is superior to state-of-the-art methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_4');
INSERT INTO `paper` VALUES (10188, 'Marker-Less 3D Human Motion Capture with Monocular Image Sequence and Height-Maps', 'Human pose estimation', 'Height-map', '', '', '', 'The recovery of 3D human pose with monocular camera is an inherently ill-posed problem due to the large number of possible projections from the same 2D image to 3D space. Aimed at improving the accuracy of 3D motion reconstruction, we introduce the additional built-in knowledge, namely height-map, into the algorithmic scheme of reconstructing the 3D pose/motion under a single-view calibrated camera. Our novel proposed framework consists of two major contributions. Firstly, the RGB image and its calculated height-map are combined to detect the landmarks of 2D joints with a dual-stream deep convolution network. Secondly, we formulate a new objective function to estimate 3D motion from the detected 2D joints in the monocular image sequence, which reinforces the temporal coherence constraints on both the camera and 3D poses. Experiments with HumanEva, Human3.6M, and MCAD dataset validate that our method outperforms the state-of-the-art algorithms on both 2D joints localization and 3D motion recovery. Moreover, the evaluation results on HumanEva indicates that the performance of our proposed single-view approach is comparable to that of the multi-view deep learning counterpart.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_2');
INSERT INTO `paper` VALUES (10189, 'MARS: A Video Benchmark for Large-Scale Person Re-Identification', 'Video person re-identification', 'Motion features', 'CNN', '', '', 'This paper considers person re-identification (re-id) in videos. We introduce a new video re-id dataset, named Motion Analysis and Re-identification Set (MARS), a video extension of the Market-1501 dataset. To our knowledge, MARS is the largest video re-id dataset to date. Containing 1,261 IDs and around 20,000 tracklets, it provides rich visual information compared to image-based datasets. Meanwhile, MARS reaches a step closer to practice. The tracklets are automatically generated by the Deformable Part Model (DPM) as pedestrian detector and the GMMCP tracker. A number of false detection/tracking results are also included as distractors which would exist predominantly in practical video databases. Extensive evaluation of the state-of-the-art methods including the space-time descriptors and CNN is presented. We show that CNN in classification mode can be trained from scratch using the consecutive bounding boxes of each identity. The learned CNN embedding outperforms other competing methods considerably and has good generalization ability on other video re-id datasets upon fine-tuning.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_52');
INSERT INTO `paper` VALUES (10190, 'Matching Handwritten Document Images', 'Handwritten word spotting', 'cnn features', 'plagiarism detection', '', '', 'We address the problem of predicting similarity between a pair of handwritten document images written by potentially different individuals. This has applications related to matching and mining in image collections containing handwritten content. A similarity score is computed by detecting patterns of text re-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (cnn) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_46');
INSERT INTO `paper` VALUES (10191, 'MeshFlow: Minimum Latency Online Video Stabilization', 'Online video stabilization', 'MeshFlow', 'Vertex profile', '', '', 'Many existing video stabilization methods often stabilize videos off-line, i.e. as a postprocessing tool of pre-recorded videos. Some methods can stabilize videos online, but either require additional hardware sensors (e.g., gyroscope) or adopt a single parametric motion model (e.g., affine, homography) which is problematic to represent spatially-variant motions. In this paper, we propose a technique for online video stabilization with only one frame latency using a novel MeshFlow motion model. The MeshFlow is a spatial smooth sparse motion field with motion vectors only at the mesh vertexes. In particular, the motion vectors on the matched feature points are transferred to their corresponding nearby mesh vertexes. The MeshFlow is produced by assigning each vertex an unique motion vector via two median filters. The path smoothing is conducted on the vertex profiles, which are motion vectors collected at the same vertex location in the MeshFlow over time. The profiles are smoothed adaptively by a novel smoothing technique, namely the Predicted Adaptive Path Smoothing (PAPS), which only uses motions from the past. In this way, the proposed method not only handles spatially-variant motions but also works online in real time, offering potential for a variety of intelligent applications (e.g., security systems, robotics, UAVs). The quantitative and qualitative evaluations show that our method can produce comparable results with the state-of-the-art off-line methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_48');
INSERT INTO `paper` VALUES (10192, 'Minimal Solvers for Generalized Pose and Scale Estimation from Two Rays and One Point', 'Absolute camera pose', 'Pose solver', 'Generalized cameras', '', '', 'Estimating the poses of a moving camera with respect to a known 3D map is a key problem in robotics and Augmented Reality applications. Instead of solving for each pose individually, the trajectory can be considered as a generalized camera. Thus, all poses can be jointly estimated by solving a generalized PnP (gPnP) problem. In this paper, we show that the gPnP problem for camera trajectories permits an extremely efficient minimal solution when exploiting the fact that pose tracking allows us to locally triangulate 3D points. We present a problem formulation based on one point-point and two point-ray correspondences that encompasses both the case where the scale of the trajectory is known and where it is unknown. Our formulation leads to closed-form solutions that are orders of magnitude faster to compute than the current state-of-the-art, while resulting in a similar or better pose accuracy.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_13');
INSERT INTO `paper` VALUES (10193, 'Modeling Context Between Objects for Referring Expression Understanding', 'Loss Function', 'Context Region', 'Positive Instance', 'Comprehension Task', 'Multiple Instance Learn', 'Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_48');
INSERT INTO `paper` VALUES (10194, 'Modeling Context in Referring Expressions', 'Language', 'Language and vision', 'Generation', 'Referring expression generation', '', 'Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_5');
INSERT INTO `paper` VALUES (10195, 'MOON: A Mixed Objective Optimization Network for the Recognition of Facial Attributes', 'Facial attributes', 'Deep neural networks', 'Multi-task learning', 'Multi-label learning', 'Domain adaptation', 'Attribute recognition, particularly facial, extracts many labels for each image. While some multi-task vision problems can be decomposed into separate tasks and stages, e.g., training independent models for each task, for a growing set of problems joint optimization across all tasks has been shown to improve performance. We show that for deep convolutional neural network (DCNN) facial attribute extraction, multi-task optimization is better. Unfortunately, it can be difficult to apply joint optimization to DCNNs when training data is imbalanced, and re-balancing multi-label data directly is structurally infeasible, since adding/removing data to balance one label will change the sampling of the other labels. This paper addresses the multi-label imbalance problem by introducing a novel mixed objective optimization network (MOON) with a loss function that mixes multiple task objectives with domain adaptive re-weighting of propagated loss. Experiments demonstrate that not only does MOON advance the state of the art in facial attribute recognition, but it also outperforms independently trained DCNNs using the same data. When using facial attributes for the LFW face recognition task, we show that our balanced (domain adapted) network outperforms the unbalanced trained network.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_2');
INSERT INTO `paper` VALUES (10196, 'MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition', 'Face recognition', 'Large scale', 'Benchmark', 'Training data', 'Celebrity recognition', 'In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_6');
INSERT INTO `paper` VALUES (10197, 'Multi-attributed Graph Matching with Multi-layer Random Walks', 'Graph matching', 'Multi-layer structure', 'Multiple attributes', '', '', 'This paper addresses the multi-attributed graph matching problem considering multiple attributes jointly while preserving the characteristics of each attribute. Since most of conventional graph matching algorithms integrate multiple attributes to construct a single attribute in an oversimplified way, the information from multiple attributes are not often fully exploited. In order to solve this problem, we propose a novel multi-layer graph structure that can preserve the particularities of each attribute in separated layers. Then, we also propose a multi-attributed graph matching algorithm based on the random walk centrality for the proposed multi-layer graph structure. We compare the proposed algorithm with other state-of-the-art graph matching algorithms based on the single-layer structure using synthetic and real datasets, and prove the superior performance of the proposed multi-layer graph structure and matching algorithm.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_12');
INSERT INTO `paper` VALUES (10198, 'Multi-label Active Learning Based on Maximum Correntropy Criterion: Towards Robust and Discriminative Labeling', 'Multi-label learning', 'Active learning', 'Correntropy', 'Robust', '', 'Multi-label learning is a challenging problem in computer vision field. In this paper, we propose a novel active learning approach to reduce the annotation costs greatly for multi-label classification. State-of-the-art active learning methods either annotate all the relevant samples without diagnosing discriminative information in the labels or annotate only limited discriminative samples manually, that has weak immunity for the outlier labels. To overcome these problems, we propose a multi-label active learning method based on Maximum Correntropy Criterion (MCC) by merging uncertainty and representativeness. We use the the labels of labeled data and the prediction labels of unknown data to enhance the uncertainty and representativeness measurement by merging strategy, and use the MCC to alleviate the influence of outlier labels for discriminative labeling. Experiments on several challenging benchmark multi-label datasets show the superior performance of our proposed method to the state-of-the-art methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_28');
INSERT INTO `paper` VALUES (10199, 'Multi-region Two-Stream R-CNN for Action Detection', 'Action detection', 'Faster R-CNN', 'Multi-region CNNs', 'Two stream R-CNN', '', 'We propose a multi-region two-stream R-CNN model for action detection in realistic videos. We start from frame-level action detection based on faster R-CNN, and make three contributions: (1) we show that a motion region proposal network generates high-quality proposals, which are complementary to those of an appearance region proposal network; (2) we show that stacking optical flow over several frames significantly improves frame-level action detection; and (3) we embed a multi-region scheme in the faster R-CNN model, which adds complementary information on body parts. We then link frame-level detections with the Viterbi algorithm, and temporally localize an action with the maximum subarray method. Experimental results on the UCF-Sports, J-HMDB and UCF101 action detection datasets show that our approach outperforms the state of the art with a significant margin in both frame-mAP and video-mAP.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_45');
INSERT INTO `paper` VALUES (10200, 'Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation', 'Action Recognition', 'Ridge Regression', 'Importance Weighting', 'Target Class', 'Auxiliary Data', 'Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing the conventional model training requirement of annotated examples for every category. This is achieved by establishing a mapping connecting low-level features and a semantic description of the label space, referred as visual-semantic mapping, on auxiliary data. Re-using the learned mapping to project target videos into an embedding space thus allows novel-classes to be recognised by nearest neighbour inference. However, existing ZSL methods suffer from auxiliary-target domain shift intrinsically induced by assuming the same mapping for the disjoint auxiliary and target classes. This compromises the generalisation accuracy of ZSL recognition on the target data. In this work, we improve the ability of ZSL to generalise across this domain shift in both model- and data-centric ways by formulating a visual-semantic mapping with better generalisation properties and a dynamic data re-weighting method to prioritise auxiliary data that are relevant to the target classes. Specifically: (1) We introduce a multi-task visual-semantic mapping to improve generalisation by constraining the semantic mapping parameters to lie on a low-dimensional manifold, (2) We explore prioritised data augmentation by expanding the pool of auxiliary data with additional instances weighted by relevance to the target domain. The proposed new model is applied to the challenging zero-shot action recognition problem to demonstrate its advantages over existing ZSL models.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_22');
INSERT INTO `paper` VALUES (10201, 'Multi-view Inverse Rendering Under Arbitrary Illumination and Albedo', 'Multi-view stereo', 'Shape from shading', 'Inverse rendering', '', '', '3D shape reconstruction with multi-view stereo (MVS) relies on a robust evaluation of photo consistencies across images. The robustness is ensured by isolating surface albedo and scene illumination from the shape recovery, i.e. shading and colour variation are regarded as a nuisance in MVS. This yields a gap in the qualities between the recovered shape and the images used. We present a method to address it by jointly estimating detailed shape, illumination and albedo using the initial shape robustly recovered by MVS. This is achieved by solving the multi-view inverse rendering problem using the geometric and photometric smoothness terms and the normalized spherical harmonics illumination model. Our method allows spatially-varying albedo and per image illumination without any prerequisites such as training data or image segmentation. We demonstrate that our method can clearly improve the 3D shape and recover illumination and albedo on real world scenes.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_46');
INSERT INTO `paper` VALUES (10202, 'Natural Image Matting Using Deep Convolutional Neural Networks', 'Alpha matting', 'Deep CNN', 'Local and nonlocal matting', '', '', 'We propose a deep Convolutional Neural Networks (CNN) method for natural image matting. Our method takes results of the closed form matting, results of the KNN matting and normalized RGB color images as inputs, and directly learns an end-to-end mapping between the inputs, and reconstructed alpha mattes. We analyze pros and cons of the closed form matting, and the KNN matting in terms of local and nonlocal principle, and show that they are complementary to each other. A major benefit of our method is that it can “recognize” different local image structures, and then combine results of local (closed form matting), and nonlocal (KNN matting) matting effectively to achieve higher quality alpha mattes than both of its inputs. Extensive experiments demonstrate that our proposed deep CNN matting produces visually and quantitatively high-quality alpha mattes. In addition, our method has achieved the highest ranking in the public alpha matting evaluation dataset in terms of the sum of absolute differences, mean squared errors, and gradient errors.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_39');
INSERT INTO `paper` VALUES (10203, 'Natural Image Stitching with the Global Similarity Prior', 'Image stitching', 'Panoramas', 'Image warping', '', '', 'This paper proposes a method for stitching multiple images together so that the stitched image looks as natural as possible. Our method adopts the local warp model and guides the warping of each image with a grid mesh. An objective function is designed for specifying the desired characteristics of the warps. In addition to good alignment and minimal local distortion, we add a global similarity prior in the objective function. This prior constrains the warp of each image so that it resembles a similarity transformation as a whole. The selection of the similarity transformation is crucial to the naturalness of the results. We propose methods for selecting the proper scale and rotation for each image. The warps of all images are solved together for minimizing the distortion globally. A comprehensive evaluation shows that the proposed method consistently outperforms several state-of-the-art methods, including AutoStitch, APAP, SPHP and ANNAP.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_12');
INSERT INTO `paper` VALUES (10204, 'Network Flow Formulations for Learning Binary Hashing', 'Binary Hash', 'Network Flow Formulation', 'Anchor Graph Hashing (AGH)', 'Spectral Hashing (SH)', 'Hash Code', 'The problem of learning binary hashing seeks the identification of a binary mapping for a set of n examples such that the corresponding Hamming distances preserve high fidelity with a given \\(n \\times n\\) matrix of distances (or affinities). This formulation has numerous applications in efficient search and retrieval of images (and other high dimensional data) on devices with storage/processing constraints. As a result, the problem has received much attention recently in vision and machine learning and a number of interesting solutions have been proposed. A common feature of most existing solutions is that they adopt continuous iterative optimization schemes which is then followed by a post-hoc rounding process to recover a feasible discrete solution. In this paper, we present a fully combinatorial network-flow based formulation for a relaxed version of this problem. The main maximum flow/minimum cut modules which drive our algorithm can be solved efficiently and can directly learn the binary codes. Despite its simplicity, we show that on most widely used benchmarks, our proposal yields competitive performance relative to a suite of nine different state of the art algorithms.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_23');
INSERT INTO `paper` VALUES (10205, 'Non-rigid 3D Shape Retrieval via Large Margin Nearest Neighbor Embedding', 'Shape retrieval', 'Shape representation', 'Supervised learning', '', '', 'In this paper, we propose a highly efficient metric learning approach to non-rigid 3D shape analysis. From a training set of 3D shapes from different classes, we learn a transformation of the shapes which optimally enforces a clustering of shapes from the same class. In contrast to existing approaches, we do not perform a transformation of individual local point descriptors, but a linear embedding of the entire distribution of shape descriptors. It turns out that this embedding of the input shapes is sufficiently powerful to enable state of the art retrieval performance using a simple nearest neighbor classifier. We demonstrate experimentally that our approach substantially outperforms the state of the art non-rigid 3D shape retrieval methods on the recent benchmark data set SHREC’14 Non-Rigid 3D Human Models, both in classification accuracy and runtime.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_21');
INSERT INTO `paper` VALUES (10206, 'Normalized Cut Meets MRF', 'Markov Random Fields', 'Spectral Cluster', 'Normalize Mutual Information', 'Swap Move', 'Label Cost', 'We propose a new segmentation or clustering model that combines Markov Random Field (MRF) and Normalized Cut (NC) objectives. Both NC and MRF models are widely used in machine learning and computer vision, but they were not combined before due to significant differences in the corresponding optimization, e.g. spectral relaxation and combinatorial max-flow techniques. On the one hand, we show that many common applications for multi-label MRF segmentation energies can benefit from a high-order NC term, e.g. enforcing balanced clustering of arbitrary high-dimensional image features combining color, texture, location, depth, motion, etc. On the other hand, standard NC applications benefit from an inclusion of common pairwise or higher-order MRF constraints, e.g. edge alignment, bin-consistency, label cost, etc. To address NC+MRF energy, we propose two efficient multi-label combinatorial optimization techniques, spectral cut and kernel cut, using new unary bounds for different NC formulations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_46');
INSERT INTO `paper` VALUES (10207, 'On Volumetric Shape Reconstruction from Implicit Forms', 'Point Cloud', 'Voronoi Cell', 'Implicit Representation', 'Voronoi Tessellation', 'Implicit Form', 'In this paper we report on the evaluation of volumetric shape reconstruction methods that consider as input implicit forms in 3D. Many visual applications build implicit representations of shapes that are converted into explicit shape representations using geometric tools such as the Marching Cubes algorithm. This is the case with image based reconstructions that produce point clouds from which implicit functions are computed, with for instance a Poisson reconstruction approach. While the Marching Cubes method is a versatile solution with proven efficiency, alternative solutions exist with different and complementary properties that are of interest for shape modeling. In this paper, we propose a novel strategy that builds on Centroidal Voronoi Tessellations (CVTs). These tessellations provide volumetric and surface representations with strong regularities in addition to provably more accurate approximations of the implicit forms considered. In order to compare the existing strategies, we present an extensive evaluation that analyzes various properties of the main strategies for implicit to explicit volumetric conversions: Marching cubes, Delaunay refinement and CVTs, including accuracy and shape quality of the resulting shape mesh.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_11');
INSERT INTO `paper` VALUES (10208, 'Online Action Detection', 'Action recognition', 'Evaluation', 'Online action detection', '', '', 'In online action detection, the goal is to detect the start of an action in a video stream as soon as it happens. For instance, if a child is chasing a ball, an autonomous car should recognize what is going on and respond immediately. This is a very challenging problem for four reasons. First, only partial actions are observed. Second, there is a large variability in negative data. Third, the start of the action is unknown, so it is unclear over what time window the information should be integrated. Finally, in real world data, large within-class variability exists. This problem has been addressed before, but only to some extent. Our contributions to online action detection are threefold. First, we introduce a realistic dataset composed of 27 episodes from 6 popular TV series. The dataset spans over 16 h of footage annotated with 30 action classes, totaling 6,231 action instances. Second, we analyze and compare various baseline methods, showing this is a challenging problem for which none of the methods provides a good solution. Third, we analyze the change in performance when there is a variation in viewpoint, occlusion, truncation, etc. We introduce an evaluation protocol for fair comparison. The dataset, the baselines and the models will all be made publicly available to encourage (much needed) further research on online action detection on realistic data.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_17');
INSERT INTO `paper` VALUES (10209, 'Partial Linearization Based Optimization for Multi-class SVM', 'Multi-class svm', 'Partial linearization', 'Optimization', '', '', 'We propose a novel partial linearization based approach for optimizing the multi-class svm learning problem. Our method is an intuitive generalization of the Frank-Wolfe and the exponentiated gradient algorithms. In particular, it allows us to combine several of their desirable qualities into one approach: (i) the use of an expectation oracle (which provides the marginals over each output class) in order to estimate an informative descent direction, similar to exponentiated gradient; (ii) analytical computation of the optimal step-size in the descent direction that guarantees an increase in the dual objective, similar to Frank-Wolfe; and (iii) a block coordinate formulation similar to the one proposed for Frank-Wolfe, which allows us to solve large-scale problems. Using the challenging computer vision problems of action classification, object recognition and gesture recognition, we demonstrate the efficacy of our approach on training multi-class svms with standard, publicly available, machine learning datasets.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_51');
INSERT INTO `paper` VALUES (10210, 'Patch-Based Low-Rank Matrix Completion for Learning of Shape and Motion Models from Few Training Samples', 'Statistical modeling', 'High-dimension-low-sample-size problem', 'Low-rank matrix completion', 'Virtual samples', '', 'Statistical models have opened up new possibilities for the automated analysis of images. However, the limited availability of representative training data, e.g. segmented images, leads to a bottleneck for the application of statistical models in practice. In this paper, we propose a novel patch-based technique that enables to learn representative statistical models of shape, appearance, or motion with a high grade of detail from a small number of observed training samples using low-rank matrix completion methods. Our method relies on the assumption that local variations have limited effects in distant areas. We evaluate our approach on three exemplary applications: (1) 2D shape modeling of faces, (2) 3D modeling of human lung shapes, and (3) population-based modeling of respiratory organ deformation. A comparison with the classical PCA-based modeling approach and FEM-PCA shows an improved generalization ability for small training sets indicating the improved flexibility of the model.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_43');
INSERT INTO `paper` VALUES (10211, 'Pattern Mining Saliency', 'Saliency detection', 'Pattern mining', 'Random walk', '', '', 'This paper presents a new method to promote the performance of existing saliency detection algorithms. Prior bottom-up methods predict saliency maps by combining heuristic saliency cues, which may be unreliable. To remove error outputs and preserve accurate predictions, we develop a pattern mining based saliency seeds selection method. Given initial saliency maps, our method can effectively recognize discriminative and representative saliency patterns (features), which are robust to the noise in initial maps and can more accurately distinguish foreground from background. According to the mined saliency patterns, more reliable saliency seeds can be acquired. To further propagate the saliency labels of saliency seeds to other image regions, an Extended Random Walk (ERW) algorithm is proposed. Compared with prior methods, the proposed ERW regularized by a quadratic Laplacian term ensures the diffusion of seeds information to more distant areas and allows the incorporation of external classifiers. The contributions of our method are complementary to existing methods. Extensive evaluations on four data sets show that our method can significantly improve accuracy of existing methods and achieves more superior performance than state-of-the-arts.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_35');
INSERT INTO `paper` VALUES (10212, 'Peak-Piloted Deep Network for Facial Expression Recognition', 'Facial expression recognition', 'Peak-piloted', 'Deep network', 'Peak gradient suppression', '', 'Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special-purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-of-the-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_27');
INSERT INTO `paper` VALUES (10213, 'Pedestrian Behavior Understanding and Prediction with Deep Neural Networks', 'Receptive Field', 'Association Strategy', 'Displacement Volume', 'Deep Neural Network', 'Walking Pattern', 'In this paper, a deep neural network (Behavior-CNN) is proposed to model pedestrian behaviors in crowded scenes, which has many applications in surveillance. A pedestrian behavior encoding scheme is designed to provide a general representation of walking paths, which can be used as the input and output of CNN. The proposed Behavior-CNN is trained with real-scene crowd data and then thoroughly investigated from multiple aspects, including the location map and location awareness property, semantic meanings of learned filters, and the influence of receptive fields on behavior modeling. Multiple applications, including walking path prediction, destination prediction, and tracking, demonstrate the effectiveness of Behavior-CNN on pedestrian behavior modeling.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_16');
INSERT INTO `paper` VALUES (10214, 'Perceptual Losses for Real-Time Style Transfer and Super-Resolution', 'Style transfer', 'Super-resolution', 'Deep learning', '', '', 'We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_43');
INSERT INTO `paper` VALUES (10215, 'Person Re-Identification by Unsupervised \\(\\ell _1\\) Graph Learning', 'Unsupervised person Re-ID', 'Dictionary learning', 'Robust graph regularisation', 'Graph learning', '', 'Most existing person re-identification (Re-ID) methods are based on supervised learning of a discriminative distance metric. They thus require a large amount of labelled training image pairs which severely limits their scalability. In this work, we propose a novel unsupervised Re-ID approach which requires no labelled training data yet is able to capture discriminative information for cross-view identity matching. Our model is based on a new graph regularised dictionary learning algorithm. By introducing a \\(\\ell _1\\)-norm graph Laplacian term, instead of the conventional squared \\(\\ell _2\\)-norm, our model is robust against outliers caused by dramatic changes in background, pose, and occlusion typical in a Re-ID scenario. Importantly we propose to learn jointly the graph and representation resulting in further alleviation of the effects of data outliers. Experiments on four benchmark datasets demonstrate that the proposed model significantly outperforms the state-of-the-art unsupervised learning based alternatives whilst being extremely efficient to compute.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_11');
INSERT INTO `paper` VALUES (10216, 'Person Re-identification via Recurrent Feature Aggregation', 'Person re-identification', 'Feature fusion', 'Long short term memory networks', '', '', 'We address the person re-identification problem by effectively exploiting a globally discriminative feature representation from a sequence of tracked human regions/patches. This is in contrast to previous person re-id works, which rely on either single frame based person to person patch matching, or graph based sequence to sequence matching. We show that a progressive/sequential fusion framework based on long short term memory (LSTM) network aggregates the frame-wise human region representation at each time stamp and yields a sequence level human feature representation. Since LSTM nodes can remember and propagate previously accumulated good features and forget newly input inferior ones, even with simple hand-crafted features, the proposed recurrent feature aggregation network (RFA-Net) is effective in generating highly discriminative sequence level human representations. Extensive experimental results on two person re-identification benchmarks demonstrate that the proposed method performs favorably against state-of-the-art person re-identification methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_42');
INSERT INTO `paper` VALUES (10217, 'Phase-Based Modification Transfer for Video', 'Phase-based method', 'Video processing', 'Edit propagation', '', '', 'We present a novel phase-based method for propagating modifications of one video frame to an entire sequence. Instead of computing accurate pixel correspondences between frames, e.g. extracting sparse features or optical flow, we use the assumption that small motion can be represented as the phase shift of individual pixels. In order to successfully apply this idea to transferring image edits, we propose a correction algorithm, which adapts the phase shift as well as the amplitude of the modified images. As our algorithm avoids expensive global optimization and all computational steps are performed per-pixel, it allows for a simple and efficient implementation. We evaluate the flexibility of the approach by applying it to various types of image modifications, ranging from compositing and colorization to image filters.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_39');
INSERT INTO `paper` VALUES (10218, 'Photo Aesthetics Ranking Network with Attributes and Content Adaptation', 'Convolutional neural network', 'Image aesthetics rating', 'Rank loss', 'Attribute learning', '', 'Real-world applications could benefit from the ability to automatically generate a fine-grained ranking of photo aesthetics. However, previous methods for image aesthetics analysis have primarily focused on the coarse, binary categorization of images into high- or low-aesthetic categories. In this work, we propose to learn a deep convolutional neural network to rank photo aesthetics in which the relative ranking of photo aesthetics are directly modeled in the loss function. Our model incorporates joint learning of meaningful photographic attributes and image content information which can help regularize the complicated photo aesthetics rating problem.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_40');
INSERT INTO `paper` VALUES (10219, 'Photometric Stereo Under Non-uniform Light Intensities and Exposures', 'Photometric stereo', 'Shape estimation', 'Unknown light intensity and exposure', 'Surface normal', '', 'This paper studies the effects of non-uniform light intensities and sensor exposures across observed images in photometric stereo. While conventional photometric stereo methods typically assume that light intensities are identical and sensor exposure is constant across observed images taken under varying lightings, these assumptions easily break down in practical settings due to individual light bulb’s characteristics and limited control over sensors. Our method explicitly models these non-uniformities and develops a method for accurately determining surface normal without affected by these factors. In addition, we show that our method is advantageous for general photometric stereo settings, where auto-exposure control is desirable. We compare our method with conventional least-squares and robust photometric stereo methods, and the experimental result shows superior accuracy of our method in this practical circumstance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_11');
INSERT INTO `paper` VALUES (10220, 'Pixelwise View Selection for Unstructured Multi-View Stereo', 'Source Image', 'Reprojection Error', 'View Selection', 'Source Patch', 'Scene Representation', 'This work presents a Multi-View Stereo system for robust and efficient dense modeling from unstructured image collections. Our core contributions are the joint estimation of depth and normal information, pixelwise view selection using photometric and geometric priors, and a multi-view geometric consistency term for the simultaneous refinement and image-based depth and normal fusion. Experiments on benchmarks and large-scale Internet photo collections demonstrate state-of-the-art performance in terms of accuracy, completeness, and efficiency.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_31');
INSERT INTO `paper` VALUES (10221, 'Playing for Data: Ground Truth from Computer Games', 'Association Rule', 'Association Rule Mining', 'Graphic Hardware', 'Visual Odometry', 'Semantic Label', 'Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we present an approach to rapidly creating pixel-accurate semantic label maps for images extracted from modern computer games. Although the source code and the internal operation of commercial games are inaccessible, we show that associations between image patches can be reconstructed from the communication between the game and the graphics hardware. This enables rapid propagation of semantic labels within and across images synthesized by the game, with no access to the source code or the content. We validate the presented approach by producing dense pixel-level semantic annotations for 25 thousand images synthesized by a photorealistic open-world computer game. Experiments on semantic segmentation datasets show that using the acquired data to supplement real-world images significantly increases accuracy and that the acquired data enables reducing the amount of hand-labeled real-world data: models trained with game data and just \\(\\tfrac{1}{3}\\) of the CamVid training set outperform models trained on the complete CamVid training set.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_7');
INSERT INTO `paper` VALUES (10222, 'Polysemous Codes', 'Asymmetric Distance Computation', 'Hamming Distance', 'Product Quantization (PQ)', 'Approximate Nearest Neighbor', 'Distance Estimation', 'This paper considers the problem of approximate nearest neighbor search in the compressed domain. We introduce polysemous codes, which offer both the distance estimation quality of product quantization and the efficient comparison of binary codes with Hamming distance. Their design is inspired by algorithms introduced in the 90’s to construct channel-optimized vector quantizers. At search time, this dual interpretation accelerates the search. Most of the indexed vectors are filtered out with Hamming distance, letting only a fraction of the vectors to be ranked with an asymmetric distance estimator. The method is complementary with a coarse partitioning of the feature space such as the inverted multi-index. This is shown by our experiments performed on several public benchmarks such as the BIGANN dataset comprising one billion vectors, for which we report state-of-the-art results for query times below 0.3 millisecond per core. Last but not least, our approach allows the approximate computation of the k-NN graph associated with the Yahoo Flickr Creative Commons 100M, described by CNN image descriptors, in less than 8 h on a single machine.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_48');
INSERT INTO `paper` VALUES (10223, 'Pose Hashing with Microlens Arrays', 'Microlens array', 'Fiducial marker', 'Pose estimation', '', '', 'We design and demonstrate a passive physical object whose appearance changes to give a discrete encoding of its pose. This object is created with a microlens array that is placed on top of a black and white pattern; when viewed from a particular viewpoint, the lenses appear black or white depending on the part of the pattern that each microlens projects towards that viewpoint. We analyze different design considerations that impact the information gained from the appearance of microlens array. In addition, we introduce the process through which the discrete microlens pattern can be turned into a viewpoint and a pose estimate. We empirically evaluate factors that impact viewpoint and pose estimation accuracy. Finally, we compare the pose estimation accuracy of the microlens array to other related fiducial markers.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_37');
INSERT INTO `paper` VALUES (10224, 'Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks', 'Texture synthesis', 'Adversarial generative networks', '', '', '', 'This paper proposes Markovian Generative Adversarial Networks (MGANs), a method for training generative networks for efficient texture synthesis. While deep neural network approaches have recently demonstrated remarkable results in terms of synthesis quality, they still come at considerable computational costs (minutes of run-time for low-res images). Our paper addresses this efficiency issue. Instead of a numerical deconvolution in previous work, we precompute a feed-forward, strided convolutional network that captures the feature statistics of Markovian patches and is able to directly generate outputs of arbitrary dimensions. Such network can directly decode brown noise to realistic texture, or photos to artistic paintings. With adversarial training, we obtain quality comparable to recent neural texture synthesis methods. As no optimization is required at generation time, our run-time performance (0.25 M pixel images at 25 Hz) surpasses previous neural texture synthesizers by a significant margin (at least 500 times faster). We apply this idea to texture synthesis, style transfer, and video stylization.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_43');
INSERT INTO `paper` VALUES (10225, 'Projective Bundle Adjustment from Arbitrary Initialization Using the Variable Projection Method', 'Projective bundle adjustment', 'Variable Projection', 'Nonlinear least squares', '', '', 'Bundle adjustment is used in structure-from-motion pipelines as final refinement stage requiring a sufficiently good initialization to reach a useful local mininum. Starting from an arbitrary initialization almost always gets trapped in a poor minimum. In this work we aim to obtain an initialization-free approach which returns global minima from a large proportion of purely random starting points. Our key inspiration lies in the success of the Variable Projection (VarPro) method for affine factorization problems, which have close to 100 % chance of reaching a global minimum from random initialization. We find empirically that this desirable behaviour does not directly carry over to the projective case, and we consequently design and evaluate strategies to overcome this limitation. Also, by unifying the affine and the projective camera settings, we obtain numerically better conditioned reformulations of original bundle adjustment algorithms.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_29');
INSERT INTO `paper` VALUES (10226, 'Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera', '6-DoF tracking', '3D reconstruction', 'Intensity reconstruction', 'Visual odometry', 'SLAM', 'We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_21');
INSERT INTO `paper` VALUES (10227, 'Real-Time Joint Tracking of a Hand Manipulating an Object from RGB-D Input', 'Random Forest', 'Augmented Reality', 'Gaussian Mixture Model', 'Leap Motion', 'Part Label', 'Real-time simultaneous tracking of hands manipulating and interacting with external objects has many potential applications in augmented reality, tangible computing, and wearable computing. However, due to difficult occlusions, fast motions, and uniform hand appearance, jointly tracking hand and object pose is more challenging than tracking either of the two separately. Many previous approaches resort to complex multi-camera setups to remedy the occlusion problem and often employ expensive segmentation and optimization steps which makes real-time tracking impossible. In this paper, we propose a real-time solution that uses a single commodity RGB-D camera. The core of our approach is a 3D articulated Gaussian mixture alignment strategy tailored to hand-object tracking that allows fast pose optimization. The alignment energy uses novel regularizers to address occlusions and hand-object contacts. For added robustness, we guide the optimization with discriminative part classification of the hand and segmentation of the object. We conducted extensive experiments on several existing datasets and introduce a new annotated hand-object dataset. Quantitative and qualitative results show the key advantages of our method: speed, accuracy, and robustness.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_19');
INSERT INTO `paper` VALUES (10228, 'Real-Time Monocular Segmentation and Pose Tracking of Multiple Objects', 'Tracking', 'Segmentation', 'Real-time', 'Monocular', 'Pose estimation', 'We present a real-time system capable of segmenting multiple 3D objects and tracking their pose using a single RGB camera, based on prior shape knowledge. The proposed method uses twist-coordinates for pose parametrization and a pixel-wise second-order optimization approach which lead to major improvements in terms of tracking robustness, especially in cases of fast motion and scale changes, compared to previous region-based approaches. Our implementation runs at about 50–100 Hz on a commodity laptop when tracking a single object without relying on GPGPU computations. We compare our method to the current state of the art in various experiments involving challenging motion sequences and different complex objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_26');
INSERT INTO `paper` VALUES (10229, 'Real-Time RGB-D Activity Prediction by Soft Regression', 'Activity prediction', 'RGB-D', 'Soft regression', '', '', 'In this paper, we propose a novel approach for predicting ongoing activities captured by a low-cost depth camera. Our approach avoids a usual assumption in existing activity prediction systems that the progress level of ongoing sequence is given. We overcome this limitation by learning a soft label for each subsequence and develop a soft regression framework for activity prediction to learn both predictor and soft labels jointly. In order to make activity prediction work in a real-time manner, we introduce a new RGB-D feature called “local accumulative frame feature (LAFF)”, which can be computed efficiently by constructing an integral feature map. Our experiments on two RGB-D benchmark datasets demonstrate that the proposed regression-based activity prediction model outperforms existing models significantly and also show that the activity prediction on RGB-D sequence is more accurate than that on RGB channel.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_17');
INSERT INTO `paper` VALUES (10230, 'Recognition from Hand Cameras: A Revisit with Deep Learning', 'Activity recognition', 'Wearable camera', '', '', '', 'We revisit the study of a wrist-mounted camera system (referred to as HandCam) for recognizing activities of hands. HandCam has two unique properties as compared to egocentric systems (referred to as HeadCam): (1) it avoids the need to detect hands; (2) it more consistently observes the activities of hands. By taking advantage of these properties, we propose a deep-learning-based method to recognize hand states (free vs. active hands, hand gestures, object categories), and discover object categories. Moreover, we propose a novel two-streams deep network to further take advantage of both HandCam and HeadCam. We have collected a new synchronized HandCam and HeadCam dataset with 20 videos captured in three scenes for hand states recognition. Experiments show that our HandCam system consistently outperforms a deep-learning-based HeadCam method (with estimated manipulation regions) and a dense-trajectory-based HeadCam method in all tasks. We also show that HandCam videos captured by different users can be easily aligned to improve free vs. active recognition accuracy (\\(3.3\\,\\%\\) improvement) in across-scenes use case. Moreover, we observe that finetuning Convolutional Neural Network consistently improves accuracy. Finally, our novel two-streams deep network combining HandCam and HeadCam achieves the best performance in four out of five tasks. With more data, we believe a joint HandCam and HeadCam system can robustly log hand states in daily life.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_31');
INSERT INTO `paper` VALUES (10231, 'Recurrent Instance Segmentation', 'Instance segmentation', 'Recurrent neural nets', 'Deep learning', '', '', 'Instance segmentation is the problem of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation approaches consist of ensembles of modules that are trained independently of each other, thus missing opportunities for joint learning. Here we propose a new instance segmentation paradigm consisting in an end-to-end method that learns how to segment instances sequentially. The model is based on a recurrent neural network that sequentially finds objects and their segmentations one at a time. This net is provided with a spatial memory that keeps track of what pixels have been explained and allows occlusion handling. In order to train the model we designed a principled loss function that accurately represents the properties of the instance segmentation problem. In the experiments carried out, we found that our method outperforms recent approaches on multiple person segmentation, and all state of the art approaches on the Plant Phenotyping dataset for leaf counting.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_19');
INSERT INTO `paper` VALUES (10232, 'Recurrent Temporal Deep Field for Semantic Video Labeling', 'Video labeling', 'Recurrent Temporal Deep Field', 'Recurrent Temporal Restricted Boltzmann Machine', 'Deconvolution', 'CRF', 'This paper specifies a new deep architecture, called Recurrent Temporal Deep Field (RTDF), for semantic video labeling. RTDF is a conditional random field (CRF) that combines a deconvolution neural network (DeconvNet) and a recurrent temporal restricted Boltzmann machine (RTRBM). DeconvNet is grounded onto pixels of a new frame for estimating the unary potential of the CRF. RTRBM estimates a high-order potential of the CRF by capturing long-term spatiotemporal dependencies of pixel labels that RTDF has already predicted in previous frames. We derive a mean-field inference algorithm to jointly predict all latent variables in both RTRBM and CRF. We also conduct end-to-end joint training of all DeconvNet, RTRBM, and CRF parameters. The joint learning and inference integrate the three components into a unified deep model – RTDF. Our evaluation on the benchmark Youtube Face Database (YFDB) and Cambridge-driving Labeled Video Database (Camvid) demonstrates that RTDF outperforms the state of the art both qualitatively and quantitatively.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_19');
INSERT INTO `paper` VALUES (10233, 'Reflection Symmetry Detection via Appearance of Structure Descriptor', 'Symmetry detection', 'Structure', 'Feature', 'Reflection', '', 'Symmetry in visual data represents repeated patterns or shapes that is easily found in natural and human-made objects. Symmetry pattern on an object works as a salient visual feature attracting human attention and letting the object to be easily recognized. Most existing symmetry detection methods are based on sparsely detected local features describing the appearance of their neighborhood, which have difficulty in capturing object structure mostly supported by edges and contours. In this work, we propose a new reflection symmetry detection method extracting robust 4-dimensional Appearance of Structure descriptors based on a set of outstanding neighbourhood edge segments in multiple scales. Our experimental evaluations on multiple public symmetry detection datasets show promising reflection symmetry detection results on challenging real world and synthetic images.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_1');
INSERT INTO `paper` VALUES (10234, 'Region-Based Semantic Segmentation with End-to-End Training', 'Object Boundary', 'Convolutional Neural Network', 'Stochastic Gradient Descent', 'Global Accuracy', 'Convolutional Layer', 'We propose a novel method for semantic segmentation, the task of labeling each pixel in an image with a semantic class. Our method combines the advantages of the two main competing paradigms. Methods based on region classification offer proper spatial support for appearance measurements, but typically operate in two separate stages, none of which targets pixel labeling performance at the end of the pipeline. More recent fully convolutional methods are capable of end-to-end training for the final pixel labeling, but resort to fixed patches as spatial support. We show how to modify modern region-based approaches to enable end-to-end training for semantic segmentation. This is achieved via a differentiable region-to-pixel layer and a differentiable free-form Region-of-Interest pooling layer. Our method improves the state-of-the-art in terms of class-average accuracy with \\(64.0\\,\\%\\) on SIFT Flow and \\(49.9\\,\\%\\) on PASCAL Context, and is particularly accurate at object boundaries.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_23');
INSERT INTO `paper` VALUES (10235, 'Reliable Attribute-Based Object Recognition Using High Predictive Value Classifiers', 'Point Cloud', 'Positive Predictive Value', 'Negative Predictive Value', 'Object Recognition', 'Attribute Classifier', 'We consider the problem of object recognition in 3D using an ensemble of attribute-based classifiers. We propose two new concepts to improve classification in practical situations, and show their implementation in an approach implemented for recognition from point-cloud data. First, the viewing conditions can have a strong influence on classification performance. We study the impact of the distance between the camera and the object and propose an approach to fusing multiple attribute classifiers, which incorporates distance into the decision making. Second, lack of representative training samples often makes it difficult to learn the optimal threshold value for best positive and negative detection rate. We address this issue, by setting in our attribute classifiers instead of just one threshold value, two threshold values to distinguish a positive, a negative and an uncertainty class, and we prove the theoretical correctness of this approach. Empirical studies demonstrate the effectiveness and feasibility of the proposed concepts.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_49');
INSERT INTO `paper` VALUES (10236, 'RepMatch: Robust Feature Matching and Pose for Reconstructing Modern Cities', 'Structure from motion', 'Correspondence', 'RANSAC', '', '', 'A perennial problem in recovering 3-D models from images is repeated structures common in modern cities. The problem can be traced to the feature matcher which needs to match less distinctive features (permitting wide-baselines and avoiding broken sequences), while simultaneously avoiding incorrect matching of ambiguous repeated features. To meet this need, we develop RepMatch, an epipolar guided (assumes predominately camera motion) feature matcher that accommodates both wide-baselines and repeated structures. RepMatch is based on using RANSAC to guide the training of match consistency curves for differentiating true and false matches. By considering the set of all nearest-neighbor matches, RepMatch can procure very large numbers of matches over wide baselines. This in turn lends stability to pose estimation. RepMatch’s performance compares favorably on standard datasets and enables more complete reconstructions of modern architectures.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_34');
INSERT INTO `paper` VALUES (10237, 'Resonant Deformable Matching: Simultaneous Registration and Reconstruction', 'Graph Match', 'Thin Plate Spline', 'Signed Distance Function', 'Density Function Representation', 'Scalar Field Representation', 'In the past decade we have seen the emergence of many efficient algorithms for estimating non-rigid deformations registering a template to target features. Registration of density functions is particularly popular. In contrast to the success enjoyed by the density function representation, we have not seen similar success with the signed distance function representation. Resonant deformable matching (RDM) simultaneously estimates a non-rigid deformation and a set of unknown target normal directions by registering fields comprising signed distance and probability density information. Resonance occurs as the reconstruction estimate comes into agreement with the registered template. We perform experiments probing two problems: point-set registration and normal estimation. RDM compares favorably to top tier point registration and graph algorithms in terms of registration and reconstruction metrics.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_4');
INSERT INTO `paper` VALUES (10238, 'Revisiting Additive Quantization', 'Local Search', 'Query Time', 'Local Search Procedure', 'Inverted Index', 'Iterate Local Search', 'We revisit Additive Quantization (AQ), an approach to vector quantization that uses multiple, full-dimensional, and non-orthogonal codebooks. Despite its elegant and simple formulation, AQ has failed to achieve state-of-the-art performance on standard retrieval benchmarks, because the encoding problem, which amounts to MAP inference in multiple fully-connected Markov Random Fields (MRFs), has proven to be hard to solve. We demonstrate that the performance of AQ can be improved to surpass the state of the art by leveraging iterated local search, a stochastic local search approach known to work well for a range of NP-hard combinatorial problems. We further show a direct application of our approach to a recent formulation of vector quantization that enforces sparsity of the codebooks. Unlike previous work, which required specialized optimization techniques, our formulation can be plugged directly into state-of-the-art lasso optimizers. This results in a conceptually simple, easily implemented method that outperforms the previous state of the art in solving sparse vector quantization. Our implementation is publicly available (https://github.com/jltmtz/local-search-quantization).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_9');
INSERT INTO `paper` VALUES (10239, 'RNN Fisher Vectors for Action Recognition and Image Annotation', 'Action recognition', 'Image annotation', 'Fisher vectors', 'Recurrent Neural Networks', '', 'Recurrent Neural Networks (RNNs) have had considerable success in classifying and predicting sequences. We demonstrate that RNNs can be effectively used in order to encode sequences and provide effective representations. The methodology we use is based on Fisher Vectors, where the RNNs are the generative probabilistic models and the partial derivatives are computed using backpropagation. State of the art results are obtained in two central but distant tasks, which both rely on sequences: video action recognition and image annotation. We also show a surprising transfer learning result from the task of image annotation to the task of video action recognition.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_50');
INSERT INTO `paper` VALUES (10240, 'Robust Face Alignment Using a Mixture of Invariant Experts', '', '', '', '', '', 'Face alignment, which is the task of finding the locations of a set of facial landmark points in an image of a face, is useful in widespread application areas. Face alignment is particularly challenging when there are large variations in pose (in-plane and out-of-plane rotations) and facial expression. To address this issue, we propose a cascade in which each stage consists of a mixture of regression experts. Each expert learns a customized regression model that is specialized to a different subset of the joint space of pose and expressions. The system is invariant to a predefined class of transformations (e.g., affine), because the input is transformed to match each expert’s prototype shape before the regression is applied. We also present a method to include deformation constraints within the discriminative alignment framework, which makes our algorithm more robust. Our algorithm significantly outperforms previous methods on publicly available face alignment datasets.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_50');
INSERT INTO `paper` VALUES (10241, 'Robust Facial Landmark Detection via Recurrent Attentive-Refinement Networks', 'Facial landmark detection', 'Occlusion', 'Face alignment', 'Recurrent neural network', '', 'In this work, we introduce a novel Recurrent Attentive-Refinement (RAR) network for facial landmark detection under unconstrained conditions, suffering from challenges like facial occlusions and/or pose variations. RAR follows the pipeline of cascaded regressions that refines landmark locations progressively. However, instead of updating all the landmark locations together, RAR refines the landmark locations sequentially at each recurrent stage. In this way, more reliable landmark points are refined earlier and help to infer locations of other challenging landmarks that may stay with occlusions and/or extreme poses. RAR can thus effectively control detection errors from those challenging landmarks and improve overall performance even in presence of heavy occlusions and/or extreme conditions. To determine the sequence of landmarks, RAR employs an attentive-refinement mechanism. The attention LSTM (A-LSTM) and refinement LSTM (R-LSTM) models are introduced in RAR. At each recurrent stage, A-LSTM implicitly identifies a reliable landmark as the attention center. Following the sequence of attention centers, R-LSTM sequentially refines the landmarks near or correlated with the attention centers and provides ultimate detection results finally. To further enhance algorithmic robustness, instead of using mean shape for initialization, RAR adaptively determines the initialization by selecting from a pool of shape centers clustered from all training shapes. As an end-to-end trainable model, RAR demonstrates superior performance in detecting challenging landmarks in comprehensive experiments and it also establishes new state-of-the-arts on the 300-W, COFW and AFLW benchmark datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_4');
INSERT INTO `paper` VALUES (10242, 'Robust Image and Video Dehazing with Visual Artifact Suppression via Gradient Residual Minimization', 'Video dehazing', 'Image dehazing', 'Contrast enhancement', 'Artifact suppression', '', 'Most existing image dehazing methods tend to boost local image contrast for regions with heavy haze. Without special treatment, these methods may significantly amplify existing image artifacts such as noise, color aliasing and blocking, which are mostly invisible in the input images but are visually intruding in the results. This is especially the case for low quality cellphone shots or compressed video frames. The recent work of Li et al. (2014) addresses blocking artifacts for dehazing, but is insufficient to handle other artifacts. In this paper, we propose a new method for reliable suppression of different types of visual artifacts in image and video dehazing. Our method makes contributions in both the haze estimation step and the image recovery step. Firstly, an image-guided, depth-edge-aware smoothing algorithm is proposed to refine the initial atmosphere transmission map generated by local priors. In the image recovery process, we propose Gradient Residual Minimization (GRM) for jointly recovering the haze-free image while explicitly minimizing possible visual artifacts in it. Our evaluation suggests that the proposed method can generate results with much less visual artifacts than previous approaches for lower quality inputs such as compressed video clips.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_36');
INSERT INTO `paper` VALUES (10243, 'Saliency Detection with Recurrent Fully Convolutional Networks', 'Saliency detection', 'Recurrent fully convolutional network', '', '', '', 'Deep networks have been proved to encode high level semantic features and delivered superior performance in saliency detection. In this paper, we go one step further by developing a new saliency model using recurrent fully convolutional networks (RFCNs). Compared with existing deep network based methods, the proposed network is able to incorporate saliency prior knowledge for more accurate inference. In addition, the recurrent architecture enables our method to automatically learn to refine the saliency map by correcting its previous errors. To train such a network with numerous parameters, we propose a pre-training strategy using semantic segmentation data, which simultaneously leverages the strong supervision of segmentation tasks for better training and enables the network to capture generic representations of objects for saliency detection. Through extensive experimental evaluations, we demonstrate that the proposed method compares favorably against state-of-the-art approaches, and that the proposed recurrent deep model as well as the pre-training method can significantly improve performance.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_50');
INSERT INTO `paper` VALUES (10244, 'Salient Deconvolutional Networks', 'DeConvNets', 'Deep convolutional neural networks', 'Saliency', 'Segmentation', '', 'Deconvolution is a popular method for visualizing deep convolutional neural networks; however, due to their heuristic nature, the meaning of deconvolutional visualizations is not entirely clear. In this paper, we introduce a family of reversed networks that generalizes and relates deconvolution, backpropagation and network saliency. We use this construction to thoroughly investigate and compare these methods in terms of quality and meaning of the produced images, and of what architectural choices are important in determining these properties. We also show an application of these generalized deconvolutional networks to weakly-supervised foreground object segmentation.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_8');
INSERT INTO `paper` VALUES (10245, 'Scalable Metric Learning via Weighted Approximate Rank Component Analysis', 'Metric learning', 'Orthonormal regularizer', 'Person re-identification', '', '', 'We are interested in the large-scale learning of Mahalanobis distances, with a particular focus on person re-identification. We propose a metric learning formulation called Weighted Approximate Rank Component Analysis (WARCA). WARCA optimizes the precision at top ranks by combining the WARP loss with a regularizer that favors orthonormal linear mappings and avoids rank-deficient embeddings. Using this new regularizer allows us to adapt the large-scale WSABIE procedure and to leverage the Adam stochastic optimization algorithm, which results in an algorithm that scales gracefully to very large data-sets. Also, we derive a kernelized version which allows to take advantage of state-of-the-art features for re-identification when data-set size permits kernel computation. Benchmarks on recent and standard re-identification data-sets show that our method beats existing state-of-the-art techniques both in terms of accuracy and speed. We also provide experimental analysis to shade lights on the properties of the regularizer we use, and how it improves performance.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_53');
INSERT INTO `paper` VALUES (10246, 'Scene Depth Profiling Using Helmholtz Stereopsis', '3D reconstruction', 'Depth profiling', 'Stereopsis', 'Helmholtz reciprocity', '', 'Helmholtz stereopsis is a 3D reconstruction technique, capturing surface depth independent of the reflection properties of the material by using Helmholtz reciprocity. In this paper we are interested in studying the applicability of Helmholtz stereopsis for surface and depth profiling of objects and general scenes in the context of perspective stereo imaging. Helmholtz stereopsis captures a pair of reciprocal images by exchanging the position of light source and camera. The resulting image pair relates the image intensities and scene depth profile by a partial differential equation. The solution of this differential equation depends on the boundary conditions provided by the scene. We propose to limit the illumination angle of the light source, such that only mutually visible parts are imaged, resulting in stable boundary conditions. By simulation and experiment we show that a unique depth profile can be recovered for a large class of scenes including multiple occluding objects.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_28');
INSERT INTO `paper` VALUES (10247, 'SDF-2-SDF: Highly Accurate 3D Object Reconstruction', 'Object reconstruction', 'Signed distance field', 'RGB-D sensors', '', '', 'This paper addresses the problem of 3D object reconstruction using RGB-D sensors. Our main contribution is a novel implicit-to-implicit surface registration scheme between signed distance fields (SDFs), utilized both for the real-time frame-to-frame camera tracking and for the subsequent global optimization. SDF-2-SDF registration circumvents expensive correspondence search and allows for incorporation of multiple geometric constraints without any dependence on texture, yielding highly accurate 3D models. An extensive quantitative evaluation on real and synthetic data demonstrates improved tracking and higher fidelity reconstructions than a variety of state-of-the-art methods. We make our data publicly available, creating the first object reconstruction dataset to include ground-truth CAD models and RGB-D sequences from sensors of various quality.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_41');
INSERT INTO `paper` VALUES (10248, 'SEAGULL: Seam-Guided Local Alignment for Parallax-Tolerant Image Stitching', 'Target Image', 'Feature Match', 'Mesh Vertex', 'Warping Model', 'Image Stitching', 'Image stitching with large parallax is a challenging problem. Global alignment usually introduces noticeable artifacts. A common strategy is to perform partial alignment to facilitate the search for a good seam for stitching. Different from existing approaches where the seam estimation process is performed sequentially after alignment, we explicitly use the estimated seam to guide the process of optimizing local alignment so that the seam quality gets improved over each iteration. Furthermore, a novel structure-preserving warping method is introduced to preserve salient curve and line structures during the warping. These measures substantially improve the effectiveness of our method in dealing with a wide range of challenging images with large parallax.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_23');
INSERT INTO `paper` VALUES (10249, 'Search-Based Depth Estimation via Coupled Dictionary Learning with Large-Margin Structure Inference', 'Single image depth estimation', 'Cross-modality retrieval', 'Coupled dictionary learning', 'Contextual refinement', '', 'Depth estimation from a single image is an emerging topic in computer vision and beyond. To this end, the existing works typically train a depth regressor from visual appearance. However, the state-of-the-art performance of these schemes is still far from satisfactory, mainly because of the over-fitting and under-fitting problems in regressor training. In this paper, we offer a different data-driven paradigm of estimating depth from a single image, which formulates depth estimation from a search-based perspective. In particular, we handle the depth estimation of local patches via a novel cross-modality retrieval scheme, which searches for the 3D patches with similar structure/appearance to the 2D query from a dataset with 2D-3D mappings. To that effect, a coupled dictionary learning formulation is proposed to link the 2D query with the 3D patches, on the reconstruction coefficients to capture the cross-modality similarity, to obtain a rough depth estimation locally. In addition, consistency on spatial context is further introduced to refine the local depth estimation using a Conditional Random Field. We demonstrate the efficacy of the proposed method by comparing it with the state-of-the-art approaches on popular public datasets such as Make3D and NYUv2, upon which significant performance gains are reported.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_52');
INSERT INTO `paper` VALUES (10250, 'Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation', 'Weakly-supervised image segmentation', 'Deep learning', '', '', '', 'We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_42');
INSERT INTO `paper` VALUES (10251, 'Segmental Spatiotemporal CNNs for Fine-Grained Action Segmentation', 'Action Recognition', 'Recurrent Neural Network', 'Spatial Unit', 'Dense Trajectory', 'Temporal Segmentation', 'Joint segmentation and classification of fine-grained actions is important for applications of human-robot interaction, video surveillance, and human skill evaluation. However, despite substantial recent progress in large-scale action classification, the performance of state-of-the-art fine-grained action recognition approaches remains low. We propose a model for action segmentation which combines low-level spatiotemporal features with a high-level segmental classifier. Our spatiotemporal CNN is comprised of a spatial component that represents relationships between objects and a temporal component that uses large 1D convolutional filters to capture how object relationships change across time. These features are used in tandem with a semi-Markov model that captures transitions from one action to another. We introduce an efficient constrained segmental inference algorithm for this model that is orders of magnitude faster than the current approach. We highlight the effectiveness of our Segmental Spatiotemporal CNN on cooking and surgical action datasets for which we observe substantially improved performance relative to recent baseline methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_3');
INSERT INTO `paper` VALUES (10252, 'Segmentation from Natural Language Expressions', 'Natural language', 'Segmentation', 'Recurrent neural network', 'Fully convolutional network', '', 'In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase “two men sitting on the right bench” requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and/or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent neural network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_7');
INSERT INTO `paper` VALUES (10253, 'Semantic 3D Reconstruction of Heads', 'Face', 'Head', 'Semantic', 'Multi-label', 'Shape prior', 'We present a novel approach that jointly reconstructs the geometry of a human head and semantically segments it into labels such as skin, hair and eyebrows. In order to get faithful reconstructions from data captured in uncontrolled environments, we propose to adapt a recently introduced implicit volumetric surface normal based shape prior formulation. Shape prior based approaches critically rely on an accurate alignment between the data and the prior to succeed. To this end, we propose an automatic alignment procedure for the used shape prior formulation. We evaluate our alignment procedure thoroughly and show head reconstruction results on challenging datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_40');
INSERT INTO `paper` VALUES (10254, 'Semantic Clustering for Robust Fine-Grained Scene Recognition', 'Training Image', 'Flower Shop', 'Target Domain', 'Source Domain', 'Scene Image', 'In domain generalization, the knowledge learnt from one or multiple source domains is transferred to an unseen target domain. In this work, we propose a novel domain generalization approach for fine-grained scene recognition. We first propose a semantic scene descriptor that jointly captures the subtle differences between fine-grained scenes, while being robust to varying object configurations across domains. We model the occurrence patterns of objects in scenes, capturing the informativeness and discriminability of each object for each scene. We then transform such occurrences into scene probabilities for each scene image. Second, we argue that scene images belong to hidden semantic topics that can be discovered by clustering our semantic descriptors. To evaluate the proposed method, we propose a new fine-grained scene dataset in cross-domain settings. Extensive experiments on the proposed dataset and three benchmark scene datasets show the effectiveness of the proposed approach for fine-grained scene transfer, where we outperform state-of-the-art scene recognition and domain generalization methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_47');
INSERT INTO `paper` VALUES (10255, 'Semantic Co-segmentation in Videos', 'Submodular Function', 'Semantic Object Extraction', 'Video Object Segmentation', 'Visual Semantics', 'Tracklet Generation', 'Discovering and segmenting objects in videos is a challenging task due to large variations of objects in appearances, deformed shapes and cluttered backgrounds. In this paper, we propose to segment objects and understand their visual semantics from a collection of videos that link to each other, which we refer to as semantic co-segmentation. Without any prior knowledge on videos, we first extract semantic objects and utilize a tracking-based approach to generate multiple object-like tracklets across the video. Each tracklet maintains temporally connected segments and is associated with a predicted category. To exploit rich information from other videos, we collect tracklets that are assigned to the same category from all videos, and co-select tracklets that belong to true objects by solving a submodular function. This function accounts for object properties such as appearances, shapes and motions, and hence facilitates the co-segmentation process. Experiments on three video object segmentation datasets show that the proposed algorithm performs favorably against the other state-of-the-art methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_46');
INSERT INTO `paper` VALUES (10256, 'Semantic Object Parsing with Graph LSTM', 'Object parsing', 'Graph LSTM', 'Recurrent neural networks', '', '', 'By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_8');
INSERT INTO `paper` VALUES (10257, 'Semi-supervised Learning Based on Joint Diffusion of Graph Functions and Laplacians', 'Semi-supervised learning', 'Graph Laplacian', 'Diffusion', 'Regularization', '', 'We observe the distances between estimated function outputs on data points to create an anisotropic graph Laplacian which, through an iterative process, can itself be regularized. Our algorithm is instantiated as a discrete regularizer on a graph’s diffusivity operator. This idea is grounded in the theory that regularizing the diffusivity operator corresponds to regularizing the metric on Riemannian manifolds, which further corresponds to regularizing the anisotropic Laplace-Beltrami operator. We show that our discrete regularization framework is consistent in the sense that it converges to (continuous) regularization on underlying data generating manifolds. In semi-supervised learning experiments, across ten standard datasets, our diffusion of Laplacian approach has the lowest average error rate of eight different established and state-of-the-art approaches, which shows the promise of our approach.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_43');
INSERT INTO `paper` VALUES (10258, 'Shading-Aware Multi-view Stereo', 'Surface Albedo', 'Geometric Error', 'Image Gradient', 'Surface Representation', 'Photometric Stereo', 'We present a novel multi-view reconstruction approach that effectively combines stereo and shape-from-shading energies into a single optimization scheme. Our method uses image gradients to transition between stereo-matching (which is more accurate at large gradients) and Lambertian shape-from-shading (which is more robust in flat regions). In addition, we show that our formulation is invariant to spatially varying albedo without explicitly modeling it. We show that the resulting energy function can be optimized efficiently using a smooth surface representation based on bicubic patches, and demonstrate that this algorithm outperforms both previous multi-view stereo algorithms and shading based refinement approaches on a number of datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_29');
INSERT INTO `paper` VALUES (10259, 'Shape Acquisition and Registration for 3D Endoscope Based on Grid Pattern Projection', 'Sparse Grid', 'Grid Pattern', 'Horizontal Edge', 'Epipolar Line', 'Pattern Projector', 'For effective endoscopic diagnosis and treatment, size measurement and shape characterization of lesions, such as tumors, is important. For this purpose, 3D endoscopic systems based on active stereo to measure the shape and size of living tissue have recently been proposed. In those works, a large problem is the degree of reconstruction instability due to image blurring caused by the strong subsurface scattering common to internal tissue. To reduce this instability problem, using a coarse pattern for structured light is an option, however it reduces the resolution of the acquired shape information. In this paper, we tackle these shortcomings by developing a new micro pattern laser projector to be inserted in the scope tool channel. There are hardware and software contributions in the paper. First, the new projector uses a Diffractive Optical Element (DOE) instead of a single lens which we proposed to solve the off-focus blur. Second, we propose a new line-based grid pattern with gap coding to counter the subsurface scattering effect. The proposed pattern is a coarse grid pattern so that the grid features are not blurred out by the subsurface scattering. Third, to increase shape resolution of line-based grid pattern, we propose to use a multiple shape data registration technique for the grid-structured shapes, which are acquired sequentially by small motions, is proposed. Quantitative experiments are conducted to show the effectiveness of the method followed by a demonstration using real endoscopic system.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_24');
INSERT INTO `paper` VALUES (10260, 'Shape from Selfies: Human Body Shape Estimation Using CCA Regression Forests', 'Body Shape', 'Canonical Correlation Analysis', 'Shape Estimation', 'Kernel Canonical Correlation Analysis', 'Template Mesh', 'In this work, we revise the problem of human body shape estimation from monocular imagery. Starting from a statistical human shape model that describes a body shape with shape parameters, we describe a novel approach to automatically estimate these parameters from a single input shape silhouette using semi-supervised learning. By utilizing silhouette features that encode local and global properties robust to noise, pose and view changes, and projecting them to lower dimensional spaces obtained through multi-view learning with canonical correlation analysis, we show how regression forests can be used to compute an accurate mapping from the silhouette to the shape parameter space. This results in a very fast, robust and automatic system under mild self-occlusion assumptions. We extensively evaluate our method on thousands of synthetic and real data and compare it to the state-of-art approaches that operate under more restrictive assumptions.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_6');
INSERT INTO `paper` VALUES (10261, 'Shape from Water: Bispectral Light Absorption for Depth Recovery', 'Depth recovery', 'Light absorption', 'Multispectral imaging', '', '', 'This paper introduces a novel depth recovery method based on light absorption in water. Water absorbs light at almost all wavelengths whose absorption coefficient is related to the wavelength. Based on the Beer-Lambert model, we introduce a bispectral depth recovery method that leverages the light absorption difference between two near-infrared wavelengths captured with a distant point source and orthographic cameras. Through extensive analysis, we show that accurate depth can be recovered irrespective of the surface texture and reflectance, and introduce algorithms to correct for nonidealities of a practical implementation, including tilted light source and camera placement and nonideal bandpass filters. We construct a coaxial bispectral depth imaging system using low-cost off-the-shelf hardware and demonstrate its use for recovering the shapes of complex and dynamic objects in water. Experimental results validate the theory and practical implementation of this novel depth recovery paradigm, which we refer to as shape from water.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_38');
INSERT INTO `paper` VALUES (10262, 'Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification', 'Unsupervised learning', 'Videos', 'Sequence verification', 'Action recognition', 'Pose estimation', 'In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_32');
INSERT INTO `paper` VALUES (10263, 'Similarity Registration Problems for 2D/3D Ultrasound Calibration', 'Calibration', 'Similarity registration', 'Ultrasound', 'Medical imaging', '', 'We propose a minimal solution for the similarity registration (rigid pose and scale) between two sets of 3D lines, and also between a set of co-planar points and a set of 3D lines. The first problem is solved up to 8 discrete solutions with a minimum of 2 line-line correspondences, while the second is solved up to 4 discrete solutions using 4 point-line correspondences. We use these algorithms to perform the extrinsic calibration between a pose tracking sensor and a 2D/3D ultrasound (US) curvilinear probe using a tracked needle as calibration target. The needle is tracked as a 3D line, and is scanned by the ultrasound as either a 3D line (3D US) or as a 2D point (2D US). Since the scale factor that converts US scan units to metric coordinates is unknown, the calibration is formulated as a similarity registration problem. We present results with both synthetic and real data and show that the minimum solutions outperform the correspondent non-minimal linear formulations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_11');
INSERT INTO `paper` VALUES (10264, 'Single Image 3D Interpreter Network', '3D structure', 'Single image 3D reconstruction', 'Keypoint estimation', 'Neural network', 'Synthetic data', 'Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_22');
INSERT INTO `paper` VALUES (10265, 'Single Image Dehazing via Multi-scale Convolutional Neural Networks', 'Image dehazing', 'Defogging', 'Convolutional neural network', '', '', 'The performance of existing image dehazing methods is limited by hand-designed features, such as the dark channel, color disparity and maximum contrast, with complex fusion schemes. In this paper, we propose a multi-scale deep neural network for single-image dehazing by learning the mapping between hazy images and their corresponding transmission maps. The proposed algorithm consists of a coarse-scale net which predicts a holistic transmission map based on the entire image, and a fine-scale net which refines results locally. To train the multi-scale deep network, we synthesize a dataset comprised of hazy images and corresponding transmission maps based on the NYU Depth dataset. Extensive experiments demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods on both synthetic and real-world images in terms of quality and speed.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_10');
INSERT INTO `paper` VALUES (10266, 'Smooth Neighborhood Structure Mining on Multiple Affinity Graphs with Applications to Context-Sensitive Similarity', 'Diffusion process', 'Image/shape retrieval', 'Affinity graph', '', '', 'Due to the ability of capturing geometry structures of the data manifold, diffusion process has demonstrated impressive performances in retrieval task by spreading the similarities on the affinity graph. In view of robustness to noise edges, diffusion process is usually localized, i.e., only propagating similarities via neighbors. However, selecting neighbors smoothly on graph-based manifolds is more or less ignored by previous works. In this paper, we propose a new algorithm called Smooth Neighborhood (SN) that mines the neighborhood structure to satisfy the manifold assumption. By doing so, nearby points on the underlying manifold are guaranteed to yield similar neighbors as much as possible. Moreover, SN is adjusted to tackle multiple affinity graphs by imposing a weight learning paradigm, and this is the primary difference compared with related works which are only applicable with one affinity graph. Exhausted experimental results and comparisons against other algorithms manifest the effectiveness of the proposed algorithm.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_37');
INSERT INTO `paper` VALUES (10267, 'Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition', '3D action recognition', 'Recurrent neural networks', 'Long short-term memory', 'Trust gate', 'Spatio-temporal analysis', '3D action recognition – analysis of human actions based on 3D skeleton data – becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_50');
INSERT INTO `paper` VALUES (10268, 'Spatio-Temporally Consistent Correspondence for Dense Dynamic Scene Modeling', 'Two-View correspondences', 'Motion consistency', '', '', '', 'We address the problem of robust two-view correspondence estimation within the context of dynamic scene modeling. To this end, we investigate the use of local spatio-temporal assumptions to both identify and refine dense low-level data associations in the absence of prior dynamic content models. By developing a strictly data-driven approach to correspondence search, based on bottom-up local 3D motion cues of local rigidity and non-local coherence, we are able to robustly address the higher-order problems of video synchronization and dynamic surface modeling. Our findings suggest an important relationship between these two tasks, in that maximizing spatial coherence of surface points serves as a direct metric for the temporal alignment of local image sequences. The obtained results for these two problems on multiple publicly available dynamic reconstruction datasets illustrate both the effectiveness and generality of our proposed approach.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_1');
INSERT INTO `paper` VALUES (10269, 'SPICE: Semantic Propositional Image Caption Evaluation', 'Propositional Semantics', 'Image Caption Evaluation', 'Scene Graph', 'Caption Generation Model', 'System-level Correlation', 'There is considerable interest in the task of automatically generating image captions. However, evaluation is challenging. Existing automatic evaluation metrics are primarily sensitive to n-gram overlap, which is neither necessary nor sufficient for the task of simulating human judgment. We hypothesize that semantic propositional content is an important component of human caption evaluation, and propose a new automated caption evaluation metric defined over scene graphs coined SPICE. Extensive evaluations across a range of models and datasets indicate that SPICE captures human judgments over model-generated captions better than other automatic metrics (e.g., system-level correlation of 0.88 with human judgments on the MS COCO dataset, versus 0.43 for CIDEr and 0.53 for METEOR). Furthermore, SPICE can answer questions such as which caption-generator best understands colors? and can caption-generators count?', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_24');
INSERT INTO `paper` VALUES (10270, 'Spot On: Action Localization from Pointly-Supervised Proposals', 'Action localization', 'Action proposals', '', '', '', 'We strive for spatio-temporal localization of actions in videos. The state-of-the-art relies on action proposals at test time and selects the best one with a classifier trained on carefully annotated box annotations. Annotating action boxes in video is cumbersome, tedious, and error prone. Rather than annotating boxes, we propose to annotate actions in video with points on a sparse subset of frames only. We introduce an overlap measure between action proposals and points and incorporate them all into the objective of a non-convex Multiple Instance Learning optimization. Experimental evaluation on the UCF Sports and UCF 101 datasets shows that (i) spatio-temporal proposals can be used to train classifiers while retaining the localization performance, (ii) point annotations yield results comparable to box annotations while being significantly faster to annotate, (iii) with a minimum amount of supervision our approach is competitive to the state-of-the-art. Finally, we introduce spatio-temporal action annotations on the train and test videos of Hollywood2, resulting in Hollywood2Tubes, available at http://tinyurl.com/hollywood2tubes.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_27');
INSERT INTO `paper` VALUES (10271, 'SSD: Single Shot MultiBox Detector', 'Real-time object detection', 'Convolutional neural network', '', '', '', 'We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For \\(300 \\times 300\\) input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for \\(512 \\times 512\\) input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_2');
INSERT INTO `paper` VALUES (10272, 'SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation', 'Image segmentation', 'Electron microscopy', 'Semi-supervised learning', 'Hierarchical segmentation', 'Connectomics', 'Region-based methods have proven necessary for improving segmentation accuracy of neuronal structures in electron microscopy (EM) images. Most region-based segmentation methods use a scoring function to determine region merging. Such functions are usually learned with supervised algorithms that demand considerable ground truth data, which are costly to collect. We propose a semi-supervised approach that reduces this demand. Based on a merge tree structure, we develop a differentiable unsupervised loss term that enforces consistent predictions from the learned function. We then propose a Bayesian model that combines the supervised and the unsupervised information for probabilistic learning. The experimental results on three EM data sets demonstrate that by using a subset of only \\(3\\,\\%\\) to \\(7\\,\\%\\) of the entire ground truth data, our approach consistently performs close to the state-of-the-art supervised method with the full labeled data set, and significantly outperforms the supervised method with the same labeled subset.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_9');
INSERT INTO `paper` VALUES (10273, 'Stereo Video Deblurring', 'Object motion blur', 'Scene flow', 'Spatially-variant deblurring', '', '', 'Videos acquired in low-light conditions often exhibit motion blur, which depends on the motion of the objects relative to the camera. This is not only visually unpleasing, but can hamper further processing. With this paper we are the first to show how the availability of stereo video can aid the challenging video deblurring task. We leverage 3D scene flow, which can be estimated robustly even under adverse conditions. We go beyond simply determining the object motion in two ways: First, we show how a piecewise rigid 3D scene flow representation allows to induce accurate blur kernels via local homographies. Second, we exploit the estimated motion boundaries of the 3D scene flow to mitigate ringing artifacts using an iterative weighting scheme. Being aware of 3D object motion, our approach can deal robustly with an arbitrary number of independently moving objects. We demonstrate its benefit over state-of-the-art video deblurring using quantitative and qualitative experiments on rendered scenes and real videos.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_35');
INSERT INTO `paper` VALUES (10274, 'Stochastic Dykstra Algorithms for Metric Learning with Positive Definite Covariance Descriptors', 'Covariance descriptor', 'Metric learning', 'Convex optimization', 'Stochastic optimization', 'Dykstra algorithm', 'Recently, covariance descriptors have received much attention as powerful representations of set of points. In this research, we present a new metric learning algorithm for covariance descriptors based on the Dykstra algorithm, in which the current solution is projected onto a half-space at each iteration, and runs at \\(O(n^3)\\) time. We empirically demonstrate that randomizing the order of half-spaces in our Dykstra-based algorithm significantly accelerates the convergence to the optimal solution. Furthermore, we show that our approach yields promising experimental results on pattern recognition tasks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_47');
INSERT INTO `paper` VALUES (10275, 'Streaming Video Segmentation via Short-Term Hierarchical Segmentation and Frame-by-Frame Markov Random Field Optimization', 'Video segmentation', 'Online segmentation', 'Streaming segmentation', 'Agglomerative clustering', 'Graph matching', 'An online video segmentation algorithm, based on short-term hierarchical segmentation (STHS) and frame-by-frame Markov random field (MRF) optimization, is proposed in this work. We develop the STHS technique, which generates initial segments by sliding a short window of frames. In STHS, we apply spatial agglomerative clustering to each frame, and then adopt inter-frame bipartite graph matching to construct initial segments. Then, we partition each frame into final segments, by minimizing an MRF energy function composed of unary and pairwise costs. We compute the unary cost using the STHS initial segments and the segmentation result at the previous frame. We set the pairwise cost to encourage similar nodes to have the same segment label. Experimental results on a video segmentation benchmark dataset, VSB100, demonstrate that the proposed algorithm outperforms state-of-the-art online video segmentation techniques significantly.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_36');
INSERT INTO `paper` VALUES (10276, 'Structure from Motion on a Sphere', 'Point Correspondence', 'Bundle Adjustment', 'Rotation Average', 'Epipolar Constraint', 'Essential Matrix', 'We describe a special case of structure from motion where the camera rotates on a sphere. The camera’s optical axis lies perpendicular to the sphere’s surface. In this case, the camera’s pose is minimally represented by three rotation parameters. From analysis of the epipolar geometry we derive a novel and efficient solution for the essential matrix relating two images, requiring only three point correspondences in the minimal case. We apply this solver in a structure-from-motion pipeline that aggregates pairwise relations by rotation averaging followed by bundle adjustment with an inverse depth parameterization. Our methods enable scene modeling with an outward-facing camera and object scanning with an inward-facing camera.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_4');
INSERT INTO `paper` VALUES (10277, 'Sublabel-Accurate Convex Relaxation of Vectorial Multilabel Energies', 'Convex relaxation', 'Optimization', 'Variational methods', '', '', 'Convex relaxations of multilabel problems have been demonstrated to produce provably optimal or near-optimal solutions to a variety of computer vision problems. Yet, they are of limited practical use as they require a fine discretization of the label space, entailing a huge demand in memory and runtime. In this work, we propose the first sublabel accurate convex relaxation for vectorial multilabel problems. Our key idea is to approximate the dataterm in a piecewise convex (rather than piecewise linear) manner. As a result we have a more faithful approximation of the original cost function that provides a meaningful interpretation for fractional solutions of the relaxed convex problem.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_37');
INSERT INTO `paper` VALUES (10278, 'Superpixel Convolutional Networks Using Bilateral Inceptions', 'Conditional Random Field', 'Intermediate Representation', 'Bilateral Filter', 'Inception Module', 'Pascal VOC12', 'In this paper we propose a CNN architecture for semantic image segmentation. We introduce a new “bilateral inception” module that can be inserted in existing CNN architectures and performs bilateral filtering, at multiple feature-scales, between superpixels in an image. The feature spaces for bilateral filtering and other parameters of the module are learned end-to-end using standard backpropagation techniques. The bilateral inception module addresses two issues that arise with general CNN segmentation architectures. First, this module propagates information between (super) pixels while respecting image edges, thus using the structured information of the problem for improved results. Second, the layer recovers a full resolution segmentation result from the lower resolution solution of a CNN. In the experiments, we modify several existing CNN architectures by inserting our inception module between the last CNN (\\(1\\times 1\\) convolution) layers. Empirical results on three different datasets show reliable improvements not only in comparison to the baseline networks, but also in comparison to several dense-pixel prediction techniques such as CRFs, while being competitive in time.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_36');
INSERT INTO `paper` VALUES (10279, 'Superpixel-Based Two-View Deterministic Fitting for Multiple-Structure Data', 'Deterministic algorithm', 'Superpixel', 'Model fitting', 'Feature appearances', '', 'This paper proposes a two-view deterministic geometric model fitting method, termed Superpixel-based Deterministic Fitting (SDF), for multiple-structure data. SDF starts from superpixel segmentation, which effectively captures prior information of feature appearances. The feature appearances are beneficial to reduce the computational complexity for deterministic fitting methods. SDF also includes two original elements, i.e., a deterministic sampling algorithm and a novel model selection algorithm. The two algorithms are tightly coupled to boost the performance of SDF in both speed and accuracy. The key characteristic of SDF is that it can efficiently and deterministically estimate the parameters of model instances in multi-structure data. Experimental results demonstrate that the proposed SDF shows superiority over several state-of-the-art fitting methods for real images with single-structure and multiple-structure data.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_31');
INSERT INTO `paper` VALUES (10280, 'Supervised Transformer Network for Efficient Face Detection', 'Face Detection', 'Recall Rate', 'Facial Landmark', 'Facial Point', 'Receptive Field Size', 'Large pose variations remain to be a challenge that confronts real-word face detection. We propose a new cascaded Convolutional Neural Network, dubbed the name Supervised Transformer Network, to address this challenge. The first stage is a multi-task Region Proposal Network (RPN), which simultaneously predicts candidate face regions along with associated facial landmarks. The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns. The second stage, which is a RCNN, then verifies if the warped candidate regions are valid faces or not. We conduct end-to-end learning of the cascaded network, including optimizing the canonical positions of the facial landmarks. This supervised learning of the transformations automatically selects the best scale to differentiate face/non-face patterns. By combining feature maps from both stages of the network, we achieve state-of-the-art detection accuracies on several public benchmarks. For real-time performance, we run the cascaded network only on regions of interests produced from a boosting cascade face detector. Our detector runs at 30 FPS on a single CPU core for a VGA-resolution image.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_8');
INSERT INTO `paper` VALUES (10281, 'Support Discrimination Dictionary Learning for Image Classification', 'Sparse Representation', 'Sparse Code', 'Dictionary Learning', 'Code Vector', 'Multitask Learning', 'Dictionary learning has been successfully applied in image classification. However, many dictionary learning methods that encode only a single image at a time while training, ignore correlation and other useful information contained within the entire training set. In this paper, we propose a new principle that uses the support of the coefficients to measure the similarity between the pairs of coefficients, instead of using Euclidian distance directly. More specifically, we proposed a support discrimination dictionary learning method, which finds a dictionary under which the coefficients of images from the same class have a common sparse structure while the size of the overlapped signal support of different classes is minimised. In addition, adopting a shared dictionary in a multi-task learning setting, this method can find the number and position of associated dictionary atoms for each class automatically by using structured sparsity on a group of images. The proposed model is extensively evaluated using various image datasets, and it shows superior performance to many state-of-the-art dictionary learning methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_24');
INSERT INTO `paper` VALUES (10282, 'Symmetric Non-rigid Structure from Motion for Category-Specific Object Structure Estimation', 'Symmetry', 'Non-rigid structure from motion', '', '', '', 'Many objects, especially these made by humans, are symmetric, e.g. cars and aeroplanes. This paper addresses the estimation of 3D structures of symmetric objects from multiple images of the same object category, e.g. different cars, seen from various viewpoints. We assume that the deformation between different instances from the same object category is non-rigid and symmetric. In this paper, we extend two leading non-rigid structure from motion (SfM) algorithms to exploit symmetry constraints. We model the both methods as energy minimization, in which we also recover the missing observations caused by occlusions. In particularly, we show that by rotating the coordinate system, the energy can be decoupled into two independent terms, which still exploit symmetry, to apply matrix factorization separately on each of them for initialization. The results on the Pascal3D+ dataset show that our methods significantly improve performance over baseline methods.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_26');
INSERT INTO `paper` VALUES (10283, 'Target Response Adaptation for Correlation Filter Tracking', 'Correlation filter tracking', 'Adaptive target design', '', '', '', 'Most correlation filter (CF) based trackers utilize the circulant structure of the training data to learn a linear filter that best regresses this data to a hand-crafted target response. These circularly shifted patches are only approximations to actual translations in the image, which become unreliable in many realistic tracking scenarios including fast motion, occlusion, etc. In these cases, the traditional use of a single centered Gaussian as the target response impedes tracker performance and can lead to unrecoverable drift. To circumvent this major drawback, we propose a generic framework that can adaptively change the target response from frame to frame, so that the tracker is less sensitive to the cases where circular shifts do not reliably approximate translations. To do that, we reformulate the underlying optimization to solve for both the filter and target response jointly, where the latter is regularized by measurements made using actual translations. This joint problem has a closed form solution and thus allows for multiple templates, kernels, and multi-dimensional features. Extensive experiments on the popular OTB100 benchmark show that our target adaptive framework can be combined with many CF trackers to realize significant overall performance improvement (ranging from 3 %–13.5 % in precision and 3.2 %–13 % in accuracy), especially in categories where this adaptation is necessary (e.g. fast motion, motion blur, etc.).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_25');
INSERT INTO `paper` VALUES (10284, 'Taxonomy-Regularized Semantic Deep Convolutional Neural Networks', 'Deep learning', 'Object categorization', 'Taxonomy', 'Ontology', '', 'We propose a novel convolutional network architecture that abstracts and differentiates the categories based on a given class hierarchy. We exploit grouped and discriminative information provided by the taxonomy, by focusing on the general and specific components that comprise each category, through the min- and difference-pooling operations. Without using any additional parameters or substantial increase in time complexity, our model is able to learn the features that are discriminative for classifying often confused sub-classes belonging to the same superclass, and thus improve the overall classification performance. We validate our method on CIFAR-100, Places-205, and ImageNet Animal datasets, on which our model obtains significant improvements over the base convolutional networks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_6');
INSERT INTO `paper` VALUES (10285, 'Temporal Model Adaptation for Person Re-identification', 'Person re-identificaion', 'Metric learning', 'Active learning', '', '', 'Person re-identification is an open and challenging problem in computer vision. Majority of the efforts have been spent either to design the best feature representation or to learn the optimal matching metric. Most approaches have neglected the problem of adapting the selected features or the learned model over time. To address such a problem, we propose a temporal model adaptation scheme with human in the loop. We first introduce a similarity-dissimilarity learning method which can be trained in an incremental fashion by means of a stochastic alternating directions methods of multipliers optimization procedure. Then, to achieve temporal adaptation with limited human effort, we exploit a graph-based approach to present the user only the most informative probe-gallery matches that should be used to update the model. Results on three datasets have shown that our approach performs on par or even better than state-of-the-art approaches while reducing the manual pairwise labeling effort by about \\(80\\,\\%\\).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_52');
INSERT INTO `paper` VALUES (10286, 'Temporally Robust Global Motion Compensation by Keypoint-Based Congealing', 'Global motion compensation', 'Congealing', 'Motion panorama', '', '', 'Global motion compensation (GMC) removes the impact of camera motion and creates a video in which the background appears static over the progression of time. Various vision problems, such as human activity recognition, background reconstruction, and multi-object tracking can benefit from GMC. Existing GMC algorithms rely on sequentially processing consecutive frames, by estimating the transformation mapping the two frames, and obtaining a composite transformation to a global motion compensated coordinate. Sequential GMC suffers from temporal drift of frames from the accurate global coordinate, due to either error accumulation or sporadic failures of motion estimation at a few frames. We propose a temporally robust global motion compensation (TRGMC) algorithm which performs accurate and stable GMC, despite complicated and long-term camera motion. TRGMC densely connects pairs of frames, by matching local keypoints of each frame. A joint alignment of these frames is formulated as a novel keypoint-based congealing problem, where the transformation of each frame is updated iteratively, such that the spatial coordinates for the start and end points of matched keypoints are identical. Experimental results demonstrate that TRGMC has superior performance in a wide range of scenarios.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_7');
INSERT INTO `paper` VALUES (10287, 'Tensor Representations via Kernel Linearization for Action Recognition from 3D Skeletons', 'Kernel descriptors', 'Skeleton action recognition', 'Higher-order tensors', '', '', 'In this paper, we explore tensor representations that can compactly capture higher-order relationships between skeleton joints for 3D action recognition. We first define RBF kernels on 3D joint sequences, which are then linearized to form kernel descriptors. The higher-order outer-products of these kernel descriptors form our tensor representations. We present two different kernels for action recognition, namely (i) a sequence compatibility kernel that captures the spatio-temporal compatibility of joints in one sequence against those in the other, and (ii) a dynamics compatibility kernel that explicitly models the action dynamics of a sequence. Tensors formed from these kernels are then used to train an SVM. We present experiments on several benchmark datasets and demonstrate state of the art results, substantiating the effectiveness of our representations.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_3');
INSERT INTO `paper` VALUES (10288, 'The Conditional Lucas & Kanade Algorithm', 'Image alignment', 'Lucas & Kanade', 'Supervised Descent Method', '', '', 'The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense image and object alignment. The approach is efficient as it attempts to model the connection between appearance and geometric displacement through a linear relationship that assumes independence across pixel coordinates. A drawback of the approach, however, is its generative nature. Specifically, its performance is tightly coupled with how well the linear model can synthesize appearance from geometric displacement, even though the alignment task itself is associated with the inverse problem. In this paper, we present a new approach, referred to as the Conditional LK algorithm, which: (i) directly learns linear models that predict geometric displacement as a function of appearance, and (ii) employs a novel strategy for ensuring that the generative pixel independence assumption can still be taken advantage of. We demonstrate that our approach exhibits superior performance to classical generative forms of the LK algorithm. Furthermore, we demonstrate its comparable performance to state-of-the-art methods such as the Supervised Descent Method with substantially less training examples, as well as the unique ability to “swap” geometric warp functions without having to retrain from scratch. Finally, from a theoretical perspective, our approach hints at possible redundancies that exist in current state-of-the-art methods for alignment that could be leveraged in vision systems of the future.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_48');
INSERT INTO `paper` VALUES (10289, 'The Curious Robot: Learning Visual Representations via Physical Interactions', 'Visual Representation', 'Image Retrieval', 'Kernel Size', 'Deep Belief Network', 'Convolutional Layer', 'What is the right supervisory signal to train visual representations? Current approaches in computer vision use category labels from datasets such as ImageNet to train ConvNets. However, in case of biological agents, visual representation learning does not require millions of semantic labels. We argue that biological agents use physical interactions with the world to learn visual representations unlike current vision systems which just use passive observations (images and videos downloaded from web). For example, babies push objects, poke them, put them in their mouth and throw them to learn representations. Towards this goal, we build one of the first systems on a Baxter platform that pushes, pokes, grasps and observes objects in a tabletop environment. It uses four different types of physical interactions to collect more than 130K datapoints, with each datapoint providing supervision to a shared ConvNet architecture allowing us to learn visual representations. We show the quality of learned representations by observing neuron activations and performing nearest neighbor retrieval on this learned representation. Quantitatively, we evaluate our learned ConvNet on image classification tasks and show improvements compared to learning without external data. Finally, on the task of instance retrieval, our network outperforms the ImageNet network on recall@1 by 3 %.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_1');
INSERT INTO `paper` VALUES (10290, 'The Fast Bilateral Solver', 'Forward Pass', 'Stereo Algorithm', 'Input Target', 'Initialization Technique', 'Computer Vision Task', 'We present the bilateral solver, a novel algorithm for edge-aware smoothing that combines the flexibility and speed of simple filtering approaches with the accuracy of domain-specific optimization algorithms. Our technique is capable of matching or improving upon state-of-the-art results on several different computer vision tasks (stereo, depth superresolution, colorization, and semantic segmentation) while being 10–1000\\(\\times \\) faster than baseline techniques with comparable accuracy, and producing lower-error output than techniques with comparable runtimes. The bilateral solver is fast, robust, straightforward to generalize to new domains, and simple to integrate into deep learning pipelines.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_38');
INSERT INTO `paper` VALUES (10291, 'The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition', 'Fine-grained Recognition', 'Stanford Dogs', 'Ground-truth Training Set', 'Label Noise', 'Learning Activity Data', 'Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of \\(92.3\\,\\%\\) on CUB-200-2011, \\(85.4\\,\\%\\) on Birdsnap, \\(93.4\\,\\%\\) on FGVC-Aircraft, and \\(80.8\\,\\%\\) on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_19');
INSERT INTO `paper` VALUES (10292, 'Title Generation for User Generated Videos', 'Video captioning', 'Video and language', '', '', '', 'A great video title describes the most salient event compactly and captures the viewer’s attention. In contrast, video captioning tends to generate sentences that describe the video as a whole. Although generating a video title automatically is a very useful task, it is much less addressed than video captioning. We address video title generation for the first time by proposing two methods that extend state-of-the-art video captioners to this new task. First, we make video captioners highlight sensitive by priming them with a highlight detector. Our framework allows for jointly training a model for title generation and video highlight localization. Second, we induce high sentence diversity in video captioners, so that the generated titles are also diverse and catchy. This means that a large number of sentences might be required to learn the sentence structure of titles. Hence, we propose a novel sentence augmentation method to train a captioner with additional sentence-only examples that come without corresponding videos. We collected a large-scale Video Titles in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos and titles. On VTW, our methods consistently improve title prediction accuracy, and achieve the best performance in both automatic and human evaluation. Finally, our sentence augmentation method also outperforms the baselines on the M-VAD dataset.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_38');
INSERT INTO `paper` VALUES (10293, 'Top-Down Learning for Structured Labeling with Convolutional Pseudoprior', 'Structured prediction', 'Deep learning', 'Semantic segmentation', 'Top-down processing', '', 'Current practice in convolutional neural networks (CNN) remains largely bottom-up and the role of top-down process in CNN for pattern analysis and visual inference is not very clear. In this paper, we propose a new method for structured labeling by developing convolutional pseudoprior (ConvPP) on the ground-truth labels. Our method has several interesting properties: (1) compared with classic machine learning algorithms like CRFs and Structural SVM, ConvPP automatically learns rich convolutional kernels to capture both short- and long- range contexts; (2) compared with cascade classifiers like Auto-Context, ConvPP avoids the iterative steps of learning a series of discriminative classifiers and automatically learns contextual configurations; (3) compared with recent efforts combining CNN models with CRFs and RNNs, ConvPP learns convolution in the labeling space with improved modeling capability and less manual specification; (4) compared with Bayesian models like MRFs, ConvPP capitalizes on the rich representation power of convolution by automatically learning priors built on convolutional filters. We accomplish our task using pseudo-likelihood approximation to the prior under a novel fixed-point network structure that facilitates an end-to-end learning process. We show state-of-the-art results on sequential labeling and image labeling benchmarks.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_19');
INSERT INTO `paper` VALUES (10294, 'Top-Down Neural Attention by Excitation Backprop', 'Noun Phrase', 'Convolutional Neural Network', 'Winner Neuron', 'Convolutional Neural Network Model', 'Absorb Markov Chain', 'We aim to model the top-down attention of a Convolutional Neural Network (CNN) classifier for generating task-specific attention maps. Inspired by a top-down human visual attention model, we propose a new backpropagation scheme, called Excitation Backprop, to pass along top-down signals downwards in the network hierarchy via a probabilistic Winner-Take-All process. Furthermore, we introduce the concept of contrastive attention to make the top-down attention maps more discriminative. In experiments, we demonstrate the accuracy and generalizability of our method in weakly supervised localization tasks on the MS COCO, PASCAL VOC07 and ImageNet datasets. The usefulness of our method is further validated in the text-to-region association task. On the Flickr30k Entities dataset, we achieve promising performance in phrase localization by leveraging the top-down attention of a CNN model that has been trained on weakly labeled web images.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_33');
INSERT INTO `paper` VALUES (10295, 'Towards Large-Scale City Reconstruction from Satellites', '3D reconstruction', 'City modeling', 'Satellite imagery', 'Urban scenes', '', 'Automatic city modeling from satellite imagery is one of the biggest challenges in urban reconstruction. Existing methods produce at best rough and dense Digital Surface Models. Inspired by recent works on semantic 3D reconstruction and region-based stereovision, we propose a method for producing compact, semantic-aware and geometrically accurate 3D city models from stereo pair of satellite images. Our approach relies on two key ingredients. First, geometry and semantics are retrieved simultaneously bringing robustness to occlusions and to low image quality. Second, we operate at the scale of geometric atomic region which allows the shape of urban objects to be well preserved, and a gain in scalability and efficiency. We demonstrate the potential of our algorithm by reconstructing different cities around the world in a few minutes.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_6');
INSERT INTO `paper` VALUES (10296, 'Towards Viewpoint Invariant 3D Human Pose Estimation', 'Random Forest', 'Depth Image', 'Error Feedback', 'Recurrent Connection', 'Convolutional Network', 'We propose a viewpoint invariant model for 3D human pose estimation from a single depth image. To achieve this, our discriminative model embeds local regions into a learned viewpoint invariant feature space. Formulated as a multi-task learning problem, our model is able to selectively predict partial poses in the presence of noise and occlusion. Our approach leverages a convolutional and recurrent network architecture with a top-down error feedback mechanism to self-correct previous pose estimates in an end-to-end manner. We evaluate our model on a previously published depth dataset and a newly collected human pose dataset containing 100 K annotated depth images from extreme viewpoints. Experiments show that our model achieves competitive performance on frontal views while achieving state-of-the-art performance on alternate viewpoints.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_10');
INSERT INTO `paper` VALUES (10297, 'Tracking Persons-of-Interest via Adaptive Discriminative Features', 'Convolutional Neural Network', 'Music Video', 'Multiple Shot', 'Visual Constraint', 'Data Association Problem', 'Multi-face tracking in unconstrained videos is a challenging problem as faces of one person often appear drastically different in multiple shots due to significant variations in scale, pose, expression, illumination, and make-up. Low-level features used in existing multi-target tracking methods are not effective for identifying faces with such large appearance variations. In this paper, we tackle this problem by learning discriminative, video-specific face features using convolutional neural networks (CNNs). Unlike existing CNN-based approaches that are only trained on large-scale face image datasets offline, we further adapt the pre-trained face CNN to specific videos using automatically discovered training samples from tracklets. Our network directly optimizes the embedding space so that the Euclidean distances correspond to a measure of semantic face similarity. This is technically realized by minimizing an improved triplet loss function. With the learned discriminative features, we apply the Hungarian algorithm to link tracklets within each shot and the hierarchical clustering algorithm to link tracklets across multiple shots to form final trajectories. We extensively evaluate the proposed algorithm on a set of TV sitcoms and music videos and demonstrate significant performance improvement over existing techniques.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_26');
INSERT INTO `paper` VALUES (10298, 'Transfer Neural Trees for Heterogeneous Domain Adaptation', 'Transfer learning', 'Domain adaptation', 'Neural Decision Forest', 'Neural network', '', 'Heterogeneous domain adaptation (HDA) addresses the task of associating data not only across dissimilar domains but also described by different types of features. Inspired by the recent advances of neural networks and deep learning, we propose Transfer Neural Trees (TNT) which jointly solves cross-domain feature mapping, adaptation, and classification in a NN-based architecture. As the prediction layer in TNT, we further propose Transfer Neural Decision Forest (Transfer-NDF), which effectively adapts the neurons in TNT for adaptation by stochastic pruning. Moreover, to address semi-supervised HDA, a unique embedding loss term for preserving prediction and structural consistency between target-domain data is introduced into TNT. Experiments on classification tasks across features, datasets, and modalities successfully verify the effectiveness of our TNT.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_25');
INSERT INTO `paper` VALUES (10299, 'Ultra-Resolving Face Images by Discriminative Generative Networks', 'Super-resolution', 'Discriminative Generative Networks', 'Face', '', '', 'Conventional face super-resolution methods, also known as face hallucination, are limited up to \\(2 \\! \\sim \\! 4\\times \\) scaling factors where \\(4 \\sim 16\\) additional pixels are estimated for each given pixel. Besides, they become very fragile when the input low-resolution image size is too small that only little information is available in the input image. To address these shortcomings, we present a discriminative generative network that can ultra-resolve a very low resolution face image of size \\(16 \\times 16\\) pixels to its \\(8\\times \\) larger version by reconstructing 64 pixels from a single pixel. We introduce a pixel-wise \\(\\ell _2\\) regularization term to the generative model and exploit the feedback of the discriminative network to make the upsampled face images more similar to real ones. In our framework, the discriminative network learns the essential constituent parts of the faces and the generative network blends these parts in the most accurate fashion to the input image. Since only frontal and ordinary aligned images are used in training, our method can ultra-resolve a wide range of very low-resolution images directly regardless of pose and facial expression variations. Our extensive experimental evaluations demonstrate that the presented ultra-resolution by discriminative generative networks (UR-DGN) achieves more appealing results than the state-of-the-art.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_20');
INSERT INTO `paper` VALUES (10300, 'Uncovering Symmetries in Polynomial Systems', 'Polynomial Equation System', 'Solving Polynomial', 'Unknown Focal Length', 'Template Elimination', 'Absolute Pose', 'In this paper we study symmetries in polynomial equation systems and how they can be integrated into the action matrix method. The main contribution is a generalization of the partial p-fold symmetry and we provide new theoretical insights as to why these methods work. We show several examples of how to use this symmetry to construct more compact polynomial solvers. As a second contribution we present a simple and automatic method for finding these symmetries for a given problem. Finally we show two examples where these symmetries occur in real applications.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_16');
INSERT INTO `paper` VALUES (10301, 'Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles', 'Unsupervised learning', 'Image representation learning', 'Self-supervised learning', 'Feature transfer', '', 'We propose a novel unsupervised learning approach to build features suitable for object detection and classification. The features are pre-trained on a large dataset without human annotation and later transferred via fine-tuning on a different, smaller and labeled dataset. The pre-training consists of solving jigsaw puzzles of natural images. To facilitate the transfer of features to other tasks, we introduce the context-free network (CFN), a siamese-ennead convolutional neural network. The features correspond to the columns of the CFN and they process image tiles independently (i.e., free of context). The later layers of the CFN then use the features to identify their geometric arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. We pre-train the CFN on the training set of the ILSVRC2012 dataset and transfer the features on the combined training and validation set of Pascal VOC 2007 for object detection (via fast RCNN) and classification. These features outperform all current unsupervised features with \\(51.8\\,\\%\\) for detection and \\(68.6\\,\\%\\) for classification, and reduce the gap with supervised learning (\\(56.5\\,\\%\\) and \\(78.2\\,\\%\\) respectively).', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_5');
INSERT INTO `paper` VALUES (10302, 'Unsupervised Visual Representation Learning by Graph-Based Consistent Constraints', 'Unsupervised feature learning', 'Semi-supervised learning', 'Image classification', 'Convolutional neural networks', '', 'Learning rich visual representations often require training on datasets of millions of manually annotated examples. This substantially limits the scalability of learning effective representations as labeled data is expensive or scarce. In this paper, we address the problem of unsupervised visual representation learning from a large, unlabeled collection of images. By representing each image as a node and each nearest-neighbor matching pair as an edge, our key idea is to leverage graph-based analysis to discover positive and negative image pairs (i.e., pairs belonging to the same and different visual categories). Specifically, we propose to use a cycle consistency criterion for mining positive pairs and geodesic distance in the graph for hard negative mining. We show that the mined positive and negative image pairs can provide accurate supervisory signals for learning effective representations using Convolutional Neural Networks (CNNs). We demonstrate the effectiveness of the proposed unsupervised constraint mining method in two settings: (1) unsupervised feature learning and (2) semi-supervised learning. For unsupervised feature learning, we obtain competitive performance with several state-of-the-art approaches on the PASCAL VOC 2007 dataset. For semi-supervised learning, we show boosted performance by incorporating the mined constraints on three image classification datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_41');
INSERT INTO `paper` VALUES (10303, 'View Synthesis by Appearance Flow', 'Input Image', 'Mental Rotation', 'Convolutional Neural Network', 'Texture Synthesis', 'View Synthesis', 'We address the problem of novel view synthesis: given an input image, synthesizing new images of the same object or scene observed from arbitrary viewpoints. We approach this as a learning task but, critically, instead of learning to synthesize pixels from scratch, we learn to copy them from the input image. Our approach exploits the observation that the visual appearance of different views of the same instance is highly correlated, and such correlation could be explicitly learned by training a convolutional neural network (CNN) to predict appearance flows – 2-D coordinate vectors specifying which pixels in the input view could be used to reconstruct the target view. Furthermore, the proposed framework easily generalizes to multiple input views by learning how to optimally combine single-view predictions. We show that for both objects and scenes, our approach is able to synthesize novel views of higher perceptual quality than previous CNN-based techniques.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_18');
INSERT INTO `paper` VALUES (10304, 'Visual Motif Discovery via First-Person Vision', 'Commonality discovery', 'First-person video', '', '', '', 'Visual motifs are images of visual experiences that are significant and shared across many people, such as an image of an informative sign viewed by many people and that of a familiar social situation such as when interacting with a clerk at a store. The goal of this study is to discover visual motifs from a collection of first-person videos recorded by a wearable camera. To achieve this goal, we develop a commonality clustering method that leverages three important aspects: inter-video similarity, intra-video sparseness, and people’s visual attention. The problem is posed as normalized spectral clustering, and is solved efficiently using a weighted covariance matrix. Experimental results suggest the effectiveness of our method over several state-of-the-art methods in terms of both accuracy and efficiency of visual motif discovery.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46475-6_12');
INSERT INTO `paper` VALUES (10305, 'Visual Relationship Detection with Language Priors', 'Image Retrieval', 'Query Image', 'Object Category', 'Visual Appearance', 'Convolutional Neural Network', 'Visual relationships capture a wide variety of interactions between pairs of objects in images (e.g. “man riding bicycle” and “man pushing bicycle”). Consequently, the set of possible relationships is extremely large and it is difficult to obtain sufficient training examples for all possible relationships. Because of this limitation, previous work on visual relationship detection has concentrated on predicting only a handful of relationships. Though most relationships are infrequent, their objects (e.g. “man” and “bicycle”) and predicates (e.g. “riding” and “pushing”) independently occur more frequently. We propose a model that uses this insight to train visual models for objects and predicates individually and later combines them together to predict multiple relationships per image. We improve on prior work by leveraging language priors from semantic word embeddings to finetune the likelihood of a predicted relationship. Our model can scale to predict thousands of types of relationships from a few examples. Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. We further demonstrate that understanding relationships can improve content based image retrieval.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_51');
INSERT INTO `paper` VALUES (10306, 'Visualizing Image Priors', 'Independent Component Analysis', 'Optical Flow', 'Sparse Representation', 'Internal Model', 'Natural Image', 'Image priors play a key role in low-level vision tasks. Over the years, many priors have been proposed, based on a wide variety of principles. While different priors capture different geometric properties, there is currently no unified approach to interpreting and comparing priors of different nature. This limits our ability to analyze failures or successes of image models in specific settings, and to identify potential improvements. In this paper, we introduce a simple technique for visualizing image priors. Our method determines how images should be deformed so as to best conform to a given image model. The deformed images constructed this way, highlight the elementary geometric structures to which the prior resonates. We use our approach to study various popular image models, and reveal interesting behaviors, which were not noticed in the past. We confirm our findings through denoising experiments. These validate that the structures we reveal as ‘optimal’ for a specific prior are indeed better denoised by this prior.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_9');
INSERT INTO `paper` VALUES (10307, 'Weakly Supervised Learning of Heterogeneous Concepts in Videos', 'Heterogeneous Concept', 'Indian Buffet Process (IBP)', 'Competitive Baseline', 'Local Constraints', 'Variational Mean-field Approximation', 'Typical textual descriptions that accompany online videos are ‘weak’: i.e., they mention the important heterogeneous concepts in the video but not their corresponding spatio-temporal locations. However, certain location constraints on these concepts can be inferred from the description. The goal of this paper is to present a generalization of the Indian Buffet Process (IBP) that can (a) systematically incorporate heterogeneous concepts in an integrated framework, and (b) enforce location constraints, for efficient classification and localization of the concepts in the videos. Finally, we develop posterior inference for the proposed formulation using mean-field variational approximation. Comparative evaluations on the Casablanca and the A2D datasets show that the proposed approach significantly outperforms other state-of-the-art techniques: 24 % relative improvement for pairwise concept classification in the Casablanca dataset and 9 % relative improvement for localization in the A2D dataset as compared to the most competitive baseline.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46466-4_17');
INSERT INTO `paper` VALUES (10308, 'Weakly Supervised Localization Using Deep Feature Maps', 'Weakly supervised methods', 'Object localization', 'Deep convolutional networks', '', '', 'Object localization is an important computer vision problem with a variety of applications. The lack of large scale object-level annotations and the relative abundance of image-level labels makes a compelling case for weak supervision in the object localization task. Deep Convolutional Neural Networks are a class of state-of-the-art methods for the related problem of object recognition. In this paper, we describe a novel object localization algorithm which uses classification networks trained on only image labels. This weakly supervised method leverages local spatial and semantic patterns captured in the convolutional layers of classification networks. We propose an efficient beam search based approach to detect and localize multiple objects in images. The proposed method significantly outperforms the state-of-the-art in standard object localization data-sets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46448-0_43');
INSERT INTO `paper` VALUES (10309, 'Weakly Supervised Object Localization Using Size Estimates', 'Training Image', 'Object Class', 'Object Size', 'Appearance Model', 'Size Order', 'We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_7');
INSERT INTO `paper` VALUES (10310, 'Weakly-Supervised Semantic Segmentation Using Motion Cues', 'Motion Segment', 'Convolutional Neural Network', 'Soft Constraint', 'Video Shot', 'Motion Segmentation', 'Fully convolutional neural networks (FCNNs) trained on a large number of images with strong pixel-level annotations have become the new state of the art for the semantic segmentation task. While there have been recent attempts to learn FCNNs from image-level weak annotations, they need additional constraints, such as the size of an object, to obtain reasonable performance. To address this issue, we present motion-CNN (M-CNN), a novel FCNN framework which incorporates motion cues and is learned from video-level weak annotations. Our learning scheme to train the network uses motion segments as soft constraints, thereby handling noisy motion information. When trained on weakly-annotated videos, our method outperforms the state-of-the-art approach [1] on the PASCAL VOC 2012 image segmentation benchmark. We also demonstrate that the performance of M-CNN learned with 150 weak video annotations is on par with state-of-the-art weakly-supervised methods trained with thousands of images. Finally, M-CNN substantially outperforms recent approaches in a related task of video co-localization on the YouTube-Objects dataset.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_24');
INSERT INTO `paper` VALUES (10311, 'Webly-Supervised Video Recognition by Mutually Voting for Relevant Web Images and Web Video Frames', 'Video Recognition', 'Selected Video Frames', 'VGGNet', 'Maximum Mean Discrepancy (MMD)', 'Redundant Frames', 'Video recognition usually requires a large amount of training samples, which are expensive to be collected. An alternative and cheap solution is to draw from the large-scale images and videos from the Web. With modern search engines, the top ranked images or videos are usually highly correlated to the query, implying the potential to harvest the labeling-free Web images and videos for video recognition. However, there are two key difficulties that prevent us from using the Web data directly. First, they are typically noisy and may be from a completely different domain from that of users’ interest (e.g. cartoons). Second, Web videos are usually untrimmed and very lengthy, where some query-relevant frames are often hidden in between the irrelevant ones. A question thus naturally arises: to what extent can such noisy Web images and videos be utilized for labeling-free video recognition? In this paper, we propose a novel approach to mutually voting for relevant Web images and video frames, where two forces are balanced, i.e. aggressive matching and passive video frame selection. We validate our approach on three large-scale video recognition datasets.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46487-9_52');
INSERT INTO `paper` VALUES (10312, 'Where Should Saliency Models Look Next?', 'Saliency maps', 'Saliency estimation', 'Eye movements', 'Deep learning', 'Image understanding', 'Recently, large breakthroughs have been observed in saliency modeling. The top scores on saliency benchmarks have become dominated by neural network models of saliency, and some evaluation scores have begun to saturate. Large jumps in performance relative to previous models can be found across datasets, image types, and evaluation metrics. Have saliency models begun to converge on human performance? In this paper, we re-examine the current state-of-the-art using a fine-grained analysis on image types, individual images, and image regions. Using experiments to gather annotations for high-density regions of human eye fixations on images in two established saliency datasets, MIT300 and CAT2000, we quantify up to 60% of the remaining errors of saliency models. We argue that to continue to approach human-level performance, saliency models will need to discover higher-level concepts in images: text, objects of gaze and action, locations of motion, and expected locations of people in images. Moreover, they will need to reason about the relative importance of image regions, such as focusing on the most important person in the room or the most informative sign on the road. More accurately tracking performance will require finer-grained evaluations and metrics. Pushing performance further will require higher-level image understanding.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_49');
INSERT INTO `paper` VALUES (10313, 'XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks', 'Convolutional Neural Network', 'Deep Neural Network', 'Weight Filter', 'Convolutional Layer', 'Binary Input', 'We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\\(\\times \\) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\\(\\times \\) faster convolutional operations (in terms of number of the high precision operations) and 32\\(\\times \\) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \\(16\\,\\%\\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_32');
INSERT INTO `paper` VALUES (10314, 'Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net', 'Human parsing', 'Part segmentation', 'Multi-scale modeling', '', '', 'Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. head, body and arms, etc.) from natural images is a challenging and fundamental problem in computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle this difficulty, we propose a “Hierarchical Auto-Zoom Net” (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two “Auto-Zoom Nets” (AZNs), each employing fully convolutional networks for two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively “zoom” (resize) predicted image regions into their proper scales to refine the parsing. We conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. In all the three categories, our approach significantly outperforms alternative state-of-the-arts by more than \\(5\\,\\%\\) mIOU and is especially better at segmenting small instances and small parts. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that we do not need to waste computational resources scaling the entire image.', 'ECCV', '2016', '16 September 2016', 'https://doi.org/10.1007/978-3-319-46454-1_39');
INSERT INTO `paper` VALUES (10315, '“What Happens If...” Learning to Predict the Effect of Forces in Images', 'Scene understanding', 'Forces', 'Motion estimation', 'Recurrent Neural Networks', '', 'What happens if one pushes a cup sitting on a table toward the edge of the table? How about pushing a desk against a wall? In this paper, we study the problem of understanding the movements of objects as a result of applying external forces to them. For a given force vector applied to a specific location in an image, our goal is to predict long-term sequential movements caused by that force. Doing so entails reasoning about scene geometry, objects, their attributes, and the physical rules that govern the movements of objects. We design a deep neural network model that learns long-term sequential dependencies of object movements while taking into account the geometry and appearance of the scene by combining Convolutional and Recurrent Neural Networks. Training our model requires a large-scale dataset of object movements caused by external forces. To build a dataset of forces in scenes, we reconstructed all images in SUN RGB-D dataset in a physics simulator to estimate the physical movements of objects caused by external forces applied to them. Our Forces in Scenes (ForScene) dataset contains 65,000 object movements in 3D which represent a variety of external forces applied to different types of objects. Our experimental evaluations show that the challenging task of predicting long-term movements of objects as their reaction to external forces is possible from a single image. The code and dataset are available at: http://allenai.org/plato/forces.', 'ECCV', '2016', '17 September 2016', 'https://doi.org/10.1007/978-3-319-46493-0_17');
INSERT INTO `paper` VALUES (10316, '2D and 3D Vascular Structures Enhancement via Multiscale Fractional Anisotropy Tensor', 'Curvilinear structures', 'Image enhancement', 'Enhancement filter', 'Tensor representation', 'Hessian matrix', 'The detection of vascular structures from noisy images is a fundamental process for extracting meaningful information in many applications. Most well-known vascular enhancing techniques often rely on Hessian-based filters. This paper investigates the feasibility and deficiencies of detecting curve-like structures using a Hessian matrix. The main contribution is a novel enhancement function, which overcomes the deficiencies of established methods. Our approach has been evaluated quantitatively and qualitatively using synthetic examples and a wide range of real 2D and 3D biomedical images. Compared with other existing approaches, the experimental results prove that our proposed approach achieves high-quality curvilinear structure enhancement.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_26');
INSERT INTO `paper` VALUES (10317, '3D Bounding Boxes for Road Vehicles: A One-Stage, Localization Prioritized Approach Using Single Monocular Images', 'Single stage 3D object detection', 'Inverse perspective mapping', 'Effective near object localization', '', '', 'Understanding 3D semantics of the surrounding objects is critically important and a challenging requirement from the safety perspective of autonomous driving. We present a localization prioritized approach for effectively localizing the position of the object in the 3D world and fit a complete 3D box around it. Our method requires a single image and performs both 2D and 3D detection in an end to end fashion. Estimating depth of an object from a monocular image is not as generalizable as pose and dimensions. Hence, we approach this problem by effectively localizing the projection of the center of bottom face of 3D bounding box (CBF) to the image. Later in our post processing stage, we use a look up table based approach to reproject the CBF in the 3D world. This stage is a single time setup and simple enough to be deployed in fixed map communities where we can store complete knowledge about the ground plane. The object’s dimension and pose are predicted in multitask fashion using a shared set of features. Experiments show that our method is able to produce smooth tracks for surround objects and outperforms existing image based approaches in 3D localization.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_39');
INSERT INTO `paper` VALUES (10318, '3D Ego-Pose Estimation via Imitation Learning', 'First-person vision', 'Pose estimation', 'Imitation learning', '', '', 'Ego-pose estimation, i.e., estimating a person’s 3D pose with a single wearable camera, has many potential applications in activity monitoring. For these applications, both accurate and physically plausible estimates are desired, with the latter often overlooked by existing work. Traditional computer vision-based approaches using temporal smoothing only take into account the kinematics of the motion without considering the physics that underlies the dynamics of motion, which leads to pose estimates that are physically invalid. Motivated by this, we propose a novel control-based approach to model human motion with physics simulation and use imitation learning to learn a video-conditioned control policy for ego-pose estimation. Our imitation learning framework allows us to perform domain adaption to transfer our policy trained on simulation data to real-world data. Our experiments with real egocentric videos show that our method can estimate both accurate and physically plausible 3D ego-pose sequences without observing the cameras wearer’s body.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_45');
INSERT INTO `paper` VALUES (10319, '3D Face Reconstruction from Light Field Images: A Model-Free Approach', '', '', '', '', '', 'Reconstructing 3D facial geometry from a single RGB image has recently instigated wide research interest. However, it is still an ill-posed problem and most methods rely on prior models hence undermining the accuracy of the recovered 3D faces. In this paper, we exploit the Epipolar Plane Images (EPI) obtained from light field cameras and learn CNN models that recover horizontal and vertical 3D facial curves from the respective horizontal and vertical EPIs. Our 3D face reconstruction network (FaceLFnet) comprises a densely connected architecture to learn accurate 3D facial curves from low resolution EPIs. To train the proposed FaceLFnets from scratch, we synthesize photo-realistic light field images from 3D facial scans. The curve by curve 3D face estimation approach allows the networks to learn from only 14K images of 80 identities, which still comprises over 11 Million EPIs/curves. The estimated facial curves are merged into a single pointcloud to which a surface is fitted to get the final 3D face. Our method is model-free, requires only a few training samples to learn FaceLFnet and can reconstruct 3D faces with high accuracy from single light field images under varying poses, expressions and lighting conditions. Comparison on the BU-3DFE and BU-4DFE datasets show that our method reduces reconstruction errors by over 20% compared to recent state of the art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_31');
INSERT INTO `paper` VALUES (10320, '3D Human Body Reconstruction from a Single Image via Volumetric Regression', '3D reconstruction', 'Human body reconstruction', 'Volumetric regression', 'VRN', 'Single image reconstruction', 'This paper proposes the use of an end-to-end Convolutional Neural Network for direct reconstruction of the 3D geometry of humans via volumetric regression. The proposed method does not require the fitting of a shape model and can be trained to work from a variety of input types, whether it be landmarks, images or segmentation masks. Additionally, non-visible parts, either self-occluded or otherwise, are still reconstructed, which is not the case with depth map regression. We present results that show that our method can handle both pose variation and detailed reconstruction given appropriate datasets for training.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_6');
INSERT INTO `paper` VALUES (10321, '3D Pose Estimation for Fine-Grained Object Categories', '', '', '', '', '', 'Existing object pose estimation datasets are related to generic object types and there is so far no dataset for fine-grained object categories. In this work, we introduce a new large dataset to benchmark pose estimation for fine-grained objects, thanks to the availability of both 2D and 3D fine-grained data recently. Specifically, we augment two popular fine-grained recognition datasets (StanfordCars and CompCars) by finding a fine-grained 3D CAD model for each sub-category and manually annotating each object in images with 3D pose. We show that, with enough training data, a full perspective model with continuous parameters can be estimated using 2D appearance information alone. We achieve this via a framework based on Faster/Mask R-CNN. This goes beyond previous works on category-level pose estimation, which only estimate discrete/continuous viewpoint angles or recover rotation matrices often with the help of key points. Furthermore, with fine-grained 3D models available, we incorporate a dense 3D representation named as location field into the CNN-based pose estimation framework to further improve the performance. The new dataset is available at www.umiacs.umd.edu/~wym/3dpose.html.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_38');
INSERT INTO `paper` VALUES (10322, '3D Recurrent Neural Networks with Context Fusion for Point Cloud Semantic Segmentation', '3D semantic segmentation', 'Unstructured point cloud', 'Recurrent neural networks', 'Pointwise pyramid pooling', '', 'Semantic segmentation of 3D unstructured point clouds remains an open research problem. Recent works predict semantic labels of 3D points by virtue of neural networks but take limited context knowledge into consideration. In this paper, a novel end-to-end approach for unstructured point cloud semantic segmentation, named 3P-RNN, is proposed to exploit the inherent contextual features. First the efficient pointwise pyramid pooling module is investigated to capture local structures at various densities by taking multi-scale neighborhood into account. Then the two-direction hierarchical recurrent neural networks (RNNs) are utilized to explore long-range spatial dependencies. Each recurrent layer takes as input the local features derived from unrolled cells and sweeps the 3D space along two directions successively to integrate structure knowledge. On challenging indoor and outdoor 3D datasets, the proposed framework demonstrates robust performance superior to state-of-the-arts.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_25');
INSERT INTO `paper` VALUES (10323, '3D Scene Flow from 4D Light Field Gradients', '', '', '', '', '', 'This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is underconstrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local ‘Lucas-Kanade’ ray flow and global ‘Horn-Schunck’ ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a \\(3 \\times 3\\) matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_41');
INSERT INTO `paper` VALUES (10324, '3D Surface Reconstruction by Pointillism', '', '', '', '', '', 'The objective of this work is to infer the 3D shape of an object from a single image. We use sculptures as our training and test bed, as these have great variety in shape and appearance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_21');
INSERT INTO `paper` VALUES (10325, '3D Vehicle Trajectory Reconstruction in Monocular Video Data Using Environment Structure Constraints', 'Vehicle trajectory reconstruction', 'Instance-aware semantic segmentation', 'Structure-from-motion', '', '', 'We present a framework to reconstruct three-dimensional vehicle trajectories using monocular video data. We track two-dimensional vehicle shapes on pixel level exploiting instance-aware semantic segmentation techniques and optical flow cues. We apply Structure from Motion techniques to vehicle and background images to determine for each frame camera poses relative to vehicle instances and background structures. By combining vehicle and background camera pose information, we restrict the vehicle trajectory to a one-parameter family of possible solutions. We compute a ground representation by fusing background structures and corresponding semantic segmentations. We propose a novel method to determine vehicle trajectories consistent to image observations and reconstructed environment structures as well as a criterion to identify frames suitable for scale ratio estimation. We show qualitative results using drone imagery as well as driving sequences from the Cityscape dataset. Due to the lack of suitable benchmark datasets we present a new dataset to evaluate the quality of reconstructed three-dimensional vehicle trajectories. The video sequences show vehicles in urban areas and are rendered using the path-tracing render engine Cycles. In contrast to previous work, we perform a quantitative evaluation of the presented approach. Our algorithm achieves an average reconstruction-to-ground-truth-trajectory distance of 0.31 m using this dataset. The dataset including evaluation scripts will be publicly available on our website (Project page: http://s.fhg.de/trajectory).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_3');
INSERT INTO `paper` VALUES (10326, '3D-CODED: 3D Correspondences by Deep Deformation', '3D deep learning', 'Computational geometry', 'Shape matching', '', '', 'We present a new deep learning approach for matching deformable shapes by introducing Shape Deformation Networks which jointly encode 3D shapes and correspondences. This is achieved by factoring the surface representation into (i) a template, that parameterizes the surface, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input surface. By predicting this feature for a new shape, we implicitly predict correspondences between this shape and the template. We show that these correspondences can be improved by an additional step which improves the shape feature by minimizing the Chamfer distance between the input and transformed template. We demonstrate that our simple approach improves on state-of-the-art results on the difficult FAUST-inter challenge, with an average correspondence error of 2.88 cm. We show, on the TOSCA dataset, that our method is robust to many types of perturbations, and generalizes to non-human shapes. This robustness allows it to perform well on real unclean, meshes from the SCAPE dataset.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_15');
INSERT INTO `paper` VALUES (10327, '3D-PSRNet: Part Segmented 3D Point Cloud Reconstruction from a Single Image', 'Point cloud', '3D reconstruction', '3D part segmentation', '', '', 'We propose a mechanism to reconstruct part annotated 3D point clouds of objects given just a single input image. We demonstrate that jointly training for both reconstruction and segmentation leads to improved performance in both the tasks, when compared to training for each task individually. The key idea is to propagate information from each task so as to aid the other during the training procedure. Towards this end, we introduce a location-aware segmentation loss in the training regime. We empirically show the effectiveness of the proposed loss in generating more faithful part reconstructions while also improving segmentation accuracy. We thoroughly evaluate the proposed approach on different object categories from the ShapeNet dataset to obtain improved results in reconstruction as well as segmentation. Codes are available at https://github.com/val-iisc/3d-psrnet.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_50');
INSERT INTO `paper` VALUES (10328, '3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues', 'Point clouds', 'K-d tree structure', 'Contextual cues', 'Hierarchical learning', '', 'Classification and segmentation of 3D point clouds are important tasks in computer vision. Because of the irregular nature of point clouds, most of the existing methods convert point clouds into regular 3D voxel grids before they are used as input for ConvNets. Unfortunately, voxel representations are highly insensitive to the geometrical nature of 3D data. More recent methods encode point clouds to higher dimensional features to cover the global 3D space. However, these models are not able to sufficiently capture the local structures of point clouds.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_24');
INSERT INTO `paper` VALUES (10329, '3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration', 'Point cloud', 'Registration', 'Deep learning', 'Weak supervision', '', 'In this paper, we propose the 3DFeat-Net which learns both 3D feature detector and descriptor for point cloud matching using weak supervision. Unlike many existing works, we do not require manual annotation of matching point clusters. Instead, we leverage on alignment and attention mechanisms to learn feature correspondences from GPS/INS tagged 3D point clouds without explicitly specifying them. We create training and benchmark outdoor Lidar datasets, and experiments show that 3DFeat-Net obtains state-of-the-art performance on these gravity-aligned datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_37');
INSERT INTO `paper` VALUES (10330, '3DMV: Joint 3D-Multi-view Prediction for 3D Semantic Scene Segmentation', 'Volumetric Grid', 'Input Views', 'PointNet', 'Subvolume', 'NYU V2', 'We present 3DMV, a novel method for 3D semantic scene segmentation of RGB-D scans in indoor environments using a joint 3D-multi-view prediction network. In contrast to existing methods that either use geometry or RGB data as input for this task, we combine both data modalities in a joint, end-to-end network architecture. Rather than simply projecting color data into a volumetric grid and operating solely in 3D – which would result in insufficient detail – we first extract feature maps from associated RGB images. These features are then mapped into the volumetric feature grid of a 3D network using a differentiable back-projection layer. Since our target is 3D scanning scenarios with possibly many frames, we use a multi-view pooling approach in order to handle a varying number of RGB input views. This learned combination of RGB and geometric features with our joint 2D-3D architecture achieves significantly better results than existing baselines. For instance, our final result on the ScanNet 3D segmentation benchmark increases from 52.8% to 75% accuracy compared to existing volumetric architectures.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_28');
INSERT INTO `paper` VALUES (10331, 'A 2.5D Deep Learning-Based Approach for Prostate Cancer Detection on T2-Weighted Magnetic Resonance Imaging', '', '', '', '', '', 'In this paper, we propose a fully automatic magnetic resonance image (MRI)-based computer aided diagnosis (CAD) system which simultaneously performs both prostate segmentation and prostate cancer diagnosis. The system utilizes a deep-learning approach to extract high-level features from raw T2-weighted MR volumes. Features are then remapped to the original input to assign a predicted label to each pixel. In the same context, we propose a 2.5D approach which exploits 3D spatial information without a compromise in computational cost. The system is evaluated on a public dataset. Preliminary results demonstrate that our approach outperforms current state-of-the-art in both prostate segmentation and cancer diagnosis.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_66');
INSERT INTO `paper` VALUES (10332, 'A Benchmark for Epithelial Cell Tracking', '', '', '', '', '', 'Segmentation and tracking of epithelial cells in light microscopy (LM) movies of developing tissue is an abundant task in cell- and developmental biology. Epithelial cells are densely packed cells that form a honeycomb-like grid. This dense packing distinguishes membrane-stained epithelial cells from the types of objects recent cell tracking benchmarks have focused on, like cell nuclei and freely moving individual cells. While semi-automated tools for segmentation and tracking of epithelial cells are available to biologists, common tools rely on classical watershed based segmentation and engineered tracking heuristics, and entail a tedious phase of manual curation. However, a different kind of densely packed cell imagery has become a focus of recent computer vision research, namely electron microscopy (EM) images of neurons. In this work we explore the benefits of two recent neuron EM segmentation methods for epithelial cell tracking in light microscopy. In particular we adapt two different deep learning approaches for neuron segmentation, namely Flood Filling Networks and MALA, to epithelial cell tracking. We benchmark these on a dataset of eight movies with up to 200 frames. We compare to Moral Lineage Tracing, a combinatorial optimization approach that recently claimed state of the art results for epithelial cell tracking. Furthermore, we compare to Tissue Analyzer, an off-the-shelf tool used by Biologists that serves as our baseline.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_33');
INSERT INTO `paper` VALUES (10333, 'A Closed-Form Solution to Photorealistic Image Stylization', 'Image stylization', 'Photorealism', 'Closed-form solution', '', '', 'Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at https://github.com/NVIDIA/FastPhotoStyle.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_28');
INSERT INTO `paper` VALUES (10334, 'A Context-Aware Capsule Network for Multi-label Classification', '', '', '', '', '', 'Recently proposed Capsule Network is a brain inspired architecture that brings a new paradigm to deep learning by modelling input domain variations through vector based representations. Despite being a seminal contribution, CapsNet does not explicitly model structured relationships between the detected entities and among the capsule features for related inputs. Motivated by the working of cortical network in HVS, we seek to resolve CapsNet limitations by proposing several intuitive modifications to the CapsNet architecture. We introduce, (1) a novel routing weight initialization technique, (2) an improved CapsNet design that exploits semantic relationships between the primary capsule activations using a densely connected Conditional Random Field and (3) a Cholesky transformation based correlation module to learn a general priority scheme. Our proposed design allows CapsNet to scale better to more complex problems, such as the multi-label classification task, where semantically related categories co-exist with various interdependencies. We present theoretical bases for our extensions and demonstrate significant improvements on ADE20K scene dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_40');
INSERT INTO `paper` VALUES (10335, 'A Dataset and Architecture for Visual Reasoning with a Working Memory', 'Visual reasoning', 'Visual question answering', 'Recurrent network', 'Working memory', '', 'A vexing problem in artificial intelligence is reasoning about events that occur in complex, changing visual stimuli such as in video analysis or game play. Inspired by a rich tradition of visual reasoning and memory in cognitive psychology and neuroscience, we developed an artificial, configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory – problems that remain challenging for modern deep learning architectures. We additionally propose a deep learning architecture that performs competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as easy settings of the COG dataset. However, several settings of COG result in datasets that are progressively more challenging to learn. After training, the network can zero-shot generalize to many new tasks. Preliminary analyses of the network architectures trained on COG demonstrate that the network accomplishes the task in a manner interpretable to humans.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_44');
INSERT INTO `paper` VALUES (10336, 'A Dataset for Lane Instance Segmentation in Urban Environments', 'Dataset', 'Urban driving', 'Road', 'Lane', 'Instance segmentation', 'Autonomous vehicles require knowledge of the surrounding road layout, which can be predicted by state-of-the-art CNNs. This work addresses the current lack of data for determining lane instances, which are needed for various driving manoeuvres. The main issue is the time-consuming manual labelling process, typically applied per image. We notice that driving the car is itself a form of annotation. Therefore, we propose a semi-automated method that allows for efficient labelling of image sequences by utilising an estimated road plane in 3D based on where the car has driven and projecting labels from this plane into all images of the sequence. The average labelling time per image is reduced to 5 s and only an inexpensive dash-cam is required for data capture. We are releasing a dataset of 24,000 images and additionally show experimental semantic segmentation and instance segmentation results.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_33');
INSERT INTO `paper` VALUES (10337, 'A Dataset of Flash and Ambient Illumination Pairs from the Crowd', 'Flash photography', 'Dataset collection', 'Crowdsourcing', 'Illumination decomposition', '', 'Illumination is a critical element of photography and is essential for many computer vision tasks. Flash light is unique in the sense that it is a widely available tool for easily manipulating the scene illumination. We present a dataset of thousands of ambient and flash illumination pairs to enable studying flash photography and other applications that can benefit from having separate illuminations. Different than the typical use of crowdsourcing in generating computer vision datasets, we make use of the crowd to directly take the photographs that make up our dataset. As a result, our dataset covers a wide variety of scenes captured by many casual photographers. We detail the advantages and challenges of our approach to crowdsourcing as well as the computational effort to generate completely separate flash illuminations from the ambient light in an uncontrolled setup. We present a brief examination of illumination decomposition, a challenging and underconstrained problem in flash photography, to demonstrate the use of our dataset in a data-driven approach.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_39');
INSERT INTO `paper` VALUES (10338, 'A Deeper Look at 3D Shape Classifiers', '', '', '', '', '', 'We investigate the role of representations and architectures for classifying 3D shapes in terms of their computational efficiency, generalization, and robustness to adversarial transformations. By varying the number of training examples and employing cross-modal transfer learning we study the role of initialization of existing deep architectures for 3D shape classification. Our analysis shows that multiview methods continue to offer the best generalization even without pretraining on large labeled image datasets, and even when trained on simplified inputs such as binary silhouettes. Furthermore, the performance of voxel-based 3D convolutional networks and point-based architectures can be improved via cross-modal transfer from image representations. Finally, we analyze the robustness of 3D shape classifiers to adversarial transformations and present a novel approach for generating adversarial perturbations of a 3D shape for multiview classifiers using a differentiable renderer. We find that point-based networks are more robust to point position perturbations while voxel-based and multiview networks are easily fooled with the addition of imperceptible noise to the input.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_49');
INSERT INTO `paper` VALUES (10339, 'A Deeply-Initialized Coarse-to-fine Ensemble of Regression Trees for Face Alignment', 'Face alignment', 'Cascaded shape regression', 'Convolutional neural networks', 'Coarse-to-Fine', 'Occlusions', 'In this paper we present DCFE, a real-time facial landmark regression method based on a coarse-to-fine Ensemble of Regression Trees (ERT). We use a simple Convolutional Neural Network (CNN) to generate probability maps of landmarks location. These are further refined with the ERT regressor, which is initialized by fitting a 3D face model to the landmark maps. The coarse-to-fine structure of the ERT lets us address the combinatorial explosion of parts deformation. With the 3D model we also tackle other key problems such as robust regressor initialization, self occlusions, and simultaneous frontal and profile face analysis. In the experiments DCFE achieves the best reported result in AFLW, COFW, and 300 W private and common public data sets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_36');
INSERT INTO `paper` VALUES (10340, 'A Digital Tool to Understand the Pictorial Procedures of 17\\(^\\mathrm{th}\\) Century Realism', 'Optical mixing', 'Convincing rendering', 'Painting reconstruction', 'Jan de Heem', '', 'To unveil the mystery of the exquisitely rendered materials in Dutch 17th century paintings, we need to understand the pictorial procedures of this period. We focused on the Dutch master Jan de Heem, known for his highly convincing still-lifes. We reconstructed his systematic multi-layered approach to paint grapes, based on pigment distribution maps, layers stratigraphy, and a 17th century textual source. We digitised the layers reconstruction to access the temporal information of the painting procedure. We combined the layers via optical mixing into a digital tool that can be used to answer “what if” art historical questions about the painting composition, by editing the order, weight and colour of the layers.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_51');
INSERT INTO `paper` VALUES (10341, 'A Fast and Scalable Pipeline for Stain Normalization of Whole-Slide Images in Histopathology', 'Histopathological image processing', 'Whole-slide images', 'Stain normalization', 'Distributed computing', 'Color deconvolution', 'Stain normalization is one of the main tasks in the processing pipeline of computer-aided diagnosis systems in modern digital pathology. Some of the challenges in this tasks are memory and runtime bottlenecks associated with large image datasets. In this work, we present a scalable and fast pipeline for stain normalization using a state-of-the-art unsupervised method based on stain-vector estimation. The proposed system supports single-node and distributed implementations. Based on a highly-optimized engine, our architecture enables high-speed and large-scale processing of high-magnification whole-slide images (WSI). We demonstrate the performance of the system using measurements from different datasets. Moreover, by using a novel pixel-sampling optimization we show lower processing time per image than the scanning time of ultrafast WSI scanners with the single-node implementation and additional 3.44 average speed-up with the 4-nodes distributed pipeline.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_32');
INSERT INTO `paper` VALUES (10342, 'A Framework for Evaluating 6-DOF Object Trackers', '3D object tracking', 'Databases', 'Deep learning', '', '', 'We present a challenging and realistic novel dataset for evaluating 6-DOF object tracking algorithms. Existing datasets show serious limitations—notably, unrealistic synthetic data, or real data with large fiducial markers—preventing the community from obtaining an accurate picture of the state-of-the-art. Using a data acquisition pipeline based on a commercial motion capture system for acquiring accurate ground truth poses of real objects with respect to a Kinect V2 camera, we build a dataset which contains a total of 297 calibrated sequences. They are acquired in three different scenarios to evaluate the performance of trackers: stability, robustness to occlusion and accuracy during challenging interactions between a person and the object. We conduct an extensive study of a deep 6-DOF tracking architecture and determine a set of optimal parameters. We enhance the architecture and the training methodology to train a 6-DOF tracker that can robustly generalize to objects never seen during training, and demonstrate favorable performance compared to previous approaches trained specifically on the objects to track.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_36');
INSERT INTO `paper` VALUES (10343, 'A Geometric Perspective on Structured Light Coding', 'SL Coding', 'Structured Light (SL)', 'Global Illumination', 'Interreflections', 'Coding Scheme', 'We present a mathematical framework for analysis and design of high-performance structured light (SL) coding schemes. Using this framework, we design Hamiltonian SL coding, a novel family of SL coding schemes that can recover 3D shape with high precision, with only a small number (as few as three) of images. We establish structural similarity between popular discrete (binary) SL coding methods, and Hamiltonian coding, which is a continuous coding approach. Based on this similarity, and by leveraging design principles from several different SL coding families, we propose a general recipe for designing Hamiltonian coding patterns with specific desirable properties, such as patterns with high spatial frequencies for dealing with global illumination. We perform several experiments to evaluate the proposed approach, and demonstrate that Hamiltonian coding based SL approaches outperform existing methods in challenging scenarios, including scenes with dark albedos, strong ambient light, and interreflections.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_6');
INSERT INTO `paper` VALUES (10344, 'A Hybrid Model for Identity Obfuscation by Face Replacement', '', '', '', '', '', 'As more and more personal photos are shared and tagged in social media, avoiding privacy risks such as unintended recognition, becomes increasingly challenging. We propose a new hybrid approach to obfuscate identities in photos by head replacement. Our approach combines state of the art parametric face synthesis with latest advances in Generative Adversarial Networks (GAN) for data-driven image synthesis. On the one hand, the parametric part of our method gives us control over the facial parameters and allows for explicit manipulation of the identity. On the other hand, the data-driven aspects allow for adding fine details and overall realism as well as seamless blending into the scene context. In our experiments we show highly realistic output of our system that improves over the previous state of the art in obfuscation rate while preserving a higher similarity to the original image content.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_34');
INSERT INTO `paper` VALUES (10345, 'A Joint Generative Model for Zero-Shot Learning', 'Zero-shot learning', 'Variational autoencoder', 'Generative adversarial network', 'Perceptual reconstruction', '', 'Zero-shot learning (ZSL) is a challenging task due to the lack of data from unseen classes during training. Existing methods tend to have the strong bias towards seen classes, which is also known as the domain shift problem. To mitigate the gap between seen and unseen class data, we propose a joint generative model to synthesize features as the replacement for unseen data. Based on the generated features, the conventional ZSL problem can be tackled in a supervised way. Specifically, our framework integrates Variational Autoencoders (VAE) and Generative Adversarial Networks (GAN) conditioned on class-level semantic attributes for feature generation based on element-wise and holistic reconstruction. A categorization network acts as the additional guide to generate features beneficial for the subsequent classification task. Moreover, we propose a perceptual reconstruction loss to preserve semantic similarities. Experimental results on five benchmarks show the superiority of our framework over the state-of-the-art approaches in terms of both conventional ZSL and generalized ZSL settings.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_50');
INSERT INTO `paper` VALUES (10346, 'A Joint Sequence Fusion Model for Video Question Answering and Retrieval', 'Multimodal retrieval', 'Video question and answering', '', '', '', 'We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (e.g. a video clip and a language sentence). Our multimodal matching network consists of two key components. First, the Joint Semantic Tensor composes a dense pairwise representation of two sequence data into a 3D tensor. Then, the Convolutional Hierarchical Decoder computes their similarity score by discovering hidden hierarchical matches between the two sequence modalities. Both modules leverage hierarchical attention mechanisms that learn to promote well-matched representation patterns while prune out misaligned ones in a bottom-up manner. Although the JSFusion is a universal model to be applicable to any multimodal sequence data, this work focuses on video-language tasks including multimodal retrieval and video QA. We evaluate the JSFusion model in three retrieval and VQA tasks in LSMDC, for which our model achieves the best performance reported so far. We also perform multiple-choice and movie retrieval tasks for the MSR-VTT dataset, on which our approach outperforms many state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_29');
INSERT INTO `paper` VALUES (10347, 'A Kinematic Chain Space for Monocular Motion Capture', '', '', '', '', '', 'This paper deals with motion capture of kinematic chains (e.g. human skeletons) from monocular image sequences taken by uncalibrated cameras. We present a method based on projecting an observation onto a kinematic chain space (KCS). An optimization of the nuclear norm is proposed that implicitly enforces structural properties of the kinematic chain. Unlike other approaches our method is not relying on training data or previously determined constraints such as particular body lengths. The proposed algorithm is able to reconstruct scenes with little or no camera motion and previously unseen motions. It is not only applicable to human skeletons but also to other kinematic chains for instance animals or industrial robots. We achieve state-of-the-art results on different benchmark databases and real world scenes.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_4');
INSERT INTO `paper` VALUES (10348, 'A Memory Model Based on the Siamese Network for Long-Term Tracking', 'Long-term tracking', 'Atkinson-Shiffrin Memory Model', 'Siamese network', 'Regional Maximum Activation of Convolutions', '', 'We propose a novel memory model using deep convolutional features for long-term tracking to handle the challenging issues, including visual deformation or target disappearance. Our memory model is separated into short- and long-term stores inspired by Atkinson-Shiffrin Memory Model (ASMM). In the tracking step, the bounding box of the target is estimated by the Siamese features obtained from both memory stores to accommodate changes in the visual appearance of the target. In the re-detection step, we take features only in the long-term store to alleviate the drift problem. At this time, we adopt a coarse-to-fine strategy to detect the target in the entire image without the dependency of the previous position. In the end, we employ Regional Maximum Activation of Convolutions (R-MAC) as key criteria. Our tracker achieves an F-score of 0.52 on the LTB35 dataset, which is 0.04 higher than the performance of the state-of-the-art algorithm.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_5');
INSERT INTO `paper` VALUES (10349, 'A Minimal Closed-Form Solution for Multi-perspective Pose Estimation using Points and Lines', 'Multi-perspective camera', 'Pose estimation', 'Points', 'Lines', '', 'We propose a minimal solution for pose estimation using both points and lines for a multi-perspective camera. In this paper, we treat the multi-perspective camera as a collection of rigidly attached perspective cameras. These type of imaging devices are useful for several computer vision applications that require a large coverage such as surveillance, self-driving cars, and motion-capture studios. While prior methods have considered the cases using solely points or lines, the hybrid case involving both points and lines has not been solved for multi-perspective cameras. We present the solutions for two cases. In the first case, we are given 2D to 3D correspondences for two points and one line. In the later case, we are given 2D to 3D correspondences for one point and two lines. We show that the solution for the case of two points and one line can be formulated as a fourth degree equation. This is interesting because we can get a closed-form solution and thereby achieve high computational efficiency. The later case involving two lines and one point can be mapped to an eighth degree equation. We show simulations and real experiments to demonstrate the advantages and benefits over existing methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_29');
INSERT INTO `paper` VALUES (10350, 'A Modulation Module for Multi-task Learning with Applications in Image Retrieval', '', '', '', '', '', 'Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset [12] and product retrieval on the UT-Zappos50K dataset [34, 35], and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_25');
INSERT INTO `paper` VALUES (10351, 'A New Large Scale Dynamic Texture Dataset with Application to ConvNet Understanding', 'Dynamic Text', 'Dynamic Texture Database (DTDB)', 'Stream Motion', 'Support Transfer Learning', 'Optical Flow', 'We introduce a new large scale dynamic texture dataset. With over 10,000 videos, our Dynamic Texture DataBase (DTDB) is two orders of magnitude larger than any previously available dynamic texture dataset. DTDB comes with two complementary organizations, one based on dynamics independent of spatial appearance and one based on spatial appearance independent of dynamics. The complementary organizations allow for uniquely insightful experiments regarding the abilities of major classes of spatiotemporal ConvNet architectures to exploit appearance vs. dynamic information. We also present a new two-stream ConvNet that provides an alternative to the standard optical-flow-based motion stream to broaden the range of dynamic patterns that can be encompassed. The resulting motion stream is shown to outperform the traditional optical flow stream by considerable margins. Finally, the utility of DTDB as a pretraining substrate is demonstrated via transfer learning on a different dynamic texture dataset as well as the companion task of dynamic scene recognition resulting in a new state-of-the-art.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_20');
INSERT INTO `paper` VALUES (10352, 'A Scalable Exemplar-Based Subspace Clustering Algorithm for Class-Imbalanced Data', 'Subspace clustering', 'Imbalanced data', 'Large-scale data', '', '', 'Subspace clustering methods based on expressing each data point as a linear combination of a few other data points (e.g., sparse subspace clustering) have become a popular tool for unsupervised learning due to their empirical success and theoretical guarantees. However, their performance can be affected by imbalanced data distributions and large-scale datasets. This paper presents an exemplar-based subspace clustering method to tackle the problem of imbalanced and large-scale datasets. The proposed method searches for a subset of the data that best represents all data points as measured by the \\(\\ell _1\\) norm of the representation coefficients. To solve our model efficiently, we introduce a farthest first search algorithm which iteratively selects the least well-represented point as an exemplar. When data comes from a union of subspaces, we prove that the computed subset contains enough exemplars from each subspace for expressing all data points even if the data are imbalanced. Our experiments demonstrate that the proposed method outperforms state-of-the-art subspace clustering methods in two large-scale image datasets that are imbalanced. We also demonstrate the effectiveness of our method on unsupervised data subset selection for a face image classification task.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_5');
INSERT INTO `paper` VALUES (10353, 'A Segmentation-Aware Deep Fusion Network for Compressed Sensing MRI', 'Compressed sensing', 'Magnetic resonance imaging', 'Medical image segmentation', 'Deep neural network', '', 'Compressed sensing MRI is a classic inverse problem in the field of computational imaging, accelerating the MR imaging by measuring less k-space data. The deep neural network models provide the stronger representation ability and faster reconstruction compared with “shallow” optimization-based methods. However, in the existing deep-based CS-MRI models, the high-level semantic supervision information from massive segmentation-labels in MRI dataset is overlooked. In this paper, we proposed a segmentation-aware deep fusion network called SADFN for compressed sensing MRI. The multilayer feature aggregation (MLFA) method is introduced here to fuse all the features from different layers in the segmentation network. Then, the aggregated feature maps containing semantic information are provided to each layer in the reconstruction network with a feature fusion strategy. This guarantees the reconstruction network is aware of the different regions in the image it reconstructs, simplifying the function mapping. We prove the utility of the cross-layer and cross-task information fusion strategy by comparative study. Extensive experiments on brain segmentation benchmark MRBrainS and BratS15 validated that the proposed SADFN model achieves state-of-the-art accuracy in compressed sensing MRI. This paper provides a novel approach to guide the low-level visual task using the information from mid- or high-level task.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_4');
INSERT INTO `paper` VALUES (10354, 'A Semi-supervised Data Augmentation Approach Using 3D Graphical Engines', 'Data augmentation', 'Deep learning', 'Domain adaptation', 'Human pose estimation', 'Low dimensional subspace learning', 'Deep learning approaches have been rapidly adopted across a wide range of fields because of their accuracy and flexibility, but require large labeled training datasets. This presents a fundamental problem for applications with limited, expensive, or private data (i.e. small data), such as human pose and behavior estimation/tracking which could be highly personalized. In this paper, we present a semi-supervised data augmentation approach that can synthesize large scale labeled training datasets using 3D graphical engines based on a physically-valid low dimensional pose descriptor. To evaluate the performance of our synthesized datasets in training deep learning-based models, we generated a large synthetic human pose dataset, called ScanAva using 3D scans of only 7 individuals based on our proposed augmentation approach. A state-of-the-art human pose estimation deep learning model then was trained from scratch using our ScanAva dataset and could achieve the pose estimation accuracy of 91.2% at PCK0.5 criteria after applying an efficient domain adaptation on the synthetic images, in which its pose estimation accuracy was comparable to the same model trained on large scale pose data from real humans such as MPII dataset and much higher than the model trained on other synthetic human dataset such as SURREAL.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_31');
INSERT INTO `paper` VALUES (10355, 'A Semi-supervised Deep Generative Model for Human Body Analysis', 'Deep generative models', 'Variational autoencoders', 'Semi-supervised learning', 'Human pose estimation', 'Analysis-by-synthesis', 'Deep generative modelling for human body analysis is an emerging problem with many interesting applications. However, the latent space learned by such models is typically not interpretable, resulting in less flexible models. In this work, we adopt a structured semi-supervised approach and present a deep generative model for human body analysis where the body pose and the visual appearance are disentangled in the latent space. Such a disentanglement allows independent manipulation of pose and appearance, and hence enables applications such as pose-transfer without being explicitly trained for such a task. In addition, our setting allows for semi-supervised pose estimation, relaxing the need for labelled data. We demonstrate the capabilities of our generative model on the Human3.6M and on the DeepFashion datasets.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_38');
INSERT INTO `paper` VALUES (10356, 'A Simple and Effective Fusion Approach for Multi-frame Optical Flow Estimation', 'Multi-frame optical flow', 'Temporal optical flow fusion', '', '', '', 'To date, top-performing optical flow estimation methods only take pairs of consecutive frames into account. While elegant and appealing, the idea of using more than two frames has not yet produced state-of-the-art results. We present a simple, yet effective fusion approach for multi-frame optical flow that benefits from longer-term temporal cues. Our method first warps the optical flow from previous frames to the current, thereby yielding multiple plausible estimates. It then fuses the complementary information carried by these estimates into a new optical flow field. At the time of writing, our method ranks first among published results in the MPI Sintel and KITTI 2015 benchmarks.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_53');
INSERT INTO `paper` VALUES (10357, 'A Simple Approach to Intrinsic Correspondence Learning on Unstructured 3D Meshes', 'Shape correspondence estimation', 'Learning on graphs', '', '', '', 'The question of representation of 3D geometry is of vital importance when it comes to leveraging the recent advances in the field of machine learning for geometry processing tasks. For common unstructured surface meshes state-of-the-art methods rely on patch-based or mapping-based techniques that introduce resampling operations in order to encode neighborhood information in a structured and regular manner. We investigate whether such resampling can be avoided, and propose a simple and direct encoding approach. It does not only increase processing efficiency due to its simplicity – its direct nature also avoids any loss in data fidelity. To evaluate the proposed method, we perform a number of experiments in the challenging domain of intrinsic, non-rigid shape correspondence estimation. In comparisons to current methods we observe that our approach is able to achieve highly competitive results.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_26');
INSERT INTO `paper` VALUES (10358, 'A Structured Listwise Approach to Learning to Rank for Image Tagging', 'Learning to rank', 'Zero-shot learning', 'Image tagging', 'Visual-semantic compatibility', 'Multimodal embedding', 'With the growing quantity and diversity of publicly available image data, computer vision plays a crucial role in understanding and organizing visual information today. Image tagging models are very often used to make this data accessible and useful. Generating image labels and ranking them by their relevance to the visual content is still an open problem. In this work, we use a bilinear compatibility function inspired from zero-shot learning that allows us to rank tags according to their relevance to the image content. We propose a novel listwise structured loss formulation to learn it from data. We leverage captioned image data and propose different “tags from captions” schemes meant to capture user attention and intra-user agreement in a simple and effective manner. We evaluate our method on the COCO-Captions, PASCAL-sentences and MIRFlickr-25k datasets showing promising results.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_42');
INSERT INTO `paper` VALUES (10359, 'A Style-Aware Content Loss for Real-Time HD Style Transfer', 'Style transfer', 'Generative network', 'Deep learning', '', '', 'Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_43');
INSERT INTO `paper` VALUES (10360, 'A Summary of the 4th International Workshop on Recovering 6D Object Pose', '', '', '', '', '', 'This document summarizes the 4th International Workshop on Recovering 6D Object Pose which was organized in conjunction with ECCV 2018 in Munich. The workshop featured four invited talks, oral and poster presentations of accepted workshop papers, and an introduction of the BOP benchmark for 6D object pose estimation. The workshop was attended by 100+ people working on relevant topics in both academia and industry who shared up-to-date advances and discussed open problems.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_36');
INSERT INTO `paper` VALUES (10361, 'A Systematic DNN Weight Pruning Framework Using Alternating Direction Method of Multipliers', 'Systematic weight pruning', 'Deep neural networks (DNNs)', 'Alternating direction method of multipliers (ADMM)', '', '', 'Weight pruning methods for deep neural networks (DNNs) have been investigated recently, but prior work in this area is mainly heuristic, iterative pruning, thereby lacking guarantees on the weight reduction ratio and convergence time. To mitigate these limitations, we present a systematic weight pruning framework of DNNs using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a nonconvex optimization problem with combinatorial constraints specifying the sparsity requirements, and then adopt the ADMM framework for systematic weight pruning. By using ADMM, the original nonconvex optimization problem is decomposed into two subproblems that are solved iteratively. One of these subproblems can be solved using stochastic gradient descent, the other can be solved analytically. Besides, our method achieves a fast convergence rate.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_12');
INSERT INTO `paper` VALUES (10362, 'A Top-Down Approach to Articulated Human Pose Estimation and Tracking', 'Multi-person pose estimation', 'Multi-person pose tracking', '', '', '', 'Both the tasks of multi-person human pose estimation and pose tracking in videos are quite challenging. Existing methods can be categorized into two groups: top-down and bottom-up approaches. In this paper, following the top-down approach, we aim to build a strong baseline system with three modules: human candidate detector, single-person pose estimator and human pose tracker. Firstly, we choose a generic object detector among state-of-the-art methods to detect human candidates. Then, cascaded pyramid network is used to estimate the corresponding human pose. Finally, we use a flow-based pose tracker to render keypoint-association across frames, i.e., assigning each human candidate a unique and temporally-consistent id, for the multi-target pose tracking purpose. We conduct extensive ablative experiments to validate various choices of models and configurations. We take part in two ECCV’18 PoseTrack challenges (https://posetrack.net/workshops/eccv2018/posetrack_eccv_2018_results.html): pose estimation and pose tracking.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_20');
INSERT INTO `paper` VALUES (10363, 'A Trilateral Weighted Sparse Coding Scheme for Real-World Image Denoising', 'Real-world image denoising', 'Sparse coding', '', '', '', 'Most of existing image denoising methods assume the corrupted noise to be additive white Gaussian noise (AWGN). However, the realistic noise in real-world noisy images is much more complex than AWGN, and is hard to be modeled by simple analytical distributions. As a result, many state-of-the-art denoising methods in literature become much less effective when applied to real-world noisy images captured by CCD or CMOS cameras. In this paper, we develop a trilateral weighted sparse coding (TWSC) scheme for robust real-world image denoising. Specifically, we introduce three weight matrices into the data and regularization terms of the sparse coding framework to characterize the statistics of realistic noise and image priors. TWSC can be reformulated as a linear equality-constrained problem and can be solved by the alternating direction method of multipliers. The existence and uniqueness of the solution and convergence of the proposed algorithm are analyzed. Extensive experiments demonstrate that the proposed TWSC scheme outperforms state-of-the-art denoising methods on removing realistic noise.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_2');
INSERT INTO `paper` VALUES (10364, 'A Unified Framework for Multi-view Multi-class Object Pose Estimation', 'Object pose estimation', 'Multi-view recognition', 'Deep learning', '', '', 'One[NOSPACE] [NOSPACE][SPACE]core challenge in object pose estimation is to ensure accurate and robust performance for large numbers of diverse foreground objects amidst complex background clutter. In this work, we present a scalable framework for accurately inferring six Degree-of-Freedom (6-DoF) pose for a large number of object classes from single or multiple views. To learn discriminative pose features, we integrate three new capabilities into a deep Convolutional Neural Network (CNN): an inference scheme that combines both classification and pose regression based on a uniform tessellation of the Special Euclidean group in three dimensions (SE(3)), the fusion of class priors into the training process via a tiled class map, and an additional regularization using deep supervision with an object mask. Further, an efficient multi-view framework is formulated to address single-view ambiguity. We show that this framework consistently improves the performance of the single-view network. We evaluate our method on three large-scale benchmarks: YCB-Video, JHUScene-50 and ObjectNet-3D. Our approach achieves competitive or superior performance over the current state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_16');
INSERT INTO `paper` VALUES (10365, 'A Zero-Shot Framework for Sketch Based Image Retrieval', 'Image retrieval', 'Zero-shot learning', '', '', '', 'Sketch-based image retrieval (SBIR) is the task of retrieving images from a natural image database that correspond to a given hand-drawn sketch. Ideally, an SBIR model should learn to associate components in the sketch (say, feet, tail, etc.) with the corresponding components in the image having similar shape characteristics. However, current evaluation methods simply focus only on coarse-grained evaluation where the focus is on retrieving images which belong to the same class as the sketch but not necessarily having the same shape characteristics as in the sketch. As a result, existing methods simply learn to associate sketches with classes seen during training and hence fail to generalize to unseen classes. In this paper, we propose a new benchmark for zero-shot SBIR where the model is evaluated on novel classes that are not seen during training. We show through extensive experiments that existing models for SBIR that are trained in a discriminative setting learn only class specific mappings and fail to generalize to the proposed zero-shot setting. To circumvent this, we propose a generative approach for the SBIR task by proposing deep conditional generative models that take the sketch as an input and fill the missing information stochastically. Experiments on this new benchmark created from the “Sketchy” dataset, which is a large-scale database of sketch-photo pairs demonstrate that the performance of these generative models is significantly better than several state-of-the-art approaches in the proposed zero-shot framework of the coarse-grained SBIR task.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_19');
INSERT INTO `paper` VALUES (10366, 'A+D Net: Training a Shadow Detector with Adversarial Shadow Attenuation', 'Shadow detection', 'GAN', 'Data augmentation', '', '', 'We propose a novel GAN-based framework for detecting shadows in images, in which a shadow detection network (D-Net) is trained together with a shadow attenuation network (A-Net) that generates adversarial training examples. The A-Net modifies the original training images constrained by a simplified physical shadow model and is focused on fooling the D-Net’s shadow predictions. Hence, it is effectively augmenting the training data for D-Net with hard-to-predict cases. The D-Net is trained to predict shadows in both original images and generated images from the A-Net. Our experimental results show that the additional training data from A-Net significantly improves the shadow detection accuracy of D-Net. Our method outperforms the state-of-the-art methods on the most challenging shadow detection benchmark (SBU) and also obtains state-of-the-art results on a cross-dataset task, testing on UCF. Furthermore, the proposed method achieves accurate real-time shadow detection at 45 frames per second.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_41');
INSERT INTO `paper` VALUES (10367, 'A-Contrario Horizon-First Vanishing Point Detection Using Second-Order Grouping Laws', 'Horizon line', 'Vanishing point detection', 'A-contrario model', 'Perceptual grouping', 'Gestalt theory', 'We show that, in images of man-made environments, the horizon line can usually be hypothesized based on a-contrario detections of second-order grouping events. This allows constraining the extraction of the horizontal vanishing points on that line, thus reducing false detections. Experiments made on three datasets show that our method, not only achieves state-of-the-art performance w.r.t. horizon line detection on two datasets, but also yields much less spurious vanishing points than the previous top-ranked methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_20');
INSERT INTO `paper` VALUES (10368, 'Accelerating Dynamic Programs via Nested Benders Decomposition with Application to Multi-Person Pose Estimation', 'Nested benders decomposition', 'Column generation', 'Multi-person pose estimation', '', '', 'We present a novel approach to solve dynamic programs (DP), which are frequent in computer vision, on tree-structured graphs with exponential node state space. Typical DP approaches have to enumerate the joint state space of two adjacent nodes on every edge of the tree to compute the optimal messages. Here we propose an algorithm based on Nested Benders Decomposition (NBD) that iteratively lower-bounds the message on every edge and promises to be far more efficient. We apply our NBD algorithm along with a novel Minimum Weight Set Packing (MWSP) formulation to a multi-person pose estimation problem. While our algorithm is provably optimal at termination it operates in linear time for practical DP problems, gaining up to 500\\({\\times }\\) speed up over traditional DP algorithm which have polynomial complexity.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_40');
INSERT INTO `paper` VALUES (10369, 'Accurate Scene Text Detection Through Border Semantics Awareness and Bootstrapping', 'Scene text detection', 'Data augmentation', 'Semantics-aware detection', 'Deep network models', '', 'This paper presents a scene text detection technique that exploits bootstrapping and text border semantics for accurate localization of texts in scenes. A novel bootstrapping technique is designed which samples multiple ‘subsections’ of a word or text line and accordingly relieves the constraint of limited training data effectively. At the same time, the repeated sampling of text ‘subsections’ improves the consistency of the predicted text feature maps which is critical in predicting a single complete instead of multiple broken boxes for long words or text lines. In addition, a semantics-aware text border detection technique is designed which produces four types of text border segments for each scene text. With semantics-aware text borders, scene texts can be localized more accurately by regressing text pixels around the ends of words or text lines instead of all text pixels which often leads to inaccurate localization while dealing with long words or text lines. Extensive experiments demonstrate the effectiveness of the proposed techniques, and superior performance is obtained over several public datasets, e.g. 80.1 f-score for the MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_22');
INSERT INTO `paper` VALUES (10370, 'Acquisition of Localization Confidence for Accurate Object Detection', 'Object localization', 'Bounding box regression', 'Non-maximum suppression', '', '', 'Modern CNN-based object detectors rely on bounding box regression and non-maximum suppression to localize objects. While the probabilities for class labels naturally reflect classification confidence, localization confidence is absent. This makes properly localized bounding boxes degenerate during iterative regression or even suppressed during NMS. In the paper we propose IoU-Net learning to predict the IoU between each detected bounding box and the matched ground-truth. The network acquires this confidence of localization, which improves the NMS procedure by preserving accurately localized bounding boxes. Furthermore, an optimization-based bounding box refinement method is proposed, where the predicted IoU is formulated as the objective. Extensive experiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well as its compatibility with and adaptivity to several state-of-the-art object detectors.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_48');
INSERT INTO `paper` VALUES (10371, 'Action Alignment from Gaze Cues in Human-Human and Human-Robot Interaction', 'Action anticipation', 'Gaze behavior', 'Action alignment', 'Human-robot interaction', '', 'Cognitive neuroscience experiments show how people intensify the exchange of non-verbal cues when they work on a joint task towards a common goal. When individuals share their intentions, it creates a social interaction that drives the mutual alignment of their actions and behavior. To understand the intentions of others, we strongly rely on the gaze cues. According to the role each person plays in the interaction, the resulting alignment of the body and gaze movements will be different. This mechanism is key to understand and model dyadic social interactions.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_17');
INSERT INTO `paper` VALUES (10372, 'Action Anticipation by Predicting Future Dynamic Images', 'Action-anticipation', 'Prediction', 'Generation', 'Motion representation', 'Dynamic image', 'Human action-anticipation methods predict what is the future action by observing only a few portion of an action in progress. This is critical for applications where computers have to react to human actions as early as possible such as autonomous driving, human-robotic interaction, assistive robotics among others. In this paper, we present a method for human action anticipation by predicting the most plausible future human motion. We represent human motion using Dynamic Images [1] and make use of tailored loss functions to encourage a generative model to produce accurate future motion prediction. Our method outperforms the currently best performing action-anticipation methods by 4% on JHMDB-21, 5.2% on UT-Interaction and 5.1% on UCF 101-24 benchmarks.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_10');
INSERT INTO `paper` VALUES (10373, 'Action Anticipation with RBF Kernelized Feature Mapping RNN', 'Human action prediction', 'novel Recurrent Neural Network', 'Radial Basis Function kernel', 'Adversarial training', '', 'We introduce a novel Recurrent Neural Network-based algorithm for future video feature generation and action anticipation called feature mapping RNN. Our novel RNN architecture builds upon three effective principles of machine learning, namely parameter sharing, Radial Basis Function kernels and adversarial training. Using only some of the earliest frames of a video, the feature mapping RNN is able to generate future features with a fraction of the parameters needed in traditional RNN. By feeding these future features into a simple multilayer perceptron facilitated with an RBF kernel layer, we are able to accurately predict the action in the video.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_19');
INSERT INTO `paper` VALUES (10374, 'Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization', 'Video understanding', 'Action localization', 'Action spotting', '', '', 'State-of-the-art temporal action detectors inefficiently search the entire video for specific actions. Despite the encouraging progress these methods achieve, it is crucial to design automated approaches that only explore parts of the video which are the most relevant to the actions being searched for. To address this need, we propose the new problem of action spotting in video, which we define as finding a specific action in a video while observing a small portion of that video. Inspired by the observation that humans are extremely efficient and accurate in spotting and finding action instances in video, we propose Action Search, a novel Recurrent Neural Network approach that mimics the way humans spot actions. Moreover, to address the absence of data recording the behavior of human annotators, we put forward the Human Searches dataset, which compiles the search sequences employed by human annotators spotting actions in the AVA and THUMOS14 datasets. We consider temporal action localization as an application of the action spotting problem. Experiments on the THUMOS14 dataset reveal that our model is not only able to explore the video efficiently (observing on average \\(\\mathbf {17.3}\\%\\) of the video) but it also accurately finds human activities with \\(\\mathbf {30.8}\\%\\) mAP.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_16');
INSERT INTO `paper` VALUES (10375, 'Active Descriptor Learning for Feature Matching', 'Feature matching', 'Active learning', 'Curriculum learning', '', '', 'Feature descriptor extraction lies at the core of many computer vision tasks including image retrieval and registration. In this paper, we present an active learning method for extracting efficient features to be used in matching image patches. We train a Siamese deep neural network by optimizing a triplet loss function. We develop a more efficient and faster training procedure compared to the state-of-the-art methods by increasing difficulty during batch training. We achieve this by adjusting the margin in the loss and picking harder samples over time. The experiments are carried out on Photo Tourism dataset. The results show a significant improvement on matching performance and faster convergence in training.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_49');
INSERT INTO `paper` VALUES (10376, 'ActiveStereoNet: End-to-End Self-supervised Learning for Active Stereo Systems', 'Active Stereo', 'Depth estimation', 'Self-supervised learning', 'Neural network', 'Occlusion handling', 'In this paper we present ActiveStereoNet, the first deep learning solution for active stereo systems. Due to the lack of ground truth, our method is fully self-supervised, yet it produces precise depth with a subpixel precision of 1 / 30th of a pixel; it does not suffer from the common over-smoothing issues; it preserves the edges; and it explicitly handles occlusions. We introduce a novel reconstruction loss that is more robust to noise and texture-less patches, and is invariant to illumination changes. The proposed loss is optimized using a window-based cost aggregation with an adaptive support weight scheme. This cost aggregation is edge-preserving and smooths the loss function, which is key to allow the network to reach compelling results. Finally we show how the task of predicting invalid regions, such as occlusions, can be trained end-to-end without ground-truth. This component is crucial to reduce blur and particularly improves predictions along depth discontinuities. Extensive quantitatively and qualitatively evaluations on real and synthetic data demonstrate state of the art results in many challenging scenes.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_48');
INSERT INTO `paper` VALUES (10377, 'Actor-Centric Relation Network', 'Spatio-temporal action detection', 'Relation networks', '', '', '', 'Current state-of-the-art approaches for spatio-temporal action localization rely on detections at the frame level and model temporal context with 3D ConvNets. Here, we go one step further and model spatio-temporal relations to capture the interactions between human actors, relevant objects and scene elements essential to differentiate similar human actions. Our approach is weakly supervised and mines the relevant elements automatically with an actor-centric relational network (ACRN). ACRN computes and accumulates pair-wise relation information from actor and global scene features, and generates relation features for action classification. It is implemented as neural networks and can be trained jointly with an existing action detection system. We show that ACRN outperforms alternative approaches which capture relation information, and that the proposed framework improves upon the state-of-the-art performance on JHMDB and AVA. A visualization of the learned relation features confirms that our approach is able to attend to the relevant relations for each action.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_20');
INSERT INTO `paper` VALUES (10378, 'Adapting Egocentric Visual Hand Pose Estimation Towards a Robot-Controlled Exoskeleton', 'Hand pose estimation', 'Egocentric view', 'Grasp assistance', 'Simulation', '', 'The basic idea behind a wearable robotic grasp assistance system is to support people that suffer from severe motor impairments in daily activities. Such a system needs to act mostly autonomously and according to the user’s intent. Vision-based hand pose estimation could be an integral part of a larger control and assistance framework. In this paper we evaluate the performance of egocentric monocular hand pose estimation for a robot-controlled hand exoskeleton in a simulation. For hand pose estimation we adopt a Convolutional Neural Network (CNN). We train and evaluate this network with computer graphics, created by our own data generator. In order to guide further design decisions we focus in our experiments on two egocentric camera viewpoints tested on synthetic data with the help of a 3D-scanned hand model, with and without an exoskeleton attached to it. We observe that hand pose estimation with a wrist-mounted camera performs more accurate than with a head-mounted camera in the context of our simulation. Further, a grasp assistance system attached to the hand alters visual appearance and can improve hand pose estimation. Our experiment provides useful insights for the integration of sensors into a context sensitive analysis framework for intelligent assistance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_16');
INSERT INTO `paper` VALUES (10379, 'Adaptive Affinity Fields for Semantic Segmentation', 'Semantic segmentation', 'Affinity field', 'Adversarial learning', '', '', 'Semantic segmentation has made much progress with increasingly powerful pixel-wise classifiers and incorporating structural priors via Conditional Random Fields (CRF) or Generative Adversarial Networks (GAN). We propose a simpler alternative that learns to verify the spatial structure of segmentation during training only. Unlike existing approaches that enforce semantic labels on individual pixels and match labels between neighbouring pixels, we propose the concept of Adaptive Affinity Fields (AAF) to capture and match the semantic relations between neighbouring pixels in the label space. We use adversarial learning to select the optimal affinity field size for each semantic category. It is formulated as a minimax problem, optimizing our segmentation neural network in a best worst-case learning scenario. AAF is versatile for representing structures as a collection of pixel-centric relations, easier to train than GAN and more efficient than CRF without run-time inference. Our extensive evaluations on PASCAL VOC 2012, Cityscapes, and GTA5 datasets demonstrate its above-par segmentation performance and robust generalization across domains.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_36');
INSERT INTO `paper` VALUES (10380, 'Adaptively Transforming Graph Matching', 'Graph matching', 'Transformation representation', 'Frank-Wolfe method', '', '', 'Recently, many graph matching methods that incorporate pairwise constraint and that can be formulated as a quadratic assignment problem (QAP) have been proposed. Although these methods demonstrate promising results for the graph matching problem, they have high complexity in space or time. In this paper, we introduce an adaptively transforming graph matching (ATGM) method from the perspective of functional representation. More precisely, under a transformation formulation, we aim to match two graphs by minimizing the discrepancy between the original graph and the transformed graph. With a linear representation map of the transformation, the pairwise edge attributes of graphs are explicitly represented by unary node attributes, which enables us to reduce the space and time complexity significantly. Due to an efficient Frank-Wolfe method-based optimization strategy, we can handle graphs with hundreds and thousands of nodes within an acceptable amount of time. Meanwhile, because transformation map can preserve graph structures, a domain adaptation-based strategy is proposed to remove the outliers. The experimental results demonstrate that our proposed method outperforms the state-of-the-art graph matching algorithms.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_38');
INSERT INTO `paper` VALUES (10381, 'Adding Attentiveness to the Neurons in Recurrent Neural Networks', 'Element-wise-Attention Gate (EleAttG)', 'Recurrent neural networks', 'Action recognition', 'Skeleton', 'RGB video', 'Recurrent neural networks (RNNs) are capable of modeling the temporal dynamics of complex sequential information. However, the structures of existing RNN neurons mainly focus on controlling the contributions of current and historical information but do not explore the different importance levels of different elements in an input vector of a time slot. We propose adding a simple yet effective Element-wise-Attention Gate (EleAttG) to an RNN block (e.g., all RNN neurons in a network layer) that empowers the RNN neurons to have the attentiveness capability. For an RNN block, an EleAttG is added to adaptively modulate the input by assigning different levels of importance, i.e., attention, to each element/dimension of the input. We refer to an RNN block equipped with an EleAttG as an EleAtt-RNN block. Specifically, the modulation of the input is content adaptive and is performed at fine granularity, being element-wise rather than input-wise. The proposed EleAttG, as an additional fundamental unit, is general and can be applied to any RNN structures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU). We demonstrate the effectiveness of the proposed EleAtt-RNN by applying it to the action recognition tasks on both 3D human skeleton data and RGB videos. Experiments show that adding attentiveness through EleAttGs to RNN blocks significantly boosts the power of RNNs.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_9');
INSERT INTO `paper` VALUES (10382, 'Adding New Tasks to a Single Network with Weight Transformations Using Binary Masks', 'Incremental learning', 'Multi-task learning', '', '', '', 'Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_14');
INSERT INTO `paper` VALUES (10383, 'Adding Object Detection Skills to Visual Dialogue Agents', 'Visual dialogue', 'Object detection', '', '', '', 'Our goal is to equip a dialogue agent that asks questions about a visual scene with object detection skills. We take the first steps in this direction within the GuessWhat?! game. We use Mask R-CNN object features as a replacement for ground-truth annotations in the Guesser module, achieving an accuracy of 57.92%. This proves that our system is a viable alternative to the original Guesser, which achieves an accuracy of 62.77% using ground-truth annotations, and thus should be considered an upper bound for our automated system. Crucially, we show that our system exploits the Mask R-CNN object features, in contrast to the original Guesser augmented with global, VGG features. Furthermore, by automating the object detection in GuessWhat?!, we open up a spectrum of opportunities, such as playing the game with new, non-annotated images and using the more granular visual features to condition the other modules of the game architecture.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_17');
INSERT INTO `paper` VALUES (10384, 'Adversarial Examples Detection in Features Distance Spaces', 'Adversarial examples', 'Distance spaces', 'Deep features', 'Machine learning security', '', 'Maliciously manipulated inputs for attacking machine learning methods – in particular deep neural networks – are emerging as a relevant issue for the security of recent artificial intelligence technologies, especially in computer vision. In this paper, we focus on attacks targeting image classifiers implemented with deep neural networks, and we propose a method for detecting adversarial images which focuses on the trajectory of internal representations (i.e. hidden layers neurons activation, also known as deep features) from the very first, up to the last. We argue that the representations of adversarial inputs follow a different evolution with respect to genuine inputs, and we define a distance-based embedding of features to efficiently encode this information. We train an LSTM network that analyzes the sequence of deep features embedded in a distance space to detect adversarial examples. The results of our preliminary experiments are encouraging: our detection scheme is able to detect adversarial inputs targeted to the ResNet-50 classifier pre-trained on the ILSVRC’12 dataset and generated by a variety of crafting algorithms.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_26');
INSERT INTO `paper` VALUES (10385, 'Adversarial Geometry-Aware Human Motion Prediction', 'Human motion prediction', 'Adversarial learning', 'Geodesic loss', '', '', 'We explore an approach to forecasting human motion in a few milliseconds given an input 3D skeleton sequence based on a recurrent encoder-decoder framework. Current approaches suffer from the problem of prediction discontinuities and may fail to predict human-like motion in longer time horizons due to error accumulation. We address these critical issues by incorporating local geometric structure constraints and regularizing predictions with plausible temporal smoothness and continuity from a global perspective. Specifically, rather than using the conventional Euclidean loss, we propose a novel frame-wise geodesic loss as a geometrically meaningful, more precise distance measurement. Moreover, inspired by the adversarial training mechanism, we present a new learning procedure to simultaneously validate the sequence-level plausibility of the prediction and its coherence with the input sequence by introducing two global recurrent discriminators. An unconditional, fidelity discriminator and a conditional, continuity discriminator are jointly trained along with the predictor in an adversarial manner. Our resulting adversarial geometry-aware encoder-decoder (AGED) model significantly outperforms state-of-the-art deep learning based approaches on the heavily benchmarked H3.6M dataset in both short-term and long-term predictions.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_48');
INSERT INTO `paper` VALUES (10386, 'Adversarial Network Compression', '', '', '', '', '', 'Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this work, our objective is to transfer knowledge from a deep and accurate model to a smaller one. Our contributions are threefold: (i) we propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) we introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) our approach generalizes on different teacher-student models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_37');
INSERT INTO `paper` VALUES (10387, 'Adversarial Open-World Person Re-Identification', '', '', '', '', '', 'In a typical real-world application of re-id, a watch-list (gallery set) of a handful of target people (e.g. suspects) to track around a large volume of non-target people are demanded across camera views, and this is called the open-world person re-id. Different from conventional (closed-world) person re-id, a large portion of probe samples are not from target people in the open-world setting. And, it always happens that a non-target person would look similar to a target one and therefore would seriously challenge a re-id system. In this work, we introduce a deep open-world group-based person re-id model based on adversarial learning to alleviate the attack problem caused by similar non-target people. The main idea is learning to attack feature extractor on the target people by using GAN to generate very target-like images (imposters), and in the meantime the model will make the feature extractor learn to tolerate the attack by discriminative learning so as to realize group-based verification. The framework we proposed is called the adversarial open-world person re-identification, and this is realized by our Adversarial PersonNet (APN) that jointly learns a generator, a person discriminator, a target discriminator and a feature extractor, where the feature extractor and target discriminator share the same weights so as to makes the feature extractor learn to tolerate the attack by imposters for better group-based verification. While open-world person re-id is challenging, we show for the first time that the adversarial-based approach helps stabilize person re-id system under imposter attack more effectively.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_18');
INSERT INTO `paper` VALUES (10388, 'ADVIO: An Authentic Dataset for Visual-Inertial Odometry', 'Visual-inertial odometry', 'Navigation', 'Benchmarking', '', '', 'The lack of realistic and open benchmarking datasets for pedestrian visual-inertial odometry has made it hard to pinpoint differences in published methods. Existing datasets either lack a full six degree-of-freedom ground-truth or are limited to small spaces with optical tracking systems. We take advantage of advances in pure inertial navigation, and develop a set of versatile and challenging real-world computer vision benchmark sets for visual-inertial odometry. For this purpose, we have built a test rig equipped with an iPhone, a Google Pixel Android phone, and a Google Tango device. We provide a wide range of raw sensor data that is accessible on almost any modern-day smartphone together with a high-quality ground-truth track. We also compare resulting visual-inertial tracks from Google Tango, ARCore, and Apple ARKit with two recent methods published in academic forums. The data sets cover both indoor and outdoor cases, with stairs, escalators, elevators, office environments, a shopping mall, and metro station.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_26');
INSERT INTO `paper` VALUES (10389, 'ADVISE: Symbolism and External Knowledge for Decoding Advertisements', 'Advertisements', 'Symbolism', 'Question answering', 'External knowledge', 'Vision and language', 'In order to convey the most content in their limited space, advertisements embed references to outside knowledge via symbolism. For example, a motorcycle stands for adventure (a positive property the ad wants associated with the product being sold), and a gun stands for danger (a negative property to dissuade viewers from undesirable behaviors). We show how to use symbolic references to better understand the meaning of an ad. We further show how anchoring ad understanding in general-purpose object recognition and image captioning improves results. We formulate the ad understanding task as matching the ad image to human-generated statements that describe the action that the ad prompts, and the rationale it provides for taking this action. Our proposed method outperforms the state of the art on this task, and on an alternative formulation of question-answering on ads. We show additional applications of our learned representations for matching ads to slogans, and clustering ads according to their topic, without extra training.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_51');
INSERT INTO `paper` VALUES (10390, 'Aerial GANeration: Towards Realistic Data Augmentation Using Conditional GANs', 'Conditional GANs', 'Sensor fusion', 'Aerial perception', 'Object detection', 'Semantic segmentation', 'Environmental perception for autonomous aerial vehicles is a rising field. Recent years have shown a strong increase of performance in terms of accuracy and efficiency with the aid of convolutional neural networks. Thus, the community has established data sets for benchmarking several kinds of algorithms. However, public data is rare for multi-sensor approaches or either not large enough to train very accurate algorithms. For this reason, we propose a method to generate multi-sensor data sets using realistic data augmentation based on conditional generative adversarial networks (cGAN). cGANs have shown impressive results for image to image translation. We use this principle for sensor simulation. Hence, there is no need for expensive and complex 3D engines. Our method encodes ground truth data, e.g. semantics or object boxes that could be drawn randomly, in the conditional image to generate realistic consistent sensor data. Our method is proven for aerial object detection and semantic segmentation on visual data, such as 3D Lidar reconstruction using the ISPRS and DOTA data set. We demonstrate qualitative accuracy improvements for state-of-the-art object detection (YOLO) using our augmentation technique.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_5');
INSERT INTO `paper` VALUES (10391, 'Affine Correspondences Between Central Cameras for Rapid Relative Pose Estimation', 'Relative pose', 'Affine correspondences', 'Central cameras', '', '', 'This paper presents a novel algorithm to estimate the relative pose, i.e. the 3D rotation and translation of two cameras, from two affine correspondences (ACs) considering any central camera model. The solver is built on new epipolar constraints describing the relationship of an AC and any central views. We also show that the pinhole case is a specialization of the proposed approach. Benefiting from the low number of required correspondences, robust estimators like LO-RANSAC need fewer samples, and thus terminate earlier than using the five-point method. Tests on publicly available datasets containing pinhole, fisheye and catadioptric camera images confirmed that the method often leads to results superior to the state-of-the-art in terms of geometric accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_30');
INSERT INTO `paper` VALUES (10392, 'Affinity Derivation and Graph Merge for Instance Segmentation', 'Instance segmentation', 'Pixel affinity', 'Graph merge', 'Proposal-free', '', 'We present an instance segmentation scheme based on pixel affinity information, which is the relationship of two pixels belonging to the same instance. In our scheme, we use two neural networks with similar structures. One predicts the pixel level semantic score and the other is designed to derive pixel affinities. Regarding pixels as the vertexes and affinities as edges, we then propose a simple yet effective graph merge algorithm to cluster pixels into instances. Experiments show that our scheme generates fine grained instance masks. With Cityscape training data, the proposed scheme achieves 27.3 AP on test set.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_42');
INSERT INTO `paper` VALUES (10393, 'AGIL: Learning Attention from Human for Visuomotor Tasks', 'Visual attention', 'Eye tracking', 'Imitation learning', '', '', 'When intelligent agents learn visuomotor behaviors from human demonstrations, they may benefit from knowing where the human is allocating visual attention, which can be inferred from their gaze. A wealth of information regarding intelligent decision making is conveyed by human gaze allocation; hence, exploiting such information has the potential to improve the agents’ performance. With this motivation, we propose the AGIL (Attention Guided Imitation Learning) framework. We collect high-quality human action and gaze data while playing Atari games in a carefully controlled experimental setting. Using these data, we first train a deep neural network that can predict human gaze positions and visual attention with high accuracy (the gaze network) and then train another network to predict human actions (the policy network). Incorporating the learned attention model from the gaze network into the policy network significantly improves the action prediction accuracy and task performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_41');
INSERT INTO `paper` VALUES (10394, 'AI Benchmark: Running Deep Neural Networks on Android Smartphones', 'AI', 'Benchmark', 'Neural networks', 'Deep learning', 'Computer vision', 'Over the last years, the computational power of mobile devices such as smartphones and tablets has grown dramatically, reaching the level of desktop computers available not long ago. While standard smartphone apps are no longer a problem for them, there is still a group of tasks that can easily challenge even high-end devices, namely running artificial intelligence algorithms. In this paper, we present a study of the current state of deep learning in the Android ecosystem and describe available frameworks, programming models and the limitations of running AI on smartphones. We give an overview of the hardware acceleration resources available on four main mobile chipset platforms: Qualcomm, HiSilicon, MediaTek and Samsung. Additionally, we present the real-world performance results of different mobile SoCs collected with AI Benchmark (http://ai-benchmark.com) that are covering all main existing hardware configurations.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_19');
INSERT INTO `paper` VALUES (10395, 'AMC: AutoML for Model Compression and Acceleration on Mobile Devices', 'AutoML', 'Reinforcement learning', 'Model compression', 'CNN acceleration', 'Mobile vision', 'Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_48');
INSERT INTO `paper` VALUES (10396, 'An Adversarial Approach to Hard Triplet Generation', 'Image retrieval', 'Hard examples', 'Adversarial nets', '', '', 'While deep neural networks have demonstrated competitive results for many visual recognition and image retrieval tasks, the major challenge lies in distinguishing similar images from different categories (i.e., hard negative examples) while clustering images with large variations from the same category (i.e., hard positive examples). The current state-of-the-art is to mine the most hard triplet examples from the mini-batch to train the network. However, mining-based methods tend to look into these triplets that are hard in terms of the current estimated network, rather than deliberately generating those hard triplets that really matter in globally optimizing the network. For this purpose, we propose an adversarial network for Hard Triplet Generation (HTG) to optimize the network ability in distinguishing similar examples of different categories as well as grouping varied examples of the same categories. We evaluate our method on the real-world challenging datasets, such as CUB200-2011, CARS196, DeepFashion and VehicleID datasets, and show that our method outperforms the state-of-the-art methods significantly.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_31');
INSERT INTO `paper` VALUES (10397, 'An Empirical Study Towards Understanding How Deep Convolutional Nets Recognize Falls', 'Deep convolutional nets', 'Fall recognition', 'Empirical study', '', '', 'Detecting unintended falls is essential for ambient intelligence and healthcare of elderly people living alone. In recent years, deep convolutional nets are widely used in human action analysis, based on which a number of fall detection methods have been proposed. Despite their highly effective performances, the behaviors of how the convolutional nets recognize falls are still not clear. In this paper, instead of proposing a novel approach, we perform a systematical empirical study, attempting to investigate the underlying fall recognition process. We propose four tasks to investigate, which involve five types of input modalities, seven net instances and different training samples. The obtained quantitative and qualitative results reveal the patterns that the nets tend to learn, and several factors that can heavily influence the performances on fall recognition. We expect that our conclusions are favorable to proposing better deep learning solutions to fall detection systems.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_8');
INSERT INTO `paper` VALUES (10398, 'An End-to-End Tree Based Approach for Instance Segmentation', '', '', '', '', '', 'This paper presents an approach for bottom-up hierarchical instance segmentation. We propose an end-to-end model to estimate energies of regions in an hierarchical region tree. To this end, we introduce a Convolutional Tree-LSTM module to leverage the tree-structured network topology. For constructing the hierarchical region tree, we utilize the accurate boundaries predicted from a pre-trained convolutional oriented boundary network. We evaluate our model on PASCAL VOC 2012 dataset showing that we obtain good trade-off between segmentation accuracy and time taken to process a single image.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_30');
INSERT INTO `paper` VALUES (10399, 'Analysis of the Effect of Sensors for End-to-End Machine Learning Odometry', 'Navigation', 'Visual', 'Inertial', 'Odometry', 'Machine learning', 'Accurate position and orientation estimations are essential for navigation in autonomous robots. Although it is a well studied problem, existing solutions rely on statistical filters, which usually require good parameter initialization or calibration and are computationally expensive. This paper addresses that problem by using an end-to-end machine learning approach. This work explores the incorporation of multiple sources of data (monocular RGB images and inertial data) to overcome the weaknesses of each source independently. Three different odometry approaches are proposed using CNNs and LSTMs and evaluated against the KITTI dataset and compared with other existing approaches. The obtained results show that the performance of the proposed approaches is similar to the state-of-the-art ones, outperforming some of them at a lower computational cost allowing their execution on resource constrained devices.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_6');
INSERT INTO `paper` VALUES (10400, 'Analyzing Clothing Layer Deformation Statistics of 3D Human Motions', 'Clothing motion analysis', '3D garment capture', 'Capture data augmentation and retargeting', '', '', 'Recent capture technologies and methods allow not only to retrieve 3D model sequence of moving people in clothing, but also to separate and extract the underlying body geometry, motion component and the clothing as a geometric layer. So far this clothing layer has only been used as raw offsets for individual applications such as retargeting a different body capture sequence with the clothing layer of another sequence, with limited scope, e.g. using identical or similar motions. The structured, semantics and motion-correlated nature of the information contained in this layer has yet to be fully understood and exploited. To this purpose we propose a comprehensive analysis of the statistics of this layer with a simple two-component model, based on PCA subspace reduction of the layer information on one hand, and a generic parameter regression model using neural networks on the other hand, designed to regress from any semantic parameter whose variation is observed in a training set, to the layer parameterization space. We show that this model not only allows to reproduce previous retargeting works, but generalizes the data generation capabilities to other semantic parameters such as clothing variation and size, or physical material parameters with synthetically generated training sequence, paving the way for many kinds of capture data-driven creation and augmentation applications.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_15');
INSERT INTO `paper` VALUES (10401, 'Analyzing Perception-Distortion Tradeoff Using Enhanced Perceptual Super-Resolution Network', 'Super-resolution', 'Deep learning', 'Perceptual quality', 'GAN', '', 'Convolutional neural network (CNN) based methods have recently achieved great success for image super-resolution (SR). However, most deep CNN based SR models attempt to improve distortion measures (e.g. PSNR, SSIM, IFC, VIF) while resulting in poor quantified perceptual quality (e.g. human opinion score, no-reference quality measures such as NIQE). Few works have attempted to improve the perceptual quality at the cost of performance reduction in distortion measures. A very recent study has revealed that distortion and perceptual quality are at odds with each other and there is always a trade-off between the two. Often the restoration algorithms that are superior in terms of perceptual quality, are inferior in terms of distortion measures. Our work attempts to analyze the trade-off between distortion and perceptual quality for the problem of single image SR. To this end, we use the well-known SR architecture- enhanced deep super-resolution (EDSR) network and show that it can be adapted to achieve better perceptual quality for a specific range of the distortion measure. While the original network of EDSR was trained to minimize the error defined based on per-pixel accuracy alone, we train our network using a generative adversarial network framework with EDSR as the generator module. Our proposed network, called enhanced perceptual super-resolution network (EPSR), is trained with a combination of mean squared error loss, perceptual loss, and adversarial loss. Our experiments reveal that EPSR achieves the state-of-the-art trade-off between distortion and perceptual quality while the existing methods perform well in either of these measures alone.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_8');
INSERT INTO `paper` VALUES (10402, 'Answering Visual What-If Questions: From Actions to Predicted Scene Descriptions', 'Scene understanding', 'Visual turing test', 'Visual question answering', 'Intuitive physics', '', 'In-depth scene descriptions and question answering tasks have greatly increased the scope of today’s definition of scene understanding. While such tasks are in principle open ended, current formulations primarily focus on describing only the current state of the scenes under consideration. In contrast, in this paper, we focus on the future states of the scenes which are also conditioned on actions. We posit this as a question answering task, where an answer has to be given about a future scene state, given observations of the current scene, and a question that includes a hypothetical action. Our solution is a hybrid model which integrates a physics engine into a question answering architecture in order to anticipate future scene states resulting from object-object interactions caused by an action. We demonstrate first results on this challenging new problem and compare to baselines, where we outperform fully data-driven end-to-end learning approaches.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_32');
INSERT INTO `paper` VALUES (10403, 'Appearance-Based Gaze Estimation via Evaluation-Guided Asymmetric Regression', 'Gaze estimation', 'Eye appearance', 'Asymmetric regression', '', '', 'Eye gaze estimation has been increasingly demanded by recent intelligent systems to accomplish a range of interaction-related tasks, by using simple eye images as input. However, learning the highly complex regression between eye images and gaze directions is nontrivial, and thus the problem is yet to be solved efficiently. In this paper, we propose the Asymmetric Regression-Evaluation Network (ARE-Net), and try to improve the gaze estimation performance to its full extent. At the core of our method is the notion of “two eye asymmetry” observed during gaze estimation for the left and right eyes. Inspired by this, we design the multi-stream ARE-Net; one asymmetric regression network (AR-Net) predicts 3D gaze directions for both eyes with a novel asymmetric strategy, and the evaluation network (E-Net) adaptively adjusts the strategy by evaluating the two eyes in terms of their performance during optimization. By training the whole network, our method achieves promising results and surpasses the state-of-the-art methods on multiple public datasets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_7');
INSERT INTO `paper` VALUES (10404, 'Approach for Video Classification with Multi-label on YouTube-8M Dataset', 'Video classification', 'Large-scale video', 'Multi-label', '', '', 'Video traffic is increasing at a considerable rate due to the spread of personal media and advancements in media technology. Accordingly, there is a growing need for techniques to automatically classify moving images. This paper use NetVLAD and NetFV models and the Huber loss function for video classification problem and YouTube-8M dataset to verify the experiment. We tried various attempts according to the dataset and optimize hyperparameters, ultimately obtain a GAP score of 0.8668.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_29');
INSERT INTO `paper` VALUES (10405, 'Are You Tampering with My Data?', 'Adversarial attack', 'Machine learning', 'Deep neural networks', 'Data poisoning', '', 'We propose a novel approach towards adversarial attacks on neural networks (NN), focusing on tampering the data used for training instead of generating attacks on trained models. Our network-agnostic method creates a backdoor during training which can be exploited at test time to force a neural network to exhibit abnormal behaviour. We demonstrate on two widely used datasets (CIFAR-10 and SVHN) that a universal modification of just one pixel per image for all the images of a class in the training set is enough to corrupt the training procedure of several state-of-the-art deep neural networks, causing the networks to misclassify any images to which the modification is applied. Our aim is to bring to the attention of the machine learning community, the possibility that even learning-based methods that are personally trained on public datasets can be subject to attacks by a skillful adversary.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_25');
INSERT INTO `paper` VALUES (10406, 'ArticulatedFusion: Real-Time Reconstruction of Motion, Geometry and Segmentation Using a Single Depth Camera', 'Fusion', 'Articulated', 'Motion', 'Segmentation', '', 'This paper proposes a real-time dynamic scene reconstruction method capable of reproducing the motion, geometry, and segmentation simultaneously given live depth stream from a single RGB-D camera. Our approach fuses geometry frame by frame and uses a segmentation-enhanced node graph structure to drive the deformation of geometry in registration step. A two-level node motion optimization is proposed. The optimization space of node motions and the range of physically-plausible deformations are largely reduced by taking advantage of the articulated motion prior, which is solved by an efficient node graph segmentation method. Compared to previous fusion-based dynamic scene reconstruction methods, our experiments show robust and improved reconstruction results for tangential and occluded motions.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_20');
INSERT INTO `paper` VALUES (10407, 'Ask, Acquire, and Attack: Data-Free UAP Generation Using Class Impressions', 'Adversarial attacks', 'Attacks on ML systems', 'Data-free attacks', 'Image-agnostic perturbations', 'Class impressions', 'Deep learning models are susceptible to input specific noise, called adversarial perturbations. Moreover, there exist input-agnostic noise, called Universal Adversarial Perturbations (UAP) that can affect inference of the models over most input samples. Given a model, there exist broadly two approaches to craft UAPs: (i) data-driven: that require data, and (ii) data-free: that do not require data samples. Data-driven approaches require actual samples from the underlying data distribution and craft UAPs with high success (fooling) rate. However, data-free approaches craft UAPs without utilizing any data samples and therefore result in lesser success rates. In this paper, for data-free scenarios, we propose a novel approach that emulates the effect of data samples with class impressions in order to craft UAPs using data-driven objectives. Class impression for a given pair of category and model is a generic representation (in the input space) of the samples belonging to that category. Further, we present a neural network based generative model that utilizes the acquired class impressions to learn crafting UAPs. Experimental evaluation demonstrates that the learned generative model, (i) readily crafts UAPs via simple feed-forwarding through neural network layers, and (ii) achieves state-of-the-art success rates for data-free scenario and closer to that for data-driven setting without actually utilizing any data samples.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_2');
INSERT INTO `paper` VALUES (10408, 'ASSIST: Personalized Indoor Navigation via Multimodal Sensors and High-Level Semantic Information', 'Indoor positioning', 'Environmental & situational awareness', 'Bluetooth beacons', 'Google Tango', '', 'Blind & visually impaired (BVI) individuals and those with Autism Spectrum Disorder (ASD) each face unique challenges in navigating unfamiliar indoor environments. In this paper, we propose an indoor positioning and navigation system that guides a user from point A to point B indoors with high accuracy while augmenting their situational awareness. This system has three major components: location recognition (a hybrid indoor localization app that uses Bluetooth Low Energy beacons and Google Tango to provide high accuracy), object recognition (a body-mounted camera to provide the user momentary situational awareness of objects and people), and semantic recognition (map-based annotations to alert the user of static environmental characteristics). This system also features personalized interfaces built upon the unique experiences that both BVI and ASD individuals have in indoor wayfinding and tailors its multimodal feedback to their needs. Here, the technical approach and implementation of this system are discussed, and the results of human subject tests with both BVI and ASD individuals are presented. In addition, we discuss and show the system’s user-centric interface and present points for future work and expansion.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_9');
INSERT INTO `paper` VALUES (10409, 'Associating Inter-image Salient Instances for Weakly Supervised Semantic Segmentation', 'Semantic segmentation', 'Weak supervision', 'Graph partitioning', '', '', 'Effectively bridging between image level keyword annotations and corresponding image pixels is one of the main challenges in weakly supervised semantic segmentation. In this paper, we use an instance-level salient object detector to automatically generate salient instances (candidate objects) for training images. Using similarity features extracted from each salient instance in the whole training set, we build a similarity graph, then use a graph partitioning algorithm to separate it into multiple subgraphs, each of which is associated with a single keyword (tag). Our graph-partitioning-based clustering algorithm allows us to consider the relationships between all salient instances in the training set as well as the information within them. We further show that with the help of attention information, our clustering algorithm is able to correct certain wrong assignments, leading to more accurate results. The proposed framework is general, and any state-of-the-art fully-supervised network structure can be incorporated to learn the segmentation network. When working with DeepLab for semantic segmentation, our method outperforms state-of-the-art weakly supervised alternatives by a large margin, achieving \\(65.6\\%\\) mIoU on the PASCAL VOC 2012 dataset. We also combine our method with Mask R-CNN for instance segmentation, and demonstrated for the first time the ability of weakly supervised instance segmentation using only keyword annotations.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_23');
INSERT INTO `paper` VALUES (10410, 'Asynchronous, Photometric Feature Tracking Using Events and Frames', 'Feature Tracking', 'Camera Effects', 'Generic Event Model', 'Standard Camera', 'High Dynamic Range (HDR)', 'We present a method that leverages the complementarity of event cameras and standard cameras to track visual features with low-latency. Event cameras are novel sensors that output pixel-level brightness changes, called “events”. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements (frames) that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide low-latency updates. In contrast to previous works, which are based on heuristics, this is the first principled method that uses raw intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are both more accurate (subpixel accuracy) and longer than the state of the art, across a wide variety of scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_46');
INSERT INTO `paper` VALUES (10411, 'Attaining Human-Level Performance with Atlas Location Autocontext for Anatomical Landmark Detection in 3D CT Data', '', '', '', '', '', 'We present an efficient neural network method for locating anatomical landmarks in 3D medical CT scans, using atlas location autocontext in order to learn long-range spatial context. Location predictions are made by regression to Gaussian heatmaps, one heatmap per landmark. This system allows patchwise application of a shallow network, thus enabling multiple volumetric heatmaps to be predicted concurrently without prohibitive GPU memory requirements. Further, the system allows inter-landmark spatial relationships to be exploited using a simple overdetermined affine mapping that is robust to detection failures and occlusion or partial views. Evaluation is performed for 22 landmarks defined on a range of structures in head CT scans. Models are trained and validated on 201 scans. Over the final test set of 20 scans which was independently annotated by 2 human annotators, the neural network reaches an accuracy which matches the annotator variability, with similar human and machine patterns of variability across landmark classes.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_34');
INSERT INTO `paper` VALUES (10412, 'Attend and Rectify: A Gated Attention Mechanism for Fine-Grained Recovery', 'Deep learning', 'Convolutional Neural Networks', 'Attention', '', '', 'We propose a novel attention mechanism to enhance Convolutional Neural Networks for fine-grained recognition. It learns to attend to lower-level feature activations without requiring part annotations and uses these activations to update and rectify the output likelihood distribution. In contrast to other approaches, the proposed mechanism is modular, architecture-independent and efficient both in terms of parameters and computation required. Experiments show that networks augmented with our approach systematically improve their classification accuracy and become more robust to clutter. As a result, Wide Residual Networks augmented with our proposal surpasses the state of the art classification accuracies in CIFAR-10, the Adience gender recognition task, Stanford dogs, and UEC Food-100.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_22');
INSERT INTO `paper` VALUES (10413, 'Attention-Aware Deep Adversarial Hashing for Cross-Modal Retrieval', 'Hashing', 'Adversarial learning', 'Attention mechanism', 'Cross modal retrieval', '', 'Due to the rapid growth of multi-modal data, hashing methods for cross-modal retrieval have received considerable attention. However, finding content similarities between different modalities of data is still challenging due to an existing heterogeneity gap. To further address this problem, we propose an adversarial hashing network with an attention mechanism to enhance the measurement of content similarities by selectively focusing on the informative parts of multi-modal data. The proposed new deep adversarial network consists of three building blocks: (1) the feature learning module to obtain the feature representations; (2) the attention module to generate an attention mask, which is used to divide the feature representations into the attended and unattended feature representations; and (3) the hashing module to learn hash functions that preserve the similarities between different modalities. In our framework, the attention and hashing modules are trained in an adversarial way: the attention module attempts to make the hashing module unable to preserve the similarities of multi-modal data w.r.t. the unattended feature representations, while the hashing module aims to preserve the similarities of multi-modal data w.r.t. the attended and unattended feature representations. Extensive evaluations on several benchmark datasets demonstrate that the proposed method brings substantial improvements over other state-of-the-art cross-modal hashing methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_36');
INSERT INTO `paper` VALUES (10414, 'Attention-Based Ensemble for Deep Metric Learning', 'Attention', 'Ensemble', 'Deep metric learning', '', '', 'Deep metric learning aims to learn an embedding function, modeled as deep neural network. This embedding function usually puts semantically similar images close while dissimilar images far from each other in the learned embedding space. Recently, ensemble has been applied to deep metric learning to yield state-of-the-art results. As one important aspect of ensemble, the learners should be diverse in their feature embeddings. To this end, we propose an attention-based ensemble, which uses multiple attention masks, so that each learner can attend to different parts of the object. We also propose a divergence loss, which encourages diversity among the learners. The proposed method is applied to the standard benchmarks of deep metric learning and experimental results show that it outperforms the state-of-the-art methods by a significant margin on image retrieval tasks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_45');
INSERT INTO `paper` VALUES (10415, 'Attention-GAN for Object Transfiguration in Wild Images', 'Generative adversarial networks', 'Attention mechanism', '', '', '', 'This paper studies the object transfiguration problem in wild images. The generative network in classical GANs for object transfiguration often undertakes a dual responsibility: to detect the objects of interests and to convert the object from source domain to another domain. In contrast, we decompose the generative network into two separated networks, each of which is only dedicated to one particular sub-task. The attention network predicts spatial attention maps of images, and the transformation network focuses on translating objects. Attention maps produced by attention network are encouraged to be sparse, so that major attention can be paid on objects of interests. No matter before or after object transfiguration, attention maps should remain constant. In addition, learning attention network can receive more instructions, given the available segmentation annotations of images. Experimental results demonstrate the necessity of investigating attention in object transfiguration, and that the proposed algorithm can learn accurate attention to improve quality of generated images.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_11');
INSERT INTO `paper` VALUES (10416, 'Attentive Semantic Alignment with Offset-Aware Correlation Kernels', 'Semantic correspondence', 'Attention process', 'Offset-aware correlation kernels', 'Attentive semantic alignment', 'Local transformation', 'Semantic correspondence is the problem of establishing correspondences across images depicting different instances of the same object or scene class. One of recent approaches to this problem is to estimate parameters of a global transformation model that densely aligns one image to the other. Since an entire correlation map between all feature pairs across images is typically used to predict such a global transformation, noisy features from different backgrounds, clutter, and occlusion distract the predictor from correct estimation of the alignment. This is a challenging issue, in particular, in the problem of semantic correspondence where a large degree of image variations is often involved. In this paper, we introduce an attentive semantic alignment method that focuses on reliable correlations, filtering out distractors. For effective attention, we also propose an offset-aware correlation kernel that learns to capture translation-invariant local transformations in computing correlation values over spatial locations. Experiments demonstrate the effectiveness of the attentive model and offset-aware kernel, and the proposed model combining both techniques achieves the state-of-the-art performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_22');
INSERT INTO `paper` VALUES (10417, 'Attribute-Guided Face Generation Using Conditional CycleGAN', 'Face generation', 'Attribute', 'GAN', '', '', 'We are interested in attribute-guided face generation: given a low-res face input image, an attribute vector that can be extracted from a high-res image (attribute image), our new method generates a high-res face image for the low-res input that satisfies the given attributes. To address this problem, we condition the CycleGAN and propose conditional CycleGAN, which is designed to (1) handle unpaired training data because the training low/high-res and high-res attribute images may not necessarily align with each other, and to (2) allow easy control of the appearance of the generated face via the input attributes. We demonstrate high-quality results on the attribute-guided conditional CycleGAN, which can synthesize realistic face images with appearance easily controlled by user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using the attribute image as identity to produce the corresponding conditional vector and by incorporating a face verification network, the attribute-guided network becomes the identity-guided conditional CycleGAN which produces high-quality and interesting results on identity transfer. We demonstrate three applications on identity-guided conditional CycleGAN: identity-preserving face superresolution, face swapping, and frontal face generation, which consistently show the advantage of our new method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_18');
INSERT INTO `paper` VALUES (10418, 'Attributes as Operators: Factorizing Unseen Attribute-Object Compositions', '', '', '', '', '', 'We present a new approach to modeling visual attributes. Prior work casts attributes in a similar role as objects, learning a latent representation where properties (e.g., sliced) are recognized by classifiers much in the way objects (e.g., apple) are. However, this common approach fails to separate the attributes observed during training from the objects with which they are composed, making it ineffectual when encountering new attribute-object compositions. Instead, we propose to model attributes as operators. Our approach learns a semantic embedding that explicitly factors out attributes from their accompanying objects, and also benefits from novel regularizers expressing attribute operators’ effects (e.g., blunt should undo the effects of sharp). Not only does our approach align conceptually with the linguistic role of attributes as modifiers, but it also generalizes to recognize unseen compositions of objects and attributes. We validate our approach on two challenging datasets and demonstrate significant improvements over the state of the art. In addition, we show that not only can our model recognize unseen compositions robustly in an open-world setting, it can also generalize to compositions where objects themselves were unseen during training.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_11');
INSERT INTO `paper` VALUES (10419, 'Audio-Visual Event Localization in Unconstrained Videos', 'Audio-visual event', 'Temporal localization', 'Attention', 'Fusion', '', 'In this paper, we introduce a novel problem of audio-visual event localization in unconstrained videos. We define an audio-visual event as an event that is both visible and audible in a video segment. We collect an Audio-Visual Event (AVE) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised audio-visual event localization, and cross-modality localization. We develop an audio-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual network (DMRN) to fuse information over the two modalities, and introduce an audio-visual distance learning network to handle the cross-modality localization. Our experiments support the following findings: joint modeling of auditory and visual modalities outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed DMRN is effective in fusing audio-visual features, and strong correlations between the two modalities enable cross-modality localization.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_16');
INSERT INTO `paper` VALUES (10420, 'Audio-Visual Scene Analysis with Self-Supervised Multisensory Features', 'Multisensory Representation', 'Source Separation Model', 'Multisensory Network', 'Self-supervised Approach', 'Audio-visual Model', 'The thud of a bouncing ball, the onset of speech as lips open—when visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that the visual and audio components of a video signal should be modeled jointly using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the source of sound in a video; (b) audio-visual action recognition; and (c) on/off-screen audio source separation, e.g. removing the off-screen translator’s voice from a foreign official’s speech. Code, models, and video results are available on our webpage: http://andrewowens.com/multisensory.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_39');
INSERT INTO `paper` VALUES (10421, 'AugGAN: Cross Domain Adaptation with GAN-Based Data Augmentation', 'Generative adversarial network', 'Image-to-image translation', 'Semantic segmentation', 'Object detection', 'Domain adaptation', 'Deep learning based image-to-image translation methods aim at learning the joint distribution of the two domains and finding transformations between them. Despite recent GAN (Generative Adversarial Network) based methods have shown compelling results, they are prone to fail at preserving image-objects and maintaining translation consistency, which reduces their practicality on tasks such as generating large-scale training data for different domains. To address this problem, we purpose a structure-aware image-to-image translation network, which is composed of encoders, generators, discriminators and parsing nets for the two domains, respectively, in a unified framework. The purposed network generates more visually plausible images compared to competing methods on different image-translation tasks. In addition, we quantitatively evaluate different methods by training Faster-RCNN and YOLO with datasets generated from the image-translation results and demonstrate significant improvement on the detection accuracies by using the proposed image-object preserving network.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_44');
INSERT INTO `paper` VALUES (10422, 'AutoLoc: Weakly-Supervised Temporal Action Localization in Untrimmed Videos', 'Temporal action localization', 'Weak supervision', 'Outer-Inner-contrastive', 'Class activation sequence', '', 'Temporal Action Localization (TAL) in untrimmed video is important for many applications. But it is very expensive to annotate the segment-level ground truth (action class and temporal boundary). This raises the interest of addressing TAL with weak supervision, namely only video-level annotations are available during training). However, the state-of-the-art weakly-supervised TAL methods only focus on generating good Class Activation Sequence (CAS) over time but conduct simple thresholding on CAS to localize actions. In this paper, we first develop a novel weakly-supervised TAL framework called AutoLoc to directly predict the temporal boundary of each action instance. We propose a novel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed segment-level supervision for training such a boundary predictor. Our method achieves dramatically improved performance: under the IoU threshold 0.5, our method improves mAP on THUMOS’14 from 13.7% to 21.2% and mAP on ActivityNet from 7.4% to 27.3%. It is also very encouraging to see that our weakly-supervised method achieves comparable results with some fully-supervised methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_10');
INSERT INTO `paper` VALUES (10423, 'Automated Facial Wrinkles Annotator', 'Wrinkles annotator', 'Hessian filter', 'Wrinkles depth', '', '', 'This paper presents an automated facial wrinkles annotator for coarse wrinkles, fine wrinkles and wrinkle depth map extraction. First we extended Hybrid Hessian Filter by introducing a multi-scale filter to isolate the coarse wrinkles from fine wrinkles. Then we generate a wrinkle probabilistic map. When evaluated on 20 high resolution full face images (10 from our in-house dataset and 10 from FERET dataset), we achieved good accuracy when the result of coarse wrinkles was validated with manual annotation. Furthermore, we visually illustrate the ability of the annotator in detecting fine wrinkles. This paper advances the field by automate the localisation of the fine wrinkles, which might not be possible to annotate manually. Our automated facial wrinkles annotator will be beneficial to large-scale data annotation and cosmetic applications.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_56');
INSERT INTO `paper` VALUES (10424, 'Automatic Classification of Low-Resolution Chromosomal Images', 'Low resolution chromosomes', 'Karyotyping', 'Chromosome classification', 'Super-Xception', 'Super-ResNet', 'Chromosome karyotyping is a two-staged process consisting of segmentation followed by pairing and ordering of 23 pairs of human chromosomes obtained from cell spread images during metaphase stage of cell division. It is carried out by cytogeneticists in clinical labs on the basis of length, centromere position, and banding pattern of chromosomes for the diagnosis of various health and genetic disorders. The entire process demands high domain expertise and considerable amount of manual effort. This motivates us to automate or partially automate karyotyping process which would benefit and aid doctors in the analysis of chromosome images. However, the non-availability of high resolution chromosome images required for classification purpose creates a hindrance in achieving high classification accuracy. To address this issue, we propose a Super-Xception network which takes the low-resolution chromosome images as input and classifies them to one of the 24 chromosome class labels after conversion into high resolution images. In this network, we integrate super-resolution deep models with standard classification networks e.g., Xception network in our case. The network is trained in an end-to-end manner in which the super-resolution layers help in conversion of low-resolution images to high-resolution images which are subsequently passed through deep classification layers for label assigning. We evaluate our proposed network’s efficacy on a publicly available online Bioimage chromosome classification dataset of healthy chromosomes and benchmark it against the baseline models created using traditional deep convolutional neural network, ResNet-50 and Xception network.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_21');
INSERT INTO `paper` VALUES (10425, 'Automatic Fusion of Segmentation and Tracking Labels', 'Label fusion', 'Image annotation', 'Segmentation labels', 'Tracking labels', '', 'Labeled training images of high quality are required for developing well-working analysis pipelines. This is, of course, also true for biological image data, where such labels are usually hard to get. We distinguish human labels (gold corpora) and labels generated by computer algorithms (silver corpora). A naturally arising problem is to merge multiple corpora into larger bodies of labeled training datasets. While fusion of labels in static images is already an established field, dealing with labels in time-lapse image data remains to be explored. Obtaining a gold corpus for segmentation is usually very time-consuming and hence expensive. For this reason, gold corpora for object tracking often use object detection markers instead of dense segmentations. If dense segmentations of tracked objects are desired later on, an automatic merge of the detection-based gold corpus with (silver) corpora of the individual time points for segmentation will be necessary. Here we present such an automatic merging system and demonstrate its utility on corpora from the Cell Tracking Challenge. We additionally release all label fusion algorithms as freely available and open plugins for Fiji (https://github.com/xulman/CTC-FijiPlugins).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_34');
INSERT INTO `paper` VALUES (10426, 'Bayesian Semantic Instance Segmentation in Open Set World', 'Instance segmentation', 'Open-set conditions', '', '', '', 'This paper addresses the semantic instance segmentation task in the open-set conditions, where input images can contain known and unknown object classes. The training process of existing semantic instance segmentation methods requires annotation masks for all object instances, which is expensive to acquire or even infeasible in some realistic scenarios, where the number of categories may increase boundlessly. In this paper, we present a novel open-set semantic instance segmentation approach capable of segmenting all known and unknown object classes in images, based on the output of an object detector trained on known object classes. We formulate the problem using a Bayesian framework, where the posterior distribution is approximated with a simulated annealing optimization equipped with an efficient image partition sampler. We show empirically that our method is competitive with state-of-the-art supervised methods on known classes, but also performs well on unknown classes when compared with unsupervised methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_1');
INSERT INTO `paper` VALUES (10427, 'Beyond Local Reasoning for Stereo Confidence Estimation with Deep Learning', 'Confidence measures', 'Stereo matching', 'Deep learning', '', '', 'Confidence measures for stereo gained popularity in recent years due to their improved capability to detect outliers and the increasing number of applications exploiting these cues. In this field, convolutional neural networks achieved top-performance compared to other known techniques in the literature by processing local information to tell disparity assignments from outliers. Despite this outstanding achievements, all approaches rely on clues extracted with small receptive fields thus ignoring most of the overall image content. Therefore, in this paper, we propose to exploit nearby and farther clues available from image and disparity domains to obtain a more accurate confidence estimation. While local information is very effective for detecting high frequency patterns, it lacks insights from farther regions in the scene. On the other hand, enlarging the receptive field allows to include clues from farther regions but produces smoother uncertainty estimation, not particularly accurate when dealing with high frequency patterns. For these reasons, we propose in this paper a multi-stage cascaded network to combine the best of the two worlds. Extensive experiments on three datasets using three popular stereo algorithms prove that the proposed framework outperforms state-of-the-art confidence estimation techniques.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_20');
INSERT INTO `paper` VALUES (10428, 'Beyond Part Models: Person Retrieval with Refined Part Pooling (and A Strong Convolutional Baseline)', 'Person retrieval', 'Part-level feature', 'Part refinement', '', '', 'Employing part-level features offers fine-grained information for pedestrian image description. A prerequisite of part discovery is that each part should be well located. Instead of using external resources like pose estimator, we consider content consistency within each part for precise part location. Specifically, we target at learning discriminative part-informed features for person retrieval and make two contributions. (i) A network named Part-based Convolutional Baseline (PCB). Given an image input, it outputs a convolutional descriptor consisting of several part-level features. With a uniform partition strategy, PCB achieves competitive results with the state-of-the-art methods, proving itself as a strong convolutional baseline for person retrieval. (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs outliers in each part, which are in fact more similar to other parts. RPP re-assigns these outliers to the parts they are closest to, resulting in refined parts with enhanced within-part consistency. Experiment confirms that RPP allows PCB to gain another round of performance boost. For instance, on the Market-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1 accuracy, surpassing the state of the art by a large margin. Code is available at: https://github.com/syfafterzy/PCB_RPP', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_30');
INSERT INTO `paper` VALUES (10429, 'Bi-box Regression for Pedestrian Detection and Occlusion Estimation', 'Pedestrian detection', 'Occlusion handling', 'Deep CNN', '', '', 'Occlusions present a great challenge for pedestrian detection in practical applications. In this paper, we propose a novel approach to simultaneous pedestrian detection and occlusion estimation by regressing two bounding boxes to localize the full body as well as the visible part of a pedestrian respectively. For this purpose, we learn a deep convolutional neural network (CNN) consisting of two branches, one for full body estimation and the other for visible part estimation. The two branches are treated differently during training such that they are learned to produce complementary outputs which can be further fused to improve detection performance. The full body estimation branch is trained to regress full body regions for positive pedestrian proposals, while the visible part estimation branch is trained to regress visible part regions for both positive and negative pedestrian proposals. The visible part region of a negative pedestrian proposal is forced to shrink to its center. In addition, we introduce a new criterion for selecting positive training examples, which contributes largely to heavily occluded pedestrian detection. We validate the effectiveness of the proposed bi-box regression approach on the Caltech and CityPersons datasets. Experimental results show that our approach achieves promising performance for detecting both non-occluded and occluded pedestrians, especially heavily occluded ones.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_9');
INSERT INTO `paper` VALUES (10430, 'Bi-GANs-ST for Perceptual Image Super-Resolution', 'Image super-resolution', 'Perceptual image', 'GAN', 'Soft-thresholding', '', 'Image quality measurement is a critical problem for image super-resolution (SR) algorithms. Usually, they are evaluated by some well-known objective metrics, e.g., PSNR and SSIM, but these indices cannot provide suitable results in accordance with the perception of human being. Recently, a more reasonable perception measurement has been proposed in [1], which is also adopted by the PIRM-SR 2018 challenge. In this paper, motivated by [1], we aim to generate a high-quality SR result which balances between the two indices, i.e., the perception index and root-mean-square error (RMSE). To do so, we design a new deep SR framework, dubbed Bi-GANs-ST, by integrating two complementary generative adversarial networks (GAN) branches. One is memory residual SRGAN (MR-SRGAN), which emphasizes on improving the objective performance, such as reducing the RMSE. The other is weight perception SRGAN (WP-SRGAN), which obtains the result that favors better subjective perception via a two-stage adversarial training mechanism. Then, to produce final result with excellent perception scores and RMSE, we use soft-thresholding method to merge the results generated by the two GANs. Our method performs well on the perceptual image super-resolution task of the PIRM 2018 challenge. Experimental results on five benchmarks show that our proposal achieves highly competent performance compared with other state-of-the-art methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_2');
INSERT INTO `paper` VALUES (10431, 'Bi-Real Net: Enhancing the Performance of 1-Bit CNNs with Improved Representational Capability and Advanced Training Algorithm', 'Convolutional Neural Network (CNNs)', 'Clip Function', 'ImageNet', 'Binary Weights', 'Standard ResNet', 'In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1-bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4% and 62.2% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10% higher top-1 accuracy with more memory saving and lower computational cost.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_44');
INSERT INTO `paper` VALUES (10432, 'Bidirectional Convolutional LSTM for the Detection of Violence in Videos', 'Violence detection', 'Convolutional LSTM', 'Bidirectional LSTM', 'Action recognition', 'Fight detection', 'The field of action recognition has gained tremendous traction in recent years. A subset of this, detection of violent activity in videos, is of great importance, particularly in unmanned surveillance or crowd footage videos. In this work, we explore this problem on three standard benchmarks widely used for violence detection: the Hockey Fights, Movies, and Violent Flows datasets. To this end, we introduce a Spatiotemporal Encoder, built on the Bidirectional Convolutional LSTM (BiConvLSTM) architecture. The addition of bidirectional temporal encodings and an elementwise max pooling of these encodings in the Spatiotemporal Encoder is novel in the field of violence detection. This addition is motivated by a desire to derive better video representations via leveraging long-range information in both temporal directions of the video. We find that the Spatiotemporal network is comparable in performance with existing methods for all of the above datasets. A simplified version of this network, the Spatial Encoder is sufficient to match state-of-the-art performance on the Hockey Fights and Movies datasets. However, on the Violent Flows dataset, the Spatiotemporal Encoder outperforms the Spatial Encoder.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_24');
INSERT INTO `paper` VALUES (10433, 'Bidirectional Feature Pyramid Network with Recurrent Attention Residual Modules for Shadow Detection', '', '', '', '', '', 'This paper presents a network to detect shadows by exploring and combining global context in deep layers and local context in shallow layers of a deep convolutional neural network (CNN). There are two technical contributions in our network design. First, we formulate the recurrent attention residual (RAR) module to combine the contexts in two adjacent CNN layers and learn an attention map to select a residual and then refine the context features. Second, we develop a bidirectional feature pyramid network (BFPN) to aggregate shadow contexts spanned across different CNN layers by deploying two series of RAR modules in the network to iteratively combine and refine context features: one series to refine context features from deep to shallow layers, and another series from shallow to deep layers. Hence, we can better suppress false detections and enhance shadow details at the same time. We evaluate our network on two common shadow detection benchmark datasets: SBU and UCF. Experimental results show that our network outperforms the best existing method with 34.88% reduction on SBU and 34.57% reduction on UCF for the balance error rate.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_8');
INSERT INTO `paper` VALUES (10434, 'BiSeNet: Bilateral Segmentation Network for Real-Time Semantic Segmentation', 'Real-time semantic segmentation', 'Bilateral Segmentation Network', '', '', '', 'Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor performance. In this paper, we address this dilemma with a novel Bilateral Segmentation Network (BiSeNet). We first design a Spatial Path with a small stride to preserve the spatial information and generate high-resolution features. Meanwhile, a Context Path with a fast downsampling strategy is employed to obtain sufficient receptive field. On top of the two paths, we introduce a new Feature Fusion Module to combine features efficiently. The proposed architecture makes a right balance between the speed and segmentation performance on Cityscapes, CamVid, and COCO-Stuff datasets. Specifically, for a 2048 \\(\\times \\) 1024 input, we achieve 68.4% Mean IOU on the Cityscapes test dataset with speed of 105 FPS on one NVIDIA Titan XP card, which is significantly faster than the existing methods with comparable performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_20');
INSERT INTO `paper` VALUES (10435, 'BodyNet: Volumetric Inference of 3D Human Body Shapes', 'Net Body', 'Body Part Segmentation', 'Intermediate Supervision', 'Body Shape Estimation', 'People Dataset', 'Human shape estimation is an important task for video editing, animation and fashion industry. Predicting 3D human body shape from natural images, however, is highly challenging due to factors such as variation in human bodies, clothing and viewpoint. Prior methods addressing this problem typically attempt to fit parametric body models with certain priors on pose and shape. In this work we argue for an alternative representation and propose BodyNet, a neural network for direct inference of volumetric body shape from a single image. BodyNet is an end-to-end trainable network that benefits from (i) a volumetric 3D loss, (ii) a multi-view re-projection loss, and (iii) intermediate supervision of 2D pose, 2D body part segmentation, and 3D pose. Each of them results in performance improvement as demonstrated by our experiments. To evaluate the method, we fit the SMPL model to our network output and show state-of-the-art results on the SURREAL and Unite the People datasets, outperforming recent approaches. Besides achieving state-of-the-art performance, our method also enables volumetric body-part segmentation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_2');
INSERT INTO `paper` VALUES (10436, 'Boosted Attention: Leveraging Human Attention for Image Captioning', 'Image captioning', 'Visual attention', 'Human attention', '', '', 'Visual attention has shown usefulness in image captioning, with the goal of enabling a caption model to selectively focus on regions of interest. Existing models typically rely on top-down language information and learn attention implicitly by optimizing the captioning objectives. While somewhat effective, the learned top-down attention can fail to focus on correct regions of interest without direct supervision of attention. Inspired by the human visual system which is driven by not only the task-specific top-down signals but also the visual stimuli, we in this work propose to use both types of attention for image captioning. In particular, we highlight the complementary nature of the two types of attention and develop a model (Boosted Attention) to integrate them for image captioning. We validate the proposed approach with state-of-the-art performance across various evaluation metrics.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_5');
INSERT INTO `paper` VALUES (10437, 'Boosting LiDAR-Based Semantic Labeling by Cross-modal Training Data Generation', 'Semantic point cloud labeling', 'Semantic segmentation', 'Semantic scene understanding', 'Automated training data generation', 'Automated label trasfer', 'Mobile robots and autonomous vehicles rely on multi-modal sensor setups to perceive and understand their surroundings. Aside from cameras, LiDAR sensors represent a central component of state-of-the-art perception systems. In addition to accurate spatial perception, a comprehensive semantic understanding of the environment is essential for efficient and safe operation. In this paper we present a novel deep neural network architecture called LiLaNet for point-wise, multi-class semantic labeling of semi-dense LiDAR data. The network utilizes virtual image projections of the 3D point clouds for efficient inference. Further, we propose an automated process for large-scale cross-modal training data generation called Autolabeling, in order to boost semantic labeling performance while keeping the manual annotation effort low. The effectiveness of the proposed network architecture as well as the automated data generation process is demonstrated on a manually annotated ground truth dataset. LiLaNet is shown to significantly outperform current state-of-the-art CNN architectures for LiDAR data. Applying our automatically generated large-scale training data yields a boost of up to 14% points compared to networks trained on manually annotated data only.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_39');
INSERT INTO `paper` VALUES (10438, 'BOP: Benchmark for 6D Object Pose Estimation', '', '', '', '', '', 'We propose a benchmark for 6D pose estimation of a rigid object from a single RGB-D input image. The training data consists of a texture-mapped 3D object model or images of the object in known 6D poses. The benchmark comprises of: (i) eight datasets in a unified format that cover different practical scenarios, including two new datasets focusing on varying lighting conditions, (ii) an evaluation methodology with a pose-error function that deals with pose ambiguities, (iii) a comprehensive evaluation of 15 diverse recent methods that captures the status quo of the field, and (iv) an online evaluation system that is open for continuous submission of new results. The evaluation shows that methods based on point-pair features currently perform best, outperforming template matching methods, learning-based methods and methods based on 3D local features. The project website is available at bop.felk.cvut.cz.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_2');
INSERT INTO `paper` VALUES (10439, 'Brain-Inspired Robust Delineation Operator', 'COSFIRE filter', 'Delineation push-pull inhibition', '', '', '', 'In this paper we present a novel filter, based on the existing COSFIRE filter, for the delineation of patterns of interest. It includes a mechanism of push-pull inhibition that improves robustness to noise in terms of spurious texture. Push-pull inhibition is a phenomenon that is observed in neurons in area V1 of the visual cortex, which suppresses the response of certain simple cells for stimuli of preferred orientation but of non-preferred contrast. This type of inhibition allows for sharper detection of the patterns of interest and improves the quality of delineation especially in images with spurious texture.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_41');
INSERT INTO `paper` VALUES (10440, 'Brand > Logo: Visual Analysis of Fashion Brands', 'Deep learning', 'Convolutional networks', 'Fashion', 'Brands', '', 'While lots of people may think branding begins and ends with a logo, fashion brands communicate their uniqueness through a wide range of visual cues such as color, patterns and shapes. In this work, we analyze learned visual representations by deep networks that are trained to recognize fashion brands. In particular, the activation strength and extent of neurons are studied to provide interesting insights about visual brand expressions. The proposed method identifies where a brand stands in the spectrum of branding strategy, i.e., from trademark-emblazoned goods with bold logos to implicit no logo marketing. By quantifying attention maps, we are able to interpret the visual characteristics of a brand present in a single image and model the general design direction of a brand as a whole. We further investigate versatility of neurons and discover “specialists” that are highly brand-specific and “generalists” that detect diverse visual features. A human experiment based on three main visual scenarios of fashion brands is conducted to verify the alignment of our quantitative measures with the human perception of brands. This paper demonstrate how deep networks go beyond logos in order to recognize clothing brands in an image.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_2');
INSERT INTO `paper` VALUES (10441, 'Bridging Machine Learning and Cryptography in Defence Against Adversarial Attacks', 'Adversarial attacks', 'Defence', 'Data-independent transform', 'Secret key', 'Cryptography principle', 'In the last decade, deep learning algorithms have become very popular thanks to the achieved performance in many machine learning and computer vision tasks. However, most of the deep learning architectures are vulnerable to so called adversarial examples. This questions the security of deep neural networks (DNN) for many security- and trust-sensitive domains. The majority of the proposed existing adversarial attacks are based on the differentiability of the DNN cost function. Defence strategies are mostly based on machine learning and signal processing principles that either try to detect-reject or filter out the adversarial perturbations and completely neglect the classical cryptographic component in the defence.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_23');
INSERT INTO `paper` VALUES (10442, 'Broadcasting Convolutional Network for Visual Relational Reasoning', 'Visual relational reasoning', 'BCN', 'Broadcast', 'CLEVR', 'Multi-RN', 'In this paper, we propose the Broadcasting Convolutional Network (BCN) that extracts key object features from the global field of an entire input image and recognizes their relationship with local features. BCN is a simple network module that collects effective spatial features, embeds location information and broadcasts them to the entire feature maps. We further introduce the Multi-Relational Network (multiRN) that improves the existing Relation Network (RN) by utilizing the BCN module. In pixel-based relation reasoning problems, with the help of BCN, multiRN extends the concept of ‘pairwise relations’ in conventional RNs to ‘multiwise relations’ by relating each object with multiple objects at once. This yields in \\(\\mathcal {O}(n)\\) complexity for n objects, which is a vast computational gain from RNs that take \\(\\mathcal {O}(n^2)\\). Through experiments, multiRN has achieved a state-of-the-art performance on CLEVR dataset, which proves the usability of BCN on relation reasoning problems.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_46');
INSERT INTO `paper` VALUES (10443, 'BSN: Boundary Sensitive Network for Temporal Action Proposal Generation', 'Temporal action proposal generation', 'Temporal action detection', 'Temporal convolution', 'Untrimmed video', '', 'Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these difficulties, we introduce an effective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts “local to global” fashion. Locally, BSN first locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally, with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the confidence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classifiers, our method significantly improves the state-of-the-art temporal action detection performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_1');
INSERT INTO `paper` VALUES (10444, 'Building A Size Constrained Predictive Models for Video Classification', 'Deep learning', 'Multi-label classification', 'Video processing', '', '', 'Herein we present the solution to the \\(2^\\mathrm{nd}\\) YouTube-8M video understanding challenge which placed \\(1^\\mathrm{st}\\). Competition participants were tasked with building a size constrained video labeling model with a model size of less than 1 GB. Our final solution consists of several submodels belonging to Fisher vectors, NetVlad, Deep Bag of Frames and Recurrent neural networks model families. To make the classifier efficient under size constraints we introduced model distillation, partial weights quantization and training with exponential moving average.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_27');
INSERT INTO `paper` VALUES (10445, 'Burst Image Deblurring Using Permutation Invariant Convolutional Neural Networks', 'Burst imaging', 'Image processing', 'Deblurring', 'Denoising', 'Convolutional neural networks', 'We propose a neural approach for fusing an arbitrary-length burst of photographs suffering from severe camera shake and noise into a sharp and noise-free image. Our novel convolutional architecture has a simultaneous view of all frames in the burst, and by construction treats them in an order-independent manner. This enables it to effectively detect and leverage subtle cues scattered across different frames, while ensuring that each frame gets a full and equal consideration regardless of its position in the sequence. We train the network with richly varied synthetic data consisting of camera shake, realistic noise, and other common imaging defects. The method demonstrates consistent state of the art burst image restoration performance for highly degraded sequences of real-world images, and extracts accurate detail that is not discernible from any of the individual frames in isolation.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_45');
INSERT INTO `paper` VALUES (10446, 'BusterNet: Detecting Copy-Move Image Forgery with Source/Target Localization', 'Copy-move', 'Image forgery detection', 'Deep learning', '', '', 'We introduce a novel deep neural architecture for image copy-move forgery detection (CMFD), code-named BusterNet. Unlike previous efforts, BusterNet is a pure, end-to-end trainable, deep neural network solution. It features a two-branch architecture followed by a fusion module. The two branches localize potential manipulation regions via visual artifacts and copy-move regions via visual similarities, respectively. To the best of our knowledge, this is the first CMFD algorithm with discernibility to localize source/target regions. We also propose simple schemes for synthesizing large-scale CMFD samples using out-of-domain datasets, and stage-wise strategies for effective BusterNet training. Our extensive studies demonstrate that BusterNet outperforms state-of-the-art copy-move detection algorithms by a large margin on the two publicly available datasets, CASIA and CoMoFoD, and that it is robust against various known attacks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_11');
INSERT INTO `paper` VALUES (10447, 'C-WSL: Count-Guided Weakly Supervised Localization', 'Weakly supervised localization', 'Count supervision', '', '', '', 'We introduce count-guided weakly supervised localization (C-WSL), an approach that uses per-class object count as a new form of supervision to improve weakly supervised localization (WSL). C-WSL uses a simple count-based region selection algorithm to select high-quality regions, each of which covers a single object instance during training, and improves existing WSL methods by training with the selected regions. To demonstrate the effectiveness of C-WSL, we integrate it into two WSL architectures and conduct extensive experiments on VOC2007 and VOC2012. Experimental results show that C-WSL leads to large improvements in WSL and that the proposed approach significantly outperforms the state-of-the-art methods. The results of annotation experiments on VOC2007 suggest that a modest extra time is needed to obtain per-class object counts compared to labeling only object categories in an image. Furthermore, we reduce the annotation time by more than 2\\(\\times \\) and 38\\(\\times \\) compared to center-click and bounding-box annotations.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_10');
INSERT INTO `paper` VALUES (10448, 'Camera Tracking for SLAM in Deformable Maps', 'Non Rigid Structure-from-Motion', 'Shape-from-Template', 'Deformation models', 'Deformable SLAM', 'Non-rigid SLAM', 'The current SLAM algorithms cannot work without assuming rigidity. We propose the first real-time tracking thread for monocular VSLAM systems that manages deformable scenes. It is based on top of the Shape-from-Template (SfT) methods to code the scene deformation model. Our proposal is a sequential method that manages efficiently large templates, i.e. deformable maps estimating at the same time the camera pose and deformation. It also can be relocated in case of tracking loss. We have created a new dataset to evaluate our system. Our results show the robustness of the method in deformable environments while running in real time with errors under 3% in depth estimation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_45');
INSERT INTO `paper` VALUES (10449, 'Can 3D Pose Be Learned from 2D Projections Alone?', 'Weakly supervised learning', 'Generative Adversarial Networks', '3D pose estimation', 'Projective geometry', '', '3D pose estimation from a single image is a challenging task in computer vision. We present a weakly supervised approach to estimate 3D pose points, given only 2D pose landmarks. Our method does not require correspondences between 2D and 3D points to build explicit 3D priors. We utilize an adversarial framework to impose a prior on the 3D structure, learned solely from their random 2D projections. Given a set of 2D pose landmarks, the generator network hypothesizes their depths to obtain a 3D skeleton. We propose a novel Random Projection layer, which randomly projects the generated 3D skeleton and sends the resulting 2D pose to the discriminator. The discriminator improves by discriminating between the generated poses and pose samples from a real distribution of 2D poses. Training does not require correspondence between the 2D inputs to either the generator or the discriminator. We apply our approach to the task of 3D human pose estimation. Results on Human3.6M dataset demonstrates that our approach outperforms many previous supervised and weakly supervised approaches.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_7');
INSERT INTO `paper` VALUES (10450, 'Cancelable Knuckle Template Generation Based on LBP-CNN', 'Cancelable biometrics', 'Knuckle-print', 'Bio-hashing', '', '', 'Security is a prime issue whenever biometric templates are stored in centralized databases. Templates are highly susceptible to varied security and privacy attacks. Unlike passwords, biometric traits are permanently unrecoverable if lost once. In this paper efforts have been made to generate cancelable knuckle print templates. To the best of our knowledge, this is the first attempt for generating secure template for this biometric-trait. Here for learning feature representation of a biometric sample, local binary pattern based CNN is used. The experimental results are evaluated on PolyU FKP knuckle database and demonstrate high performance. The proposed protected template is resilient to various privacy attacks as well as it satisfies one important criteria of cancelable biometrics i.e. revocability.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_65');
INSERT INTO `paper` VALUES (10451, 'CapsuleGAN: Generative Adversarial Capsule Network', 'Capsule networks', 'Generative adversarial networks', '', '', '', 'We present Generative Adversarial Capsule Network (CapsuleGAN), a framework that uses capsule networks (CapsNets) instead of the standard convolutional neural networks (CNNs) as discriminators within the generative adversarial network (GAN) setting, while modeling image data. We provide guidelines for designing CapsNet discriminators and the updated GAN objective function, which incorporates the CapsNet margin loss, for training CapsuleGAN models. We show that CapsuleGAN outperforms convolutional-GAN at modeling image data distribution on MNIST and CIFAR-10 datasets, evaluated on the generative adversarial metric and at semi-supervised image classification.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_38');
INSERT INTO `paper` VALUES (10452, 'CAR-Net: Clairvoyant Attentive Recurrent Network', '', '', '', '', '', 'We present an interpretable framework for path prediction that leverages dependencies between agents’ behaviors and their spatial navigation environment. We exploit two sources of information: the past motion trajectory of the agent of interest and a wide top-view image of the navigation scene. We propose a Clairvoyant Attentive Recurrent Network (CAR-Net) that learns where to look in a large image of the scene when solving the path prediction task. Our method can attend to any area, or combination of areas, within the raw image (e.g., road intersections) when predicting the trajectory of the agent. This allows us to visualize fine-grained semantic elements of navigation scenes that influence the prediction of trajectories. To study the impact of space on agents’ trajectories, we build a new dataset made of top-view images of hundreds of scenes (Formula One racing tracks) where agents’ behaviors are heavily influenced by known areas in the images (e.g., upcoming turns). CAR-Net successfully attends to these salient regions. Additionally, CAR-Net reaches state-of-the-art accuracy on the standard trajectory forecasting benchmark, Stanford Drone Dataset (SDD). Finally, we show CAR-Net’s ability to generalize to unseen scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_10');
INSERT INTO `paper` VALUES (10453, 'CARN: Convolutional Anchored Regression Network for Fast and Accurate Single Image Super-Resolution', 'Convolutional anchored regression network', 'Convolutional neural network', 'Super-resolution', '', '', 'Although the accuracy of super-resolution (SR) methods based on convolutional neural networks (CNN) soars high, the complexity and computation also explode with the increased depth and width of the network. Thus, we propose the convolutional anchored regression network (CARN) for fast and accurate single image super-resolution (SISR). Inspired by locally linear regression methods (A+ and ARN), the new architecture consists of regression blocks that map input features from one feature space to another. Different from A+ and ARN, CARN is no longer relying on or limited by hand-crafted features. Instead, it is an end-to-end design where all the operations are converted to convolutions so that the key concepts, i.e., features, anchors, and regressors, are learned jointly. The experiments show that CARN achieves the best speed and accuracy trade-off among the SR methods. The code is available at https://github.com/ofsoundof/CARN.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_11');
INSERT INTO `paper` VALUES (10454, 'Category-Level 6D Object Pose Recovery in Depth Images', 'Category-level', '6D object pose', '3D skeleton', 'Graph matching', 'Privileged one-class learning', 'Intra-class variations, distribution shifts among source and target domains are the major challenges of category-level tasks. In this study, we address category-level full 6D object pose estimation in the context of depth modality, introducing a novel part-based architecture that can tackle the above-mentioned challenges. Our architecture particularly adapts the distribution shifts arising from shape discrepancies, and naturally removes the variations of texture, illumination, pose, etc., so we call it as “Intrinsic Structure Adaptor (ISA)”. We engineer ISA based on the followings: (i) “Semantically Selected Centers (SSC)” are proposed in order to define the “6D pose” at the level of categories. (ii) 3D skeleton structures, which we derive as shape-invariant features, are used to represent the parts extracted from the instances of given categories, and privileged one-class learning is employed based on these parts. (iii) Graph matching is performed during training in such a way that the adaptation/generalization capability of the proposed architecture is improved across unseen instances. Experiments validate the promising performance of the proposed architecture using both synthetic and real datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_41');
INSERT INTO `paper` VALUES (10455, 'CBAM: Convolutional Block Attention Module', 'Object recognition', 'Attention mechanism', 'Gated convolution', '', '', 'We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_1');
INSERT INTO `paper` VALUES (10456, 'CentralNet: A Multilayer Approach for Multimodal Fusion', 'Multimodal fusion', 'Neural networks', 'Representation learning', 'Multi-task learning', '', 'This paper proposes a novel multimodal fusion approach, aiming to produce best possible decisions by integrating information coming from multiple media. While most of the past multimodal approaches either work by projecting the features of different modalities into the same space, or by coordinating the representations of each modality through the use of constraints, our approach borrows from both visions. More specifically, assuming each modality can be processed by a separated deep convolutional network, allowing to take decisions independently from each modality, we introduce a central network linking the modality specific networks. This central network not only provides a common feature embedding but also regularizes the modality specific networks through the use of multi-task learning. The proposed approach is validated on 4 different computer vision tasks on which it consistently improves the accuracy of existing multimodal fusion approaches.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_44');
INSERT INTO `paper` VALUES (10457, 'CGIntrinsics: Better Intrinsic Image Decomposition Through Physically-Based Rendering', '', '', '', '', '', 'Intrinsic image decomposition is a challenging, long-standing computer vision problem for which ground truth data is very difficult to acquire. We explore the use of synthetic data for training CNN-based intrinsic image decomposition models, then applying these learned models to real-world images. To that end, we present CGIntrinsics, a new, large-scale dataset of physically-based rendered images of scenes with full ground truth decompositions. The rendering process we use is carefully designed to yield high-quality, realistic images, which we find to be crucial for this problem domain. We also propose a new end-to-end training method that learns better decompositions by leveraging CGIntrinsics, and optionally IIW and SAW, two recent datasets of sparse annotations on real-world images. Surprisingly, we find that a decomposition network trained solely on our synthetic data outperforms the state-of-the-art on both IIW and SAW, and performance improves even further when IIW and SAW data is added during training. Our work demonstrates the suprising effectiveness of carefully-rendered synthetic data for the intrinsic images task.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_23');
INSERT INTO `paper` VALUES (10458, 'ChangeNet: A Deep Learning Architecture for Visual Change Detection', 'Change detection', 'CNN', '', '', '', 'The increasing urban population in cities necessitates the need for the development of smart cities that can offer better services to its citizens. Drone technology plays a crucial role in the smart city environment and is already involved in a number of functions in smart cities such as traffic control and construction monitoring. A major challenge in fast growing cities is the encroachment of public spaces. A robotic solution using visual change detection can be used for such purposes. For the detection of encroachment, a drone can monitor outdoor urban areas over a period of time to infer the visual changes. Visual change detection is a higher level inference task that aims at accurately identifying variations between a reference image (historical) and a new test image depicting the current scenario. In case of images, the challenges are complex considering the variations caused by environmental conditions that are actually unchanged events. Human mind interprets the change by comparing the current status with historical data at intelligence level rather than using only visual information. In this paper, we present a deep architecture called ChangeNet for detecting changes between pairs of images and express the same semantically (label the change). A parallel deep convolutional neural network (CNN) architecture for localizing and identifying the changes between image pair has been proposed in this paper. The architecture is evaluated with VL-CMU-CD street view change detection, TSUNAMI and Google Street View (GSV) datasets that resemble drone captured images. The performance of the model for different lighting and seasonal conditions are experimented quantitatively and qualitatively. The result shows that ChangeNet outperforms the state of the art by achieving 98.3% pixel accuracy, 77.35% object based Intersection over Union (IoU) and 88.9% area under Receiver Operating Characteristics (RoC) curve.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_10');
INSERT INTO `paper` VALUES (10459, 'Channel Pruning for Visual Tracking', 'Correlation filter', 'Deep feature', 'Channel pruning', 'Iterative optimization', '', 'Deep convolutional feature based Correlation Filter trackers have achieved record-breaking accuracy, but the huge computational complexity limits their application. In this paper, we derive the efficient convolution operators (ECO) tracker which obtains the top rank on VOT-2016. Firstly, we introduce a channel pruned VGG16 model to fast extract most representative channels for deep features. Then an Average Feature Energy Ratio method is put forward to select advantageous convolution channels, and an adaptive iterative strategy is designed to optimize object location. Finally, extensive experimental results on four benchmarks OTB-2013, OTB-2015, VOT-2016 and VOT-2017, demonstrate that our tracker performs favorably against the state-of-the-art methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_3');
INSERT INTO `paper` VALUES (10460, 'Characterization of Visual Object Representations in Rat Primary Visual Cortex', 'Rat’s visual system', 'Core Object Recognition', 'Objects classification', '', '', 'For most animal species, quick and reliable identification of visual objects is critical for survival. This applies also to rodents, which, in recent years, have become increasingly popular models of visual functions. For this reason in this work we analyzed how various properties of visual objects are represented in rat primary visual cortex (V1). The analysis has been carried out through supervised (classification) and unsupervised (clustering) learning methods. We assessed quantitatively the discrimination capabilities of V1 neurons by demonstrating how photometric properties (luminosity and object position in the scene) can be derived directly from the neuronal responses.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_43');
INSERT INTO `paper` VALUES (10461, 'Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation', 'Semantic segmentation', 'Adversarial example', 'Spatial consistency', '', '', 'Deep Neural Networks (DNNs) have been widely applied in various recognition tasks. However, recently DNNs have been shown to be vulnerable against adversarial examples, which can mislead DNNs to make arbitrary incorrect predictions. While adversarial examples are well studied in classification tasks, other learning problems may have different properties. For instance, semantic segmentation requires additional components such as dilated convolutions and multiscale processing. In this paper, we aim to characterize adversarial examples based on spatial context information in semantic segmentation. We observe that spatial consistency information can be potentially leveraged to detect adversarial examples robustly even when a strong adaptive attacker has access to the model and detection strategies. We also show that adversarial examples based on attacks considered within the paper barely transfer among models, even though transferability is common in classification. Our observations shed new light on developing adversarial attacks and defenses to better understand the vulnerabilities of DNNs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_14');
INSERT INTO `paper` VALUES (10462, 'Chasing Feet in the Wild: A Proposed Egocentric Motion-Aware Gait Assessment Tool', 'Ambulatory gait analysis', 'Wearable sensors', 'Deep convolutional neural networks', 'Egocentric vision', 'Optical flow', 'Despite advances in gait analysis tools, including optical motion capture and wireless electrophysiology, our understanding of human mobility is largely limited to controlled conditions in a clinic and/or laboratory. In order to examine human mobility under natural conditions, or the ‘wild’, this paper presents a novel markerless model to obtain gait patterns by localizing feet in the egocentric video data. Based on a belt-mounted camera feed, the proposed hybrid FootChaser model consists of: (1) the FootRegionProposer, a ConvNet that proposes regions with high probability of containing feet in RGB frames (global appearance of feet), and (2) LocomoNet, which is sensitive to the periodic gait patterns, and further examines the temporal content in the stacks of optical flow corresponding to the proposed region. The LocomoNet significantly boosted the overall model’s result by filtering out the false positives proposed by the FootRegionProposer. This work advances our long-term objective to develop novel markerless models to extract spatiotemporal gait parameters, particularly step width, to complement existing inertial measurement unit (IMU) based methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_12');
INSERT INTO `paper` VALUES (10463, 'Choose Your Neuron: Incorporating Domain Knowledge Through Neuron-Importance', 'Zero Shot Learning', 'Interpretability', 'Grad-CAM', '', '', 'Individual neurons in convolutional neural networks supervised for image-level classification tasks have been shown to implicitly learn semantically meaningful concepts ranging from simple textures and shapes to whole or partial objects – forming a “dictionary” of concepts acquired through the learning process. In this work we introduce a simple, efficient zero-shot learning approach based on this observation. Our approach, which we call Neuron Importance-Aware Weight Transfer (NIWT), learns to map domain knowledge about novel “unseen” classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts – essentially learning classifiers by discovering and composing learned semantic concepts in deep networks. Our approach shows improvements over previous approaches on the CUBirds and AWA2 generalized zero-shot learning benchmarks. We demonstrate our approach on a diverse set of semantic inputs as external domain knowledge including attributes and natural language captions. Moreover by learning inverse mappings, NIWT can provide visual and textual explanations for the predictions made by the newly learned classifiers and provide neuron names. Our code is available at https://github.com/ramprs/neuron-importance-zsl.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_32');
INSERT INTO `paper` VALUES (10464, 'CIRL: Controllable Imitative Reinforcement Learning for Vision-Based Self-driving', 'Imitative reinforcement learning', 'Autonomous driving', '', '', '', 'Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy by reinforcement learning in the high-fidelity simulator, which performs better than supervised imitation learning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_36');
INSERT INTO `paper` VALUES (10465, 'Clustering Convolutional Kernels to Compress Deep Neural Networks', 'CNNs', 'Compression', 'Quantization', 'Weight sharing', 'Clustering', 'In this paper, we propose a novel method to compress CNNs by reconstructing the network from a small set of spatial convolution kernels. Starting from a pre-trained model, we extract representative 2D kernel centroids using k-means clustering. Each centroid replaces the corresponding kernels of the same cluster, and we use indexed representations instead of saving whole kernels. Kernels in the same cluster share their weights, and we fine-tune the model while keeping the compressed state. Furthermore, we also suggest an efficient way of removing redundant calculations in the compressed convolutional layers. We experimentally show that our technique works well without harming the accuracy of widely-used CNNs. Also, our ResNet-18 even outperforms its uncompressed counterpart at ILSVRC2012 classification task with over 10x compression ratio.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_14');
INSERT INTO `paper` VALUES (10466, 'CNN-PS: CNN-Based Photometric Stereo for General Non-convex Surfaces', 'Photometric stereo', 'Convolutional neural networks', '', '', '', 'Most conventional photometric stereo algorithms inversely solve a BRDF-based image formation model. However, the actual imaging process is often far more complex due to the global light transport on the non-convex surfaces. This paper presents a photometric stereo network that directly learns relationships between the photometric stereo input and surface normals of a scene. For handling unordered, arbitrary number of input images, we merge all the input data to the intermediate representation called observation map that has a fixed shape, is able to be fed into a CNN. To improve both training and prediction, we take into account the rotational pseudo-invariance of the observation map that is derived from the isotropic constraint. For training the network, we create a synthetic photometric stereo dataset that is generated by a physics-based renderer, therefore the global light transport is considered. Our experimental results on both synthetic and real datasets show that our method outperforms conventional BRDF-based photometric stereo algorithms especially when scenes are highly non-convex.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_1');
INSERT INTO `paper` VALUES (10467, 'Coded Illumination and Imaging for Fluorescence Based Classification', 'Fluorescence', 'Coded illumination', 'Classification', '', '', 'The quick detection of specific substances in objects such as produce items via non-destructive visual cues is vital to ensuring the quality and safety of consumer products. At the same time, it is well-known that the fluorescence excitation-emission characteristics of many organic objects can serve as a kind of “fingerprint” for detecting the presence of specific substances in classification tasks such as determining if something is safe to consume. However, conventional capture of the fluorescence excitation-emission matrix can take on the order of minutes and can only be done for point measurements. In this paper, we propose a coded illumination approach whereby light spectra are learned such that key visual fluorescent features can be easily seen for material classification. We show that under a single coded illuminant, we can capture one RGB image and perform pixel-level classifications of materials at high accuracy. This is demonstrated through effective classification of different types of honey and alcohol using real images.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_31');
INSERT INTO `paper` VALUES (10468, 'Coded Two-Bucket Cameras for Computer Vision', 'Actual Bucket', 'Programmable Light Source', 'Image Formation Model', 'Photometric Stereo', 'Tensor Code', 'We introduce coded two-bucket (C2B) imaging, a new operating principle for computational sensors with applications in active 3D shape estimation and coded-exposure imaging. A C2B sensor modulates the light arriving at each pixel by controlling which of the pixel’s two “buckets” should integrate it. C2B sensors output two images per video frame—one per bucket—and allow rapid, fully-programmable, per-pixel control of the active bucket. Using these properties as a starting point, we (1) develop an image formation model for these sensors, (2) couple them with programmable light sources to acquire illumination mosaics, i.e., images of a scene under many different illumination conditions whose pixels have been multiplexed and acquired in one shot, and (3) show how to process illumination mosaics to acquire live disparity or normal maps of dynamic scenes at the sensor’s native resolution. We present the first experimental demonstration of these capabilities, using a fully-functional C2B camera prototype. Key to this unique prototype is a novel programmable CMOS sensor that we designed from the ground up, fabricated and turned into a working system.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_4');
INSERT INTO `paper` VALUES (10469, 'Collaborative Deep Reinforcement Learning for Multi-object Tracking', 'Object tracking', 'Multi-object', 'Deep reinforcement learning', '', '', 'In this paper, we propose a collaborative deep reinforcement learning (C-DRL) method for multi-object tracking. Most existing multi-object tracking methods employ the tracking-by-detection strategy which first detects objects in each frame and then associates them across different frames. However, the performance of these methods rely heavily on the detection results, which are usually unsatisfied in many real applications, especially in crowded scenes. To address this, we develop a deep prediction-decision network in our C-DRL, which simultaneously detects and predicts objects under a unified network via deep reinforcement learning. Specifically, we consider each object as an agent and track it via the prediction network, and seek the optimal tracked results by exploiting the collaborative interactions of different agents and environments via the decision network. Experimental results on the challenging MOT15 and MOT16 benchmarks are presented to show the effectiveness of our approach.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_36');
INSERT INTO `paper` VALUES (10470, 'Coloring with Words: Guiding Image Colorization Through Text-Based Palette Generation', '', '', '', '', '', 'This paper proposes a novel approach to generate multiple color palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated color palette. In contrast to existing approaches, our model can understand rich text, whether it is a single word, a phrase, or a sentence, and generate multiple possible palettes from it. For this task, we introduce our manually curated dataset called Palette-and-Text (PAT). Our proposed model called Text2Colors consists of two conditional generative adversarial networks: the text-to-palette generation networks and the palette-based colorization networks. The former captures the semantics of the text input and produce relevant color palettes. The latter colorizes a grayscale image using the generated color palette. Our evaluation results show that people preferred our generated palettes over ground truth palettes and that our model can effectively reflect the given palette when colorizing an image.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_27');
INSERT INTO `paper` VALUES (10471, 'Combination of Spatially-Modulated ToF and Structured Light for MPI-Free Depth Estimation', 'ToF sensors', 'Multi-path', 'Structured Light', 'Depth acquisition', 'Data fusion', 'Multi-path Interference (MPI) is one of the major sources of error in Time-of-Flight (ToF) camera depth measurements. A possible solution for its removal is based on the separation of direct and global light through the projection of multiple sinusoidal patterns. In this work we extend this approach by applying a Structured Light (SL) technique on the same projected patterns. This allows to compute two depth maps with a single ToF acquisition, one with the Time-of-Flight principle and the other with the Structured Light principle. The two depth fields are finally combined using a Maximum-Likelihood approach in order to obtain an accurate depth estimation free from MPI error artifacts. Experimental results demonstrate that the proposed method has very good MPI correction properties with state-of-the-art performances.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_21');
INSERT INTO `paper` VALUES (10472, 'Combining 3D Model Contour Energy and Keypoints for Object Tracking', '3D tracking', 'Monocular', 'Model-based', 'Pose estimation', '', 'We present a new combined approach for monocular model-based 3D tracking. A preliminary object pose is estimated by using a keypoint-based technique. The pose is then refined by optimizing the contour energy function. The energy determines the degree of correspondence between the contour of the model projection and the image edges. It is calculated based on both the intensity and orientation of the raw image gradient. For optimization, we propose a technique and search area constraints that allow overcoming the local optima and taking into account information obtained through keypoint-based pose estimation. Owing to its combined nature, our method eliminates numerous issues of keypoint-based and edge-based approaches. We demonstrate the efficiency of our method by comparing it with state-of-the-art methods on a public benchmark dataset that includes videos with various lighting conditions, movement patterns, and speed.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_4');
INSERT INTO `paper` VALUES (10473, 'Compact Deep Aggregation for Set Retrieval', '', '', '', '', '', 'The objective of this work is to learn a compact embedding of a set of descriptors that is suitable for efficient retrieval and ranking, whilst maintaining discriminability of the individual descriptors. We focus on a specific example of this general problem – that of retrieving images containing multiple faces from a large scale dataset of images. Here the set consists of the face descriptors in each image, and given a query for multiple identities, the goal is then to retrieve, in order, images which contain all the identities, all but one, etc.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_36');
INSERT INTO `paper` VALUES (10474, 'Comparator Networks', 'Network Comparison', 'Weight Templates', 'Verification Template', 'ResNet', 'Front Mouth', 'The objective of this work is set-based verification, e.g. to decide if two sets of images of a face are of the same person or not. The traditional approach to this problem is to learn to generate a feature vector per image, aggregate them into one vector to represent the set, and then compute the cosine similarity between sets. Instead, we design a neural network architecture that can directly learn set-wise verification.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_48');
INSERT INTO `paper` VALUES (10475, 'Comparing Methods for Assessment of Facial Dynamics in Patients with Major Neurocognitive Disorders', 'Facial dynamics', 'Facial expressions', 'Neurocognitive disorders', 'Alzheimer’s disease', '', 'Assessing facial dynamics in patients with major neurocognitive disorders and specifically with Alzheimer’s disease (AD) has shown to be highly challenging. Classically such assessment is performed by clinical staff, evaluating verbal and non-verbal language of AD-patients, since they have lost a substantial amount of their cognitive capacity, and hence communication ability. In addition, patients need to communicate important messages, such as discomfort or pain. Automated methods would support the current healthcare system by allowing for telemedicine, i.e., lesser costly and logistically inconvenient examination. In this work we compare methods for assessing facial dynamics such as talking, singing, neutral and smiling in AD-patients, captured during music mnemotherapy sessions. Specifically, we compare 3D ConvNets, Very Deep Neural Network based Two-Stream ConvNets, as well as Improved Dense Trajectories. We have adapted these methods from prominent action recognition methods and our promising results suggest that the methods generalize well to the context of facial dynamics. The Two-Stream ConvNets in combination with ResNet-152 obtains the best performance on our dataset, capturing well even minor facial dynamics and has thus sparked high interest in the medical community.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_10');
INSERT INTO `paper` VALUES (10476, 'Complex-YOLO: An Euler-Region-Proposal for Real-Time 3D Object Detection on Point Clouds', '3D object detection', 'Point cloud processing', 'Lidar', 'Autonomous driving', '', 'Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_11');
INSERT INTO `paper` VALUES (10477, 'Compositing-Aware Image Search', 'Background Scene', 'Foreground Objects', 'Learn Feature Representations', 'Common Embedding Space', 'Triplet Extension', 'We present a new image search technique that, given a background image, returns compatible foreground objects for image compositing tasks. The compatibility of a foreground object and a background scene depends on various aspects such as semantics, surrounding context, geometry, style and color. However, existing image search techniques measure the similarities on only a few aspects, and may return many results that are not suitable for compositing. Moreover, the importance of each factor may vary for different object categories and image content, making it difficult to manually define the matching criteria. In this paper, we propose to learn feature representations for foreground objects and background scenes respectively, where image content and object category information are jointly encoded during training. As a result, the learned features can adaptively encode the most important compatibility factors. We project the features to a common embedding space, so that the compatibility scores can be easily measured using the cosine similarity, enabling very efficient search. We collect an evaluation set consisting of eight object categories commonly used in compositing tasks, on which we demonstrate that our approach significantly outperforms other search techniques.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_31');
INSERT INTO `paper` VALUES (10478, 'Composition Loss for Counting, Density Map Estimation and Localization in Dense Crowds', 'Crowd counting', 'Localization', 'Composition loss', 'Convolution Neural Networks', '', 'With multiple crowd gatherings of millions of people every year in events ranging from pilgrimages to protests, concerts to marathons, and festivals to funerals; visual crowd analysis is emerging as a new frontier in computer vision. In particular, counting in highly dense crowds is a challenging problem with far-reaching applicability in crowd safety and management, as well as gauging political significance of protests and demonstrations. In this paper, we propose a novel approach that simultaneously solves the problems of counting, density map estimation and localization of people in a given dense crowd image. Our formulation is based on an important observation that the three problems are inherently related to each other making the loss function for optimizing a deep CNN decomposable. Since localization requires high-quality images and annotations, we introduce UCF-QNRF dataset that overcomes the shortcomings of previous datasets, and contains 1.25 million humans manually marked with dot annotations. Finally, we present evaluation measures and comparison with recent deep CNNs, including those developed specifically for crowd counting. Our approach significantly outperforms state-of-the-art on the new dataset, which is the most challenging dataset with the largest number of crowd annotations in the most diverse set of scenes.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_33');
INSERT INTO `paper` VALUES (10479, 'Compositional Learning for Human Object Interaction', '', '', '', '', '', 'The world of human-object interactions is rich. While generally we sit on chairs and sofas, if need be we can even sit on TVs or top of shelves. In recent years, there has been progress in modeling actions and human-object interactions. However, most of these approaches require lots of data. It is not clear if the learned representations of actions are generalizable to new categories. In this paper, we explore the problem of zero-shot learning of human-object interactions. Given limited verb-noun interactions in training data, we want to learn a model than can work even on unseen combinations. To deal with this problem, In this paper, we propose a novel method using external knowledge graph and graph convolutional networks which learns how to compose classifiers for verb-noun pairs. We also provide benchmarks on several dataset for zero-shot learning including both image and video. We hope our method, dataset and baselines will facilitate future research in this direction.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_15');
INSERT INTO `paper` VALUES (10480, 'Compound Memory Networks for Few-Shot Video Classification', 'Few-shot video learning', 'Video classification', 'Memory-augmented neural networks', 'Compound memory networks', '', 'In this paper, we propose a new memory network structure for few-shot video classification by making the following contributions. First, we propose a compound memory network (CMN) structure under the key-value memory network paradigm, in which each key memory involves multiple constituent keys. These constituent keys work collaboratively for training, which enables the CMN to obtain an optimal video representation in a larger space. Second, we introduce a multi-saliency embedding algorithm which encodes a variable-length video sequence into a fixed-size matrix representation by discovering multiple saliencies of interest. For example, given a video of car auction, some people are interested in the car, while others are interested in the auction activities. Third, we design an abstract memory on top of the constituent keys. The abstract memory and constituent keys form a layered structure, which makes the CMN more efficient and capable of being scaled, while also retaining the representation capability of the multiple keys. We compare CMN with several state-of-the-art baselines on a new few-shot video classification dataset and show the effectiveness of our approach.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_46');
INSERT INTO `paper` VALUES (10481, 'Compressing the Input for CNNs with the First-Order Scattering Transform', 'CNN', 'SIFT', 'Image descriptors', 'First-order scattering', '', 'We study the first-order scattering transform as a candidate for reducing the signal processed by a convolutional neural network (CNN). We show theoretical and empirical evidence that in the case of natural images and sufficiently small translation invariance, this transform preserves most of the signal information needed for classification while substantially reducing the spatial resolution and total signal size. We demonstrate that cascading a CNN with this representation performs on par with ImageNet classification models, commonly used in downstream tasks, such as the ResNet-50. We subsequently apply our trained hybrid ImageNet model as a base model on a detection system, which has typically larger image inputs. On Pascal VOC and COCO detection tasks we demonstrate improvements in the inference speed and training memory consumption compared to models trained directly on the input image.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_19');
INSERT INTO `paper` VALUES (10482, 'Computer Vision for Medical Infant Motion Analysis: State of the Art and RGB-D Data Set', 'Motion analysis', 'Infants', 'Pose estimation', 'RGB-D', 'Data set', 'Assessment of spontaneous movements of infants lets trained experts predict neurodevelopmental disorders like cerebral palsy at a very young age, allowing early intervention for affected infants. An automated motion analysis system requires to accurately capture body movements, ideally without markers or attached sensors to not affect the movements of infants. A vast majority of recent approaches for human pose estimation focuses on adults, leading to a degradation of accuracy if applied to infants. Hence, multiple systems for infant pose estimation have been developed. Due to the lack of publicly available benchmark data sets, a standardized evaluation, let alone a comparison of different approaches is impossible. We fill this gap by releasing the Moving INfants In RGB-D (MINI-RGBD) (Data set available for research purposes at http://s.fhg.de/mini-rgbd) data set, created using the recently introduced Skinned Multi-Infant Linear body model (SMIL). We map real infant movements to the SMIL model with realistic shapes and textures, and generate RGB and depth images with precise ground truth 2D and 3D joint positions. We evaluate our data set with state-of-the-art methods for 2D pose estimation in RGB images and for 3D pose estimation in depth images. Evaluation of 2D pose estimation results in a PCKh rate of 88.1% and 94.5% (depending on correctness threshold), and PCKh rates of 64.2%, respectively 90.4% for 3D pose estimation. We hope to foster research in medical infant motion analysis to get closer to an automated system for early detection of neurodevelopmental disorders.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_3');
INSERT INTO `paper` VALUES (10483, 'Concept Mask: Large-Scale Segmentation from Semantic Concepts', 'Semantic segmentation', 'Large-scale segmentation', 'Semi-supervised learning', 'Weakly-supervised learning', 'Zero-shot learning', 'Existing works on semantic segmentation typically consider a small number of labels, ranging from tens to a few hundreds. With a large number of labels, training and evaluation of such task become extremely challenging due to correlation between labels and lack of datasets with complete annotations. We formulate semantic segmentation as a problem of image segmentation given a semantic concept, and propose a novel system which can potentially handle an unlimited number of concepts, including objects, parts, stuff, and attributes. We achieve this using a weakly and semi-supervised framework leveraging multiple datasets with different levels of supervision. We first train a deep neural network on a 6M stock image dataset with only image-level labels to learn visual-semantic embedding on 18K concepts. Then, we refine and extend the embedding network to predict an attention map, using a curated dataset with bounding box annotations on 750 concepts. Finally, we train an attention-driven class agnostic segmentation network using an 80-category fully annotated dataset. We perform extensive experiments to validate that the proposed system performs competitively to the state of the art on fully supervised concepts, and is capable of producing accurate segmentations for weakly learned and unseen concepts.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_33');
INSERT INTO `paper` VALUES (10484, 'Conditional Image-Text Embedding Networks', 'Natural language grounding', 'Phrase localization', 'Embedding methods', 'Conditional models', '', 'This paper presents an approach for grounding phrases in images which jointly learns multiple text-conditioned embeddings in a single end-to-end model. In order to differentiate text phrases into semantically distinct subspaces, we propose a concept weight branch that automatically assigns phrases to embeddings, whereas prior works predefine such assignments. Our proposed solution simplifies the representation requirements for individual embeddings and allows the underrepresented concepts to take advantage of the shared representations before feeding them into concept-specific layers. Comprehensive experiments verify the effectiveness of our approach across three phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where we obtain a (resp.) 4%, 3%, and 4% improvement in grounding performance over a strong region-phrase embedding baseline (Code: https://github.com/BryanPlummer/cite).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_16');
INSERT INTO `paper` VALUES (10485, 'Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency', 'Visual attention', 'Gaze estimation', 'Saliency', '', '', 'This paper addresses the challenging problem of estimating the general visual attention of people in images. Our proposed method is designed to work across multiple naturalistic social scenarios and provides a full picture of the subject’s attention and gaze. In contrast, earlier works on gaze and attention estimation have focused on constrained problems in more specific contexts. In particular, our model explicitly represents the gaze direction and handles out-of-frame gaze targets. We leverage three different datasets using a multi-task learning approach. We evaluate our method on widely used benchmarks for single-tasks such as gaze angle estimation and attention-within-an-image, as well as on the new challenging task of generalized visual attention prediction. In addition, we have created extended annotations for the MMDB and GazeFollow datasets which are used in our experiments, which we will publicly release.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_24');
INSERT INTO `paper` VALUES (10486, 'Consensus-Driven Propagation in Massive Unlabeled Data for Face Recognition', '', '', '', '', '', 'Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the “committee” and the “mediator”, which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_35');
INSERT INTO `paper` VALUES (10487, 'Constrained Optimization Based Low-Rank Approximation of Deep Neural Networks', 'Low-rank approximation', 'Resource allocation', 'Constrained optimization', 'Integer relaxiation', '', 'We present COBLA—Constrained Optimization Based Low-rank Approximation—a systematic method of finding an optimal low-rank approximation of a trained convolutional neural network, subject to constraints in the number of multiply-accumulate (MAC) operations and the memory footprint. COBLA optimally allocates the constrained computation resources into each layer of the approximated network. The singular value decomposition of the network weight is computed, then a binary masking variable is introduced to denote whether a particular singular value and the corresponding singular vectors are used in low-rank approximation. With this formulation, the number of the MAC operations and the memory footprint are represented as linear constraints in terms of the binary masking variables. The resulted 0–1 integer programming problem is approximately solved by sequential quadratic programming. COBLA does not introduce any hyperparameter. We empirically demonstrate that COBLA outperforms prior art using the SqueezeNet and VGG-16 architecture on the ImageNet dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_45');
INSERT INTO `paper` VALUES (10488, 'Constrained-Size Tensorflow Models for YouTube-8M Video Understanding Challenge', 'Computer vision', 'Video analysis', 'Deep learning', 'Tensorflow', '', 'This paper presents our 7th place solution to the second YouTube-8M video understanding competition which challenges participates to build a constrained-size model to classify millions of YouTube videos into thousands of classes. Our final model consists of four single models aggregated into one Tensorflow graph. For each single model, we use the same network architecture as in the winning solution of the first YouTube-8M video understanding competition, namely Gated NetVLAD. We train the single models separately in Tensorflow’s default float32 precision, then replace weights with float16 precision and ensemble them in the evaluation and inference stages, achieving 48.5% compression rate without loss of precision. Our best model achieved 88.324% GAP on private leaderboard. The code is publicly available at https://github.com/boliu61/youtube-8m.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_22');
INSERT INTO `paper` VALUES (10489, 'Constraint-Aware Deep Neural Network Compression', '', '', '', '', '', 'Deep neural network compression has the potential to bring modern resource-hungry deep networks to resource-limited devices. However, in many of the most compelling deployment scenarios of compressed deep networks, the operational constraints matter: for example, a pedestrian detection network on a self-driving car may have to satisfy a latency constraint for safe operation. We propose the first principled treatment of deep network compression under operational constraints. We formulate the compression learning problem from the perspective of constrained Bayesian optimization, and introduce a cooling (annealing) strategy to guide the network compression towards the target constraints. Experiments on ImageNet demonstrate the value of modelling constraints directly in network compression.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_25');
INSERT INTO `paper` VALUES (10490, 'Contemplating Visual Emotions: Understanding and Overcoming Dataset Bias', 'Emotion recognition', 'Webly supervised learning', '', '', '', 'While machine learning approaches to visual emotion recognition offer great promise, current methods consider training and testing models on small scale datasets covering limited visual emotion concepts. Our analysis identifies an important but long overlooked issue of existing visual emotion benchmarks in the form of dataset biases. We design a series of tests to show and measure how such dataset biases obstruct learning a generalizable emotion recognition model. Based on our analysis, we propose a webly supervised approach by leveraging a large quantity of stock image data. Our approach uses a simple yet effective curriculum guided training strategy for learning discriminative emotion features. We discover that the models learned using our large scale stock image dataset exhibit significantly better generalization ability than the existing datasets without the manual collection of even a single label. Moreover, visual representation learned using our approach holds a lot of promise across a variety of tasks on different image and video datasets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_36');
INSERT INTO `paper` VALUES (10491, 'Context Graph Based Video Frame Prediction Using Locally Guided Objective', '', '', '', '', '', 'This paper proposes a feature reconstruction based approach using pixel-graph and Generative Adversarial Networks (GAN) for solving the problem of synthesizing future frames from video scenes. Recent methods of frame synthesis often generate blurry outcomes in case of long-range prediction and scenes involving multiple objects moving at different velocities due to their holistic approach. Our proposed method introduces a novel pixel-graph based context aggregation layer (PixGraph) which efficiently captures long range dependencies. PixGraph incorporates a weighting scheme through which the internal features of each pixel (or a group of neighboring pixels) can be modeled independently of the others, thus handling the issue of separate objects moving in different directions and with very dissimilar speed. We also introduce a novel objective function, the Locally Guided Gram Loss (LGGL), which aides the GAN based model to maximize the similarity between the intermediate features of the ground-truth and the network output by constructing Gram matrices from locally extracted patches over several levels of the generator. Our proposed model is end-to-end trainable and exhibits superior performance compared to the state-of-the-art on four real-world benchmark video datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_15');
INSERT INTO `paper` VALUES (10492, 'Context Refinement for Object Detection', 'Object detection', 'Context analysis', 'Deep convolutional neural network', '', '', 'Current two-stage object detectors, which consists of a region proposal stage and a refinement stage, may produce unreliable results due to ill-localized proposed regions. To address this problem, we propose a context refinement algorithm that explores rich contextual information to better refine each proposed region. In particular, we first identify neighboring regions that may contain useful contexts and then perform refinement based on the extracted and unified contextual information. In practice, our method effectively improves the quality of the final detection results as well as region proposals. Empirical studies show that context refinement yields substantial and consistent improvements over different baseline detectors. Moreover, the proposed algorithm brings around 3% performance gain on PASCAL VOC benchmark and around 6% gain on MS COCO benchmark respectively.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_5');
INSERT INTO `paper` VALUES (10493, 'Contextual-Based Image Inpainting: Infer, Match, and Translate', 'Image inpainting', 'GANs', 'Feature manipulation', '', '', 'We study the task of image inpainting, which is to fill in the missing region of an incomplete image with plausible contents. To this end, we propose a learning-based approach to generate visually coherent completion given a high-resolution image with missing components. In order to overcome the difficulty to directly learn the distribution of high-dimensional image data, we divide the task into inference and translation as two separate steps and model each step with a deep neural network. We also use simple heuristics to guide the propagation of local textures from the boundary to the hole. We show that, by using such techniques, inpainting reduces to the problem of learning two image-feature translation functions in much smaller space and hence easier to train. We evaluate our method on several public datasets and show that we generate results of better visual quality than previous state-of-the-art methods.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_1');
INSERT INTO `paper` VALUES (10494, 'ContextVP: Fully Context-Aware Video Prediction', '', '', '', '', '', 'Video prediction models based on convolutional networks, recurrent networks, and their combinations often result in blurry predictions. We identify an important contributing factor for imprecise predictions that has not been studied adequately in the literature: blind spots, i.e., lack of access to all relevant past information for accurately predicting the future. To address this issue, we introduce a fully context-aware architecture that captures the entire available past context for each pixel using Parallel Multi-Dimensional LSTM units and aggregates it using blending units. Our model outperforms a strong baseline network of 20 recurrent convolutional layers and yields state-of-the-art performance for next step prediction on three challenging real-world video datasets: Human 3.6M, Caltech Pedestrian, and UCF-101. Moreover, it does so with fewer parameters than several recently proposed models, and does not rely on deep convolutional networks, multi-scale architectures, separation of background and foreground modeling, motion flow learning, or adversarial training. These results highlight that full awareness of past context is of crucial importance for video prediction.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_46');
INSERT INTO `paper` VALUES (10495, 'Contour Knowledge Transfer for Salient Object Detection', 'Saliency detection', 'Deep learning', 'Transfer learning', '', '', 'In recent years, deep Convolutional Neural Networks (CNNs) have broken all records in salient object detection. However, training such a deep model requires a large amount of manual annotations. Our goal is to overcome this limitation by automatically converting an existing deep contour detection model into a salient object detection model without using any manual salient object masks. For this purpose, we have created a deep network architecture, namely Contour-to-Saliency Network (C2S-Net), by grafting a new branch onto a well-trained contour detection network. Therefore, our C2S-Net has two branches for performing two different tasks: (1) predicting contours with the original contour branch, and (2) estimating per-pixel saliency score of each image with the newly-added saliency branch. To bridge the gap between these two tasks, we further propose a contour-to-saliency transferring method to automatically generate salient object masks which can be used to train the saliency branch from outputs of the contour branch. Finally, we introduce a novel alternating training pipeline to gradually update the network parameters. In this scheme, the contour branch generates saliency masks for training the saliency branch, while the saliency branch, in turn, feeds back saliency knowledge in the form of saliency-aware contour labels, for fine-tuning the contour branch. The proposed method achieves state-of-the-art performance on five well-known benchmarks, outperforming existing fully supervised methods while also maintaining high efficiency.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_22');
INSERT INTO `paper` VALUES (10496, 'ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases', 'Bias detection', 'Interpretability', 'Adversarial examples', '', '', 'ConvNets and ImageNet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement combined with the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases question the reliability of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. The contribution of this study is threefold. We first experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are vastly underestimated. Next, we show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user. We finally introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a valuable tool both for improving our understanding of ConvNets’ predictions and for designing more reliable models.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_31');
INSERT INTO `paper` VALUES (10497, 'Convolutional Networks for Object Category and 3D Pose Estimation from 2D Images', '3D pose estimation', 'Category-dependent pose networks', 'Multi-task networks', 'ResNet architecture', '', 'Current CNN-based algorithms for recovering the 3D pose of an object in an image assume knowledge about both the object category and its 2D localization in the image. In this paper, we relax one of these constraints and propose to solve the task of joint object category and 3D pose estimation from an image assuming known 2D localization. We design a new architecture for this task composed of a feature network that is shared between subtasks, an object categorization network built on top of the feature network, and a collection of category dependent pose regression networks. We also introduce suitable loss functions and a training method for the new architecture. Experiments on the challenging PASCAL3D+ dataset show state-of-the-art performance in the joint categorization and pose estimation task. Moreover, our performance on the joint task is comparable to the performance of state-of-the-art methods on the simpler 3D pose estimation with known object category task.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_43');
INSERT INTO `paper` VALUES (10498, 'Convolutional Networks with Adaptive Inference Graphs', '', '', '', '', '', 'Do convolutional networks really need a fixed feed-forward structure? What if, after identifying the high-level concept of an image, a network could move directly to a layer that can distinguish fine-grained differences? Currently, a network would first need to execute sometimes hundreds of intermediate layers that specialize in unrelated aspects. Ideally, the more a network already knows about an image, the better it should be at deciding which layer to compute next. In this work, we propose convolutional networks with adaptive inference graphs (ConvNet-AIG) that adaptively define their network topology conditioned on the input image. Following a high-level structure similar to residual networks (ResNets), ConvNet-AIG decides for each input image on the fly which layers are needed. In experiments on ImageNet we show that ConvNet-AIG learns distinct inference graphs for different categories. Both ConvNet-AIG with 50 and 101 layers outperform their ResNet counterpart, while using \\(20\\%\\) and \\(33\\%\\) less computations respectively. By grouping parameters into layers for related classes and only executing relevant layers, ConvNet-AIG improves both efficiency and overall classification quality. Lastly, we also study the effect of adaptive inference graphs on the susceptibility towards adversarial examples. We observe that ConvNet-AIG shows a higher robustness than ResNets, complementing other known defense mechanisms.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_1');
INSERT INTO `paper` VALUES (10499, 'Convolutional Neural Network for Trajectory Prediction', 'Convolutional neural network', 'Trajectory prediction', 'Anticipating human behavior', '', '', 'Predicting trajectories of pedestrians is quintessential for autonomous robots which share the same environment with humans. In order to effectively and safely interact with humans, trajectory prediction needs to be both precise and computationally efficient. In this work, we propose a convolutional neural network (CNN) based human trajectory prediction approach. Unlike more recent LSTM-based moles which attend sequentially to each frame, our model supports increased parallelism and effective temporal representation. The proposed compact CNN model is faster than the current approaches yet still yields competitive results.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_16');
INSERT INTO `paper` VALUES (10500, 'Convolutional Photomosaic Generation via Multi-scale Perceptual Losses', 'Photomosaic', 'ASCII text', 'Deep learning', 'Perceptual loss', 'Multi-scale analysis', 'Photographic mosaics (or simply photomosaics) are images comprised of smaller, equally-sized image tiles such that when viewed from a distance, the tiled images of the mosaic collectively resemble a perceptually plausible image. In this paper, we consider the challenge of automatically generating a photomosaic from an input image. Although computer-generated photomosaicking has existed for quite some time, none have considered simultaneously exploiting colour/grayscale intensity and the structure of the input across scales, as well as image semantics. We propose a convolutional network for generating photomosaics guided by a multi-scale perceptual loss to capture colour, structure, and semantics across multiple scales. We demonstrate the effectiveness of our multi-scale perceptual loss by experimenting with producing extremely high resolution photomosaics and through the inclusion of ablation experiments that compare with a single-scale variant of the perceptual loss. We show that, overall, our approach produces visually pleasing results, providing a substantial improvement over common baselines.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_9');
INSERT INTO `paper` VALUES (10501, 'Coreset-Based Neural Network Compression', '', '', '', '', '', 'We propose a novel Convolutional Neural Network (CNN) compression algorithm based on coreset representations of filters. We exploit the redundancies extant in the space of CNN weights and neuronal activations (across samples) in order to obtain compression. Our method requires no retraining, is easy to implement, and obtains state-of-the-art compression performance across a wide variety of CNN architectures. Coupled with quantization and Huffman coding, we create networks that provide AlexNet-like accuracy, with a memory footprint that is 832\\(\\times \\) smaller than the original AlexNet, while also introducing significant reductions in inference time as well. Additionally these compressed networks when fine-tuned, successfully generalize to other domains as well.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_28');
INSERT INTO `paper` VALUES (10502, 'CornerNet: Detecting Objects as Paired Keypoints', 'Object detection', '', '', '', '', 'We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.1% AP on MS COCO, outperforming all existing one-stage detectors.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_45');
INSERT INTO `paper` VALUES (10503, 'Correcting the Triplet Selection Bias for Triplet Loss', 'Triplet loss', 'Selection bias', 'Domain adaptation', '', '', 'Triplet loss, popular for metric learning, has made a great success in many computer vision tasks, such as fine-grained image classification, image retrieval, and face recognition. Considering that the number of triplets grows cubically with the size of training data, triplet selection is thus indispensable for efficiently training with triplet loss. However, in practice, the training is usually very sensitive to the selection of triplets, e.g., it almost does not converge with randomly selected triplets and selecting the hardest triplets also leads to bad local minima. We argue that the bias in the selection of triplets degrades the performance of learning with triplet loss. In this paper, we propose a new variant of triplet loss, which tries to reduce the bias in triplet selection by adaptively correcting the distribution shift on the selected triplets. We refer to this new triplet loss as adapted triplet loss. We conduct a number of experiments on MNIST and Fashion-MNIST for image classification, and on CARS196, CUB200-2011, and Stanford Online Products for image retrieval. The experimental results demonstrate the effectiveness of the proposed method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_5');
INSERT INTO `paper` VALUES (10504, 'CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps', 'Image geolocalization', 'Combinatorial partitioning', 'Fine-grained classification', '', '', 'Image geolocalization is the task of identifying the location depicted in a photo based only on its visual information. This task is inherently challenging since many photos have only few, possibly ambiguous cues to their geolocation. Recent work has cast this task as a classification problem by partitioning the earth into a set of discrete cells that correspond to geographic regions. The granularity of this partitioning presents a critical trade-off; using fewer but larger cells results in lower location accuracy while using more but smaller cells reduces the number of training examples per class and increases model size, making the model prone to overfitting. To tackle this issue, we propose a simple but effective algorithm, combinatorial partitioning, which generates a large number of fine-grained output classes by intersecting multiple coarse-grained partitionings of the earth. Each classifier votes for the fine-grained classes that overlap with their respective coarse-grained ones. This technique allows us to predict locations at a fine scale while maintaining sufficient training examples per class. Our algorithm achieves the state-of-the-art performance in location recognition on multiple benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_33');
INSERT INTO `paper` VALUES (10505, 'CRAFT: Complementary Recommendation by Adversarial Feature Transform', 'Recommender systems', 'Complementary recommendation', 'Generative adversarial network', 'Unsupervised learning', 'Adversarial learning', 'We propose a framework that harnesses visual cues in an unsupervised manner to learn the co-occurrence distribution of items in real-world images for complementary recommendation. Our model learns a non-linear transformation between the two manifolds of source and target item categories (e.g., tops and bottoms in outfits). Given a large dataset of images containing instances of co-occurring items, we train a generative transformer network directly on the feature representation by casting it as an adversarial optimization problem. Such a conditional generative model can produce multiple novel samples of complementary items (in the feature space) for a given query item. We demonstrate our framework for the task of recommending complementary top apparel for a given bottom clothing item. The recommendations made by our system are diverse, and are favored by human experts over the baseline approaches.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_7');
INSERT INTO `paper` VALUES (10506, 'Cross-Modal and Hierarchical Modeling of Video and Text', 'Hierarchical sequence embedding', 'Video text retrieval', 'Video description generation', 'Action recognition', 'Zero-shot transfer', 'Visual data and text data are composed of information at multiple granularities. A video can describe a complex scene that is composed of multiple clips or shots, where each depicts a semantically coherent event or action. Similarly, a paragraph may contain sentences with different topics, which collectively conveys a coherent message or story. In this paper, we investigate the modeling techniques for such hierarchical sequential data where there are correspondences across multiple modalities. Specifically, we introduce hierarchical sequence embedding (hse), a generic model for embedding sequential data of different modalities into hierarchically semantic spaces, with either explicit or implicit correspondence information. We perform empirical studies on large-scale video and paragraph retrieval datasets and demonstrated superior performance by the proposed methods. Furthermore, we examine the effectiveness of our learned embeddings when applied to downstream tasks. We show its utility in zero-shot action recognition and video captioning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_23');
INSERT INTO `paper` VALUES (10507, 'Cross-modal Embeddings for Video and Audio Retrieval', 'Cross-modal', 'Retrieval', 'YouTube-8M', '', '', 'In this work, we explore the multi-modal information provided by the Youtube-8M dataset by projecting the audio and visual features into a common feature space, to obtain joint audio-visual embeddings. These links are used to retrieve audio samples that fit well to a given silent video, and also to retrieve images that match a given query audio. The results in terms of Recall@K obtained over a subset of YouTube-8M videos show the potential of this unsupervised approach for cross-modal feature learning.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_62');
INSERT INTO `paper` VALUES (10508, 'Cross-Modal Hamming Hashing', 'Deep hashing', 'Cross-modal hashing', 'Hamming space retrieval', '', '', 'Cross-modal hashing enables similarity retrieval across different content modalities, such as searching relevant images in response to text queries. It provide with the advantages of computation efficiency and retrieval quality for multimedia retrieval. Hamming space retrieval enables efficient constant-time search that returns data items within a given Hamming radius to each query, by hash lookups instead of linear scan. However, Hamming space retrieval is ineffective in existing cross-modal hashing methods, subject to their weak capability of concentrating the relevant items to be within a small Hamming ball, while worse still, the Hamming distances between hash codes from different modalities are inevitably large due to the large heterogeneity across different modalities. This work presents Cross-Modal Hamming Hashing (CMHH), a novel deep cross-modal hashing approach that generates compact and highly concentrated hash codes to enable efficient and effective Hamming space retrieval. The main idea is to penalize significantly on similar cross-modal pairs with Hamming distance larger than the Hamming radius threshold, by designing a pairwise focal loss based on the exponential distribution. Extensive experiments demonstrate that CMHH can generate highly concentrated hash codes and achieve state-of-the-art cross-modal retrieval performance for both hash lookups and linear scan scenarios on three benchmark datasets, NUS-WIDE, MIRFlickr-25K, and IAPR TC-12.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_13');
INSERT INTO `paper` VALUES (10509, 'Cross-Modal Ranking with Soft Consistency and Noisy Labels for Robust RGB-T Tracking', 'Visual tracking', 'Information fusion', 'Manifold ranking', 'Soft cross-modality consistency', 'Label optimization', 'Due to the complementary benefits of visible (RGB) and thermal infrared (T) data, RGB-T object tracking attracts more and more attention recently for boosting the performance under adverse illumination conditions. Existing RGB-T tracking methods usually localize a target object with a bounding box, in which the trackers or detectors is often affected by the inclusion of background clutter. To address this problem, this paper presents a novel approach to suppress background effects for RGB-T tracking. Our approach relies on a novel cross-modal manifold ranking algorithm. First, we integrate the soft cross-modality consistency into the ranking model which allows the sparse inconsistency to account for the different properties between these two modalities. Second, we propose an optimal query learning method to handle label noises of queries. In particular, we introduce an intermediate variable to represent the optimal labels, and formulate it as a \\(l_1\\)-optimization based sparse learning problem. Moreover, we propose a single unified optimization algorithm to solve the proposed model with stable and efficient convergence behavior. Finally, the ranking results are incorporated into the patch-based object features to address the background effects, and the structured SVM is then adopted to perform RGB-T tracking. Extensive experiments suggest that the proposed approach performs well against the state-of-the-art methods on large-scale benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_49');
INSERT INTO `paper` VALUES (10510, 'CrossNet: An End-to-End Reference-Based Super Resolution Network Using Cross-Scale Warping', 'Reference-based Super Resolution', 'Light field imaging', 'Image synthesis', 'Encoder-decoder', 'Optical flow', 'The Reference-based Super-resolution (RefSR) super-resolves a low-resolution (LR) image given an external high-resolution (HR) reference image, where the reference image and LR image share similar viewpoint but with significant resolution gap (\\(8{\\times }\\)). Existing RefSR methods work in a cascaded way such as patch matching followed by synthesis pipeline with two independently defined objective functions, leading to the inter-patch misalignment, grid effect and inefficient optimization. To resolve these issues, we present CrossNet, an end-to-end and fully-convolutional deep neural network using cross-scale warping. Our network contains image encoders, cross-scale warping layers, and fusion decoder: the encoder serves to extract multi-scale features from both the LR and the reference images; the cross-scale warping layers spatially aligns the reference feature map with the LR feature map; the decoder finally aggregates feature maps from both domains to synthesize the HR output. Using cross-scale warping, our network is able to perform spatial alignment at pixel-level in an end-to-end fashion, which improves the existing schemes [1, 2] both in precision (around 2 dB–4 dB) and efficiency (more than 100 times faster).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_6');
INSERT INTO `paper` VALUES (10511, 'CTAP: Complementary Temporal Action Proposal Generation', 'Temporal action proposal', 'Temporal action detection', '', '', '', 'Temporal action proposal generation is an important task, akin to object proposals, temporal action proposals are intended to capture “clips” or temporal intervals in videos that are likely to contain an action. Previous methods can be divided to two groups: sliding window ranking and actionness score grouping. Sliding windows uniformly cover all segments in videos, but the temporal boundaries are imprecise; grouping based method may have more precise boundaries but it may omit some proposals when the quality of actionness score is low. Based on the complementary characteristics of these two methods, we propose a novel Complementary Temporal Action Proposal (CTAP) generator. Specifically, we apply a Proposal-level Actionness Trustworthiness Estimator (PATE) on the sliding windows proposals to generate the probabilities indicating whether the actions can be correctly detected by actionness scores, the windows with high scores are collected. The collected sliding windows and actionness proposals are then processed by a temporal convolutional neural network for proposal ranking and boundary adjustment. CTAP outperforms state-of-the-art methods on average recall (AR) by a large margin on THUMOS-14 and ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation method in an existing action detector, and show consistent significant improvements.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_5');
INSERT INTO `paper` VALUES (10512, 'CubeNet: Equivariance to 3D Rotation and Translation', 'Deep learning', 'Equivariance', '3D representations', '', '', '3D Convolutional Neural Networks are sensitive to transformations applied to their input. This is a problem because a voxelized version of a 3D object, and its rotated clone, will look unrelated to each other after passing through to the last layer of a network. Instead, an idealized model would preserve a meaningful representation of the voxelized object, while explaining the pose-difference between the two inputs. An equivariant representation vector has two components: the invariant identity part, and a discernable encoding of the transformation. Models that can’t explain pose-differences risk “diluting” the representation, in pursuit of optimizing a classification or regression loss function.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_35');
INSERT INTO `paper` VALUES (10513, 'CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images', 'Curriculum learning', 'Weakly supervised', 'Noisy data', 'Large-scale', 'Web images', 'We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a high-performance CNN the model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge [18] for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_9');
INSERT INTO `paper` VALUES (10514, 'Data-Driven Sparse Structure Selection for Deep Neural Networks', 'Sparse', 'Model acceleration', 'Deep network structure learning', '', '', 'Deep convolutional neural networks have liberated its extraordinary power on various tasks. However, it is still very challenging to deploy state-of-the-art models into real-world applications due to their high computational complexity. How can we design a compact and effective network without massive experiments and expert knowledge? In this paper, we propose a simple and effective framework to learn and prune deep models in an end-to-end manner. In our framework, a new type of parameter – scaling factor is first introduced to scale the outputs of specific structures, such as neurons, groups or residual blocks. Then we add sparsity regularizations on these factors, and solve this optimization problem by a modified stochastic Accelerated Proximal Gradient (APG) method. By forcing some of the factors to zero, we can safely remove the corresponding structures, thus prune the unimportant parts of a CNN. Comparing with other structure selection methods that may need thousands of trials or iterative fine-tuning, our method is trained fully end-to-end in one training pass without bells and whistles. We evaluate our method, Sparse Structure Selection with several state-of-the-art CNNs, and demonstrate very promising results with adaptive depth and width selection. Code is available at: https://github.com/huangzehao/sparse-structure-selection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_19');
INSERT INTO `paper` VALUES (10515, 'DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation', '', '', '', '', '', 'Harvesting dense pixel-level annotations to train deep neural networks for semantic segmentation is extremely expensive and unwieldy at scale. While learning from synthetic data where labels are readily available sounds promising, performance degrades significantly when testing on novel realistic data due to domain discrepancies. We present Dual Channel-wise Alignment Networks (DCAN), a simple yet effective approach to reduce domain shift at both pixel-level and feature-level. Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network. In particular, given an image from the source domain and unlabeled samples from the target domain, the generator synthesizes new images on-the-fly to resemble samples from the target domain in appearance and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike much recent and concurrent work relying on adversarial training, our framework is lightweight and easy to train. Extensive experiments on adapting models trained on synthetic segmentation benchmarks to real urban scenes demonstrate the effectiveness of the proposed framework.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_32');
INSERT INTO `paper` VALUES (10516, 'DDRNet: Depth Map Denoising and Refinement for Consumer Depth Cameras Using Cascaded CNNs', 'Depth enhancement', 'Consumer depth camera', 'Unsupervised learning', 'Convolutional neural networks', 'DynamicFusion', 'Consumer depth sensors are more and more popular and come to our daily lives marked by its recent integration in the latest Iphone X. However, they still suffer from heavy noises which limit their applications. Although plenty of progresses have been made to reduce the noises and boost geometric details, due to the inherent illness and the real-time requirement, the problem is still far from been solved. We propose a cascaded Depth Denoising and Refinement Network (DDRNet) to tackle this problem by leveraging the multi-frame fused geometry and the accompanying high quality color image through a joint training strategy. The rendering equation is exploited in our network in an unsupervised manner. In detail, we impose an unsupervised loss based on the light transport to extract the high-frequency geometry. Experimental results indicate that our network achieves real-time single depth enhancement on various categories of scenes. Thanks to the well decoupling of the low and high frequency information in the cascaded network, we achieve superior performance over the state-of-the-art techniques.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_10');
INSERT INTO `paper` VALUES (10517, 'Deblurring Natural Image Using Super-Gaussian Fields', 'Blind Image Deblurring (BID)', 'Blurred Observations', 'Sparse Priors', 'Traditional MRFs', 'Markov Random Field (MRFs)', 'Blind image deblurring is a challenging problem due to its ill-posed nature, of which the success is closely related to a proper image prior. Although a large number of sparsity-based priors, such as the sparse gradient prior, have been successfully applied for blind image deblurring, they inherently suffer from several drawbacks, limiting their applications. Existing sparsity-based priors are usually rooted in modeling the response of images to some specific filters (e.g., image gradients), which are insufficient to capture the complicated image structures. Moreover, the traditional sparse priors or regularizations model the filter response (e.g., image gradients) independently and thus fail to depict the long-range correlation among them. To address the above issues, we present a novel image prior for image deblurring based on a Super-Gaussian field model with adaptive structures. Instead of modeling the response of the fixed short-term filters, the proposed Super-Gaussian fields capture the complicated structures in natural images by integrating potentials on all cliques (e.g., centring at each pixel) into a joint probabilistic distribution. Considering that the fixed filters in different scales are impractical for the coarse-to-fine framework of image deblurring, we define each potential function as a super-Gaussian distribution. Through this definition, the partition function, the curse for traditional MRFs, can be theoretically ignored, and all model parameters of the proposed Super-Gaussian fields can be data-adaptively learned and inferred from the blurred observation with a variational framework. Extensive experiments on both blind deblurring and non-blind deblurring demonstrate the effectiveness of the proposed method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_28');
INSERT INTO `paper` VALUES (10518, 'Decoding Generic Visual Representations from Human Brain Activity Using Machine Learning', 'Neural decoding', 'Deep visual representations', '', '', '', 'Among the most impressive recent applications of neural decoding is the visual representation decoding, where the category of an object that a subject either sees or imagines is inferred by observing his/her brain activity. Even though there is an increasing interest in the aforementioned visual representation decoding task, there is no extensive study of the effect of using different machine learning models on the decoding accuracy. In this paper we provide an extensive evaluation of several machine learning models, along with different similarity metrics, for the aforementioned task, drawing many interesting conclusions. That way, this paper (a) paves the way for developing more advanced and accurate methods and (b) provides an extensive and easily reproducible baseline for the aforementioned decoding task.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_45');
INSERT INTO `paper` VALUES (10519, 'Decouple Learning for Parameterized Image Operators', '', '', '', '', '', 'Many different deep networks have been used to approximate, accelerate or improve traditional image operators, such as image smoothing, super-resolution and denoising. Among these traditional operators, many contain parameters which need to be tweaked to obtain the satisfactory results, which we refer to as “parameterized image operators”. However, most existing deep networks trained for these operators are only designed for one specific parameter configuration, which does not meet the needs of real scenarios that usually require flexible parameters settings. To overcome this limitation, we propose a new decouple learning algorithm to learn from the operator parameters to dynamically adjust the weights of a deep network for image operators, denoted as the base network. The learned algorithm is formed as another network, namely the weight learning network, which can be end-to-end jointly trained with the base network. Experiments demonstrate that the proposed framework can be successfully applied to many traditional parameterized image operators. We provide more analysis to better understand the proposed framework, which may inspire more promising research in this direction. Our codes and models have been released in https://github.com/fqnchina/DecoupleLearning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_27');
INSERT INTO `paper` VALUES (10520, 'Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment', 'Joint learning', 'Facial AU detection', 'Face alignment', 'Adaptive attention learning', '', 'Facial action unit (AU) detection and face alignment are two highly correlated tasks since facial landmarks can provide precise AU locations to facilitate the extraction of meaningful local features for AU detection. Most existing AU detection works often treat face alignment as a preprocessing and handle the two tasks independently. In this paper, we propose a novel end-to-end deep learning framework for joint AU detection and face alignment, which has not been explored before. In particular, multi-scale shared features are learned firstly, and high-level features of face alignment are fed into AU detection. Moreover, to extract precise local features, we propose an adaptive attention learning module to refine the attention map of each AU adaptively. Finally, the assembled local features are integrated with face alignment features and global features for AU detection. Experiments on BP4D and DISFA benchmarks demonstrate that our framework significantly outperforms the state-of-the-art methods for AU detection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_43');
INSERT INTO `paper` VALUES (10521, 'Deep Adversarial Attention Alignment for Unsupervised Domain Adaptation: The Benefit of Target Expectation Maximization', 'Domain adaptation', 'CycleGAN', 'Attention', 'EM', '', 'In this paper, we make two contributions to unsupervised domain adaptation (UDA) using the convolutional neural network (CNN). First, our approach transfers knowledge in all the convolutional layers through attention alignment. Most previous methods align high-level representations, e.g., activations of the fully connected (FC) layers. In these methods, however, the convolutional layers which underpin critical low-level domain knowledge cannot be updated directly towards reducing domain discrepancy. Specifically, we assume that the discriminative regions in an image are relatively invariant to image style changes. Based on this assumption, we propose an attention alignment scheme on all the target convolutional layers to uncover the knowledge shared by the source domain. Second, we estimate the posterior label distribution of the unlabeled data for target network training. Previous methods, which iteratively update the pseudo labels by the target network and refine the target network by the updated pseudo labels, are vulnerable to label estimation errors. Instead, our approach uses category distribution to calculate the cross-entropy loss for training, thereby ameliorating the error accumulation of the estimated labels. The two contributions allow our approach to outperform the state-of-the-art methods by +2.6% on the Office-31 dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_25');
INSERT INTO `paper` VALUES (10522, 'Deep Attention Neural Tensor Network for Visual Question Answering', 'Visual question answering', 'Neural tensor network', 'Open-ended VQA', '', '', 'Visual question answering (VQA) has drawn great attention in cross-modal learning problems, which enables a machine to answer a natural language question given a reference image. Significant progress has been made by learning rich embedding features from images and questions by bilinear models, while neglects the key role from answers. In this paper, we propose a novel deep attention neural tensor network (DA-NTN) for visual question answering, which can discover the joint correlations over images, questions and answers with tensor-based representations. First, we model one of the pairwise interaction (e.g., image and question) by bilinear features, which is further encoded with the third dimension (e.g., answer) to be a triplet by bilinear tensor product. Second, we decompose the correlation of different triplets by different answer and question types, and further propose a slice-wise attention module on tensor to select the most discriminative reasoning process for inference. Third, we optimize the proposed DA-NTN by learning a label regression with KL-divergence losses. Such a design enables scalable training and fast convergence over a large number of answer set. We integrate the proposed DA-NTN structure into the state-of-the-art VQA models (e.g., MLB and MUTAN). Extensive experiments demonstrate the superior accuracy than the original MLB and MUTAN models, with 1.98%, 1.70% relative increases on VQA-2.0 dataset, respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_2');
INSERT INTO `paper` VALUES (10523, 'Deep Autoencoder for Combined Human Pose Estimation and Body Model Upscaling', 'Deep learning', 'Pose estimation', 'Multiple viewpoint video', '', '', 'We present a method for simultaneously estimating 3D human pose and body shape from a sparse set of wide-baseline camera views. We train a symmetric convolutional autoencoder with a dual loss that enforces learning of a latent representation that encodes skeletal joint positions, and at the same time learns a deep representation of volumetric body shape. We harness the latter to up-scale input volumetric data by a factor of 4\\(\\times \\), whilst recovering a 3D estimate of joint positions with equal or greater accuracy than the state of the art. Inference runs in real-time (25 fps) and has the potential for passive human behaviour monitoring where there is a requirement for high fidelity estimation of human body shape and pose.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_48');
INSERT INTO `paper` VALUES (10524, 'Deep Bilevel Learning', 'Bilevel optimization', 'Regularization', 'Generalization', 'Neural networks', 'Noisy labels', 'We present a novel regularization approach to train neural networks that enjoys better generalization and test error than standard stochastic gradient descent. Our approach is based on the principles of cross-validation, where a validation set is used to limit the model overfitting. We formulate such principles as a bilevel optimization problem. This formulation allows us to define the optimization of a cost on the validation set subject to another optimization on the training set. The overfitting is controlled by introducing weights on each mini-batch in the training set and by choosing their values so that they minimize the error on the validation set. In practice, these weights define mini-batch learning rates in a gradient descent update equation that favor gradients with better generalization capabilities. Because of its simplicity, this approach can be integrated with other regularization methods and training schemes. We evaluate extensively our proposed algorithm on several neural network architectures and datasets, and find that it consistently improves the generalization of the model, especially when labels are noisy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_38');
INSERT INTO `paper` VALUES (10525, 'Deep Bilinear Learning for RGB-D Action Recognition', 'Deep bilinear', 'RGB-D action', 'Feature learning', 'Cube', '', 'In this paper, we focus on exploring modality-temporal mutual information for RGB-D action recognition. In order to learn time-varying information and multi-modal features jointly, we propose a novel deep bilinear learning framework. In the framework, we propose bilinear blocks that consist of two linear pooling layers for pooling the input cube features from both modality and temporal directions, separately. To capture rich modality-temporal information and facilitate our deep bilinear learning, a new action feature called modality-temporal cube is presented in a tensor structure for characterizing RGB-D actions from a comprehensive perspective. Our method is extensively tested on two public datasets with four different evaluation settings, and the results show that the proposed method outperforms the state-of-the-art approaches.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_21');
INSERT INTO `paper` VALUES (10526, 'Deep Boosting for Image Denoising', 'Color Image Denoising', 'Dilated Convolution', 'Boosting Algorithm', 'Connection Density', 'Model-based Learning', 'Boosting is a classic algorithm which has been successfully applied to diverse computer vision tasks. In the scenario of image denoising, however, the existing boosting algorithms are surpassed by the emerging learning-based models. In this paper, we propose a novel deep boosting framework (DBF) for denoising, which integrates several convolutional networks in a feed-forward fashion. Along with the integrated networks, however, the depth of the boosting framework is substantially increased, which brings difficulty to training. To solve this problem, we introduce the concept of dense connection that overcomes the vanishing of gradients during training. Furthermore, we propose a path-widening fusion scheme cooperated with the dilated convolution to derive a lightweight yet efficient convolutional network as the boosting unit, named Dilated Dense Fusion Network (DDFN). Comprehensive experiments demonstrate that our DBF outperforms existing methods on widely used benchmarks, in terms of different denoising tasks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_1');
INSERT INTO `paper` VALUES (10527, 'Deep Burst Denoising', '', '', '', '', '', 'Noise is an inherent issue of low-light image capture, which is worsened on mobile devices due to their narrow apertures and small sensors. One strategy for mitigating noise in low-light situations is to increase the shutter time, allowing each photosite to integrate more light and decrease noise variance. However, there are two downsides of long exposures: (a) bright regions can exceed the sensor range, and (b) camera and scene motion will cause blur. Another way of gathering more light is to capture multiple short (thus noisy) frames in a burst and intelligently integrate the content, thus avoiding the above downsides. In this paper, we use the burst-capture strategy and implement the intelligent integration via a recurrent fully convolutional deep neural net (CNN). We build our novel, multi-frame architecture to be a simple addition to any single frame denoising model. The resulting architecture denoises all frames in a sequence of arbitrary length. We show that it achieves state of the art denoising results on our burst dataset, improving on the best published multi-frame techniques, such as VBM4D and FlexISP. Finally, we explore other applications of multi-frame image enhancement and show that our CNN architecture generalizes well to image super-resolution.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_33');
INSERT INTO `paper` VALUES (10528, 'Deep Clustering for Unsupervised Learning of Visual Features', 'Unsupervised learning', 'Clustering', '', '', '', 'Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large-scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_9');
INSERT INTO `paper` VALUES (10529, 'Deep Co-Training for Semi-Supervised Image Recognition', 'Co-Training', 'Deep networks', 'Semi-supervised learning', '', '', 'In this paper, we study the problem of semi-supervised image recognition, which is to learn classifiers using both labeled and unlabeled images. We present Deep Co-Training, a deep learning based method inspired by the Co-Training framework. The original Co-Training learns two classifiers on two views which are data from different sources that describe the same instances. To extend this concept to deep learning, Deep Co-Training trains multiple deep neural networks to be the different views and exploits adversarial examples to encourage view difference, in order to prevent the networks from collapsing into each other. As a result, the co-trained networks provide different and complementary information about the data, which is necessary for the Co-Training framework to achieve good results. We test our method on SVHN, CIFAR-10/100 and ImageNet datasets, and our method outperforms the previous state-of-the-art methods by a large margin.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_9');
INSERT INTO `paper` VALUES (10530, 'Deep Component Analysis via Alternating Direction Neural Networks', 'Component analysis', 'Deep learning', 'Constraints', '', '', 'Despite a lack of theoretical understanding, deep neural networks have achieved unparalleled performance in a wide range of applications. On the other hand, shallow representation learning with component analysis is associated with rich intuition and theory, but smaller capacity often limits its usefulness. To bridge this gap, we introduce Deep Component Analysis (DeepCA), an expressive multilayer model formulation that enforces hierarchical structure through constraints on latent variables in each layer. For inference, we propose a differentiable optimization algorithm implemented using recurrent Alternating Direction Neural Networks (ADNNs) that enable parameter learning using standard backpropagation. By interpreting feed-forward networks as single-iteration approximations of inference in our model, we provide both a novel perspective for understanding them and a practical technique for constraining predictions with prior knowledge. Experimentally, we demonstrate performance improvements on a variety of tasks, including single-image depth prediction with sparse output constraints.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_50');
INSERT INTO `paper` VALUES (10531, 'Deep Continuous Fusion for Multi-sensor 3D Object Detection', '3D object detection', 'Multi-sensor fusion', 'Autonomous driving', '', '', 'In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_39');
INSERT INTO `paper` VALUES (10532, 'Deep Convolutional Neural Networks Based Framework for Estimation of Stomata Density and Structure from Microscopic Images', 'High-throughput phenotyping', 'Deep convolutional neural networks', 'Stomata counting', 'Stomata quantification', '', 'Analysis of stomata density and its configuration based on scanning electron microscopic (SEM) image of a leaf surface, is an effective way to characterize the plant’s behaviour under various environmental stresses (drought, salinity etc.). Existing methods for phenotyping these stomatal traits are often based on manual or semi-automatic labeling and segmentation of SEM images. This is a low-throughput process when large number of SEM images is investigated for statistical analysis. To overcome this limitation, we propose a novel automated pipeline leveraging deep convolutional neural networks for stomata detection and its quantification. The proposed framework shows a superior performance in contrast to the existing stomata detection methods in terms of precision and recall, 0.91 and 0.89 respectively. Furthermore, the morphological traits (i.e. length & width) obtained at stomata quantification step shows a correlation of 0.95 and 0.91 with manually computed traits, resulting in an efficient and high-throughput solution for stomata phenotyping.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_31');
INSERT INTO `paper` VALUES (10533, 'Deep Cross-Modal Projection Learning for Image-Text Matching', 'Image-text matching', 'Cross-modal projection', 'Joint embedding learning', 'Deep learning', '', 'The key point of image-text matching is how to accurately measure the similarity between visual and textual inputs. Despite the great progress of associating the deep cross-modal embeddings with the bi-directional ranking loss, developing the strategies for mining useful triplets and selecting appropriate margins remains a challenge in real applications. In this paper, we propose a cross-modal projection matching (CMPM) loss and a cross-modal projection classification (CMPC) loss for learning discriminative image-text embeddings. The CMPM loss minimizes the KL divergence between the projection compatibility distributions and the normalized matching distributions defined with all the positive and negative samples in a mini-batch. The CMPC loss attempts to categorize the vector projection of representations from one modality onto another with the improved norm-softmax loss, for further enhancing the feature compactness of each class. Extensive analysis and experiments on multiple datasets demonstrate the superiority of the proposed approach.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_42');
INSERT INTO `paper` VALUES (10534, 'Deep Cross-Modality Adaptation via Semantics Preserving Adversarial Learning for Sketch-Based 3D Shape Retrieval', 'Sketch-based 3D shape retrieval', 'Cross-modality transformation', 'Adversarial learning', 'Importance-aware metric learning', '', 'Due to the large cross-modality discrepancy between 2D sketches and 3D shapes, retrieving 3D shapes by sketches is a significantly challenging task. To address this problem, we propose a novel framework to learn a discriminative deep cross-modality adaptation model in this paper. Specifically, we first separately adopt two metric networks, following two deep convolutional neural networks (CNNs), to learn modality-specific discriminative features based on an importance-aware metric learning method. Subsequently, we explicitly introduce a cross-modality transformation network to compensate for the divergence between two modalities, which can transfer features of 2D sketches to the feature space of 3D shapes. We develop an adversarial learning based method to train the transformation model, by simultaneously enhancing the holistic correlations between data distributions of two modalities, and mitigating the local semantic divergences through minimizing a cross-modality mean discrepancy term. Experimental results on the SHREC 2013 and SHREC 2014 datasets clearly show the superior retrieval performance of our proposed model, compared to the state-of-the-art approaches.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_37');
INSERT INTO `paper` VALUES (10535, 'Deep Depth from Defocus: How Can Defocus Blur Improve 3D Estimation Using Dense Neural Networks?', 'Depth from defocus', 'Domain adaptation', 'Depth estimation', 'Single-image depth prediction', '', 'Depth estimation is critical interest for scene understanding and accurate 3D reconstruction. Most recent approaches with deep learning exploit geometrical structures of standard sharp images to predict depth maps. However, cameras can also produce images with defocus blur depending on the depth of the objects and camera settings. Hence, these features may represent an important hint for learning to predict depth. In this paper, we propose a full system for single-image depth prediction in the wild using depth-from-defocus and neural networks. We carry out thorough experiments real and simulated defocused images using a realistic model of blur variation with respect to depth. We also investigate the influence of blur on depth prediction observing model uncertainty with a Bayesian neural network approach. From these studies, we show that out-of-focus blur greatly improves the depth-prediction network performances. Furthermore, we transfer the ability learned on a synthetic, indoor dataset to real, indoor and outdoor images. For this purpose, we present a new dataset with real all-focus and defocused images from a DSLR camera, paired with ground truth depth maps obtained with an active 3D sensor for indoor scenes. The proposed approach is successfully validated on both this new dataset and standard ones as NYUv2 or Depth-in-the-Wild. Code and new datasets are available at https://github.com/marcelampc/d3net_depth_estimation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_18');
INSERT INTO `paper` VALUES (10536, 'Deep Directional Statistics: Pose Estimation with Uncertainty Quantification', 'Pose estimation', 'Deep probabilistic models', 'Uncertainty quantification', 'Directional statistics', '', 'Modern deep learning systems successfully solve many perception tasks such as object pose estimation when the input image is of high quality. However, in challenging imaging conditions such as on low resolution images or when the image is corrupted by imaging artifacts, current systems degrade considerably in accuracy. While a loss in performance is unavoidable, we would like our models to quantify their uncertainty to achieve robustness against images of varying quality. Probabilistic deep learning models combine the expressive power of deep learning with uncertainty quantification. In this paper we propose a novel probabilistic deep learning model for the task of angular regression. Our model uses von Mises distributions to predict a distribution over object pose angle. Whereas a single von Mises distribution is making strong assumptions about the shape of the distribution, we extend the basic model to predict a mixture of von Mises distributions. We show how to learn a mixture model using a finite and infinite number of mixture components. Our model allows for likelihood-based training and efficient inference at test time. We demonstrate on a number of challenging pose estimation datasets that our model produces calibrated probability predictions and competitive or superior point estimates compared to the current state-of-the-art.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_33');
INSERT INTO `paper` VALUES (10537, 'Deep Discriminative Model for Video Classification', '', '', '', '', '', 'This paper presents a new deep learning approach for video-based scene classification. We design a Heterogeneous Deep Discriminative Model (HDDM) whose parameters are initialized by performing an unsupervised pre-training in a layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBM). In order to avoid the redundancy of adjacent frames, we extract spatiotemporal variation patterns within frames and represent them sparsely using Sparse Cubic Symmetrical Pattern (SCSP). Then, a pre-initialized HDDM is separately trained using the videos of each class to learn class-specific models. According to the minimum reconstruction error from the learnt class-specific models, a weighted voting strategy is employed for the classification. The performance of the proposed method is extensively evaluated on two action recognition datasets; UCF101 and Hollywood II, and three dynamic texture and dynamic scene datasets; DynTex, YUPENN, and Maryland. The experimental results and comparisons against state-of-the-art methods demonstrate that the proposed method consistently achieves superior performance on all datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_24');
INSERT INTO `paper` VALUES (10538, 'Deep Domain Generalization via Conditional Invariant Adversarial Networks', 'Domain generalization', 'Adversarial networks', 'Domain invariant representation', '', '', 'Domain generalization aims to learn a classification model from multiple source domains and generalize it to unseen target domains. A critical problem in domain generalization involves learning domain-invariant representations. Let X and Y denote the features and the labels, respectively. Under the assumption that the conditional distribution P(Y|X) remains unchanged across domains, earlier approaches to domain generalization learned the invariant representation T(X) by minimizing the discrepancy of the marginal distribution P(T(X)). However, such an assumption of stable P(Y|X) does not necessarily hold in practice. In addition, the representation learning function T(X) is usually constrained to a simple linear transformation or shallow networks. To address the above two drawbacks, we propose an end-to-end conditional invariant deep domain generalization approach by leveraging deep neural networks for domain-invariant representation learning. The domain-invariance property is guaranteed through a conditional invariant adversarial network that can learn domain-invariant representations w.r.t. the joint distribution P(T(X), Y) if the target domain data are not severely class unbalanced. We perform various experiments to demonstrate the effectiveness of the proposed method.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_38');
INSERT INTO `paper` VALUES (10539, 'Deep Execution Monitor for Robot Assistive Tasks', '', '', '', '', '', 'We consider a novel approach to high-level robot task execution for a robot assistive task. In this work we explore the problem of learning to predict the next subtask by introducing a deep model for both sequencing goals and for visually evaluating the state of a task. We show that deep learning for monitoring robot tasks execution very well supports the interconnection between task-level planning and robot operations. These solutions can also cope with the natural non-determinism of the execution monitor. We show that a deep execution monitor leverages robot performance. We measure the improvement taking into account some robot helping tasks performed at a warehouse.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_11');
INSERT INTO `paper` VALUES (10540, 'Deep Expander Networks: Efficient Deep Networks from Graph Theory', '', '', '', '', '', 'Efficient CNN designs like ResNets and DenseNet were proposed to improve accuracy vs efficiency trade-offs. They essentially increased the connectivity, allowing efficient information flow across layers. Inspired by these techniques, we propose to model connections between filters of a CNN using graphs which are simultaneously sparse and well connected. Sparsity results in efficiency while well connectedness can preserve the expressive power of the CNNs. We use a well-studied class of graphs from theoretical computer science that satisfies these properties known as Expander graphs. Expander graphs are used to model connections between filters in CNNs to design networks called X-Nets. We present two guarantees on the connectivity of X-Nets: Each node influences every node in a layer in logarithmic steps, and the number of paths between two sets of nodes is proportional to the product of their sizes. We also propose efficient training and inference algorithms, making it possible to train deeper and wider X-Nets effectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_2');
INSERT INTO `paper` VALUES (10541, 'Deep Factorised Inverse-Sketching', 'Sketch-based Image Retrieval (SBIR)', 'Style Transfer', 'Free-hand Sketches', 'Consistent Embedding', 'Exposure Contours', 'Modelling human free-hand sketches has become topical recently, driven by practical applications such as fine-grained sketch based image retrieval (FG-SBIR). Sketches are clearly related to photo edge-maps, but a human free-hand sketch of a photo is not simply a clean rendering of that photo’s edge map. Instead there is a fundamental process of abstraction and iconic rendering, where overall geometry is warped and salient details are selectively included. In this paper we study this sketching process and attempt to invert it. We model this inversion by translating iconic free-hand sketches to contours that resemble more geometrically realistic projections of object boundaries, and separately factorise out the salient added details. This factorised re-representation makes it easier to match a free-hand sketch to a photo instance of an object. Specifically, we propose a novel unsupervised image style transfer model based on enforcing a cyclic embedding consistency constraint. A deep FG-SBIR model is then formulated to accommodate complementary discriminative detail from each factorised sketch for better matching with the corresponding photo. Our method is evaluated both qualitatively and quantitatively to demonstrate its superiority over a number of state-of-the-art alternatives for style transfer and FG-SBIR.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_3');
INSERT INTO `paper` VALUES (10542, 'Deep Fashion Analysis with Feature Map Upsampling and Landmark-Driven Attention', 'Fashion analysis', 'Landmark detection', 'Clothing category classification', 'Attention mechanism', 'Deep learning', 'In this paper, we propose an attentive fashion network to address three problems of fashion analysis, namely landmark localization, category classification and attribute prediction. By utilizing a landmark prediction branch with upsampling network structure, we boost the accuracy of fashion landmark localization. With the aid of the predicted landmarks, a landmark-driven attention mechanism is proposed to help improve the precision of fashion category classification and attribute prediction. Experimental results show that our approach outperforms the state-of-the-arts on the DeepFashion dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_4');
INSERT INTO `paper` VALUES (10543, 'Deep Feature Factorization for Concept Discovery', 'Neural network interpretability', 'Part co-segmentation', 'Co-segmentation', 'Co-localization', 'Non-negative matrix factorization', 'We propose Deep Feature Factorization (DFF), a method capable of localizing similar semantic concepts within an image or a set of images. We use DFF to gain insight into a deep convolutional neural network’s learned features, where we detect hierarchical cluster structures in feature space. This is visualized as heat maps, which highlight semantically matching regions across a set of images, revealing what the network ‘perceives’ as similar. DFF can also be used to perform co-segmentation and co-localization, and we report state-of-the-art results on these tasks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_21');
INSERT INTO `paper` VALUES (10544, 'Deep Feature Pyramid Reconfiguration for Object Detection', 'Object detection', 'Feature pyramids', 'Global-local reconfiguration', '', '', 'State-of-the-art object detectors usually learn multi-scale representations to get better results by employing feature pyramids. However, the current designs for feature pyramids are still inefficient to integrate the semantic information over different scales. In this paper, we begin by investigating current feature pyramids solutions, and then reformulate the feature pyramid construction as the feature reconfiguration process. Finally, we propose a novel reconfiguration architecture to combine low-level representations with high-level semantic features in a highly-nonlinear yet efficient way. In particular, our architecture which consists of global attention and local reconfigurations, is able to gather task-oriented features across different spatial locations and scales, globally and locally. Both the global attention and local reconfiguration are lightweight, in-place, and end-to-end trainable. Using this method in the basic SSD system, our models achieve consistent and significant boosts compared with the original model and its other variations, without losing real-time processing speed.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_11');
INSERT INTO `paper` VALUES (10545, 'Deep Fundamental Matrix Estimation Without Correspondences', 'Fundamental matrix', 'Epipolar geometry', 'Deep learning', 'Stereo', '', 'Estimating fundamental matrices is a classic problem in computer vision. Traditional methods rely heavily on the correctness of estimated key-point correspondences, which can be noisy and unreliable. As a result, it is difficult for these methods to handle image pairs with large occlusion or significantly different camera poses. In this paper, we propose novel neural network architectures to estimate fundamental matrices in an end-to-end manner without relying on point correspondences. New modules and layers are introduced in order to preserve mathematical properties of the fundamental matrix as a homogeneous rank-2 matrix with seven degrees of freedom. We analyze performance of the proposed models using various metrics on the KITTI dataset, and show that they achieve competitive performance with traditional methods without the need for extracting correspondences.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_35');
INSERT INTO `paper` VALUES (10546, 'Deep Fundamental Matrix Estimation', '', '', '', '', '', 'We present an approach to robust estimation of fundamental matrices from noisy data contaminated by outliers. The problem is cast as a series of weighted homogeneous least-squares problems, where robust weights are estimated using deep networks. The presented formulation acts directly on putative correspondences and thus fits into standard 3D vision pipelines that perform feature extraction, matching, and model fitting. The approach can be trained end-to-end and yields computationally efficient robust estimators. Our experiments indicate that the presented approach is able to train robust estimators that outperform classic approaches on real data by a significant margin.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_18');
INSERT INTO `paper` VALUES (10547, 'Deep Fusion Network for Splicing Forgery Localization', 'Image forensics', 'Splicing forgery detection', 'Forgery localization', 'Deep convolutional network', 'Fusion network', 'Digital splicing is a common type of image forgery: some regions of an image are replaced with contents from other images. To locate altered regions in a tampered picture is a challenging work because the difference is unknown between the altered regions and the original regions and it is thus necessary to search the large hypothesis space for a convincing result. In this paper, we proposed a novel deep fusion network to locate tampered area by tracing its border. A group of deep convolutional neural networks called Base-Net were firstly trained to response the certain type of splicing forgery respectively. Then, some layers of the Base-Net are selected and combined as a deep fusion neural network (Fusion-Net). After fine-tuning by a very small number of pictures, Fusion-Net is able to discern whether an image block is synthesized from different origins. Experiments on the benchmark datasets show that our method is effective in various situations and outperform state-of-the-art methods.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_21');
INSERT INTO `paper` VALUES (10548, 'Deep Generative Models for Weakly-Supervised Multi-Label Classification', 'Multi-label classification', 'Generative models', 'Semi-supervised learning', 'Weakly-supervised learning', '', 'In order to train learning models for multi-label classification (MLC), it is typically desirable to have a large amount of fully annotated multi-label data. Since such annotation process is in general costly, we focus on the learning task of weakly-supervised multi-label classification (WS-MLC). In this paper, we tackle WS-MLC by learning deep generative models for describing the collected data. In particular, we introduce a sequential network architecture for constructing our generative model with the ability to approximate observed data posterior distributions. We show that how information of training data with missing labels or unlabeled ones can be exploited, which allows us to learn multi-label classifiers via scalable variational inferences. Empirical studies on various scales of datasets demonstrate the effectiveness of our proposed model, which performs favorably against state-of-the-art MLC algorithms.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_25');
INSERT INTO `paper` VALUES (10549, 'Deep High Dynamic Range Imaging with Large Foreground Motions', 'High dynamic range imaging', 'Computational photography', '', '', '', 'This paper proposes the first non-flow-based deep framework for high dynamic range (HDR) imaging of dynamic scenes with large-scale foreground motions. In state-of-the-art deep HDR imaging, input images are first aligned using optical flows before merging, which are still error-prone due to occlusion and large motions. In stark contrast to flow-based methods, we formulate HDR imaging as an image translation problem without optical flows. Moreover, our simple translation network can automatically hallucinate plausible HDR details in the presence of total occlusion, saturation and under-exposure, which are otherwise almost impossible to recover by conventional optimization approaches. Our framework can also be extended for different reference images. We performed extensive qualitative and quantitative comparisons to show that our approach produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art methods, and is robust across various inputs, including images without radiometric calibration.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_8');
INSERT INTO `paper` VALUES (10550, 'Deep Image Demosaicking Using a Cascade of Convolutional Residual Denoising Networks', 'Deep learning', 'Denoising', 'Demosaicking', 'Proximal method', 'Residual denoising', 'Demosaicking and denoising are among the most crucial steps of modern digital camera pipelines and their joint treatment is a highly ill-posed inverse problem where at-least two-thirds of the information are missing and the rest are corrupted by noise. This poses a great challenge in obtaining meaningful reconstructions and a special care for the efficient treatment of the problem is required. While there are several machine learning approaches that have been recently introduced to deal with joint image demosaicking-denoising, in this work we propose a novel deep learning architecture which is inspired by powerful classical image regularization methods and large-scale convex optimization techniques. Consequently, our derived network is more transparent and has a clear interpretation compared to alternative competitive deep learning approaches. Our extensive experiments demonstrate that our network outperforms any previous approaches on both noisy and noise-free data. This improvement in reconstruction quality is attributed to the principled way we design our network architecture, which also requires fewer trainable parameters than the current state-of-the-art deep network solution. Finally, we show that our network has the ability to generalize well even when it is trained on small datasets, while keeping the overall number of trainable parameters low.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_19');
INSERT INTO `paper` VALUES (10551, 'Deep Imbalanced Attribute Classification Using Visual Attention Aggregation', 'Visual attributes', 'Deep imbalanced learning', 'Visual attention', '', '', 'For many computer vision applications, such as image description and human identification, recognizing the visual attributes of humans is an essential yet challenging problem. Its challenges originate from its multi-label nature, the large underlying class imbalance and the lack of spatial annotations. Existing methods follow either a computer vision approach while failing to account for class imbalance, or explore machine learning solutions, which disregard the spatial and semantic relations that exist in the images. With that in mind, we propose an effective method that extracts and aggregates visual attention masks at different scales. We introduce a loss function to handle class imbalance both at class and at an instance level and further demonstrate that penalizing attention masks with high prediction variance accounts for the weak supervision of the attention mechanism. By identifying and addressing these challenges, we achieve state-of-the-art results with a simple attention mechanism in both PETA and WIDER-Attribute datasets without additional context or side information.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_42');
INSERT INTO `paper` VALUES (10552, 'Deep Kalman Filtering Network for Video Compression Artifact Reduction', 'Compression artifact reduction', 'Deep neural network', 'Kalman model', 'Recursive filtering', 'Video restoration', 'When lossy video compression algorithms are applied, compression artifacts often appear in videos, making decoded videos unpleasant for human visual systems. In this paper, we model the video artifact reduction task as a Kalman filtering procedure and restore decoded frames through a deep Kalman filtering network. Different from the existing works using the noisy previous decoded frames as temporal information in the restoration problem, we utilize the less noisy previous restored frame and build a recursive filtering scheme based on the Kalman model. This strategy can provide more accurate and consistent temporal information, which produces higher quality restoration results. In addition, the strong prior information of prediction residual is also exploited for restoration through a well designed neural network. These two components are combined under the Kalman framework and optimized through the deep Kalman filtering network. Our approach can well bridge the gap between the model-based methods and learning-based methods by integrating the recursive nature of the Kalman model and highly non-linear transformation ability of deep neural network. Experimental results on the benchmark dataset demonstrate the effectiveness of our proposed method.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_35');
INSERT INTO `paper` VALUES (10553, 'Deep Learning for Assistive Computer Vision', 'Assistive technologies', 'Computer vision', 'Deep learning', '', '', 'This paper revises the main advances in assistive computer vision recently fostered by deep learning. To this aim, we first discuss how the application of deep learning in computer vision has contributed to the development of assistive techinologies, then analyze the recent advances in assistive technologies achieved in five main areas, namely, object classification and localization, scene understanding, human pose estimation and tracking, action/event recognition and anticipation. The paper is concluded with a discussion and insights for future directions.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_1');
INSERT INTO `paper` VALUES (10554, 'Deep Learning for Automated Tagging of Fashion Images', 'Deep learning', 'Image recognition', 'Fashion attributes', '', '', 'We present 9 deep learning classifiers to predict Fashion attributes in 4 different categories: apparel (dresses and tops), shoes, watches and luggages. Our prediction system hosts several classifiers working at scale to populate a catalogue of millions of products. We provide details of our models as well as the challenges involved in predicting Fashion attributes in a relatively homogeneous problem space.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_1');
INSERT INTO `paper` VALUES (10555, 'Deep Learning for Multi-path Error Removal in ToF Sensors', 'ToF sensors', 'Denoising', 'Multi-path interference', 'Depth acquisition', 'Convolutional Neural Networks', 'The removal of Multi-Path Interference (MPI) is one of the major open challenges in depth estimation with Time-of-Flight (ToF) cameras. In this paper we propose a novel method for MPI removal and depth refinement exploiting an ad-hoc deep learning architecture working on data from a multi-frequency ToF camera. In order to estimate the MPI we use a Convolutional Neural Network (CNN) made of two sub-networks: a coarse network analyzing the global structure of the data at a lower resolution and a fine one exploiting the output of the coarse network in order to remove the MPI while preserving the small details. The critical issue of the lack of ToF data with ground truth is solved by training the CNN with synthetic information. Finally, the residual zero-mean error is removed with an adaptive bilateral filter guided from a noise model for the camera. Experimental results prove the effectiveness of the proposed approach on both synthetic and real data.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_30');
INSERT INTO `paper` VALUES (10556, 'Deep Learning of Appearance Models for Online Object Tracking', '', '', '', '', '', 'This paper introduces a deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. An online fine-tuning step is carried out at every frame to learn the appearance of the target. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_57');
INSERT INTO `paper` VALUES (10557, 'Deep Metric Learning with Hierarchical Triplet Loss', 'Deep metric learning', 'Image retrieval', 'Triplet loss', 'Anchor-neighbor sampling', '', 'We present a novel hierarchical triplet loss (HTL) capable of automatically collecting informative training samples (triplets) via a defined hierarchical tree that encodes global context information. This allows us to cope with the main limitation of random sampling in training a conventional triplet loss, which is a central issue for deep metric learning. Our main contributions are two-fold. (i) we construct a hierarchical class-level tree where neighboring classes are merged recursively. The hierarchical structure naturally captures the intrinsic data distribution over the whole dataset. (ii) we formulate the problem of triplet collection by introducing a new violate margin, which is computed dynamically based on the designed hierarchical tree. This allows it to automatically select meaningful hard samples with the guide of global context. It encourages the model to learn more discriminative features from visual similar classes, leading to faster convergence and better performance. Our method is evaluated on the tasks of image retrieval and face recognition, where it outperforms the standard triplet loss substantially by 1%–18%, and achieves new state-of-the-art performance on a number of benchmarks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_17');
INSERT INTO `paper` VALUES (10558, 'Deep Model-Based 6D Pose Refinement in RGB', 'Pose estimation', 'Pose refinement', 'Tracking', '', '', 'We present a novel approach for model-based 6D pose refinement in color data. Building on the established idea of contour-based pose tracking, we teach a deep neural network to predict a translational and rotational update. At the core, we propose a new visual loss that drives the pose update by aligning object contours, thus avoiding the definition of any explicit appearance model. In contrast to previous work our method is correspondence-free, segmentation-free, can handle occlusion and is agnostic to geometrical symmetry as well as visual ambiguities. Additionally, we observe a strong robustness towards rough initialization. The approach can run in real-time and produces pose accuracies that come close to 3D ICP without the need for depth data. Furthermore, our networks are trained from purely synthetic data and will be published together with the refinement code at http://campar.in.tum.de/Main/FabianManhardt to ensure reproducibility.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_49');
INSERT INTO `paper` VALUES (10559, 'Deep Modular Network Architecture for Depth Estimation from Single Indoor Images', 'Depth estimation', 'Convolutional Neural Network', '', '', '', 'We propose a novel deep modular network architecture for indoor scene depth estimation from single RGB images. The proposed architecture consists of a main depth estimation network and two auxiliary semantic segmentation networks. Our insight is that semantic and geometrical structures in a scene are strongly correlated, thus we utilize global (i.e. room layout) and mid-level (i.e. objects in a room) semantic structures to enhance depth estimation. The first auxiliary network, or layout network, is responsible for room layout estimation to infer the positions of walls, floor, and ceiling of a room. The second auxiliary network, or object network, estimates per-pixel class labels of the objects in a scene, such as furniture, to give mid-level semantic cues. Estimated semantic structures are effectively fed into the depth estimation network using newly proposed discriminator networks, which discern the reliability of the estimated structures. The evaluation result shows that our architecture achieves significant performance improvements over previous approaches on the standard NYU Depth v2 indoor scene dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_19');
INSERT INTO `paper` VALUES (10560, 'Deep Multi-task Learning to Recognise Subtle Facial Expressions of Mental States', '', '', '', '', '', 'Facial expression recognition is a topical task. However, very little research investigates subtle expression recognition, which is important for mental activity analysis, deception detection, etc. We address subtle expression recognition through convolutional neural networks (CNNs) by developing multi-task learning (MTL) methods to effectively leverage a side task: facial landmark detection. Existing MTL methods follow a design pattern of shared bottom CNN layers and task-specific top layers. However, the sharing architecture is usually heuristically chosen, as it is difficult to decide which layers should be shared. Our approach is composed of (1) a novel MTL framework that automatically learns which layers to share through optimisation under tensor trace norm regularisation and (2) an invariant representation learning approach that allows the CNN to leverage tasks defined on disjoint datasets without suffering from dataset distribution shift. To advance subtle expression recognition, we contribute a Large-scale Subtle Emotions and Mental States in the Wild database (LSEMSW). LSEMSW includes a variety of cognitive states as well as basic emotions. It contains 176K images, manually annotated with 13 emotions, and thus provides the first subtle expression dataset large enough for training deep CNNs. Evaluations on LSEMSW and 300-W (landmark) databases show the effectiveness of the proposed methods. In addition, we investigate transferring knowledge learned from LSEMSW database to traditional (non-subtle) expression recognition. We achieve very competitive performance on Oulu-Casia NIR&Vis and CK+ databases via transfer learning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_7');
INSERT INTO `paper` VALUES (10561, 'Deep Multitask Gaze Estimation with a Constrained Landmark-Gaze Model', '', '', '', '', '', 'As an indicator of attention, gaze is an important cue for human behavior and social interaction analysis. Recent deep learning methods for gaze estimation rely on plain regression of the gaze from images without accounting for potential mismatches in eye image cropping and normalization. This may impact the estimation of the implicit relation between visual cues and the gaze direction when dealing with low resolution images or when training with a limited amount of data. In this paper, we propose a deep multitask framework for gaze estimation, with the following contributions. (i) we proposed a multitask framework which relies on both synthetic data and real data for end-to-end training. During training, each dataset provides the label of only one task but the two tasks are combined in a constrained way. (ii) we introduce a Constrained Landmark-Gaze Model (CLGM) modeling the joint variation of eye landmark locations (including the iris center) and gaze directions. By relating explicitly visual information (landmarks) to the more abstract gaze values, we demonstrate that the estimator is more accurate and easier to learn. (iii) by decomposing our deep network into a network inferring jointly the parameters of the CLGM model and the scale and translation parameters of eye regions on one hand, and a CLGM based decoder deterministically inferring landmark positions and gaze from these parameters and head pose on the other hand, our framework decouples gaze estimation from irrelevant geometric variations in the eye image (scale, translation), resulting in a more robust model. Thorough experiments on public datasets demonstrate that our method achieves competitive results, improving over state-of-the-art results in challenging free head pose gaze estimation tasks and on eye landmark localization (iris location) ones.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_35');
INSERT INTO `paper` VALUES (10562, 'Deep Networks for Image-to-Image Translation with Mux and Demux Layers', 'Mux layer', 'Demux layer', 'Image enhancement', 'Deep learning', '', 'Image processing methods using deep convolutional networks have achieved great successes on quantitative and qualitative assessments in many tasks, such as super–resolution, style transfer and enhancement. Most of these solutions use many layers, many filters and complex architectures. It is difficult to implement them on mobile devices, e.g. smart phones, because of the limited resources. Many applications need to deploy these methods on mobile devices. But it is difficult because of limited resources. In this paper we present a lightweight end–to–end deep learning approach for image enhancement. To improve the performance, we present mux layer and demux layers, which could perform up–sampling and down–sampling by shuffling the pixels without losing any information of feature maps. For further higher performance, denseblocks are used in the models. To ensure the consistency of the output and input, we use weighted L1 loss to increase PSNR. To improve image quality, we use adversarial loss, contextual loss and perceptual loss as parts of the objective functions during training. And NIQE is used for validation to get the best parameters for perceptual quality. Experiments show that, compared to the state–of–the–art, our method could improve both the quantitative and qualitative assessments, as well as the performance. With this system, we get the third place in PIRM Enhancement–On–Smartphones Challenge 2018 (PIRM–EoS Challenge 2018).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_10');
INSERT INTO `paper` VALUES (10563, 'Deep Normal Estimation for Automatic Shading of Hand-Drawn Characters', 'Cartoons', 'Non-photorealistic rendering', 'Normal estimation', 'Deep learning', '', 'We present a new fully automatic pipeline for generating shading effects on hand-drawn characters. Our method takes as input a single digitized sketch of any resolution and outputs a dense normal map estimation suitable for rendering without requiring any human input. At the heart of our method lies a deep residual, encoder-decoder convolutional network. The input sketch is first sampled using several equally sized 3-channel windows, with each window capturing a local area of interest at 3 different scales. Each window is then passed through the previously trained network for normal estimation. Finally, network outputs are arranged together to form a full-size normal map of the input sketch. We also present an efficient and effective way to generate a rich set of training data. Resulting renders offer a rich quality without any effort from the 2D artist. We show both quantitative and qualitative results demonstrating the effectiveness and quality of our network and method.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_20');
INSERT INTO `paper` VALUES (10564, 'Deep Pictorial Gaze Estimation', 'Appearance-based gaze estimation', 'Eye tracking', '', '', '', 'Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_44');
INSERT INTO `paper` VALUES (10565, 'Deep Randomized Ensembles for Metric Learning', 'Embedding Function', 'VehicleID (VID)', 'Car Dataset', 'Cube Dataset', 'Image Embedding', 'Learning embedding functions, which map semantically related inputs to nearby locations in a feature space supports a variety of classification and information retrieval tasks. In this work, we propose a novel, generalizable and fast method to define a family of embedding functions that can be used as an ensemble to give improved results. Each embedding function is learned by randomly bagging the training labels into small subsets. We show experimentally that these embedding ensembles create effective embedding functions. The ensemble output defines a metric space that improves state of the art performance for image retrieval on CUB-200-2011, Cars-196, In-Shop Clothes Retrieval and VehicleID.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_44');
INSERT INTO `paper` VALUES (10566, 'Deep Recursive HDRI: Inverse Tone Mapping Using Generative Adversarial Networks', 'High dynamic range imaging', 'Inverse tone mapping', 'Image restoration', 'Computational photography', 'Generative adversarial network', 'High dynamic range images contain luminance information of the physical world and provide more realistic experience than conventional low dynamic range images. Because most images have a low dynamic range, recovering the lost dynamic range from a single low dynamic range image is still prevalent. We propose a novel method for restoring the lost dynamic range from a single low dynamic range image through a deep neural network. The proposed method is the first framework to create high dynamic range images based on the estimated multi-exposure stack using the conditional generative adversarial network structure. In this architecture, we train the network by setting an objective function that is a combination of L1 loss and generative adversarial network loss. In addition, this architecture has a simplified structure than the existing networks. In the experimental results, the proposed network generated a multi-exposure stack consisting of realistic images with varying exposure values while avoiding artifacts on public benchmarks, compared with the existing methods. In addition, both the multi-exposure stacks and high dynamic range images estimated by the proposed method are significantly similar to the ground truth than other state-of-the-art algorithms.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_37');
INSERT INTO `paper` VALUES (10567, 'Deep Regionlets for Object Detection', 'Object Detection', 'Deep Learning', 'Deep Regionlets', 'Spatial Transformation', '', 'In this paper, we propose a novel object detection framework named “Deep Regionlets” by establishing a bridge between deep neural networks and conventional detection schema for accurate generic object detection. Motivated by the abilities of regionlets for modeling object deformation and multiple aspect ratios, we incorporate regionlets into an end-to-end trainable deep learning framework. The deep regionlets framework consists of a region selection network and a deep regionlet learning module. Specifically, given a detection bounding box proposal, the region selection network provides guidance on where to select regions to learn the features from. The regionlet learning module focuses on local feature selection and transformation to alleviate local variations. To this end, we first realize non-rectangular region selection within the detection framework to accommodate variations in object appearance. Moreover, we design a “gating network” within the regionlet leaning module to enable soft regionlet selection and pooling. The Deep Regionlets framework is trained end-to-end without additional efforts. We perform ablation studies and conduct extensive experiments on the PASCAL VOC and Microsoft COCO datasets. The proposed framework outperforms state-of-the-art algorithms, such as RetinaNet and Mask R-CNN, even without additional segmentation labels.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_49');
INSERT INTO `paper` VALUES (10568, 'Deep Regression Tracking with Shrinkage Loss', 'Regression networks', 'Shrinkage loss', 'Object tracking', '', '', 'Regression trackers directly learn a mapping from regularly dense samples of target objects to soft labels, which are usually generated by a Gaussian function, to estimate target positions. Due to the potential for fast-tracking and easy implementation, regression trackers have recently received increasing attention. However, state-of-the-art deep regression trackers do not perform as well as discriminative correlation filters (DCFs) trackers. We identify the main bottleneck of training regression networks as extreme foreground-background data imbalance. To balance training data, we propose a novel shrinkage loss to penalize the importance of easy training data. Additionally, we apply residual connections to fuse multiple convolutional layers as well as their output response maps. Without bells and whistles, the proposed deep regression tracking method performs favorably against state-of-the-art trackers, especially in comparison with DCFs trackers, on five benchmark datasets including OTB-2013, OTB-2015, Temple-128, UAV-123 and VOT-2016.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_22');
INSERT INTO `paper` VALUES (10569, 'Deep Reinforcement Learning with Iterative Shift for Visual Tracking', 'Visual object tracking', 'Reinforcement learning', 'Actor-critic algorithm', '', '', 'Visual tracking is confronted by the dilemma to locate a target both accurately and efficiently, and make decisions online whether and how to adapt the appearance model or even restart tracking. In this paper, we propose a deep reinforcement learning with iterative shift (DRL-IS) method for single object tracking, where an actor-critic network is introduced to predict the iterative shifts of object bounding boxes, and evaluate the shifts to take actions on whether to update object models or re-initialize tracking. Since locating an object is achieved by an iterative shift process, rather than online classification on many sampled locations, the proposed method is robust to cope with large deformations and abrupt motion, and computationally efficient since finding a target takes up to 10 shifts. In offline training, the critic network guides to learn how to make decisions jointly on motion estimation and tracking status in an end-to-end manner. Experimental results on the OTB benchmarks with large deformation improve the tracking precision by 1.7% and runs about 5 times faster than the competing state-of-the-art methods.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_42');
INSERT INTO `paper` VALUES (10570, 'Deep Residual Attention Network for Spectral Image Super-Resolution', 'Spectral image', 'Super-resolution', 'Channel attention', '', '', 'Spectral imaging sensors often suffer from low spatial resolution, as there exists an essential tradeoff between the spectral and spatial resolutions that can be simultaneously achieved, especially when the temporal resolution needs to be retained. In this paper, we propose a novel deep residual attention network for the spatial super-resolution (SR) of spectral images. The proposed method extends the classic residual network by (1) directly using the 3D low-resolution (LR) spectral image as input instead of upsampling the 2D bandwise images separately, and (2) integrating the channel attention mechanism into the residual network. These two operations fully exploit the correlations across both the spectral and spatial dimensions and greatly promote the performance of spectral image SR. In addition, for the scenario when stereo pairs of LR spectral and high-resolution (HR) RGB measurements are available, we design a fusion framework based on the proposed network. The spatial resolution of the spectral input is enhanced in one branch, while the spectral resolution of the RGB input is enhanced in the other. These two branches are then fused together through the attention mechanism again to reconstruct the final HR spectral image, which achieves further improvement compared to using the single LR spectral input. Experimental results demonstrate the superiority of the proposed method over plain residual networks, and our method is one of the winning solutions in the PIRM 2018 Spectral Super-resolution Challenge.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_14');
INSERT INTO `paper` VALUES (10571, 'Deep Shape Matching', 'Shape matching', 'Cross-modal recognition and retrieval', '', '', '', 'We cast shape matching as metric learning with convolutional networks. We break the end-to-end process of image representation into two parts. Firstly, well established efficient methods are chosen to turn the images into edge maps. Secondly, the network is trained with edge maps of landmark images, which are automatically obtained by a structure-from-motion pipeline. The learned representation is evaluated on a range of different tasks, providing improvements on challenging cases of domain generalization, generic sketch-based image retrieval or its fine-grained counterpart. In contrast to other methods that learn a different model per task, object category, or domain, we use the same network throughout all our experiments, achieving state-of-the-art results in multiple benchmarks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_46');
INSERT INTO `paper` VALUES (10572, 'Deep Structure Inference Network for Facial Action Unit Recognition', 'Computer vision', 'Machine learning', 'Deep learning', 'Facial expression analysis', 'Facial action units', 'Facial expressions are combinations of basic components called Action Units (AU). Recognizing AUs is key for general facial expression analysis. Recently, efforts in automatic AU recognition have been dedicated to learning combinations of local features and to exploiting correlations between AUs. We propose a deep neural architecture that tackles both problems by combining learned local and global features in its initial stages and replicating a message passing algorithm between classes similar to a graphical model inference approach in later stages. We show that by training the model end-to-end with increased supervision we improve state-of-the-art by 5.3% and 8.2% performance on BP4D and DISFA datasets, respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_19');
INSERT INTO `paper` VALUES (10573, 'Deep Texture and Structure Aware Filtering Network for Image Smoothing', 'Image smoothing', 'Texture prediction', 'Deep learning', '', '', 'Image smoothing is a fundamental task in computer vision, that attempts to retain salient structures and remove insignificant textures. In this paper, we aim to address the fundamental shortcomings of existing image smoothing methods, which cannot properly distinguish textures and structures with similar low-level appearance. While deep learning approaches have started to explore structure preservation through image smoothing, existing work does not yet properly address textures. To this end, we generate a large dataset by blending natural textures with clean structure-only images, and use this to build a texture prediction network (TPN) that predicts the location and magnitude of textures. We then combine the TPN with a semantic structure prediction network (SPN) so that the final texture and structure aware filtering network (TSAFN) is able to identify the textures to remove (“texture-awareness”) and the structures to preserve (“structure-awareness”). The proposed model is easy to understand and implement, and shows good performance on real images in the wild as well as our generated dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_14');
INSERT INTO `paper` VALUES (10574, 'Deep Transfer Learning for Art Classification Problems', 'Deep Convolutional Neural Networks', 'Art classification', 'Transfer learning', 'Visual attention', '', 'In this paper we investigate whether Deep Convolutional Neural Networks (DCNNs), which have obtained state of the art results on the ImageNet challenge, are able to perform equally well on three different art classification problems. In particular, we assess whether it is beneficial to fine tune the networks instead of just using them as off the shelf feature extractors for a separately trained softmax classifier. Our experiments show how the first approach yields significantly better results and allows the DCNNs to develop new selective attention mechanisms over the images, which provide powerful insights about which pixel regions allow the networks successfully tackle the proposed classification challenges. Furthermore, we also show how DCNNs, which have been fine tuned on a large artistic collection, outperform the same architectures which are pre-trained on the ImageNet dataset only, when it comes to the classification of heritage objects from a different dataset.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_48');
INSERT INTO `paper` VALUES (10575, 'Deep Variational Metric Learning', 'Metric learning', 'Variational auto-encoder', 'Discriminative samples generating', '', '', 'Deep metric learning has been extensively explored recently, which trains a deep neural network to produce discriminative embedding features. Most existing methods usually enforce the model to be indiscriminating to intra-class variance, which makes the model over-fitting to the training set to minimize loss functions on these specific changes and leads to low generalization power on unseen classes. However, these methods ignore a fact that in the central latent space, the distribution of variance within classes is actually independent on classes. In this paper, we propose a deep variational metric learning (DVML) framework to explicitly model the intra-class variance and disentangle the intra-class invariance, namely, the class centers. With the learned distribution of intra-class variance, we can simultaneously generate discriminative samples to improve robustness. Our method is applicable to most of existing metric learning algorithms, and extensive experiments on three benchmark datasets including CUB-200-2011, Cars196 and Stanford Online Products show that our DVML significantly boosts the performance of currently popular deep metric learning methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_42');
INSERT INTO `paper` VALUES (10576, 'Deep Video Generation, Prediction and Completion of Human Action Sequences', 'Video generation', 'Generative models', '', '', '', 'Current video generation/prediction/completion results are limited, due to the severe ill-posedness inherent in these three problems. In this paper, we focus on human action videos, and propose a general, two-stage deep framework to generate human action videos with no constraints or arbitrary number of constraints, which uniformly addresses the three problems: video generation given no input frames, video prediction given the first few frames, and video completion given the first and last frames. To solve video generation from scratch, we build a two-stage framework where we first train a deep generative model that generates human pose sequences from random noise, and then train a skeleton-to-image network to synthesize human action videos given the human pose sequences generated. To solve video prediction and completion, we exploit our trained model and conduct optimization over the latent space to generate videos that best suit the given input frame constraints. With our novel method, we sidestep the original ill-posed problems and produce for the first time high-quality video generation/prediction/completion results of much longer duration. We present quantitative and qualitative evaluations to show that our approach outperforms state-of-the-art methods in all three tasks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_23');
INSERT INTO `paper` VALUES (10577, 'Deep Video Quality Assessor: From Spatio-Temporal Visual Sensitivity to a Convolutional Neural Aggregation Network', 'Video quality assessment', 'Visual sensitivity', 'Convolutional neural network', 'Attention mechanism', 'HVS', 'Incorporating spatio-temporal human visual perception into video quality assessment (VQA) remains a formidable issue. Previous statistical or computational models of spatio-temporal perception have limitations to be applied to the general VQA algorithms. In this paper, we propose a novel full-reference (FR) VQA framework named Deep Video Quality Assessor (DeepVQA) to quantify the spatio-temporal visual perception via a convolutional neural network (CNN) and a convolutional neural aggregation network (CNAN). Our framework enables to figure out the spatio-temporal sensitivity behavior through learning in accordance with the subjective score. In addition, to manipulate the temporal variation of distortions, we propose a novel temporal pooling method using an attention model. In the experiment, we show DeepVQA remarkably achieves the state-of-the-art prediction accuracy of more than 0.9 correlation, which is \\(\\sim \\)5% higher than those of conventional methods on the LIVE and CSIQ video databases.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_14');
INSERT INTO `paper` VALUES (10578, 'Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry', 'Monocular depth estimation', 'Monocular visual odometry', 'Semi-supervised learning', '', '', 'Monocular visual odometry approaches that purely rely on geometric cues are prone to scale drift and require sufficient motion parallax in successive frames for motion estimation and 3D reconstruction. In this paper, we propose to leverage deep monocular depth prediction to overcome limitations of geometry-based monocular visual odometry. To this end, we incorporate deep depth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo measurements. For depth prediction, we design a novel deep network that refines predicted depth from a single image in a two-stage process. We train our network in a semi-supervised way on photoconsistency in stereo images and on consistency with accurate sparse depth reconstructions from Stereo DSO. Our deep predictions excel state-of-the-art approaches for monocular depth on the KITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds previous monocular and deep-learning based methods in accuracy. It even achieves comparable performance to the state-of-the-art stereo methods, while only relying on a single camera.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_50');
INSERT INTO `paper` VALUES (10579, 'Deep Volumetric Video From Very Sparse Multi-view Performance Capture', 'Human performance capture', 'Neural networks for multi-view stereo', 'Wide-baseline reconstruction', '', '', 'We present a deep learning based volumetric approach for performance capture using a passive and highly sparse multi-view capture system. State-of-the-art performance capture systems require either pre-scanned actors, large number of cameras or active sensors. In this work, we focus on the task of template-free, per-frame 3D surface reconstruction from as few as three RGB sensors, for which conventional visual hull or multi-view stereo methods fail to generate plausible results. We introduce a novel multi-view Convolutional Neural Network (CNN) that maps 2D images to a 3D volumetric field and we use this field to encode the probabilistic distribution of surface points of the captured subject. By querying the resulting field, we can instantiate the clothed human body at arbitrary resolutions. Our approach scales to different numbers of input images, which yield increased reconstruction quality when more views are used. Although only trained on synthetic data, our network can generalize to handle real footage from body performance capture. Our method is suitable for high-quality low-cost full body volumetric capture solutions, which are gaining popularity for VR and AR content creation. Experimental results demonstrate that our method is significantly more robust and accurate than existing techniques when only very sparse views are available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_21');
INSERT INTO `paper` VALUES (10580, 'DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model', 'Robust regression', 'Deep neural networks', 'Mixture model', 'Outlier detection', '', 'In this paper we address the problem of how to robustly train a ConvNet for regression, or deep robust regression. Traditionally, deep regression employ the \\(L_2\\) loss function, known to be sensitive to outliers, i.e. samples that either lie at an abnormal distance away from the majority of the training samples, or that correspond to wrongly annotated targets. This means that, during back-propagation, outliers may bias the training process due to the high magnitude of their gradient. In this paper, we propose DeepGUM: a deep regression model that is robust to outliers thanks to the use of a Gaussian-uniform mixture model. We derive an optimization algorithm that alternates between the unsupervised detection of outliers using expectation-maximization, and the supervised training with cleaned samples using stochastic gradient descent. DeepGUM is able to adapt to a continuously evolving outlier distribution, avoiding to manually impose any threshold on the proportion of outliers in the training set. Extensive experimental evaluations on four different tasks (facial and fashion landmark detection, age and head pose estimation) lead us to conclude that our novel robust technique provides reliability in the presence of various types of noise and protection against a high percentage of outliers.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_13');
INSERT INTO `paper` VALUES (10581, 'DeepIM: Deep Iterative Matching for 6D Pose Estimation', '3D object recognition', '6D object pose estimation', '', '', '', 'Estimating the 6D pose of objects from images is an important problem in various applications such as robot manipulation and virtual reality. While direct regression of images to object poses has limited accuracy, matching rendered images of an object against the input image can produce accurate results. In this work, we propose a novel deep neural network for 6D pose matching named DeepIM. Given an initial pose estimation, our network is able to iteratively refine the pose by matching the rendered image against the observed image. The network is trained to predict a relative pose transformation using an untangled representation of 3D location and 3D orientation and an iterative training process. Experiments on two commonly used benchmarks for 6D pose estimation demonstrate that DeepIM achieves large improvements over state-of-the-art methods. We furthermore show that DeepIM is able to match previously unseen objects.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_42');
INSERT INTO `paper` VALUES (10582, 'DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation', 'Deep domain adaptation', 'Optimal transport', '', '', '', 'In computer vision, one is often confronted with problems of domain shifts, which occur when one applies a classifier trained on a source dataset to target data sharing similar characteristics (e.g. same classes), but also different latent data structures (e.g. different acquisition conditions). In such a situation, the model will perform poorly on the new data, since the classifier is specialized to recognize visual cues specific to the source domain. In this work we explore a solution, named DeepJDOT, to tackle this problem: through a measure of discrepancy on joint deep representations/labels based on optimal transport, we not only learn new data representations aligned between the source and target domain, but also simultaneously preserve the discriminative information used by the classifier. We applied DeepJDOT to a series of visual recognition tasks, where it compares favorably against state-of-the-art deep domain adaptation methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_28');
INSERT INTO `paper` VALUES (10583, 'Deeply Learned Compositional Models for Human Pose Estimation', '', '', '', '', '', 'Compositional models represent patterns with hierarchies of meaningful parts and subparts. Their ability to characterize high-order relationships among body parts helps resolve low-level ambiguities in human pose estimation (HPE). However, prior compositional models make unrealistic assumptions on subpart-part relationships, making them incapable to characterize complex compositional patterns. Moreover, state spaces of their higher-level parts can be exponentially large, complicating both inference and learning. To address these issues, this paper introduces a novel framework, termed as Deeply Learned Compositional Model (DLCM), for HPE. It exploits deep neural networks to learn the compositionality of human bodies. This results in a novel network with a hierarchical compositional architecture and bottom-up/top-down inference stages. In addition, we propose a novel bone-based part representation. It not only compactly encodes orientations, scales and shapes of parts, but also avoids their potentially large state spaces. With significantly lower complexities, our approach outperforms state-of-the-art methods on three benchmark datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_12');
INSERT INTO `paper` VALUES (10584, 'DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks', '', '', '', '', '', 'Non-contact video-based physiological measurement has many applications in health care and human-computer interaction. Practical applications require measurements to be accurate even in the presence of large head rotations. We propose the first end-to-end system for video-based measurement of heart and breathing rate using a deep convolutional network. The system features a new motion representation based on a skin reflection model and a new attention mechanism using appearance information to guide motion estimation, both of which enable robust measurement under heterogeneous lighting and major motions. Our approach significantly outperforms all current state-of-the-art methods on both RGB and infrared video datasets. Furthermore, it allows spatial-temporal distributions of physiological signals to be visualized via the attention mechanism.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_22');
INSERT INTO `paper` VALUES (10585, 'DeepTAM: Deep Tracking and Mapping', 'Camera tracking', 'Multi view stereo', 'ConvNets', '', '', 'We present a system for keyframe-based dense camera tracking and depth map estimation that is entirely learned. For tracking, we estimate small pose increments between the current camera image and a synthetic viewpoint. This significantly simplifies the learning problem and alleviates the dataset bias for camera motions. Further, we show that generating a large number of pose hypotheses leads to more accurate predictions. For mapping, we accumulate information in a cost volume centered at the current depth estimate. The mapping network then combines the cost volume and the keyframe image to update the depth prediction, thereby effectively making use of depth measurements and image-based priors. Our approach yields state-of-the-art results with few images and is robust with respect to noisy camera poses. We demonstrate that the performance of our 6 DOF tracking competes with RGB-D tracking algorithms.We compare favorably against strong classic and deep learning powered dense depth algorithms.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_50');
INSERT INTO `paper` VALUES (10586, 'DeepVS: A Deep Learning Based Video Saliency Prediction Approach', 'Saliency prediction', 'Convolutional LSTM', 'Eye-tracking database', '', '', 'In this paper, we propose a novel deep learning based video saliency prediction method, named DeepVS. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which includes 32 subjects’ fixations on 538 videos. We find from LEDOV that human attention is more likely to be attracted by objects, particularly the moving objects or the moving parts of objects. Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets. In OM-CNN, cross-net mask and hierarchical feature normalization are proposed to combine the spatial features of the objectness subnet and the temporal features of the motion subnet. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. We thus propose saliency-structured convolutional long short-term memory (SS-ConvLSTM) network, using the extracted features from OM-CNN as the input. Consequently, the inter-frame saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps. Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_37');
INSERT INTO `paper` VALUES (10587, 'DeepWrinkles: Accurate and Realistic Clothing Modeling', '3D surface deformation modeling', 'Cloth simulation', 'Normal maps', 'Deep neural networks', '', 'We present a novel method to generate accurate and realistic clothing deformation from real data capture. Previous methods for realistic cloth modeling mainly rely on intensive computation of physics-based simulation (with numerous heuristic parameters), while models reconstructed from visual observations typically suffer from lack of geometric details. Here, we propose an original framework consisting of two modules that work jointly to represent global shape deformation as well as surface details with high fidelity. Global shape deformations are recovered from a subspace model learned from 3D data of clothed people in motion, while high frequency details are added to normal maps created using a conditional Generative Adversarial Network whose architecture is designed to enforce realism and temporal consistency. This leads to unprecedented high-quality rendering of clothing deformation sequences, where fine wrinkles from (real) high resolution observations can be recovered. In addition, as the model is learned independently from body shape and pose, the framework is suitable for applications that require retargeting (e.g., body animation). Our experiments show original high quality results with a flexible model. We claim an entirely data-driven approach to realistic cloth wrinkle generation is possible.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_41');
INSERT INTO `paper` VALUES (10588, 'DeeSIL: Deep-Shallow Incremental Learning', 'Incremental learning', 'SVM', 'ImageNet', '', '', 'Incremental Learning (IL) is an interesting AI problem when the algorithm is assumed to work on a budget. This is especially true when IL is modeled using a deep learning approach, where two complex challenges arise due to limited memory, which induces catastrophic forgetting and delays related to the retraining needed in order to incorporate new classes. Here we introduce DeeSIL, an adaptation of a known transfer learning scheme that combines a fixed deep representation used as feature extractor and learning independent shallow classifiers to increase recognition capacity. This scheme tackles the two aforementioned challenges since it works well with a limited memory budget and each new concept can be added within a minute. Moreover, since no deep retraining is needed when the model is incremented, DeeSIL can integrate larger amounts of initial data that provide more transferable features. Performance is evaluated on ImageNet LSVRC 2012 against three state of the art algorithms. Results show that, at scale, DeeSIL performance is 23 and 33 points higher than the best baseline when using the same and more initial data respectively.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_11');
INSERT INTO `paper` VALUES (10589, 'Deformable Pose Traversal Convolution for 3D Action and Gesture Recognition', 'Pose Traversal', 'Pose Convolution', 'Deformable Convolution', '3D action and gesture recognition', '', 'The representation of 3D pose plays a critical role for 3D action and gesture recognition. Rather than representing a 3D pose directly by its joint locations, in this paper, we propose a Deformable Pose Traversal Convolution Network that applies one-dimensional convolution to traverse the 3D pose for its representation. Instead of fixing the receptive field when performing traversal convolution, it optimizes the convolution kernel for each joint, by considering contextual joints with various weights. This deformable convolution better utilizes the contextual joints for action and gesture recognition and is more robust to noisy joints. Moreover, by feeding the learned pose feature to a LSTM, we perform end-to-end training that jointly optimizes 3D pose representation and temporal sequence recognition. Experiments on three benchmark datasets validate the competitive performance of our proposed method, as well as its efficiency and robustness to handle noisy joints of pose.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_9');
INSERT INTO `paper` VALUES (10590, 'Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance', 'Morph Expression', 'Canonical Coordinate System', 'Canonical Appearance', 'Warp Field', 'ReLU Layer', 'In this work we introduce Deforming Autoencoders, a generative model for images that disentangles shape from appearance in an unsupervised manner. As in the deformable template paradigm, shape is represented as a deformation between a canonical coordinate system (‘template’) and an observed image, while appearance is modeled in deformation-invariant, template coordinates. We introduce novel techniques that allow this approach to be deployed in the setting of autoencoders and show that this method can be used for unsupervised group-wise image alignment. We show experiments with expression morphing in humans, hands, and digits, face manipulation, such as shape and appearance interpolation, as well as unsupervised landmark localization. We also achieve a more powerful form of unsupervised disentangling in template coordinates, that successfully decomposes face images into shading and albedo, allowing us to further manipulate face images.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_40');
INSERT INTO `paper` VALUES (10591, 'Dense Pose Transfer', 'Pose Transfer', 'Neural Synthesis', 'Target Pose', 'Perceptual Loss', 'Deep Generative Models', 'In this work we integrate ideas from surface-based modeling with neural synthesis: we propose a combination of surface-based pose estimation and deep generative models that allows us to perform accurate pose transfer, i.e. synthesize a new image of a person based on a single image of that person and the image of a pose donor. We use a dense pose estimation system that maps pixels from both images to a common surface-based coordinate system, allowing the two images to be brought in correspondence with each other. We inpaint and refine the source image intensities in the surface coordinate system, prior to warping them onto the target pose. These predictions are fused with those of a convolutional predictive module through a neural synthesis module allowing for training the whole pipeline jointly end-to-end, optimizing a combination of adversarial and perceptual losses. We show that dense pose estimation is a substantially more powerful conditioning input than landmark-, or mask-based alternatives, and report systematic improvements over state of the art generators on DeepFashion and MVC datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_8');
INSERT INTO `paper` VALUES (10592, 'Dense Semantic and Topological Correspondence of 3D Faces without Landmarks', '3D face', 'Dense correspondence', 'Point set registration', '', '', 'Many previous literatures use landmarks to guide the correspondence of 3D faces. However, these landmarks, either manually or automatically annotated, are hard to define consistently across different faces in many circumstances. We propose a general framework for dense correspondence of 3D faces without landmarks in this paper. The dense correspondence goal is revisited in two perspectives: semantic and topological correspondence. Starting from a template facial mesh, we sequentially perform global alignment, primary correspondence by template warping, and contextual mesh refinement, to reach the final correspondence result. The semantic correspondence is achieved by a local iterative closest point (ICP) algorithm of kernelized version, allowing accurate matching of local features. Then, robust deformation from the template to the target face is formulated as a minimization problem. Furthermore, this problem leads to a well-posed sparse linear system such that the solution is unique and efficient. Finally, a contextual mesh refining algorithm is applied to ensure topological correspondence. In the experiment, the proposed method is evaluated both qualitatively and quantitatively on two datasets including a publicly available FRGC v2.0 dataset, demonstrating reasonable and reliable correspondence results.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_32');
INSERT INTO `paper` VALUES (10593, 'Densely Connected Stacked U-network for Filament Segmentation in Microscopy Images', 'Image segmentation', 'Filaments segmentation', 'Neural networks', 'Microscopy images', '', 'Segmenting filamentous structures in confocal microscopy images is important for analyzing and quantifying related biological processes. However, thin structures, especially in noisy imagery, are difficult to accurately segment. In this paper, we introduce a novel deep network architecture for filament segmentation in confocal microscopy images that improves upon the state-of-the-art U-net and SOAX methods. We also propose a strategy for data annotation, and create datasets for microtubule and actin filaments. Our experiments show that our proposed network outperforms state-of-the-art approaches and that our segmentation results are not only better in terms of accuracy, but also more suitable for biological analysis and understanding by reducing the number of falsely disconnected filaments in segmentation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_30');
INSERT INTO `paper` VALUES (10594, 'Dependency-Aware Attention Control for Unconstrained Face Recognition with Image Sets', 'Deep reinforcement learning', 'Actor-critic', 'Face recognition', 'Set-to-set', 'Attention control', 'This paper targets the problem of image set-based face verification and identification. Unlike traditional single media (an image or video) setting, we encounter a set of heterogeneous contents containing orderless images and videos. The importance of each image is usually considered either equal or based on their independent quality assessment. How to model the relationship of orderless images within a set remains a challenge. We address this problem by formulating it as a Markov Decision Process (MDP) in the latent space. Specifically, we first present a dependency-aware attention control (DAC) network, which resorts to actor-critic reinforcement learning for sequential attention decision of each image embedding to fully exploit the rich correlation cues among the unordered images. Moreover, we introduce its sample-efficient variant with off-policy experience replay to speed up the learning process. The pose-guided representation scheme can further boost the performance at the extremes of the pose variation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_34');
INSERT INTO `paper` VALUES (10595, 'Depth Estimation via Affinity Learned with Convolutional Spatial Propagation Network', 'Depth estimation', 'Convolutional spatial propagation', '', '', '', 'Depth estimation from a single image is a fundamental problem in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for depth prediction. Specifically, we adopt an efficient linear propagation model, where the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We apply the designed CSPN to two depth estimation tasks given a single image: (1) Refine the depth output from existing state-of-the-art (SOTA) methods; (2) Convert sparse depth samples to a dense depth map by embedding the depth samples within the propagation procedure. The second task is inspired by the availability of LiDAR that provides sparse but accurate depth measurements. We experimented the proposed CSPN over the popular NYU v2 [1] and KITTI [2] datasets, where we show that our proposed approach improves not only quality (e.g., 30% more reduction in depth error), but also speed (e.g., 2 to 5\\(\\times \\) faster) of depth maps than previous SOTA methods. The codes of CSPN are available at: https://github.com/XinJCheng/CSPN.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_7');
INSERT INTO `paper` VALUES (10596, 'Depth-Aware CNN for RGB-D Segmentation', 'Geometry in CNN', 'RGB-D semantic segmentation', '', '', '', 'Convolutional neural networks (CNN) are limited by the lack of capability to handle geometric information due to the fixed grid kernel structure. The availability of depth data enables progress in RGB-D semantic segmentation with CNNs. State-of-the-art methods either use depth as additional images or process spatial information in 3D volumes or point clouds. These methods suffer from high computation and memory cost. To address these issues, we present Depth-aware CNN by introducing two intuitive, flexible and effective operations: depth-aware convolution and depth-aware average pooling. By leveraging depth similarity between pixels in the process of information propagation, geometry is seamlessly incorporated into CNN. Without introducing any additional parameters, both operators can be easily integrated into existing CNNs. Extensive experiments and ablation studies on challenging RGB-D semantic segmentation benchmarks validate the effectiveness and flexibility of our approach.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_9');
INSERT INTO `paper` VALUES (10597, 'Descending, Lifting or Smoothing: Secrets of Robust Cost Optimization', '', '', '', '', '', 'Robust cost optimization is the challenging task of fitting a large number of parameters to data points containing a significant and unknown fraction of outliers. In this work we identify three classes of deterministic second-order algorithms that are able to tackle this type of optimization problem: direct approaches that aim to optimize the robust cost directly with a second order method, lifting-based approaches that add so called lifting variables to embed the given robust cost function into a higher dimensional space, and graduated optimization methods that solve a sequence of smoothed cost functions. We study each of these classes of algorithms and propose improvements either to reduce their computational time or to make them find better local minima. Finally, we experimentally demonstrate the superiority of our improved graduated optimization method over the state of the art algorithms both on synthetic and real data for four different problems.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_34');
INSERT INTO `paper` VALUES (10598, 'DesIGN: Design Inspiration from Generative Networks', 'Fashion image generation', 'Generative adversarial networks', '', '', '', 'Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost novelty in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) a new loss function that encourages novelty, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies. We show that our proposed creativity loss yields better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_5');
INSERT INTO `paper` VALUES (10599, 'Detecting Parallel-Moving Objects in the Monocular Case Employing CNN Depth Maps', '', '', '', '', '', 'This paper presents a method for detecting independently moving objects (IMOs) from a monocular camera mounted on a moving car. We use an existing state of the art monocular sparse visual odometry/SLAM framework, and specifically attack the notorious problem of identifying those IMOs which move parallel to the ego-car motion, that is, in an ‘epipolar-conformant’ way. IMO candidate patches are obtained from an existing CNN-based car instance detector. While crossing IMOs can be identified as such by epipolar consistency checks, IMOs that move parallel to the camera motion are much harder to detect as their epipolar conformity allows to misinterpret them as static objects in a wrong distance. We employ a CNN to provide an appearance-based depth estimate, and the ambiguity problem can be solved through depth verification. The obtained motion labels (IMO/static) are then propagated over time using the combination of motion cues and appearance-based information of the IMO candidate patches. We evaluate the performance of our method on the KITTI dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_22');
INSERT INTO `paper` VALUES (10600, 'Detecting Synapse Location and Connectivity by Signed Proximity Estimation and Pruning with Deep Nets', '', '', '', '', '', 'Synaptic connectivity detection is a critical task for neural reconstruction from Electron Microscopy (EM) data. Most of the existing algorithms for synapse detection do not identify the cleft location and direction of connectivity simultaneously. The few methods that computes direction along with contact location have only been demonstrated to work on either dyadic (most common in vertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on both types. In this paper, we present an algorithm to automatically predict the location as well as the direction of both dyadic and polyadic synapses. The proposed algorithm first generates candidate synaptic connections from voxelwise predictions of signed proximity generated by a 3D U-net. A second 3D CNN then prunes the set of candidates to produce the final detection of cleft and connectivity orientation. Experimental results demonstrate that the proposed method outperforms the existing methods for determining synapses in both rodent and fruit fly brain. (Code at: https://github.com/paragt/EMSynConn).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_25');
INSERT INTO `paper` VALUES (10601, 'Deterministic Consensus Maximization with Biconvex Programming', 'Robust fitting', 'Consensus maximization', 'Biconvex programming', '', '', 'Consensus maximization is one of the most widely used robust fitting paradigms in computer vision, and the development of algorithms for consensus maximization is an active research topic. In this paper, we propose an efficient deterministic optimization algorithm for consensus maximization. Given an initial solution, our method conducts a deterministic search that forcibly increases the consensus of the initial solution. We show how each iteration of the update can be formulated as an instance of biconvex programming, which we solve efficiently using a novel biconvex optimization algorithm. In contrast to our algorithm, previous consensus improvement techniques rely on random sampling or relaxations of the objective function, which reduce their ability to significantly improve the initial consensus. In fact, on challenging instances, the previous techniques may even return a worse off solution. Comprehensive experiments show that our algorithm can consistently and greatly improve the quality of the initial solution, without substantial cost. (Matlab demo program is available in the supplementary material)', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_42');
INSERT INTO `paper` VALUES (10602, 'DetNet: Design Backbone for Object Detection', 'Object detection', 'Convolutional neural network', 'Image classification', '', '', 'Recent CNN based object detectors, either one-stage methods like YOLO, SSD, and RetinaNet, or two-stage detectors like Faster R-CNN, R-FCN and FPN, are usually trying to directly finetune from ImageNet pre-trained models designed for the task of image classification. However, there has been little work discussing the backbone feature extractor specifically designed for the task of object detection. More importantly, there are several differences between the tasks of image classification and object detection. (i) Recent object detectors like FPN and RetinaNet usually involve extra stages against the task of image classification to handle the objects with various scales. (ii) Object detection not only needs to recognize the category of the object instances but also spatially locate them. Large downsampling factors bring large valid receptive field, which is good for image classification, but compromises the object location ability. Due to the gap between the image classification and object detection, we propose DetNet in this paper, which is a novel backbone network specifically designed for object detection. Moreover, DetNet includes the extra stages against traditional backbone network for image classification, while maintains high spatial resolution in deeper layers. Without any bells and whistles, state-of-the-art results have been obtained for both object detection and instance segmentation on the MSCOCO benchmark based on our DetNet (4.8G FLOPs) backbone. Codes will be released (https://github.com/zengarden/DetNet).', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_21');
INSERT INTO `paper` VALUES (10603, 'Devon: Deformable Volume Network for Learning Optical Flow', '', '', '', '', '', 'We propose a new neural network module, Deformable Cost Volume, for learning large displacement optical flow. The module does not distort the original images or their feature maps and therefore avoids the artifacts associated with warping. Based on this module, a new neural network model is proposed. The full version of this paper can be found online (https://arxiv.org/abs/1802.07351).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_50');
INSERT INTO `paper` VALUES (10604, 'DF-Net: Unsupervised Joint Learning of Depth and Flow Using Cross-Task Consistency', '', '', '', '', '', 'We present an unsupervised learning framework for simultaneously training single-view depth prediction and optical flow estimation models using unlabeled video sequences. Existing unsupervised methods often exploit brightness constancy and spatial smoothness priors to train depth or flow models. In this paper, we propose to leverage geometric consistency as additional supervisory signals. Our core idea is that for rigid regions we can use the predicted scene depth and camera motion to synthesize 2D optical flow by backprojecting the induced 3D scene flow. The discrepancy between the rigid flow (from depth prediction and camera motion) and the estimated flow (from optical flow model) allows us to impose a cross-task consistency loss. While all the networks are jointly optimized during training, they can be applied independently at test time. Extensive experiments demonstrate that our depth and flow models compare favorably with state-of-the-art unsupervised methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_3');
INSERT INTO `paper` VALUES (10605, 'DFT-based Transformation Invariant Pooling Layer for Visual Classification', '', '', '', '', '', 'We propose a novel discrete Fourier transform-based pooling layer for convolutional neural networks. The DFT magnitude pooling replaces the traditional max/average pooling layer between the convolution and fully-connected layers to retain translation invariance and shape preserving (aware of shape difference) properties based on the shift theorem of the Fourier transform. Thanks to the ability to handle image misalignment while keeping important structural information in the pooling stage, the DFT magnitude pooling improves the classification accuracy significantly. In addition, we propose the DFT+ method for ensemble networks using the middle convolution layer outputs. The proposed methods are extensively evaluated on various classification tasks using the ImageNet, CUB 2010-2011, MIT Indoors, Caltech 101, FMD and DTD datasets. The AlexNet, VGG-VD 16, Inception-v3, and ResNet are used as the base networks, upon which DFT and DFT+ methods are implemented. Experimental results show that the proposed methods improve the classification performance in all networks and datasets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_6');
INSERT INTO `paper` VALUES (10606, 'Diagnosing Error in Temporal Action Detectors', 'Temporal action detection', 'Error analysis', 'Diagnosis tool', 'Action localization', '', 'Despite the recent progress in video understanding and the continuous rate of improvement in temporal action localization throughout the years, it is still unclear how far (or close?) we are to solving the problem. To this end, we introduce a new diagnostic tool to analyze the performance of temporal action detectors in videos and compare different methods beyond a single scalar metric. We exemplify the use of our tool by analyzing the performance of the top rewarded entries in the latest ActivityNet action localization challenge. Our analysis shows that the most impactful areas to work on are: strategies to better handle temporal context around the instances, improving the robustness w.r.t. the instance absolute and relative size, and strategies to reduce the localization errors. Moreover, our experimental analysis finds the lack of agreement among annotator is not a major roadblock to attain progress in the field. Our diagnostic tool is publicly available to keep fueling the minds of other researchers with additional insights about their algorithms.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_16');
INSERT INTO `paper` VALUES (10607, 'Direct Sparse Odometry with Rolling Shutter', 'Direct monocular visual odometry', 'Rolling shutter', '', '', '', 'Neglecting the effects of rolling-shutter cameras for visual odometry (VO) severely degrades accuracy and robustness. In this paper, we propose a novel direct monocular VO method that incorporates a rolling-shutter model. Our approach extends direct sparse odometry which performs direct bundle adjustment of a set of recent keyframe poses and the depths of a sparse set of image points. We estimate the velocity at each keyframe and impose a constant-velocity prior for the optimization. In this way, we obtain a near real-time, accurate direct VO method. Our approach achieves improved results on challenging rolling-shutter sequences over state-of-the-art global-shutter VO.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_42');
INSERT INTO `paper` VALUES (10608, 'Discriminative Feature Selection by Optimal Manifold Search for Neoplastic Image Recognition', 'Feature selection', 'Manifold learning', 'Texture feature', 'Convolutional neural network', 'Endocytoscopic images', 'An endocytoscope provides ultramagnified observation that enables physicians to achieve minimally invasive and real-time diagnosis in colonoscopy. However, great pathological knowledge and clinical experiences are required for this diagnosis. The computer-aided diagnosis (CAD) system is required that decreases the chances of overlooking neoplastic polyps in endocytoscopy. Towards the construction of a CAD system, we have developed texture-feature-based classification between neoplastic and non-neoplastic images of polyps. We propose a feature-selection method that selects discriminative features from texture features for such two-category classification by searching for an optimal manifold. With an optimal manifold, where selected features are distributed, the distance between two linear subspaces is maximised. We experimentally evaluated the proposed method by comparing the classification accuracy before and after the feature selection for texture features and deep-learning features. Furthermore, we clarified the characteristics of an optimal manifold by exploring the relation between the classification accuracy and the output probability of a support vector machine (SVM). The classification with our feature-selection method achieved 84.7% accuracy, which is 7.2% higher than the direct application of Haralick features and SVM.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_43');
INSERT INTO `paper` VALUES (10609, 'Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation', 'GAN', 'DRPAN', 'Image-to-image translation', '', '', 'Image-to-image translation has been made much progress with embracing Generative Adversarial Networks (GANs). However, it’s still very challenging for translation tasks that require high quality, especially at high-resolution and photorealism. In this paper, we present Discriminative Region Proposal Adversarial Networks (DRPAN) for high-quality image-to-image translation. We decompose the procedure of image-to-image translation task into three iterated steps, first is to generate an image with global structure but some local artifacts (via GAN), second is using our DRPnet to propose the most fake region from the generated image, and third is to implement “image inpainting” on the most fake region for more realistic result through a reviser, so that the system (DRPAN) can be gradually optimized to synthesize images with more attention on the most artifact local part. Experiments on a variety of image-to-image translation tasks and datasets validate that our method outperforms state-of-the-arts for producing high-quality translation results in terms of both human perceptual studies and automatic quantitative measures.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_47');
INSERT INTO `paper` VALUES (10610, 'Disentangling Factors of Variation with Cycle-Consistent Variational Auto-encoders', 'Disentangling factors of variation', 'Cycle-consistent architecture', 'Variational auto-encoders', '', '', 'Generative models that learn disentangled representations for different factors of variation in an image can be very useful for targeted data augmentation. By sampling from the disentangled latent subspace of interest, we can efficiently generate new data necessary for a particular task. Learning disentangled representations is a challenging problem, especially when certain factors of variation are difficult to label. In this paper, we introduce a novel architecture that disentangles the latent space into two complementary subspaces by using only weak supervision in form of pairwise similarity labels. Inspired by the recent success of cycle-consistent adversarial architectures, we use cycle-consistency in a variational auto-encoder framework. Our non-adversarial approach is in contrast with the recent works that combine adversarial training with auto-encoders to disentangle representations. We show compelling results of disentangled latent subspaces on three datasets and compare with recent works that leverage adversarial training.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_49');
INSERT INTO `paper` VALUES (10611, 'Dist-GAN: An Improved GAN Using Distance Constraints', 'Generative Adversarial Networks', 'Image generation', 'Distance constraints', 'Autoencoders', '', 'We introduce effective training algorithms for Generative Adversarial Networks (GAN) to alleviate mode collapse and gradient vanishing. In our system, we constrain the generator by an Autoencoder (AE). We propose a formulation to consider the reconstructed samples from AE as “real” samples for the discriminator. This couples the convergence of the AE with that of the discriminator, effectively slowing down the convergence of discriminator and reducing gradient vanishing. Importantly, we propose two novel distance constraints to improve the generator. First, we propose a latent-data distance constraint to enforce compatibility between the latent sample distances and the corresponding data sample distances. We use this constraint to explicitly prevent the generator from mode collapse. Second, we propose a discriminator-score distance constraint to align the distribution of the generated samples with that of the real samples through the discriminator score. We use this constraint to guide the generator to synthesize samples that resemble the real ones. Our proposed GAN using these distance constraints, namely Dist-GAN, can achieve better results than state-of-the-art methods across benchmark datasets: synthetic, MNIST, MNIST-1K, CelebA, CIFAR-10 and STL-10 datasets. Our code is published here (https://github.com/tntrung/gan) for research.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_23');
INSERT INTO `paper` VALUES (10612, 'Distant Vehicle Detection: How Well Can Region Proposal Networks Cope with Tiny Objects at Low Resolution?', 'Saliency maps', 'Region proposal network', 'Low resolution', 'Object detection', '', 'High-performance faster R-CNN has been applied to many detection tasks. Detecting tiny objects at very low resolution remains a challenge, however, and a few studies addressed explicitly the detection of such objects yet. Focusing on distant object detection at very low resolution images for driver assistance systems, we introduce post-trained net surgery to (1) analyze the network activation patterns, (2) study the potential of prior information to improve localization and binary classification performance, and (3) to support the development of priors for improving the network performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_17');
INSERT INTO `paper` VALUES (10613, 'Distinctive-Attribute Extraction for Image Captioning', 'Image captioning', 'Semantic information', 'Distinctive-attribute', 'Term frequency-inverse document frequency (TF-IDF)', '', 'Image captioning has evolved with the progress of deep neural networks. However, generating qualitatively detailed and distinctive captions is still an open issue. In previous works, a caption involving semantic description can be generated by applying additional information into the RNNs. In this approach, we propose a distinctive-attribute extraction (DaE) method that extracts attributes which explicitly encourage RNNs to generate an accurate caption. We evaluate the proposed method with a challenge data and verify that this method improves the performance, describing images in more detail. The method can be plugged into various models to improve their performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_12');
INSERT INTO `paper` VALUES (10614, 'Distortion-Aware Convolutional Filters for Dense Prediction in Panoramic Images', '', '', '', '', '', 'There is a high demand of 3D data for 360\\(^\\circ \\) panoramic images and videos, pushed by the growing availability on the market of specialized hardware for both capturing (e.g., omni-directional cameras) as well as visualizing in 3D (e.g., head mounted displays) panoramic images and videos. At the same time, 3D sensors able to capture 3D panoramic data are expensive and/or hardly available. To fill this gap, we propose a learning approach for panoramic depth map estimation from a single image. Thanks to a specifically developed distortion-aware deformable convolution filter, our method can be trained by means of conventional perspective images, then used to regress depth for panoramic images, thus bypassing the effort needed to create annotated panoramic training dataset. We also demonstrate our approach for emerging tasks such as panoramic monocular SLAM, panoramic semantic segmentation and panoramic style transfer.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_43');
INSERT INTO `paper` VALUES (10615, 'Distractor-Aware Siamese Networks for Visual Object Tracking', 'Visual tracking', 'Distractor-aware', 'Siamese networks', '', '', 'Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_7');
INSERT INTO `paper` VALUES (10616, 'Diverse and Coherent Paragraph Generation from Images', 'Captioning', 'Review generation', 'Variational autoencoders', '', '', 'Paragraph generation from images, which has gained popularity recently, is an important task for video summarization, editing, and support of the disabled. Traditional image captioning methods fall short on this front, since they aren’t designed to generate long informative descriptions. Moreover, the vanilla approach of simply concatenating multiple short sentences, possibly synthesized from a classical image captioning system, doesn’t embrace the intricacies of paragraphs: coherent sentences, globally consistent structure, and diversity. To address those challenges, we propose to augment paragraph generation techniques with “coherence vectors,” “global topic vectors,” and modeling of the inherent ambiguity of associating paragraphs with images, via a variational auto-encoder formulation. We demonstrate the effectiveness of the developed approach on two datasets, outperforming existing state-of-the-art techniques on both.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_45');
INSERT INTO `paper` VALUES (10617, 'Diverse Conditional Image Generation by Stochastic Regression with Latent Drop-Out Codes', 'Image generation', 'Improving diversity', 'One-to-many mapping', 'Nonparametrics', '', 'Recent advances in Deep Learning and probabilistic modeling have led to strong improvements in generative models for images. On the one hand, Generative Adversarial Networks (GANs) have contributed a highly effective adversarial learning procedure, but still suffer from stability issues. On the other hand, Conditional Variational Auto-Encoders (CVAE) models provide a sound way of conditional modeling but suffer from mode-mixing issues. Therefore, recent work has turned back to simple and stable regression models that are effective at generation but give up on the sampling mechanism and the latent code representation. We propose a novel and efficient stochastic regression approach with latent drop-out codes that combines the merits of both lines of research. In addition, a new training objective increases coverage of the training distribution leading to improvements over the state of the art in terms of accuracy as well as diversity.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_25');
INSERT INTO `paper` VALUES (10618, 'Diverse Feature Visualizations Reveal Invariances in Early Layers of Deep Neural Networks', 'Feature visualization', 'Invariance', 'Phase invariance', 'Deep neural networks', 'Early visual system', 'Visualizing features in deep neural networks (DNNs) can help understanding their computations. Many previous studies aimed to visualize the selectivity of individual units by finding meaningful images that maximize their activation. However, comparably little attention has been paid to visualizing to what image transformations units in DNNs are invariant. Here we propose a method to discover invariances in the responses of hidden layer units of deep neural networks. Our approach is based on simultaneously searching for a batch of images that strongly activate a unit while at the same time being as distinct from each other as possible. We find that even early convolutional layers in VGG-19 exhibit various forms of response invariance: near-perfect phase invariance in some units and invariance to local diffeomorphic transformations in others. At the same time, we uncover representational differences with ResNet-50 in its corresponding layers. We conclude that invariance transformations are a major computational component learned by DNNs and we provide a systematic method to study them.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_14');
INSERT INTO `paper` VALUES (10619, 'Diverse Image-to-Image Translation via Disentangled Representations', 'Disentangled Representation', 'Diverse Output', 'Content Space', 'Discriminative Content', 'Leg Cycling', 'Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: (1) the lack of aligned training pairs and (2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Using the disentangled features as inputs greatly reduces mode collapse. To handle unpaired training data, we introduce a novel cross-cycle consistency loss. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks. We validate the effectiveness of our approach through extensive evaluation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_3');
INSERT INTO `paper` VALUES (10620, 'Dividing and Aggregating Network for Multi-view Action Recognition', 'Dividing and Aggregating Network', 'Multi-view action recognition', 'Large-scale action recognition', '', '', 'In this paper, we propose a new Dividing and Aggregating Network (DA-Net) for multi-view action recognition. In our DA-Net, we learn view-independent representations shared by all views at lower layers, while we learn one view-specific representation for each view at higher layers. We then train view-specific action classifiers based on the view-specific representation for each view and a view classifier based on the shared representation at lower layers. The view classifier is used to predict how likely each video belongs to each view. Finally, the predicted view probabilities from multiple views are used as the weights when fusing the prediction scores of view-specific action classifiers. We also propose a new approach based on the conditional random field (CRF) formulation to pass message among view-specific representations from different branches to help each other. Comprehensive experiments on two benchmark datasets clearly demonstrate the effectiveness of our proposed DA-Net for multi-view action recognition.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_28');
INSERT INTO `paper` VALUES (10621, 'DNN Feature Map Compression Using Learned Representation over GF(2)', 'Feature map compression', 'Dimensionality reduction', 'Network quantization', 'Memory-efficient inference', '', 'In this paper, we introduce a method to compress intermediate feature maps of deep neural networks (DNNs) to decrease memory storage and bandwidth requirements during inference. Unlike previous works, the proposed method is based on converting fixed-point activations into vectors over the smallest GF(2) finite field followed by nonlinear dimensionality reduction (NDR) layers embedded into a DNN. Such an end-to-end learned representation finds more compact feature maps by exploiting quantization redundancies within the fixed-point activations along the channel or spatial dimensions. We apply the proposed network architectures derived from modified SqueezeNet and MobileNetV2 to the tasks of ImageNet classification and PASCAL VOC object detection. Compared to prior approaches, the conducted experiments show a factor of 2 decrease in memory requirements with minor degradation in accuracy while adding only bitwise computations.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_41');
INSERT INTO `paper` VALUES (10622, 'DOCK: Detecting Objects by Transferring Common-Sense Knowledge', '', '', '', '', '', 'We present a scalable approach for Detecting Objects by transferring Common-sense Knowledge (DOCK) from source to target categories. In our setting, the training data for the source categories have bounding box annotations, while those for the target categories only have image-level annotations. Current state-of-the-art approaches focus on image-level visual or semantic similarity to adapt a detector trained on the source categories to the new target categories. In contrast, our key idea is to (i) use similarity not at the image-level, but rather at the region-level, and (ii) leverage richer common-sense (based on attribute, spatial, etc.) to guide the algorithm towards learning the correct detections. We acquire such common-sense cues automatically from readily-available knowledge bases without any extra human effort. On the challenging MS COCO dataset, we find that common-sense knowledge can substantially improve detection performance over existing transfer-learning baselines.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_30');
INSERT INTO `paper` VALUES (10623, 'Does Haze Removal Help CNN-Based Image Classification?', 'Hazy images', 'Haze removal', 'Image classification', 'Dehazing', 'Classification accuracy', 'Hazy images are common in real scenarios and many dehazing methods have been developed to automatically remove the haze from images. Typically, the goal of image dehazing is to produce clearer images from which human vision can better identify the object and structural details present in the images. When the ground-truth haze-free image is available for a hazy image, quantitative evaluation of image dehazing is usually based on objective metrics, such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). However, in many applications, large-scale images are collected not for visual examination by human. Instead, they are used for many high-level vision tasks, such as automatic classification, recognition and categorization. One fundamental problem here is whether various dehazing methods can produce clearer images that can help improve the performance of the high-level tasks. In this paper, we empirically study this problem in the important task of image classification by using both synthetic and real hazy image datasets. From the experimental results, we find that the existing image-dehazing methods cannot improve much the image-classification performance and sometimes even reduce the image-classification performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_42');
INSERT INTO `paper` VALUES (10624, 'Domain Adaptation Through Synthesis for Unsupervised Person Re-identification', 'Synthetic', 'Identification', 'Unsupervised', 'Domain adaptation', '', 'Drastic variations in illumination across surveillance cameras make the person re-identification problem extremely challenging. Current large scale re-identification datasets have a significant number of training subjects, but lack diversity in lighting conditions. As a result, a trained model requires fine-tuning to become effective under an unseen illumination condition. To alleviate this problem, we introduce a new synthetic dataset that contains hundreds of illumination conditions. Specifically, we use 100 virtual humans illuminated with multiple HDR environment maps which accurately model realistic indoor and outdoor lighting. To achieve better accuracy in unseen illumination conditions we propose a novel domain adaptation technique that takes advantage of our synthetic data and performs fine-tuning in a completely unsupervised way. Our approach yields significantly higher accuracy than semi-supervised and unsupervised state-of-the-art methods, and is very competitive with supervised techniques.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_12');
INSERT INTO `paper` VALUES (10625, 'Domain Adaptive Semantic Segmentation Through Structure Enhancement', 'Unsupervised domain adaptation', 'Semantic segmentation', 'Deep learning', 'Transfer learning', '', 'Although fully convolutional networks have recently achieved great advances in semantic segmentation, the performance leaps heavily rely on supervision with pixel-level annotations which are extremely expensive and time-consuming to collect. Training models on synthetic data is a feasible way to relieve the annotation burden. However, the domain shift between synthetic and real images usually lead to poor generalization performance. In this work, we propose an effective method to adapt the segmentation network trained on synthetic images to real scenarios in an unsupervised fashion. To improve the adaptation performance for semantic segmentation, we enhance the structure information of the target images at both the feature level and the output level. Specifically, we enforce the segmentation network to learn a representation that encodes the target images’ visual cues through image reconstruction, which is beneficial to the structured prediction of the target images. Further more, we implement adversarial training at the output space of the segmentation network to align the structured prediction of the source and target images based on the similar spatial structure they share. To validate the performance of our method, we conduct comprehensive experiments on the “GTA5 to Cityscapes” dataset which is a standard domain adaptation benchmark for semantic segmentation. The experimental results clearly demonstrate that our method can effectively bridge the synthetic and real image domains and obtain better adaptation performance compared with the existing state-of-the-art methods.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_13');
INSERT INTO `paper` VALUES (10626, 'Domain Transfer Through Deep Activation Matching', 'Domain adaptation', 'Image classification', 'Semantic segmentation', 'Activation matching', 'GTA', 'We introduce a layer-wise unsupervised domain adaptation approach for semantic segmentation. Instead of merely matching the output distributions of the source and target domains, our approach aligns the distributions of activations of intermediate layers. This scheme exhibits two key advantages. First, matching across intermediate layers introduces more constraints for training the network in the target domain, making the optimization problem better conditioned. Second, the matched activations at each layer provide similar inputs to the next layer for both training and adaptation, and thus alleviate covariate shift. We use a Generative Adversarial Network (or GAN) to align activation distributions. Experimental results show that our approach achieves state-of-the-art results on a variety of popular domain adaptation tasks, including (1) from GTA to Cityscapes for semantic segmentation, (2) from SYNTHIA to Cityscapes for semantic segmentation, and (3) adaptations on USPS and MNIST for image classification (The website of this paper is https://rsents.github.io/dam.html).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_36');
INSERT INTO `paper` VALUES (10627, 'Double JPEG Detection in Mixed JPEG Quality Factors Using Deep Convolutional Neural Network', 'Image forensics', 'Image manipulation', 'Fake images', 'Double JPEG', 'Convolutional neural networks', 'Double JPEG detection is essential for detecting various image manipulations. This paper proposes a novel deep convolutional neural network for double JPEG detection using statistical histogram features from each block with a vectorized quantization table. In contrast to previous methods, the proposed approach handles mixed JPEG quality factors and is suitable for real-world situations. We collected real-world JPEG images from the image forensic service and generated a new double JPEG dataset with 1120 quantization tables to train the network. The proposed approach was verified experimentally to produce a state-of-the-art performance, successfully detecting various image manipulations.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_39');
INSERT INTO `paper` VALUES (10628, 'DPP-Net: Device-Aware Progressive Search for Pareto-Optimal Neural Architectures', 'Architecture search', 'Multi-objective optimization', '', '', '', 'Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in applications such as image classification and language modeling. However, these techniques typically ignore device-related objectives such as inference time, memory usage, and power consumption. Optimizing neural architecture for device-related objectives is immensely crucial for deploying deep networks on portable devices with limited computing resources. We propose DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architectures, optimizing for both device-related (e.g., inference time and memory usage) and device-agnostic (e.g., accuracy and model size) objectives. DPP-Net employs a compact search space inspired by current state-of-the-art mobile CNNs, and further improves search efficiency by adopting progressive search (Liu et al. 2017). Experimental results on CIFAR-10 are poised to demonstrate the effectiveness of Pareto-optimal networks found by DPP-Net, for three different devices: (1) a workstation with Titan X GPU, (2) NVIDIA Jetson TX1 embedded system, and (3) mobile phone with ARM Cortex-A53. Compared to CondenseNet and NASNet (Mobile), DPP-Net achieves better performances: higher accuracy & shorter inference time on various devices. Additional experimental results show that models found by DPP-Net also achieve considerably-good performance on ImageNet as well.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_32');
INSERT INTO `paper` VALUES (10629, 'DrawInAir: A Lightweight Gestural Interface Based on Fingertip Regression', 'Egocentric gestures', 'Coordinate regression', 'Augmented reality', '', '', 'Hand gestures form a natural way of interaction on Head-Mounted Devices (HMDs) and smartphones. HMDs such as the Microsoft HoloLens and ARCore/ARKit platform enabled smartphones are expensive and are equipped with powerful processors and sensors such as multiple cameras, depth and IR sensors to process hand gestures. To enable mass market reach via inexpensive Augmented Reality (AR) headsets without built-in depth or IR sensors, we propose a real-time, in-air gestural framework that works on monocular RGB input, termed, DrawInAir. DrawInAir uses fingertip for writing in air analogous to a pen on paper. The major challenge in training egocentric gesture recognition models is in obtaining sufficient labeled data for end-to-end learning. Thus, we design a cascade of networks, consisting of a CNN with differentiable spatial to numerical transform (DSNT) layer, for fingertip regression, followed by a Bidirectional Long Short-Term Memory (Bi-LSTM), for a real-time pointing hand gesture classification. We highlight how a model, that is separately trained to regress fingertip in conjunction with a classifier trained on limited classification data, would perform better over end-to-end models. We also propose a dataset of 10 egocentric pointing gestures designed for AR applications for testing our model. We show that the framework takes 1.73 s to run end-to-end and has a low memory footprint of 14 MB while achieving an accuracy of 88.0% on egocentric video dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_15');
INSERT INTO `paper` VALUES (10630, 'Driving Data Collection Framework Using Low Cost Hardware', 'Autonomous driving', 'Data collection', 'ROS', 'Sensing', 'Perception', 'Autonomous driving is driven by data. The availability of large and diverse data set from different geographies can help in maturing Autonomous driving technology faster. It is challenging to build a system to collect driving data which is cost intensive especially in emerging economies. Paradoxically these economies have chaotic driving conditions leading to a valuable data set. To address the issue of cost and scale, we have developed a data collection framework. In this paper, we’ll discuss our motive for the framework, performance bottlenecks, a two stage pipeline design and insights on how to tune the system to get maximum throughput.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_38');
INSERT INTO `paper` VALUES (10631, 'Dual-Agent Deep Reinforcement Learning for Deformable Face Tracking', 'Deformable face tracking', 'Reinforcement learning', 'Deep learning', '', '', 'In this paper, we propose a dual-agent deep reinforcement learning (DADRL) method for deformable face tracking, which generates bounding boxes and detects facial landmarks interactively from face videos. Most existing deformable face tracking methods learn models for these two tasks individually, and perform these two procedures subsequently during the testing phase, which ignore the intrinsic connections of these two tasks. Motivated by the fact that the performance of facial landmark detection depends heavily on the accuracy of the generated bounding boxes, we exploit the interactions of these two tasks in probabilistic manner by following a Bayesian model and propose a unified framework for simultaneous bounding box tracking and landmark detection. By formulating it as a Markov decision process, we define two agents to exploit the relationships and pass messages via an adaptive sequence of actions under a deep reinforcement learning framework to iteratively adjust the positions of the bounding boxes and facial landmarks. Our proposed DADRL achieves performance improvements over the state-of-the-art deformable face tracking methods on the most challenging category of the 300-VW dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_47');
INSERT INTO `paper` VALUES (10632, 'DYAN: A Dynamical Atoms-Based Network for Video Prediction', 'Video autoencoder', 'Sparse coding', 'Video prediction', '', '', 'The ability to anticipate the future is essential when making real time critical decisions, provides valuable information to understand dynamic natural scenes, and can help unsupervised video representation learning. State-of-art video prediction is based on complex architectures that need to learn large numbers of parameters, are potentially hard to train, slow to run, and may produce blurry predictions. In this paper, we introduce DYAN, a novel network with very few parameters and easy to train, which produces accurate, high quality frame predictions, faster than previous approaches. DYAN owes its good qualities to its encoder and decoder, which are designed following concepts from systems identification theory and exploit the dynamics-based invariants of the data. Extensive experiments using several standard video datasets show that DYAN is superior generating frames and that it generalizes well across domains.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_11');
INSERT INTO `paper` VALUES (10633, 'Dynamic Adaptation on Non-stationary Visual Domains', '', '', '', '', '', 'Domain adaptation aims to learn models on a supervised source domain that perform well on an unsupervised target. Prior work has examined domain adaptation in the context of stationary domain shifts, i.e. static data sets. However, with large-scale or dynamic data sources, data from a defined domain is not usually available all at once. For instance, in a streaming data scenario, dataset statistics effectively become a function of time. We introduce a framework for adaptation over non-stationary distribution shifts applicable to large-scale and streaming data scenarios. The model is adapted sequentially over incoming unsupervised streaming data batches. This enables improvements over several batches without the need for any additionally annotated data. To demonstrate the effectiveness of our proposed framework, we modify associative domain adaptation to work well on source and target data batches with unequal class distributions. We apply our method to several adaptation benchmark datasets for classification and show improved classifier accuracy not only for the currently adapted batch, but also when applied on future stream batches. Furthermore, we show the applicability of our associative learning modifications to semantic segmentation, where we achieve competitive results.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_12');
INSERT INTO `paper` VALUES (10634, 'Dynamic Conditional Networks for Few-Shot Learning', 'Conditional model', 'Few-shot learning', 'Deep learning', 'Dynamic convolution', 'Filter bank', 'This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_2');
INSERT INTO `paper` VALUES (10635, 'Dynamic Filtering with Large Sampling Field for ConvNets', 'Large sampling field', 'Object detection', 'Semantic segmentation', 'Flow estimation', '', 'We propose a dynamic filtering strategy with large sampling field for ConvNets (LS-DFN), where the position-specific kernels learn from not only the identical position but also multiple sampled neighbour regions. During sampling, residual learning is introduced to ease training and an attention mechanism is applied to fuse features from different samples. Such multiple samples enlarge the kernels’ receptive fields significantly without requiring more parameters. While LS-DFN inherits the advantages of DFN [5], namely avoiding feature map blurring by positionwise kernels while keeping translation invariance, it also efficiently alleviates the overfitting issue caused by much more parameters than normal CNNs. Our model is efficient and can be trained end-to-end via standard back-propagation. We demonstrate the merits of our LS-DFN on both sparse and dense prediction tasks involving object detection, semantic segmentation and flow estimation. Our results show LS-DFN enjoys stronger recognition abilities in object detection and semantic segmentation tasks on VOC benchmark [8] and sharper responses in flow estimation on FlyingChairs dataset [6] compared to strong baselines.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_12');
INSERT INTO `paper` VALUES (10636, 'Dynamic Multimodal Instance Segmentation Guided by Natural Language Queries', 'Referring expressions', 'Instance segmentation', 'Multimodal interaction', 'Dynamic convolutional filters', 'Natural language processing', 'We address the problem of segmenting an object given a natural language expression that describes it. Current techniques tackle this task by either (i) directly or recursively merging linguistic and visual information in the channel dimension and then performing convolutions; or by (ii) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a convolution can be applied to look for the object. We propose a novel method that integrates these two insights in order to fully exploit the recursive nature of language. Additionally, during the upsampling process, we take advantage of the intermediate information generated when downsampling the image, so that detailed segmentations can be obtained. We compare our method against the state-of-the-art approaches in four standard datasets, in which it surpasses all previous methods in six of eight of the splits for this task.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_39');
INSERT INTO `paper` VALUES (10637, 'Dynamic Task Prioritization for Multitask Learning', '', '', '', '', '', 'We propose dynamic task prioritization for multitask learning. This allows a model to dynamically prioritize difficult tasks during training, where difficulty is inversely proportional to performance, and where difficulty changes over time. In contrast to curriculum learning, where easy tasks are prioritized above difficult tasks, we present several studies showing the importance of prioritizing difficult tasks first. We observe that imbalances in task difficulty can lead to unnecessary emphasis on easier tasks, thus neglecting and slowing progress on difficult tasks. Motivated by this finding, we introduce a notion of dynamic task prioritization to automatically prioritize more difficult tasks by adaptively adjusting the mixing weight of each task’s loss objective. Additional ablation studies show the impact of the task hierarchy, or the task ordering, when explicitly encoded in the network architecture. Our method outperforms existing multitask methods and demonstrates competitive results with modern single-task models on the COCO and MPII datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_17');
INSERT INTO `paper` VALUES (10638, 'EC-Net: An Edge-Aware Point Set Consolidation Network', 'Point cloud', 'Learning', 'Neural network', 'Edge-aware', '', 'Point clouds obtained from 3D scans are typically sparse, irregular, and noisy, and required to be consolidated. In this paper, we present the first deep learning based edge-aware technique to facilitate the consolidation of point clouds. We design our network to process points grouped in local patches, and train it to learn and help consolidate points, deliberately for edges. To achieve this, we formulate a regression component to simultaneously recover 3D point coordinates and point-to-edge distances from upsampled features, and an edge-aware joint loss function to directly minimize distances from output points to 3D meshes and to edges. Compared with previous neural network based works, our consolidation is edge-aware. During the synthesis, our network can attend to the detected sharp edges and enable more accurate 3D reconstructions. Also, we trained our network on virtual scanned point clouds, demonstrated the performance of our method on both synthetic and real point clouds, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_24');
INSERT INTO `paper` VALUES (10639, 'ECO: Efficient Convolutional Network for Online Video Understanding', 'Online video understanding', 'Real-time', 'Action recognition', 'Video captioning', '', 'The state of the art in video understanding suffers from two problems: (1) The major part of reasoning is performed locally in the video, therefore, it misses important relationships within actions that span several seconds. (2) While there are local methods with fast per-frame processing, the processing of the whole video is not efficient and hampers fast video retrieval or online classification of long-term activities. In this paper, we introduce a network architecture (https://github.com/mzolfaghari/ECO-efficient-video-understanding) that takes long-term content into account and enables fast per-video processing at the same time. The architecture is based on merging long-term content already in the network rather than in a post-hoc fusion. Together with a sampling strategy, which exploits that neighboring frames are largely redundant, this yields high-quality action classification and video captioning at up to 230 videos per second, where each video can consist of a few hundred frames. The approach achieves competitive performance across all datasets while being 10\\(\\times \\) to 80\\(\\times \\) faster than state-of-the-art methods.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_43');
INSERT INTO `paper` VALUES (10640, 'Effective Use of Synthetic Data for Urban Scene Semantic Segmentation', 'Synthetic data', 'Semantic segmentation', 'Object detection', 'Instance-level annotation', '', 'Training a deep network to perform semantic segmentation requires large amounts of labeled data. To alleviate the manual effort of annotating real images, researchers have investigated the use of synthetic data, which can be labeled automatically. Unfortunately, a network trained on synthetic data performs relatively poorly on real images. While this can be addressed by domain adaptation, existing methods all require having access to real images during training. In this paper, we introduce a drastically different way to handle synthetic images that does not require seeing any real images at training time. Our approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. In particular, the former should be handled in a detection-based manner to better account for the fact that, while their texture in synthetic images is not photo-realistic, their shape looks natural. Our experiments evidence the effectiveness of our approach on Cityscapes and CamVid with models trained on synthetic data only.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_6');
INSERT INTO `paper` VALUES (10641, 'Efficient 6-DoF Tracking of Handheld Objects from an Egocentric Viewpoint', 'Virtual reality', '6DoF dataset', 'Handheld object tracking', 'MobileNet', 'SSD', 'Virtual and augmented reality technologies have seen significant growth in the past few years. A key component of such systems is the ability to track the pose of head mounted displays and controllers in 3D space. We tackle the problem of efficient 6-DoF tracking of a handheld controller from egocentric camera perspectives. We collected the HMD Controller dataset which consist of over 540,000 stereo image pairs labelled with the full 6-DoF pose of the handheld controller. Our proposed SSD-AF-Stereo3D model achieves a mean average error of 33.5 mm in 3D keypoint prediction and is used in conjunction with an IMU sensor on the controller to enable 6-DoF tracking. We also present results on approaches for model based full 6-DoF tracking. All our models operate under the strict constraints of real time mobile CPU inference.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_26');
INSERT INTO `paper` VALUES (10642, 'Efficient Dense Point Cloud Object Reconstruction Using Deformation Vector Fields', '3D object reconstruction', 'Dense point clouds', 'Deep learning', '', '', 'Some existing CNN-based methods for single-view 3D object reconstruction represent a 3D object as either a 3D voxel occupancy grid or multiple depth-mask image pairs. However, these representations are inefficient since empty voxels or background pixels are wasteful. We propose a novel approach that addresses this limitation by replacing masks with “deformation-fields”. Given a single image at an arbitrary viewpoint, a CNN predicts multiple surfaces, each in a canonical location relative to the object. Each surface comprises a depth-map and corresponding deformation-field that ensures every pixel-depth pair in the depth-map lies on the object surface. These surfaces are then fused to form the full 3D shape. During training we use a combination of per-view loss and multi-view losses. The novel multi-view loss encourages the 3D points back-projected from a particular view to be consistent across views. Extensive experiments demonstrate the efficiency and efficacy of our method on single-view 3D object reconstruction.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_31');
INSERT INTO `paper` VALUES (10643, 'Efficient Global Point Cloud Registration by Matching Rotation Invariant Features Through Translation Search', 'Point cloud registration', 'Global optimization', 'Rotation invariant feature', '', '', 'Three-dimensional rigid point cloud registration has many applications in computer vision and robotics. Local methods tend to fail, causing global methods to be needed, when the relative transformation is large or the overlap ratio is small. Most existing global methods utilize BnB optimization over the 6D parameter space of SE(3) . Such methods are usually very slow because the time complexity of BnB optimization is exponential in the dimensionality of the parameter space. In this paper, we decouple the optimization of translation and rotation, and we propose a fast BnB algorithm to globally optimize the 3D translation parameter first. The optimal rotation is then calculated by utilizing the global optimal translation found by the BnB algorithm. The separate optimization of translation and rotation is realized by using a newly proposed rotation invariant feature. Experiments on challenging data sets demonstrate that the proposed method outperforms state-of-the-art global methods in terms of both speed and accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_28');
INSERT INTO `paper` VALUES (10644, 'Efficient Interactive Multi-object Segmentation in Medical Images', 'Medical image segmentation', 'Interactive segmentation', 'Graph-based image segmentation', 'Superpixels segmentation', '', 'In medical image segmentation, it is common to have several complex objects that are difficult to detect with simple models without user interaction. Hence, interactive graph-based methods are commonly used in this task, where the image is modeled as a connected graph, since graphs can naturally represent the objects and their relationships. In this work, we propose an efficient method for the multiple object segmentation of medical images. For each object, the method constructs an associated weighted digraph of superpixels, attending its individual high-level priors. Then, all individual digraphs are integrated into a hierarchical graph, considering structural relations of inclusion and exclusion. Finally, a single energy optimization is performed in the hierarchical weighted digraph satisfying all the constraints and leading to globally optimal results. The experimental evaluation on 2D medical images indicates promising results comparable to the state-of-the-art methods, with low computational complexity.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_61');
INSERT INTO `paper` VALUES (10645, 'Efficient Relative Attribute Learning Using Graph Neural Networks', 'Relative attribute learning', 'Graph neural networks', 'Multi-task learning', 'Message passing', '', 'A sizable body of work on relative attributes provides evidence that relating pairs of images along a continuum of strength pertaining to a visual attribute yields improvements in a variety of vision tasks. In this paper, we show how emerging ideas in graph neural networks can yield a solution to various problems that broadly fall under relative attribute learning. Our main idea is the observation that relative attribute learning naturally benefits from exploiting the graph of dependencies among the different relative attributes of images, especially when only partial ordering is provided at training time. We use message passing to perform end to end learning of the image representations, their relationships as well as the interplay between different attributes. Our experiments show that this simple framework is effective in achieving competitive accuracy with specialized methods for both relative attribute learning and binary attribute prediction, while relaxing the requirements on the training data and/or the number of parameters, or both.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_34');
INSERT INTO `paper` VALUES (10646, 'Efficient Semantic Scene Completion Network with Spatial Group Convolution', 'Spatial group convolution', 'Sparse convolutional network', 'Efficient neural network', 'Semantic scene completion', '', 'We introduce Spatial Group Convolution (SGC) for accelerating the computation of 3D dense prediction tasks. SGC is orthogonal to group convolution, which works on spatial dimensions rather than feature channel dimension. It divides input voxels into different groups, then conducts 3D sparse convolution on these separated groups. As only valid voxels are considered when performing convolution, computation can be significantly reduced with a slight loss of accuracy. The proposed operations are validated on semantic scene completion task, which aims to predict a complete 3D volume with semantic labels from a single depth image. With SGC, we further present an efficient 3D sparse convolutional network, which harnesses a multiscale architecture and a coarse-to-fine prediction strategy. Evaluations are conducted on the SUNCG dataset, achieving state-of-the-art performance and fast speed.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_45');
INSERT INTO `paper` VALUES (10647, 'Efficient Sliding Window Computation for NN-Based Template Matching', '', '', '', '', '', 'Template matching is a fundamental problem in computer vision, with many applications. Existing methods use sliding window computation for choosing an image-window that best matches the template. For classic algorithms based on SSD, SAD and normalized cross-correlation, efficient algorithms have been developed allowing them to run in real-time. Current state of the art algorithms are based on nearest neighbor (NN) matching of small patches within the template to patches in the image. These algorithms yield state-of-the-art results since they can deal better with changes in appearance, viewpoint, illumination, non-rigid transformations, and occlusion. However, NN-based algorithms are relatively slow not only due to NN computation for each image patch, but also since their sliding window computation is inefficient. We therefore propose in this paper an efficient NN-based algorithm. Its accuracy is similar (in some cases slightly better) than the existing algorithms and its running time is 43–200 times faster depending on the sizes of the images and templates used. The main contribution of our method is an algorithm for incrementally computing the score of each image window based on the score computed for the previous window. This is in contrast to computing the score for each image window independently, as in previous NN-based methods. The complexity of our method is therefore O(|I|) instead of O(|I||T|), where I and T are the image and the template respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_25');
INSERT INTO `paper` VALUES (10648, 'Efficient Texture Retrieval Using Multiscale Local Extrema Descriptors and Covariance Embedding', 'Texture retrieval', 'Handcrafted features', 'Local extrema', 'Feature covariance matrix', '', 'We present an efficient method for texture retrieval using multiscale feature extraction and embedding based on the local extrema keypoints. The idea is to first represent each texture image by its local maximum and local minimum pixels. The image is then divided into regular overlapping blocks and each one is characterized by a feature vector constructed from the radiometric, geometric and structural information of its local extrema. All feature vectors are finally embedded into a covariance matrix which will be exploited for dissimilarity measurement within retrieval task. Thanks to the method’s simplicity, multiscale scheme can be easily implemented to improve its scale-space representation capacity. We argue that our handcrafted features are easy to implement, fast to run but can provide very competitive performance compared to handcrafted and CNN-based learned descriptors from the literature. In particular, the proposed framework provides highly competitive retrieval rate for several texture databases including 94.95\\(\\%\\) for MIT Vistex, 79.87\\(\\%\\) for Stex, 76.15\\(\\%\\) for Outex TC-00013 and 89.74\\(\\%\\) for USPtex.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_45');
INSERT INTO `paper` VALUES (10649, 'Efficient Uncertainty Estimation for Semantic Segmentation in Videos', 'Uncertainty', 'Segmentation', 'Video', 'Efficient', '', 'Uncertainty estimation in deep learning becomes more important recently. A deep learning model can’t be applied in real applications if we don’t know whether the model is certain about the decision or not. Some literature proposes the Bayesian neural network which can estimate the uncertainty by Monte Carlo Dropout (MC dropout). However, MC dropout needs to forward the model N times which results in N times slower. For real-time applications such as a self-driving car system, which needs to obtain the prediction and the uncertainty as fast as possible, so that MC dropout becomes impractical. In this work, we propose the region-based temporal aggregation (RTA) method which leverages the temporal information in videos to simulate the sampling procedure. Our RTA method with Tiramisu backbone is 10x faster than the MC dropout with Tiramisu backbone (\\(N=5\\)). Furthermore, the uncertainty estimation obtained by our RTA method is comparable to MC dropout’s uncertainty estimation on pixel-level and frame-level metrics.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_32');
INSERT INTO `paper` VALUES (10650, 'Egocentric Activity Prediction via Event Modulated Attention', 'Egocentric video', 'Prediction', 'Event', 'Gaze', 'Attention', 'Predicting future activities from an egocentric viewpoint is of particular interest in assisted living. However, state-of-the-art egocentric activity understanding techniques are mostly NOT capable of predictive tasks, as their synchronous processing architecture performs poorly in either modeling event dependency or pruning temporal redundant features. This work explicitly addresses these issues by proposing an asynchronous gaze-event driven attentive activity prediction network. This network is built on a gaze-event extraction module inspired by the fact that gaze moving in/out of a certain object most probably indicates the occurrence/ending of a certain activity. The extracted gaze events are input to: (1) an asynchronous module which reasons about the temporal dependency between events and (2) a synchronous module which softly attends to informative temporal durations for more compact and discriminative feature extraction. Both modules are seamlessly integrated for collaborative prediction. Extensive experimental results on egocentric activity prediction as well as recognition well demonstrate the effectiveness of the proposed method.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_13');
INSERT INTO `paper` VALUES (10651, 'Eigendecomposition-Free Training of Deep Networks with Zero Eigenvalue-Based Losses', 'End-to-end learning', 'Eigendecomposition', 'Singular value decomposition', 'Geometric vision', '', 'Many classical Computer Vision problems, such as essential matrix computation and pose estimation from 3D to 2D correspondences, can be solved by finding the eigenvector corresponding to the smallest, or zero, eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry, instead of having the network implicitly learn them from data. However, performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable, this introduces numerical instability in the optimization process in practice.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_47');
INSERT INTO `paper` VALUES (10652, 'EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection', '', '', '', '', '', 'Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_15');
INSERT INTO `paper` VALUES (10653, 'ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes', 'Face attribute transfer', 'Image generation by exemplars', 'Attributes disentanglement', 'Generative adversarial networks', '', 'Recent studies on face attribute transfer have achieved great success. A lot of models are able to transfer face attributes with an input image. However, they suffer from three limitations: (1) incapability of generating image by exemplars; (2) being unable to transfer multiple face attributes simultaneously; (3) low quality of generated images, such as low-resolution or artifacts. To address these limitations, we propose a novel model which receives two images of opposite attributes as inputs. Our model can transfer exactly the same type of attributes from one image to another by exchanging certain part of their encodings. All the attributes are encoded in a disentangled manner in the latent space, which enables us to manipulate several attributes simultaneously. Besides, our model learns the residual images so as to facilitate training on higher resolution images. With the help of multi-scale discriminators for adversarial training, it can even generate high-quality images with finer details and less artifacts. We demonstrate the effectiveness of our model on overcoming the above three limitations by comparing with other methods on the CelebA face database. A pytorch implementation is available at https://github.com/Prinsphield/ELEGANT.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_11');
INSERT INTO `paper` VALUES (10654, 'Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360\\(^\\circ \\) Panoramic Imagery', 'Object detection', 'Panoramic imagery', 'Monocular 3D object detection', 'Style transfer', 'Monocular depth', 'Recent automotive vision work has focused almost exclusively on processing forward-facing cameras. However, future autonomous vehicles will not be viable without a more comprehensive surround sensing, akin to a human driver, as can be provided by 360\\(^\\circ \\)panoramic cameras. We present an approach to adapt contemporary deep network architectures developed on conventional rectilinear imagery to work on equirectangular 360\\(^\\circ \\) panoramic imagery. To address the lack of annotated panoramic automotive datasets availability, we adapt contemporary automotive dataset, via style and projection transformations, to facilitate the cross-domain retraining of contemporary algorithms for panoramic imagery. Following this approach we retrain and adapt existing architectures to recover scene depth and 3D pose of vehicles from monocular panoramic imagery without any panoramic training labels or calibration parameters. Our approach is evaluated qualitatively on crowd-sourced panoramic images and quantitatively using an automotive environment simulator to provide the first benchmark for such techniques within panoramic imagery.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_48');
INSERT INTO `paper` VALUES (10655, 'EmoP3D: A Brain Like Pyramidal Deep Neural Network for Emotion Recognition', 'Emotion recognition', 'Pyramidal neural network', '3DPyraNet', 'Convolutional neural network', '', 'The paper reports a new model based on the understanding and encompassing intelligence from brain i.e. biological pyramidal neurons, tailored for emotion recognition. Our objective is to introduce and utilize usage of non-Convolutional layers in models and show comparable or state-of-the-art performance for multi-class emotion recognition problem. We open-sourced the optimized code for researchers. Our model shows state-of-the-art performance on two emotion recognition datasets (eNTERFACE and Youtube) enhancing previous best result by \\(9.47\\%\\) and \\(20.8\\%\\), respectively.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_46');
INSERT INTO `paper` VALUES (10656, 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation', 'Semantic image segmentation', 'Spatial pyramid pooling', 'Encoder-decoder', 'Depthwise separable convolution', '', 'Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_49');
INSERT INTO `paper` VALUES (10657, 'End-to-End 6-DoF Object Pose Estimation Through Differentiable Rasterization', '6-DoF pose estimation', 'Differentiable rendering', '', '', '', 'Here we introduce an approximated differentiable renderer to refine a 6-DoF pose prediction using only 2D alignment information. To this end, a two-branched convolutional encoder network is employed to jointly estimate the object class and its 6-DoF pose in the scene. We then propose a new formulation of an approximated differentiable renderer to re-project the 3D object on the image according to its predicted pose; in this way the alignment error between the observed and the re-projected object silhouette can be measured. Since the renderer is differentiable, it is possible to back-propagate through it to correct the estimated pose at test time in an online learning fashion. Eventually we show how to leverage the classification branch to profitably re-project a representative model of the predicted class (i.e. a medoid) instead. Each object in the scene is processed independently and novel viewpoints in which both objects arrangement and mutual pose are preserved can be rendered.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_53');
INSERT INTO `paper` VALUES (10658, 'End-to-End Deep Structured Models for Drawing Crosswalks', 'Deep structured models', 'Convolutional neural networks', 'Drawing crosswalks', 'Mapping', 'Autonomous vehicles', 'In this paper we address the problem of detecting crosswalks from LiDAR and camera imagery. Towards this goal, given multiple LiDAR sweeps and the corresponding imagery, we project both inputs onto the ground surface to produce a top down view of the scene. We then leverage convolutional neural networks to extract semantic cues about the location of the crosswalks. These are then used in combination with road centerlines from freely available maps (e.g., OpenStreetMaps) to solve a structured optimization problem which draws the final crosswalk boundaries. Our experiments over crosswalks in a large city area show that 96.6% automation can be achieved.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_25');
INSERT INTO `paper` VALUES (10659, 'End-to-End Incremental Learning', 'Incremental learning', 'CNN', 'Distillation loss', 'Image classification', '', 'Although deep learning approaches have stood out in recent years due to their state-of-the-art results, they continue to suffer from catastrophic forgetting, a dramatic decrease in overall performance when training with new classes added incrementally. This is due to current neural network architectures requiring the entire dataset, consisting of all the samples from the old as well as the new classes, to update the model—a requirement that becomes easily unsustainable as the number of classes grows. We address this issue with our approach to learn deep neural networks incrementally, using new data and only a small exemplar set corresponding to samples from the old classes. This is based on a loss composed of a distillation measure to retain the knowledge acquired from the old classes, and a cross-entropy loss to learn the new classes. Our incremental training is achieved while keeping the entire framework end-to-end, i.e., learning the data representation and the classifier jointly, unlike recent methods with no such guarantees. We evaluate our method extensively on the CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and show state-of-the-art performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_15');
INSERT INTO `paper` VALUES (10660, 'End-to-End Joint Semantic Segmentation of Actors and Actions in Video', 'Semantic segmentation', 'Actor', 'Action', 'Video', 'End-to-End', 'Traditional video understanding tasks include human action recognition and actor/object semantic segmentation. However, the combined task of providing semantic segmentation for different actor classes simultaneously with their action class remains a challenging but necessary task for many applications. In this work, we propose a new end-to-end architecture for tackling this task in videos. Our model effectively leverages multiple input modalities, contextual information, and multitask learning in the video to directly output semantic segmentations in a single unified framework. We train and benchmark our model on the Actor-Action Dataset (A2D) for joint actor-action semantic segmentation, and demonstrate state-of-the-art performance for both segmentation and detection. We also perform experiments verifying our approach improves performance for zero-shot recognition, indicating generalizability of our jointly learned feature space.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_43');
INSERT INTO `paper` VALUES (10661, 'End-to-End Learning of Driving Models with Surround-View Cameras and Route Planners', 'Autonomous driving', 'End-to-end learning of driving', 'Route planning for driving', 'Surround-view cameras', 'Driving dataset', 'For human drivers, having rear and side-view mirrors is vital for safe driving. They deliver a more complete view of what is happening around the car. Human drivers also heavily exploit their mental map for navigation. Nonetheless, several methods have been published that learn driving models with only a front-facing camera and without a route planner. This lack of information renders the self-driving task quite intractable. We investigate the problem in a more realistic setting, which consists of a surround-view camera system with eight cameras, a route planner, and a CAN bus reader. In particular, we develop a sensor setup that provides data for a 360-degree view of the area surrounding the vehicle, the driving route to the destination, and low-level driving maneuvers (e.g. steering angle and speed) by human drivers. With such a sensor setup we collect a new driving dataset, covering diverse driving scenarios and varying weather/illumination conditions. Finally, we learn a novel driving model by integrating information from the surround-view cameras and the route planner. Two route planners are exploited: (1) by representing the planned routes on OpenStreetMap as a stack of GPS coordinates, and (2) by rendering the planned routes on TomTom Go Mobile and recording the progression into a video. Our experiments show that: (1) 360-degree surround-view cameras help avoid failures made with a single front-view camera, in particular for city driving and intersection scenarios; and (2) route planners help the driving task significantly, especially for steering angle prediction. Code, data and more visual results will be made available at http://www.vision.ee.ethz.ch/~heckers/Drive360.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_27');
INSERT INTO `paper` VALUES (10662, 'End-to-End Trained CNN Encoder-Decoder Networks for Image Steganography', 'Steganography', 'CNN', 'Encoder-decoder', 'Deep neural networks', '', 'All the existing image steganography methods use manually crafted features to hide binary payloads into cover images. This leads to small payload capacity and image distortion. Here we propose a convolutional neural network based encoder-decoder architecture for embedding of images as payload. To this end, we make following three major contributions: (i) we propose a deep learning based generic encoder-decoder architecture for image steganography; (ii) we introduce a new loss function that ensures joint end-to-end training of encoder-decoder networks; (iii) we perform extensive empirical evaluation of proposed architecture on a range of challenging publicly available datasets (MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art payload capacity at high PSNR and SSIM values.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_64');
INSERT INTO `paper` VALUES (10663, 'End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN', 'View synthesis', 'Light Field', 'End-to-end', 'Pseudo 4DCNN', '', 'Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2 dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_21');
INSERT INTO `paper` VALUES (10664, 'Enhanced Two-Stage Multi-person Pose Estimation', 'Multi-person pose estimation', 'Keypoint detection', '', '', '', 'In this paper we introduce an enhanced multi-person pose estimation method for the competition of the PoseTrack [6] workshop in ECCV 2018. We employ a two-stage human pose detector, where human region detection and keypoint detection are separately performed. A strong encoder-decoder network for keypoint detection has achieved 70.4% mAP for PoseTrack 2018 validation dataset.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_18');
INSERT INTO `paper` VALUES (10665, 'Escaping from Collapsing Modes in a Constrained Space', '', '', '', '', '', 'Generative adversarial networks (GANs) often suffer from unpredictable mode-collapsing during training. We study the issue of mode collapse of Boundary Equilibrium Generative Adversarial Network (BEGAN), which is one of the state-of-the-art generative models. Despite its potential of generating high-quality images, we find that BEGAN tends to collapse at some modes after a period of training. We propose a new model, called BEGAN with a Constrained Space (BEGAN-CS), which includes a latent-space constraint in the loss function. We show that BEGAN-CS can significantly improve training stability and suppress mode collapse without either increasing the model complexity or degrading the image quality. Further, we visualize the distribution of latent vectors to elucidate the effect of latent-space constraint. The experimental results show that our method has additional advantages of being able to train on small datasets and to generate images similar to a given real image yet with variations of designated attributes on-the-fly.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_13');
INSERT INTO `paper` VALUES (10666, 'ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation', '', '', '', '', '', 'We introduce a fast and efficient convolutional neural network, ESPNet, for semantic segmentation of high resolution images under resource constraints. ESPNet is based on a new convolutional module, efficient spatial pyramid (ESP), which is efficient in terms of computation, memory, and power. ESPNet is 22 times faster (on a standard GPU) and 180 times smaller than the state-of-the-art semantic segmentation network PSPNet, while its category-wise accuracy is only 8% less. We evaluated ESPNet on a variety of semantic segmentation datasets including Cityscapes, PASCAL VOC, and a breast biopsy whole slide image dataset. Under the same constraints on memory and computation, ESPNet outperforms all the current efficient CNN networks such as MobileNet, ShuffleNet, and ENet on both standard metrics and our newly introduced performance metrics that measure efficiency on edge devices. Our network can process high resolution images at a rate of 112 and 9 frames per second on a standard GPU and edge device, respectively. Our code is open-source and available at https://sacmehta.github.io/ESPNet/.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_34');
INSERT INTO `paper` VALUES (10667, 'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks', '', '', '', '', '', 'The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN – network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge (region 3) with the best perceptual index. The code is available at https://github.com/xinntao/ESRGAN.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_5');
INSERT INTO `paper` VALUES (10668, 'Estimating 2D Multi-hand Poses from Single Depth Images', 'Multi-hand pose estimation', 'Pictorial Structure', 'Mask R-CNN', '', '', 'We present a novel framework based on Pictorial Structure (PS) models to estimate 2D multi-hand poses from depth images. Most existing single-hand pose estimation algorithms are either subject to strong assumptions or depend on a weak detector to detect the human hand. We utilize Mask R-CNN to avoid both aforementioned constraints. The proposed framework allows detection of multi-hand instances and localization of hand joints simultaneously. Our experiments show that our method is superior to existing methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_17');
INSERT INTO `paper` VALUES (10669, 'Estimating Depth from RGB and Sparse Sensing', 'Sparse-to-dense depth', 'Depth estimation', 'Deep learning', '', '', 'We present a deep model that can accurately produce dense depth maps given an RGB image with known depth at a very sparse set of pixels. The model works simultaneously for both indoor/outdoor scenes and produces state-of-the-art dense depth maps at nearly real-time speeds on both the NYUv2 and KITTI datasets. We surpass the state-of-the-art for monocular depth estimation even with depth values for only 1 out of every \\({\\sim }10000\\) image pixels, and we outperform other sparse-to-dense depth methods at all sparsity levels. With depth values for \\(1{\\slash }256\\) of the image pixels, we achieve a mean error of less than \\(1\\%\\) of actual depth on indoor scenes, comparable to the performance of consumer-grade depth sensor hardware. Our experiments demonstrate that it would indeed be possible to efficiently transform sparse depth measurements obtained using e.g. lower-power depth sensors or SLAM systems into high-quality dense depth maps.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_11');
INSERT INTO `paper` VALUES (10670, 'Estimating the Success of Unsupervised Image to Image Translation', 'Unsupervised learning', 'Generalization bounds', 'Image to image translation', 'GANs', '', 'While in supervised learning, the validation error is an unbiased estimator of the generalization (test) error and complexity-based generalization bounds are abundant, no such bounds exist for learning a mapping in an unsupervised way. As a result, when training GANs and specifically when using GANs for learning to map between domains in a completely unsupervised way, one is forced to select the hyperparameters and the stopping epoch by subjectively examining multiple options. We propose a novel bound for predicting the success of unsupervised cross domain mapping methods, which is motivated by the recently proposed Simplicity Principle. The bound can be applied both in expectation, for comparing hyperparameters and for selecting a stopping criterion, or per sample, in order to predict the success of a specific cross-domain translation. The utility of the bound is demonstrated in an extensive set of experiments employing multiple recent algorithms. Our code is available at https://github.com/sagiebenaim/gan_bound.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_14');
INSERT INTO `paper` VALUES (10671, 'Evaluating Capability of Deep Neural Networks for Image Classification via Information Plane', 'Information bottleneck', 'Mutual information', 'Neural networks', 'Image classification', '', 'Inspired by the pioneering work of information bottleneck principle for Deep Neural Networks (DNNs) analysis, we design an information plane based framework to evaluate the capability of DNNs for image classification tasks, which not only helps understand the capability of DNNs, but also helps us choose a neural network which leads to higher classification accuracy more efficiently. Further, with experiments, the relationship among the model accuracy, I(X; T) and I(T; Y) are analyzed, where I(X; T) and I(T; Y) are the mutual information of DNN’s output T with input X and label Y. We also show the information plane is more informative than loss curve and apply mutual information to infer the model’s capability for recognizing objects of each class. Our studies would facilitate a better understanding of DNNs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_11');
INSERT INTO `paper` VALUES (10672, 'Evaluation of CNN-Based Single-Image Depth Estimation Methods', 'Single-image depth estimation', 'Deep learning', 'CNN', 'RGB-D', 'Benchmark', 'While an increasing interest in deep models for single-image depth estimation (SIDE) can be observed, established schemes for their evaluation are still limited. We propose a set of novel quality criteria, allowing for a more detailed analysis by focusing on specific characteristics of depth maps. In particular, we address the preservation of edges and planar regions, depth consistency, and absolute distance accuracy. In order to employ these metrics to evaluate and compare state-of-the-art SIDE approaches, we provide a new high-quality RGB-D dataset. We used a digital single-lens reflex (DSLR) camera together with a laser scanner to acquire high-resolution images and highly accurate depth maps. Experimental results show the validity of our proposed evaluation protocol.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_25');
INSERT INTO `paper` VALUES (10673, 'Event Extraction Using Transportation of Temporal Optical Flow Fields', '', '', '', '', '', 'In this paper, we develop a method to transform a sequence of images to a sequence of events. Optical flow, which is the vector fields of pointwise motion computed from monocular image sequences, describes pointwise motion in an environment. The method extracts the global smoothness and continuity of motion fields and detects collapses of the smoothness of the motion fields in long-time image sequences using transportation of the temporal optical flow field.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_52');
INSERT INTO `paper` VALUES (10674, 'Every Pixel Counts: Unsupervised Geometry Learning with Holistic 3D Motion Understanding', '', '', '', '', '', 'Learning to estimate 3D geometry in a single image by watching unlabeled videos via deep convolutional network has made significant process recently. Current state-of-the-art (SOTA) methods, are based on the learning framework of rigid structure-from-motion, where only 3D camera ego motion is modeled for geometry estimation. However, moving objects also exist in many videos, e.g. moving cars in a street scene. In this paper, we tackle such motion by additionally incorporating per-pixel 3D object motion into the learning framework, which provides holistic 3D scene flow understanding and helps single image geometry estimation. Specifically, given two consecutive frames from a video, we adopt a motion network to predict their relative 3D camera pose and a segmentation mask distinguishing moving objects and rigid background. An optical flow network is used to estimate dense 2D per-pixel correspondence. A single image depth network predicts depth maps for both images. The four types of information, i.e. 2D flow, camera pose, segment mask and depth maps, are integrated into a differentiable holistic 3D motion parser (HMP), where per-pixel 3D motion for rigid background and moving objects are recovered. We design various losses w.r.t. the two types of 3D motions for training the depth and motion networks, yielding further error reduction for estimated geometry. Finally, in order to solve the 3D motion confusion from monocular videos, we combine stereo images into joint training. Experiments on KITTI 2015 dataset show that our estimated geometry, 3D motion and moving object masks, not only are constrained to be consistent, but also significantly outperforms other SOTA algorithms, demonstrating the benefits of our approach.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_43');
INSERT INTO `paper` VALUES (10675, 'ExFuse: Enhancing Feature Fusion for Semantic Segmentation', 'Semantic segmentation', 'Convolutional neural networks', '', '', '', 'Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and high-resolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_17');
INSERT INTO `paper` VALUES (10676, 'Explainable Neural Computation via Stack Neural Module Networks', 'Neural module networks', 'Visual question answering', 'Interpretable reasoning', '', '', 'In complex inferential tasks like question answering, machine learning models must confront two challenges: the need to implement a compositional reasoning process, and, in many applications, the need for this reasoning process to be interpretable to assist users in both development and prediction. Existing models designed to produce interpretable traces of their decision-making process typically require these traces to be supervised at training time. In this paper, we present a novel neural modular approach that performs compositional reasoning by automatically inducing a desired sub-task decomposition without relying on strong supervision. Our model allows linking different reasoning tasks though shared modules that handle common routines across tasks. Experiments show that the model is more interpretable to human evaluators compared to other state-of-the-art models: users can better understand the model’s underlying reasoning procedure and predict when it will succeed or fail based on observing its intermediate outputs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_4');
INSERT INTO `paper` VALUES (10677, 'ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations', 'Neural networks', 'Model interpretation', '', '', '', 'We introduce a new method for interpreting computer vision models: visually perceptible, decision-boundary crossing transformations. Our goal is to answer a simple question: why did a model classify an image as being of class A instead of class B? Existing approaches to model interpretation, including saliency and explanation-by-nearest neighbor, fail to visually illustrate examples of transformations required for a specific input to alter a model’s prediction. On the other hand, algorithms for creating decision-boundary crossing transformations (e.g., adversarial examples) produce differences that are visually imperceptible and do not enable insightful explanation. To address this we introduce ExplainGAN, a generative model that produces visually perceptible decision-boundary crossing transformations. These transformations provide high-level conceptual insights which illustrate how a model makes decisions. We validate our model using both traditional quantitative interpretation metrics and introduce a new validation scheme for our approach and generative models more generally.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_41');
INSERT INTO `paper` VALUES (10678, 'Exploiting Multi-layer Features Using a CNN-RNN Approach for RGB-D Object Recognition', 'Convolutional neural network', 'Recursive neural network', 'Transfer learning', 'RGB-D object recognition', '', 'This paper proposes an approach for RGB-D object recognition by integrating a CNN model with recursive neural networks. It first employs a pre-trained CNN model as the underlying feature extractor to get visual features at different layers for RGB and depth modalities. Then, a deep recursive model is applied to map these features into high-level representations. Finally, multi-level information is fused to produce a strong global representation of the entire object image. In order to utilize the CNN model trained on large-scale RGB datasets for depth domain, depth images are converted to a representation similar to RGB images. Experimental results on the Washington RGB-D Object dataset show that the proposed approach outperforms previous approaches.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_51');
INSERT INTO `paper` VALUES (10679, 'Exploiting Single Image Depth Prediction for Mono-stixel Estimation', 'Mono-stixel', 'Single image depth prediction', 'Scene reconstruction', 'Scene flow', 'Monocamera', 'The stixel-world is a compact and detailed environment representation specially designed for street scenes and automotive vision applications. A recent work proposes a monocamera based stixel estimation method based on the structure from motion principle and scene model to predict the depth and translational motion of the static and dynamic parts of the scene. In this paper, we propose to exploit the recent advantages in deep learning based single image depth prediction for mono-stixel estimation. In our approach, the mono-stixels are estimated based on the single image depth predictions, a dense optical flow field and semantic segmentation supported by the prior knowledge about the characteristic of typical street scenes. To provide a meaningful estimation, it is crucial to model the statistical distribution of all measurements, which is especially challenging for the single image depth predictions. Therefore, we present a semantic class dependent measurement model of the single image depth prediction derived from the empirical error distribution on the Kitti dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_14');
INSERT INTO `paper` VALUES (10680, 'Exploiting Temporal Information for 3D Human Pose Estimation', '3D human pose', 'Sequence-to-sequence networks', 'Layer normalized LSTM', 'Residual connections', '', 'In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately \\(12.2\\%\\) and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_5');
INSERT INTO `paper` VALUES (10681, 'Exploiting Vector Fields for Geometric Rectification of Distorted Document Images', 'Document image processing', 'Geometric rectification', 'Vector fields', '3D shape recovery', 'OCR', 'This paper proposes a segment-free method for geometric rectification of a distorted document image captured by a hand-held camera. The method can recover the 3D page shape by exploiting the intrinsic vector fields of the image. Based on the assumption that the curled page shape is a general cylindrical surface, we estimate the parameters related to the camera and the 3D shape model through weighted majority voting on the vector fields. Then the spatial directrix of the surface is recovered by solving an ordinary differential equation (ODE) through the Euler method. Finally, the geometric distortions in images can be rectified by flattening the estimated 3D page surface onto a plane. Our method can exploit diverse types of visual cues available in a distorted document image to estimate its vector fields for 3D page shape recovery. In comparison to the state-of-the-art methods, the great advantage is that it is a segment-free method and does not have to extract curved text lines or textual blocks, which is still a very challenging problem especially for a distorted document image. Our method can therefore be freely applied to document images with extremely complicated page layouts and severe image quality degradation. Extensive experiments are implemented to demonstrate the effectiveness of the proposed method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_11');
INSERT INTO `paper` VALUES (10682, 'Exploring Bias in Primate Face Detection and Recognition', 'Animal biometrics', 'Deep learning', 'Biometrics', 'Bias', 'Face detection', 'Deforestation and loss of habitat have resulted in rapid decline of certain species of primates in forests. On the other hand, uncontrolled growth of a few species of primates in urban areas has led to safety issues and nuisance for the local residents. Hence, identifying individual primates has become the need of the hour - not only for conservation and effective mitigation in the wild but also in zoological parks and wildlife sanctuaries. Primates and human faces share a lot of common features like position and shape of eyes, nose and mouth. It is worth exploring whether the knowledge of human faces and recent methods learned from human face detection and recognition can be extended to primate faces. However, similar challenges relating to bias in human faces will also occur in primates. The quality and orientation of primate images along with different species of primates - ranging from monkeys to gorillas and chimpanzees will contribute to bias in effective detection and recognition. Experimental results on a primate dataset of over 80 identities show the effect of bias in this research problem.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_33');
INSERT INTO `paper` VALUES (10683, 'Exploring the Limits of Weakly Supervised Pretraining', '', '', '', '', '', 'State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards “small”. Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_12');
INSERT INTO `paper` VALUES (10684, 'Exploring Visual Relationship for Image Captioning', 'Image captioning', 'Graph convolutional networks', 'Visual relationship', 'Long short-term memory', '', 'It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_42');
INSERT INTO `paper` VALUES (10685, 'Extended Non-local Feature for Visual Saliency Detection in Low Contrast Images', 'Deep learning', 'Non-local feature', 'Saliency detection', 'Low contrast images', '', 'Saliency detection model can substantially facilitate a wide range of applications. Conventional saliency detection models primarily rely on high level features from deep learning and hand-crafted low-level image features. However, they may face great challenges in nighttime scenario, due to the lack of well-defined feature to represent saliency information in low contrast images. This paper proposes a saliency detection model for nighttime scene. This model is capable of extracting non-local feature that is jointly learned with local features under a unified deep learning framework. The key idea of the proposed model is to hierarchically introduce non-local module with local contrast processing blocks, aiming to provide robust representation of saliency information towards low contrast images with low signal-to-noise ratio property. Besides, both nighttime and daytime images are utilized in training to provide complementary information. Extensive experiments have been conducted on five challenging datasets and our nighttime image dataset to evaluate the performance of the proposed model.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_46');
INSERT INTO `paper` VALUES (10686, 'Extending Layered Models to 3D Motion', 'Motion', 'Video segmentation', 'Layered models', '', '', 'We consider the problem of inferring a layered representation, its depth ordering and motion segmentation from video in which objects may undergo 3D non-planar motion relative to the camera. We generalize layered inference to that case and corresponding self-occlusion phenomena. We accomplish this by introducing a flattened 3D object representation, which is a compact representation of an object that contains all visible portions of the object seen in the video, including parts of an object that are self-occluded (as well as occluded) in one frame but seen in another. We formulate the inference of such flattened representations and motion segmentation, and derive an optimization scheme. We also introduce a new depth ordering scheme, which is independent of layered inference and addresses the case of self-occlusion. It requires little computation given the flattened representations. Experiments on benchmark datasets show the advantage of our method over existing layered methods, which do not model 3D motion and self-occlusion.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_27');
INSERT INTO `paper` VALUES (10687, 'Extreme Network Compression via Filter Group Approximation', 'Convolutional neural networks', 'Network compression', 'Low-rank decomposition', 'Filter group convolution', 'Image classification', 'In this paper we propose a novel decomposition method based on filter group approximation, which can significantly reduce the redundancy of deep convolutional neural networks (CNNs) while maintaining the majority of feature representation. Unlike other low-rank decomposition algorithms which operate on spatial or channel dimension of filters, our proposed method mainly focuses on exploiting the filter group structure for each layer. For several commonly used CNN models, including VGG and ResNet, our method can reduce over 80% floating-point operations (FLOPs) with less accuracy drop than state-of-the-art methods on various image classification datasets. Besides, experiments demonstrate that our method is conducive to alleviating degeneracy of the compressed network, which hurts the convergence and performance of the network.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_19');
INSERT INTO `paper` VALUES (10688, 'Face De-spoofing: Anti-spoofing via Noise Modeling', 'Face anti-spoofing', 'Generative model', 'CNN', 'Image decomposition', '', 'Many prior face anti-spoofing works develop discriminative models for recognizing the subtle differences between live and spoof faces. Those approaches often regard the image as an indivisible unit, and process it holistically, without explicit modeling of the spoofing process. In this work, motivated by the noise modeling and denoising algorithms, we identify a new problem of face de-spoofing, for the purpose of anti-spoofing: inversely decomposing a spoof face into a spoof noise and a live face, and then utilizing the spoof noise for classification. A CNN architecture with proper constraints and supervisions is proposed to overcome the problem of having no ground truth for the decomposition. We evaluate the proposed method on multiple face anti-spoofing databases. The results show promising improvements due to our spoof noise modeling. Moreover, the estimated spoof noise provides a visualization which helps to understand the added spoof noise by each spoof medium.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_18');
INSERT INTO `paper` VALUES (10689, 'Face Recognition with Contrastive Convolution', 'Face recognition', 'Convolutional neural networks', 'Contrastive convolution', 'Kernel generator', '', 'In current face recognition approaches with convolutional neural network (CNN), a pair of faces to compare are independently fed into the CNN for feature extraction. For both faces the same kernels are applied and hence the representation of a face stays fixed regardless of whom it is compared with. As for us humans, however, one generally focuses on varied characteristics of a face when comparing it with distinct persons as shown in Fig. 1. Inspired, we propose a novel CNN structure with what we referred to as contrastive convolution, which specifically focuses on the distinct characteristics between the two faces to compare, i.e., those contrastive characteristics. Extensive experiments on the challenging LFW, and IJB-A show that our proposed contrastive convolution significantly improves the vanilla CNN and achieves quite promising performance in face verification task.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_8');
INSERT INTO `paper` VALUES (10690, 'Face Super-Resolution Guided by Facial Component Heatmaps', 'Face', 'Super-resolution', 'Hallucination', 'Facial component localization', 'Multi-task neural networks', 'State-of-the-art face super-resolution methods leverage deep convolutional neural networks to learn a mapping between low-resolution (LR) facial patterns and their corresponding high-resolution (HR) counterparts by exploring local appearance information. However, most of these methods do not account for facial structure and suffer from degradations due to large pose variations and misalignments. In this paper, we propose a method that explicitly incorporates structural information of faces into the face super-resolution process by using a multi-task convolutional neural network (CNN). Our CNN has two branches: one for super-resolving face images and the other branch for predicting salient regions of a face coined facial component heatmaps. These heatmaps encourage the upsampling stream to generate super-resolved faces with higher-quality details. Our method not only uses low-level information (i.e., intensity similarity), but also middle-level information (i.e., face structure) to further explore spatial constraints of facial components from LR inputs images. Therefore, we are able to super-resolve very small unaligned face images \\((16\\,\\times \\,16\\hbox { pixels})\\) with a large upscaling factor of 8\\(\\times \\), while preserving face structure. Extensive experiments demonstrate that our network achieves superior face hallucination results and outperforms the state-of-the-art.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_14');
INSERT INTO `paper` VALUES (10691, 'Faces as Lighting Probes via Unsupervised Deep Highlight Extraction', 'Illumination estimation', 'Unsupervised learning', '', '', '', 'We present a method for estimating detailed scene illumination using human faces in a single image. In contrast to previous works that estimate lighting in terms of low-order basis functions or distant point lights, our technique estimates illumination at a higher precision in the form of a non-parametric environment map. Based on the observation that faces can exhibit strong highlight reflections from a broad range of lighting directions, we propose a deep neural network for extracting highlights from faces, and then trace these reflections back to the scene to acquire the environment map. Since real training data for highlight extraction is very limited, we introduce an unsupervised scheme for finetuning the network on real images, based on the consistent diffuse chromaticity of a given face seen in multiple real images. In tracing the estimated highlights to the environment, we reduce the blurring effect of skin reflectance on reflected light through a deconvolution determined by prior knowledge on face material properties. Comparisons to previous techniques for highlight extraction and illumination estimation show the state-of-the-art performance of this approach on a variety of indoor and outdoor scenes.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_20');
INSERT INTO `paper` VALUES (10692, 'Facial Dynamics Interpreter Network: What Are the Important Relations Between Local Dynamics for Facial Trait Estimation?', 'Facial dynamics', 'Interpretable deep learning', 'Relation between local dynamics', 'Facial trait estimation', '', 'Human face analysis is an important task in computer vision. According to cognitive-psychological studies, facial dynamics could provide crucial cues for face analysis. The motion of a facial local region in facial expression is related to the motion of other facial local regions. In this paper, a novel deep learning approach, named facial dynamics interpreter network, has been proposed to interpret the important relations between local dynamics for estimating facial traits from expression sequence. The facial dynamics interpreter network is designed to be able to encode a relational importance, which is used for interpreting the relation between facial local dynamics and estimating facial traits. By comparative experiments, the effectiveness of the proposed method has been verified. The important relations between facial local dynamics are investigated by the proposed facial dynamics interpreter network in gender classification and age estimation. Moreover, experimental results show that the proposed method outperforms the state-of-the-art methods in gender classification and age estimation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_29');
INSERT INTO `paper` VALUES (10693, 'Facial Expression Recognition with Inconsistently Annotated Datasets', '', '', '', '', '', 'Annotation errors and bias are inevitable among different facial expression datasets due to the subjectiveness of annotating facial expressions. Ascribe to the inconsistent annotations, performance of existing facial expression recognition (FER) methods cannot keep improving when the training set is enlarged by merging multiple datasets. To address the inconsistency, we propose an Inconsistent Pseudo Annotations to Latent Truth (IPA2LT) framework to train a FER model from multiple inconsistently labeled datasets and large scale unlabeled data. In IPA2LT, we assign each sample more than one labels with human annotations or model predictions. Then, we propose an end-to-end LTNet with a scheme of discovering the latent truth from the inconsistent pseudo labels and the input face images. To our knowledge, IPA2LT serves as the first work to solve the training problem with inconsistently labeled FER datasets. Experiments on synthetic data validate the effectiveness of the proposed method in learning from inconsistent labels. We also conduct extensive experiments in FER and show that our method outperforms other state-of-the-art and optional methods under a rigorous evaluation protocol involving 7 FER datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_14');
INSERT INTO `paper` VALUES (10694, 'Factorizable Net: An Efficient Subgraph-Based Framework for Scene Graph Generation', 'Visual Relationship Detection', 'Scene graph generation', 'Scene understanding', 'Object interactions', 'Language and vision', 'Generating scene graph to describe the object interactions inside an image gains increasing interests these years. However, most of the previous methods use complicated structures with slow inference speed or rely on the external data, which limits the usage of the model in real-life scenarios. To improve the efficiency of scene graph generation, we propose a subgraph-based connection graph to concisely represent the scene graph during the inference. A bottom-up clustering method is first used to factorize the entire graph into subgraphs, where each subgraph contains several objects and a subset of their relationships. By replacing the numerous relationship representations of the scene graph with fewer subgraph and object features, the computation in the intermediate stage is significantly reduced. In addition, spatial information is maintained by the subgraph features, which is leveraged by our proposed Spatial-weighted Message Passing (SMP) structure and Spatial-sensitive Relation Inference (SRI) module to facilitate the relationship recognition. On the recent Visual Relationship Detection and Visual Genome datasets, our method outperforms the state-of-the-art method in both accuracy and speed. Code has been made publicly available (https://github.com/yikang-li/FactorizableNet).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_21');
INSERT INTO `paper` VALUES (10695, 'FashionSearchNet: Fashion Search with Attribute Manipulation', 'CNNs', 'Fashion retrieval', 'Similarity learning', 'Attribute localization', '', 'The focus of this paper is on retrieval of fashion images after manipulating attributes of the query images. This task is particularly useful in search scenarios where the user is interested in small variations of an image, i.e., replacing the mandarin collar with a buttondown. Keeping the desired attributes of the query image while manipulating its other attributes is a challenging problem which is accomplished by our proposed network called FashionSearchNet. FashionSearchNet is able to learn attribute specific representations by leveraging on weakly-supervised localization. The localization module is used to ignore the unrelated features of attributes in the feature map, thus improve the similarity learning. Experiments conducted on two recent fashion datasets show that FashionSearchNet outperforms the other state-of-the-art fashion search techniques.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_6');
INSERT INTO `paper` VALUES (10696, 'Fast and Accurate Camera Covariance Computation for Large 3D Reconstruction', 'Uncertainty', 'Covariance propagation', 'Structure from motion', '3D reconstruction', '', 'Estimating uncertainty of camera parameters computed in Structure from Motion (SfM) is an important tool for evaluating the quality of the reconstruction and guiding the reconstruction process. Yet, the quality of the estimated parameters of large reconstructions has been rarely evaluated due to the computational challenges. We present a new algorithm which employs the sparsity of the uncertainty propagation and speeds the computation up about ten times w.r.t. previous approaches. Our computation is accurate and does not use any approximations. We can compute uncertainties of thousands of cameras in tens of seconds on a standard PC. We also demonstrate that our approach can be effectively used for reconstructions of any size by applying it to smaller sub-reconstructions.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_42');
INSERT INTO `paper` VALUES (10697, 'Fast and Accurate Intrinsic Symmetry Detection', 'Intrinsic symmetry', 'Functional map', 'Eigenfunction', '', '', 'In computer vision and graphics, various types of symmetries are extensively studied since symmetry present in objects is a fundamental cue for understanding the shape and the structure of objects. In this work, we detect the intrinsic reflective symmetry in triangle meshes where we have to find the intrinsically symmetric point for each point of the shape. We establish correspondences between functions defined on the shapes by extending the functional map framework and then recover the point-to-point correspondences. Previous approaches using the functional map for this task find the functional correspondences matrix by solving a non-linear optimization problem which makes them slow. In this work, we propose a closed form solution for this matrix which makes our approach faster. We find the closed-form solution based on our following results. If the given shape is intrinsically symmetric, then the shortest length geodesic between two intrinsically symmetric points is also intrinsically symmetric. If an eigenfunction of the Laplace-Beltrami operator for the given shape is an even (odd) function, then its restriction on the shortest length geodesic between two intrinsically symmetric points is also an even (odd) function. The sign of a low-frequency eigenfunction is the same on the neighboring points. Our method is invariant to the ordering of the eigenfunctions and has the least time complexity. We achieve the best performance on the SCAPE dataset and comparable performance with the state-of-the-art methods on the TOSCA dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_26');
INSERT INTO `paper` VALUES (10698, 'Fast and Efficient Image Quality Enhancement via Desubpixel Convolutional Neural Networks', 'Image super-resolution', 'Image enhancement', 'Mobile devices', '', '', 'This paper considers a convolutional neural network for image quality enhancement referred to as the fast and efficient quality enhancement (FEQE) that can be trained for either image super-resolution or image enhancement to provide accurate yet visually pleasing images on mobile devices by addressing the following three main issues. First, the considered FEQE performs majority of its computation in a low-resolution space. Second, the number of channels used in the convolutional layers is small which allows FEQE to be very deep. Third, the FEQE performs downsampling referred to as desubpixel that does not lead to loss of information. Experimental results on a number of standard benchmark datasets show significant improvements in image fidelity and reduction in processing time of the proposed FEQE compared to the recent state-of-the-art methods. In the PIRM 2018 challenge, the proposed FEQE placed first on the image super-resolution task for mobile devices. The code is available at https://github.com/thangvubk/FEQE.git.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_16');
INSERT INTO `paper` VALUES (10699, 'Fast Light Field Reconstruction with Deep Coarse-to-Fine Modeling of Spatial-Angular Clues', 'Light field', 'Deep learning', 'Convolutional neural network', 'Super resolution', 'View synthesis', 'Densely-sampled light fields (LFs) are beneficial to many applications such as depth inference and post-capture refocusing. However, it is costly and challenging to capture them. In this paper, we propose a learning based algorithm to reconstruct a densely-sampled LF fast and accurately from a sparsely-sampled LF in one forward pass. Our method uses computationally efficient convolutions to deeply characterize the high dimensional spatial-angular clues in a coarse-to-fine manner. Specifically, our end-to-end model first synthesizes a set of intermediate novel sub-aperture images (SAIs) by exploring the coarse characteristics of the sparsely-sampled LF input with spatial-angular alternating convolutions. Then, the synthesized intermediate novel SAIs are efficiently refined by further recovering the fine relations from all SAIs via guided residual learning and stride-2 4-D convolutions. Experimental results on extensive real-world and synthetic LF images show that our model can provide more than 3 dB advantage in reconstruction quality in average than the state-of-the-art methods while being computationally faster by a factor of 30. Besides, more accurate depth can be inferred from the reconstructed densely-sampled LFs by our method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_9');
INSERT INTO `paper` VALUES (10700, 'Fast Perceptual Image Enhancement', '', '', '', '', '', 'The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints the mobile phones cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of Ignatov et al., where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3\\(\\times \\) on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_17');
INSERT INTO `paper` VALUES (10701, 'Fast Semantic Segmentation on Video Using Block Motion-Based Feature Interpolation', 'Semantic segmentation', 'Efficient inference', 'Video segmentation', 'Video compression', 'H.264 video', 'Convolutional networks optimized for accuracy on challenging, dense prediction tasks are often prohibitively slow to run on each frame in a video. The spatial similarity of nearby video frames, however, suggests opportunity to reuse computation. Existing work has explored basic feature reuse and feature warping based on optical flow, but has encountered limits to the speedup attainable with these techniques. In this paper, we present a new, two part approach to accelerating inference on video. First, we propose a fast feature propagation technique that utilizes the block motion vectors present in compressed video (e.g. H.264 codecs) to cheaply propagate features from frame to frame. Second, we develop a novel feature estimation scheme, termed feature interpolation, that fuses features propagated from enclosing keyframes to render accurate feature estimates, even at sparse keyframe frequencies. We evaluate our system on the Cityscapes and CamVid datasets, comparing to both a frame-by-frame baseline and related work. We find that we are able to substantially accelerate semantic segmentation on video, achieving twice the average inference speed as prior work at any target accuracy level.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_1');
INSERT INTO `paper` VALUES (10702, 'Fast, Accurate, and Lightweight Super-Resolution with Cascading Residual Network', 'Super-resolution', 'Deep convolutional neural network', '', '', '', 'In recent years, deep learning methods have been successfully applied to single-image super-resolution tasks. Despite their great performances, deep learning methods cannot be easily applied to real-world applications due to the requirement of heavy computation. In this paper, we address this issue by proposing an accurate and lightweight deep network for image super-resolution. In detail, we design an architecture that implements a cascading mechanism upon a residual network. We also present variant models of the proposed cascading residual network to further improve efficiency. Our extensive experiments show that even with much fewer parameters and operations, our models achieve performance comparable to that of state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_16');
INSERT INTO `paper` VALUES (10703, 'Fast, Visual and Interactive Semi-supervised Dimensionality Reduction', 'Interactive dimensionality reduction', 'Similarity-embeddings', '', '', '', 'Recent advances in machine learning allow us to analyze and describe the content of high-dimensional data ranging from images and video to text and audio data. In order to visualize that data in 2D or 3D, usually Dimensionality Reduction (DR) techniques are employed. Most of these techniques produce static projections without taking into account corrections from humans or other data exploration scenarios. In this work, we propose a novel interactive DR framework that is able to learn the optimal projection by exploiting the user interactions with the projected data. We evaluate the proposed method under a widely used interaction scenario in multidimensional projection literature, i.e., project a subset of the data, rearrange them better in classes, and then project the rest of the dataset, and we show that it greatly outperforms competitive baseline and state-of-the-art techniques, while also being able to readily adapt to the computational requirements of different applications.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_44');
INSERT INTO `paper` VALUES (10704, 'Feature2Mass: Visual Feature Processing in Latent Space for Realistic Labeled Mass Generation', 'Feature processing in latent space', 'Image synthesis', 'Bio-image generation', 'Medical mass generation', '', 'This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be well-behaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: (1) The generated bio-image does not seem realistic; (2) the variation of generated bio-image is limited; and (3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_22');
INSERT INTO `paper` VALUES (10705, 'Few-Shot Human Motion Prediction via Meta-learning', 'Human motion prediction', 'Few-shot learning', 'Meta-learning', '', '', 'Human motion prediction, forecasting human motion in a few milliseconds conditioning on a historical 3D skeleton sequence, is a long-standing problem in computer vision and robotic vision. Existing forecasting algorithms rely on extensive annotated motion capture data and are brittle to novel actions. This paper addresses the problem of few-shot human motion prediction, in the spirit of the recent progress on few-shot learning and meta-learning. More precisely, our approach is based on the insight that having a good generalization from few examples relies on both a generic initial model and an effective strategy for adapting this model to novel tasks. To accomplish this, we propose proactive and adaptive meta-learning (PAML) that introduces a novel combination of model-agnostic meta-learning and model regression networks and unifies them into an integrated, end-to-end framework. By doing so, our meta-learner produces a generic initial model through aggregating contextual information from a variety of prediction tasks, while effectively adapting this model for use as a task-specific one by leveraging learning-to-learn knowledge about how to transform few-shot model parameters to many-shot model parameters. The resulting PAML predictor model significantly improves the prediction performance on the heavily benchmarked H3.6M dataset in the small-sample size regime.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_27');
INSERT INTO `paper` VALUES (10706, 'Fictitious GAN: Training GANs with Historical Models', '', '', '', '', '', 'Generative adversarial networks (GANs) are powerful tools for learning generative models. In practice, the training may suffer from lack of convergence. GANs are commonly viewed as a two-player zero-sum game between two neural networks. Here, we leverage this game theoretic view to study the convergence behavior of the training process. Inspired by the fictitious play learning process, a novel training method, referred to as Fictitious GAN, is introduced. Fictitious GAN trains the deep neural networks using a mixture of historical models. Specifically, the discriminator (resp. generator) is updated according to the best-response to the mixture outputs from a sequence of previously trained generators (resp. discriminators). It is shown that Fictitious GAN can effectively resolve some convergence issues that cannot be resolved by the standard training approach. It is proved that asymptotically the average of the generator outputs has the same distribution as the data samples.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_8');
INSERT INTO `paper` VALUES (10707, 'Fighting Fake News: Image Splice Detection via Learned Self-Consistency', 'Visual forensics', 'Image splicing', 'Self-supervised learning', 'EXIF', '', 'Advances in photo editing and manipulation tools have made it significantly easier to create fake imagery. Learning to detect such manipulations, however, remains a challenging problem due to the lack of sufficient amounts of manipulated training data. In this paper, we propose a learning algorithm for detecting visual image manipulations that is trained only using a large dataset of real photographs. The algorithm uses the automatically recorded photo EXIF metadata as supervisory signal for training a model to determine whether an image is self-consistent — that is, whether its content could have been produced by a single imaging pipeline. We apply this self-consistency model to the task of detecting and localizing image splices. The proposed method obtains state-of-the-art performance on several image forensics benchmarks, despite never seeing any manipulated images at training. That said, it is merely a step in the long quest for a truly general purpose visual forensics tool.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_7');
INSERT INTO `paper` VALUES (10708, 'Filling the Gaps: Predicting Missing Joints of Human Poses Using Denoising Autoencoders', 'Human pose estimation', 'Generative methods', '', '', '', 'State of the art pose estimators are able to deal with different challenges present in real-world scenarios, such as varying body appearance, lighting conditions and rare body poses. However, when body parts are severely occluded by objects or other people, the resulting poses might be incomplete, negatively affecting applications where estimating a full body pose is important (e.g. gesture and pose-based behavior analysis). In this work, we propose a method for predicting the missing joints from incomplete human poses. In our model we consider missing joints as noise in the input and we use an autoencoder-based solution to enhance the pose prediction. The method can be easily combined with existing pipelines and, by using only 2D coordinates as input data, the resulting model is small and fast to train, yet powerful enough to learn a robust representation of the low dimensional domain. Finally, results show improved predictions over existing pose estimation algorithms.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_29');
INSERT INTO `paper` VALUES (10709, 'Find and Focus: Retrieve and Localize Video Events with Natural Language Queries', 'Natural Language Queries', 'Video Retrieval', 'Clip Localization', 'Clipper Proposal', 'Stage Focus', 'The thriving of video sharing services brings new challenges to video retrieval, e.g. the rapid growth in video duration and content diversity. Meeting such challenges calls for new techniques that can effectively retrieve videos with natural language queries. Existing methods along this line, which mostly rely on embedding videos as a whole, remain far from satisfactory for real-world applications due to the limited expressive power. In this work, we aim to move beyond this limitation by delving into the internal structures of both sides, the queries and the videos. Specifically, we propose a new framework called Find and Focus (FIFO), which not only performs top-level matching (paragraph vs. video), but also makes part-level associations, localizing a video clip for each sentence in the query with the help of a focusing guide. These levels are complementary – the top-level matching narrows the search while the part-level localization refines the results. On both ActivityNet Captions and modified LSMDC datasets, the proposed framework achieves remarkable performance gains (Project Page: https://ycxioooong.github.io/projects/fifo).', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_13');
INSERT INTO `paper` VALUES (10710, 'Fine-Grained Vehicle Classification with Unsupervised Parts Co-occurrence Learning', '', '', '', '', '', 'Vehicle fine-grained classification is a challenging research problem with little attention in the field. In this paper, we propose a deep network architecture for vehicles fine-grained classification without the need of parts or 3D bounding boxes annotation. Co-occurrence layer (COOC) layer is exploited for unsupervised parts discovery. In addition, a two-step procedure with transfer learning and fine-tuning is utilized. This enables us to better fine-tune models with pre-trained weights on ImageNet in some layers while having random initialization in some others. Our model achieves 86.5% accuracy outperforming the state of the art methods in BoxCars116K by 4%. In addition, we achieve 95.5% and 93.19% on CompCars on both train-test splits, 70-30 and 50-50, outperforming the other methods by 4.5% and 8% respectively.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_54');
INSERT INTO `paper` VALUES (10711, 'Fine-Grained Video Categorization with Redundancy Reduction Attention', 'Fine-grained video categorization', 'Attention mechanism', '', '', '', 'For fine-grained categorization tasks, videos could serve as a better source than static images as videos have a higher chance of containing discriminative patterns. Nevertheless, a video sequence could also contain a lot of redundant and irrelevant frames. How to locate critical information of interest is a challenging task. In this paper, we propose a new network structure, known as Redundancy Reduction Attention (RRA), which learns to focus on multiple discriminative patterns by suppressing redundant feature channels. Specifically, it firstly summarizes the video by weight-summing all feature vectors in the feature maps of selected frames with a spatio-temporal soft attention, and then predicts which channels to suppress or to enhance according to this summary with a learned non-linear transform. Suppression is achieved by modulating the feature maps and threshing out weak activations. The updated feature maps are then used in the next iteration. Finally, the video is classified based on multiple summaries. The proposed method achieves outstanding performances in multiple video classification datasets. Furthermore, we have collected two large-scale video datasets, YouTube-Birds and YouTube-Cars, for future researches on fine-grained video categorization. The datasets are available at http://www.cs.umd.edu/~chenzhu/fgvc.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_9');
INSERT INTO `paper` VALUES (10712, 'Fine-Grained Visual Categorization Using Meta-learning Optimization with Sample Selection of Auxiliary Data', 'Fine-Grained Visual Categorization', 'Meta-learning', 'Sample selection', '', '', 'Fine-grained visual categorization (FGVC) is challenging due in part to the fact that it is often difficult to acquire an enough number of training samples. To employ large models for FGVC without suffering from overfitting, existing methods usually adopt a strategy of pre-training the models using a rich set of auxiliary data, followed by fine-tuning on the target FGVC task. However, the objective of pre-training does not take the target task into account, and consequently such obtained models are suboptimal for fine-tuning. To address this issue, we propose in this paper a new deep FGVC model termed MetaFGNet. Training of MetaFGNet is based on a novel regularized meta-learning objective, which aims to guide the learning of network parameters so that they are optimal for adapting to the target FGVC task. Based on MetaFGNet, we also propose a simple yet effective scheme for selecting more useful samples from the auxiliary data. Experiments on benchmark FGVC datasets show the efficacy of our proposed method.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_15');
INSERT INTO `paper` VALUES (10713, 'FishEyeRecNet: A Multi-context Collaborative Deep Network for Fisheye Image Rectification', 'Fisheye image rectification', 'Distortion parameter estimation', 'Collaborative deep network', '', '', 'Images captured by fisheye lenses violate the pinhole camera assumption and suffer from distortions. Rectification of fisheye images is therefore a crucial preprocessing step for many computer vision applications. In this paper, we propose an end-to-end multi-context collaborative deep network for removing distortions from single fisheye images. In contrast to conventional approaches, which focus on extracting hand-crafted features from input images, our method learns high-level semantics and low-level appearance features simultaneously to estimate the distortion parameters. To facilitate training, we construct a synthesized dataset that covers various scenes and distortion parameter settings. Experiments on both synthesized and real-world datasets show that the proposed model significantly outperforms current state of the art methods. Our code and synthesized dataset will be made publicly available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_29');
INSERT INTO `paper` VALUES (10714, 'FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans', 'Floorplan reconstruction', '3D Computer Vision', '3D CNN', '', '', 'This paper proposes a novel deep neural architecture that automatically reconstructs a floorplan by walking through a house with a smartphone, an ultimate goal of indoor mapping research. The challenge lies in the processing of RGBD streams spanning a large 3D space. The proposed neural architecture, dubbed FloorNet, effectively processes the data through three neural network branches: (1) PointNet with 3D points, exploiting 3D information; (2) CNN with a 2D point density image in a top-down view, enhancing local spatial reasoning; and (3) CNN with RGB images, utilizing full image information. FloorNet exchanges intermediate features across the branches to exploit all the architectures. We have created a benchmark for floorplan reconstruction by acquiring RGBD video streams for 155 residential houses or apartments with Google Tango phones and annotating complete floorplan information. Our qualitative and quantitative evaluations demonstrate that the fusion of three branches effectively improves the reconstruction quality. We hope that the paper together with the benchmark will be an important step towards solving a challenging vector-graphics floorplan reconstruction problem.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_13');
INSERT INTO `paper` VALUES (10715, 'Flow-Grounded Spatial-Temporal Video Prediction from Still Images', 'Future prediction', 'Conditional variational autoencoder', '3D convolutions', '', '', 'Existing video prediction methods mainly rely on observing multiple historical frames or focus on predicting the next one-frame. In this work, we study the problem of generating consecutive multiple future frames by observing one single still image only. We formulate the multi-frame prediction task as a multiple time step flow (multi-flow) prediction phase followed by a flow-to-frame synthesis phase. The multi-flow prediction is modeled in a variational probabilistic manner with spatial-temporal relationships learned through 3D convolutions. The flow-to-frame synthesis is modeled as a generative process in order to keep the predicted results lying closer to the manifold shape of real video sequence. Such a two-phase design prevents the model from directly looking at the high-dimensional pixel space of the frame sequence and is demonstrated to be more effective in predicting better and diverse results. Extensive experimental results on videos with different types of motion show that the proposed algorithm performs favorably against existing methods in terms of quality, diversity and human perceptual evaluation.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_37');
INSERT INTO `paper` VALUES (10716, 'Focus, Segment and Erase: An Efficient Network for Multi-label Brain Tumor Segmentation', 'Brain tumor segmentation', 'Convolutional neural network', 'Class imbalance', 'Inter-class interference', '', 'In multi-label brain tumor segmentation, class imbalance and inter-class interference are common and challenging problems. In this paper, we propose a novel end-to-end trainable network named FSENet to address the aforementioned issues. The proposed FSENet has a tumor region pooling component to restrict the prediction within the tumor region (“focus”), thus mitigating the influence of the dominant non-tumor region. Furthermore, the network decomposes the more challenging multi-label brain tumor segmentation problem into several simpler binary segmentation tasks (“segment”), where each task focuses on a specific tumor tissue. To alleviate inter-class interference, we adopt a simple yet effective idea in our work: we erase the segmented regions before proceeding to further segmentation of tumor tissue (“erase”), thus reduces competition among different tumor classes. Our single-model FSENet ranks \\(3^{rd}\\) on the multi-modal brain tumor segmentation benchmark 2015 (BraTS 2015) without relying on ensembles or complicated post-processing steps.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_40');
INSERT INTO `paper` VALUES (10717, 'Folded Recurrent Neural Networks for Future Video Prediction', 'Future video prediction', 'Unsupervised learning', 'Recurrent neural networks', '', '', 'This work introduces double-mapping Gated Recurrent Units (dGRU), an extension of standard GRUs where the input is considered as a recurrent state. An extra set of logic gates is added to update the input given the output. Stacking multiple such layers results in a recurrent auto-encoder: the operators updating the outputs comprise the encoder, while the ones updating the inputs form the decoder. Since the states are shared between corresponding encoder and decoder layers, the representation is stratified during learning: some information is not passed to the next layers. We test our model on future video prediction. Main challenges for this task include high variability in videos, temporal propagation of errors, and non-specificity of future frames. We show how only the encoder or decoder needs to be applied for encoding or prediction. This reduces the computational cost and avoids re-encoding predictions when generating multiple frames, mitigating error propagation. Furthermore, it is possible to remove layers from a trained model, giving an insight to the role of each layer. Our approach improves state of the art results on MMNIST and UCF101, being competitive on KTH with 2 and 3 times less memory usage and computational cost than the best scored approach.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_44');
INSERT INTO `paper` VALUES (10718, 'Forecasting Hands and Objects in Future Frames', 'Future location forecast', 'Activity prediction', 'Object forecast', '', '', 'This paper presents an approach to forecast future presence and location of human hands and objects. Given an image frame, the goal is to predict what objects will appear in the future frame (e.g., 5 s later) and where they will be located at, even when they are not visible in the current frame. The key idea is that (1) an intermediate representation of a convolutional object recognition model abstracts scene information in its frame and that (2) we can predict (i.e., regress) such representations corresponding to the future frames based on that of the current frame. We present a new two-stream fully convolutional neural network (CNN) architecture designed for forecasting future objects given a video. The experiments confirm that our approach allows reliable estimation of future objects in videos, obtaining much higher accuracy compared to the state-of-the-art future object presence forecast method on public datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_12');
INSERT INTO `paper` VALUES (10719, 'ForestHash: Semantic Hashing with Shallow Random Forests and Tiny Convolutional Networks', '', '', '', '', '', 'In this paper, we introduce a random forest semantic hashing scheme that embeds tiny convolutional neural networks (CNN) into shallow random forests. A binary hash code for a data point is obtained by a set of decision trees, setting ‘1’ for the visited tree leaf, and ‘0’ for the rest. We propose to first randomly group arriving classes at each tree split node into two groups, obtaining a significantly simplified two-class classification problem that can be a handled with a light-weight CNN weak learner. Code uniqueness is achieved via the random class grouping, whilst code consistency is achieved using a low-rank loss in the CNN weak learners that encourages intra-class compactness for the two random class groups. Finally, we introduce an information-theoretic approach for aggregating codes of individual trees into a single hash code, producing a near-optimal unique hash for each class. The proposed approach significantly outperforms state-of-the-art hashing methods for image retrieval tasks on large-scale public datasets, and is comparable to image classification methods while utilizing a more compact, efficient and scalable representation. This work proposes a principled and robust procedure to train and deploy in parallel an ensemble of light-weight CNNs, instead of simply going deeper.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_27');
INSERT INTO `paper` VALUES (10720, 'From Attribute-Labels to Faces: Face Generation Using a Conditional Generative Adversarial Network', 'Attributes', 'Generative adversarial network', 'Face generation', '', '', 'Facial attributes are instrumental in semantically characterizing faces. Automated classification of such attributes (i.e., age, gender, ethnicity) has been a well studied topic. We here seek to explore the inverse problem, namely given attribute-labels the generation of attribute-associated faces. The interest in this topic is fueled by related applications in law enforcement and entertainment. In this work, we propose two models for attribute-label based facial image and video generation incorporating 2D and 3D deep conditional generative adversarial networks (DCGAN). The attribute-labels serve as a tool to determine the specific representations of generated images and videos. While these are early results, our findings indicate the methods’ ability to generate realistic faces from attribute labels.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_59');
INSERT INTO `paper` VALUES (10721, 'From Face Recognition to Models of Identity: A Bayesian Approach to Learning About Unknown Identities from Unsupervised Data', '', '', '', '', '', 'Current face recognition systems robustly recognize identities across a wide variety of imaging conditions. In these systems recognition is performed via classification into known identities obtained from supervised identity annotations. There are two problems with this current paradigm: (1) current systems are unable to benefit from unlabelled data which may be available in large quantities; and (2) current systems equate successful recognition with labelling a given input image. Humans, on the other hand, regularly perform identification of individuals completely unsupervised, recognising the identity of someone they have seen before even without being able to name that individual. How can we go beyond the current classification paradigm towards a more human understanding of identities? We propose an integrated Bayesian model that coherently reasons about the observed images, identities, partial knowledge about names, and the situational context of each observation. While our model achieves good recognition performance against known identities, it can also discover new identities from unsupervised data and learns to associate identities with different contexts depending on which identities tend to be observed together. In addition, the proposed semi-supervised component is able to handle not only acquaintances, whose names are known, but also unlabelled familiar faces and complete strangers in a unified framework.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_46');
INSERT INTO `paper` VALUES (10722, 'Frustratingly Easy Trade-off Optimization Between Single-Stage and Two-Stage Deep Object Detectors', 'Object detection', 'Deep neural networks', 'Single-shot multibox detector', 'Faster R-CNN', '', 'There are mainly two types of state-of-the-art object detectors. On one hand, we have two-stage detectors, such as Faster R-CNN (Region-based Convolutional Neural Networks) or Mask R-CNN, that (i) use a Region Proposal Network to generate regions of interests in the first stage and (ii) send the region proposals down the pipeline for object classification and bounding-box regression. Such models reach the highest accuracy rates, but are typically slower. On the other hand, we have single-stage detectors, such as YOLO (You Only Look Once) and SSD (Singe Shot MultiBox Detector), that treat object detection as a simple regression problem, by taking an input image and learning the class probabilities and bounding box coordinates. Such models reach lower accuracy rates, but are much faster than two-stage object detectors. In this paper, we propose and evaluate four simple and straightforward approaches to achieve an optimal trade-off between accuracy and speed in object detection. All the approaches are based on separating the test images in two batches, an easy batch that is fed to a faster single-stage detector and a difficult batch that is fed to a more accurate two-stage detector. The difference between the four approaches is the criterion used for splitting the images in two batches. The criteria are the image difficulty score (easier images go into the easy batch), the number of detected objects (images with less objects go into the easy batch), the average size of the detected objects (images with bigger objects go into the easy batch), and the number of detected objects divided by their average size (images with less and bigger objects go into the easy batch). The first approach is based on an image difficulty predictor, while the other three approaches employ a faster single-stage detector to determine the approximate number of objects and their sizes. Our experiments on PASCAL VOC 2007 show that using image difficulty compares favorably to a random split of the images. However, splitting the images based on the number objects divided by their size, an approach that is frustratingly easy to implement, produces even better results. Remarkably, it shortens the processing time nearly by half, while reducing the mean Average Precision of Faster R-CNN by only \\(0.5\\%\\).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_33');
INSERT INTO `paper` VALUES (10723, 'Full-Body High-Resolution Anime Generation with Progressive Structure-Conditional Generative Adversarial Networks', 'Generative adversarial networks', 'Anime generation', 'Image generation', 'Video generation', '', 'We propose Progressive Structure-conditional Generative Adversarial Networks (PSGAN), a new framework that can generate full-body and high-resolution character images based on structural information. Recent progress in generative adversarial networks with progressive training has made it possible to generate high-resolution images. However, existing approaches have limitations in achieving both high image quality and structural consistency at the same time. Our method tackles the limitations by progressively increasing the resolution of both generated images and structural conditions during training. In this paper, we empirically demonstrate the effectiveness of this method by showing the comparison with existing approaches and video generation results of diverse anime characters at 1024 \\(\\times \\) 1024 based on target pose sequences. We also create a novel dataset containing full-body 1024 \\(\\times \\) 1024 high-resolution images and exact 2D pose keypoints using Unity 3D Avatar models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_8');
INSERT INTO `paper` VALUES (10724, 'Fully Motion-Aware Network for Video Object Detection', 'Video object detection', 'Feature calibration', 'Pixel-level', 'Instance-level', 'End-to-end', 'Video objection detection is challenging in the presence of appearance deterioration in certain video frames. One of typical solutions is to enhance per-frame features through aggregating neighboring frames. But the features of objects are usually not spatially calibrated across frames due to motion from object and camera. In this paper, we propose an end-to-end model called fully motion-aware network (MANet), which jointly calibrates the features of objects on both pixel-level and instance-level in a unified framework. The pixel-level calibration is flexible in modeling detailed motion while the instance-level calibration captures more global motion cues in order to be robust to occlusion. To our best knowledge, MANet is the first work that can jointly train the two modules and dynamically combine them according to the motion patterns. It achieves leading performance on the large-scale ImageNet VID dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_33');
INSERT INTO `paper` VALUES (10725, 'Fully-Convolutional Point Networks for Large-Scale Point Clouds', 'Point clouds', '3D deep learning', 'Scene understanding', 'Fully-convolutional', 'Semantic segmentation', 'This work proposes a general-purpose, fully-convolutional network architecture for efficiently processing large-scale 3D data. One striking characteristic of our approach is its ability to process unorganized 3D representations such as point clouds as input, then transforming them internally to ordered structures to be processed via 3D convolutions. In contrast to conventional approaches that maintain either unorganized or organized representations, from input to output, our approach has the advantage of operating on memory efficient input data representations while at the same time exploiting the natural structure of convolutional operations to avoid the redundant computing and storing of spatial information in the network. The network eliminates the need to pre- or post process the raw sensor data. This, together with the fully-convolutional nature of the network, makes it an end-to-end method able to process point clouds of huge spaces or even entire rooms with up to 200k points at once. Another advantage is that our network can produce either an ordered output or map predictions directly onto the input cloud, thus making it suitable as a general-purpose point cloud descriptor applicable to many 3D tasks. We demonstrate our network’s ability to effectively learn both low-level features as well as complex compositional relationships by evaluating it on benchmark datasets for semantic voxel segmentation, semantic part segmentation and 3D scene captioning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_37');
INSERT INTO `paper` VALUES (10726, 'GA-Based Filter Selection for Representation in Convolutional Neural Networks', 'CNN', 'Feature representation', 'Filter optimization', '', '', 'One of the deep learning models, a convolutional neural network (CNN) has been very successful in a variety of computer vision tasks. Features of a CNN are automatically generated, however, they can be further optimized since they often require large scale parallel operations and there exist the possibility of overlapping redundant features. The aim of this paper is to use feature selection via evolutionary algorithms to remove the irrelevant deep features. This will minimize the computational complexity and the amount of overfitting while maintaining a good quality of representation. We demonstrate the improvement of the filter representation by performing experiments on three data sets of CIFAR10, metal surface defects, and variation of MNIST and by analyzing the classification performance as well as the variance of the filter.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_48');
INSERT INTO `paper` VALUES (10727, 'Gait Energy Image Reconstruction from Degraded Gait Cycle Using Deep Learning', 'Gait energy image', 'Gait recognition', 'Deep learning', '', '', 'Gait energy image (GEI) is considered as an effective gait representation for gait-based human identification. In gait recognition, normally, GEI is computed from one full gait cycle. However in many circumstances, such a full gait cycle might not be available due to occlusion. Thus, the GEI is not complete, giving a rise to degrading gait identification rate. In this paper, we address this issue by proposing a novel method to reconstruct a complete GEI from a few frames of gait cycle. To do so, we propose a deep learning-based approach to transform incomplete GEI to the corresponding complete GEI obtained from a full gait cycle. More precisely, this transformation is done gradually by training several fully convolutional networks independently and then combining these as a uniform model. Experimental results on a large public gait dataset, namely OULP demonstrate the validity of the proposed method for gait identification when dealing with very incomplete gait cycles.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_52');
INSERT INTO `paper` VALUES (10728, 'GAL: Geometric Adversarial Loss for Single-View 3D-Object Reconstruction', '3D Reconstruction', 'Adversarial loss', 'Geometric consistency', 'Point cloud', '3D Neural network', 'In this paper, we present a framework for reconstructing a point-based 3D model of an object from a single-view image. We found distance metrics, like Chamfer distance, were used in previous work to measure the difference of two point sets and serve as the loss function in point-based reconstruction. However, such point-point loss does not constrain the 3D model from a global perspective. We propose adding geometric adversarial loss (GAL). It is composed of two terms where the geometric loss ensures consistent shape of reconstructed 3D models close to ground-truth from different viewpoints, and the conditional adversarial loss generates a semantically-meaningful point cloud. GAL benefits predicting the obscured part of objects and maintaining geometric structure of the predicted 3D model. Both the qualitative results and quantitative analysis manifest the generality and suitability of our method.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_49');
INSERT INTO `paper` VALUES (10729, 'GANimation: Anatomically-Aware Facial Animation from a Single Image', 'GANs', 'Face animation', 'Action-unit condition', '', '', 'Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs’ generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_50');
INSERT INTO `paper` VALUES (10730, 'Generalized Bayesian Canonical Correlation Analysis with Missing Modalities', 'Multi-modal learning', 'Missing modalities', 'Bayesian inference', 'Canonical Correlation Analysis', '', 'Multi-modal learning aims to build models that can relate information from multiple modalities. One challenge of multi-modal learning is the prediction of a target modality based on a set of multiple modalities. However, there are two challenges associated with the goal: Firstly, collecting a large, complete dataset containing all required modalities is difficult; some of the modalities can be missing. Secondly, the features of modalities are likely to be high dimensional and noisy. To deal with these challenges, we propose a method called Generalized Bayesian Canonical Correlation Analysis with Missing Modalities. This method can utilize the incomplete sets of modalities. By including them in the likelihood function during training, it can estimate the relationships among the non-missing modalities and the feature space in the non-missing modality accurately. In addition, this method can work well on high dimensional and noisy features of modalities. This is because, by a probabilistic model based on the prior knowledge, it is strong against outliers and can reduce the amount of data necessary for the model learning even if features of modalities are high dimensional. Experiments with artificial and real data demonstrate our method outperforms conventional methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_48');
INSERT INTO `paper` VALUES (10731, 'Generalized Loss-Sensitive Adversarial Learning with Manifold Margins', 'Regularized GAN', 'Image generation', 'Semi-supervised classification', 'Lipschitz regularization', '', 'The classic Generative Adversarial Net and its variants can be roughly categorized into two large families: the unregularized versus regularized GANs. By relaxing the non-parametric assumption on the discriminator in the classic GAN, the regularized GANs have better generalization ability to produce new samples drawn from the real distribution. It is well known that the real data like natural images are not uniformly distributed over the whole data space. Instead, they are often restricted to a low-dimensional manifold of the ambient space. Such a manifold assumption suggests the distance over the manifold should be a better measure to characterize the distinct between real and fake samples. Thus, we define a pullback operator to map samples back to their data manifold, and a manifold margin is defined as the distance between the pullback representations to distinguish between real and fake samples and learn the optimal generators. We justify the effectiveness of the proposed model both theoretically and empirically.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_6');
INSERT INTO `paper` VALUES (10732, 'Generalizing a Person Retrieval Model Hetero- and Homogeneously', 'Person re-identification', 'Unsupervised domain adaptation', '', '', '', 'Person re-identification (re-ID) poses unique challenges for unsupervised domain adaptation (UDA) in that classes in the source and target sets (domains) are entirely different and that image variations are largely caused by cameras. Given a labeled source training set and an unlabeled target training set, we aim to improve the generalization ability of re-ID models on the target testing set. To this end, we introduce a Hetero-Homogeneous Learning (HHL) method. Our method enforces two properties simultaneously: (1) camera invariance, learned via positive pairs formed by unlabeled target images and their camera style transferred counterparts; (2) domain connectedness, by regarding source/target images as negative matching pairs to the target/source images. The first property is implemented by homogeneous learning because training pairs are collected from the same domain. The second property is achieved by heterogeneous learning because we sample training pairs from both the source and target domains. On Market-1501, DukeMTMC-reID and CUHK03, we show that the two properties contribute indispensably and that very competitive re-ID UDA accuracy is achieved. Code is available at: https://github.com/zhunzhong07/HHL.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_11');
INSERT INTO `paper` VALUES (10733, 'Generating 3D Faces Using Convolutional Mesh Autoencoders', '', '', '', '', '', 'Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We show that, replacing the expression space of an existing state-of-the-art face model with our model, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_43');
INSERT INTO `paper` VALUES (10734, 'Generating Shared Latent Variables for Robots to Imitate Human Movements and Understand Their Physical Limitations', 'Robot imitation', 'Transfer knowledge', 'Physical rehabilitation', 'Shared Gaussian Process Latent Variable Model', 'Motion analysis', 'Assistive robotics and particularly robot coaches may be very helpful for rehabilitation healthcare. In this context, we propose a method based on Gaussian Process Latent Variable Model (GP-LVM) to transfer knowledge between a physiotherapist, a robot coach and a patient. Our model is able to map visual human body features to robot data in order to facilitate the robot learning and imitation. In addition, we propose to extend the model to adapt the robots’ understanding to patients’ physical limitations during assessment of rehabilitation exercises. Experimental evaluation demonstrates promising results for both robot imitation and model adaptation according to patients’ limitations.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_15');
INSERT INTO `paper` VALUES (10735, 'Generating Synthetic Video Sequences by Explicitly Modeling Object Motion', '', '', '', '', '', 'Recent GAN-based video generation approaches model videos as the combination of a time-independent scene component and a time-varying motion component, thus factorizing the generation problem into generating background and foreground separately. One of the main limitations of current approaches is that both factors are learned by mapping one source latent space to videos, which complicates the generation task as a single data point must be informative of both background and foreground content. In this paper we propose a GAN framework for video generation that, instead, employs two latent spaces in order to structure the generative process in a more natural way: (1) a latent space to generate the static visual content of a scene (background), which remains the same for the whole video, and (2) a latent space where motion is encoded as a trajectory between sampled points and whose dynamics are modeled through an RNN encoder (jointly trained with the generator and the discriminator) and then mapped by the generator to visual objects’ motion. Performance evaluation showed that our approach is able to control effectively the generation process as well as to synthesize more realistic videos than state-of-the-art methods.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_37');
INSERT INTO `paper` VALUES (10736, 'Generative Adversarial Network with Spatial Attention for Face Attribute Editing', 'Face attribute editing', 'GAN', 'Spatial attention', 'Data augmentation', '', 'Face attribute editing aims at editing the face image with the given attribute. Most existing works employ Generative Adversarial Network (GAN) to operate face attribute editing. However, these methods inevitably change the attribute-irrelevant regions, as shown in Fig. 1. Therefore, we introduce the spatial attention mechanism into GAN framework (referred to as SaGAN), to only alter the attribute-specific region and keep the rest unchanged. Our approach SaGAN consists of a generator and a discriminator. The generator contains an attribute manipulation network (AMN) to edit the face image, and a spatial attention network (SAN) to localize the attribute-specific region which restricts the alternation of AMN within this region. The discriminator endeavors to distinguish the generated images from the real ones, and classify the face attribute. Experiments demonstrate that our approach can achieve promising visual results, and keep those attribute-irrelevant regions unchanged. Besides, our approach can benefit the face recognition by data augmentation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_26');
INSERT INTO `paper` VALUES (10737, 'Generative Adversarial Network-Based Image Super-Resolution Using Perceptual Content Losses', 'Super-resolution', 'Deep learning', 'Perception', 'Distortion', '', 'In this paper, we propose a deep generative adversarial network for super-resolution considering the trade-off between perception and distortion. Based on good performance of a recently developed model for super-resolution, i.e., deep residual network using enhanced upscale modules (EUSR) [9], the proposed model is trained to improve perceptual performance with only slight increase of distortion. For this purpose, together with the conventional content loss, i.e., reconstruction loss such as L1 or L2, we consider additional losses in the training phase, which are the discrete cosine transform coefficients loss and differential content loss. These consider perceptual part in the content loss, i.e., consideration of proper high frequency components is helpful for the trade-off problem in super-resolution. The experimental results show that our proposed model has good performance for both perception and distortion, and is effective in perceptual super-resolution applications.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_4');
INSERT INTO `paper` VALUES (10738, 'Generative Adversarial Networks for Unsupervised Monocular Depth Prediction', '', '', '', '', '', 'Estimating depth from a single image is a very challenging and exciting topic in computer vision with implications in several application domains. Recently proposed deep learning approaches achieve outstanding results by tackling it as an image reconstruction task and exploiting geometry constraints (e.g., epipolar geometry) to obtain supervisory signals for training. Inspired by these works and compelling results achieved by Generative Adversarial Network (GAN) on image reconstruction and generation tasks, in this paper we propose to cast unsupervised monocular depth estimation within a GAN paradigm. The generator network learns to infer depth from the reference image to generate a warped target image. At training time, the discriminator network learns to distinguish between fake images generated by the generator and target frames acquired with a stereo rig. To the best of our knowledge, our proposal is the first successful attempt to tackle monocular depth estimation with a GAN paradigm and the extensive evaluation on CityScapes and KITTI datasets confirm that it enables to improve traditional approaches. Additionally, we highlight a major issue with data deployed by a standard evaluation protocol widely used in this field and fix this problem using a more reliable dataset recently made available by the KITTI evaluation benchmark.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_20');
INSERT INTO `paper` VALUES (10739, 'Generative Domain-Migration Hashing for Sketch-to-Image Retrieval', 'Domain-migration', 'Hash function', 'SBIR', '', '', 'Due to the succinct nature of free-hand sketch drawings, sketch-based image retrieval (SBIR) has abundant practical use cases in consumer electronics. However, SBIR remains a long-standing unsolved problem mainly because of the significant discrepancy between the sketch domain and the image domain. In this work, we propose a Generative Domain-migration Hashing (GDH) approach, which for the first time generates hashing codes from synthetic natural images that are migrated from sketches. The generative model learns a mapping that the distributions of sketches can be indistinguishable from the distribution of natural images using an adversarial loss, and simultaneously learns an inverse mapping based on the cycle consistency loss in order to enhance the indistinguishability. With the robust mapping learned from the generative model, GDH can migrate sketches to their indistinguishable image counterparts while preserving the domain-invariant information of sketches. With an end-to-end multi-task learning framework, the generative model and binarized hashing codes can be jointly optimized. Comprehensive experiments of both category-level and fine-grained SBIR on multiple large-scale datasets demonstrate the consistently balanced superiority of GDH in terms of efficiency, memory costs and effectiveness (Models and code at https://github.com/YCJGG/GDH).', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_19');
INSERT INTO `paper` VALUES (10740, 'Generative Semantic Manipulation with Mask-Contrasting GAN', 'Generative Adversarial Network', 'Image semantic manipulation', '', '', '', 'Despite the promising results on paired/unpaired image-to-image translation achieved by Generative Adversarial Networks (GANs), prior works often only transfer the low-level information (e.g. color or texture changes), but fail to manipulate high-level semantic meanings (e.g., geometric structure or content) of different object regions. On the other hand, while some researches can synthesize compelling real-world images given a class label or caption, they cannot condition on arbitrary shapes or structures, which largely limits their application scenarios and interpretive capability of model results. In this work, we focus on a more challenging semantic manipulation task, aiming at modifying the semantic meaning of an object while preserving its own characteristics (e.g. viewpoints and shapes), such as cow\\(\\rightarrow \\)sheep, motor\\(\\rightarrow \\)bicycle, cat\\(\\rightarrow \\)dog. To tackle such large semantic changes, we introduce a contrasting GAN (contrast-GAN) with a novel adversarial contrasting objective which is able to perform all types of semantic translations with one category-conditional generator. Instead of directly making the synthesized samples close to target data as previous GANs did, our adversarial contrasting objective optimizes over the distance comparisons between samples, that is, enforcing the manipulated data be semantically closer to the real data with target category than the input data. Equipped with the new contrasting objective, a novel mask-conditional contrast-GAN architecture is proposed to enable disentangle image background with object semantic changes. Extensive qualitative and quantitative experiments on several semantic manipulation tasks on ImageNet and MSCOCO dataset show considerable performance gain by our contrast-GAN over other conditional GANs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_34');
INSERT INTO `paper` VALUES (10741, 'GeoDesc: Learning Local Descriptors by Integrating Geometry Constraints', 'Local features', 'Feature descriptors', 'Deep learning', '', '', 'Learned local descriptors based on Convolutional Neural Networks (CNNs) have achieved significant improvements on patch-based benchmarks, whereas not having demonstrated strong generalization ability on recent benchmarks of image-based 3D reconstruction. In this paper, we mitigate this limitation by proposing a novel local descriptor learning approach that integrates geometry constraints from multi-view reconstructions, which benefits the learning process in terms of data generation, data sampling and loss computation. We refer to the proposed descriptor as GeoDesc, and demonstrate its superior performance on various large-scale benchmarks, and in particular show its great success on challenging reconstruction tasks. Moreover, we provide guidelines towards practical integration of learned descriptors in Structure-from-Motion (SfM) pipelines, showing the good trade-off that GeoDesc delivers to 3D reconstruction tasks between accuracy and efficiency.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_11');
INSERT INTO `paper` VALUES (10742, 'Geolocation Estimation of Photos Using a Hierarchical Model and Scene Classification', 'Geolocation estimation', 'Scene classification', 'Deep learning', 'Context-based classification', '', 'While the successful estimation of a photo’s geolocation enables a number of interesting applications, it is also a very challenging task. Due to the complexity of the problem, most existing approaches are restricted to specific areas, imagery, or worldwide landmarks. Only a few proposals predict GPS coordinates without any limitations. In this paper, we introduce several deep learning methods, which pursue the latter approach and treat geolocalization as a classification problem where the earth is subdivided into geographical cells. We propose to exploit hierarchical knowledge of multiple partitionings and additionally extract and take the photo’s scene content into account, i.e., indoor, natural, or urban setting etc. As a result, contextual information at different spatial resolutions as well as more specific features for various environmental settings are incorporated in the learning process of the convolutional neural network. Experimental results on two benchmarks demonstrate the effectiveness of our approach outperforming the state of the art while using a significant lower number of training images and without relying on retrieval methods that require an appropriate reference dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_35');
INSERT INTO `paper` VALUES (10743, 'Geometric Constrained Joint Lane Segmentation and Lane Boundary Detection', 'Lane segmentation', 'Semantic segmentation', '', '', '', 'Lane detection is playing an indispensable role in advanced driver assistance systems. The existing approaches for lane detection can be categorized as lane area segmentation and lane boundary detection. Most of these methods abandon a great quantity of complementary information, such as geometric priors, when exploiting the lane area and the lane boundaries alternatively. In this paper, we establish a multiple-task learning framework to segment lane areas and detect lane boundaries simultaneously. The main contributions of the proposed framework are highlighted in two facets: (1) We put forward a multiple-task learning framework with mutually interlinked sub-structures between lane segmentation and lane boundary detection to improve overall performance. (2) A novel loss function is proposed with two geometric constraints considered, as assumed that the lane boundary is predicted as the outer contour of the lane area while the lane area is predicted as the area integration result within the lane boundary lines. With an end-to-end training process, these improvements extremely enhance the robustness and accuracy of our approach on several metrics. The proposed framework is evaluated on KITTI dataset, CULane dataset and RVD dataset. Compared with the state of the arts, our approach achieves the best performance on the metrics and a robust detection in varied traffic scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_30');
INSERT INTO `paper` VALUES (10744, 'Give Ear to My Face: Modelling Multimodal Attention to Social Interactions', 'Audio-visual attention', 'Social interaction', 'Multimodal perception', '', '', 'We address the deployment of perceptual attention to social interactions as displayed in conversational clips, when relying on multimodal information (audio and video). A probabilistic modelling framework is proposed that goes beyond the classic saliency paradigm while integrating multiple information cues. Attentional allocation is determined not just by stimulus-driven selection but, importantly, by social value as modulating the selection history of relevant multimodal items. Thus, the construction of attentional priority is the result of a sampling procedure conditioned on the potential value dynamics of socially relevant objects emerging moment to moment within the scene. Preliminary experiments on a publicly available dataset are presented.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_27');
INSERT INTO `paper` VALUES (10745, 'Goal-Oriented Visual Question Generation via Intermediate Rewards', 'Goal-oriented', 'VQG', 'Intermediate rewards', '', '', 'Despite significant progress in a variety of vision-and-language problems, developing a method capable of asking intelligent, goal-oriented questions about images is proven to be an inscrutable challenge. Towards this end, we propose a Deep Reinforcement Learning framework based on three new intermediate rewards, namely goal-achieved, progressive and informativeness that encourage the generation of succinct questions, which in turn uncover valuable information towards the overall goal. By directly optimizing for questions that work quickly towards fulfilling the overall goal, we avoid the tendency of existing methods to generate long series of inane queries that add little value. We evaluate our model on the GuessWhat?! dataset and show that the resulting questions can help a standard ‘Guesser’ identify a specific object in an image at a much higher success rate.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_12');
INSERT INTO `paper` VALUES (10746, 'Good Line Cutting: Towards Accurate Pose Tracking of Line-Assisted VO/VSLAM', 'SLAM', 'Line feature', 'Least squares', '', '', 'This paper tackles a problem in line-assisted VO/VSLAM: accurately solving the least squares pose optimization with unreliable 3D line input. The solution we present is good line cutting, which extracts the most-informative sub-segment from each 3D line for use within the pose optimization formulation. By studying the impact of line cutting towards the information gain of pose estimation in line-based least squares problem, we demonstrate the applicability of improving pose estimation accuracy with good line cutting. To that end, we describe an efficient algorithm that approximately approaches the joint optimization problem of good line cutting. The proposed algorithm is integrated into a state-of-the-art line-assisted VSLAM system. When evaluated in two target scenarios of line-assisted VO/VSLAM, low-texture and motion blur, the accuracy of pose tracking is improved, while the robustness is preserved.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_32');
INSERT INTO `paper` VALUES (10747, 'Graininess-Aware Deep Feature Learning for Pedestrian Detection', 'Pedestrian detection', 'Attention', 'Deep learning', 'Graininess', '', 'In this paper, we propose a graininess-aware deep feature learning method for pedestrian detection. Unlike most existing pedestrian detection methods which only consider low resolution feature maps, we incorporate fine-grained information into convolutional features to make them more discriminative for human body parts. Specifically, we propose a pedestrian attention mechanism which efficiently identifies pedestrian regions. Our method encodes fine-grained attention masks into convolutional feature maps, which significantly suppresses background interference and highlights pedestrians. Hence, our graininess-aware features become more focused on pedestrians, in particular those of small size and with occlusion. We further introduce a zoom-in-zoom-out module, which enhances the features by incorporating local details and context information. We integrate these two modules into a deep neural network, forming an end-to-end trainable pedestrian detector. Comprehensive experimental results on four challenging pedestrian benchmarks demonstrate the effectiveness of the proposed approach.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_45');
INSERT INTO `paper` VALUES (10748, 'Graph Adaptive Knowledge Transfer for Unsupervised Domain Adaptation', 'Domain adaptation', 'Adaptive graph', 'Semi-supervised learning', '', '', 'Unsupervised domain adaptation has caught appealing attentions as it facilitates the unlabeled target learning by borrowing existing well-established source domain knowledge. Recent practice on domain adaptation manages to extract effective features by incorporating the pseudo labels for the target domain to better solve cross-domain distribution divergences. However, existing approaches separate target label optimization and domain-invariant feature learning as different steps. To address that issue, we develop a novel Graph Adaptive Knowledge Transfer (GAKT) model to jointly optimize target labels and domain-free features in a unified framework. Specifically, semi-supervised knowledge adaptation and label propagation on target data are coupled to benefit each other, and hence the marginal and conditional disparities across different domains will be better alleviated. Experimental evaluation on two cross-domain visual datasets demonstrates the effectiveness of our designed approach on facilitating the unlabeled target task learning, compared to the state-of-the-art domain adaptation approaches.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_3');
INSERT INTO `paper` VALUES (10749, 'Graph Distillation for Action Detection with Privileged Modalities', '', '', '', '', '', 'We propose a technique that tackles action detection in multimodal videos under a realistic and challenging condition in which only limited training data and partially observed modalities are available. Common methods in transfer learning do not take advantage of the extra modalities potentially available in the source domain. On the other hand, previous work on multimodal learning only focuses on a single domain or task and does not handle the modality discrepancy between training and testing. In this work, we propose a method termed graph distillation that incorporates rich privileged information from a large-scale multimodal dataset in the source domain, and improves the learning in the target domain where training data and modalities are scarce. We evaluate our approach on action classification and detection tasks in multimodal videos, and show that our model outperforms the state-of-the-art by a large margin on the NTU RGB+D and PKU-MMD benchmarks. The code is released at http://alan.vision/eccv18_graph/.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_11');
INSERT INTO `paper` VALUES (10750, 'Graph R-CNN for Scene Graph Generation', 'Graph R-CNN', 'Scene graph generation', 'Relation proposal network', 'Attentional graph convolutional network', '', 'We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_41');
INSERT INTO `paper` VALUES (10751, 'Grassmann Pooling as Compact Homogeneous Bilinear Pooling for Fine-Grained Visual Classification', 'Fine-grained visual classification', 'Bilinear pooling', 'Singular Value Decomposition', 'Grassmann manifold', 'Visual burstiness', 'Designing discriminative and invariant features is the key to visual recognition. Recently, the bilinear pooled feature matrix of Convolutional Neural Network (CNN) has shown to achieve state-of-the-art performance on a range of fine-grained visual recognition tasks. The bilinear feature matrix collects second-order statistics and is closely related to the covariance matrix descriptor. However, the bilinear feature could suffer from the visual burstiness phenomenon similar to other visual representations such as VLAD and Fisher Vector. The reason is that the bilinear feature matrix is sensitive to the magnitudes and correlations of local CNN feature elements which can be measured by its singular values. On the other hand, the singular vectors are more invariant and reasonable to be adopted as the feature representation. Motivated by this point, we advocate an alternative pooling method which transforms the CNN feature matrix to an orthonormal matrix consists of its principal singular vectors. Geometrically, such orthonormal matrix lies on the Grassmann manifold, a Riemannian manifold whose points represent subspaces of the Euclidean space. Similarity measurement of images reduces to comparing the principal angles between these “homogeneous” subspaces and thus is independent of the magnitudes and correlations of local CNN activations. In particular, we demonstrate that the projection distance on the Grassmann manifold deduces a bilinear feature mapping without explicitly computing the bilinear feature matrix, which enables a very compact feature and classifier representation. Experimental results show that our method achieves an excellent balance of model complexity and accuracy on a variety of fine-grained image classification datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_22');
INSERT INTO `paper` VALUES (10752, 'Gray-Box Adversarial Training', 'Adversarial perturbations', 'Attacks on machine learning models', 'Adversarial training', 'Robust machine learning models', '', 'Adversarial samples are perturbed inputs crafted to mislead the machine learning systems. A training mechanism, called adversarial training, which presents adversarial samples along with clean samples has been introduced to learn robust models. In order to scale adversarial training for large datasets, these perturbations can only be crafted using fast and simple methods (e.g., gradient ascent). However, it is shown that adversarial training converges to a degenerate minimum, where the model appears to be robust by generating weaker adversaries. As a result, the models are vulnerable to simple black-box attacks.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_13');
INSERT INTO `paper` VALUES (10753, 'GreenWarps: A Two-Stage Warping Model for Stitching Images Using Diffeomorphic Meshes and Green Coordinates', 'Green coordinates', 'Diffeomorphic registration', 'Content preserving warps', 'Image stitching', '', 'Image Stitching is a hard task to solve in the presence of large parallax in the images. Specifically, for a sequence of frames from unconstrained videos which are considerably shaky, recent works fail to align such a sequence of images accurately. The proposed method “GreenWarps” aims to accurately align frames/images with large parallax. The method consists of two novel stages, namely, Prewarping and Diffeomorphic Mesh warping. The first stage warps unaligned image to the reference image using Green Coordinates. The second stage of the model refines the alignment by using a demon-based diffeomorphic warping method for mesh deformation termed “DiffeoMeshes”. The warping is performed using Green Coordinates in both the stages without the assumption of any motion model. The combination of the two stages provide accurate alignment of the images. Experiments were performed on two standard image stitching datasets and one dataset consisting of images created from unconstrained videos. The results show superior performance of our method compared to the state-of-the-art methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_67');
INSERT INTO `paper` VALUES (10754, 'GridFace: Face Rectification via Learning Local Homography Transformations', 'Face recognition', 'Face rectification', 'Homography transformation', '', '', 'In this paper, we propose a method, called GridFace, to reduce facial geometric variations and improve the recognition performance. Our method rectifies the face by local homography transformations, which are estimated by a face rectification network. To encourage the image generation with canonical views, we apply a regularization based on the natural face distribution. We learn the rectification network and recognition network in an end-to-end manner. Extensive experiments show our method greatly reduces geometric variations, and gains significant improvements in unconstrained face recognition scenarios.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_1');
INSERT INTO `paper` VALUES (10755, 'Grounding Visual Explanations', 'Explainability', 'Counterfactuals', 'Grounding', 'Phrase correction', '', 'Existing visual explanation generating agents learn to fluently justify a class prediction. However, they may mention visual attributes which reflect a strong class prior, although the evidence may not actually be in the image. This is particularly concerning as ultimately such agents fail in building trust with human users. To overcome this limitation, we propose a phrase-critic model to refine generated candidate explanations augmented with flipped phrases which we use as negative examples while training. At inference time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image. Our explainable AI agent is capable of providing counter arguments for an alternative prediction, i.e. counterfactuals, along with explanations that justify the correct classification decisions. Our model improves the textual explanation quality of fine-grained classification decisions on the CUB dataset by mentioning phrases that are grounded in the image. Moreover, on the FOIL tasks, our agent detects when there is a mistake in the sentence, grounds the incorrect phrase and corrects it significantly better than other models.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_17');
INSERT INTO `paper` VALUES (10756, 'Group LSTM: Group Trajectory Prediction in Crowded Scenarios', 'Group prediction', 'Crowd analysis', 'Trajectory clustering', 'Social-LSTM', '', 'The analysis of crowded scenes is one of the most challenging scenarios in visual surveillance, and a variety of factors need to be taken into account, such as the structure of the environments, and the presence of mutual occlusions and obstacles. Traditional prediction methods (such as RNN, LSTM, VAE, etc.) focus on anticipating individual’s future path based on the precise motion history of a pedestrian. However, since tracking algorithms are generally not reliable in highly dense scenes, these methods are not easily applicable in real environments. Nevertheless, it is very common that people (friends, couples, family members, etc.) tend to exhibit coherent motion patterns. Motivated by this phenomenon, we propose a novel approach to predict future trajectories in crowded scenes, at the group level. First, by exploiting the motion coherency, we cluster trajectories that have similar motion trends. In this way, pedestrians within the same group can be well segmented. Then, an improved social-LSTM is adopted for future path prediction. We evaluate our approach on standard crowd benchmarks (the UCY dataset and the ETH dataset), demonstrating its efficacy and applicability.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_18');
INSERT INTO `paper` VALUES (10757, 'Group Normalization', '', '', '', '', '', 'Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems—BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_1');
INSERT INTO `paper` VALUES (10758, 'HairNet: Single-View Hair Reconstruction Using Convolutional Neural Networks', 'Hair', 'Reconstruction', 'Real-time', 'DNN', '', 'We introduce a deep learning-based method to generate full 3D hair geometry from an unconstrained image. Our method can recover local strand details and has real-time performance. State-of-the-art hair modeling techniques rely on large hairstyle collections for nearest neighbor retrieval and then perform ad-hoc refinement. Our deep learning approach, in contrast, is highly efficient in storage and can run 1000 times faster while generating hair with 30K strands. The convolutional neural network takes the 2D orientation field of a hair image as input and generates strand features that are evenly distributed on the parameterized 2D scalp. We introduce a collision loss to synthesize more plausible hairstyles, and the visibility of each strand is also used as a weight term to improve the reconstruction accuracy. The encoder-decoder architecture of our network naturally provides a compact and continuous representation for hairstyles, which allows us to interpolate naturally between hairstyles. We use a large set of rendered synthetic hair models to train our network. Our method scales to real images because an intermediate 2D orientation field, automatically calculated from the real image, factors out the difference between synthetic and real hairs. We demonstrate the effectiveness and robustness of our method on a wide range of challenging real Internet pictures, and show reconstructed hair sequences from videos.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_15');
INSERT INTO `paper` VALUES (10759, 'Hand Pose Estimation via Latent 2.5D Heatmap Regression', 'Hand pose', '2D to 3D', '3D reconstruction', '2.5D heatmaps', '', 'Estimating the 3D pose of a hand is an essential part of human-computer interaction. Estimating 3D pose using depth or multi-view sensors has become easier with recent advances in computer vision, however, regressing pose from a single RGB image is much less straightforward. The main difficulty arises from the fact that 3D pose requires some form of depth estimates, which are ambiguous given only an RGB image. In this paper we propose a new method for 3D hand pose estimation from a monocular image through a novel 2.5D pose representation. Our new representation estimates pose up to a scaling factor, which can be estimated additionally if a prior of the hand size is given. We implicitly learn depth maps and heatmap distributions with a novel CNN architecture. Our system achieves state-of-the-art accuracy for 2D and 3D hand pose estimation on several challenging datasets in presence of severe occlusions.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_8');
INSERT INTO `paper` VALUES (10760, 'Hand-Tremor Frequency Estimation in Videos', 'Video hand-tremor analysis', 'Phase-based tremor frequency detection', 'Human tremor dataset', 'Eulerian hand tremors', '', 'We focus on the problem of estimating human hand-tremor frequency from input RGB video data. Estimating tremors from video is important for non-invasive monitoring, analyzing and diagnosing patients suffering from motor-disorders such as Parkinson’s disease. We consider two approaches for hand-tremor frequency estimation: (a) a Lagrangian approach where we detect the hand at every frame in the video, and estimate the tremor frequency along the trajectory; and (b) an Eulerian approach where we first localize the hand, we subsequently remove the large motion along the movement trajectory of the hand, and we use the video information over time encoded as intensity values or phase information to estimate the tremor frequency. We estimate hand tremors on a new human tremor dataset, TIM-Tremor, containing static tasks as well as a multitude of more dynamic tasks, involving larger motion of the hands. The dataset has 55 tremor patient recordings together with: associated ground truth accelerometer data from the most affected hand, RGB video data, and aligned depth data.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_14');
INSERT INTO `paper` VALUES (10761, 'HandMap: Robust Hand Pose Estimation via Intermediate Dense Guidance Map Supervision', 'Hand pose estimation', 'Dense guidance map', 'Intermediate supervision', '', '', 'This work presents a novel hand pose estimation framework via intermediate dense guidance map supervision. By leveraging the advantage of predicting heat maps of hand joints in detection-based methods, we propose to use dense feature maps through intermediate supervision in a regression-based framework that is not limited to the resolution of the heat map. Our dense feature maps are delicately designed to encode the hand geometry and the spatial relation between local joint and global hand. The proposed framework significantly improves the state-of-the-art in both 2D and 3D on the recent benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_15');
INSERT INTO `paper` VALUES (10762, 'HANDS18: Methods, Techniques and Applications for Hand Observation', 'Hand detection', 'Hand pose estimation', 'Hand tracking', 'Gesture recognition', 'Hand-object interaction', 'This report outlines the proceedings of the Fourth International Workshop on Observing and Understanding Hands in Action (HANDS 2018). The fourth instantiation of this workshop attracted significant interest from both academia and the industry. The program of the workshop included regular papers that are published as the workshop’s proceedings, extended abstracts, invited posters, and invited talks. Topics of the submitted works and invited talks and posters included novel methods for hand pose estimation from RGB, depth, or skeletal data, datasets for special cases and real-world applications, and techniques for hand motion re-targeting and hand gesture recognition. The invited speakers are leaders in their respective areas of specialization, coming from both industry and academia. The main conclusions that can be drawn are the turn of the community towards RGB data and the maturation of some methods and techniques, which in turn has led to increasing interest for real-world applications.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_20');
INSERT INTO `paper` VALUES (10763, 'Hard-Aware Point-to-Set Deep Metric for Person Re-identification', 'Person re-identification', 'Deep metric learning', 'Triplet loss', '', '', 'Person re-identification (re-ID) is a highly challenging task due to large variations of pose, viewpoint, illumination, and occlusion. Deep metric learning provides a satisfactory solution to person re-ID by training a deep network under supervision of metric loss, e.g., triplet loss. However, the performance of deep metric learning is greatly limited by traditional sampling methods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S) loss with a soft hard-mining scheme. Based on the point-to-set triplet loss framework, the HAP2S loss adaptively assigns greater weights to harder samples. Several advantageous properties are observed when compared with other state-of-the-art loss functions: (1) Accuracy: HAP2S loss consistently achieves higher re-ID accuracies than other alternatives on three large-scale benchmark datasets; (2) Robustness: HAP2S loss is more robust to outliers than other losses; (3) Flexibility: HAP2S loss does not rely on a specific weight function, i.e., different instantiations of HAP2S loss are equally effective. (4) Generality: In addition to person re-ID, we apply the proposed method to generic deep metric learning benchmarks including CUB-200-2011 and Cars196, and also achieve state-of-the-art results.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_12');
INSERT INTO `paper` VALUES (10764, 'Hashing with Binary Matrix Pursuit', 'Hashing Methods', 'Image Retrieval Benchmarks', 'Learn Hash Functions', 'Affinity Matrix Construction', 'Hash Code Length', 'We propose theoretical and empirical improvements for two-stage hashing methods. We first provide a theoretical analysis on the quality of the binary codes and show that, under mild assumptions, a residual learning scheme can construct binary codes that fit any neighborhood structure with arbitrary accuracy. Secondly, we show that with high-capacity hash functions such as CNNs, binary code inference can be greatly simplified for many standard neighborhood definitions, yielding smaller optimization problems and more robust codes. Incorporating our findings, we propose a novel two-stage hashing method that significantly outperforms previous hashing studies on widely used image retrieval benchmarks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_21');
INSERT INTO `paper` VALUES (10765, 'HBE: Hand Branch Ensemble Network for Real-Time 3D Hand Pose Estimation', 'Hand pose estimation', 'Depth image', 'Convolutional Neural Networks', '', '', 'The goal of this paper is to estimate the 3D coordinates of the hand joints from a single depth image. To give consideration to both the accuracy and the real time performance, we design a novel three-branch Convolutional Neural Networks named Hand Branch Ensemble network (HBE), where the three branches correspond to the three parts of a hand: the thumb, the index finger and the other fingers. The structural design inspiration of the HBE network comes from the understanding of the differences in the functional importance of different fingers. In addition, a feature ensemble layer along with a low-dimensional embedding layer ensures the overall hand shape constraints. The experimental results on three public datasets demonstrate that our approach achieves comparable or better performance to state-of-the-art methods with less training data, shorter training time and faster frame rate.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_31');
INSERT INTO `paper` VALUES (10766, 'HGMR: Hierarchical Gaussian Mixtures for Adaptive 3D Registration', 'Hierarchical Gaussian Mixture', 'Point Cloud Registration', 'Associative Data', 'Adaptive Multiscale', 'Local Data Distribution', 'Point cloud registration sits at the core of many important and challenging 3D perception problems including autonomous navigation, SLAM, object/scene recognition, and augmented reality. In this paper, we present a new registration algorithm that is able to achieve state-of-the-art speed and accuracy through its use of a Hierarchical Gaussian Mixture representation. Our method, Hierarchical Gaussian Mixture Registration (HGMR), constructs a top-down multi-scale representation of point cloud data by recursively running many small-scale data likelihood segmentations in parallel on a GPU. We leverage the resulting representation using a novel optimization criterion that adaptively finds the best scale to perform data association between spatial subsets of point cloud data. Compared to previous Iterative Closest Point and GMM-based techniques, our tree-based point association algorithm performs data association in logarithmic-time while dynamically adjusting the level of detail to best match the complexity and spatial distribution characteristics of local scene geometry. In addition, unlike other GMM methods that restrict covariances to be isotropic, our new PCA-based optimization criterion well-approximates the true MLE solution even when fully anisotropic Gaussian covariances are used. Efficient data association, multi-scale adaptability, and a robust MLE approximation produce an algorithm that is up to an order of magnitude both faster and more accurate than current state-of-the-art on a wide variety of 3D datasets captured from LiDAR to structured light.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_43');
INSERT INTO `paper` VALUES (10767, 'HiDDeN: Hiding Data With Deep Networks', 'Adversarial networks', 'Steganography', 'Robust blind watermarking', 'Deep learning', 'Convolutional networks', 'Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_40');
INSERT INTO `paper` VALUES (10768, 'Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition', 'Fine-grained visual recognition', 'Cross-layer interaction', 'Hierarchical bilinear pooling', '', '', 'Fine-grained visual recognition is challenging because it highly relies on the modeling of various semantic parts and fine-grained feature learning. Bilinear pooling based models have been shown to be effective at fine-grained recognition, while most previous approaches neglect the fact that inter-layer part feature interaction and fine-grained feature learning are mutually correlated and can reinforce each other. In this paper, we present a novel model to address these issues. First, a cross-layer bilinear pooling approach is proposed to capture the inter-layer part feature relations, which results in superior performance compared with other bilinear pooling based approaches. Second, we propose a novel hierarchical bilinear pooling framework to integrate multiple cross-layer bilinear features to enhance their representation capability. Our formulation is intuitive, efficient and achieves state-of-the-art results on the widely used fine-grained recognition datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_35');
INSERT INTO `paper` VALUES (10769, 'Hierarchical Metric Learning and Matching for 2D and 3D Geometric Correspondences', 'Hierarchical metric learning', 'Hierarchical matching', 'Geometric correspondences', 'Dense correspondences', '', 'Interest point descriptors have fueled progress on almost every problem in computer vision. Recent advances in deep neural networks have enabled task-specific learned descriptors that outperform hand-crafted descriptors on many problems. We demonstrate that commonly used metric learning approaches do not optimally leverage the feature hierarchies learned in a Convolutional Neural Network (CNN), especially when applied to the task of geometric feature matching. While a metric loss applied to the deepest layer of a CNN, is often expected to yield ideal features irrespective of the task, in fact the growing receptive field as well as striding effects cause shallower features to be better at high precision matching tasks. We leverage this insight together with explicit supervision at multiple levels of the feature hierarchy for better regularization, to learn more effective descriptors in the context of geometric matching tasks. Further, we propose to use activation maps at different layers of a CNN, as an effective and principled replacement for the multi-resolution image pyramids often used for matching tasks. We propose concrete CNN architectures employing these ideas, and evaluate them on multiple datasets for 2D and 3D geometric matching as well as optical flow, demonstrating state-of-the-art results and generalization across datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_49');
INSERT INTO `paper` VALUES (10770, 'Hierarchical Relational Networks for Group Activity Recognition and Retrieval', '', '', '', '', '', 'Modeling structured relationships between people in a scene is an important step toward visual understanding. We present a Hierarchical Relational Network that computes relational representations of people, given graph structures describing potential interactions. Each relational layer is fed individual person representations and a potential relationship graph. Relational representations of each person are created based on their connections in this particular graph. We demonstrate the efficacy of this model by applying it in both supervised and unsupervised learning paradigms. First, given a video sequence of people doing a collective activity, the relational scene representation is utilized for multi-person activity recognition. Second, we propose a Relational Autoencoder model for unsupervised learning of features for action and scene retrieval. Finally, a Denoising Autoencoder variant is presented to infer missing people in the scene from their context. Empirical results demonstrate that this approach learns relational feature representations that can effectively discriminate person and group activity classes.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_44');
INSERT INTO `paper` VALUES (10771, 'Hierarchical Video Frame Sequence Representation with Deep Convolutional Graph Network', 'Video classification', 'Sequence representation', 'Graph neural network', 'Deep convolutional neural network', '', 'High accuracy video label prediction (classification) models are attributed to large scale data. These data could be frame feature sequences extracted by a pre-trained convolutional-neural-network, which promote the efficiency for creating models. Unsupervised solutions such as feature average pooling, as a simple label-independent parameter-free based method, has limited ability to represent the video. While the supervised methods, like RNN, can greatly improve the recognition accuracy. However, the video length is usually long, and there are hierarchical relationships between frames across events in the video, the performance of RNN based models are decreased. In this paper, we proposes a novel video classification method based on a deep convolutional graph neural network (DCGN). The proposed method utilize the characteristics of the hierarchical structure of the video, and performed multi-level feature extraction on the video frame sequence through the graph network, obtained a video representation reflecting the event semantics hierarchically. We test our model on YouTube-8M Large-Scale Video Understanding dataset, and the result outperforms RNN based benchmarks.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_24');
INSERT INTO `paper` VALUES (10772, 'Hierarchical Video Understanding', 'Video understanding', 'Hierarchical models', 'Fine-grained targets', 'Video classification', 'Video captioning', 'We introduce a hierarchical architecture for video understanding that exploits the structure of real world actions by capturing targets at different levels of granularity. We design the model such that it first learns simpler coarse-grained tasks, and then moves on to learn more fine-grained targets. The model is trained with a joint loss on different granularity levels. We demonstrate empirical results on the recent release of Something-Something (Second release of Something-Something is used throughout this paper) dataset, which provides a hierarchy of targets, namely coarse-grained action groups, fine-grained action categories, and captions. Experiments suggest that models that exploit targets at different levels of granularity achieve better performance on all levels.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_53');
INSERT INTO `paper` VALUES (10773, 'Hierarchy of Alternating Specialists for Scene Recognition', 'Deep learning', 'Hierarchy of specialists', 'Scene recognition', '', '', 'We introduce a method for improving convolutional neural networks (CNNs) for scene classification. We present a hierarchy of specialist networks, which disentangles the intra-class variation and inter-class similarity in a coarse to fine manner. Our key insight is that each subset within a class is often associated with different types of inter-class similarity. This suggests that existing network of experts approaches that organize classes into coarse categories are suboptimal. In contrast, we group images based on high-level appearance features rather than their class membership and dedicate a specialist model per group. In addition, we propose an alternating architecture with a global ordered- and a global orderless-representation to account for both the coarse layout of the scene and the transient objects. We demonstrate that it leads to better performance than using a single type of representation as well as the fused features. We also introduce a mini-batch soft k-means that allows end-to-end fine-tuning, as well as a novel routing function for assigning images to specialists. Experimental results show that the proposed approach achieves a significant improvement over baselines including the existing tree-structured CNNs with class-based grouping.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_28');
INSERT INTO `paper` VALUES (10774, 'High Quality Facial Surface and Texture Synthesis via Generative Adversarial Networks', '', '', '', '', '', 'In the past several decades, many attempts have been made to model synthetic realistic geometric data. The goal of such models is to generate plausible 3D geometries and textures. Perhaps the best known of its kind is the linear 3D morphable model (3DMM) for faces. Such models can be found at the core of many computer vision applications such as face reconstruction, recognition and authentication to name just a few.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_36');
INSERT INTO `paper` VALUES (10775, 'Highly-Economized Multi-view Binary Compression for Scalable Image Clustering', 'Large-scale image clustering', 'Binary code learning', 'Binary clustering', 'Multi-view features', '', 'How to economically cluster large-scale multi-view images is a long-standing problem in computer vision. To tackle this challenge, we introduce a novel approach named Highly-economized Scalable Image Clustering (HSIC) that radically surpasses conventional image clustering methods via binary compression. We intuitively unify the binary representation learning and efficient binary cluster structure learning into a joint framework. In particular, common binary representations are learned by exploiting both sharable and individual information across multiple views to capture their underlying correlations. Meanwhile, cluster assignment with robust binary centroids is also performed via effective discrete optimization under \\(\\ell _{21}\\)-norm constraint. By this means, heavy continuous-valued Euclidean distance computations can be successfully reduced by efficient binary XOR operations during the clustering procedure. To our best knowledge, HSIC is the first binary clustering work specifically designed for scalable multi-view image clustering. Extensive experimental results on four large-scale image datasets show that HSIC consistently outperforms the state-of-the-art approaches, whilst significantly reducing computational time and memory footprint.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_44');
INSERT INTO `paper` VALUES (10776, 'Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image', '3D scene parsing and reconstruction', 'Analysis-by-synthesis', 'Holistic Scene Grammar', 'Markov chain Monte Carlo', '', 'We propose a computational framework to jointly parse a single RGB image and reconstruct a holistic 3D configuration composed by a set of CAD models using a stochastic grammar model. Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene structure, which characterizes a joint distribution over the functional and geometric space of indoor scenes. The proposed HSG captures three essential and often latent dimensions of the indoor scenes: (i) latent human context, describing the affordance and the functionality of a room arrangement, (ii) geometric constraints over the scene configurations, and (iii) physical constraints that guarantee physically plausible parsing and reconstruction. We solve this joint parsing and reconstruction problem in an analysis-by-synthesis fashion, seeking to minimize the differences between the input image and the rendered images generated by our 3D representation, over the space of depth, surface normal, and object segmentation map. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable solution space, jointly optimizing object localization, 3D layout, and hidden human context. Experimental results demonstrate that the proposed algorithm improves the generalization ability and significantly outperforms prior methods on 3D layout estimation, 3D object detection, and holistic scene understanding.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_12');
INSERT INTO `paper` VALUES (10777, 'How Clever Is the FiLM Model, and How Clever Can it Be?', 'VQA', 'Synthetic data', 'Evaluation', 'Deep learning', '', 'The FiLM model achieves close-to-perfect performance on the diagnostic CLEVR dataset and is distinguished from other such models by having a comparatively simple and easily transferable architecture. In this paper, we investigate in more detail the ability of FiLM to learn various linguistic constructions. Our results indicate that (a) FiLM is not able to learn relational statements straight away except for very simple instances, (b) training on a broader set of instances as well as pretraining on simpler instance types can help alleviate these learning difficulties, (c) mixing is less robust than pretraining and very sensitive to the compositional structure of the dataset. Overall, our results suggest that the approach of big all-encompassing datasets and the paradigm of “the effectiveness of data” may have fundamental limitations.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_15');
INSERT INTO `paper` VALUES (10778, 'How Good Is My GAN?', '', '', '', '', '', 'Generative adversarial networks (GANs) are one of the most popular methods for generating images today. While impressive results have been validated by visual inspection, a number of quantitative criteria have emerged only recently. We argue here that the existing ones are insufficient and need to be in adequation with the task at hand. In this paper we introduce two measures based on image classification—GAN-train and GAN-test, which approximate the recall (diversity) and precision (quality of the image) of GANs respectively. We evaluate a number of recent GAN approaches based on these two measures and demonstrate a clear difference in performance. Furthermore, we observe that the increasing difficulty of the dataset, from CIFAR10 over CIFAR100 to ImageNet, shows an inverse correlation with the quality of the GANs, as clearly evident from our measures.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_14');
INSERT INTO `paper` VALUES (10779, 'How Local Is the Local Diversity? Reinforcing Sequential Determinantal Point Processes with Dynamic Ground Sets for Supervised Video Summarization', '', '', '', '', '', 'The large volume of video content and high viewing frequency demand automatic video summarization algorithms, of which a key property is the capability of modeling diversity. If videos are lengthy like hours-long egocentric videos, it is necessary to track the temporal structures of the videos and enforce local diversity. The local diversity refers to that the shots selected from a short time duration are diverse but visually similar shots are allowed to co-exist in the summary if they appear far apart in the video. In this paper, we propose a novel probabilistic model, built upon SeqDPP, to dynamically control the time span of a video segment upon which the local diversity is imposed. In particular, we enable SeqDPP to learn to automatically infer how local the local diversity is supposed to be from the input video. The resulting model is extremely involved to train by the hallmark maximum likelihood estimation (MLE), which further suffers from the exposure bias and non-differentiable evaluation metrics. To tackle these problems, we instead devise a reinforcement learning algorithm for training the proposed model. Extensive experiments verify the advantages of our model and the new learning algorithm over MLE-based methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_10');
INSERT INTO `paper` VALUES (10780, 'How to Make an RGBD Tracker?', 'Visual object tracking', 'RGBD tracking', '', '', '', 'We propose a generic framework for converting an arbitrary short-term RGB tracker into an RGBD tracker. The proposed framework has two mild requirements – the short-term tracker provides a bounding box and its object model update can be stopped and resumed. The core of the framework is a depth augmented foreground segmentation which is formulated as an energy minimization problem solved by graph cuts. The proposed framework offers two levels of integration. The first requires that the RGB tracker can be stopped and resumed according to the decision on target visibility. The level-two integration requires that the tracker accept an external mask (foreground region) in the target update. We integrate in the proposed framework the Discriminative Correlation Filter (DCF), and three state-of-the-art trackers – Efficient Convolution Operators for Tracking (ECOhc, ECOgpu) and Discriminative Correlation Filter with Channel and Spatial Reliability (CSR-DCF). Comprehensive experiments on Princeton Tracking Benchmark (PTB) show that level-one integration provides significant improvements for all trackers: DCF average rank improves from 18th to 17th, ECOgpu from 16th to 10th, ECOhc from 15th to 5th and CSR-DCF from 19th to 14th. CSR-DCF with level-two integration achieves the top rank by a clear margin on PTB. Our framework is particularly powerful in occlusion scenarios where it provides 13.5% average improvement and 26% for the best tracker (CSR-DCF).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_8');
INSERT INTO `paper` VALUES (10781, 'How to Read Paintings: Semantic Art Understanding with Multi-modal Retrieval', 'Semantic art understanding', 'Art analysis', 'Image-text retrieval', 'Multi-modal retrieval', '', 'Automatic art analysis has been mostly focused on classifying artworks into different artistic styles. However, understanding an artistic representation involves more complex processes, such as identifying the elements in the scene or recognizing author influences. We present SemArt, a multi-modal dataset for semantic art understanding. SemArt is a collection of fine-art painting images in which each image is associated to a number of attributes and a textual artistic comment, such as those that appear in art catalogues or museum collections. To evaluate semantic art understanding, we envisage the Text2Art challenge, a multi-modal retrieval task where relevant paintings are retrieved according to an artistic text, and vice versa. We also propose several models for encoding visual and textual artistic representations into a common semantic space. Our best approach is able to find the correct image within the top 10 ranked images in the 45.5% of the test samples. Moreover, our models show remarkable levels of art understanding when compared against human evaluation.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_52');
INSERT INTO `paper` VALUES (10782, 'Human Action Recognition Based on Temporal Pose CNN and Multi-dimensional Fusion', 'Action recognition', 'Multi-stream', 'Fusion', 'Pose estimation', '', 'To take advantage of recent advances in human pose estimation from images, we develop a deep neural network model for action recognition from videos by computing temporal human pose features with a 3D CNN model. The proposed temporal pose features can provide more discriminative human action information than previous video features, such as appearance and short-term motion. In addition, we propose a novel fusion network that combines temporal pose, spatial and motion feature maps for the classification by bridging the gap between the dimension difference between 3D and 2D CNN feature maps. We show that the proposed action recognition system provides superior accuracy compared to the previous methods through experiments on Sub-JHMDB and PennAction datasets.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_33');
INSERT INTO `paper` VALUES (10783, 'Human Motion Analysis with Deep Metric Learning', '', '', '', '', '', 'Effectively measuring the similarity between two human motions is necessary for several computer vision tasks such as gait analysis, person identification and action retrieval. Nevertheless, we believe that traditional approaches such as L2 distance or Dynamic Time Warping based on hand-crafted local pose metrics fail to appropriately capture the semantic relationship across motions and, as such, are not suitable for being employed as metrics within these tasks. This work addresses this limitation by means of a triplet-based deep metric learning specifically tailored to deal with human motion data, in particular with the problem of varying input size and computationally expensive hard negative mining due to motion pair alignment. Specifically, we propose (1) a novel metric learning objective based on a triplet architecture and Maximum Mean Discrepancy; as well as, (2) a novel deep architecture based on attentive recurrent neural networks. One benefit of our objective function is that it enforces a better separation within the learned embedding space of the different motion categories by means of the associated distribution moments. At the same time, our attentive recurrent neural network allows processing varying input sizes to a fixed size of embedding while learning to focus on those motion parts that are semantically distinctive. Our experiments on two different datasets demonstrate significant improvements over conventional human motion metrics.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_41');
INSERT INTO `paper` VALUES (10784, 'Human-Centric Visual Relation Segmentation Using Mask R-CNN and VTransE', 'Human-centric', 'Visual relation segmentation', 'Mask R-CNN', 'VTransE', '', 'In this paper, we propose a novel human-centric visual relation segmentation method based on Mask R-CNN model and VTransE model. We first retain the Mask R-CNN model, and segment both human and object instances. Because Mask R-CNN may omit some human instances in instance segmentation, we further detect the omitted faces and extend them to localize the corresponding human instances. Finally, we retrain the last layer of VTransE model, and detect the visual relations between each pair of human instance and human/object instance. The experimental results show that our method obtains 0.4799, 0.4069, and 0.2681 on the criteria of R@100 with the m-IoU of 0.25, 0.50 and 0.75, respectively, which outperforms other methods in Person in Context Challenge.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_44');
INSERT INTO `paper` VALUES (10785, 'Human-Computer Interaction Approaches for the Assessment and the Practice of the Cognitive Capabilities of Elderly People', 'Virtual reality', 'Exergames', 'Human pose tracking', 'Cognitive assessment', 'Immersive environments', 'The cognitive assessment of elderly people is usually performed by means of paper-pencil tests, which may not provide an exhaustive evaluation of the cognitive abilities of the subject. Here, we analyze two solutions based on interaction in virtual environments. In particular, we consider a non-immersive exergame based on a standard tablet, and an immersive VR environment based on a head-mounted display. We show the potential use of such tools, by comparing a set of computed metrics with the results of standard clinical tests, and we discuss the potential use of such tools to perform more complex evaluations. In particular, the use of immersive environments, which could be implemented both with head-mounted displays or with configurations of stereoscopic displays, allows us to track the patients’ pose, and to analyze his/her movements and posture, when performing Activities of Daily Living, with the aim of having a further way to assess cognitive capabilities.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_5');
INSERT INTO `paper` VALUES (10786, 'HybridFusion: Real-Time Performance Capture Using a Single Depth Sensor and Sparse IMUs', 'Performance capture', 'Real-time', 'Single-view', 'IMU', '', 'We propose a light-weight yet highly robust method for real-time human performance capture based on a single depth camera and sparse inertial measurement units (IMUs). Our method combines non-rigid surface tracking and volumetric fusion to simultaneously reconstruct challenging motions, detailed geometries and the inner human body of a clothed subject. The proposed hybrid motion tracking algorithm and efficient per-frame sensor calibration technique enable non-rigid surface reconstruction for fast motions and challenging poses with severe occlusions. Significant fusion artifacts are reduced using a new confidence measurement for our adaptive TSDF-based fusion. The above contributions are mutually beneficial in our reconstruction system, which enable practical human performance capture that is real-time, robust, low-cost and easy to deploy. Experiments show that extremely challenging performances and loop closure problems can be handled successfully.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_24');
INSERT INTO `paper` VALUES (10787, 'HybridNet: Classification and Reconstruction Cooperation for Semi-supervised Learning', 'Deep learning', 'Semi-supervised learning', 'Regularization', 'Reconstruction', 'Invariance and stability', 'In this paper, we introduce a new model for leveraging unlabeled data to improve generalization performances of image classifiers: a two-branch encoder-decoder architecture called HybridNet. The first branch receives supervision signal and is dedicated to the extraction of invariant class-related representations. The second branch is fully unsupervised and dedicated to model information discarded by the first branch to reconstruct input data. To further support the expected behavior of our model, we propose an original training objective. It favors stability in the discriminative branch and complementarity between the learned representations in the two branches. HybridNet is able to outperform state-of-the-art results on CIFAR-10, SVHN and STL-10 in various semi-supervised settings. In addition, visualizations and ablation studies validate our contributions and the behavior of the model on both CIFAR-10 and STL-10 datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_10');
INSERT INTO `paper` VALUES (10788, 'ICNet for Real-Time Semantic Segmentation on High-Resolution Images', 'Real-time', 'High-resolution', 'Semantic segmentation', '', '', 'We focus on the challenging task of real-time semantic segmentation in this paper. It finds many practical applications and yet is with fundamental difficulty of reducing a large portion of computation for pixel-wise label inference. We propose an image cascade network (ICNet) that incorporates multi-resolution branches under proper label guidance to address this challenge. We provide in-depth analysis of our framework and introduce the cascade feature fusion unit to quickly achieve high-quality segmentation. Our system yields real-time inference on a single GPU card with decent quality results evaluated on challenging datasets like Cityscapes, CamVid and COCO-Stuff.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_25');
INSERT INTO `paper` VALUES (10789, 'Identification of C. elegans Strains Using a Fully Convolutional Neural Network on Behavioural Dynamics', 'Behavioural phenotyping', 'Classification', 'Deep learning', '', '', 'The nematode C. elegans is a promising model organism to understand the genetic basis of behaviour due to its anatomical simplicity. In this work, we present a deep learning model capable of discerning genetically diverse strains based only on their recorded spontaneous activity, and explore how its performance changes as different embeddings are used as input. The model outperforms hand-crafted features on strain classification when trained directly on time series of worm postures.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_35');
INSERT INTO `paper` VALUES (10790, 'Image Generation from Sketch Constraint Using Contextual GAN', 'Image generation', 'Contextual completion', '', '', '', 'In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint image’s point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_13');
INSERT INTO `paper` VALUES (10791, 'Image Inpainting for Irregular Holes Using Partial Convolutions', 'Partial convolution', 'Image inpainting', '', '', '', 'Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image, using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes (typically the mean value). This often leads to artifacts such as color discrepancy and blurriness. Post-processing is usually used to reduce such artifacts, but are expensive and may fail. We propose the use of partial convolutions, where the convolution is masked and renormalized to be conditioned on only valid pixels. We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass. Our model outperforms other methods for irregular masks. We show qualitative and quantitative comparisons with other methods to validate our approach.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_6');
INSERT INTO `paper` VALUES (10792, 'Image Manipulation with Perceptual Discriminators', 'Image translation', 'Image editing', 'Perceptual loss', 'Generative adversarial networks', '', 'Systems that perform image manipulation using deep convolutional networks have achieved remarkable realism. Perceptual losses and losses based on adversarial discriminators are the two main classes of learning objectives behind these advances. In this work, we show how these two ideas can be combined in a principled and non-additive manner for unaligned image translation tasks. This is accomplished through a special architecture of the discriminator network inside generative adversarial learning framework. The new architecture, that we call a perceptual discriminator, embeds the convolutional parts of a pre-trained deep classification network inside the discriminator network. The resulting architecture can be trained on unaligned image datasets, while benefiting from the robustness and efficiency of perceptual losses. We demonstrate the merits of the new architecture in a series of qualitative and quantitative comparisons with baseline approaches and state-of-the-art frameworks for unaligned image translation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_36');
INSERT INTO `paper` VALUES (10793, 'Image Reassembly Combining Deep Learning and Shortest Path Problem', 'Fragments reassembly', 'Jigsaw puzzle', 'Image classification', 'Cultural heritage', 'Deep learning', 'This paper addresses the problem of reassembling images from disjointed fragments. More specifically, given an unordered set of fragments, we aim at reassembling one or several possibly incomplete images. The main contributions of this work are: (1) several deep neural architectures to predict the relative position of image fragments that outperform the previous state of the art; (2) casting the reassembly problem into the shortest path in a graph problem for which we provide several construction algorithms depending on available information; (3) a new dataset of images taken from the Metropolitan Museum of Art (MET) dedicated to image reassembly for which we provide a clear setup and a strong baseline.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_10');
INSERT INTO `paper` VALUES (10794, 'Image Splicing Localization via Semi-global Network and Fully Connected Conditional Random Fields', 'Image splicing localization', 'Image forgery localization', 'Multimedia security', '', '', 'We address the problem of image splicing localization: given an input image, localizing the spliced region which is cut from another image. We formulate this as a classification task but, critically, instead of classifying the spliced region by local patch, we leverage the features from whole image and local patch together to classify patch. We call this structure Semi-Global Network. Our approach exploits the observation that the spliced region should not only highly relate to local features (spliced edges), but also global features (semantic information, illumination, etc.) from the whole image. Furthermore, we first integrate Fully Connected Conditional Random Fields as post-processing technique in image splicing to improve the consistency between the input image and the output of the network. We show that our method outperforms other state-of-the-art methods in three popular datasets.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_22');
INSERT INTO `paper` VALUES (10795, 'Image Super-Resolution Using Very Deep Residual Channel Attention Networks', 'Super-resolution', 'Residual in residual', 'Channel attention', '', '', 'Convolutional neural network (CNN) depth is of crucial importance for image super-resolution (SR). However, we observe that deeper networks for image SR are more difficult to train. The low-resolution inputs and features contain abundant low-frequency information, which is treated equally across channels, hence hindering the representational ability of CNNs. To solve these problems, we propose the very deep residual channel attention networks (RCAN). Specifically, we propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections. Each residual group contains some residual blocks with short skip connections. Meanwhile, RIR allows abundant low-frequency information to be bypassed through multiple skip connections, making the main network focus on learning high-frequency information. Furthermore, we propose a channel attention mechanism to adaptively rescale channel-wise features by considering interdependencies among channels. Extensive experiments show that our RCAN achieves better accuracy and visual improvements against state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_18');
INSERT INTO `paper` VALUES (10796, 'Image-Sensitive Language Modeling for Automatic Speech Recognition', 'Multimodal speech recognition', 'Multimodal language model', '', '', '', 'Typically language models in a speech recognizer just use the previous words as a context. Thus they are insensitive to context from the real world. This paper explores the benefits of introducing the visual modality as context information to automatic speech recognition. We use neural multimodal language models to rescore the recognition results of utterances that describe visual scenes. We provide a comprehensive survey of how much the language model improves when adding the image to the conditioning set. The image was introduced to a purely text-based RNN-LM using three different composition methods. Our experiments show that using the visual modality helps the recognition process by a \\(7.8\\%\\) relative improvement, but can also hurt the results because of overfitting to the visual input.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_16');
INSERT INTO `paper` VALUES (10797, 'Image-to-Voxel Model Translation with Conditional Adversarial Networks', 'Conditional GAN', 'Voxel model', '6D pose estimation', '', '', 'We present a single-view voxel model prediction method that uses generative adversarial networks. Our method utilizes correspondences between 2D silhouettes and slices of a camera frustum to predict a voxel model of a scene with multiple object instances. We exploit pyramid shaped voxel and a generator network with skip connections between 2D and 3D feature maps. We collected two datasets VoxelCity and VoxelHome to train our framework with 36,416 images of 28 scenes with ground-truth 3D models, depth maps, and 6D object poses. We made the datasets publicly available (http://www.zefirus.org/Z_GAN). We evaluate our framework on 3D shape datasets to show that it delivers robust 3D scene reconstruction results that compete with and surpass state-of-the-art in a scene reconstruction with multiple non-rigid objects.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_37');
INSERT INTO `paper` VALUES (10798, 'Images of Image Machines. Visual Interpretability in Computer Vision for Art', 'Interpretability', 'Feature visualization', 'Digital art history', 'Representation', '', 'Despite the emergence of interpretable machine learning as a distinct area of research, the role and possible uses of interpretability in digital art history are still unclear. Focusing on feature visualization as the most common technical manifestation of visual interpretability, we argue that in computer vision for art visual interpretability is desirable, if not indispensable. We propose that feature visualization images can be a useful tool if they are used in a non-traditional way that embraces their peculiar representational status. Moreover, we suggest that exactly because of this peculiar representational status, feature visualization images themselves deserve more attention from the computer vision and digital art history communities.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_54');
INSERT INTO `paper` VALUES (10799, 'Imagine This! Scripts to Compositions to Videos', '', '', '', '', '', 'Imagining a scene described in natural language with realistic layout and appearance of entities is the ultimate test of spatial, visual, and semantic world knowledge. Towards this goal, we present the Composition, Retrieval and Fusion Network (Craft), a model capable of learning this knowledge from video-caption data and applying it while generating videos from novel captions. Craft explicitly predicts a temporal-layout of mentioned entities (characters and objects), retrieves spatio-temporal entity segments from a video database and fuses them to generate scene videos. Our contributions include sequential training of components of Craft while jointly modeling layout and appearances, and losses that encourage learning compositional representations for retrieval. We evaluate Craft on semantic fidelity to caption, composition consistency, and visual quality. Craft outperforms direct pixel generation approaches and generalizes well to unseen captions and to unseen video databases with no text annotations. We demonstrate Craft on Flintstones (Flintstones is available at https://prior.allenai.org/projects/craft), a new richly annotated video-caption dataset with over 25000 videos. For a glimpse of videos generated by Craft, see https://youtu.be/688Vv86n0z8.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_37');
INSERT INTO `paper` VALUES (10800, 'Imitation Learning of Path-Planned Driving Using Disparity-Depth Images', 'End-to-End training', 'Autonomous driving', 'Path planning', 'Collision avoidance', 'Depth images', 'Sensor data representation in autonomous driving is a defining factor for the final performance and convergence of End-to-End trained driving systems. When theoretically a network, trained in a perfect way, should be able to abstract the most useful information from camera data depending on the task, practically this is a challenge. Therefore, many approaches explore leveraging human designed intermediate representations as segmented images. We continue work in the field of depth-image based steering angle prediction and compare networks trained purely on either RGB-stereo images or depth-from-stereo (disparity) images. Since no dedicated depth sensor is used, we consider this as a pixel grouping method where pixel are labeled by their stereo disparity instead of relying on human segment annotations.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_33');
INSERT INTO `paper` VALUES (10801, 'Implicit 3D Orientation Learning for 6D Object Detection from RGB Images', '6D object detection', 'Pose estimation', 'Domain Randomization', 'Autoencoder', 'Synthetic data', 'We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_43');
INSERT INTO `paper` VALUES (10802, 'Improved Dictionary Learning with Enriched Information for Biomedical Images', '', '', '', '', '', 'With dictionary learning using k-means or k-means++, the optimal value of k is traditionally determined empirically using a validation set. The optimal k, which should depend on the particular problem, is chosen with previously determined values from prior work. We argue that there is rich information from clustering with a number of values of k. We propose a novel method to extract information from clustering with all reasonable values of k at the same time. It is shown that our method improves the performance of dictionary learning for the popular bag-of-features model in image classification with simple patterns like cells such as biomedical images. Our experiments demonstrate that, our proposed dictionary learning method outperforms popular methods, on two well-known datasets by 12.5\\(\\%\\) and 8.5\\(\\%\\) compared to k-means/k-means++ dictionary learning and by 8.9\\(\\%\\) and 6.1\\(\\%\\) compared to sparse coding.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_27');
INSERT INTO `paper` VALUES (10803, 'Improved Structure from Motion Using Fiducial Marker Matching', 'Structure from motion', 'SFM', 'Fiducial markers', '3D reconstruction', 'Simultaneous localization and mapping', 'In this paper, we present an incremental structure from motion (SfM) algorithm that significantly outperforms existing algorithms when fiducial markers are present in the scene, and that matches the performance of existing algorithms when no markers are present. Our algorithm uses markers to limit potential incorrect image matches, change the order in which images are added to the reconstruction, and enforce new bundle adjustment constraints. To validate our algorithm, we introduce a new dataset with 16 image collections of large indoor scenes with challenging characteristics (e.g., blank hallways, glass facades, brick walls) and with markers placed throughout. We show that our algorithm produces complete, accurate reconstructions on all 16 image collections, most of which cause other algorithms to fail. Further, by selectively masking fiducial markers, we show that the presence of even a small number of markers can improve the results of our algorithm.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_17');
INSERT INTO `paper` VALUES (10804, 'Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association', 'Person re-identification', 'Local-global language association', 'Image-text correspondence', '', '', 'Person re-identification is an important task that requires learning discriminative visual features for distinguishing different person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for effective visual features. Compared with other auxiliary information, language can describe a specific person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the effectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_4');
INSERT INTO `paper` VALUES (10805, 'Improving DNN Robustness to Adversarial Attacks Using Jacobian Regularization', 'Deep learning', 'Neural networks', 'Adversarial examples', 'Data perturbation', 'Jacobian regularization', 'Deep neural networks have lately shown tremendous performance in various applications including vision and speech processing tasks. However, alongside their ability to perform these tasks with such high accuracy, it has been shown that they are highly susceptible to adversarial attacks: a small change in the input would cause the network to err with high confidence. This phenomenon exposes an inherent fault in these networks and their ability to generalize well. For this reason, providing robustness to adversarial attacks is an important challenge in networks training, which has led to extensive research. In this work, we suggest a theoretically inspired novel approach to improve the networks’ robustness. Our method applies regularization using the Frobenius norm of the Jacobian of the network, which is applied as post-processing, after regular training has finished. We demonstrate empirically that it leads to enhanced robustness results with a minimal change in the original network’s accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_32');
INSERT INTO `paper` VALUES (10806, 'Improving Generalization via Scalable Neighborhood Component Analysis', 'k-nearest neighbors', 'Large-scale object recognition', 'Neighborhood component analysis', 'Transfer learning', 'Few-shot learning', 'Current major approaches to visual recognition follow an end-to-end formulation that classifies an input image into one of the pre-determined set of semantic categories. Parametric softmax classifiers are a common choice for such a closed world with fixed categories, especially when big labeled data is available during training. However, this becomes problematic for open-set scenarios where new categories are encountered with very few examples for learning a generalizable parametric classifier. We adopt a non-parametric approach for visual recognition by optimizing feature embeddings instead of parametric classifiers. We use a deep neural network to learn the visual feature that preserves the neighborhood structure in the semantic space, based on the Neighborhood Component Analysis (NCA) criterion. Limited by its computational bottlenecks, we devise a mechanism to use augmented memory to scale NCA for large datasets and very deep networks. Our experiments deliver not only remarkable performance on ImageNet classification for such a simple non-parametric method, but most importantly a more generalizable feature representation for sub-category discovery and few-shot recognition.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_42');
INSERT INTO `paper` VALUES (10807, 'Improving Sequential Determinantal Point Processes for Supervised Video Summarization', 'Determinantal Point Process (DPP)', 'Egocentric Video', 'Large Margin Algorithm', 'Exposure Bias', 'Generalized DPP (GDPP)', 'It is now much easier than ever before to produce videos. While the ubiquitous video data is a great source for information discovery and extraction, the computational challenges are unparalleled. Automatically summarizing the videos has become a substantial need for browsing, searching, and indexing visual content. This paper is in the vein of supervised video summarization using sequential determinantal point processes (SeqDPPs), which models diversity by a probabilistic distribution. We improve this model in two folds. In terms of learning, we propose a large-margin algorithm to address the exposure bias problem in SeqDPP. In terms of modeling, we design a new probabilistic distribution such that, when it is integrated into SeqDPP, the resulting model accepts user input about the expected length of the summary. Moreover, we also significantly extend a popular video summarization dataset by (1) more egocentric videos, (2) dense user annotations, and (3) a refined evaluation scheme. We conduct extensive experiments on this dataset (about 60 h of videos in total) and compare our approach to several competitive baselines.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_32');
INSERT INTO `paper` VALUES (10808, 'Improving Shape Deformation in Unsupervised Image-to-Image Translation', 'Generative adversarial networks', 'Image translation', '', '', '', 'Unsupervised image-to-image translation techniques are able to map local texture between two domains, but they are typically unsuccessful when the domains require larger shape change. Inspired by semantic segmentation, we introduce a discriminator with dilated convolutions that is able to use information from across the entire image to train a more context-aware generator. This is coupled with a multi-scale perceptual loss that is better able to represent error in the underlying shape of objects. We demonstrate that this design is more capable of representing shape deformation in a challenging toy dataset, plus in complex mappings with significant dataset variation between humans, dolls, and anime faces, and between cats and dogs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_40');
INSERT INTO `paper` VALUES (10809, 'Improving Spatiotemporal Self-supervision by Deep Reinforcement Learning', 'Deep reinforcement learning', 'Self-supervision', 'Shuffling', 'Action recognition', 'Image understanding', 'Self-supervised learning of convolutional neural networks can harness large amounts of cheap unlabeled data to train powerful feature representations. As surrogate task, we jointly address ordering of visual data in the spatial and temporal domain. The permutations of training samples, which are at the core of self-supervision by ordering, have so far been sampled randomly from a fixed preselected set. Based on deep reinforcement learning we propose a sampling policy that adapts to the state of the network, which is being trained. Therefore, new permutations are sampled according to their expected utility for updating the convolutional feature representation. Experimental evaluation on unsupervised and transfer learning tasks demonstrates competitive performance on standard benchmarks for image and video classification and nearest neighbor retrieval.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_47');
INSERT INTO `paper` VALUES (10810, 'Improving Thin Structures in Surface Reconstruction from Sparse Point Cloud', 'Thin structures', 'Sparse features', '3D Delaunay triangulation', 'Visibility', 'Environment modeling', 'Methods were proposed to estimate a surface from a sparse cloud of points reconstructed from images. These methods are interesting in several contexts including large scale scenes, limited computational resources and initialization of dense stereo. However they are deficient in presence of thin structures such as posts, which are often present in both urban and natural scenes: these scene components can be partly or even completely removed. Here we reduce this problem by introducing a pre-processing, assuming that (1) some of the points form polygonal chains approximating curves and occluding contours of the scene and (2) the direction of the thin structures is roughly known (e.g. vertical). In the experiments, our pre-processing improves the results of two different surface reconstruction methods applied on videos taken by helmet-held 360 cameras.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_27');
INSERT INTO `paper` VALUES (10811, 'In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video', '', '', '', '', '', 'We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. We propose a novel deep model for joint gaze estimation and action recognition in First Person Vision. Our method describes the participant’s gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We sample from these stochastic units to generate an attention map. This attention map guides the aggregation of visual features in action recognition, thereby providing coupling between gaze and action. We evaluate our method on the standard EGTEA dataset and demonstrate performance that exceeds the state-of-the-art by a significant margin of \\(3.5\\%\\).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_38');
INSERT INTO `paper` VALUES (10812, 'Incomplete Multi-view Clustering via Graph Regularized Matrix Factorization', 'Multi-view clustering', 'Incomplete view', 'Common latent representation', 'Out-of-sample', '', 'Clustering with incomplete views is a challenge in multi-view clustering. In this paper, we provide a novel and simple method to address this issue. Specially, the proposed method simultaneously exploits the local information of each view and the complementary information among views to learn the common latent representation for all samples, which can greatly improve the compactness and discriminability of the obtained representation. Compared with the conventional graph embedding methods, the proposed method does not introduce any extra regularization term and corresponding penalty parameter to preserve the local structure of data, and thus does not increase the burden of extra parameter selection. By imposing the orthogonal constraint on the basis matrix of each view, the proposed method is able to handle the out-of-sample. Moreover, the proposed method can be viewed as a unified framework for multi-view learning since it can handle both incomplete and complete multi-view clustering and classification tasks. Extensive experiments conducted on several multi-view datasets prove that the proposed method can significantly improve the clustering performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_47');
INSERT INTO `paper` VALUES (10813, 'Increasing the Robustness of CNN-Based Human Body Segmentation in Range Images by Modeling Sensor-Specific Artifacts', 'Human body segmentation', 'Structured-light imaging', 'Convolutional neural network', '', '', 'This paper addresses the problem of human body parts segmentation in range images acquired using a structured-light imaging system. We propose a solution based on a fully convolutional neural network trained on realistic synthetic data that were simulated in a way that closely emulates our structured-light imaging system with its inherent artifacts such as occlusions, noise and missing data. The results on synthetic test data demonstrate quantitatively the performance of our method in identifying 33 body parts, with negligible confusion between the front and back sides of the body and between the left and right limbs. Our experiments highlight the importance of sensor-specific data augmentation in the training set to improve the robustness of the segmentation. Most importantly, when applied to range data actually acquired by our system, the method was capable of accurately segmenting the different body parts with inter-frame consistency in real-time.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_55');
INSERT INTO `paper` VALUES (10814, 'Incremental Multi-graph Matching via Diversity and Randomness Based Graph Clustering', 'Multi-graph matching', 'Incremental graph matching', 'Determinantal point process', 'Graph clustering', '', 'Multi-graph matching refers to finding correspondences across graphs, which are traditionally solved by matching all the graphs in a single batch. However in real-world applications, graphs are often collected incrementally, rather than once for all. In this paper, we present an incremental multi-graph matching approach, which deals with the arriving graph utilizing the previous matching results under the global consistency constraint. When a new graph arrives, rather than re-optimizing over all graphs, we propose to partition graphs into subsets with certain topological structure and conduct optimization within each subset. The partitioning procedure is guided by the diversity within partitions and randomness over iterations, and we present an interpretation showing why these two factors are essential. The final matching results are calculated over all subsets via an intersection graph. Extensive experimental results on synthetic and real image datasets show that our algorithm notably improves the efficiency without sacrificing the accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_9');
INSERT INTO `paper` VALUES (10815, 'Incremental Non-Rigid Structure-from-Motion with Unknown Focal Length', 'Unknown Focal Length', 'Non-Rigid Structure-from-Motion (NRSfM)', 'Perspective Camera', 'Euclidean Distance Location', 'Correct Intrinsics', 'The perspective camera and the isometric surface prior have recently gathered increased attention for Non-Rigid Structure-from-Motion (NRSfM). Despite the recent progress, several challenges remain, particularly the computational complexity and the unknown camera focal length. In this paper we present a method for incremental Non-Rigid Structure-from-Motion (NRSfM) with the perspective camera model and the isometric surface prior with unknown focal length. In the template-based case, we provide a method to estimate four parameters of the camera intrinsics. For the template-less scenario of NRSfM, we propose a method to upgrade reconstructions obtained for one focal length to another based on local rigidity and the so-called Maximum Depth Heuristics (MDH). On its basis we propose a method to simultaneously recover the focal length and the non-rigid shapes. We further solve the problem of incorporating a large number of points and adding more views in MDH-based NRSfM and efficiently solve them with Second-Order Cone Programming (SOCP). This does not require any shape initialization and produces results orders of times faster than many methods. We provide evaluations on standard sequences with ground-truth and qualitative reconstructions on challenging YouTube videos. These evaluations show that our method performs better in both speed and accuracy than the state of the art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_46');
INSERT INTO `paper` VALUES (10816, 'Inferring Human Knowledgeability from Eye Gaze in Mobile Learning Environments', 'Assistive mobile applications', 'Noninvasive gaze tracking', 'Analysis of eye movements', 'Human knowledgeability prediction', '', 'What people look at during a visual task reflects an interplay between ocular motor functions and cognitive processes. In this paper, we study the links between eye gaze and cognitive states to investigate whether eye gaze reveal information about an individual’s knowledgeability. We focus on a mobile learning scenario where a user and a virtual agent play a quiz game using a hand-held mobile device. To the best of our knowledge, this is the first attempt to predict user’s knowledgeability from eye gaze using a noninvasive eye tracking method on mobile devices: we perform gaze estimation using front-facing camera of mobile devices in contrast to using specialised eye tracking devices. First, we define a set of eye movement features that are discriminative for inferring user’s knowledgeability. Next, we train a model to predict users’ knowledgeability in the course of responding to a question. We obtain a classification performance of 59.1% achieving human performance, using eye movement features only, which has implications for (1) adapting behaviours of the virtual agent to user’s needs (e.g., virtual agent can give hints); (2) personalising quiz questions to the user’s perceived knowledgeability.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_13');
INSERT INTO `paper` VALUES (10817, 'Inner Space Preserving Generative Pose Machine', 'conditional Generative adversarial networks (cGANS)', 'Inner space preserving', 'Generative pose models', 'Articulated bodies', '', 'Image-based generative methods, such as generative adversarial networks (GANs) have already been able to generate realistic images with much context control, specially when they are conditioned. However, most successful frameworks share a common procedure which performs an image-to-image translation with pose of figures in the image untouched. When the objective is reposing a figure in an image while preserving the rest of the image, the state-of-the-art mainly assumes a single rigid body with simple background and limited pose shift, which can hardly be extended to the images under normal settings. In this paper, we introduce an image “inner space” preserving model that assigns an interpretable low-dimensional pose descriptor (LDPD) to an articulated figure in the image. Figure reposing is then generated by passing the LDPD and the original image through multi-stage augmented hourglass networks in a conditional GAN structure, called inner space preserving generative pose machine (ISP-GPM). We evaluated ISP-GPM on reposing human figures, which are highly articulated with versatile variations. Test of a state-of-the-art pose estimator on our reposed dataset gave an accuracy over 80% on PCK0.5 metric. The results also elucidated that our ISP-GPM is able to preserve the background with high accuracy while reasonably recovering the area blocked by the figure to be reposed.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_44');
INSERT INTO `paper` VALUES (10818, 'Instance Segmentation of Neural Cells', 'Neural cell', 'Instance segmentation', 'Cell detection', 'Cell segmentation', '', 'Instance segmentation of neural cells plays an important role in brain study. However, this task is challenging due to the special shapes and behaviors of neural cells. Existing methods are not precise enough to capture their tiny structures, e.g., filopodia and lamellipodia, which are critical to the understanding of cell interaction and behavior. To this end, we propose a novel deep multi-task learning model to jointly detect and segment neural cells instance-wise. Our method is built upon SSD, with ResNet101 as the backbone to achieve both high detection accuracy and fast speed. Furthermore, unlike existing works which tend to produce wavy and inaccurate boundaries, we embed a deconvolution module into SSD to better capture details. Experiments on a dataset of neural cell microscopic images show that our method is able to achieve better performance in terms of accuracy and efficiency, comparing favorably with current state-of-the-art methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_29');
INSERT INTO `paper` VALUES (10819, 'Instance-Level Human Parsing via Part Grouping Network', 'Instance-level human parsing', 'Semantic part segmentation', 'Part grouping network', '', '', 'Instance-level human parsing towards real-world human analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in parsing multiple instances in a single pass. Several related works all follow the “parsing-by-detection” pipeline that heavily relies on separately trained detection models to localize instances and then performs human parsing for each instance sequentially. Nonetheless, two discrepant optimization targets of detection and parsing lead to suboptimal representation learning and error accumulation for final results. In this work, we make the first attempt to explore a detection-free Part Grouping Network (PGN) for efficiently parsing multiple people in an image in a single pass. Our PGN reformulates instance-level human parsing as two twinned sub-tasks that can be jointly learned and mutually refined via a unified network: (1) semantic part segmentation for assigning each pixel as a human part (e.g., face, arms); (2) instance-aware edge detection to group semantic parts into distinct person instances. Thus the shared intermediate representation would be endowed with capabilities in both characterizing fine-grained parts and inferring instance belongings of each part. Finally, a simple instance partition process is employed to get final results during inference. We conducted experiments on PASCAL-Person-Part dataset and our PGN outperforms all state-of-the-art methods. Furthermore, we show its superiority on a newly collected multi-person parsing dataset (CIHP) including 38,280 diverse images, which is the largest dataset so far and can facilitate more advanced human analysis. The CIHP benchmark and our source code are available at http://sysu-hcp.net/lip/.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_47');
INSERT INTO `paper` VALUES (10820, 'Integral Human Pose Regression', 'Integral regression', 'Human pose estimation', 'Deep learning', '', '', 'State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as non-differentiable post-processing and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint regression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is convincingly validated via comprehensive ablation experiments under various settings, specifically on 3D pose estimation, for the first time.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_33');
INSERT INTO `paper` VALUES (10821, 'Integrating Egocentric Videos in Top-View Surveillance Videos: Joint Identification and Temporal Alignment', 'Egocentric Video', 'Temporal Alignment', 'Camera Holder', 'Graph Cuts', 'Visual Reasoning', 'Videos recorded from first person (egocentric) perspective have little visual appearance in common with those from third person perspective, especially with videos captured by top-view surveillance cameras. In this paper, we aim to relate these two sources of information from a surveillance standpoint, namely in terms of identification and temporal alignment. Given an egocentric video and a top-view video, our goals are to: (a) identify the egocentric camera holder in the top-view video (self-identification), (b) identify the humans visible in the content of the egocentric video, within the content of the top-view video (re-identification), and (c) temporally align the two videos. The main challenge is that each of these tasks is highly dependent on the other two. We propose a unified framework to jointly solve all three problems. We evaluate the efficacy of the proposed approach on a publicly available dataset containing a variety of videos recorded in different scenarios.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_18');
INSERT INTO `paper` VALUES (10822, 'Interaction-Aware Spatio-Temporal Pyramid Attention Networks for Action Classification', '', '', '', '', '', 'Local features at neighboring spatial positions in feature maps have high correlation since their receptive fields are often overlapped. Self-attention usually uses the weighted sum (or other functions) with internal elements of each local feature to obtain its weight score, which ignores interactions among local features. To address this, we propose an effective interaction-aware self-attention model inspired by PCA to learn attention maps. Furthermore, since different layers in a deep network capture feature maps of different scales, we use these feature maps to construct a spatial pyramid and then utilize multi-scale information to obtain more accurate attention scores, which are used to weight the local features in all spatial positions of feature maps to calculate attention maps. Moreover, our spatial pyramid attention is unrestricted to the number of its input feature maps so it is easily extended to a spatio-temporal version. Finally, our model is embedded in general CNNs to form end-to-end attention networks for action classification. Experimental results show that our method achieves the state-of-the-art results on the UCF101, HMDB51 and untrimmed Charades.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_23');
INSERT INTO `paper` VALUES (10823, 'Interactive Boundary Prediction for Object Selection', 'Boundary Prediction', 'Boundary-based Segmentation', 'Interactive Segmentation', 'Semantic Edges', 'Input Control Points', 'Interactive image segmentation is critical for many image editing tasks. While recent advanced methods on interactive segmentation focus on the region-based paradigm, more traditional boundary-based methods such as Intelligent Scissor are still popular in practice as they allow users to have active control of the object boundaries. Existing methods for boundary-based segmentation solely rely on low-level image features, such as edges for boundary extraction, which limits their ability to adapt to high-level image content and user intention. In this paper, we introduce an interaction-aware method for boundary-based image segmentation. Instead of relying on pre-defined low-level image features, our method adaptively predicts object boundaries according to image content and user interactions. Therein, we develop a fully convolutional encoder-decoder network that takes both the image and user interactions (e.g. clicks on boundary points) as input and predicts semantically meaningful boundaries that match user intentions. Our method explicitly models the dependency of boundary extraction results on image content and user interactions. Experiments on two public interactive segmentation benchmarks show that our method significantly improves the boundary quality of segmentation results compared to state-of-the-art methods while requiring fewer user interactions.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_2');
INSERT INTO `paper` VALUES (10824, 'Interest Point Detectors Stability Evaluation on ApolloScape Dataset', 'Keypoint detectors', 'Interest points stability', '', '', '', 'In the recent years, a number of novel, deep-learning based, interest point detectors, such as LIFT, DELF, Superpoint or LF-Net was proposed. However there’s a lack of a standard benchmark to evaluate suitability of these novel keypoint detectors for real-live applications such as autonomous driving. Traditional benchmarks (e.g. Oxford VGG) are rather limited, as they consist of relatively few images of mostly planar scenes taken in favourable conditions. In this paper we verify if the recent, deep-learning based interest point detectors have the advantage over the traditional, hand-crafted keypoint detectors. To this end, we evaluate stability of a number of hand crafted and recent, learning-based interest point detectors on the street-level view ApolloScape dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_45');
INSERT INTO `paper` VALUES (10825, 'Interpolating Convolutional Neural Networks Using Batch Normalization', 'Neural network interpolation', 'Batch normalization', 'Few-shot learning', 'Style transfer', '', 'Perceiving a visual concept as a mixture of learned ones is natural for humans, aiding them to grasp new concepts and strengthening old ones. For all their power and recent success, deep convolutional networks do not have this ability. Inspired by recent work on universal representations for neural networks, we propose a simple emulation of this mechanism by purposing batch normalization layers to discriminate visual classes, and formulating a way to combine them to solve new tasks. We show that this can be applied for 2-way few-shot learning where we obtain between 4% and 17% better accuracy compared to straightforward full fine-tuning, and demonstrate that it can also be extended to the orthogonal application of style transfer.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_35');
INSERT INTO `paper` VALUES (10826, 'Interpretable Basis Decomposition for Visual Explanation', '', '', '', '', '', 'Explanations of the decisions made by a deep neural network are important for human end-users to be able to understand and diagnose the trustworthiness of the system. Current neural networks used for visual recognition are generally used as black boxes that do not provide any human interpretable justification for a prediction. In this work we propose a new framework called Interpretable Basis Decomposition for providing visual explanations for classification networks. By decomposing the neural activations of the input image into semantically interpretable components pre-trained from a large concept corpus, the proposed framework is able to disentangle the evidence encoded in the activation feature vector, and quantify the contribution of each piece of evidence to the final prediction. We apply our framework for providing explanations to several popular networks for visual recognition, and show it is able to explain the predictions given by the networks in a human-interpretable way. The human interpretability of the visual explanations provided by our framework and other recent explanation methods is evaluated through Amazon Mechanical Turk, showing that our framework generates more faithful and interpretable explanations (The code and data are available at https://github.com/CSAILVision/IBD).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_8');
INSERT INTO `paper` VALUES (10827, 'Interpretable Intuitive Physics Model', 'Intuitive physics', 'Interpretable models', 'Physical properties', '', '', 'Humans have a remarkable ability to use physical commonsense and predict the effect of collisions. But do they understand the underlying factors? Can they predict if the underlying factors have changed? Interestingly, in most cases humans can predict the effects of similar collisions with different conditions such as changes in mass, friction, etc. It is postulated this is primarily because we learn to model physics with meaningful latent variables. This does not imply we can estimate the precise values of these meaningful variables (estimate exact values of mass or friction). Inspired by this observation, we propose an interpretable intuitive physics model where specific dimensions in the bottleneck layers correspond to different physical properties. In order to demonstrate that our system models these underlying physical properties, we train our model on collisions of different shapes (cube, cone, cylinder, spheres etc.) and test on collisions of unseen combinations of shapes. Furthermore, we demonstrate our model generalizes well even when similar scenes are simulated with different underlying properties.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_6');
INSERT INTO `paper` VALUES (10828, 'Into the Twilight Zone: Depth Estimation Using Joint Structure-Stereo Optimization', 'Stereo matching', 'Depth estimation', 'Low-light vision', 'Structure extraction', 'Joint optimization', 'We present a joint Structure-Stereo optimization model that is robust for disparity estimation under low-light conditions. Eschewing the traditional denoising approach – which we show to be ineffective for stereo due to its artefacts and the questionable use of the PSNR metric, we propose to instead rely on structures comprising of piecewise constant regions and principal edges in the given image, as these are the important regions for extracting disparity information. We also judiciously retain the coarser textures for stereo matching, discarding the finer textures as they are apt to be inextricably mixed with noise. This selection process in the structure-texture decomposition step is aided by the stereo matching constraint in our joint Structure-Stereo formulation. The resulting optimization problem is complex but we are able to decompose it into sub-problems that admit relatively standard solutions. Our experiments confirm that our joint model significantly outperforms the baseline methods on both synthetic and real noise datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_7');
INSERT INTO `paper` VALUES (10829, 'Investigating Depth Domain Adaptation for Efficient Human Pose Estimation', 'Human pose estimation', 'Adversarial learning', 'Domain adaptation', 'Machine learning', '', 'Convolutional Neural Networks (CNN) are the leading models for human body landmark detection from RGB vision data. However, as such models require high computational load, an alternative is to rely on depth images which, due to their more simple nature, can allow the use of less complex CNNs and hence can lead to a faster detector. As learning CNNs from scratch requires large amounts of labeled data, which are not always available or expensive to obtain, we propose to rely on simulations and synthetic examples to build a large training dataset with precise labels. Nevertheless, the final performance on real data will suffer from the mismatch between the training and test data, also called domain shift between the source and target distributions. Thus in this paper, our main contribution is to investigate the use of unsupervised domain adaptation techniques to fill the gap in performance introduced by these distribution differences. The challenge lies in the important noise differences (not only gaussian noise, but many missing values around body limbs) between synthetic and real data, as well as the fact that we address a regression task rather than a classification one. In addition, we introduce a new public dataset of synthetically generated depth images to cover the cases of multi-person pose estimation. Our experiments show that domain adaptation provides some improvement, but that further network fine-tuning with real annotated data is worth including to supervise the adaptation process.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_28');
INSERT INTO `paper` VALUES (10830, 'Is Robustness the Cost of Accuracy? – A Comprehensive Study on the Robustness of 18 Deep Image Classification Models', 'Deep neural networks', 'Adversarial attacks', 'Robustness', '', '', 'The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights: (1) linear scaling law - the empirical \\(\\ell _2\\) and \\(\\ell _\\infty \\) distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in \\(\\ell _\\infty \\) distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at https://github.com/huanzhang12/Adversarial_Survey.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_39');
INSERT INTO `paper` VALUES (10831, 'ISNN: Impact Sound Neural Network for Audio-Visual Object Classification', 'Impact Sounds', 'Object Geometry', 'Striking Object', 'Vision-based Reconstruction', 'VoxNet', '3D object geometry reconstruction remains a challenge when working with transparent, occluded, or highly reflective surfaces. While recent methods classify shape features using raw audio, we present a multimodal neural network optimized for estimating an object’s geometry and material. Our networks use spectrograms of recorded and synthesized object impact sounds and voxelized shape estimates to extend the capabilities of vision-based reconstruction. We evaluate our method on multiple datasets of both recorded and synthesized sounds. We further present an interactive application for real-time scene reconstruction in which a user can strike objects, producing sound that can instantly classify and segment the struck object, even if the object is transparent or visually occluded.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_34');
INSERT INTO `paper` VALUES (10832, 'Iterative Crowd Counting', 'Crowd counting', 'Density estimation', 'Multi-stage CNN', '', '', 'In this work, we tackle the problem of crowd counting in images. We present a Convolutional Neural Network (CNN) based density estimation approach to solve this problem. Predicting a high resolution density map in one go is a challenging task. Hence, we present a two branch CNN architecture for generating high resolution density maps, where the first branch generates a low resolution density map, and the second branch incorporates the low resolution prediction and feature maps from the first branch to generate a high resolution density map. We also propose a multi-stage extension of our approach where each stage in the pipeline utilizes the predictions from all the previous stages. Empirical comparison with the previous state-of-the-art crowd counting methods shows that our method achieves the lowest mean absolute error on three challenging crowd counting benchmarks: Shanghaitech, WorldExpo’10, and UCF datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_17');
INSERT INTO `paper` VALUES (10833, 'It’s Not All About Size: On the Role of Data Properties in Pedestrian Detection', 'Pedestrian detection', 'Data properties', 'Pedestrian attributes', 'Benchmark dataset', 'Evaluation framework', 'Pedestrian detection is central in applications such as autonomous driving. The performance of algorithms tailored to solve this problem has been extensively evaluated on benchmark datasets, such as Caltech, which do not adequately represent the diversity of traffic scenes. Consequently, the true performance of algorithms and their limitations in practice remain understudied.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_12');
INSERT INTO `paper` VALUES (10834, 'Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network', '3D face reconstruction', 'Dense face alignment', '', '', '', 'We propose a straightforward method that simultaneously reconstructs the 3D facial structure and provides dense alignment. To achieve this, we design a 2D representation called UV position map which records the 3D shape of a complete face in UV space, then train a simple Convolutional Neural Network to regress it from a single 2D image. We also integrate a weight mask into the loss function during training to improve the performance of the network. Our method does not rely on any prior face model, and can reconstruct full facial geometry along with semantic meaning. Meanwhile, our network is very light-weighted and spends only 9.8 ms to process an image, which is extremely faster than previous works. Experiments on multiple challenging datasets show that our method surpasses other state-of-the-art methods on both reconstruction and alignment tasks by a large margin. Code is available at https://github.com/YadiraF/PRNet.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_33');
INSERT INTO `paper` VALUES (10835, 'Joint 3D Tracking of a Deformable Object in Interaction with a Hand', 'Deformable Objects', 'Contact Configuration', 'Hand Pose', 'Joint Optimization Framework', 'Template Mesh', 'We present a novel method that is able to track a complex deformable object in interaction with a hand. This is achieved by formulating and solving an optimization problem that jointly considers the hand, the deformable object and the hand/object contact points. The optimization evaluates several hand/object contact configuration hypotheses and adopts the one that results in the best fit of the object’s model to the available RGBD observations in the vicinity of the hand. Thus, the hand is not treated as a distractor that occludes parts of the deformable object, but as a source of valuable information. Experimental results on a dataset that has been developed specifically for this new problem illustrate the superior performance of the proposed approach against relevant, state of the art solutions.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_30');
INSERT INTO `paper` VALUES (10836, 'Joint and Progressive Learning from High-Dimensional Data for Multi-label Classification', 'Alternating direction method of multipliers', 'High-dimensional data', 'Manifold regularization', 'Multi-label classification', 'Joint learning', 'Despite the fact that nonlinear subspace learning techniques (e.g. manifold learning) have successfully applied to data representation, there is still room for improvement in explainability (explicit mapping), generalization (out-of-samples), and cost-effectiveness (linearization). To this end, a novel linearized subspace learning technique is developed in a joint and progressive way, called joint and progressive learning strategy (J-Play), with its application to multi-label classification. The J-Play learns high-level and semantically meaningful feature representation from high-dimensional data by (1) jointly performing multiple subspace learning and classification to find a latent subspace where samples are expected to be better classified; (2) progressively learning multi-coupled projections to linearly approach the optimal mapping bridging the original space with the most discriminative subspace; (3) locally embedding manifold structure in each learnable latent subspace. Extensive experiments are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_29');
INSERT INTO `paper` VALUES (10837, 'Joint Blind Motion Deblurring and Depth Estimation of Light Field', 'Light field', '6-DOF camera motion', 'Motion blur', 'Blind motion deblurring', 'Depth estimation', 'Removing camera motion blur from a single light field is a challenging task since it is highly ill-posed inverse problem. The problem becomes even worse when blur kernel varies spatially due to scene depth variation and high-order camera motion. In this paper, we propose a novel algorithm to estimate all blur model variables jointly, including latent sub-aperture image, camera motion, and scene depth from the blurred 4D light field. Exploiting multi-view nature of a light field relieves the inverse property of the optimization by utilizing strong depth cues and multi-view blur observation. The proposed joint estimation achieves high quality light field deblurring and depth estimation simultaneously under arbitrary 6-DOF camera motion and unconstrained scene depth. Intensive experiment on real and synthetic blurred light field confirms that the proposed algorithm outperforms the state-of-the-art light field deblurring and depth estimation methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_18');
INSERT INTO `paper` VALUES (10838, 'Joint Camera Spectral Sensitivity Selection and Hyperspectral Image Recovery', 'Camera spectral sensitivity selection', 'Hyperspectral image recovery', 'Spectral nonlinear mapping', 'Spatial similarity', '', 'Hyperspectral image (HSI) recovery from a single RGB image has attracted much attention, whose performance has recently been shown to be sensitive to the camera spectral sensitivity (CSS). In this paper, we present an efficient convolutional neural network (CNN) based method, which can jointly select the optimal CSS from a candidate dataset and learn a mapping to recover HSI from a single RGB image captured with this algorithmically selected camera. Given a specific CSS, we first present a HSI recovery network, which accounts for the underlying characteristics of the HSI, including spectral nonlinear mapping and spatial similarity. Later, we append a CSS selection layer onto the recovery network, and the optimal CSS can thus be automatically determined from the network weights under the nonnegative sparse constraint. Experimental results show that our HSI recovery network outperforms state-of-the-art methods in terms of both quantitative metrics and perceptive quality, and the selection layer always returns a CSS consistent to the best one determined by exhaustive search.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_48');
INSERT INTO `paper` VALUES (10839, 'Joint Exploitation of Features and Optical Flow for Real-Time Moving Object Detection on Drones', 'Moving object detection', 'Optical flow', 'UAV', 'Drones', 'Embedded vision', 'Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_8');
INSERT INTO `paper` VALUES (10840, 'Joint Future Semantic and Instance Segmentation Prediction', '', '', '', '', '', 'The ability to predict what will happen next from observing the past is a key component of intelligence. Methods that forecast future frames were recently introduced towards better machine intelligence. However, predicting directly in the image color space seems an overly complex task, and predicting higher level representations using semantic or instance segmentation approaches were shown to be more accurate. In this work, we introduce a novel prediction approach that encodes instance and semantic segmentation information in a single representation based on distance maps. Our graph-based modeling of the instance segmentation prediction problem allows us to obtain temporal tracks of the objects as an optimal solution to a watershed algorithm. Our experimental results on the Cityscapes dataset present state-of-the-art semantic segmentation predictions, and instance segmentation results outperforming a strong baseline based on optical flow.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_14');
INSERT INTO `paper` VALUES (10841, 'Joint Learning of Intrinsic Images and Semantic Segmentation', '', '', '', '', '', 'Semantic segmentation of outdoor scenes is problematic when there are variations in imaging conditions. It is known that albedo (reflectance) is invariant to all kinds of illumination effects. Thus, using reflectance images for semantic segmentation task can be favorable. Additionally, not only segmentation may benefit from reflectance, but also segmentation may be useful for reflectance computation. Therefore, in this paper, the tasks of semantic segmentation and intrinsic image decomposition are considered as a combined process by exploring their mutual relationship in a joint fashion. To that end, we propose a supervised end-to-end CNN architecture to jointly learn intrinsic image decomposition and semantic segmentation. We analyze the gains of addressing those two problems jointly. Moreover, new cascade CNN architectures for intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as single tasks. Furthermore, a dataset of 35K synthetic images of natural environments is created with corresponding albedo and shading (intrinsics), as well as semantic labels (segmentation) assigned to each object/scene. The experiments show that joint learning of intrinsic image decomposition and semantic segmentation is beneficial for both tasks for natural scenes. Dataset and models are available at: (https://ivi.fnwi.uva.nl/cv/intrinseg).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_18');
INSERT INTO `paper` VALUES (10842, 'Joint Map and Symmetry Synchronization', 'Correspondences', 'Symmetry group', 'Cycle-consistency', 'Optimization', '', 'Most existing techniques in map computation (e.g., in the form of feature or dense correspondences) assume that the underlying map between an object pair is unique. This assumption, however, easily breaks when visual objects possess self-symmetries. In this paper, we study the problem of jointly optimizing symmetry groups and pair-wise maps among a collection of symmetric objects. We introduce a lifting map representation for encoding both symmetry groups and maps between symmetry groups. Based on this representation, we introduce a computational framework for joint symmetry and map synchronization. Experimental results show that this approach outperforms state-of-the-art approaches for symmetry detection from a single object as well as joint map optimization among an object collection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_16');
INSERT INTO `paper` VALUES (10843, 'Joint Optimization for Compressive Video Sensing and Reconstruction Under Hardware Constraints', 'Compressive sensing', 'Video reconstruction', 'Deep neural network', '', '', 'Compressive video sensing is the process of encoding multiple sub-frames into a single frame with controlled sensor exposures and reconstructing the sub-frames from the single compressed frame. It is known that spatially and temporally random exposures provide the most balanced compression in terms of signal recovery. However, sensors that achieve a fully random exposure on each pixel cannot be easily realized in practice because the circuit of the sensor becomes complicated and incompatible with the sensitivity and resolution. Therefore, it is necessary to design an exposure pattern by considering the constraints enforced by hardware. In this paper, we propose a method of jointly optimizing the exposure patterns of compressive sensing and the reconstruction framework under hardware constraints. By conducting a simulation and actual experiments, we demonstrated that the proposed framework can reconstruct multiple sub-frame images with higher quality.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_39');
INSERT INTO `paper` VALUES (10844, 'Joint Person Segmentation and Identification in Synchronized First- and Third-Person Videos', 'Synchronized first- and third-person cameras', '', '', '', '', 'In a world of pervasive cameras, public spaces are often captured from multiple perspectives by cameras of different types, both fixed and mobile. An important problem is to organize these heterogeneous collections of videos by finding connections between them, such as identifying correspondences between the people appearing in the videos and the people holding or wearing the cameras. In this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person videos of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera A corresponds with whom in camera B), and (2) given one or more synchronized third-person videos as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person videos. Unlike previous work which requires ground truth bounding boxes to estimate the correspondences, we perform person segmentation and identification jointly. We find that solving these two problems simultaneously is mutually beneficial, because better fine-grained segmentation allows us to better perform matching across views, and information from multiple views helps us perform more accurate segmentation. We evaluate our approach on two challenging datasets of interacting people captured from multiple wearable cameras, and show that our proposed method performs significantly better than the state-of-the-art on both person segmentation and identification.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_39');
INSERT INTO `paper` VALUES (10845, 'Joint Representation and Truncated Inference Learning for Correlation Filter Based Tracking', 'Visual tracking', 'Correlation filters', 'Convolutional neural networks', 'Unrolled optimization', '', 'Correlation filter (CF) based trackers generally include two modules, i.e., feature representation and on-line model adaptation. In existing off-line deep learning models for CF trackers, the model adaptation usually is either abandoned or has closed-form solution to make it feasible to learn deep representation in an end-to-end manner. However, such solutions fail to exploit the advances in CF models, and cannot achieve competitive accuracy in comparison with the state-of-the-art CF trackers. In this paper, we investigate the joint learning of deep representation and model adaptation, where an updater network is introduced for better tracking on future frame by taking current frame representation, tracking result, and last CF tracker as input. By modeling the representor as convolutional neural network (CNN), we truncate the alternating direction method of multipliers (ADMM) and interpret it as a deep network of updater, resulting in our model for learning representation and truncated inference (RTINet). Experiments demonstrate that our RTINet tracker achieves favorable tracking accuracy against the state-of-the-art trackers and its rapid version can run at a real-time speed of 24 fps. The code and pre-trained models will be publicly available at https://github.com/tourmaline612/RTINet.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_34');
INSERT INTO `paper` VALUES (10846, 'Joint Task-Recursive Learning for Semantic Segmentation and Depth Estimation', 'Depth estimation', 'Semantic segmentation', 'Recursive learning', 'Recurrent neural network', 'Deep learning', 'In this paper, we propose a novel joint Task-Recursive Learning (TRL) framework for the closing-loop semantic segmentation and monocular depth estimation tasks. TRL can recursively refine the results of both tasks through serialized task-level interactions. In order to mutually-boost for each other, we encapsulate the interaction into a specific Task-Attentional Module (TAM) to adaptively enhance some counterpart patterns of both tasks. Further, to make the inference more credible, we propagate previous learning experiences on both tasks into the next network evolution by explicitly concatenating previous responses. The sequence of task-level interactions are finally evolved along a coarse-to-fine scale space such that the required details may be reconstructed progressively. Extensive experiments on NYU-Depth v2 and SUN RGB-D datasets demonstrate that our method achieves state-of-the-art results for monocular depth estimation and semantic segmentation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_15');
INSERT INTO `paper` VALUES (10847, 'Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input', 'Vision and language', 'Sound', 'Speech', 'Convolutional networks', 'Multimodal learning', 'In this paper, we explore neural network models that learn to associate segments of spoken audio captions with the semantically relevant portions of natural images that they refer to. We demonstrate that these audio-visual associative localizations emerge from network-internal representations learned as a by-product of training to perform an image-audio retrieval task. Our models operate directly on the image pixels and speech waveform, and do not rely on any conventional supervision in the form of labels, segmentations, or alignments between the modalities during training. We perform analysis using the Places 205 and ADE20k datasets demonstrating that our models implicitly learn semantically-coupled object and word detectors.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_40');
INSERT INTO `paper` VALUES (10848, 'K-convexity Shape Priors for Segmentation', '', '', '', '', '', 'This work extends popular star-convexity and other more general forms of convexity priors. We represent an object as a union of “convex” overlappable subsets. Since an arbitrary shape can always be divided into convex parts, our regularization model restricts the number of such parts. Previous k-part shape priors are limited to disjoint parts. For example, one approach segments an object via optimizing its k coverage by disjoint convex parts, which we show is highly sensitive to local minima. In contrast, our shape model allows the convex parts to overlap, which both relaxes and simplifies the coverage problem, e.g. fewer parts are needed to represent any object. As shown in the paper, for many forms of convexity our regularization model is significantly more descriptive for any given k. Our shape prior is useful in practice, e.g. biomedical applications, and its optimization is robust to local minima.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_3');
INSERT INTO `paper` VALUES (10849, 'Key-Word-Aware Network for Referring Expression Image Segmentation', 'Referring expression image segmentation', 'Key word extraction', 'Query attention', 'Key-word-aware visual context', '', 'Referring expression image segmentation aims to segment out the object referred by a natural language query expression. Without considering the specific properties of visual and textual information, existing works usually deal with this task by directly feeding a foreground/background classifier with cascaded image and text features, which are extracted from each image region and the whole query, respectively. On the one hand, they ignore that each word in a query expression makes different contributions to identify the desired object, which requires a differential treatment in extracting text feature. On the other hand, the relationships of different image regions are not considered as well, even though they are greatly important to eliminate the undesired foreground object in accordance with specific query. To address aforementioned issues, in this paper, we propose a key-word-aware network, which contains a query attention model and a key-word-aware visual context model. In extracting text features, the query attention model attends to assign higher weights for the words which are more important for identifying object. Meanwhile, the key-word-aware visual context model describes the relationships among different image regions, according to corresponding query. Our proposed method outperforms state-of-the-art methods on two referring expression image segmentation databases.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_3');
INSERT INTO `paper` VALUES (10850, 'Know What Your Neighbors Do: 3D Semantic Segmentation of Point Clouds', '', '', '', '', '', 'In this paper, we present a deep learning architecture which addresses the problem of 3D semantic segmentation of unstructured point clouds (Fig. 1). Compared to previous work, we introduce grouping techniques which define point neighborhoods in the initial world space and the learned feature space. Neighborhoods are important as they allow to compute local or global point features depending on the spatial extend of the neighborhood. Additionally, we incorporate dedicated loss functions to further structure the learned point feature space: the pairwise distance loss and the centroid loss. We show how to apply these mechanisms to the task of 3D semantic segmentation of point clouds and report state-of-the-art performance on indoor and outdoor datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_29');
INSERT INTO `paper` VALUES (10851, 'Knowing When to Look for What and Where: Evaluating Generation of Spatial Descriptions with Adaptive Attention', 'Image descriptions', 'Grounded neural language model', 'Attention model', 'Spatial descriptions', '', 'We examine and evaluate adaptive attention [17] (which balances the focus on visual features and focus on textual features) in generating image captions in end-to-end neural networks, in particular how adaptive attention is informative for generating spatial relations. We show that the model generates spatial relations more on the basis of textual rather than visual features and therefore confirm the previous observations that the learned visual features are missing information about geometric relations between objects.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_14');
INSERT INTO `paper` VALUES (10852, 'Knowing Where to Look? Analysis on Attention of Visual Question Answering System', 'Attention', 'Visual question answering', '', '', '', 'Attention mechanisms have been widely used in Visual Question Answering (VQA) solutions due to their capacity to model deep cross-domain interactions. Analyzing attention maps offers us a perspective to find out limitations of current VQA systems and an opportunity to further improve them. In this paper, we select two state-of-the-art VQA approaches with attention mechanisms to study their robustness and disadvantages by visualizing and analyzing their estimated attention maps. We find that both methods are sensitive to features, and simultaneously, they perform badly for counting and multi-object related questions. We believe that the findings and analytical method will help researchers identify crucial challenges on the way to improve their own VQA systems.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_13');
INSERT INTO `paper` VALUES (10853, 'Label Denoising with Large Ensembles of Heterogeneous Neural Networks', 'Video processing', 'Learning from noisy labels', 'Attention-based models', 'Recurrent neural networks', 'Deep learning', 'Despite recent advances in computer vision based on various convolutional architectures, video understanding remains an important challenge. In this work, we present and discuss a top solution for the large-scale video classification (labeling) problem introduced as a Kaggle competition based on the YouTube-8M dataset. We show and compare different approaches to preprocessing, data augmentation, model architectures, and model combination. Our final model is based on a large ensemble of video- and frame-level models but fits into rather limiting hardware constraints. We apply an approach based on knowledge distillation to deal with noisy labels in the original dataset and the recently developed mixup technique to improve the basic models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_23');
INSERT INTO `paper` VALUES (10854, 'Lambda Twist: An Accurate Fast Robust Perspective Three Point (P3P) Solver', 'P3P', 'PnP', 'Visual odometry', 'Camera geometry', '', 'We present Lambda Twist; a novel P3P solver which is accurate, fast and robust. Current state-of-the-art P3P solvers find all roots to a quartic and discard geometrically invalid and duplicate solutions in a post-processing step. Instead of solving a quartic, the proposed P3P solver exploits the underlying elliptic equations which can be solved by a fast and numerically accurate diagonalization. This diagonalization requires a single real root of a cubic which is then used to find the, up to four, P3P solutions. Unlike the direct quartic solvers our method never computes geometrically invalid or duplicate solutions.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_20');
INSERT INTO `paper` VALUES (10855, 'LAPRAN: A Scalable Laplacian Pyramid Reconstructive Adversarial Network for Flexible Compressive Sensing Reconstruction', 'Compressive sensing', 'Reconstruction', 'Laplacian pyramid', 'Reconstructive adversarial network', 'Feature fusion', 'This paper addresses the single-image compressive sensing (CS) and reconstruction problem. We propose a scalable Laplacian pyramid reconstructive adversarial network (LAPRAN) that enables high-fidelity, flexible and fast CS images reconstruction. LAPRAN progressively reconstructs an image following the concept of the Laplacian pyramid through multiple stages of reconstructive adversarial networks (RANs). At each pyramid level, CS measurements are fused with a contextual latent vector to generate a high-frequency image residual. Consequently, LAPRAN can produce hierarchies of reconstructed images and each with an incremental resolution and improved quality. The scalable pyramid structure of LAPRAN enables high-fidelity CS reconstruction with a flexible resolution that is adaptive to a wide range of compression ratios (CRs), which is infeasible with existing methods. Experimental results on multiple public datasets show that LAPRAN offers an average 7.47 dB and 5.98 dB PSNR, and an average 57.93\\(\\%\\) and 33.20\\(\\%\\) SSIM improvement compared to model-based and data-driven baselines, respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_30');
INSERT INTO `paper` VALUES (10856, 'Large Scale Urban Scene Modeling from MVS Meshes', 'Urban reconstruction', 'Building modeling', 'Markov random field', 'Segment based modeling', '', 'In this paper we present an efficient modeling framework for large scale urban scenes. Taking surface meshes derived from multi-view-stereo systems as input, our algorithm outputs simplified models with semantics at different levels of detail (LODs). Our key observation is that urban building is usually composed of planar roof tops connected with vertical walls. There are two major steps in our framework: segmentation and building modeling. The scene is first segmented into four classes with a Markov random field combining height and image features. In the following modeling step, various 2D line segments sketching the roof boundaries are detected and slice the plane into faces. Through assigning each face with a roof plane, the final model is constructed by extruding the faces to the corresponding planes. By combining geometric and appearance cues together, the proposed method is robust and fast compared to the state-of-the-art algorithms.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_38');
INSERT INTO `paper` VALUES (10857, 'Large-Scale Video Classification with Feature Space Augmentation Coupled with Learned Label Relations and Ensembling', 'Video classification', 'YouTube-8M dataset', '', '', '', 'This paper presents the Axon AI’s solution to the 2nd YouTube-8M Video Understanding Challenge, achieving the final global average precision (GAP) of 88.733% on the private test set (ranked 3rd among 394 teams, not considering the model size constraint), and 87.287% using a model that meets size requirement. Two sets of 7 individual models belonging to 3 different families were trained separately. Then, the inference results on a training data were aggregated from these multiple models and fed to train a compact model that meets the model size requirement. In order to further improve performance we explored and employed data over/sub-sampling in feature space, an additional regularization term during training exploiting label relationship, and learned weights for ensembling different individual models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_31');
INSERT INTO `paper` VALUES (10858, 'Layer-Structured 3D Scene Inference via View Synthesis', 'View Synthesis', 'Proxy Task', 'Single Input Image', 'Visible Pixels', 'Layered Depth Images (LDI)', 'We present an approach to infer a layer-structured 3D representation of a scene from a single input image. This allows us to infer not only the depth of the visible pixels, but also to capture the texture and depth for content in the scene that is not directly visible. We overcome the challenge posed by the lack of direct supervision by instead leveraging a more naturally available multi-view supervisory signal. Our insight is to use view synthesis as a proxy task: we enforce that our representation (inferred from a single image), when rendered from a novel perspective, matches the true observed image. We present a learning framework that operationalizes this insight using a new, differentiable novel view renderer. We provide qualitative and quantitative validation of our approach in two different settings, and demonstrate that we can learn to capture the hidden aspects of a scene. The project website can be found at https://shubhtuls.github.io/lsi/.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_19');
INSERT INTO `paper` VALUES (10859, 'LBP-Motivated Colour Texture Classification', 'Colour', 'Texture', 'Local Binary Patterns', '', '', 'In this paper we investigate extensions of Local Binary Patterns (LBP), Improved Local Binary Patterns (ILBP) and Extended Local Binary Patterns (ELBP) to colour textures via two different strategies: intra-/inter-channel features and colour orderings. We experimentally evaluate the proposed methods over 15 datasets of general and biomedical colour textures. Intra- and inter-channel features from the RGB space emerged as the best descriptors and we found that the best accuracy was achieved by combining multi-resolution intra-channel features with single-resolution inter-channel features.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_42');
INSERT INTO `paper` VALUES (10860, 'Learn-to-Score: Efficient 3D Scene Exploration by Predicting View Utility', '3D reconstruction', 'Exploration', 'Active vision', '3D CNN', '', 'Camera equipped drones are nowadays being used to explore large scenes and reconstruct detailed 3D maps. When free space in the scene is approximately known, an offline planner can generate optimal plans to efficiently explore the scene. However, for exploring unknown scenes, the planner must predict and maximize usefulness of where to go on the fly. Traditionally, this has been achieved using handcrafted utility functions. We propose to learn a better utility function that predicts the usefulness of future viewpoints. Our learned utility function is based on a 3D convolutional neural network. This network takes as input a novel volumetric scene representation that implicitly captures previously visited viewpoints and generalizes to new scenes. We evaluate our method on several large 3D models of urban scenes using simulated depth cameras. We show that our method outperforms existing utility measures in terms of reconstruction performance and is robust to sensor noise.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_27');
INSERT INTO `paper` VALUES (10861, 'Learnable PINs: Cross-modal Embeddings for Person Identity', 'Joint embedding', 'Cross-modal', 'Multi-modal', 'Self-supervised', 'Face recognition', 'We propose and investigate an identity sensitive joint embedding of face and voice. Such an embedding enables cross-modal retrieval from voice to face and from face to voice.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_5');
INSERT INTO `paper` VALUES (10862, 'Learnable Pooling Methods for Video Classification', 'Video classification', 'Youtube-8M', 'NetVLAD', 'Attention', 'Pooling', 'We introduce modifications to state-of-the-art approaches to aggregating local video descriptors by using attention mechanisms and function approximations. Rather than using ensembles of existing architectures, we provide an insight on creating new architectures. We demonstrate our solutions in the “The 2nd YouTube-8M Video Understanding Challenge”, by using frame-level video and audio descriptors. We obtain testing accuracy similar to the state of the art, while meeting budget constraints, and touch upon strategies to improve the state of the art. Model implementations are available in https://github.com/pomonam/LearnablePoolingMethods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_21');
INSERT INTO `paper` VALUES (10863, 'Learning 3D Human Pose from Structure and Motion', '', '', '', '', '', '3D human pose estimation from a single image is a challenging problem, especially for in-the-wild settings due to the lack of 3D annotated data. We propose two anatomically inspired loss functions and use them with a weakly-supervised learning framework to jointly learn from large-scale in-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal network that exploits temporal and structural cues present in predicted pose sequences to temporally harmonize the pose estimations. We carefully analyze the proposed contributions through loss surface visualizations and sensitivity analysis to facilitate deeper understanding of their working mechanism. Jointly, the two networks capture the anatomical constraints in static and kinetic states of the human body. Our complete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M and MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics card.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_41');
INSERT INTO `paper` VALUES (10864, 'Learning 3D Keypoint Descriptors for Non-rigid Shape Matching', 'Local feature descriptor', 'Triplet CNNs', 'Non-rigid shapes', '', '', 'In this paper, we present a novel deep learning framework that derives discriminative local descriptors for 3D surface shapes. In contrast to previous convolutional neural networks (CNNs) that rely on rendering multi-view images or extracting intrinsic shape properties, we parameterize the multi-scale localized neighborhoods of a keypoint into regular 2D grids, which are termed as ‘geometry images’. The benefits of such geometry images include retaining sufficient geometric information, as well as allowing the usage of standard CNNs. Specifically, we leverage a triplet network to perform deep metric learning, which takes a set of triplets as input, and a newly designed triplet loss function is minimized to distinguish between similar and dissimilar pairs of keypoints. At the testing stage, given a geometry image of a point of interest, our network outputs a discriminative local descriptor for it. Experimental results for non-rigid shape matching on several benchmarks demonstrate the superior performance of our learned descriptors over traditional descriptors and the state-of-the-art learning-based alternatives.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_1');
INSERT INTO `paper` VALUES (10865, 'Learning 3D Shapes as Multi-layered Height-Maps Using 2D Convolutional Networks', 'CNN on 3D shapes', '3D shape representation', 'ModelNet', 'Shape classification', 'Shape generation', 'We present a novel global representation of 3D shapes, suitable for the application of 2D CNNs. We represent 3D shapes as multi-layered height-maps (MLH) where at each grid location, we store multiple instances of height maps, thereby representing 3D shape detail that is hidden behind several layers of occlusion. We provide a novel view merging method for combining view dependent information (Eg. MLH descriptors) from multiple views. Because of the ability of using 2D CNNs, our method is highly memory efficient in terms of input resolution compared to the voxel based input. Together with MLH descriptors and our multi view merging, we achieve the state-of-the-art result in classification on ModelNet dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_5');
INSERT INTO `paper` VALUES (10866, 'Learning a Robust Society of Tracking Parts Using Co-occurrence Constraints', 'Unsupervised tracking', 'Co-occurrences', 'Part-based tracker', '', '', 'Object tracking is an essential problem in computer vision that has been researched for several decades. One of the main challenges in tracking is to adapt to object appearance changes over time and avoiding drifting to background clutter. We address this challenge by proposing a deep neural network composed of different parts, which functions as a society of tracking parts. They work in conjunction according to a certain policy and learn from each other in a robust manner, using co-occurrence constraints that ensure robust inference and learning. From a structural point of view, our network is composed of two main pathways. One pathway is more conservative. It carefully monitors a large set of simple tracker parts learned as linear filters over deep feature activation maps. It assigns the parts different roles. It promotes the reliable ones and removes the inconsistent ones. We learn these filters simultaneously in an efficient way, with a single closed-form formulation, for which we propose novel theoretical properties. The second pathway is more progressive. It is learned completely online and thus it is able to better model object appearance changes. In order to adapt in a robust manner, it is learned only on highly confident frames, which are decided using co-occurrences with the first pathway. Thus, our system has the full benefit of two main approaches in tracking. The larger set of simpler filter parts offers robustness, while the full deep network learned online provides adaptability to change. As shown in the experimental section, our approach achieves state of the art performance on the challenging VOT17 benchmark, outperforming the published methods both on the general EAO metric and in the number of fails, by a significant margin.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_9');
INSERT INTO `paper` VALUES (10867, 'Learning and Matching Multi-View Descriptors for Registration of Point Clouds', 'Point cloud registration', '3D descriptor', 'Robust matching', '', '', 'Critical to the registration of point clouds is the establishment of a set of accurate correspondences between points in 3D space. The correspondence problem is generally addressed by the design of discriminative 3D local descriptors on the one hand, and the development of robust matching strategies on the other hand. In this work, we first propose a multi-view local descriptor, which is learned from the images of multiple views, for the description of 3D keypoints. Then, we develop a robust matching approach, aiming at rejecting outlier matches based on the efficient inference via belief propagation on the defined graphical model. We have demonstrated the boost of our approaches to registration on the public scanning and multi-view stereo datasets. The superior performance has been verified by the intensive comparisons against a variety of descriptors and matching methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_31');
INSERT INTO `paper` VALUES (10868, 'Learning Blind Video Temporal Consistency', '', '', '', '', '', 'Applying image processing algorithms independently to each frame of a video often leads to undesired inconsistent results over time. Developing temporally consistent video-based extensions, however, requires domain knowledge for individual tasks and is unable to generalize to other applications. In this paper, we present an efficient approach based on a deep recurrent network for enforcing temporal consistency in a video. Our method takes the original and per-frame processed videos as inputs to produce a temporally consistent video. Consequently, our approach is agnostic to specific image processing algorithms applied to the original video. We train the proposed network by minimizing both short-term and long-term temporal losses as well as a perceptual loss to strike a balance between temporal coherence and perceptual similarity with the processed frames. At test time, our model does not require computing optical flow and thus achieves real-time speed even for high-resolution videos. We show that our single model can handle multiple and unseen tasks, including but not limited to artistic style transfer, enhancement, colorization, image-to-image translation and intrinsic image decomposition. Extensive objective evaluation and subject study demonstrate that the proposed approach performs favorably against the state-of-the-art methods on various types of videos.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_11');
INSERT INTO `paper` VALUES (10869, 'Learning Category-Specific Mesh Reconstruction from Image Collections', 'Annotated Image Collections', 'Canonical Appearance', 'Mean Shape', 'Appearance Space', 'PASCAL 3D', 'We present a learning framework for recovering the 3D shape, camera, and texture of an object from a single image. The shape is represented as a deformable 3D mesh model of an object category where a shape is parameterized by a learned mean shape and per-instance predicted deformation. Our approach allows leveraging an annotated image collection for training, where the deformable model and the 3D prediction mechanism are learned without relying on ground-truth 3D or multi-view supervision. Our representation enables us to go beyond existing 3D prediction approaches by incorporating texture inference as prediction of an image in a canonical appearance space. Additionally, we show that semantic keypoints can be easily associated with the predicted shapes. We present qualitative and quantitative results of our approach on CUB and PASCAL3D datasets and show that we can learn to predict diverse shapes and textures across objects using only annotated image collections. The project website can be found at https://akanazawa.github.io/cmr/.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_23');
INSERT INTO `paper` VALUES (10870, 'Learning CCA Representations for Misaligned Data', 'Canonical correlation analysis', 'Learning compact representations', 'Misalignment resilience', 'Change detection', '', 'Canonical correlation analysis (CCA) is a statistical learning method that seeks to build view-independent latent representations from multi-view data. This method has been successfully applied to several pattern analysis tasks such as image-to-text mapping and view-invariant object/action recognition. However, this success is highly dependent on the quality of data pairing (i.e., alignments) and mispairing adversely affects the generalization ability of the learned CCA representations.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_39');
INSERT INTO `paper` VALUES (10871, 'Learning Class Prototypes via Structure Alignment for Zero-Shot Recognition', 'Zero-shot learning', 'Visual-semantic structures', 'Coupled dictionary learning', 'Class prototypes', '', 'Zero-shot learning (ZSL) aims to recognize objects of novel classes without any training samples of specific classes, which is achieved by exploiting the semantic information and auxiliary datasets. Recently most ZSL approaches focus on learning visual-semantic embeddings to transfer knowledge from the auxiliary datasets to the novel classes. However, few works study whether the semantic information is discriminative or not for the recognition task. To tackle such problem, we propose a coupled dictionary learning approach to align the visual-semantic structures using the class prototypes, where the discriminative information lying in the visual space is utilized to improve the less discriminative semantic space. Then, zero-shot recognition can be performed in different spaces by the simple nearest neighbor approach using the learned class prototypes. Extensive experiments on four benchmark datasets show the effectiveness of the proposed approach.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_8');
INSERT INTO `paper` VALUES (10872, 'Learning Compression from Limited Unlabeled Data', 'Deep neural networks', 'Label-free network compression', '', '', '', 'Convolutional neural networks (CNNs) have dramatically advanced the state-of-art in a number of domains. However, most models are both computation and memory intensive, which arouse the interest of network compression. While existing compression methods achieve good performance, they suffer from three limitations: (1) the inevitable retraining with enormous labeled data; (2) the massive GPU hours for retraining; (3) the training tricks for model compression. Especially the requirement of retraining on original datasets makes it difficult to apply in many real-world scenarios, where training data is not publicly available. In this paper, we reveal that re-normalization is the practical and effective way to alleviate the above limitations. Through quantization or pruning, most methods may compress a large number of parameters but ignore the core role in performance degradation, which is the Gaussian conjugate prior induced by batch normalization. By employing the re-estimated statistics in batch normalization, we significantly improve the accuracy of compressed CNNs. Extensive experiments on ImageNet show it outperforms baselines by a large margin and is comparable to label-based methods. Besides, the fine-tuning process takes less than 5 min on CPU, using 1000 unlabeled images.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_46');
INSERT INTO `paper` VALUES (10873, 'Learning Data Terms for Non-blind Deblurring', 'Image deblurring', 'Learning data terms', 'Shrinkage function', 'Noise and outliers', '', 'Existing deblurring methods mainly focus on developing effective image priors and assume that blurred images contain insignificant amounts of noise. However, state-of-the-art deblurring methods do not perform well on real-world images degraded with significant noise or outliers. To address these issues, we show that it is critical to learn data fitting terms beyond the commonly used \\(\\ell _1\\) or \\(\\ell _2\\) norm. We propose a simple and effective discriminative framework to learn data terms that can adaptively handle blurred images in the presence of severe noise and outliers. Instead of learning the distribution of the data fitting errors, we directly learn the associated shrinkage function for the data term using a cascaded architecture, which is more flexible and efficient. Our analysis shows that the shrinkage functions learned at the intermediate stages can effectively suppress noise and preserve image structures. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_46');
INSERT INTO `paper` VALUES (10874, 'Learning Deep Representations with Probabilistic Knowledge Transfer', 'Knowledge transfer', 'Neural network distillation', '', '', '', 'Knowledge Transfer (KT) techniques tackle the problem of transferring the knowledge from a large and complex neural network into a smaller and faster one. However, existing KT methods are tailored towards classification tasks and they cannot be used efficiently for other representation learning tasks. In this paper we propose a novel probabilistic knowledge transfer method that works by matching the probability distribution of the data in the feature space instead of their actual representation. Apart from outperforming existing KT techniques, the proposed method allows for overcoming several of their limitations providing new insight into KT as well as novel KT applications, ranging from KT from handcrafted feature extractors to cross-modal KT from the textual modality into the representation extracted from the visual modality of the data.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_17');
INSERT INTO `paper` VALUES (10875, 'Learning Discriminative Video Representations Using Adversarial Perturbations', 'Adversarial Perturbations', 'Riemannian Optimization', 'Adversarial Noise', 'Discriminative Subspace', 'Noise-like Pattern', 'Adversarial perturbations are noise-like patterns that can subtly change the data, while failing an otherwise accurate classifier. In this paper, we propose to use such perturbations for improving the robustness of video representations. To this end, given a well-trained deep model for per-frame video recognition, we first generate adversarial noise adapted to this model. Using the original data features from the full video sequence and their perturbed counterparts, as two separate bags, we develop a binary classification problem that learns a set of discriminative hyperplanes – as a subspace – that will separate the two bags from each other. This subspace is then used as a descriptor for the video, dubbed discriminative subspace pooling. As the perturbed features belong to data classes that are likely to be confused with the original features, the discriminative subspace will characterize parts of the feature space that are more representative of the original data, and thus may provide robust video representations. To learn such descriptors, we formulate a subspace learning objective on the Stiefel manifold and resort to Riemannian optimization methods for solving it efficiently. We provide experiments on several video datasets and demonstrate state-of-the-art results.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_42');
INSERT INTO `paper` VALUES (10876, 'Learning Driving Behaviors for Automated Cars in Unstructured Environments', 'Reinforcement Learning', 'DDPG', 'Overtaking', 'Blocking', 'Driving in traffic', 'The core of Reinforcement learning lies in learning from experiences. The performance of the agent is hugely impacted by the training conditions, reward functions and exploration policies. Deep Deterministic Policy Gradient (DDPG) is a well known approach to solve continuous control problems in RL. We use DDPG with intelligent choice of reward function and exploration policy to learn various driving behaviors (Lanekeeping, Overtaking, Blocking, Defensive, Opportunistic) for a simulated car in unstructured environments. In cluttered scenes, where the opponent agents are not following any driving pattern, it is difficult to anticipate their behavior and henceforth decide our agent’s actions. DDPG enables us to propose a solution which requires only the sensor information at current time step to predict the action to be taken. Our main contribution is generating a behavior based motion model for simulated cars, which plans for every instant.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_36');
INSERT INTO `paper` VALUES (10877, 'Learning Dynamic Memory Networks for Object Tracking', 'Addressable memory', 'Gated residual template learning', '', '', '', 'Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target object’s appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target’s appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the object’s information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target’s appearance changes by updating the external memory. Moreover, unlike other tracking methods where the model capacity is fixed after offline training – the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_10');
INSERT INTO `paper` VALUES (10878, 'Learning Efficient Single-Stage Pedestrian Detectors by Asymptotic Localization Fitting', 'Pedestrian detection', 'Convolutional neural networks', 'Asymptotic localization fitting', '', '', 'Though Faster R-CNN based two-stage detectors have witnessed significant boost in pedestrian detection accuracy, it is still slow for practical applications. One solution is to simplify this working flow as a single-stage detector. However, current single-stage detectors (e.g. SSD) have not presented competitive accuracy on common pedestrian detection benchmarks. This paper is towards a successful pedestrian detector enjoying the speed of SSD while maintaining the accuracy of Faster R-CNN. Specifically, a structurally simple but effective module called Asymptotic Localization Fitting (ALF) is proposed, which stacks a series of predictors to directly evolve the default anchor boxes of SSD step by step into improving detection results. As a result, during training the latter predictors enjoy more and better-quality positive samples, meanwhile harder negatives could be mined with increasing IoU thresholds. On top of this, an efficient single-stage pedestrian detection architecture (denoted as ALFNet) is designed, achieving state-of-the-art performance on CityPersons and Caltech, two of the largest pedestrian detection benchmarks, and hence resulting in an attractive pedestrian detector in both accuracy and speed. Code is available at https://github.com/VideoObjectSearch/ALFNet.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_38');
INSERT INTO `paper` VALUES (10879, 'Learning Event Representations by Encoding the Temporal Context', 'Representation learning', 'Event learning', 'LSTM', 'Neural networks', '', 'This work aims at learning image representations suitable for event segmentation, a largely unexplored problem in the computer vision literature. The proposed approach is a self-supervised neural network that captures patterns of temporal overlap by learning to predict the feature vector of neighbor frames, given the one of the current frame. The model is inspired to recent experimental findings in neuroscience, showing that stimuli associated with similar temporal contexts are grouped together in the representational space. Experiments performed on image sequences captured at regular intervals have shown that a representation able to encode the temporal context provides very promising results on the task of temporal segmentation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_44');
INSERT INTO `paper` VALUES (10880, 'Learning from #Barcelona Instagram Data What Locals and Tourists Post About Its Neighbourhoods', 'Self-supervised learning', 'Webly supervised learning', 'Social media analysis', 'City tourism analysis', '', 'Massive tourism is becoming a big problem for some cities, such as Barcelona, due to its concentration in some neighborhoods. In this work we gather Instagram data related to Barcelona consisting on images-captions pairs and, using the text as a supervisory signal, we learn relations between images, words and neighborhoods. Our goal is to learn which visual elements appear in photos when people is posting about each neighborhood. We perform a language separate treatment of the data and show that it can be extrapolated to a tourists and locals separate analysis, and that tourism is reflected in Social Media at a neighborhood level. The presented pipeline allows analyzing the differences between the images that tourists and locals associate to the different neighborhoods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_41');
INSERT INTO `paper` VALUES (10881, 'Learning Human-Object Interactions by Graph Parsing Neural Networks', 'Human-object interaction', 'Message passing', 'Graph parsing', 'Neural networks', '', 'This paper addresses the task of detecting and recognizing human-object interactions (HOI) in images and videos. We introduce the Graph Parsing Neural Network (GPNN), a framework that incorporates structural knowledge while being differentiable end-to-end. For a given scene, GPNN infers a parse graph that includes (i) the HOI graph structure represented by an adjacency matrix, and (ii) the node labels. Within a message passing inference framework, GPNN iteratively computes the adjacency matrices and node labels. We extensively evaluate our model on three HOI detection benchmarks on images and videos: HICO-DET, V-COCO, and CAD-120 datasets. Our approach significantly outperforms state-of-art methods, verifying that GPNN is scalable to large datasets and applies to spatial-temporal settings.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_25');
INSERT INTO `paper` VALUES (10882, 'Learning Monocular Depth by Distilling Cross-Domain Stereo Networks', 'Monocular depth estimation', 'Stereo matching', '', '', '', 'Monocular depth estimation aims at estimating a pixelwise depth map for a single image, which has wide applications in scene understanding and autonomous driving. Existing supervised and unsupervised methods face great challenges. Supervised methods require large amounts of depth measurement data, which are generally difficult to obtain, while unsupervised methods are usually limited in estimation accuracy. Synthetic data generated by graphics engines provide a possible solution for collecting large amounts of depth data. However, the large domain gaps between synthetic and realistic data make directly training with them challenging. In this paper, we propose to use the stereo matching network as a proxy to learn depth from synthetic data and use predicted stereo disparity maps for supervising the monocular depth estimation network. Cross-domain synthetic data could be fully utilized in this novel framework. Different strategies are proposed to ensure learned depth perception capability well transferred across different domains. Our extensive experiments show state-of-the-art results of monocular depth estimation on KITTI dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_30');
INSERT INTO `paper` VALUES (10883, 'Learning Priors for Semantic 3D Reconstruction', '', '', '', '', '', 'We present a novel semantic 3D reconstruction framework which embeds variational regularization into a neural network. Our network performs a fixed number of unrolled multi-scale optimization iterations with shared interaction weights. In contrast to existing variational methods for semantic 3D reconstruction, our model is end-to-end trainable and captures more complex dependencies between the semantic labels and the 3D geometry. Compared to previous learning-based approaches to 3D reconstruction, we integrate powerful long-range dependencies using variational coarse-to-fine optimization. As a result, our network architecture requires only a moderate number of parameters while keeping a high level of expressiveness which enables learning from very little data. Experiments on real and synthetic datasets demonstrate that our network achieves higher accuracy compared to a purely variational approach while at the same time requiring two orders of magnitude less iterations to converge. Moreover, our approach handles ten times more semantic class labels using the same computational resources.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_20');
INSERT INTO `paper` VALUES (10884, 'Learning Region Features for Object Detection', '', '', '', '', '', 'While most steps in the modern object detection methods are learnable, the region feature extraction step remains largely hand-crafted, featured by RoI pooling methods. This work proposes a general viewpoint that unifies existing region feature extraction methods and a novel method that is end-to-end learnable. The proposed method removes most heuristic choices and outperforms its RoI pooling counterparts. It moves further towards fully learnable object detection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_24');
INSERT INTO `paper` VALUES (10885, 'Learning Relationship-Aware Visual Features', 'CLEVR', 'Content-based image retrieval', 'Deep learning', 'Relational reasoning', 'Relation networks', 'Relational reasoning in Computer Vision has recently shown impressive results on visual question answering tasks. On the challenging dataset called CLEVR, the recently proposed Relation Network (RN), a simple plug-and-play module and one of the state-of-the-art approaches, has obtained a very good accuracy (95.5%) answering relational questions. In this paper, we define a sub-field of Content-Based Image Retrieval (CBIR) called Relational-CBIR (R-CBIR), in which we are interested in retrieving images with given relationships among objects. To this aim, we employ the RN architecture in order to extract relation-aware features from CLEVR images. To prove the effectiveness of these features, we extended both CLEVR and Sort-of-CLEVR datasets generating a ground-truth for R-CBIR by exploiting relational data embedded into scene-graphs. Furthermore, we propose a modification of the RN module – a two-stage Relation Network (2S-RN) – that enabled us to extract relation-aware features by using a preprocessing stage able to focus on the image content, leaving the question apart. Experiments show that our RN features, especially the 2S-RN ones, outperform the RMAC state-of-the-art features on this new challenging task.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_40');
INSERT INTO `paper` VALUES (10886, 'Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion Field Estimation', 'Rigidity estimation', 'Dynamic scene analysis', 'Scene flow', 'Motion segmentation', '', 'Estimation of 3D motion in a dynamic scene from a temporal pair of images is a core task in many scene understanding problems. In real-world applications, a dynamic scene is commonly captured by a moving camera (i.e., panning, tilting or hand-held), increasing the task complexity because the scene is observed from different viewpoints. The primary challenge is the disambiguation of the camera motion from scene motion, which becomes more difficult as the amount of rigidity observed decreases, even with successful estimation of 2D image correspondences. Compared to other state-of-the-art 3D scene flow estimation methods, in this paper, we propose to learn the rigidity of a scene in a supervised manner from an extensive collection of dynamic scene data, and directly infer a rigidity mask from two sequential images with depths. With the learned network, we show how we can effectively estimate camera motion and projected scene flow using computed 2D optical flow and the inferred rigidity mask. For training and testing the rigidity network, we also provide a new semi-synthetic dynamic scene dataset (synthetic foreground objects with a real background) and an evaluation split that accounts for the percentage of observed non-rigid pixels. Through our evaluation, we show the proposed framework outperforms current state-of-the-art scene flow estimation methods in challenging dynamic scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_29');
INSERT INTO `paper` VALUES (10887, 'Learning Shape Priors for Single-View 3D Completion And Reconstruction', 'Shape priors', 'Shape completion', '3D reconstruction', '', '', 'The problem of single-view 3D shape completion or reconstruction is challenging, because among the many possible shapes that explain an observation, most are implausible and do not correspond to natural objects. Recent research in the field has tackled this problem by exploiting the expressiveness of deep convolutional networks. In fact, there is another level of ambiguity that is often overlooked: among plausible shapes, there are still multiple shapes that fit the 2D image equally well; i.e., the ground truth shape is non-deterministic given a single-view input. Existing fully supervised approaches fail to address this issue, and often produce blurry mean shapes with smooth surfaces but no fine details. In this paper, we propose ShapeHD, pushing the limit of single-view shape completion and reconstruction by integrating deep generative models with adversarially learned shape priors. The learned priors serve as a regularizer, penalizing the model only if its output is unrealistic, not if it deviates from the ground truth. Our design thus overcomes both levels of ambiguity aforementioned. Experiments demonstrate that ShapeHD outperforms state of the art by a large margin in both shape completion and shape reconstruction on multiple real datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_40');
INSERT INTO `paper` VALUES (10888, 'Learning Single-View 3D Reconstruction with Limited Pose Supervision', 'Single-image 3D-reconstruction', 'Few-shot learning', 'GANs', '', '', 'It is expensive to label images with 3D structure or precise camera pose. Yet, this is precisely the kind of annotation required to train single-view 3D reconstruction models. In contrast, unlabeled images or images with just category labels are easy to acquire, but few current models can use this weak supervision. We present a unified framework that can combine both types of supervision: a small amount of camera pose annotations are used to enforce pose-invariance and view-point consistency, and unlabeled images combined with an adversarial loss are used to enforce the realism of rendered, generated models. We use this unified framework to measure the impact of each form of supervision in three paradigms: semi-supervised, multi-task, and transfer learning. We show that with a combination of these ideas, we can train single-view reconstruction models that improve up to 7 points in performance (AP) when using only 1% pose annotated training data.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_6');
INSERT INTO `paper` VALUES (10889, 'Learning SO(3) Equivariant Representations with Spherical CNNs', 'Convolutional Neural Network (CNNs)', 'Spherical Convolution', 'Spherical Harmonic Domain', 'Spherical Fourier Transform (SFT)', 'Group Convolution', 'We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_4');
INSERT INTO `paper` VALUES (10890, 'Learning Spatiotemporal 3D Convolution with Video Order Self-supervision', '3D Convolutional Neural Network', 'Self-supervised learning', 'Motion feature', 'Human action recognition', '', 'The purpose of this work is to explore self-supervised learning (SSL) strategy to capture a better feature with spatiotemporal 3D convolution. Although one of the next frontier in video recognition must be spatiotemporal 3D CNN, the convergence of the 3D convolutions is really difficult because of their enormous parameters or missing temporal(motion) feature. One of the effective solutions is to collect a \\(10^5\\)-order video database such as Kinetics/Moments in Time. However, this is not an efficient with burden of manual annotations. In the paper, we train 3D CNN on wrong video-sequence detection tasks in a self-supervised manner (without any manual annotation). The shuffling and verification of consecutive video-frame-order is effective for 3D CNN to capture temporal feature and get a good start point of parameters to be fine-tuned. In the experimental section, we verify that our pretrained 3D CNN on wrong clip detection improves the level of performance on UCF101 (\\(+3.99\\%\\) better than baseline, namely training 3D convolution from scratch).', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_45');
INSERT INTO `paper` VALUES (10891, 'Learning Spectral Transform Network on 3D Surface for Non-rigid Shape Analysis', 'Non-rigid shape analysis', 'Spectral transform', 'Shape representation', '', '', 'Designing a network on 3D surface for non-rigid shape analysis is a challenging task. In this work, we propose a novel spectral transform network on 3D surface to learn shape descriptors. The proposed network architecture consists of four stages: raw descriptor extraction, surface second-order pooling, mixture of power function-based spectral transform, and metric learning. The proposed network is simple and shallow. Quantitative experiments on challenging benchmarks show its effectiveness for non-rigid shape retrieval and classification, e.g., it achieved the highest accuracies on SHREC’14, 15 datasets as well as the “range” subset of SHREC’17 dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_28');
INSERT INTO `paper` VALUES (10892, 'Learning Structure-from-Motion from Motion', '', '', '', '', '', 'This work is based on a questioning of the quality metrics used by deep neural networks performing depth prediction from a single image, and then of the usability of recently published works on unsupervised learning of depth from videos. These works are all predicting depth from a single image, thus it is only known up to an undetermined scale factor, which is not sufficient for practical use cases that need an absolute depth map, i.e. the determination of the scaling factor. To overcome these limitations, we propose to learn in the same unsupervised manner a depth map inference system from monocular videos that takes a pair of images as input. This algorithm actually learns structure-from-motion from motion, and not only structure from context appearance. The scale factor issue is explicitly treated, and the absolute depth map can be estimated from camera displacement magnitude, which can be easily measured from cheap external sensors. Our solution is also much more robust with respect to domain variation and adaptation via fine tuning, because it does not rely entirely on depth from context. Two use cases are considered, unstabilized moving camera videos, and stabilized ones. This choice is motivated by the UAV (for Unmanned Aerial Vehicle) use case that generally provides reliable orientation measurement. We provide a set of experiments showing that, used in real conditions where only speed can be known, our network outperforms competitors for most depth quality measures. Results are given on the well known KITTI dataset [5], which provides robust stabilization for our second use case, but also contains moving scenes which are very typical of the in-car road context. We then present results on a synthetic dataset that we believe to be more representative of typical UAV scenes. Lastly, we present two domain adaptation use cases showing superior robustness of our method compared to single view depth algorithms, which indicates that it is better suited for highly variable visual contexts.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_27');
INSERT INTO `paper` VALUES (10893, 'Learning to Anonymize Faces for Privacy Preserving Action Detection', '', '', '', '', '', 'There is an increasing concern in computer vision devices invading users’ privacy by recording unwanted videos. On the one hand, we want the camera systems to recognize important events and assist human daily lives by understanding its videos, but on the other hand we want to ensure that they do not intrude people’s privacy. In this paper, we propose a new principled approach for learning a video face anonymizer. We use an adversarial training setting in which two competing systems fight: (1) a video anonymizer that modifies the original video to remove privacy-sensitive information while still trying to maximize spatial action detection performance, and (2) a discriminator that tries to extract privacy-sensitive information from the anonymized videos. The end result is a video anonymizer that performs pixel-level modifications to anonymize each person’s face, with minimal effect on action detection performance. We experimentally confirm the benefits of our approach compared to conventional hand-crafted anonymization methods including masking, blurring, and noise adding. Code, demo, and more results can be found on our project page https://jason718.github.io/project/privacy/main.html.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_38');
INSERT INTO `paper` VALUES (10894, 'Learning to Blend Photos', '', '', '', '', '', 'Photo blending is a common technique to create aesthetically pleasing artworks by combining multiple photos. However, the process of photo blending is usually time-consuming, and care must be taken in the process of blending, filtering, positioning, and masking each of the source photos. To make photo blending accessible to general public, we propose an efficient approach for automatic photo blending via deep learning. Specifically, given a foreground image and a background image, our proposed method automatically generates a set of blending photos with scores that indicate the aesthetics quality with the proposed quality network and policy network. Experimental results show that the proposed approach can effectively generate high quality blending photos with efficiency.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_5');
INSERT INTO `paper` VALUES (10895, 'Learning to Capture Light Fields Through a Coded Aperture Camera', 'Light field', 'CNN', 'Coded aperture', '', '', 'We propose a learning-based framework for acquiring a light field through a coded aperture camera. Acquiring a light field is a challenging task due to the amount of data. To make the acquisition process efficient, coded aperture cameras were successfully adopted; using these cameras, a light field is computationally reconstructed from several images that are acquirToshiakied with different aperture patterns. However, it is still difficult to reconstruct a high-quality light field from only a few acquired images. To tackle this limitation, we formulated the entire pipeline of light field acquisition from the perspective of an auto-encoder. This auto-encoder was implemented as a stack of fully convolutional layers and was trained end-to-end by using a collection of training samples. We experimentally show that our method can successfully learn good image-acquisition and reconstruction strategies. With our method, light fields consisting of 5 \\(\\times \\) 5 or 8 \\(\\times \\) 8 images can be successfully reconstructed only from a few acquired images. Moreover, our method achieved superior performance over several state-of-the-art methods. We also applied our method to a real prototype camera to show that it is capable of capturing a real 3-D scene.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_26');
INSERT INTO `paper` VALUES (10896, 'Learning to Detect and Track Visible and Occluded Body Joints in a Virtual World', 'Pose estimation', 'Tracking', 'Surveillance', 'Occlusions', '', 'Multi-People Tracking in an open-world setting requires a special effort in precise detection. Moreover, temporal continuity in the detection phase gains more importance when scene cluttering introduces the challenging problems of occluded targets. For the purpose, we propose a deep network architecture that jointly extracts people body parts and associates them across short temporal spans. Our model explicitly deals with occluded body parts, by hallucinating plausible solutions of not visible joints. We propose a new end-to-end architecture composed by four branches (visible heatmaps, occluded heatmaps, part affinity fields and temporal affinity fields) fed by a time linker feature extractor. To overcome the lack of surveillance data with tracking, body part and occlusion annotations we created the vastest Computer Graphics dataset for people tracking in urban scenarios by exploiting a photorealistic videogame. It is up to now the vastest dataset (about 500.000 frames, almost 10 million body poses) of human body parts for people tracking in urban scenarios. Our architecture trained on virtual data exhibits good generalization capabilities also on public real tracking benchmarks, when image resolution and sharpness are high enough, producing reliable tracklets useful for further batch data association or re-id modules.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_27');
INSERT INTO `paper` VALUES (10897, 'Learning to Dodge A Bullet: Concyclic View Morphing via Deep Learning', 'Bullet-time effect', 'Image-based rendering', 'View morphing', 'Convolutional neural network (CNN)', '', 'The bullet-time effect, presented in feature film “The Matrix”, has been widely adopted in feature films and TV commercials to create an amazing stopping-time illusion. Producing such visual effects, however, typically requires using a large number of cameras/images surrounding the subject. In this paper, we present a learning-based solution that is capable of producing the bullet-time effect from only a small set of images. Specifically, we present a view morphing framework that can synthesize smooth and realistic transitions along a circular view path using as few as three reference images. We apply a novel cyclic rectification technique to align the reference images onto a common circle and then feed the rectified results into a deep network to predict its motion field and per-pixel visibility for new view interpolation. Comprehensive experiments on synthetic and real data show that our new framework outperforms the state-of-the-art and provides an inexpensive and practical solution for producing the bullet-time effects.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_14');
INSERT INTO `paper` VALUES (10898, 'Learning to Forecast and Refine Residual Motion for Image-to-Video Generation', 'Video generation', 'Motion forecasting', 'Residual learning', '', '', 'We consider the problem of image-to-video translation, where an input image is translated into an output video containing motions of a single object. Recent methods for such problems typically train transformation networks to generate future frames conditioned on the structure sequence. Parallel work has shown that short high-quality motions can be generated by spatiotemporal generative networks that leverage temporal knowledge from the training data. We combine the benefits of both approaches and propose a two-stage generation framework where videos are generated from structures and then refined by temporal signals. To model motions more efficiently, we train networks to learn residual motion between the current and future frames, which avoids learning motion-irrelevant details. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state-of-the-art methods on both tasks demonstrate the effectiveness of our approach.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_24');
INSERT INTO `paper` VALUES (10899, 'Learning to Fuse Proposals from Multiple Scanline Optimizations in Semi-Global Matching', '', '', '', '', '', 'Semi-Global Matching (SGM) uses an aggregation scheme to combine costs from multiple 1D scanline optimizations that tends to hurt its accuracy in difficult scenarios. We propose replacing this aggregation scheme with a new learning-based method that fuses disparity proposals estimated using scanline optimization. Our proposed SGM-Forest algorithm solves this problem using per-pixel classification. SGM-Forest currently ranks 1st on the ETH3D stereo benchmark and is ranked competitively on the Middlebury 2014 and KITTI 2015 benchmarks. It consistently outperforms SGM in challenging settings and under difficult training protocols that demonstrate robust generalization, while adding only a small computational overhead to SGM.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_45');
INSERT INTO `paper` VALUES (10900, 'Learning to Learn from Web Data Through Deep Semantic Embeddings', 'Self-supervised learning', 'Webly supervised learning', 'Text embeddings', 'Multimodal retrieval', 'Multimodal embeddings', 'In this paper we propose to learn a multimodal image and text embedding from Web and Social Media data, aiming to leverage the semantic knowledge learnt in the text domain and transfer it to a visual model for semantic image retrieval. We demonstrate that the pipeline can learn from images with associated text without supervision and perform a thorough analysis of five different text embeddings in three different benchmarks. We show that the embeddings learnt with Web and Social Media data have competitive performances over supervised methods in the text based image retrieval task, and we clearly outperform state of the art in the MIRFlickr dataset when training in the target data. Further we demonstrate how semantic multimodal image retrieval can be performed using the learnt embeddings, going beyond classical instance-level retrieval problems. Finally, we present a new dataset, InstaCities1M, composed by Instagram images and their associated texts that can be used for fair comparison of image-text embeddings.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_40');
INSERT INTO `paper` VALUES (10901, 'Learning to Look around Objects for Top-View Representations of Outdoor Scenes', '3D scene understanding', 'Occlusion reasoning', 'Semantic top-view representations', '', '', 'Given a single RGB image of a complex outdoor road scene in the perspective view, we address the novel problem of estimating an occlusion-reasoned semantic scene layout in the top-view. This challenging problem not only requires an accurate understanding of both the 3D geometry and the semantics of the visible scene, but also of occluded areas. We propose a convolutional neural network that learns to predict occluded portions of the scene layout by looking around foreground objects like cars or pedestrians. But instead of hallucinating RGB values, we show that directly predicting the semantics and depths in the occluded areas enables a better transformation into the top-view. We further show that this initial top-view representation can be significantly enhanced by learning priors and rules about typical road layouts from simulated or, if available, map data. Crucially, training our model does not require costly or subjective human annotations for occluded areas or the top-view, but rather uses readily available annotations for standard semantic segmentation in the perspective view. We extensively evaluate and analyze our approach on the KITTI and Cityscapes data sets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_48');
INSERT INTO `paper` VALUES (10902, 'Learning to Navigate for Fine-Grained Classification', 'Fine-grained Classification', 'Informative Regions', 'Ground Truth Class', 'Scrutinizing', 'Stanford Cars', 'Fine-grained classification is challenging due to the difficulty of finding discriminative features. Finding those subtle traits that fully characterize the object is not straightforward. To handle this circumstance, we propose a novel self-supervision mechanism to effectively localize informative regions without the need of bounding-box/part annotations. Our model, termed NTS-Net for Navigator-Teacher-Scrutinizer Network, consists of a Navigator agent, a Teacher agent and a Scrutinizer agent. In consideration of intrinsic consistency between informativeness of the regions and their probability being ground-truth class, we design a novel training paradigm, which enables Navigator to detect most informative regions under the guidance from Teacher. After that, the Scrutinizer scrutinizes the proposed regions from Navigator and makes predictions. Our model can be viewed as a multi-agent cooperation, wherein agents benefit from each other, and make progress together. NTS-Net can be trained end-to-end, while provides accurate fine-grained classification predictions as well as highly informative regions during inference. We achieve state-of-the-art performance in extensive benchmark datasets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_26');
INSERT INTO `paper` VALUES (10903, 'Learning to Predict Crisp Boundaries', 'Edge detection', 'Contour detection', 'Convolutional neural networks', '', '', 'Recent methods for boundary or edge detection built on Deep Convolutional Neural Networks (CNNs) typically suffer from the issue of predicted edges being thick and need post-processing to obtain crisp boundaries. Highly imbalanced categories of boundary versus background in training data is one of main reasons for the above problem. In this work, the aim is to make CNNs produce sharp boundaries without post-processing. We introduce a novel loss for boundary detection, which is very effective for classifying imbalanced data and allows CNNs to produce crisp boundaries. Moreover, we propose an end-to-end network which adopts the bottom-up/top-down architecture to tackle the task. The proposed network effectively leverages hierarchical features and produces pixel-accurate boundary mask, which is critical to reconstruct the edge map. Our experiments illustrate that directly making crisp prediction not only promotes the visual results of CNNs, but also achieves better results against the state-of-the-art on the BSDS500 dataset (ODS F-score of .815) and the NYU Depth dataset (ODS F-score of .762).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_35');
INSERT INTO `paper` VALUES (10904, 'Learning to Reconstruct High-Quality 3D Shapes with Cascaded Fully Convolutional Networks', 'High-fidelity 3D reconstruction', 'Cascaded architecture', '', '', '', 'We present a data-driven approach to reconstructing high-resolution and detailed volumetric representations of 3D shapes. Although well studied, algorithms for volumetric fusion from multi-view depth scans are still prone to scanning noise and occlusions, making it hard to obtain high-fidelity 3D reconstructions. In this paper, inspired by recent advances in efficient 3D deep learning techniques, we introduce a novel cascaded 3D convolutional network architecture, which learns to reconstruct implicit surface representations from noisy and incomplete depth maps in a progressive, coarse-to-fine manner. To this end, we also develop an algorithm for end-to-end training of the proposed cascaded structure. Qualitative and quantitative experimental results on both simulated and real-world datasets demonstrate that the presented approach outperforms existing state-of-the-art work in terms of quality and fidelity of reconstructed models.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_38');
INSERT INTO `paper` VALUES (10905, 'Learning to Segment via Cut-and-Paste', 'Instance segmentation', 'Weakly-supervised', 'Deep-learning', '', '', 'This paper presents a weakly-supervised approach to object instance segmentation. Starting with known or predicted object bounding boxes, we learn object masks by playing a game of cut-and-paste in an adversarial learning setup. A mask generator takes a detection box and Faster R-CNN features, and constructs a segmentation mask that is used to cut-and-paste the object into a new image location. The discriminator tries to distinguish between real objects, and those cut and pasted via the generator, giving a learning signal that leads to improved object masks. We verify our method experimentally using Cityscapes, COCO, and aerial image datasets, learning to segment objects without ever having seen a mask in training. Our method exceeds the performance of existing weakly supervised methods, without requiring hand-tuned segment proposals, and reaches \\(90\\%\\) of supervised performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_3');
INSERT INTO `paper` VALUES (10906, 'Learning to Separate Object Sounds by Watching Unlabeled Video', '', '', '', '', '', 'Perceiving a scene most fully requires all the senses. Yet modeling how objects look and sound is challenging: most natural scenes and events contain multiple objects, and the audio track mixes all the sound sources together. We propose to learn audio-visual object models from unlabeled video, then exploit the visual context to perform audio source separation in novel videos. Our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual objects, even without observing/hearing those objects in isolation. We show how the recovered disentangled bases can be used to guide audio source separation to obtain better-separated, object-level sounds. Our work is the first to learn audio source separation from large-scale “in the wild” videos containing multiple audio sources per video. We obtain state-of-the-art results on visually-aided audio source separation and audio denoising. Our video results: http://vision.cs.utexas.edu/projects/separating_object_sounds/.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_3');
INSERT INTO `paper` VALUES (10907, 'Learning to Solve Nonlinear Least Squares for Monocular Stereo', 'Optimization', 'SLAM', 'Least squares', 'Gauss-Newton', 'Levenberg-Marquadt', 'Sum-of-squares objective functions are very popular in computer vision algorithms. However, these objective functions are not always easy to optimize. The underlying assumptions made by solvers are often not satisfied and many problems are inherently ill-posed. In this paper, we propose a neural nonlinear least squares optimization algorithm which learns to effectively optimize these cost functions even in the presence of adversities. Unlike traditional approaches, the proposed solver requires no hand-crafted regularizers or priors as these are implicitly learned from the data. We apply our method to the problem of motion stereo ie. jointly estimating the motion and scene geometry from pairs of images of a monocular sequence. We show that our learned optimizer is able to efficiently and effectively solve this challenging optimization problem.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_18');
INSERT INTO `paper` VALUES (10908, 'Learning to Zoom: A Saliency-Based Sampling Layer for Neural Networks', 'Task saliency', 'Image sampling', 'Attention', 'Spatial transformer', 'Convolutional neural networks', 'We introduce a saliency-based distortion layer for convolutional neural networks that helps to improve the spatial sampling of input data for a given task. Our differentiable layer can be added as a preprocessing block to existing task networks and trained altogether in an end-to-end fashion. The effect of the layer is to efficiently estimate how to sample from the original data in order to boost task performance. For example, for an image classification task in which the original data might range in size up to several megapixels, but where the desired input images to the task network are much smaller, our layer learns how best to sample from the underlying high resolution data in a manner which preserves task-relevant information better than uniform downsampling. This has the effect of creating distorted, caricature-like intermediate images, in which idiosyncratic elements of the image that improve task performance are zoomed and exaggerated. Unlike alternative approaches such as spatial transformer networks, our proposed layer is inspired by image saliency, computed efficiently from uniformly downsampled data, and degrades gracefully to a uniform sampling strategy under uncertainty. We apply our layer to improve existing networks for the tasks of human gaze estimation and fine-grained object classification. Code for our method is available in: http://github.com/recasens/Saliency-Sampler.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_4');
INSERT INTO `paper` VALUES (10909, 'Learning Type-Aware Embeddings for Fashion Compatibility', 'Fashion', 'Embedding methods', 'Appearance representations', '', '', 'Outfits in online fashion data are composed of items of many different types (e.g. top, bottom, shoes) that share some stylistic relationship with one another. A representation for building outfits requires a method that can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type that can go together in an outfit). This paper presents an approach to learning an image embedding that respects item type, and jointly learns notions of item similarity and compatibility in an end-to-end model. To evaluate the learned representation, we crawled 68,306 outfits created by users on the Polyvore website. Our approach obtains 3–5% improvement over the state-of-the-art on outfit compatibility prediction and fill-in-the-blank tasks using our dataset, as well as an established smaller dataset, while supporting a variety of useful queries (Code and data: https://github.com/mvasil/fashion-compatibility).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_24');
INSERT INTO `paper` VALUES (10910, 'Learning Video Features for Multi-label Classification', 'RNN', 'LSTM', 'MoE', 'ResidualCNN', '', 'This paper studies some approaches to learn representation of videos. This work was done as a part of Youtube-8M Video Understanding Challenge. The main focus is to analyze various approaches used to model temporal data and evaluate the performance of such approaches on this problem. Also, a model is proposed which reduces the size of feature vector by 70% but does not compromise on accuracy.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_30');
INSERT INTO `paper` VALUES (10911, 'Learning Visual Question Answering by Bootstrapping Hard Attention', 'Visual question answering', 'Visual Turing Test', 'Attention', '', '', 'Attention mechanisms in biological perception are thought to select subsets of perceptual information for more sophisticated processing which would be prohibitive to perform on all sensory inputs. In computer vision, however, there has been relatively little exploration of hard attention, where some information is selectively ignored, in spite of the success of soft attention, where information is re-weighted and aggregated, but never filtered out. Here, we introduce a new approach for hard attention and find it achieves very competitive performance on a recently-released visual question answering datasets, equalling and in some cases surpassing similar soft attention architectures while entirely ignoring some features. Even though the hard attention mechanism is thought to be non-differentiable, we found that the feature magnitudes correlate with semantic relevance, and provide a useful signal for our mechanism’s attentional selection criterion. Because hard attention selects important features of the input information, it can also be more efficient than analogous soft attention mechanisms. This is especially important for recent approaches that use non-local pairwise operations, whereby computational and memory costs are quadratic in the size of the set of features.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_1');
INSERT INTO `paper` VALUES (10912, 'Learning Warped Guidance for Blind Face Restoration', 'Face hallucination', 'Blind image restoration', 'Flow field', '', '', 'This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_17');
INSERT INTO `paper` VALUES (10913, 'Learning with Biased Complementary Labels', 'Multi-class classification', 'Biased complementary labels', 'Transition matrix', 'Modified loss function', '', 'In this paper, we study the classification problem in which we have access to easily obtainable surrogate for true labels, namely complementary labels, which specify classes that observations do not belong to. Let Y and \\(\\bar{Y}\\) be the true and complementary labels, respectively. We first model the annotation of complementary labels via transition probabilities \\(P(\\bar{Y}=i|Y=j), i\\ne j\\in \\{1,\\cdots ,c\\}\\), where c is the number of classes. Previous methods implicitly assume that \\(P(\\bar{Y}=i|Y=j), \\forall i\\ne j\\), are identical, which is not true in practice because humans are biased toward their own experience. For example, as shown in Fig. 1, if an annotator is more familiar with monkeys than prairie dogs when providing complementary labels for meerkats, she is more likely to employ “monkey” as a complementary label. We therefore reason that the transition probabilities will be different. In this paper, we propose a framework that contributes three main innovations to learning with biased complementary labels: (1) It estimates transition probabilities with no bias. (2) It provides a general method to modify traditional loss functions and extends standard deep neural network classifiers to learn with biased complementary labels. (3) It theoretically ensures that the classifier learned with complementary labels converges to the optimal one learned with true labels. Comprehensive experiments on several benchmark datasets validate the superiority of our method to current state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_5');
INSERT INTO `paper` VALUES (10914, 'Learning-Based Video Motion Magnification', 'Motion manipulation', 'Motion magnification', 'Deep convolutional neural network', '', '', 'Video motion magnification techniques allow us to see small motions previously invisible to the naked eyes, such as those of vibrating airplane wings, or swaying buildings under the influence of the wind. Because the motion is small, the magnification results are prone to noise or excessive blurring. The state of the art relies on hand-designed filters to extract representations that may not be optimal. In this paper, we seek to learn the filters directly from examples using deep convolutional neural networks. To make training tractable, we carefully design a synthetic dataset that captures small motion well, and use two-frame input for training. We show that the learned filters achieve high-quality results on real videos, with less ringing artifacts and better noise characteristics than previous methods. While our model is not trained with temporal filters, we found that the temporal filters can be used with our extracted representations up to a moderate magnification, enabling a frequency-based motion selection. Finally, we analyze the learned filters and show that they behave similarly to the derivative filters used in previous works. Our code, trained model, and datasets will be available online.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_39');
INSERT INTO `paper` VALUES (10915, 'Less Is More: Picking Informative Frames for Video Captioning', '', '', '', '', '', 'In video captioning task, the best practice has been achieved by attention-based models which associate salient visual components with sentences in the video. However, existing study follows a common procedure which includes a frame-level appearance modeling and motion modeling on equal interval frame sampling, which may bring about redundant visual information, sensitivity to content noise and unnecessary computation cost. We propose a plug-and-play PickNet to perform informative frame picking in video captioning. Based on a standard encoder-decoder framework, we develop a reinforcement-learning-based procedure to train the network sequentially, where the reward of each frame picking action is designed by maximizing visual diversity and minimizing discrepancy between generated caption and the ground-truth. The rewarded candidate will be selected and the corresponding latent representation of encoder-decoder will be updated for future trials. This procedure goes on until the end of the video sequence. Consequently, a compact frame subset can be selected to represent the visual information and perform video captioning without performance degradation. Experiment results show that our model can achieve competitive performance across popular benchmarks while only 6–8 frames are used.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_22');
INSERT INTO `paper` VALUES (10916, 'Leveraging Motion Priors in Videos for Improving Human Segmentation', 'Active learning', 'Domain adaptation', 'Human segmentation', '', '', 'Despite many advances in deep-learning based semantic segmentation, performance drop due to distribution mismatch is often encountered in the real world. Recently, a few domain adaptation and active learning approaches have been proposed to mitigate the performance drop. However, very little attention has been made toward leveraging information in videos which are naturally captured in most camera systems. In this work, we propose to leverage “motion prior” in videos for improving human segmentation in a weakly-supervised active learning setting. By extracting motion information using optical flow in videos, we can extract candidate foreground motion segments (referred to as motion prior) potentially corresponding to human segments. We propose to learn a memory-network-based policy model to select strong candidate segments (referred to as strong motion prior) through reinforcement learning. The selected segments have high precision and are directly used to finetune the model. In a newly collected surveillance camera dataset and a publicly available UrbanStreet dataset, our proposed method improves the performance of human segmentation across multiple scenes and modalities (i.e., RGB to Infrared (IR)). Last but not least, our method is empirically complementary to existing domain adaptation approaches such that additional performance gain is achieved by combining our weakly-supervised active learning approach with domain adaptation approaches.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_14');
INSERT INTO `paper` VALUES (10917, 'Leveraging Uncertainty to Rethink Loss Functions and Evaluation Measures for Egocentric Action Anticipation', 'Egocentric vision', 'Action anticipation', 'Loss functions', 'First person vision', '', 'Current action anticipation approaches often neglect the intrinsic uncertainty of future predictions when loss functions or evaluation measures are designed. The uncertainty of future observations is especially relevant in the context of egocentric visual data, which is naturally exposed to a great deal of variability. Considering the problem of egocentric action anticipation, we investigate how loss functions and evaluation measures can be designed to explicitly take into account the natural multi-modality of future events. In particular, we discuss suitable measures to evaluate egocentric action anticipation and study how loss functions can be defined to incorporate the uncertainty arising from the prediction of future events. Experiments performed on the EPIC-KITCHENS dataset show that the proposed loss function allows improving the results of both egocentric action anticipation and recognition methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_24');
INSERT INTO `paper` VALUES (10918, 'License Plate Detection and Recognition in Unconstrained Scenarios', 'License plate', 'Deep learning', 'Convolutional neural networks', '', '', 'Despite the large number of both commercial and academic methods for Automatic License Plate Recognition (ALPR), most existing approaches are focused on a specific license plate (LP) region (e.g. European, US, Brazilian, Taiwanese, etc.), and frequently explore datasets containing approximately frontal images. This work proposes a complete ALPR system focusing on unconstrained capture scenarios, where the LP might be considerably distorted due to oblique views. Our main contribution is the introduction of a novel Convolutional Neural Network (CNN) capable of detecting and rectifying multiple distorted license plates in a single image, which are fed to an Optical Character Recognition (OCR) method to obtain the final result. As an additional contribution, we also present manual annotations for a challenging set of LP images from different regions and acquisition conditions. Our experimental results indicate that the proposed method, without any parameter adaptation or fine tuning for a specific scenario, performs similarly to state-of-the-art commercial systems in traditional scenarios, and outperforms both academic and commercial approaches in challenging ones.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_36');
INSERT INTO `paper` VALUES (10919, 'Lifelong Learning via Progressive Distillation and Retrospection', 'Lifelong learning', 'Knowledge distillation', 'Retrospection', '', '', 'Lifelong learning aims at adapting a learned model to new tasks while retaining the knowledge gained earlier. A key challenge for lifelong learning is how to strike a balance between the preservation on old tasks and the adaptation to a new one within a given model. Approaches that combine both objectives in training have been explored in previous works. Yet the performance still suffers from considerable degradation in a long sequence of tasks. In this work, we propose a novel approach to lifelong learning, which tries to seek a better balance between preservation and adaptation via two techniques: Distillation and Retrospection. Specifically, the target model adapts to the new task by knowledge distillation from an intermediate expert, while the previous knowledge is more effectively preserved by caching a small subset of data for old tasks. The combination of Distillation and Retrospection leads to a more gentle learning curve for the target model, and extensive experiments demonstrate that our approach can bring consistent improvements on both old and new tasks (Project page: http://mmlab.ie.cuhk.edu.hk/projects/lifelong/).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_27');
INSERT INTO `paper` VALUES (10920, 'Lifting Layers: Analysis and Applications', 'Machine learning', 'Deep learning', 'Interpolation', 'Approximation theory', 'Convex relaxation', 'The great advances of learning-based approaches in image processing and computer vision are largely based on deeply nested networks that compose linear transfer functions with suitable non-linearities. Interestingly, the most frequently used non-linearities in imaging applications (variants of the rectified linear unit) are uncommon in low dimensional approximation problems. In this paper we propose a novel non-linear transfer function, called lifting, which is motivated from a related technique in convex optimization. A lifting layer increases the dimensionality of the input, naturally yields a linear spline when combined with a fully connected layer, and therefore closes the gap between low and high dimensional approximation problems. Moreover, applying the lifting operation to the loss layer of the network allows us to handle non-convex and flat (zero-gradient) cost functions. We analyze the proposed lifting theoretically, exemplify interesting properties in synthetic experiments and demonstrate its effectiveness in deep learning approaches to image classification and denoising.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_4');
INSERT INTO `paper` VALUES (10921, 'Light Structure from Pin Motion: Simple and Accurate Point Light Calibration for Physics-Based Modeling', 'Light source calibration', 'Photometric stereo', 'Shape-from-shading', 'Appearance modeling', 'Physics-based modeling', 'We present a practical method for geometric point light source calibration. Unlike in prior works that use Lambertian spheres, mirror spheres, or mirror planes, our calibration target consists of a Lambertian plane and small shadow casters at unknown positions above the plane. Due to their small size, the casters’ shadows can be localized more precisely than highlights on mirrors. We show that, given shadow observations from a moving calibration target and a fixed camera, the shadow caster positions and the light position or direction can be simultaneously recovered in a structure from motion framework. Our evaluation on simulated and real scenes shows that our method yields light estimates that are stable and more accurate than existing techniques while having a considerably simpler setup and requiring less manual labor.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_1');
INSERT INTO `paper` VALUES (10922, 'Linear RGB-D SLAM for Planar Environments', 'Linear SLAM', 'Manhattan world', 'Bayesian filtering', '', '', 'We propose a new formulation for including orthogonal planar features as a global model into a linear SLAM approach based on sequential Bayesian filtering. Previous planar SLAM algorithms estimate the camera poses and multiple landmark planes in a pose graph optimization. However, since it is formulated as a high dimensional nonlinear optimization problem, there is no guarantee the algorithm will converge to the global optimum. To overcome these limitations, we present a new SLAM method that jointly estimates camera position and planar landmarks in the map within a linear Kalman filter framework. It is rotations that make the SLAM problem highly nonlinear. Therefore, we solve for the rotational motion of the camera using structural regularities in the Manhattan world (MW), resulting in a linear SLAM formulation. We test our algorithm on standard RGB-D benchmarks as well as additional large indoors environments, demonstrating comparable performance to other state-of-the-art SLAM methods without the use of expensive nonlinear optimization.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_21');
INSERT INTO `paper` VALUES (10923, 'Linear Span Network for Object Skeleton Detection', 'Linear span framework', 'Linear span unit', 'Linear span network', 'Skeleton detection', '', 'Robust object skeleton detection requires to explore rich representative visual features and effective feature fusion strategies. In this paper, we first re-visit the implementation of HED, the essential principle of which can be ideally described with a linear reconstruction model. Hinted by this, we formalize a Linear Span framework, and propose Linear Span Network (LSN) which introduces Linear Span Units (LSUs) to minimizes the reconstruction error. LSN further utilizes subspace linear span besides the feature linear span to increase the independence of convolutional features and the efficiency of feature integration, which enhances the capability of fitting complex ground-truth. As a result, LSN can effectively suppress the cluttered backgrounds and reconstruct object skeletons. Experimental results validate the state-of-the-art performance of the proposed LSN.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_9');
INSERT INTO `paper` VALUES (10924, 'Lip Movements Generation at a Glance', 'Lip movements generation', 'Audio visual correlation', '', '', '', 'In this paper, we consider the task: given an arbitrary audio speech and one lip image of arbitrary target identity, generate synthesized lip movements of the target identity saying the speech. To perform well, a model needs to not only consider the retention of target identity, photo-realistic of synthesized images, consistency and smoothness of lip images in a sequence, but more importantly, learn the correlations between audio speech and lip movements. To solve the collective problems, we devise a network to synthesize lip movements and propose a novel correlation loss to synchronize lip changes and speech changes. Our full model utilizes four losses for a comprehensive consideration; it is trained end-to-end and is robust to lip shapes, view angles and different facial characteristics. Thoughtful experiments on three datasets ranging from lab-recorded to lips in-the-wild show that our model significantly outperforms other state-of-the-art methods extended to this task.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_32');
INSERT INTO `paper` VALUES (10925, 'Liquid Pouring Monitoring via Rich Sensory Inputs', 'Monitoring manipulation', 'Multimodal fusion', 'Auxiliary tasks', '', '', 'Humans have the amazing ability to perform very subtle manipulation task using a closed-loop control system with imprecise mechanics (i.e., our body parts) but rich sensory information (e.g., vision, tactile, etc.). In the closed-loop system, the ability to monitor the state of the task via rich sensory information is important but often less studied. In this work, we take liquid pouring as a concrete example and aim at learning to continuously monitor whether liquid pouring is successful (e.g., no spilling) or not via rich sensory inputs. We mimic humans’ rich sensories using synchronized observation from a chest-mounted camera and a wrist-mounted IMU sensor. Given many success and failure demonstrations of liquid pouring, we train a hierarchical LSTM with late fusion for monitoring. To improve the robustness of the system, we propose two auxiliary tasks during training: inferring (1) the initial state of containers and (2) forecasting the one-step future 3D trajectory of the hand with an adversarial training procedure. These tasks encourage our method to learn representation sensitive to container states and how objects are manipulated in 3D. With these novel components, our method achieves \\(\\sim \\)8% and \\(\\sim \\)11% better monitoring accuracy than the baseline method without auxiliary tasks on unseen containers and unseen users respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_21');
INSERT INTO `paper` VALUES (10926, 'Local Orthogonal-Group Testing', 'Approximate nearest neighbours', 'Group testing', 'Image retrieval', '', '', 'This work addresses approximate nearest neighbor search applied in the domain of large-scale image retrieval. Within the group testing framework we propose an efficient off-line construction of the search structures. The linear-time complexity orthogonal grouping increases the probability that at most one element from each group is matching to a given query. Non-maxima suppression with each group efficiently reduces the number of false positive results at no extra cost. Unlike in other well-performing approaches, all processing is local, fast, and suitable to process data in batches and in parallel. We experimentally show that the proposed method achieves search accuracy of the exhaustive search with significant reduction in the search complexity. The method can be naturally combined with existing embedding methods.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_28');
INSERT INTO `paper` VALUES (10927, 'Local Spectral Graph Convolution for Point Set Feature Learning', 'Point set features', 'Graph convolution', 'Spectral filtering', 'Spectral coordinates', 'Clustering', 'Feature learning on point clouds has shown great promise, with the introduction of effective and generalizable deep learning frameworks such as pointnet++. Thus far, however, point features have been abstracted in an independent and isolated manner, ignoring the relative layout of neighboring points as well as their features. In the present article, we propose to overcome this limitation by using spectral graph convolution on a local graph, combined with a novel graph pooling strategy. In our approach, graph convolution is carried out on a nearest neighbor graph constructed from a point’s neighborhood, such that features are jointly learned. We replace the standard max pooling step with a recursive clustering and pooling strategy, devised to aggregate information from within clusters of nodes that are close to one another in their spectral coordinates, leading to richer overall feature descriptors. Through extensive experiments on diverse datasets, we show a consistent demonstrable advantage for the tasks of both point set classification and segmentation. Our implementations are available at https://github.com/fate3439/LocalSpecGCN.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_4');
INSERT INTO `paper` VALUES (10928, 'Localisation via Deep Imagination: Learn the Features Not the Map', 'Localization', 'Deep Imagination', 'VMCL', 'FCU-Net', '', 'How many times does a human have to drive through the same area to become familiar with it? To begin with, we might first build a mental model of our surroundings. Upon revisiting this area, we can use this model to extrapolate to new unseen locations and imagine their appearance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_44');
INSERT INTO `paper` VALUES (10929, 'Localization Recall Precision (LRP): A New Performance Metric for Object Detection', 'Average precision', 'Object detection', 'Performance metric', 'Optimal threshold', 'Recall-precision', 'Average precision (AP), the area under the recall-precision (RP) curve, is the standard performance measure for object detection. Despite its wide acceptance, it has a number of shortcomings, the most important of which are (i) the inability to distinguish very different RP curves, and (ii) the lack of directly measuring bounding box localization accuracy. In this paper, we propose “Localization Recall Precision (LRP) Error”, a new metric specifically designed for object detection. LRP Error is composed of three components related to localization, false negative (FN) rate and false positive (FP) rate. Based on LRP, we introduce the “Optimal LRP” (oLRP), the minimum achievable LRP error representing the best achievable configuration of the detector in terms of recall-precision and the tightness of the boxes. In contrast to AP, which considers precisions over the entire recall domain, oLRP determines the “best” confidence score threshold for a class, which balances the trade-off between localization and recall-precision. In our experiments, we show that oLRP provides richer and more discriminative information than AP. We also demonstrate that the best confidence score thresholds vary significantly among classes and detectors. Moreover, we present LRP results of a simple online video object detector and show that the class-specific optimized thresholds increase the accuracy against the common approach of using a general threshold for all classes. Our experiments demonstrate that LRP is more competent than AP in capturing the performance of detectors. Our source code for PASCAL VOC AND MSCOCO datasets are provided at https://github.com/cancam/LRP.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_31');
INSERT INTO `paper` VALUES (10930, 'Long-Term Tracking in the Wild: A Benchmark', 'Object Disappearance', 'SiamFC', 'Tracking Benchmark', 'Large-scale Video Datasets', 'Target Disappearance', 'We introduce the OxUvA dataset and benchmark for evaluating single-object tracking algorithms. Benchmarks have enabled great strides in the field of object tracking by defining standardized evaluations on large sets of diverse videos. However, these works have focused exclusively on sequences that are just tens of seconds in length and in which the target is always visible. Consequently, most researchers have designed methods tailored to this “short-term” scenario, which is poorly representative of practitioners’ needs. Aiming to address this disparity, we compile a long-term, large-scale tracking dataset of sequences with average length greater than two minutes and with frequent target object disappearance. The OxUvA dataset is much larger than the object tracking datasets of recent years: it comprises 366 sequences spanning 14 h of video. We assess the performance of several algorithms, considering both the ability to locate the target and to determine whether it is present or absent. Our goal is to offer the community a large and diverse benchmark to enable the design and evaluation of tracking methods ready to be used “in the wild”. The project website is oxuva.net.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_41');
INSERT INTO `paper` VALUES (10931, 'Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation', 'Vision-and-language navigation', 'First-person view video', 'Model-based reinforcement learning', '', '', 'Existing research studies on vision and language grounding for robot navigation focus on improving model-free deep reinforcement learning (DRL) models in synthetic environments. However, model-free DRL models do not consider the dynamics in the real-world environments, and they often fail to generalize to new scenes. In this paper, we take a radical approach to bridge the gap between synthetic studies and real-world practices—We propose a novel, planned-ahead hybrid reinforcement learning model that combines model-free and model-based reinforcement learning to solve a real-world vision-language navigation task. Our look-ahead module tightly integrates a look-ahead policy model with an environment model that predicts the next state and the reward. Experimental results suggest that our proposed method significantly outperforms the baselines and achieves the best on the real-world Room-to-Room dataset. Moreover, our scalable method is more generalizable when transferring to unseen environments.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_3');
INSERT INTO `paper` VALUES (10932, 'Look Deeper into Depth: Monocular Depth Estimation with Semantic Booster and Attention-Driven Loss', 'Monocular depth', 'Semantic labeling', 'Attention loss', '', '', 'Monocular depth estimation benefits greatly from learning based techniques. By studying the training data, we observe that the per-pixel depth values in existing datasets typically exhibit a long-tailed distribution. However, most previous approaches treat all the regions in the training data equally regardless of the imbalanced depth distribution, which restricts the model performance particularly on distant depth regions. In this paper, we investigate the long tail property and delve deeper into the distant depth regions (i.e. the tail part) to propose an attention-driven loss for the network supervision. In addition, to better leverage the semantic information for monocular depth estimation, we propose a synergy network to automatically learn the information sharing strategies between the two tasks. With the proposed attention-driven loss and synergy network, the depth estimation and semantic labeling tasks can be mutually improved. Experiments on the challenging indoor dataset show that the proposed approach achieves state-of-the-art performance on both monocular depth estimation and semantic labeling tasks.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_4');
INSERT INTO `paper` VALUES (10933, 'LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks', 'Deep neural networks', 'Quantization', 'Compression', '', '', 'Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_23');
INSERT INTO `paper` VALUES (10934, 'LSQ++: Lower Running Time and Higher Recall in Multi-codebook Quantization', '', '', '', '', '', 'Multi-codebook quantization (MCQ) is the task of expressing a set of vectors as accurately as possible in terms of discrete entries in multiple bases. Work in MCQ is heavily focused on lowering quantization error, thereby improving distance estimation and recall on benchmarks of visual descriptors at a fixed memory budget. However, recent studies and methods in this area are hard to compare against each other, because they use different datasets, different protocols, and, perhaps most importantly, different computational budgets. In this work, we first benchmark a series of MCQ baselines on an equal footing and provide an analysis of their recall-vs-running-time performance. We observe that local search quantization (LSQ) is in practice much faster than its competitors, but is not the most accurate method in all cases. We then introduce two novel improvements that render LSQ (i) more accurate and (ii) faster. These improvements are easy to implement, and define a new state of the art in MCQ.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_30');
INSERT INTO `paper` VALUES (10935, 'MACNet: Multi-scale Atrous Convolution Networks for Food Places Classification in Egocentric Photo-Streams', 'Deep learning', 'Food pattern classification', 'Egocentric photo-streams', 'Visual lifelogging', '', 'First-person (wearable) camera continually captures unscripted interactions of the camera user with objects, people, and scenes reflecting his personal and relational tendencies. One of the preferences of people is their interaction with food events. The regulation of food intake and its duration has a great importance to protect against diseases. Consequently, this work aims to develop a smart model that is able to determine the recurrences of a person on food places during a day. This model is based on a deep end-to-end model for automatic food places recognition by analyzing egocentric photo-streams. In this paper, we apply multi-scale Atrous convolution networks to extract the key features related to food places of the input images. The proposed model is evaluated on an in-house private dataset called “EgoFoodPlaces”. Experimental results shows promising results of food places classification in egocentric photo-streams.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_26');
INSERT INTO `paper` VALUES (10936, 'Macro-Micro Adversarial Network for Human Parsing', 'Human parsing', 'Adversarial network', 'Inconsistency', 'Macro-Micro', '', 'In human parsing, the pixel-wise classification loss has drawbacks in its low-level local inconsistency and high-level semantic inconsistency. The introduction of the adversarial network tackles the two problems using a single discriminator. However, the two types of parsing inconsistency are generated by distinct mechanisms, so it is difficult for a single discriminator to solve them both. To address the two kinds of inconsistencies, this paper proposes the Macro-Micro Adversarial Net (MMAN). It has two discriminators. One discriminator, Macro D, acts on the low-resolution label map and penalizes semantic inconsistency, e.g., misplaced body parts. The other discriminator, Micro D, focuses on multiple patches of the high-resolution label map to address the local inconsistency, e.g., blur and hole. Compared with traditional adversarial networks, MMAN not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of adversarial networks when handling high resolution images. In our experiment, we validate that the two discriminators are complementary to each other in improving the human parsing accuracy. The proposed framework is capable of producing competitive parsing performance compared with the state-of-the-art methods, i.e., mIoU = 46.81% and 59.91% on LIP and PASCAL-Person-Part, respectively. On a relatively small dataset PPSS, our pre-trained model demonstrates impressive generalization ability. The code is publicly available at https://github.com/RoyalVane/MMAN.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_26');
INSERT INTO `paper` VALUES (10937, 'Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose Estimation', '3D object pose estimation', 'Heatmaps', 'Occlusions', '', '', 'We introduce a novel method for robust and accurate 3D object pose estimation from a single color image under large occlusions. Following recent approaches, we first predict the 2D projections of 3D points related to the target object and then compute the 3D pose from these correspondences using a geometric method. Unfortunately, as the results of our experiments show, predicting these 2D projections using a regular CNN or a Convolutional Pose Machine is highly sensitive to partial occlusions, even when these methods are trained with partially occluded examples. Our solution is to predict heatmaps from multiple small patches independently and to accumulate the results to obtain accurate and robust predictions. Training subsequently becomes challenging because patches with similar appearances but different positions on the object correspond to different heatmaps. However, we provide a simple yet effective solution to deal with such ambiguities. We show that our approach outperforms existing methods on two challenging datasets: The Occluded LineMOD dataset and the YCB-Video dataset, both exhibiting cluttered scenes with highly occluded objects.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_8');
INSERT INTO `paper` VALUES (10938, 'MAM: Transfer Learning for Fully Automatic Video Annotation and Specialized Detector Creation', 'Automatic annotation', 'Detector creation', 'Eyelids', 'Eye detection', 'Training set clustering', 'Accurate point detection on image data is an important task for many applications, such as in robot perception, scene understanding, gaze point regression in eye tracking, head pose estimation, or object outline estimation. In addition, it can be beneficial for various object detection tasks where minimal bounding boxes are searched and the method can be applied to each corner. We propose a novel self training method, Multiple Annotation Maturation (MAM) that enables fully automatic labeling of large amounts of image data. Moreover, MAM produces detectors, which can be used online afterward. We evaluated our algorithm on data from different detection tasks for eye, pupil center (head mounted and remote), and eyelid outline point and compared the performance to the state-of-the-art. The evaluation was done on over 300,000 images, and our method shows outstanding adaptability and robustness. In addition, we contribute a new dataset with more than 16,200 accurate manually-labeled images from the remote eyelid, pupil center, and pupil outline detection. This dataset was recorded in a prototype car interior equipped with all standard tools, posing various challenges to object detection such as reflections, occlusion from steering wheel movement, or large head movements. The data set and library are available for download at http://ti.uni-tuebingen.de/Projekte.1801.0.html.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_23');
INSERT INTO `paper` VALUES (10939, 'Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-Identification', 'Person re-ID', 'Attention', 'Curriculum sampling', 'Multi-task learning', '', 'We propose a novel deep network called Mancs that solves the person re-identification problem from the following aspects: fully utilizing the attention mechanism for the person misalignment problem and properly sampling for the ranking loss to obtain more stable person representation. Technically, we contribute a novel fully attentional block which is deeply supervised and can be plugged into any CNN, and a novel curriculum sampling method which is effective for training ranking losses. The learning tasks are integrated into a unified framework and jointly optimized. Experiments have been carried out on Market1501, CUHK03 and DukeMTMC. All the results show that Mancs can significantly outperform the previous state-of-the-arts. In addition, the effectiveness of the newly proposed ideas has been confirmed by extensive ablation studies.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_23');
INSERT INTO `paper` VALUES (10940, 'Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting Text with Arbitrary Shapes', 'Scene text spotting', 'Neural network', 'Arbitrary shapes', '', '', 'Recently, models based on deep neural networks have dominated the fields of scene text detection and recognition. In this paper, we investigate the problem of scene text spotting, which aims at simultaneous text detection and recognition in natural images. An end-to-end trainable neural network model for scene text spotting is proposed. The proposed model, named as Mask TextSpotter, is inspired by the newly published work Mask R-CNN. Different from previous methods that also accomplish text spotting with end-to-end trainable deep neural networks, Mask TextSpotter takes advantage of simple and smooth end-to-end learning procedure, in which precise text detection and recognition are acquired via semantic segmentation. Moreover, it is superior to previous methods in handling text instances of irregular shapes, for example, curved text. Experiments on ICDAR2013, ICDAR2015 and Total-Text demonstrate that the proposed method achieves state-of-the-art results in both scene text detection and end-to-end text recognition tasks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_5');
INSERT INTO `paper` VALUES (10941, 'MaskConnect: Connectivity Learning by Gradient Descent', 'Connectivity learning', 'Image categorization', '', '', '', 'Although deep networks have recently emerged as the model of choice for many computer vision problems, in order to yield good results they often require time-consuming architecture search. To combat the complexity of design choices, prior work has adopted the principle of modularized design which consists in defining the network in terms of a composition of topologically identical or similar building blocks (a.k.a. modules). This reduces architecture search to the problem of determining the number of modules to compose and how to connect such modules. Again, for reasons of design complexity and training cost, previous approaches have relied on simple rules of connectivity, e.g., connecting each module to only the immediately preceding module or perhaps to all of the previous ones. Such simple connectivity rules are unlikely to yield the optimal architecture for the given problem.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_22');
INSERT INTO `paper` VALUES (10942, 'MASON: A Model AgnoStic ObjectNess Framework', 'Object localization', 'Deep learning', '', '', '', 'This paper proposes a simple, yet very effective method to localize dominant foreground objects in an image, to pixel-level precision. The proposed method ‘MASON’ (Model-AgnoStic ObjectNess) uses a deep convolutional network to generate category-independent and model-agnostic heat maps for any image. The network is not explicitly trained for the task, and hence, can be used off-the-shelf in tandem with any other network or task. We show that this framework scales to a wide variety of images, and illustrate the effectiveness of MASON in three varied application contexts.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_40');
INSERT INTO `paper` VALUES (10943, 'Massively Parallel Video Networks', 'Video processing', 'Pipelining', 'Depth-parallelism', '', '', 'We introduce a class of causal video understanding models that aims to improve efficiency of video processing by maximising throughput, minimising latency, and reducing the number of clock cycles. Leveraging operation pipelining and multi-rate clocks, these models perform a minimal amount of computation (e.g. as few as four convolutional layers) for each frame per timestep to produce an output. The models are still very deep, with dozens of such operations being performed but in a pipelined fashion that enables depth-parallel computation. We illustrate the proposed principles by applying them to existing image architectures and analyse their behaviour on two video tasks: action recognition and human keypoint localisation. The results show that a significant degree of parallelism, and implicitly speedup, can be achieved with little loss in performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_40');
INSERT INTO `paper` VALUES (10944, 'Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone Image', '', '', '', '', '', 'We propose a material acquisition approach to recover the spatially-varying BRDF and normal map of a near-planar surface from a single image captured by a handheld mobile phone camera. Our method images the surface under arbitrary environment lighting with the flash turned on, thereby avoiding shadows while simultaneously capturing high-frequency specular highlights. We train a CNN to regress an SVBRDF and surface normals from this image. Our network is trained using a large-scale SVBRDF dataset and designed to incorporate physical insights for material estimation, including an in-network rendering layer to model appearance and a material classifier to provide additional supervision during training. We refine the results from the network using a dense CRF module whose terms are designed specifically for our task. The framework is trained end-to-end and produces high quality results for a variety of materials. We provide extensive ablation studies to evaluate our network on both synthetic and real data, while demonstrating significant improvements in comparisons with prior works.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_5');
INSERT INTO `paper` VALUES (10945, 'Maximum Margin Metric Learning over Discriminative Nullspace for Person Re-identification', 'Person re-identification', 'Metric learning', 'Small sample size problem', '', '', 'In this paper we propose a novel metric learning framework called Nullspace Kernel Maximum Margin Metric Learning (NK3ML) which efficiently addresses the small sample size (SSS) problem inherent in person re-identification and offers a significant performance gain over existing state-of-the-art methods. Taking advantage of the very high dimensionality of the feature space, the metric is learned using a maximum margin criterion (MMC) over a discriminative nullspace where all training sample points of a given class map onto a single point, minimizing the within class scatter. A kernel version of MMC is used to obtain a better between class separation. Extensive experiments on four challenging benchmark datasets for person re-identification demonstrate that the proposed algorithm outperforms all existing methods. We obtain 99.8% rank-1 accuracy on the most widely accepted and challenging dataset VIPeR, compared to the previous state of the art being only 63.92%.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_8');
INSERT INTO `paper` VALUES (10946, 'Memory Aware Synapses: Learning What (not) to Forget', 'Previous Task', 'Catastrophic Forgetting', 'Limited Capacity Model', 'Online Manner', 'Importance Weights', 'Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb’s rule, which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting <subject, predicate, object> triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_9');
INSERT INTO `paper` VALUES (10947, 'Meta-tracker: Fast and Robust Online Adaptation for Visual Object Trackers', '', '', '', '', '', 'This paper improves state-of-the-art visual object trackers that use online adaptation. Our core contribution is an offline meta-learning-based method to adjust the initial deep networks used in online adaptation-based tracking. The meta learning is driven by the goal of deep networks that can quickly be adapted to robustly model a particular target in future frames. Ideally the resulting models focus on features that are useful for future frames, and avoid overfitting to background clutter, small parts of the target, or noise. By enforcing a small number of update iterations during meta-learning, the resulting networks train significantly faster. We demonstrate this approach on top of the high performance tracking approaches: tracking-by-detection based MDNet [1] and the correlation based CREST [2]. Experimental results on standard benchmarks, OTB2015 [3] and VOT2016 [4], show that our meta-learned versions of both trackers improve speed, accuracy, and robustness.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_35');
INSERT INTO `paper` VALUES (10948, 'Metrics for Real-Time Mono-VSLAM Evaluation Including IMU Induced Drift with Application to UAV Flight', '', '', '', '', '', 'Vision based algorithms became popular for state estimation and subsequent (local) control of mobile robots. Currently a large variety of such algorithms exists and their performance is often characterized through their drift relative to the total trajectory traveled. However, this metric has relatively low relevance for local vehicle control/stabilization. In this paper, we propose a set of metrics which allows to evaluate a vision based algorithm with respect to its usability for state estimation and subsequent (local) control of highly dynamic autonomous mobile platforms such as multirotor UAVs. As such platforms usually make use of inertial measurements to mitigate the relatively low update rate of the visual algorithm, we particularly focus on a new metric taking the expected IMU-induced drift between visual readings into consideration based on the probabilistic properties of the sensor. We demonstrate this set of metrics by comparing ORB-SLAM, LSD-SLAM and DSO on different datasets.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_6');
INSERT INTO `paper` VALUES (10949, 'Mitigating Bias in Gender, Age and Ethnicity Classification: A Multi-task Convolution Neural Network Approach', 'Bias', 'Facial analysis', 'Age', 'Gender and race', 'Soft biometrics', 'This work explores joint classification of gender, age and race. Specifically, we here propose a Multi-Task Convolution Neural Network (MTCNN) employing joint dynamic loss weight adjustment towards classification of named soft biometrics, as well as towards mitigation of soft biometrics related bias. The proposed algorithm achieves promising results on the UTKFace and the Bias Estimation in Face Analytics (BEFA) datasets and was ranked first in the BEFA Challenge of the European Conference of Computer Vision (ECCV) 2018.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_35');
INSERT INTO `paper` VALUES (10950, 'ML-LocNet: Improving Object Localization with Multi-view Learning Network', 'Weakly supervised learning', 'Object localization', 'Multi-view learning', 'Object instance mining', '', 'This paper addresses Weakly Supervised Object Localization (WSOL) with only image-level supervision. We propose a Multi-view Learning Localization Network (ML-LocNet) by incorporating multi-view learning into a two-phase WSOL model. The multi-view learning would benefit localization due to the complementary relationships among the learned features from different views and the consensus property among the mined instances from each view. In the first phase, the representation is augmented by integrating features learned from multiple views, and in the second phase, the model performs multi-view co-training to enhance localization performance of one view with the help of instances mined from other views, which thus effectively avoids early fitting. ML-LocNet can be easily combined with existing WSOL models to further improve the localization accuracy. Its effectiveness has been proved experimentally. Notably, it achieves \\(68.6\\%\\) CorLoc and \\(49.7\\%\\) mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_15');
INSERT INTO `paper` VALUES (10951, 'MoA-Net: Self-supervised Motion Segmentation', 'Optical flow', 'Motion segmentation', 'Video segmentation', 'Camera motion', 'Visual ecology', 'Most recent approaches to motion segmentation use optical flow to segment an image into the static environment and independently moving objects. Neural network based approaches usually require large amounts of labeled training data to achieve state-of-the-art performance. In this work we propose a new approach to train a motion segmentation network in a self-supervised manner. Inspired by visual ecology, the human visual system, and by prior approaches to motion modeling, we break down the problem of motion segmentation into two smaller subproblems: (1) modifying the flow field to remove the observer’s rotation and (2) segmenting the rotation-compensated flow into static environment and independently moving objects. Compensating for rotation leads to essential simplifications that allow us to describe an independently moving object with just a few criteria which can be learned by our new motion segmentation network - the Motion Angle Network (MoA-Net). We compare our network with two other motion segmentation networks and show state-of-the-art performance on Sintel.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_55');
INSERT INTO `paper` VALUES (10952, 'MobileFace: 3D Face Reconstruction with Efficient CNN Regression', '3D face reconstruction', 'Morphable model', 'CNN', '', '', 'Estimation of facial shapes plays a central role for face transfer and animation. Accurate 3D face reconstruction, however, often deploys iterative and costly methods preventing real-time applications. In this work we design a compact and fast CNN model enabling real-time face reconstruction on mobile devices. For this purpose, we first study more traditional but slow morphable face models and use them to automatically annotate a large set of images for CNN training. We then investigate a class of efficient MobileNet CNNs and adapt such models for the task of shape regression. Our evaluation on three datasets demonstrates significant improvements in the speed and the size of our model while maintaining state-of-the-art reconstruction accuracy.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_3');
INSERT INTO `paper` VALUES (10953, 'Modality Distillation with Multiple Stream Networks for Action Recognition', 'Action recognition', 'Deep multimodal learning', 'Distillation', 'Privileged information', '', 'Diverse input data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while a (training) dataset could be accurately designed to include a variety of sensory inputs, it is often the case that not all modalities are available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to learn robust representations leveraging multimodal data in the training stage, while considering limitations at test time, such as noisy or missing modalities. This paper presents a new approach for multimodal video action recognition, developed within the unified frameworks of distillation and privileged information, named generalized distillation. Particularly, we consider the case of learning representations from depth and RGB videos, while relying on RGB data only at test time. We propose a new approach to train an hallucination network that learns to distill depth features through multiplicative connections of spatiotemporal representations, leveraging soft labels and hard labels, as well as distance between feature maps. We report state-of-the-art results on video action classification on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the UWA3DII and Northwestern-UCLA.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_7');
INSERT INTO `paper` VALUES (10954, 'Model Adaptation with Synthetic and Real Data for Semantic Dense Foggy Scene Understanding', 'Semantic foggy scene understanding', 'Fog simulation', 'Synthetic data', 'Curriculum Model Adaptation', 'Curriculum learning', 'This work addresses the problem of semantic scene understanding under dense fog. Although considerable progress has been made in semantic scene understanding, it is mainly related to clear-weather scenes. Extending recognition methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both synthetic and real foggy data. In addition, we present three other main stand-alone contributions: (1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; (2) a new fog density estimator; (3) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 16 images with dense fog. Our experiments show that (1) our fog simulation slightly outperforms a state-of-the-art competing simulation with respect to the task of semantic foggy scene understanding (SFSU); (2) CMAda improves the performance of state-of-the-art models for SFSU significantly by leveraging unlabeled real foggy data. The datasets and code will be made publicly available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_42');
INSERT INTO `paper` VALUES (10955, 'Model Selection for Generalized Zero-Shot Learning', 'Model selection', 'Generalized zero-shot learning', 'Generative Adversarial Network', '', '', 'In the problem of generalized zero-shot learning, the datapoints from unknown classes are not available during training. The main challenge for generalized zero-shot learning is the unbalanced data distribution which makes it hard for the classifier to distinguish if a given testing sample comes from a seen or unseen class. However, using Generative Adversarial Network (GAN) to generate auxiliary datapoints by the semantic embeddings of unseen classes alleviates the above problem. Current approaches combine the auxiliary datapoints and original training data to train the generalized zero-shot learning model and obtain state-of-the-art results. Inspired by such models, we propose to feed the generated data via a model selection mechanism. Specifically, we leverage two sources of datapoints (observed and auxiliary) to train some classifier to recognize which test datapoints come from seen and which from unseen classes. This way, generalized zero-shot learning can be divided into two disjoint classification tasks, thus reducing the negative influence of the unbalanced data distribution. Our evaluations on four publicly available datasets for generalized zero-shot learning show that our model obtains state-of-the-art results.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_16');
INSERT INTO `paper` VALUES (10956, 'Model-free Consensus Maximization for Non-Rigid Shapes', '', '', '', '', '', 'Many computer vision methods use consensus maximization to relate measurements containing outliers with the correct transformation model. In the context of rigid shapes, this is typically done using Random Sampling and Consensus (RANSAC) by estimating an analytical model that agrees with the largest number of measurements (inliers). However, small parameter models may not be always available. In this paper, we formulate the model-free consensus maximization as an Integer Program in a graph using ‘rules’ on measurements. We then provide a method to solve it optimally using the Branch and Bound (BnB) paradigm. We focus its application on non-rigid shapes, where we apply the method to remove outlier 3D correspondences and achieve performance superior to the state of the art. Our method works with outlier ratio as high as 80%. We further derive a similar formulation for 3D template to image matching, achieving similar or better performance compared to the state of the art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_8');
INSERT INTO `paper` VALUES (10957, 'Modeling Camera Effects to Improve Visual Learning from Synthetic Data', 'Deep learning', 'Image augmentation', 'Object detection', '', '', 'Recent work has focused on generating synthetic imagery to increase the size and variability of training data for learning visual tasks in urban scenes. This includes increasing the occurrence of occlusions or varying environmental and weather effects. However, few have addressed modeling variation in the sensor domain. Sensor effects can degrade real images, limiting generalizability of network performance on visual tasks trained on synthetic data and tested in real environments. This paper proposes an efficient, automatic, physically-based augmentation pipeline to vary sensor effects – chromatic aberration, blur, exposure, noise, and color temperature – for synthetic imagery. In particular, this paper illustrates that augmenting synthetic training datasets with the proposed pipeline reduces the domain gap between synthetic and real domains for the task of object detection in urban driving scenes.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_31');
INSERT INTO `paper` VALUES (10958, 'Modeling Varying Camera-IMU Time Offset in Optimization-Based Visual-Inertial Odometry', 'Visual-inertial odometry', 'Online temporal camera-IMU calibration', 'Rolling shutter cameras', '', '', 'Combining cameras and inertial measurement units (IMUs) has been proven effective in motion tracking, as these two sensing modalities offer complementary characteristics that are suitable for fusion. While most works focus on global-shutter cameras and synchronized sensor measurements, consumer-grade devices are mostly equipped with rolling-shutter cameras and suffer from imperfect sensor synchronization. In this work, we propose a nonlinear optimization-based monocular visual inertial odometry (VIO) with varying camera-IMU time offset modeled as an unknown variable. Our approach is able to handle the rolling-shutter effects and imperfect sensor synchronization in a unified way. Additionally, we introduce an efficient algorithm based on dynamic programming and red-black tree to speed up IMU integration over variable-length time intervals during the optimization. An uncertainty-aware initialization is also presented to launch the VIO robustly. Comparisons with state-of-the-art methods on the Euroc dataset and mobile phone data are shown to validate the effectiveness of our approach.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_30');
INSERT INTO `paper` VALUES (10959, 'Modeling Visual Context Is Key to Augmenting Object Detection Datasets', 'Object detection', 'Data augmentation', 'Visual context', '', '', 'Performing data augmentation for learning deep neural networks is well known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. For object detection, classical approaches for data augmentation consist of generating images obtained by basic geometrical transformations and color changes of original training images. In this work, we go one step further and leverage segmentation annotations to increase the number of object instances present on training data. For this approach to be successful, we show that modeling appropriately the visual context surrounding objects is crucial to place them in the right environment. Otherwise, we show that the previous strategy actually hurts. With our context model, we achieve significant mean average precision improvements when few labeled examples are available on the VOC’12 benchmark.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_23');
INSERT INTO `paper` VALUES (10960, 'Modular Generative Adversarial Networks', 'Neural modular network', 'Generative adversarial network', 'Image generation', 'Image translation', '', 'Existing methods for multi-domain image-to-image translation (or generation) attempt to directly map an input image (or a random vector) to an image in one of the output domains. However, most existing methods have limited scalability and robustness, since they require building independent models for each pair of domains in question. This leads to two significant shortcomings: (1) the need to train exponential number of pairwise models, and (2) the inability to leverage data from other domains when training a particular pairwise mapping. Inspired by recent work on module networks, this paper proposes ModularGAN for multi-domain image generation and image-to-image translation. ModularGAN consists of several reusable and composable modules that carry on different functions (e.g., encoding, decoding, transformations). These modules can be trained simultaneously, leveraging data from all domains, and then combined to construct specific GAN networks at test time, according to the specific image translation task. This leads to ModularGAN’s superior flexibility of generating (or translating to) an image in any desired domain. Experimental results demonstrate that our model not only presents compelling perceptual results but also outperforms state-of-the-art methods on multi-domain facial attribute transfer.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_10');
INSERT INTO `paper` VALUES (10961, 'Monocular Depth Estimation Using Whole Strip Masking and Reliability-Based Refinement', 'Monocular depth estimation', 'Whole strip masking', 'Reliability', 'Depth map refinement', '', 'We propose a monocular depth estimation algorithm based on whole strip masking (WSM) and reliability-based refinement. First, we develop a convolutional neural network (CNN) tailored for the depth estimation. Specifically, we design a novel filter, called WSM, to exploit the tendency that a scene has similar depths in horizonal or vertical directions. The proposed CNN combines WSM upsampling blocks with a ResNet encoder. Second, we measure the reliability of an estimated depth, by appending additional layers to the main CNN. Using the reliability information, we perform conditional random field (CRF) optimization to refine the estimated depth map. Experimental results demonstrate that the proposed algorithm provides the state-of-the-art depth estimation performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_3');
INSERT INTO `paper` VALUES (10962, 'Monocular Depth Estimation with Affinity, Vertical Pooling, and Label Enhancement', 'Monocular depth', 'Affinity', 'Vertical aggregation', '', '', 'Significant progress has been made in monocular depth estimation with Convolutional Neural Networks (CNNs). While absolute features, such as edges and textures, could be effectively extracted, the depth constraint of neighboring pixels, namely relative features, has been mostly ignored by recent CNN-based methods. To overcome this limitation, we explicitly model the relationships of different image locations with an affinity layer and combine absolute and relative features in an end-to-end network. In addition, we consider prior knowledge that major depth changes lie in the vertical direction, and thus, it is beneficial to capture long-range vertical features for refined depth estimation. In the proposed algorithm we introduce vertical pooling to aggregate image features vertically to improve the depth accuracy. Furthermore, since the Lidar depth ground truth is quite sparse, we enhance the depth labels by generating high-quality dense depth maps with off-the-shelf stereo matching method taking left-right image pairs as input. We also integrate multi-scale structure in our network to obtain global understanding of the image depth and exploit residual learning to help depth refinement. We demonstrate that the proposed algorithm performs favorably against state-of-the-art methods both qualitatively and quantitatively on the KITTI driving dataset.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_14');
INSERT INTO `paper` VALUES (10963, 'MoQA – A Multi-modal Question Answering Architecture', '', '', '', '', '', 'Multi-Modal Machine Comprehension (M3C) deals with extracting knowledge from multiple modalities such as figures, diagrams and text. Particularly, Textbook Question Answering (TQA) focuses on questions based on the school curricula, where the text and diagrams are extracted from textbooks. A subset of questions cannot be answered solely based on diagrams, but requires external knowledge of the surrounding text. In this work, we propose a novel deep model that is able to handle different knowledge modalities in the context of the question answering task. We compare three different information representations encountered in TQA: a visual representation learned from images, a graph representation of diagrams and a language-based representation learned from accompanying text. We evaluate our model on the TQA dataset that contains text and diagrams from the sixth grade material. Even though our model obtains competing results compared to state-of-the-art, we still witness a significant gap in performance compared to humans. We discuss in this work the shortcomings of the model and show the reason behind the large gap to human performance, by exploring the distribution of the multiple classes of mistakes that the model makes.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_9');
INSERT INTO `paper` VALUES (10964, 'Motion Feature Network: Fixed Motion Filter for Action Recognition', 'Action recognition', 'Motion filter', 'MFNet', 'Spatio-temporal representation', '', 'Spatio-temporal representations in frame sequences play an important role in the task of action recognition. Previously, a method of using optical flow as a temporal information in combination with a set of RGB images that contain spatial information has shown great performance enhancement in the action recognition tasks. However, it has an expensive computational cost and requires two-stream (RGB and optical flow) framework. In this paper, we propose MFNet (Motion Feature Network) containing motion blocks which make it possible to encode spatio-temporal information between adjacent frames in a unified network that can be trained end-to-end. The motion block can be attached to any existing CNN-based action recognition frameworks with only a small additional cost. We evaluated our network on two of the action recognition datasets (Jester and Something-Something) and achieved competitive performances for both datasets by training the networks from scratch.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_24');
INSERT INTO `paper` VALUES (10965, 'Motion Segmentation Using Spectral Clustering on Indian Road Scenes', 'Motion segmentation', 'Object detection', 'Spectral clustering', '', '', 'We propose a novel motion segmentation formulation over spatio-temporal depth images obtained from stereo sequences that segments multiple motion models in the scene in an unsupervised manner. The motion segmentation is obtained at frame rates that compete with the speed of the stereo depth computation. This is possible due to a decoupling framework that first delineates spatial clusters and subsequently assigns motion labels to each of these cluster with analysis of a novel motion graph model. A principled computation of the weights of the motion graph that signifies the relative shear and stretch between possible clusters lends itself to a high fidelity segmentation of the motion models in the scene.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_42');
INSERT INTO `paper` VALUES (10966, 'Motion Selectivity of Neurons in Self-driving Networks', 'Optical flow', 'Motion selectivity', 'Self-driving', 'Autonomous driving', 'Convolutional neural network', 'We investigated if optical flow filters were implicitly learned by a neural network trained to drive a vehicle. The network was not trained to predict optical flow across the frames, but, through a series of controlled experiments, we claim that optical flow filters are present in the network. However, this appears to be only the case for sideways flows more relevant for steering predictions. For motor throttle predictions, the network looks at the variance of the pixels over time rather than computing optical flow. In addition, the filters that are likely used for motor throttle predictions dominate primarily in the middle of the network.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_32');
INSERT INTO `paper` VALUES (10967, 'Move Forward and Tell: A Progressive Generator of Video Descriptions', 'Video captioning', 'Move forward and tell', 'Recurrent network', 'Reinforcement learning', 'Repetition evaluation', 'We present an efficient framework that can generate a coherent paragraph to describe a given video. Previous works on video captioning usually focus on video clips. They typically treat an entire video as a whole and generate the caption conditioned on a single embedding. On the contrary, we consider videos with rich temporal structures and aim to generate paragraph descriptions that can preserve the story flow while being coherent and concise. Towards this goal, we propose a new approach, which produces a descriptive paragraph by assembling temporally localized descriptions. Given a video, it selects a sequence of distinctive clips and generates sentences thereon in a coherent manner. Particularly, the selection of clips and the production of sentences are done jointly and progressively driven by a recurrent network – what to describe next depends on what have been said before. Here, the recurrent network is learned via self-critical sequence training with both sentence-level and paragraph-level rewards. On the ActivityNet Captions dataset, our method demonstrated the capability of generating high-quality paragraph descriptions for videos. Compared to those by other methods, the descriptions produced by our method are often more relevant, more coherent, and more concise.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_29');
INSERT INTO `paper` VALUES (10968, 'MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models', 'Graphical models', 'Block-Coordinate-Ascent', 'Message passing algorithms', '', '', 'Dense, discrete Graphical Models with pairwise potentials are a powerful class of models which are employed in state-of-the-art computer vision and bio-imaging applications. This work introduces a new MAP-solver, based on the popular Dual Block-Coordinate Ascent principle. Surprisingly, by making a small change to a low-performing solver, the Max Product Linear Programming (MPLP) algorithm [7], we derive the new solver MPLP++ that significantly outperforms all existing solvers by a large margin, including the state-of-the-art solver Tree-Reweighted Sequential (TRW-S) message-passing algorithm [17]. Additionally, our solver is highly parallel, in contrast to TRW-S, which gives a further boost in performance with the proposed GPU and multi-thread CPU implementations. We verify the superiority of our algorithm on dense problems from publicly available benchmarks as well as a new benchmark for 6D Object Pose estimation. We also provide an ablation study with respect to graph density.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_16');
INSERT INTO `paper` VALUES (10969, 'MRF Optimization with Separable Convex Prior on Partially Ordered Labels', 'Multi-labeling problem', 'Poset', 'Sub-modular relaxation', '', '', 'Solving a multi-labeling problem with a convex penalty can be achieved in polynomial time if the label set is totally ordered. In this paper we propose a generalization to partially ordered sets. To this end, we assume that the label set is the Cartesian product of totally ordered sets and the convex prior is separable. For this setting we introduce a general combinatorial optimization framework that provides an approximate solution. More specifically, we first construct a graph whose minimal cut provides a lower bound to our energy. The result of this relaxation is then used to get a feasible solution via classical move-making cuts. To speed up the optimization, we propose an efficient coarse-to-fine approach over the label space. We demonstrate the proposed framework through extensive experiments for optical flow estimation.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_21');
INSERT INTO `paper` VALUES (10970, 'MT-VAE: Learning Motion Transformations to Generate Multimodal Human Dynamics', '', '', '', '', '', 'Long-term human motion can be represented as a series of motion modes—motion sequences that capture short-term temporal dynamics—with transitions between them. We leverage this structure and present a novel Motion Transformation Variational Auto-Encoders (MT-VAE) for learning motion sequence generation. Our model jointly learns a feature embedding for motion modes (that the motion sequence can be reconstructed from) and a feature transformation that represents the transition of one motion mode to the next motion mode. Our model is able to generate multiple diverse and plausible motion sequences in the future from the same input. We apply our approach to both facial and full body motion, and demonstrate applications like analogy-based motion transfer and video synthesis.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_17');
INSERT INTO `paper` VALUES (10971, 'Multi-Attention Multi-Class Constraint for Fine-grained Image Recognition', 'Fine-grained classification', 'Metric learning', 'Visual attention', 'Multi-attention Multi-class constraint', 'One-squeeze Multi-excitation', 'Attention-based learning for fine-grained image recognition remains a challenging task, where most of the existing methods treat each object part in isolation, while neglecting the correlations among them. In addition, the multi-stage or multi-scale mechanisms involved make the existing methods less efficient and hard to be trained end-to-end. In this paper, we propose a novel attention-based convolutional neural network (CNN) which regulates multiple object parts among different input images. Our method first learns multiple attention region features of each input image through the one-squeeze multi-excitation (OSME) module, and then apply the multi-attention multi-class constraint (MAMC) in a metric learning framework. For each anchor feature, the MAMC functions by pulling same-attention same-class features closer, while pushing different-attention or different-class features away. Our method can be easily trained end-to-end, and is highly efficient which requires only one training stage. Moreover, we introduce Dogs-in-the-Wild, a comprehensive dog species dataset that surpasses similar existing datasets by category coverage, data volume and annotation quality. Extensive experiments are conducted to show the substantial improvements of our method on four benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_49');
INSERT INTO `paper` VALUES (10972, 'Multi-class Model Fitting by Energy Minimization and Mode-Seeking', 'Multi-model fitting', 'Clustering', 'Energy minimization', '', '', 'We propose a general formulation, called Multi-X, for multi-class multi-instance model fitting – the problem of interpreting the input data as a mixture of noisy observations originating from multiple instances of multiple classes. We extend the commonly used \\(\\alpha \\)-expansion-based technique with a new move in the label space. The move replaces a set of labels with the corresponding density mode in the model parameter domain, thus achieving fast and robust optimization. Key optimization parameters like the bandwidth of the mode seeking are set automatically within the algorithm. Considering that a group of outliers may form spatially coherent structures in the data, we propose a cross-validation-based technique removing statistically insignificant instances. Multi-X outperforms significantly the state-of-the-art on publicly available datasets for diverse problems: multiple plane and rigid motion detection; motion segmentation; simultaneous plane and cylinder fitting; circle and line fitting.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_14');
INSERT INTO `paper` VALUES (10973, 'Multi-Domain Pose Network for Multi-Person Pose Estimation and Tracking', 'Human pose estimation', 'Multi-domain learning', '', '', '', 'Multi-person human pose estimation and tracking in the wild is important and challenging. For training a powerful model, large-scale training data are crucial. While there are several datasets for human pose estimation, the best practice for training on multi-dataset has not been investigated. In this paper, we present a simple network called Multi-Domain Pose Network (MDPN) to address this problem. By treating the task as multi-domain learning, our methods can learn a better representation for pose prediction. Together with prediction heads fine-tuning and multi-branch combination, it shows significant improvement over baselines and achieves the best performance on PoseTrack ECCV 2018 Challenge without additional datasets other than MPII and COCO.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_17');
INSERT INTO `paper` VALUES (10974, 'Multi-fiber Networks for Video Recognition', 'Deep learning', 'Neural networks', 'Video', 'Classification', 'Action recognition', 'In this paper, we aim to reduce the computational cost of spatio-temporal deep neural networks, making them run as fast as their 2D counterparts while preserving state-of-the-art accuracy on video recognition benchmarks. To this end, we present the novel Multi-Fiber architecture that slices a complex neural network into an ensemble of lightweight networks or fibers that run through the network. To facilitate information flow between fibers we further incorporate multiplexer modules and end up with an architecture that reduces the computational cost of 3D networks by an order of magnitude, while increasing recognition performance at the same time. Extensive experimental results show that our multi-fiber architecture significantly boosts the efficiency of existing convolution networks for both image and video recognition tasks, achieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics datasets. Our proposed model requires over 9\\(\\times \\) and 13\\(\\times \\) less computations than the I3D [1] and R(2+1)D [2] models, respectively, yet providing higher accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_22');
INSERT INTO `paper` VALUES (10975, 'Multi-kernel Diffusion CNNs for Graph-Based Learning on Point Clouds', 'Graph convolutional networks', 'Point descriptor learning', 'Point cloud segmentation', '', '', 'Graph convolutional networks are a new promising learning approach to deal with data on irregular domains. They are predestined to overcome certain limitations of conventional grid-based architectures and will enable efficient handling of point clouds or related graphical data representations, e.g. superpixel graphs. Learning feature extractors and classifiers on 3D point clouds is still an underdeveloped area and has potential restrictions to equal graph topologies. In this work, we derive a new architectural design that combines rotationally and topologically invariant graph diffusion operators and node-wise feature learning through \\(1\\times 1\\) convolutions. By combining multiple isotropic diffusion operations based on the Laplace-Beltrami operator, we can learn an optimal linear combination of diffusion kernels for effective feature propagation across nodes on an irregular graph. We validated our approach for learning point descriptors as well as semantic classification on real 3D point clouds of human poses and demonstrate an improvement from 85% to 95% in Dice overlap with our multi-kernel approach.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_33');
INSERT INTO `paper` VALUES (10976, 'Multi-level Activation for Segmentation of Hierarchically-Nested Classes', 'Segmentation', 'Multiclass', 'Inclusion', 'Nested classes', 'Class hierarchy', 'For many biological image segmentation tasks, including topological knowledge, such as the nesting of classes, can greatly improve results. However, most ‘out-of-the-box’ CNN models are still blind to such prior information. In this paper, we propose a novel approach to encode this information, through a multi-level activation layer and three compatible losses. We benchmark all of them on nuclei segmentation in bright-field microscopy cell images from the 2018 Data Science Bowl challenge, offering an exemplary segmentation task with cells and nested subcellular structures. Our scheme greatly speeds up learning, and outperforms standard multi-class classification with soft-max activation and a previously proposed method stemming from it, improving the Dice score significantly (p-values \\(<0.007\\)). Our approach is conceptually simple, easy to implement and can be integrated in any CNN architecture. It can be generalized to a higher number of classes, with or without further relations of containment.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_24');
INSERT INTO `paper` VALUES (10977, 'Multi-modal Cycle-Consistent Generalized Zero-Shot Learning', 'Generalized zero-shot learning', 'Generative adversarial networks', 'Cycle consistency loss', '', '', 'In generalized zero shot learning (GZSL), the set of classes are split into seen and unseen classes, where training relies on the semantic features of the seen and unseen classes and the visual representations of only the seen classes, while testing uses the visual representations of the seen and unseen classes. Current methods address GZSL by learning a transformation from the visual to the semantic space, exploring the assumption that the distribution of classes in the semantic and visual spaces is relatively similar. Such methods tend to transform unseen testing visual representations into one of the seen classes’ semantic features instead of the semantic features of the correct unseen class, resulting in low accuracy GZSL classification. Recently, generative adversarial networks (GAN) have been explored to synthesize visual representations of the unseen classes from their semantic features - the synthesized representations of the seen and unseen classes are then used to train the GZSL classifier. This approach has been shown to boost GZSL classification accuracy, but there is one important missing constraint: there is no guarantee that synthetic visual representations can generate back their semantic feature in a multi-modal cycle-consistent manner. This missing constraint can result in synthetic visual representations that do not represent well their semantic features, which means that the use of this constraint can improve GAN-based approaches. In this paper, we propose the use of such constraint based on a new regularization for the GAN training that forces the generated visual features to reconstruct their original semantic features. Once our model is trained with this multi-modal cycle-consistent semantic compatibility, we can then synthesize more representative visual representations for the seen and, more importantly, for the unseen classes. Our proposed approach shows the best GZSL classification results in the field in several publicly available datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_2');
INSERT INTO `paper` VALUES (10978, 'Multi-modal Spectral Image Super-Resolution', 'Spectral reconstruction', 'Spectral image super-resolution', 'Residual learning', 'Image completion', 'Multi-modality', 'Recent advances have shown the great power of deep convolutional neural networks (CNN) to learn the relationship between low and high-resolution image patches. However, these methods only take a single-scale image as input and require large amount of data to train without the risk of overfitting. In this paper, we tackle the problem of multi-modal spectral image super-resolution while constraining ourselves to a small dataset. We propose the use of different modalities to improve the performance of neural networks on the spectral super-resolution problem. First, we use multiple downscaled versions of the same image to infer a better high-resolution image for training, we refer to these inputs as a multi-scale modality. Furthermore, color images are usually taken at a higher resolution than spectral images, so we make use of color images as another modality to improve the super-resolution network. By combining both modalities, we build a pipeline that learns to super-resolve using multi-scale spectral inputs guided by a color image. Finally, we validate our method and show that it is economic in terms of parameters and computation time, while still producing state-of-the-art results (Code at https://github.com/IVRL/Multi-Modal-Spectral-Image-Super-Resolution).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_3');
INSERT INTO `paper` VALUES (10979, 'Multi-object Tracking with Neural Gating Using Bilinear LSTM', 'Long Short-term Memory (LSTM)', 'Missing Detections', 'Recursive Least', 'Appearance Model', 'Proposed Tracker', 'In recent deep online and near-online multi-object tracking approaches, a difficulty has been to incorporate long-term appearance models to efficiently score object tracks under severe occlusion and multiple missing detections. In this paper, we propose a novel recurrent network model, the Bilinear LSTM, in order to improve the learning of long-term appearance models via a recurrent network. Based on intuitions drawn from recursive least squares, Bilinear LSTM stores building blocks of a linear predictor in its memory, which is then coupled with the input in a multiplicative manner, instead of the additive coupling in conventional LSTM approaches. Such coupling resembles an online learned classifier/regressor at each time step, which we have found to improve performances in using LSTM for appearance modeling. We also propose novel data augmentation approaches to efficiently train recurrent models that score object tracks on both appearance and motion. We train an LSTM that can score object tracks based on both appearance and motion and utilize it in a multiple hypothesis tracking framework. In experiments, we show that with our novel LSTM model, we achieved state-of-the-art performance on near-online multiple object tracking on the MOT 2016 and MOT 2017 benchmarks.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_13');
INSERT INTO `paper` VALUES (10980, 'Multi-person Pose Estimation for Pose Tracking with Enhanced Cascaded Pyramid Network', 'Pose estimation', 'Pose tracking', '', '', '', 'Multi-person pose estimation is a fundamental yet challenging task in machine learning. In parallel, recent development of pose estimation has increased interests on pose tracking in recent years. In this work, we propose an efficient and powerful method to locate and track human pose. Our proposed method builds upon the state-of-the-art single person pose estimation system (Cascaded Pyramid Network), and adopts the IOU-tracker module to identify the people in the wild. We conduct experiments on the released multi-person video pose estimation benchmark (PoseTrack2018) to validate the effectiveness of our network. Our model achieves an accuracy of 80.9% on the validation and 77.1% on the test set using the Mean Average Precision (MAP) metric, an accuracy of 64.0% on the validation and 57.4% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_19');
INSERT INTO `paper` VALUES (10981, 'Multi-scale Context Intertwining for Semantic Segmentation', 'Semantic segmentation', 'Deep learning', 'Convolutional neural network', 'Long short-term memory', '', 'Accurate semantic image segmentation requires the joint consideration of local appearance, semantic information, and global scene context. In today’s age of pre-trained deep networks and their powerful convolutional features, state-of-the-art semantic segmentation approaches differ mostly in how they choose to combine together these different kinds of information. In this work, we propose a novel scheme for aggregating features from different scales, which we refer to as Multi-Scale Context Intertwining (MSCI). In contrast to previous approaches, which typically propagate information between scales in a one-directional manner, we merge pairs of feature maps in a bidirectional and recurrent fashion, via connections between two LSTM chains. By training the parameters of the LSTM units on the segmentation task, the above approach learns how to extract powerful and effective features for pixel-level semantic segmentation, which are then combined hierarchically. Furthermore, rather than using fixed information propagation routes, we subdivide images into super-pixels, and use the spatial relationship between them in order to perform image-adapted context aggregation. Our extensive evaluation on public benchmarks indicates that all of the aforementioned components of our approach increase the effectiveness of information propagation throughout the network, and significantly improve its eventual segmentation accuracy.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_37');
INSERT INTO `paper` VALUES (10982, 'Multi-scale Residual Network for Image Super-Resolution', 'Super-resolution', 'Convolutional neural network', 'Multi-scale residual network', '', '', 'Recent studies have shown that deep neural networks can significantly improve the quality of single-image super-resolution. Current researches tend to use deeper convolutional neural networks to enhance performance. However, blindly increasing the depth of the network cannot ameliorate the network effectively. Worse still, with the depth of the network increases, more problems occurred in the training process and more training tricks are needed. In this paper, we propose a novel multi-scale residual network (MSRN) to fully exploit the image features, which outperform most of the state-of-the-art methods. Based on the residual block, we introduce convolution kernels of different sizes to adaptively detect the image features in different scales. Meanwhile, we let these features interact with each other to get the most efficacious image information, we call this structure Multi-scale Residual Block (MSRB). Furthermore, the outputs of each MSRB are used as the hierarchical features for global feature fusion. Finally, all these features are sent to the reconstruction module for recovering the high-quality image.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_32');
INSERT INTO `paper` VALUES (10983, 'Multi-scale Spatially-Asymmetric Recalibration for Image Classification', 'Large-scale image classification', 'Convolutional Neural Networks', 'Multi-Scale Spatially Asymmetric Recalibration', '', '', 'Convolution is spatially-symmetric, i.e., the visual features are independent of its position in the image, which limits its ability to utilize contextual cues for visual recognition. This paper addresses this issue by introducing a recalibration process, which refers to the surrounding region of each neuron, computes an importance value and multiplies it to the original neural response. Our approach is named multi-scale spatially-asymmetric recalibration (MS-SAR), which extracts visual cues from surrounding regions at multiple scales, and designs a weighting scheme which is asymmetric in the spatial domain. MS-SAR is implemented in an efficient way, so that only small fractions of extra parameters and computations are required. We apply MS-SAR to several popular building blocks, including the residual block and the densely-connected block, and demonstrate its superior performance in both CIFAR and ILSVRC2012 classification tasks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_31');
INSERT INTO `paper` VALUES (10984, 'Multi-Scale Structure-Aware Network for Human Pose Estimation', 'Human pose estimation', 'Conv-deconv network', 'Multi-scale supervision', '', '', 'We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_44');
INSERT INTO `paper` VALUES (10985, 'Multi-style Generative Network for Real-Time Transfer', '', '', '', '', '', 'Despite the rapid progress in style transfer, existing approaches using feed-forward generative network for multi-style or arbitrary-style transfer are usually compromised of image quality and model flexibility. We find it is fundamentally difficult to achieve comprehensive style modeling using 1-dimensional style embedding. Motivated by this, we introduce CoMatch Layer that learns to match the second order feature statistics with the target styles. With the CoMatch Layer, we build a Multi-style Generative Network (MSG-Net), which achieves real-time performance. In addition, we employ an specific strategy of upsampled convolution which avoids checkerboard artifacts caused by fractionally-strided convolution. Our method has achieved superior image quality comparing to state-of-the-art approaches. The proposed MSG-Net as a general approach for real-time style transfer is compatible with most existing techniques including content-style interpolation, color-preserving, spatial control and brush stroke size control. MSG-Net is the first to achieve real-time brush-size control in a purely feed-forward manner for style transfer. Our implementations and pre-trained models for Torch, PyTorch and MXNet frameworks will be publicly available (Links can be found at http://hangzhang.org/).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_32');
INSERT INTO `paper` VALUES (10986, 'Multi-view to Novel View: Synthesizing Novel Views With Self-learned Confidence', 'Novel view synthesis', 'Multi-view novel view synthesis', '', '', '', 'In this paper, we address the task of multi-view novel view synthesis, where we are interested in synthesizing a target image with an arbitrary camera pose from given source images. We propose an end-to-end trainable framework that learns to exploit multiple viewpoints to synthesize a novel view without any 3D supervision. Specifically, our model consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. To merge the predictions produced by the two modules given multi-view source images, we introduce a self-learned confidence aggregation mechanism. We evaluate our model on images rendered from 3D object models as well as real and synthesized scenes. We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_10');
INSERT INTO `paper` VALUES (10987, 'Multichannel Semantic Segmentation with Unsupervised Domain Adaptation', 'Semantic segmentation', 'Domain adaptation', 'RGB-depth', 'Multi-task learning', '', 'Most contemporary robots have depth sensors, and research on semantic segmentation with RGBD images has shown that depth images boost the accuracy of segmentation. Since it is time-consuming to annotate images with semantic labels per pixel, it would be ideal if we could avoid this laborious work by utilizing an existing dataset or a synthetic dataset which we can generate on our own. Robot motions are often tested in a synthetic environment, where multichannel (e.g., RGB + depth + instance boundary) images plus their pixel-level semantic labels are available. However, models trained simply on synthetic images tend to demonstrate poor performance on real images. In order to address this, we propose two approaches that can efficiently exploit multichannel inputs combined with an unsupervised domain adaptation (UDA) algorithm. One is a fusion-based approach that uses depth images as inputs. The other is a multitask learning approach that uses depth images as outputs. We demonstrated that the segmentation results were improved by using a multitask learning approach with a post-process and created a benchmark for this task.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_37');
INSERT INTO `paper` VALUES (10988, 'Multimodal Dual Attention Memory for Video Story Question Answering', 'Video story QA', 'Visual QA', 'Attention mechanism', 'Multimodal learning', 'Deep learning', 'We propose a video story question-answering (QA) architecture, Multimodal Dual Attention Memory (MDAM). The key idea is to use a dual attention mechanism with late fusion. MDAM uses self-attention to learn the latent concepts in scene frames and captions. Given a question, MDAM uses the second attention over these latent concepts. Multimodal fusion is performed after the dual attention processes (late fusion). Using this processing pipeline, MDAM learns to infer a high-level vision-language joint representation from an abstraction of the full video content. We evaluate MDAM on PororoQA and MovieQA datasets which have large-scale QA annotations on cartoon videos and movies, respectively. For both datasets, MDAM achieves new state-of-the-art results with significant margins compared to the runner-up models. We confirm the best performance of the dual attention mechanism combined with late fusion by ablation studies. We also perform qualitative analysis by visualizing the inference mechanisms of MDAM.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_41');
INSERT INTO `paper` VALUES (10989, 'Multimodal Image Alignment Through a Multiscale Chain of Neural Networks with Application to Remote Sensing', 'Multimodal', 'Alignment', 'Registration', 'Remote sensing', '', 'We tackle here the problem of multimodal image non-rigid registration, which is of prime importance in remote sensing and medical imaging. The difficulties encountered by classical registration approaches include feature design and slow optimization by gradient descent. By analyzing these methods, we note the significance of the notion of scale. We design easy-to-train, fully-convolutional neural networks able to learn scale-specific features. Once chained appropriately, they perform global registration in linear time, getting rid of gradient descent schemes by predicting directly the deformation. We show their performance in terms of quality and speed through various tasks of remote sensing multimodal image alignment. In particular, we are able to register correctly cadastral maps of buildings as well as road polylines onto RGB images, and outperform current keypoint matching methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_40');
INSERT INTO `paper` VALUES (10990, 'Multimodal Unsupervised Image-to-Image Translation', 'GANs', 'Image-to-image translation', 'Style transfer', '', '', 'Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any examples of corresponding image pairs. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image \\(\\text{ Translation } \\text{(MUNIT) }\\) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to state-of-the-art approaches further demonstrate the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_11');
INSERT INTO `paper` VALUES (10991, 'Multiple Connected Residual Network for Image Enhancement on Smartphones', 'Image enhancement', 'Generator', 'Residual Network', 'Multiple connections', 'Perceptual quality', 'Image enhancement on smartphones needs rapid processing speed with comparable performance. Recently, convolutional neural networks (CNNs) have achieved outstanding performance in image processing tasks such as image super-resolution and enhancement. In this paper, we propose a lightweight generator for image enhancement based on CNN to keep a balance between quality and speed, called multi-connected residual network (MCRN). The proposed network consists of one discriminator and one generator. The generator is a two-stage network: (1) The first stage extracts structural features; (2) the second stage focuses on enhancing perceptual visual quality. By utilizing the style of multiple connections, we achieve good performance in image enhancement while making our network converge fast. Experimental results demonstrate that the proposed method outperforms the state-of-the-art approaches in terms of the perceptual quality and runtime. The code is available at https://github.com/JieLiu95/MCRN.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_12');
INSERT INTO `paper` VALUES (10992, 'Multiple Context Features in Siamese Networks for Visual Object Tracking', 'Object tracking', 'Siamese network', 'ResNet', '', '', 'Siamese networks have been successfully utilized to learn a robust matching function between pairs of images. Visual object tracking methods based on siamese networks have been gaining popularity recently due to their robustness and speed. However, existing siamese approaches are still unable to perform on par with the most accurate trackers. In this paper, we propose to extend the SiamFC tracker [1] to extract features at multiple context and semantic levels from very deep networks. We show that our approach effectively extracts complementary features for siamese matching from different layers, which provides a significant performance boost when fused. Experimental results on VOT and OTB datasets show that our multi-context tracker is comparable to the most accurate methods, while still being faster than most of them. In particular, we outperform several other state-of-the-art siamese methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_6');
INSERT INTO `paper` VALUES (10993, 'Multiple Wavelet Pooling for CNNs', 'Wavelet', 'CNN', 'Pooling functions', 'Object recognition', '', 'Pooling layers are an essential part of any Convolutional Neural Network. The most popular pooling methods, as max pooling or average pooling, are based on a neighborhood approach that can be too simple and easily introduce visual distortion. To tackle these problems, recently a pooling method based on Haar wavelet transform was proposed. Following the same line of research, in this work, we explore the use of more sophisticated wavelet transforms (Coiflet, Daubechies) to perform the pooling. Additionally, considering that wavelets work similarly to filters, we propose a new pooling method for Convolutional Neural Network that combines multiple wavelet transforms. The results achieved demonstrate the benefits of our approach, improving the performance on different public object recognition datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_55');
INSERT INTO `paper` VALUES (10994, 'Multiple-Gaze Geometry: Inferring Novel 3D Locations from Gazes Observed in Monocular Video', '3D temporal scene understanding', '3D gaze estimation', 'Monocular video', 'Discovering objects', 'MCMC', 'We develop using person gaze direction for scene understanding. In particular, we use intersecting gazes to learn 3D locations that people tend to look at, which is analogous to having multiple camera views. The 3D locations that we discover need not be visible to the camera. Conversely, knowing 3D locations of scene elements that draw visual attention, such as other people in the scene, can help infer gaze direction. We provide a Bayesian generative model for the temporal scene that captures the joint probability of camera parameters, locations of people, their gaze, what they are looking at, and locations of visual attention. Both the number of people in the scene and the number of extra objects that draw attention are unknown and need to be inferred. To execute this joint inference we use a probabilistic data association approach that enables principled comparison of model hypotheses. We use MCMC for inference over the discrete correspondence variables, and approximate the marginalization over continuous parameters using the Metropolis-Laplace approximation, using Hamiltonian (Hybrid) Monte Carlo for maximization. As existing data sets do not provide the 3D locations of what people are looking at, we contribute a small data set that does. On this data set, we infer what people are looking at with 59% precision compared with 13% for a baseline approach, and where those objects are within about 0.58 m.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_38');
INSERT INTO `paper` VALUES (10995, 'MultiPoseNet: Fast Multi-Person Pose Estimation Using Pose Residual Network', 'Multi-task learning', 'Multi-person pose estimation', 'Semantic segmentation', 'MultiPoseNet', 'Pose residual network', 'In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with \\(\\sim 23\\) frames/sec.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_26');
INSERT INTO `paper` VALUES (10996, 'Multiresolution Tree Networks for 3D Point Cloud Processing', 'Point Cloud', 'Variational Autoencoder (VAE)', 'Shape Classification', 'Spatial Sorting', 'Chamfer Distance (CD)', 'We present multiresolution tree-structured networks to process point clouds for 3D shape understanding and generation tasks. Our network represents a 3D shape as a set of locality-preserving 1D ordered list of points at multiple resolutions. This allows efficient feed-forward processing through 1D convolutions, coarse-to-fine analysis through a multi-grid architecture, and it leads to faster convergence and small memory footprint during training. The proposed tree-structured encoders can be used to classify shapes and outperform existing point-based architectures on shape classification benchmarks, while tree-structured decoders can be used for generating point clouds directly and they outperform existing approaches for image-to-shape inference tasks learned using the ShapeNet dataset. Our model also allows unsupervised learning of point-cloud based shapes by using a variational autoencoder, leading to higher-quality generated shapes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_7');
INSERT INTO `paper` VALUES (10997, 'Multi–scale Recursive and Perception–Distortion Controllable Image Super–Resolution', 'Backprojection', 'Multigrid', 'Perceptual quality', '', '', 'We describe our solution for the PIRM Super–Resolution Challenge 2018 where we achieved the \\(\\varvec{2^{nd}}\\) best perceptual quality for average \\(RMSE\\leqslant 16\\), \\(5^{th}\\) best for \\(RMSE\\leqslant 12.5\\), and \\(7^{th}\\) best for \\(RMSE\\leqslant 11.5\\). We modify a recently proposed Multi–Grid Back–Projection (MGBP) architecture to work as a generative system with an input parameter that can control the amount of artificial details in the output. We propose a discriminator for adversarial training with the following novel properties: it is multi–scale that resembles a progressive–GAN; it is recursive that balances the architecture of the generator; and it includes a new layer to capture significant statistics of natural images. Finally, we propose a training strategy that avoids conflicts between reconstruction and perceptual losses. Our configuration uses only 281 k parameters and upscales each image of the competition in 0.2 s in average.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_1');
INSERT INTO `paper` VALUES (10998, 'Museum Exhibit Identification Challenge for the Supervised Domain Adaptation and Beyond', 'Domain Adaptation', 'Official Dataset', 'Wearable Camera', 'Convolutional Neural Network (CNN)', 'Scatter Matrices', 'We study an open problem of artwork identification and propose a new dataset dubbed Open Museum Identification Challenge (Open MIC). It contains photos of exhibits captured in 10 distinct exhibition spaces of several museums which showcase paintings, timepieces, sculptures, glassware, relics, science exhibits , natural history pieces, ceramics, pottery, tools and indigenosus crafts. The goal of Open MIC is to stimulate research in domain adaptation, egocentric recognition and few-shot learning by providing a testbed complementary to the famous Office dataset which reaches \\(\\sim \\)90% accuracy. To form our dataset, we captured a number of images per art piece with a mobile phone and wearable cameras to form the source and target data splits, respectively. To achieve robust baselines, we build on a recent approach that aligns per-class scatter matrices of the source and target CNN streams. Moreover, we exploit the positive definite nature of such representations by using end-to-end Bregman divergences and the Riemannian metric. We present baselines such as training/evaluation per exhibition and training/evaluation on the combined set covering 866 exhibit identities. As each exhibition poses distinct challenges e.g., quality of lighting, motion blur, occlusions, clutter, viewpoint and scale variations, rotations, glares, transparency, non-planarity, clipping, we break down results w.r.t. these factors.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_48');
INSERT INTO `paper` VALUES (10999, 'Mutual Learning to Adapt for Joint Human Parsing and Pose Estimation', 'Human pose estimation', 'Human parsing', 'Mutual learning', '', '', 'This paper presents a novel Mutual Learning to Adapt model (MuLA) for joint human parsing and pose estimation. It effectively exploits mutual benefits from both tasks and simultaneously boosts their performance. Different from existing post-processing or multi-task learning based methods, MuLA predicts dynamic task-specific model parameters via recurrently leveraging guidance information from its parallel tasks. Thus MuLA can fast adapt parsing and pose models to provide more powerful representations by incorporating information from their counterparts, giving more robust and accurate results. MuLA is implemented with convolutional neural networks and end-to-end trainable. Comprehensive experiments on benchmarks LIP and extended PASCAL-Person-Part demonstrate the effectiveness of the proposed MuLA model with superior performance to well established baselines.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_31');
INSERT INTO `paper` VALUES (11000, 'MVSNet: Depth Inference for Unstructured Multi-view Stereo', 'Multi-view stereo', 'Depth map', 'Deep learning', '', '', 'We present an end-to-end deep learning architecture for depth map inference from multi-view images. In the network, we first extract deep visual image features, and then build the 3D cost volume upon the reference camera frustum via the differentiable homography warping. Next, we apply 3D convolutions to regularize and regress the initial depth map, which is then refined with the reference image to generate the final output. Our framework flexibly adapts arbitrary N-view inputs using a variance-based cost metric that maps multiple features into one cost feature. The proposed MVSNet is demonstrated on the large-scale indoor DTU dataset. With simple post-processing, our method not only significantly outperforms previous state-of-the-arts, but also is several times faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks and Temples dataset, where our method ranks first before April 18, 2018 without any fine-tuning, showing the strong generalization ability of MVSNet.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_47');
INSERT INTO `paper` VALUES (11001, 'MVTec D2S: Densely Segmented Supermarket Dataset', 'Instance segmentation dataset', 'Industrial application', '', '', '', 'We introduce the Densely Segmented Supermarket (D2S) dataset, a novel benchmark for instance-aware semantic segmentation in an industrial domain. It contains 21 000 high-resolution images with pixel-wise labels of all object instances. The objects comprise groceries and everyday products from 60 categories. The benchmark is designed such that it resembles the real-world setting of an automatic checkout, inventory, or warehouse system. The training images only contain objects of a single class on a homogeneous background, while the validation and test sets are much more complex and diverse. To further benchmark the robustness of instance segmentation methods, the scenes are acquired with different lightings, rotations, and backgrounds. We ensure that there are no ambiguities in the labels and that every instance is labeled comprehensively. The annotations are pixel-precise and allow using crops of single instances for articial data augmentation. The dataset covers several challenges highly relevant in the field, such as a limited amount of training data and a high diversity in the test and validation sets. The evaluation of state-of-the-art object detection and instance segmentation methods on D2S reveals significant room for improvement.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_35');
INSERT INTO `paper` VALUES (11002, 'NAM: Non-Adversarial Unsupervised Domain Mapping', '', '', '', '', '', 'Several methods were recently proposed for the task of translating images between domains without prior knowledge in the form of correspondences. The existing methods apply adversarial learning to ensure that the distribution of the mapped source domain is indistinguishable from the target domain, which suffers from known stability issues. In addition, most methods rely heavily on “cycle” relationships between the domains, which enforce a one-to-one mapping. In this work, we introduce an alternative method: Non-Adversarial Mapping (NAM), which separates the task of target domain generative modeling from the cross-domain mapping task. NAM relies on a pre-trained generative model of the target domain, and aligns each source image with an image synthesized from the target domain, while jointly optimizing the domain mapping function. It has several key advantages: higher quality and resolution image translations, simpler and more stable training and reusable target models. Extensive experiments are presented validating the advantages of our method.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_27');
INSERT INTO `paper` VALUES (11003, 'Navigational Affordance Cortical Responses Explained by Scene-Parsing Model', 'Deep Neural Networks', 'Representational similarity analysis', 'Occipital Place Area', 'Neural Encoding', '', 'Deep Neural Networks (DNNs) are the leading models for explaining the population responses of neurons in the visual cortex. Recent studies show that responses of some task-specific brain regions can also be explained by a DNN trained for classification. In this work, we propose that responses of task-specific brain regions are better explained by DNNs trained on a similar task. We first show that responses of scene selective visual areas like parahippocampal place area (PPA) and Occipital Place Area (OPA) are better explained by a DNN trained for scene classification than one trained for object classification. Next, we consider a particular case of OPA which has been shown to encode navigational affordances. We argue that a scene parsing task, which predicts the class of each pixel in the scene is more related to navigational affordances than scene classification. Our results show that the responses in OPA are better explained by the scene parsing model than the scene classification model.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_39');
INSERT INTO `paper` VALUES (11004, 'NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications', 'Mobile CPU', 'Direct Metal', 'Indirect Metrics', 'Pre-trained Network', 'MorphNet', 'This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7\\(\\times \\) speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_18');
INSERT INTO `paper` VALUES (11005, 'Neural Graph Matching Networks for Fewshot 3D Action Recognition', '', '', '', '', '', 'We propose Neural Graph Matching (NGM) Networks, a novel framework that can learn to recognize a previous unseen 3D action class with only a few examples. We achieve this by leveraging the inherent structure of 3D data through a graphical representation. This allows us to modularize our model and lead to strong data-efficiency in few-shot learning. More specifically, NGM Networks jointly learn a graph generator and a graph matching metric function in an end-to-end fashion to directly optimize the few-shot learning objective. We evaluate NGM on two 3D action recognition datasets, CAD-120 and PiGraphs, and show that learning to generate and match graphs both lead to significant improvement of few-shot 3D action recognition over the holistic baselines.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_40');
INSERT INTO `paper` VALUES (11006, 'Neural Network Encapsulation', 'Network architecture design', 'Capsule feature learning', '', '', '', 'A capsule is a collection of neurons which represents different variants of a pattern in the network. The routing scheme ensures only certain capsules which resemble lower counterparts in the higher layer should be activated. However, the computational complexity becomes a bottleneck for scaling up to larger networks, as lower capsules need to correspond to each and every higher capsule. To resolve this limitation, we approximate the routing process with two branches: a master branch which collects primary information from its direct contact in the lower layer and an aide branch that replenishes master based on pattern variants encoded in other lower capsules. Compared with previous iterative and unsupervised routing scheme, these two branches are communicated in a fast, supervised and one-time pass fashion. The complexity and runtime of the model are therefore decreased by a large margin. Motivated by the routing to make higher capsule have agreement with lower capsule, we extend the mechanism as a compensation for the rapid loss of information in nearby layers. We devise a feedback agreement unit to send back higher capsules as feedback. It could be regarded as an additional regularization to the network. The feedback agreement is achieved by comparing the optimal transport divergence between two distributions (lower and higher capsules). Such an add-on witnesses a unanimous gain in both capsule and vanilla networks. Our proposed EncapNet performs favorably better against previous state-of-the-arts on CIFAR10/100, SVHN and a subset of ImageNet.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_16');
INSERT INTO `paper` VALUES (11007, 'Neural Procedural Reconstruction for Residential Buildings', '3D reconstruction', 'CAD', 'Deep learning', 'Procedural modeling', '', 'This paper proposes a novel 3D reconstruction approach, dubbed Neural Procedural Reconstruction (NPR). NPR infers a sequence of shape grammar rule applications and reconstructs CAD-quality models with procedural structure from 3D points. While most existing methods rely on low-level geometry analysis to extract primitive structures, our approach conducts global analysis of entire building structures by deep neural networks (DNNs), enabling the reconstruction even from incomplete and sparse input data. We demonstrate the proposed system for residential buildings with aerial LiDAR as the input. Our 3D models boast compact geometry and semantically segmented architectural components. Qualitative and quantitative evaluations on hundreds of houses demonstrate that the proposed approach makes significant improvements over the existing state-of-the-art.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_45');
INSERT INTO `paper` VALUES (11008, 'Neural Stereoscopic Image Style Transfer', 'Neural style transfer', 'Stereoscopic image', '', '', '', 'Neural style transfer is an emerging technique which is able to endow daily-life images with attractive artistic styles. Previous work has succeeded in applying convolutional neural networks (CNNs) to style transfer for monocular images or videos. However, style transfer for stereoscopic images is still a missing piece. Different from processing a monocular image, the two views of a stylized stereoscopic pair are required to be consistent to provide observers a comfortable visual experience. In this paper, we propose a novel dual path network for view-consistent style transfer on stereoscopic images. While each view of the stereoscopic pair is processed in an individual path, a novel feature aggregation strategy is proposed to effectively share information between the two paths. Besides a traditional perceptual loss being used for controlling the style transfer quality in each view, a multi-layer view loss is leveraged to enforce the network to coordinate the learning of both the paths to generate view-consistent stylized results. Extensive experiments show that, compared against previous methods, our proposed model can produce stylized stereoscopic images which achieve decent view consistency.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_4');
INSERT INTO `paper` VALUES (11009, 'NeXtVLAD: An Efficient Neural Network to Aggregate Frame-Level Features for Large-Scale Video Classification', 'Neural network', 'VLAD', 'Video classification', 'Youtube8M', '', 'This paper introduces a fast and efficient network architecture, NeXtVLAD, to aggregate frame-level features into a compact feature vector for large-scale video classification. Briefly speaking, the basic idea is to decompose a high-dimensional feature into a group of relatively low-dimensional vectors with attention before applying NetVLAD aggregation over time. This NeXtVLAD approach turns out to be both effective and parameter efficient in aggregating temporal information. In the 2nd Youtube-8M video understanding challenge, a single NeXtVLAD model with less than 80M parameters achieves a GAP score of 0.87846 in private leaderboard. A mixture of 3 NeXtVLAD models results in 0.88722, which is ranked 3rd over 394 teams. The code is publicly available at https://github.com/linrongc/youtube-8m.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_19');
INSERT INTO `paper` VALUES (11010, 'NNEval: Neural Network Based Evaluation Metric for Image Captioning', 'Image captioning', 'Automatic evaluation metric', 'Neural networks', 'Correlation', 'Accuracy', 'The automatic evaluation of image descriptions is an intricate task, and it is highly important in the development and fine-grained analysis of captioning systems. Existing metrics to automatically evaluate image captioning systems fail to achieve a satisfactory level of correlation with human judgements at the sentence level. Moreover, these metrics, unlike humans, tend to focus on specific aspects of quality, such as the n-gram overlap or the semantic meaning. In this paper, we present the first learning-based metric to evaluate image captions. Our proposed framework enables us to incorporate both lexical and semantic information into a single learned metric. This results in an evaluator that takes into account various linguistic features to assess the caption quality. The experiments we performed to assess the proposed metric, show improvements upon the state of the art in terms of correlation with human judgements and demonstrate its superior robustness to distractions.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_3');
INSERT INTO `paper` VALUES (11011, 'Non-local NetVLAD Encoding for Video Classification', '', '', '', '', '', 'This paper describes our solution for the 2\\(^\\text {nd}\\) YouTube-8M video understanding challenge organized by Google AI. Unlike the video recognition benchmarks, such as Kinetics and Moments, the YouTube-8M challenge provides pre-extracted visual and audio features instead of raw videos. In this challenge, the submitted model is restricted to 1 GB, which encourages participants focus on constructing one powerful single model rather than incorporating of the results from a bunch of models. Our system fuses six different sub-models into one single computational graph, which are categorized into three families. More specifically, the most effective family is the model with non-local operations following the NetVLAD encoding. The other two family models are Soft-BoF and GRU, respectively. In order to further boost single models performance, the model parameters of different checkpoints are averaged. Experimental results demonstrate that our proposed system can effectively perform the video classification task, achieving 0.88763 on the public test set and 0.88704 on the private set in terms of GAP@20, respectively. We finally ranked at the fourth place in the YouTube-8M video understanding challenge.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_20');
INSERT INTO `paper` VALUES (11012, 'Non-rigid 3D Shape Registration Using an Adaptive Template', '3D registration', '3D shape morphing', '3D morphable models', '', '', 'We present a new fully-automatic non-rigid 3D shape registration (morphing) framework comprising (1) a new 3D landmarking and pose normalisation method; (2) an adaptive shape template method to improve the convergence of registration algorithms and achieve a better final shape correspondence and (3) a new iterative registration method that combines Iterative Closest Points with Coherent Point Drift (CPD) to achieve a more stable and accurate correspondence establishment than standard CPD. We call this new morphing approach Iterative Coherent Point Drift (ICPD). Our proposed framework is evaluated qualitatively and quantitatively on three datasets: Headspace, BU3D and a synthetic LSFM dataset, and is compared with several other methods. The proposed framework is shown to give state-of-the-art performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_5');
INSERT INTO `paper` VALUES (11013, 'Normalized Blind Deconvolution', '', '', '', '', '', 'We introduce a family of novel approaches to single-image blind deconvolution, i.e., the problem of recovering a sharp image and a blur kernel from a single blurry input. This problem is highly ill-posed, because infinite (image, blur) pairs produce the same blurry image. Most research effort has been devoted to the design of priors for natural images and blur kernels, which can drastically prune the set of possible solutions. Unfortunately, these priors are usually not sufficient to favor the sharp solution. In this paper we address this issue by looking at a much less studied aspect: the relative scale ambiguity between the sharp image and the blur. Most prior work eliminates this ambiguity by fixing the \\(L^1\\) norm of the blur kernel. In principle, however, this choice is arbitrary. We show that a careful design of the blur normalization yields a blind deconvolution formulation with remarkable accuracy and robustness to noise. Specifically, we show that using the Frobenius norm to fix the scale ambiguity enables convex image priors, such as the total variation, to achieve state-of-the-art results on both synthetic and real datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_41');
INSERT INTO `paper` VALUES (11014, 'Object Detection at 200 Frames per Second', '', '', '', '', '', 'In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture, loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally light-weight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed light-weight student network. We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_41');
INSERT INTO `paper` VALUES (11015, 'Object Detection in Video with Spatiotemporal Sampling Networks', '', '', '', '', '', 'We propose a Spatiotemporal Sampling Network (STSN) that uses deformable convolutions across time for object detection in videos. Our STSN performs object detection in a video frame by learning to spatially sample features from the adjacent frames. This naturally renders the approach robust to occlusion or motion blur in individual frames. Our framework does not require additional supervision, as it optimizes sampling locations directly with respect to object detection performance. Our STSN outperforms the state-of-the-art on the ImageNet VID dataset and compared to prior video object detection methods it uses a simpler design, and does not require optical flow data for training.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_21');
INSERT INTO `paper` VALUES (11016, 'Object Level Visual Reasoning in Videos', 'Video understanding', 'Human-object interaction', '', '', '', 'Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatio-temporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_7');
INSERT INTO `paper` VALUES (11017, 'Object Pose Estimation from Monocular Image Using Multi-view Keypoint Correspondence', 'Pose estimation', '3D structure', 'Keypoint estimation', 'Correspondence network', 'Convolutional neural network', 'Understanding the geometry and pose of objects in 2D images is a fundamental necessity for a wide range of real world applications. Driven by deep neural networks, recent methods have brought significant improvements to object pose estimation. However, they suffer due to scarcity of keypoint/pose-annotated real images and hence can not exploit the object’s 3D structural information effectively. In this work, we propose a data-efficient method which utilizes the geometric regularity of intraclass objects for pose estimation. First, we learn pose-invariant local descriptors of object parts from simple 2D RGB images. These descriptors, along with keypoints obtained from renders of a fixed 3D template model are then used to generate keypoint correspondence maps for a given monocular real image. Finally, a pose estimation network predicts 3D pose of the object using these correspondence maps. This pipeline is further extended to a multi-view approach, which assimilates keypoint information from correspondence sets generated from multiple views of the 3D template model. Fusion of multi-view information significantly improves geometric comprehension of the system which in turn enhances the pose estimation performance. Furthermore, use of correspondence framework responsible for the learning of pose invariant keypoint descriptor also allows us to effectively alleviate the data-scarcity problem. This enables our method to achieve state-of-the-art performance on multiple real-image viewpoint estimation datasets, such as Pascal3D+ and ObjectNet3D. To encourage reproducible research, we have released the codes for our proposed approach (Code: https://github.com/val-iisc/pose_estimation).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_23');
INSERT INTO `paper` VALUES (11018, 'Object-Centered Image Stitching', '', '', '', '', '', 'Image stitching is typically decomposed into three phases: registration, which aligns the source images with a common target image; seam finding, which determines for each target pixel the source image it should come from; and blending, which smooths transitions over the seams. As described in [1], the seam finding phase attempts to place seams between pixels where the transition between source images is not noticeable. Here, we observe that the most problematic failures of this approach occur when objects are cropped, omitted, or duplicated. We therefore take an object-centered approach to the problem, leveraging recent advances in object detection [2, 3, 4]. We penalize candidate solutions with this class of error by modifying the energy function used in the seam finding stage. This produces substantially more realistic stitching results on challenging imagery. In addition, these methods can be used to determine when there is non-recoverable occlusion in the input data, and also suggest a simple evaluation metric that can be used to evaluate the output of stitching algorithms.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_50');
INSERT INTO `paper` VALUES (11019, 'Objects that Sound', 'Cross-modal Retrieval', 'Visual Embedding', 'Unlabeled Video', 'Embedding Audio', 'Prevalence Cutoffs', 'In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of cross-modal self-supervision from video.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_27');
INSERT INTO `paper` VALUES (11020, 'Occlusion Resistant Object Rotation Regression from Point Cloud Segments', '6D pose estimation', 'Convolutional neural network', 'Point cloud', 'Lie algebra', '', 'Rotation estimation of known rigid objects is important for robotic applications such as dexterous manipulation. Most existing methods for rotation estimation use intermediate representations such as templates, global or local feature descriptors, or object coordinates, which require multiple steps in order to infer the object pose. We propose to directly regress a pose vector from point cloud segments using a convolutional neural network. Experimental results show that our method achieves competitive performance compared to a state-of-the-art method, while also showing more robustness against occlusion. Our method does not require any post processing such as refinement with the iterative closest point algorithm.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_44');
INSERT INTO `paper` VALUES (11021, 'Occlusion-Aware Hand Pose Estimation Using Hierarchical Mixture Density Network', '3D hand pose estimation', 'Occlusion', 'Multi-valued mapping', 'Convolutional Neural Network', 'Mixture density network', 'Learning and predicting the pose parameters of a 3D hand model given an image, such as locations of hand joints, is challenging due to large viewpoint changes and articulations, and severe self-occlusions exhibited particularly in egocentric views. Both feature learning and prediction modeling have been investigated to tackle the problem. Though effective, most existing discriminative methods yield a single deterministic estimation of target poses. Due to their single-value mapping intrinsic, they fail to adequately handle self-occlusion problems, where occluded joints present multiple modes. In this paper, we tackle the self-occlusion issue and provide a complete description of observed poses given an input depth image by a novel method called hierarchical mixture density networks (HMDN). The proposed method leverages the state-of-the-art hand pose estimators based on Convolutional Neural Networks to facilitate feature learning, while it models the multiple modes in a two-level hierarchy to reconcile single-valued and multi-valued mapping in its output. The whole framework with a mixture of two differentiable density functions is naturally end-to-end trainable. In the experiments, HMDN produces interpretable and diverse candidate samples, and significantly outperforms the state-of-the-art methods on two benchmarks with occlusions, and performs comparably on another benchmark free of occlusions.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_49');
INSERT INTO `paper` VALUES (11022, 'Occlusion-Aware R-CNN: Detecting Pedestrians in a Crowd', 'Pedestrian detection', 'Occlusion-aware', 'Convolutional network', 'Structure information', 'Visibility prediction', 'Pedestrian detection in crowded scenes is a challenging problem since the pedestrians often gather together and occlude each other. In this paper, we propose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy in the crowd. Specifically, we design a new aggregation loss to enforce proposals to be close and locate compactly to the corresponding objects. Meanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling unit to replace the RoI pooling layer in order to integrate the prior structure information of human body with visibility prediction into the network to handle occlusion. Our detector is trained in an end-to-end fashion, which achieves state-of-the-art results on three pedestrian detection datasets, i.e., CityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on Caltech.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_39');
INSERT INTO `paper` VALUES (11023, 'Occlusions, Motion and Depth Boundaries with a Generic Network for Disparity, Optical Flow or Scene Flow Estimation', '', '', '', '', '', 'Occlusions play an important role in disparity and optical flow estimation, since matching costs are not available in occluded areas and occlusions indicate depth or motion boundaries. Moreover, occlusions are relevant for motion segmentation and scene flow estimation. In this paper, we present an efficient learning-based approach to estimate occlusion areas jointly with disparities or optical flow. The estimated occlusions and motion boundaries clearly improve over the state-of-the-art. Moreover, we present networks with state-of-the-art performance on the popular KITTI benchmark and good generic performance. Making use of the estimated occlusions, we also show improved results on motion segmentation and scene flow estimation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_38');
INSERT INTO `paper` VALUES (11024, 'OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas', 'Omnidirectional media', '\\({360}^{\\circ }\\)', 'Spherical panorama', 'Scene understanding', 'Depth estimation', 'Recent work on depth estimation up to now has only focused on projective images ignoring \\({360}^{\\circ }\\) content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on \\({360}^{\\circ }\\) datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality \\({360}^{\\circ }\\) datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to \\({360}^{\\circ }\\) via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from \\({360}^{\\circ }\\) images. We show promising results in our synthesized data as well as in unseen realistic images.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_28');
INSERT INTO `paper` VALUES (11025, 'On Offline Evaluation of Vision-Based Driving Models', 'Autonomous driving', 'Deep learning', '', '', '', 'Autonomous driving models should ideally be evaluated by deploying them on a fleet of physical vehicles in the real world. Unfortunately, this approach is not practical for the vast majority of researchers. An attractive alternative is to evaluate models offline, on a pre-collected validation dataset with ground truth annotation. In this paper, we investigate the relation between various online and offline metrics for evaluation of autonomous driving models. We find that offline prediction error is not necessarily correlated with driving quality, and two models with identical prediction error can differ dramatically in their driving performance. We show that the correlation of offline evaluation with driving quality can be significantly improved by selecting an appropriate validation dataset and suitable offline metrics.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_15');
INSERT INTO `paper` VALUES (11026, 'On Pre-trained Image Features and Synthetic Images for Deep Learning', '', '', '', '', '', 'Deep Learning methods usually require huge amounts of training data to perform at their full potential, and often require expensive manual labeling. Using synthetic images is therefore very attractive to train object detectors, as the labeling comes for free, and several approaches have been proposed to combine synthetic and real images for training. In this paper, we evaluate if ‘freezing’ the layers responsible for feature extraction to generic layers pre-trained on real images, and training only the remaining layers with plain OpenGL rendering may allow for training with synthetic images only. Our experiments with very recent deep architectures for object recognition (Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet and Resnet) show this simple approach performs surprisingly well.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_42');
INSERT INTO `paper` VALUES (11027, 'On Regularized Losses for Weakly-supervised CNN Segmentation', 'Regularization', 'Semi-supervised Learning', 'CNN segmentation', '', '', 'Minimization of regularized losses is a principled approach to weak supervision well-established in deep learning, in general. However, it is largely overlooked in semantic segmentation currently dominated by methods mimicking full supervision via “fake” fully-labeled masks (proposals) generated from available partial input. To obtain such full masks the typical methods explicitly use standard regularization techniques for “shallow” segmentation, e.g. graph cuts or dense CRFs. In contrast, we integrate such standard regularizers directly into the loss functions over partial input. This approach simplifies weakly-supervised training by avoiding extra MRF/CRF inference steps or layers explicitly generating full masks, while improving both the quality and efficiency of training. This paper proposes and experimentally compares different losses integrating MRF/CRF regularization terms. We juxtapose our regularized losses with earlier proposal-generation methods. Our approach achieves state-of-the-art accuracy in semantic segmentation with near full-supervision quality.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_31');
INSERT INTO `paper` VALUES (11028, 'On the Optimization of Advanced DCF-Trackers', '', '', '', '', '', 'Trackers based on discriminative correlation filters (DCF) have recently seen widespread success and in this work we dive into their numerical core. DCF-based trackers interleave learning of the target detector and target state inference based on this detector. Whereas the original formulation includes a closed-form solution for the filter learning, recently introduced improvements to the framework no longer have known closed-form solutions. Instead a large-scale linear least squares problem must be solved each time the detector is updated. We analyze the procedure used to optimize the detector and let the popular scheme introduced with ECO serve as a baseline. The ECO implementation is revisited in detail and several mechanisms are provided with alternatives. With comprehensive experiments we show which configurations are superior in terms of tracking capabilities and optimization performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_2');
INSERT INTO `paper` VALUES (11029, 'On the Solvability of Viewing Graphs', 'Viewing graph', 'Fundamental matrix', '3D reconstruction', '', '', 'A set of fundamental matrices relating pairs of cameras in some configuration can be represented as edges of a “viewing graph”. Whether or not these fundamental matrices are generically sufficient to recover the global camera configuration depends on the structure of this graph. We study characterizations of “solvable” viewing graphs, and present several new results that can be applied to determine which pairs of views may be used to recover all camera parameters. We also discuss strategies for verifying the solvability of a graph computationally.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_20');
INSERT INTO `paper` VALUES (11030, 'Onboard Hyperspectral Image Compression Using Compressed Sensing and Deep Learning', 'Fast Hyperspectral imaging', 'Fast on-board data compression', 'Compressed sensing', 'Deep learning', '', 'We propose a real-time onboard compression scheme for hyperspectral datacube which consists of a very low complexity encoder and a deep learning based parallel decoder architecture for fast decompression. The encoder creates a set of coded snapshots from a given datacube using a measurement code matrix. The decoder decompresses the coded snapshots by using a sparse recovery algorithm. We solve this sparse recovery problem using a deep neural network for fast reconstruction. We present experimental results which demonstrate that our technique performs very well in terms of quality of reconstruction and in terms of computational requirements compared to other transform based techniques with some tradeoff in PSNR. The proposed technique also enables faster inference in compressed domain, suitable for on-board requirements.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_3');
INSERT INTO `paper` VALUES (11031, 'Online Detection of Action Start in Untrimmed, Streaming Videos', 'Online detection', 'Action start', 'Generative Adversarial Network', 'Evaluation protocol', '', 'We aim to tackle a novel task in action detection - Online Detection of Action Start (ODAS) in untrimmed, streaming videos. The goal of ODAS is to detect the start of an action instance, with high categorization accuracy and low detection latency. ODAS is important in many applications such as early alert generation to allow timely security or emergency response. We propose three novel methods to specifically address the challenges in training ODAS models: (1) hard negative samples generation based on Generative Adversarial Network (GAN) to distinguish ambiguous background, (2) explicitly modeling the temporal consistency between data around action start and data succeeding action start, and (3) adaptive sampling strategy to handle the scarcity of training data. We conduct extensive experiments using THUMOS’14 and ActivityNet. We show that our proposed methods lead to significant performance gains and improve the state-of-the-art methods. An ablation study confirms the effectiveness of each proposed method.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_33');
INSERT INTO `paper` VALUES (11032, 'Online Dictionary Learning for Approximate Archetypal Analysis', 'Archetypal analysis', 'Convex hull', 'Sparsity', '', '', 'Archetypal analysis is an unsupervised learning approach which represents data by convex combinations of a set of archetypes. The archetypes generally correspond to the extremal points in the dataset and are learned by requiring them to be convex combinations of the training data. In spite of its nice property of interpretability, the method is slow. We propose a variant of archetypal analysis which scales gracefully to large datasets. The core idea is to decouple the binding between data and archetypes and require them to be unit normalized. Geometrically, the method learns a convex hull inside the unit sphere and represents the data by their projections on the closest surfaces of the convex hull. By minimizing the representation error, the method pushes the convex hull surfaces close to the regions of the sphere where the data reside. The vertices of the convex hull are the learned archetypes. We apply the method to human faces and poses to validate its effectiveness in the context of reconstructions and classifications.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_30');
INSERT INTO `paper` VALUES (11033, 'Online Multi-Object Tracking with Dual Matching Attention Networks', 'Multi-Object Tracking', 'Cost-sensitive tracking loss', 'Dual Matching Attention Network', '', '', 'In this paper, we propose an online Multi-Object Tracking (MOT) approach which integrates the merits of single object tracking and data association methods in a unified framework to handle noisy detections and frequent interactions between targets. Specifically, for applying single object tracking in MOT, we introduce a cost-sensitive tracking loss based on the state-of-the-art visual tracker, which encourages the model to focus on hard negative distractors during online learning. For data association, we propose Dual Matching Attention Networks (DMAN) with both spatial and temporal attention mechanisms. The spatial attention module generates dual attention maps which enable the network to focus on the matching patterns of the input image pair, while the temporal attention module adaptively allocates different levels of attention to different samples in the tracklet to suppress noisy observations. Experimental results on the MOT benchmark datasets show that the proposed algorithm performs favorably against both online and offline trackers in terms of identity-preserving metrics.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_23');
INSERT INTO `paper` VALUES (11034, 'Open Set Domain Adaptation by Backpropagation', 'Domain adaptation', 'Open set recognition', 'Adversarial learning', '', '', 'Numerous algorithms have been proposed for transferring knowledge from a label-rich domain (source) to a label-scarce domain (target). Most of them are proposed for closed-set scenario, where the source and the target domain completely share the class of their samples. However, in practice, a target domain can contain samples of classes that are not shared by the source domain. We call such classes the “unknown class” and algorithms that work well in the open set situation are very practical. However, most existing distribution matching methods for domain adaptation do not work well in this setting because unknown target samples should not be aligned with the source. In this paper, we propose a method for an open set domain adaptation scenario, which utilizes adversarial training. This approach allows to extract features that separate unknown target from known target samples. During training, we assign two options to the feature generator: aligning target samples with source known ones or rejecting them as unknown target ones. Our method was extensively evaluated and outperformed other methods with a large margin in most settings.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_10');
INSERT INTO `paper` VALUES (11035, 'Open Set Learning with Counterfactual Images', 'Generative Adversarial Networks', 'Unknown Class', 'Augmented Dataset', 'OpenMAX', 'Adversarial Examples', 'In open set recognition, a classifier must label instances of known classes while detecting instances of unknown classes not encountered during training. To detect unknown classes while still generalizing to new instances of existing classes, we introduce a dataset augmentation technique that we call counterfactual image generation. Our approach, based on generative adversarial networks, generates examples that are close to training set examples yet do not belong to any training category. By augmenting training with examples generated by this optimization, we can reformulate open set recognition as classification with one additional class, which includes the set of novel and unknown examples. Our approach outperforms existing open set recognition algorithms on a selection of image classification tasks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_38');
INSERT INTO `paper` VALUES (11036, 'Open-World Stereo Video Matching with Deep RNN', 'Stereo video matching', 'Open world', 'Recurrent neural network', 'Convolutional LSTM', '', 'Deep Learning based stereo matching methods have shown great successes and achieved top scores across different benchmarks. However, like most data-driven methods, existing deep stereo matching networks suffer from some well-known drawbacks such as requiring large amount of labeled training data, and that their performances are fundamentally limited by the generalization ability. In this paper, we propose a novel Recurrent Neural Network (RNN) that takes a continuous (possibly previously unseen) stereo video as input, and directly predicts a depth-map at each frame without a pre-training process, and without the need of ground-truth depth-maps as supervision. Thanks to the recurrent nature (provided by two convolutional-LSTM blocks), our network is able to memorize and learn from its past experiences, and modify its inner parameters (network weights) to adapt to previously unseen or unfamiliar environments. This suggests a remarkable generalization ability of the net, making it applicable in an open world setting. Our method works robustly with changes in scene content, image statistics, and lighting and season conditions etc. By extensive experiments, we demonstrate that the proposed method seamlessly adapts between different scenarios. Equally important, in terms of the stereo matching accuracy, it outperforms state-of-the-art deep stereo approaches on standard benchmark datasets such as KITTI and Middlebury stereo.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_7');
INSERT INTO `paper` VALUES (11037, 'Optimizing Body Region Classification with Deep Convolutional Activation Features', 'Bag-of-Keypoints', 'DeCaf', 'Deep learning', 'Multimodal representation', 'Natural language processing', 'The goal of this work is to automatically apply generated image keywords as text representations, to optimize medical image classification accuracies of body regions. To create a keyword generative model, a Long Short-Term Memory (LSTM) based Recurrent Neural Network (RNN) is adopted, which is trained with preprocessed biomedical image captions as text representation and visual features extracted using Convolutional Neural Networks (CNN). For image representation, deep convolutional activation features and Bag-of-Keypoints (BoK) features are extracted for each radiograph and combined with the automatically generated keywords. Random Forest models and Support Vector Machines are trained with these multimodal image representations, as well as just visual representation, to predict body regions. Adopting multimodal image features proves to be the better approach, as the prediction accuracy for body regions is increased.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_60');
INSERT INTO `paper` VALUES (11038, 'Ordinal Regression with Neuron Stick-Breaking for Medical Diagnosis', 'Medical diagnosis', 'Ordinal regression', 'Deep neural network', 'Stick-breaking', 'Unimodal label smoothing', 'The classification for medical diagnosis usually involves inherently ordered labels corresponding to the level of health risk. Previous multi-task classifiers on ordinal data often use several binary classification branches to compute a series of cumulative probabilities. However, these cumulative probabilities are not guaranteed to be monotonically decreasing. It also introduces a large number of hyper-parameters to be fine-tuned manually. This paper aims to eliminate or at least largely reduce the effects of those problems. We propose a simple yet efficient way to rephrase the output layer of the conventional deep neural network. We show that our methods lead to the state-of-the-art accuracy on Diabetic Retinopathy dataset and Ultrasound Breast dataset with very little additional cost.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_23');
INSERT INTO `paper` VALUES (11039, 'Orthogonal Deep Features Decomposition for Age-Invariant Face Recognition', 'Age-invariant face recognition', 'Convolutional neural networks', 'Cross-age face dataset', '', '', 'As facial appearance is subject to significant intra-class variations caused by the aging process over time, age-invariant face recognition (AIFR) remains a major challenge in face recognition community. To reduce the intra-class discrepancy caused by the aging, in this paper we propose a novel approach (namely, Orthogonal Embedding CNNs, or OE-CNNs) to learn the age-invariant deep face features. Specifically, we decompose deep face features into two orthogonal components to represent age-related and identity-related features. As a result, identity-related features that are robust to aging are then used for AIFR. Besides, for complementing the existing cross-age datasets and advancing the research in this field, we construct a brand-new large-scale Cross-Age Face dataset (CAF). Extensive experiments conducted on the three public domain face aging datasets (MORPH Album 2, CACD-VS and FG-NET) have shown the effectiveness of the proposed approach and the value of the constructed CAF dataset on AIFR. Benchmarking our algorithm on one of the most popular general face recognition (GFR) dataset LFW additionally demonstrates the comparable generalization performance on GFR.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_45');
INSERT INTO `paper` VALUES (11040, 'Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-Out Classifiers', 'Anomaly detection', 'Out-of-distribution', '', '', '', 'As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin m between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al. [7] and the current state-of-the-art ODIN [13] on several OOD detection benchmarks.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_34');
INSERT INTO `paper` VALUES (11041, 'Paired 3D Model Generation with Conditional Generative Adversarial Networks', 'Conditional Generative Adversarial Network (CGAN)', 'Pair generation', 'Joint learning', '3D voxel model', '', 'Generative Adversarial Networks (GANs) are shown to be successful at generating new and realistic samples including 3D object models. Conditional GAN, a variant of GANs, allows generating samples in given conditions. However, objects generated for each condition are different and it does not allow generation of the same object in different conditions. In this paper, we first adapt conditional GAN, which is originally designed for 2D image generation, to the problem of generating 3D models in different rotations. We then propose a new approach to guide the network to generate the same 3D sample in different and controllable rotation angles (sample pairs). Unlike previous studies, the proposed method does not require modification of the standard conditional GAN architecture and it can be integrated into the training step of any conditional GAN. Experimental results and visual comparison of 3D models show that the proposed method is successful at generating model pairs in different conditions.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_29');
INSERT INTO `paper` VALUES (11042, 'Pairwise Body-Part Attention for Recognizing Human-Object Interactions', 'Human-object interactions', 'Body-part correlations', 'Attention model', '', '', 'In human-object interactions (HOI) recognition, conventional methods consider the human body as a whole and pay a uniform attention to the entire body region. They ignore the fact that normally, human interacts with an object by using some parts of the body. In this paper, we argue that different body parts should be paid with different attention in HOI recognition, and the correlations between different body parts should be further considered. This is because our body parts always work collaboratively. We propose a new pairwise body-part attention model which can learn to focus on crucial parts, and their correlations for HOI recognition. A novel attention based feature selection method and a feature representation scheme that can capture pairwise correlations between body parts are introduced in the model. Our proposed approach achieved \\(\\mathbf {10}\\%\\) relative improvement (36.1 mAP \\(\\rightarrow \\) 39.9 mAP) over the state-of-the-art results in HOI recognition on the HICO dataset. We will make our model and source codes publicly available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_4');
INSERT INTO `paper` VALUES (11043, 'Pairwise Confusion for Fine-Grained Visual Classification', 'Fine-grained Visual Categorization (FGVC)', 'Intra-class Variations', 'Spatial Transformer Network', 'Bilinear Pooling', 'High Inter-class Similarity', 'Fine-Grained Visual Classification (FGVC) datasets contain small sample sizes, along with significant intra-class variation and inter-class similarity. While prior work has addressed intra-class variation using localization and segmentation techniques, inter-class similarity may also affect feature learning and reduce classification performance. In this work, we address this problem using a novel optimization procedure for the end-to-end neural network training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces overfitting by intentionally introducing confusion in the activations. With PC regularization, we obtain state-of-the-art performance on six of the most widely-used FGVC datasets and demonstrate improved localization ability. PC is easy to implement, does not need excessive hyperparameter tuning during training, and does not add significant overhead during test time.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_5');
INSERT INTO `paper` VALUES (11044, 'Pairwise Relational Networks for Face Recognition', 'Pairwise relational network', 'Relations', 'Face recognition', '', '', 'Existing face recognition using deep neural networks is difficult to know what kind of features are used to discriminate the identities of face images clearly. To investigate the effective features for face recognition, we propose a novel face recognition method, called a pairwise relational network (PRN), that obtains local appearance patches around landmark points on the feature map, and captures the pairwise relation between a pair of local appearance patches. The PRN is trained to capture unique and discriminative pairwise relations among different identities. Because the existence and meaning of pairwise relations should be identity dependent, we add a face identity state feature, which obtains from the long short-term memory (LSTM) units network with the sequential local appearance patches on the feature maps, to the PRN. To further improve accuracy of face recognition, we combined the global appearance representation with the pairwise relational feature. Experimental results on the LFW show that the PRN using only pairwise relations achieved 99.65% accuracy and the PRN using both pairwise relations and face identity state feature achieved 99.76% accuracy. On the YTF, both the PRN using only pairwise relations and the PRN using pairwise relations and the face identity state feature achieved the state-of-the-art (95.7% and 96.3%). The PRN also achieved comparable results to the state-of-the-art for both face verification and face identification tasks on the IJB-A, and the state-of-the-art on the IJB-B.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_39');
INSERT INTO `paper` VALUES (11045, 'Parallel Feature Pyramid Network for Object Detection', 'Real-time object detection', 'Feature pyramid', '', '', '', 'Recently developed object detectors employ a convolutional neural network (CNN) by gradually increasing the number of feature layers with a pyramidal shape instead of using a featurized image pyramid. However, the different abstraction levels of CNN feature layers often limit the detection performance, especially on small objects. To overcome this limitation, we propose a CNN-based object detection architecture, referred to as a parallel feature pyramid (FP) network (PFPNet), where the FP is constructed by widening the network width instead of increasing the network depth. First, we adopt spatial pyramid pooling and some additional feature transformations to generate a pool of feature maps with different sizes. In PFPNet, the additional feature transformation is performed in parallel, which yields the feature maps with similar levels of semantic abstraction across the scales. We then resize the elements of the feature pool to a uniform size and aggregate their contextual information to generate each level of the final FP. The experimental results confirmed that PFPNet increases the performance of the latest version of the single-shot multi-box detector (SSD) by mAP of 6.4% AP and especially, 7.8% \\(\\text {AP}_\\text {small}\\) on the MS-COCO dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_15');
INSERT INTO `paper` VALUES (11046, 'PARN: Pyramidal Affine Regression Networks for Dense Semantic Correspondence', 'Dense semantic correspondence', 'Hierarchical graph model', '', '', '', 'This paper presents a deep architecture for dense semantic correspondence, called pyramidal affine regression networks (PARN), that estimates locally-varying affine transformation fields across images. To deal with intra-class appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed within deep networks. PARN estimates residual affine transformations at each level and composes them to estimate final affine transformations. Furthermore, to overcome the limitations of insufficient training data for semantic correspondence, we propose a novel weakly-supervised training scheme that generates progressive supervisions by leveraging a correspondence consistency across image pairs. Our method is fully learnable in an end-to-end manner and does not require quantizing infinite continuous affine transformation fields. To the best of our knowledge, it is the first work that attempts to estimate dense affine transformation fields in a coarse-to-fine manner within deep networks. Experimental results demonstrate that PARN outperforms the state-of-the-art methods for dense semantic correspondence on various benchmarks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_22');
INSERT INTO `paper` VALUES (11047, 'Part-Activated Deep Reinforcement Learning for Action Prediction', 'Action prediction', 'Deep reinforcement learning', 'Skeleton', 'Part model', '', 'In this paper, we propose a part-activated deep reinforcement learning (PA-DRL) method for action prediction. Most existing methods for action prediction utilize the evolution of whole frames to model actions, which cannot avoid the noise of the current action, especially in the early prediction. Moreover, the loss of structural information of human body diminishes the capacity of features to describe actions. To address this, we design the PA-DRL to exploit the structure of the human body by extracting skeleton proposals under a deep reinforcement learning framework. Specifically, we extract features from different parts of the human body individually and activate the action-related parts in features to enhance the representation. Our method not only exploits the structure information of the human body, but also considers the saliency part for expressing actions. We evaluate our method on three popular action prediction datasets: UT-Interaction, BIT-Interaction and UCF101. Our experimental results demonstrate that our method achieves the performance with state-of-the-arts.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_26');
INSERT INTO `paper` VALUES (11048, 'Part-Aligned Bilinear Representations for Person Re-identification', 'Person re-identification', 'Part alignment', 'Bilinear pooling', '', '', 'Comparing the appearance of corresponding body parts is essential for person re-identification. As body parts are frequently misaligned between the detected human boxes, an image representation that can handle this misalignment is required. In this paper, we propose a network that learns a part-aligned representation for person re-identification. Our model consists of a two-stream network, which generates appearance and body part feature maps respectively, and a bilinear-pooling layer that fuses two feature maps to an image descriptor. We show that it results in a compact descriptor, where the image matching similarity is equivalent to an aggregation of the local appearance similarities of the corresponding body parts. Since the image similarity does not depend on the relative positions of parts, our approach significantly reduces the part misalignment problem. Training the network does not require any part annotation on the person re-identification dataset. Instead, we simply initialize the part sub-stream using a pre-trained sub-network of an existing pose estimation network and train the whole network to minimize the re-identification loss. We validate the effectiveness of our approach by demonstrating its superiority over the state-of-the-art methods on the standard benchmark datasets including Market-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_25');
INSERT INTO `paper` VALUES (11049, 'Partial Adversarial Domain Adaptation', '', '', '', '', '', 'Domain adversarial learning aligns the feature distributions across the source and target domains in a two-player minimax game. Existing domain adversarial networks generally assume identical label space across different domains. In the presence of big data, there is strong motivation of transferring deep models from existing big domains to unknown small domains. This paper introduces partial domain adaptation as a new domain adaptation scenario, which relaxes the fully shared label space assumption to that the source label space subsumes the target label space. Previous methods typically match the whole source domain to the target domain, which are vulnerable to negative transfer for the partial domain adaptation problem due to the large mismatch between label spaces. We present Partial Adversarial Domain Adaptation (PADA), which simultaneously alleviates negative transfer by down-weighing the data of outlier source classes for training both source classifier and domain adversary, and promotes positive transfer by matching the feature distributions in the shared label space. Experiments show that PADA exceeds state-of-the-art results for partial domain adaptation tasks on several datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_9');
INSERT INTO `paper` VALUES (11050, 'PathGAN: Visual Scanpath Prediction with Generative Adversarial Networks', 'Saliency', 'Scanpath', 'Adversarial training', 'GAN', 'cGAN', 'We introduce PathGAN, a deep neural network for visual scanpath prediction trained on adversarial examples. A visual scanpath is defined as the sequence of fixation points over an image defined by a human observer with its gaze. PathGAN is composed of two parts, the generator and the discriminator. Both parts extract features from images using off-the-shelf networks, and train recurrent layers to generate or discriminate scanpaths accordingly. In scanpath prediction, the stochastic nature of the data makes it very difficult to generate realistic predictions using supervised learning strategies, but we adopt adversarial training as a suitable alternative. Our experiments prove how PathGAN improves the state of the art of visual scanpath prediction on the iSUN and Salient360! datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_25');
INSERT INTO `paper` VALUES (11051, 'Penalizing Top Performers: Conservative Loss for Semantic Segmentation Adaptation', 'Conservative Loss', 'Domain Adaptation', 'Gradient Ascend Method', 'Source Domain', 'Domain Invariant Representation', 'Due to the expensive and time-consuming annotations (e.g., segmentation) for real-world images, recent works in computer vision resort to synthetic data. However, the performance on the real image often drops significantly because of the domain shift between the synthetic data and the real images. In this setting, domain adaptation brings an appealing option. The effective approaches of domain adaptation shape the representations that (1) are discriminative for the main task and (2) have good generalization capability for domain shift. To this end, we propose a novel loss function, i.e., Conservative Loss, which penalizes the extreme good and bad cases while encouraging the moderate examples. More specifically, it enables the network to learn features that are discriminative by gradient descent and are invariant to the change of domains via gradient ascend method. Extensive experiments on synthetic to real segmentation adaptation show our proposed method achieves state of the art results. Ablation studies give more insights into properties of the Conservative Loss. Exploratory experiments and discussion demonstrate that our Conservative Loss has good flexibility rather than restricting an exact form.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_35');
INSERT INTO `paper` VALUES (11052, 'Perception-Enhanced Image Super-Resolution via Relativistic Generative Adversarial Networks', 'Super-resolution', 'Perceptual quality', '', '', '', 'This paper considers a deep Generative Adversarial Networks (GAN) based method referred to as the Perception-Enhanced Super-Resolution (PESR) for Single Image Super Resolution (SISR) that enhances the perceptual quality of the reconstructed images by considering the following three issues: (1) ease GAN training by replacing an absolute with a relativistic discriminator, (2) include in the loss function a mechanism to emphasize difficult training samples which are generally rich in texture and (3) provide a flexible quality control scheme at test time to trade-off between perception and fidelity. Based on extensive experiments on six benchmark datasets, PESR outperforms recent state-of-the-art SISR methods in terms of perceptual quality. The code is available at https://github.com/thangvubk/PESR.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_7');
INSERT INTO `paper` VALUES (11053, 'Perception-Preserving Convolutional Networks for Image Enhancement on Smartphones', 'Image enhancement', 'Perception-preserving measurement error', 'Knowledge transfer', '', '', 'Although the configuration of smartphone cameras is getting better and better, the quality of smartphone photos still cannot match DSLR camera photos due to the limitation of physical space, hardware and cost. In this work, we present a fast and accurate image enhancement approach based on generative adversarial nets, which elevates the quality of photos on smartphones. We propose the lightweight local residual convolutional network to learn the mapping between ordinary photos and DSLR-quality images. To make the generated images look real, we introduce the perception-preserving measurement error, which comprises content, color, and adversarial losses. Especially, the content loss is constituted of contextual and SSIM losses, which maintains the natural internal statistics and the structure of images. In addition, we introduce the knowledge transfer strategy to ensure the high performance of the proposed network. The experiments demonstrate that our proposed method produces better results compared with the state-of-the-art approaches, both qualitatively and quantitatively. The code is available at https://github.com/Zheng222/PPCN.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_13');
INSERT INTO `paper` VALUES (11054, 'Person Re-identification with Deep Similarity-Guided Graph Neural Network', 'Deep learning', 'Person re-identification', 'Graph Neural Networks', '', '', 'The person re-identification task requires to robustly estimate visual similarities between person images. However, existing person re-identification models mostly estimate the similarities of different image pairs of probe and gallery images independently while ignores the relationship information between different probe-gallery pairs. As a result, the similarity estimation of some hard samples might not be accurate. In this paper, we propose a novel deep learning framework, named Similarity-Guided Graph Neural Network (SGGNN) to overcome such limitations. Given a probe image and several gallery images, SGGNN creates a graph to represent the pairwise relationships between probe-gallery pairs (nodes) and utilizes such relationships to update the probe-gallery relation features in an end-to-end manner. Accurate similarity estimation can be achieved by using such updated probe-gallery relation features for prediction. The input features for nodes on the graph are the relation features of different probe-gallery image pairs. The probe-gallery relation feature updating is then performed by the messages passing in SGGNN, which takes other nodes’ information into account for similarity estimation. Different from conventional GNN approaches, SGGNN learns the edge weights with rich labels of gallery instance pairs directly, which provides relation fusion more precise information. The effectiveness of our proposed method is validated on three public person re-identification datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_30');
INSERT INTO `paper` VALUES (11055, 'Person Search by Multi-Scale Matching', 'Person search', 'Person detection and re-identification', 'Multi-scale matching', 'Feature pyramid', 'Image pyramid', 'We consider the problem of person search in unconstrained scene images. Existing methods usually focus on improving the person detection accuracy to mitigate negative effects imposed by misalignment, mis-detections, and false alarms resulted from noisy people auto-detection. In contrast to previous studies, we show that sufficiently reliable person instance cropping is achievable by slightly improved state-of-the-art deep learning object detectors (e.g. Faster-RCNN), and the under-studied multi-scale matching problem in person search is a more severe barrier. In this work, we address this multi-scale person search challenge by proposing a Cross-Level Semantic Alignment (CLSA) deep learning approach capable of learning more discriminative identity feature representations in a unified end-to-end model. This is realised by exploiting the in-network feature pyramid structure of a deep neural network enhanced by a novel cross pyramid-level semantic alignment loss function. This favourably eliminates the need for constructing a computationally expensive image pyramid and a complex multi-branch network architecture. Extensive experiments show the modelling advantages and performance superiority of CLSA over the state-of-the-art person search and multi-scale matching methods on two large person search benchmarking datasets: CUHK-SYSU and PRW.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_33');
INSERT INTO `paper` VALUES (11056, 'Person Search in Videos with One Portrait Through Visual and Temporal Links', 'Person search', 'Portrait', 'Visual and temporal', 'Progressive Propagation', 'Competitive Consensus', 'In real-world applications, e.g. law enforcement and video retrieval, one often needs to search a certain person in long videos with just one portrait. This is much more challenging than the conventional settings for person re-identification, as the search may need to be carried out in the environments different from where the portrait was taken. In this paper, we aim to tackle this challenge and propose a novel framework, which takes into account the identity invariance along a tracklet, thus allowing person identities to be propagated via both the visual and the temporal links. We also develop a novel scheme called Progressive Propagation via Competitive Consensus, which significantly improves the reliability of the propagation process. To promote the study of person search, we construct a large-scale benchmark, which contains 127K manually annotated tracklets from 192 movies. Experiments show that our approach remarkably outperforms mainstream person re-id methods, raising the mAP from \\(42.16\\%\\) to \\(62.27\\%\\) (Code at https://github.com/hqqasw/person-search-PPCC).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_26');
INSERT INTO `paper` VALUES (11057, 'Person Search via a Mask-Guided Two-Stream CNN Model', 'Person search', 'Pedestrian detection', 'Person re-identification', 'Foreground', '', 'In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identification (re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. On the standard person search benchmark datasets, we achieve mAP of \\(83.0\\%\\) and \\(32.6\\%\\) respectively for CUHK-SYSU and PRW, surpassing the state of the art by a large margin (more than 5 pp).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_45');
INSERT INTO `paper` VALUES (11058, 'PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up, Part-Based, Geometric Embedding Model', 'Person detection and pose estimation', 'Segmentation and grouping', '', '', '', 'We present a box-free bottom-up approach for the tasks of pose estimation and instance segmentation of people in multi-person images using an efficient single-shot model. The proposed PersonLab model tackles both semantic-level reasoning and object-part associations using part-based modeling. Our model employs a convolutional network which learns to detect individual keypoints and predict their relative displacements, allowing us to group keypoints into person pose instances. Further, we propose a part-induced geometric embedding descriptor which allows us to associate semantic person pixels with their corresponding person instance, delivering instance-level person segmentations. Our system is based on a fully-convolutional architecture and allows for efficient inference, with runtime essentially independent of the number of people present in the scene. Trained on COCO data alone, our system achieves COCO test-dev keypoint average precision of 0.665 using single-scale inference and 0.687 using multi-scale inference, significantly outperforming all previous bottom-up pose estimation systems. We are also the first bottom-up method to report competitive results for the person class in the COCO instance segmentation task, achieving a person category average precision of 0.417.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_17');
INSERT INTO `paper` VALUES (11059, 'Perturbation Robust Representations of Topological Persistence Diagrams', 'Invariance learning', 'Topological data analysis', 'Persistence diagrams', 'Grassmann manifold', 'Perturbed topological signature', 'Topological methods for data analysis present opportunities for enforcing certain invariances of broad interest in computer vision, including view-point in activity analysis, articulation in shape analysis, and measurement invariance in non-linear dynamical modeling. The increasing success of these methods is attributed to the complementary information that topology provides, as well as availability of tools for computing topological summaries such as persistence diagrams. However, persistence diagrams are multi-sets of points and hence it is not straightforward to fuse them with features used for contemporary machine learning tools like deep-nets. In this paper we present theoretically well-grounded approaches to develop novel perturbation robust topological representations, with the long-term view of making them amenable to fusion with contemporary learning architectures. We term the proposed representation as Perturbed Topological Signatures, which live on a Grassmann manifold and hence can be efficiently used in machine learning pipelines. We explore the use of the proposed descriptor on three applications: 3D shape analysis, view-invariant activity analysis, and non-linear dynamical modeling. We show favorable results in both high-level recognition performance and time-complexity when compared to other baseline methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_38');
INSERT INTO `paper` VALUES (11060, 'Photorealistic Facial Synthesis in the Dimensional Affect Space', 'Dimensional facial affect synthesis', 'Valence', 'Arousal', 'Discretization', 'Blendshape models', 'This paper presents a novel approach for synthesizing facial affect, which is based on our annotating 600,000 frames of the 4DFAB database in terms of valence and arousal. The input of this approach is a pair of these emotional state descriptors and a neutral 2D image of a person to whom the corresponding affect will be synthesized. Given this target pair, a set of 3D facial meshes is selected, which is used to build a blendshape model and generate the new facial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting is performed and the reconstructed face is deformed to generate the target facial expressions. Last, the new face is rendered into the original image. Both qualitative and quantitative experimental studies illustrate the generation of realistic images, when the neutral image is sampled from a variety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE, AFEW-VA, BU-3DFE, Bosphorus.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_36');
INSERT INTO `paper` VALUES (11061, 'Physical Primitive Decomposition', 'Physical Primitives', 'Primitive Decomposition', 'Tower Block', 'Physical Parameter Estimation', 'Bullet Physics Engine', 'Objects are made of parts, each with distinct geometry, physics, functionality, and affordances. Developing such a distributed, physical, interpretable representation of objects will facilitate intelligent agents to better explore and interact with the world. In this paper, we study physical primitive decomposition—understanding an object through its components, each with physical and geometric attributes. As annotated data for object parts and physics are rare, we propose a novel formulation that learns physical primitives by explaining both an object’s appearance and its behaviors in physical events. Our model performs well on block towers and tools in both synthetic and real scenarios; we also demonstrate that visual and physical observations often provide complementary signals. We further present ablation and behavioral studies to better understand our model and contrast it with human performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_1');
INSERT INTO `paper` VALUES (11062, 'Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights', 'Incremental learning', 'Binary networks', '', '', '', 'This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that “piggyback” on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_5');
INSERT INTO `paper` VALUES (11063, 'PIRM Challenge on Perceptual Image Enhancement on Smartphones: Report', 'Image enhancement', 'Image super-resolution', 'Challenge', 'Efficiency', 'Deep learning', 'This paper reviews the first challenge on efficient perceptual image enhancement with the focus on deploying deep learning models on smartphones. The challenge consisted of two tracks. In the first one, participants were solving the classical image super-resolution problem with a bicubic downscaling factor of 4. The second track was aimed at real-world photo enhancement, and the goal was to map low-quality photos from the iPhone 3GS device to the same photos captured with a DSLR camera. The target metric used in this challenge combined the runtime, PSNR scores and solutions’ perceptual results measured in the user study. To ensure the efficiency of the submitted models, we additionally measured their runtime and memory requirements on Android smartphones. The proposed solutions significantly improved baseline results defining the state-of-the-art for image enhancement on smartphones.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_20');
INSERT INTO `paper` VALUES (11064, 'PIRM2018 Challenge on Spectral Image Super-Resolution: Dataset and Study', 'Super-resolution', 'Hyperspectral', 'Multispectral', 'RGB', 'Stereo', 'This paper introduces a newly collected and novel dataset (StereoMSI) for example-based single and colour-guided spectral image super-resolution. The dataset was first released and promoted during the PIRM2018 spectral image super-resolution challenge. To the best of our knowledge, the dataset is the first of its kind, comprising 350 registered colour-spectral image pairs. The dataset has been used for the two tracks of the challenge and, for each of these, we have provided a split into training, validation and testing. This arrangement is a result of the challenge structure and phases, with the first track focusing on example-based spectral image super-resolution and the second one aiming at exploiting the registered stereo colour imagery to improve the resolution of the spectral images. Each of the tracks and splits has been selected to be consistent across a number of image quality metrics. The dataset is quite general in nature and can be used for a wide variety of applications in addition to the development of spectral image super-resolution methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_18');
INSERT INTO `paper` VALUES (11065, 'PIRM2018 Challenge on Spectral Image Super-Resolution: Methods and Results', 'Super-resolution', 'Multispectral', 'Hyperspectral', 'RGB', 'Stereo', 'In this paper, we describe the Perceptual Image Restoration and Manipulation (PIRM) workshop challenge on spectral image super-resolution, motivate its structure and conclude on results obtained by the participants. The challenge is one of the first of its kind, aiming at leveraging modern machine learning techniques to achieve spectral image super-resolution. It comprises of two tracks. The first of these (Track 1) is about example-based single spectral image super-resolution. The second one (Track 2) is on colour-guided spectral image super-resolution. In this manner, Track 1 focuses on the problem of super-resolving the spatial resolution of spectral images given training pairs of low and high spatial resolution spectral images. Track 2, on the other hand, aims to leverage the inherently higher spatial resolution of colour (RGB) cameras and the link between spectral and trichromatic images of the scene. The challenge in both tracks is then to recover a super-resolved image making use of low-resolution imagery at the input. We also elaborate upon the methods used by the participants, summarise the results and discuss their rankings.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_22');
INSERT INTO `paper` VALUES (11066, 'Pivot Correlational Neural Network for Multimodal Video Categorization', 'Video categorization', 'Multimodal representation', 'Sequential modeling', 'Deep learning', '', 'This paper considers an architecture for multimodal video categorization referred to as Pivot Correlational Neural Network (Pivot CorrNN). The architecture consists of modal-specific streams dedicated exclusively to one specific modal input as well as modal-agnostic pivot stream that considers all modal inputs without distinction, and the architecture tries to refine the pivot prediction based on modal-specific predictions. The Pivot CorrNN consists of three modules: (1) maximizing pivot-correlation module that maximizes the correlation between the hidden states as well as the predictions of the modal-agnostic pivot stream and modal-specific streams in the network, (2) contextual Gated Recurrent Unit (cGRU) module which extends the capability of a generic GRU to take multimodal inputs in updating the pivot hidden-state, and (3) adaptive aggregation module that aggregates all modal-specific predictions as well as the modal-agnostic pivot predictions into one final prediction. We evaluate the Pivot CorrNN on two publicly available large-scale multimodal video categorization datasets, FCVID and YouTube-8M. From the experimental results, Pivot CorrNN achieves the best performance on the FCVID database and performance comparable to the state-of-the-art on YouTube-8M database.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_24');
INSERT INTO `paper` VALUES (11067, 'Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images', '3D shape generation', 'Graph convolutional neural network', 'Mesh reconstruction', 'Coarse-to-fine', 'End-to-end framework', 'We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_4');
INSERT INTO `paper` VALUES (11068, 'Plane-Based Humanoid Robot Navigation and Object Model Construction for Grasping', 'Object grasping', 'Humanoid robot', 'Pose recovery', '', '', 'In this work we present an approach to humanoid robot navigation and object model construction for grasping using only RGB-D data from an onboard depth sensor. A plane-based representation is used to provide a high-level model of the workspace, to estimate both the global robot pose and pose with respect to the object, and to determine the object pose as well as its dimensions. A visual feedback is used to achieve the desired robot pose for grasping. In the pre–grasping pose the robot determines the object pose as well as its dimensions. In such a local grasping approach, a simulator with our high-level scene representation and a virtual camera is used to fine-tune the motion controllers as well as to simulate and validate the process of grasping. We present experimental results that were obtained in simulations with virtual camera and robot as well as with real humanoid robot equipped with RGB-D camera, which performed object grasping in low-texture layouts.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_40');
INSERT INTO `paper` VALUES (11069, 'PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction', 'RGB-D registration', 'Co-planarity', 'Loop closure', '', '', 'We introduce a novel RGB-D patch descriptor designed for detecting coplanar surfaces in SLAM reconstruction. The core of our method is a deep convolutional neural network that takes in RGB, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to find coplanar patches from other images. We train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity RGB-D scans. Experiments show that our learned descriptor outperforms alternatives extended for this new task by a significant margin. In addition, we demonstrate the benefits of coplanarity matching in a robust RGBD reconstruction formulation. We find that coplanarity constraints detected with our method are sufficient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on established benchmarks when combined with traditional keypoint matching.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_46');
INSERT INTO `paper` VALUES (11070, 'PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-Modalities', 'Cross-modal representation', 'Generative adversarial networks', 'Infrared action recognition', 'Infrared dataset', '', 'Data of different modalities generally convey complimentary but heterogeneous information, and a more discriminative representation is often preferred by combining multiple data modalities like the RGB and infrared features. However in reality, obtaining both data channels is challenging due to many limitations. For example, the RGB surveillance cameras are often restricted from private spaces, which is in conflict with the need of abnormal activity detection for personal security. As a result, using partial data channels to build a full representation of multi-modalities is clearly desired. In this paper, we propose a novel Partial-modal Generative Adversarial Networks (PM-GANs) that learns a full-modal representation using data from only partial modalities. The full representation is achieved by a generated representation in place of the missing data channel. Extensive experiments are conducted to verify the performance of our proposed method on action recognition, compared with four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset for action recognition is introduced, and will be the first publicly available action dataset that contains paired infrared and visible spectrum. (The dataset will be available at http://www.escience.cn/people/gaochenqiang/Publications.html).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_24');
INSERT INTO `paper` VALUES (11071, 'Point-to-Point Regression PointNet for 3D Hand Pose Estimation', '3D hand pose estimation', '', '', '', '', 'Convolutional Neural Networks (CNNs)-based methods for 3D hand pose estimation with depth cameras usually take 2D depth images as input and directly regress holistic 3D hand pose. Different from these methods, our proposed Point-to-Point Regression PointNet directly takes the 3D point cloud as input and outputs point-wise estimations, i.e., heat-maps and unit vector fields on the point cloud, representing the closeness and direction from every point in the point cloud to the hand joint. The point-wise estimations are used to infer 3D joint locations with weighted fusion. To better capture 3D spatial information in the point cloud, we apply a stacked network architecture for PointNet with intermediate supervision, which is trained end-to-end. Experiments show that our method can achieve outstanding results when compared with state-of-the-art methods on three challenging hand pose datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_29');
INSERT INTO `paper` VALUES (11072, 'Polarimetric Three-View Geometry', 'Three-view Geometry', 'Mixed Polarization', 'Camera Rotation', 'Camera Translation', 'Relative Pose', 'This paper theorizes the connection between polarization and three-view geometry. It presents a ubiquitous polarization-induced constraint that regulates the relative pose of a system of three cameras. We demonstrate that, in a multi-view system, the polarization phase obtained for a surface point is induced from one of the two pencils of planes: one by specular reflections with its axis aligned with the incident light; one by diffusive reflections with its axis aligned with the surface normal. Differing from the traditional three-view geometry, we show that this constraint directly encodes camera rotation and projection, and is independent of camera translation. In theory, six polarized diffusive point-point-point correspondences suffice to determine the camera rotations. In practise, a cross-validation mechanism using correspondences of specularites can effectively resolve the ambiguities caused by mixed polarization. The experiments on real world scenes validate our proposed theory.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_2');
INSERT INTO `paper` VALUES (11073, 'Polygonal Reconstruction of Building Interiors from Cluttered Pointclouds', '', '', '', '', '', 'In this paper, we propose a framework for reconstructing a compact geometric model from point clouds of building interiors. Geometric reconstruction of indoor scenes is especially challenging due to clutter in the scene, such as furniture and cabinets. The clutter may (partially) hide the structural components of the interior. The proposed framework is able to cope with this clutter by using a hypothesizing and selection strategy, in which candidate faces are firstly generated by intersecting the extracted planar primitives. Secondly, an optimal subset of candidate faces is selected by optimizing a binary labeling problem. We formulate the selection problem as a continuous quadratic optimization problem, allowing us to incorporate a cost function specifically for indoor scenes. The obtained polygonal surface is not only 2-manifold but also oriented, meaning that the surface normals of each polygon are consistently oriented towards the interior. All adjacent and coplanar faces that were selected, are merged into a single face in order to obtain a final geometric model that is as compact as possible. This compact model of the room uses less memory and allows for faster processing when used in virtual reality applications. The method of Nan et al. was used as a starting point for our proposed framework. Finally, as opposed to other state-of-the-art interior modeling approaches, the only input that is required, is the point cloud itself. We do not rely on viewpoint information, nor do we assume constrained input environments with a 2.5D or, more restrictively, a Manhattan-world structure. To demonstrate the practical applicability of our proposed method, we performed various experiments on actual scan data of building interiors.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_28');
INSERT INTO `paper` VALUES (11074, 'Pose Guided Human Image Synthesis by View Disentanglement and Enhanced Weighting Loss', 'Pose-guided view synthesis', 'Generative models', 'Structural similarity', '', '', 'View synthesis aims at generating a novel, unseen view of an object. This is a challenging task in the presence of occlusions and asymmetries. In this paper, we present View-Disentangled Generator (VDG), a two-stage deep network for pose-guided human-image generation that performs coarse view prediction followed by a refinement stage. In the first stage, the network predicts the output from a target human pose, the source-image and the corresponding human pose, which are processed in different branches separately. This enables the network to learn a disentangled representation from the source and target view. In the second stage, the coarse output from the first stage is refined by adversarial training. Specifically, we introduce a masked version of the structural similarity loss that facilitates the network to focus on generating a higher quality view. Experiments on Market-1501 and DeepFashion demonstrate the effectiveness of the proposed generator.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_30');
INSERT INTO `paper` VALUES (11075, 'Pose Guided Human Video Generation', 'Human video generation', 'Pose synthesis', 'Generation adversarial network', '', '', 'Due to the emergence of Generative Adversarial Networks, video synthesis has witnessed exceptional breakthroughs. However, existing methods lack a proper representation to explicitly control the dynamics in videos. Human pose, on the other hand, can represent motion patterns intrinsically and interpretably, and impose the geometric constraints regardless of appearance. In this paper, we propose a pose guided method to synthesize human videos in a disentangled way: plausible motion prediction and coherent appearance generation. In the first stage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an adversarial manner to yield pose sequences conditioned on the class label. In the second stage, a Semantic Consistent Generative Adversarial Network (SCGAN) generates video frames from the poses while preserving coherent appearances in the input image. By enforcing semantic consistency between the generated and ground-truth poses at a high feature level, our SCGAN is robust to noisy or abnormal poses. Extensive experiments on both human action and human face datasets manifest the superiority of the proposed method over other state-of-the-arts.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_13');
INSERT INTO `paper` VALUES (11076, 'Pose Partition Networks for Multi-person Pose Estimation', 'Multi-person pose estimation', 'Pose partition', 'Dense regression', '', '', 'This paper proposes a novel Pose Partition Network (PPN) to address the challenging multi-person pose estimation problem. The proposed PPN is favorably featured by low complexity and high accuracy of joint detection and partition. In particular, PPN performs dense regressions from global joint candidates within a specific embedding space, which is parameterized by centroids of persons, to efficiently generate robust person detection and joint partition. Then, PPN infers body joint configurations through conducting graph partition for each person detection locally, utilizing reliable global affinity cues. In this way, PPN reduces computation complexity and improves multi-person pose estimation significantly. We implement PPN with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF show the efficiency of PPN with new state-of-the-art performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_42');
INSERT INTO `paper` VALUES (11077, 'Pose Proposal Networks', 'Human pose estimation', 'Object detection', '', '', '', 'We propose a novel method to detect an unknown number of articulated 2D poses in real time. To decouple the runtime complexity of pixel-wise body part detectors from their convolutional neural network (CNN) feature map resolutions, our approach, called pose proposal networks, introduces a state-of-the-art single-shot object detection paradigm using grid-wise image feature maps in a bottom-up pose detection scenario. Body part proposals, which are represented as region proposals, and limbs are detected directly via a single-shot CNN. Specialized to such detections, a bottom-up greedy parsing step is probabilistically redesigned to take into account the global context. Experimental results on the MPII Multi-Person benchmark confirm that our method achieves 72.8% mAP comparable to state-of-the-art bottom-up approaches while its total runtime using a GeForce GTX1080Ti card reaches up to 5.6 ms (180 FPS), which exceeds the bottleneck runtimes that are observed in state-of-the-art approaches.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_21');
INSERT INTO `paper` VALUES (11078, 'Pose-Normalized Image Generation for Person Re-identification', 'Person re-id', 'GAN', 'Pose normalization', '', '', 'Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id features free of the influence of pose variations. We show that these features are complementary to features learned with the original images. Importantly, a more realistic unsupervised learning setting is considered in this work, and our model is shown to have the potential to be generalizable to a new re-id dataset without any fine-tuning. The codes will be released at https://github.com/naiq/PN_GAN.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_40');
INSERT INTO `paper` VALUES (11079, 'PosIX-GAN: Generating Multiple Poses Using GAN for Pose-Invariant Face Recognition', 'Face recognition', 'Pose', 'GAN', 'Multi-task Learning', '', 'Pose-Invariant Face Recognition (PIFR) has been a serious challenge in the general field of face recognition (FR). The performance of face recognition algorithms deteriorate due to various degradations such as pose, illuminaton, occlusions, blur, noise, aliasing, etc. In this paper, we deal with the problem of 3D pose variation of a face. for that we design and propose PosIX Generative Adversarial Network (PosIX-GAN) that has been trained to generate a set of nice (high quality) face images with 9 different pose variations, when provided with a face image in any arbitrary pose as input. The discriminator of the GAN has also been trained to perform the task of face recognition along with the job of discriminating between real and generated (fake) images. Results when evaluated using two benchmark datasets, reveal the superior performance of PosIX-GAN over state-of-the-art shallow as well as deep learning methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_31');
INSERT INTO `paper` VALUES (11080, 'PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors', '3D deep learning', 'Local features', 'Descriptors', 'Rotation invariance', '', 'We present PPF-FoldNet for unsupervised learning of 3D local descriptors on pure point cloud geometry. Based on the folding-based auto-encoding of well known point pair features, PPF-FoldNet offers many desirable properties: it necessitates neither supervision, nor a sensitive local reference frame, benefits from point-set sparsity, is end-to-end, fast, and can extract powerful rotation invariant descriptors. Thanks to a novel feature visualization, its evolution can be monitored to provide interpretable insights. Our extensive experiments demonstrate that despite having six degree-of-freedom invariance and lack of training labels, our network achieves state of the art results in standard benchmark datasets and outperforms its competitors when rotations and varying point densities are present. PPF-FoldNet achieves 9% higher recall on standard benchmarks, 23% higher recall when rotations are introduced into the same datasets and finally, a margin of >35% is attained when point density is significantly decreased.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_37');
INSERT INTO `paper` VALUES (11081, 'Practical Black-Box Attacks on Deep Neural Networks Using Efficient Query Mechanisms', 'Deep neural networks', 'Image classification', 'Adversarial examples', 'Black-box attacks', '', 'Existing black-box attacks on deep neural networks (DNNs) have largely focused on transferability, where an adversarial instance generated for a locally trained model can “transfer” to attack other learning models. In this paper, we propose novel Gradient Estimation black-box attacks for adversaries with query access to the target model’s class probabilities, which do not rely on transferability. We also propose strategies to decouple the number of queries required to generate each adversarial sample from the dimensionality of the input. An iterative variant of our attack achieves close to 100% attack success rates for both targeted and untargeted attacks on DNNs. We carry out a thorough comparative evaluation of black-box attacks and show that Gradient Estimation attacks achieve attack success rates similar to state-of-the-art white-box attacks on the MNIST and CIFAR-10 datasets. We also apply the Gradient Estimation attacks successfully against real-world classifiers hosted by Clarifai. Further, we evaluate black-box attacks against state-of-the-art defenses based on adversarial training and show that the Gradient Estimation attacks are very effective even against these defenses.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_10');
INSERT INTO `paper` VALUES (11082, 'Pre-gen Metrics: Predicting Caption Quality Metrics Without Generating Captions', 'Image captioning', 'Neural architectures', 'Evaluation metrics', '', '', 'Image caption generation systems are typically evaluated against reference outputs. We show that it is possible to predict output quality without generating the captions, based on the probability assigned by the neural model to the reference captions. Such pre-gen metrics are strongly correlated to standard evaluation metrics.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_10');
INSERT INTO `paper` VALUES (11083, 'Pre-training on Grayscale ImageNet Improves Medical Image Classification', 'Domain adaptation', 'Transfer learning', '', '', '', 'Deep learning is quickly becoming the de facto standard approach for solving a range of medical image analysis tasks. However, large medical image datasets appropriate for training deep neural network models from scratch are difficult to assemble due to privacy restrictions and expert ground truth requirements, with typical open source datasets ranging from hundreds to thousands of images. A standard approach to counteract limited-size medical datasets is to pre-train models on large datasets in other domains, such as ImageNet for classification of natural images, before fine-tuning on the specific medical task of interest. However, ImageNet contains color images, which introduces artefacts and inefficiencies into models that are intended for single-channel medical images. To address this issue, we pre-trained an Inception-V3 model on ImageNet after converting the images to grayscale through a common transformation. Surprisingly, these models do not show a significant degradation in performance on the original ImageNet classification task, suggesting that color is not a critical feature of natural image classification. Furthermore, models pre-trained on grayscale ImageNet outperformed color ImageNet models in terms of both speed and accuracy when refined on disease classification from chest X-ray images.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_37');
INSERT INTO `paper` VALUES (11084, 'Predicting Action Tubes', '', '', '', '', '', 'In this work, we present a method to predict an entire ‘action tube’ (a set of temporally linked bounding boxes) in a trimmed video just by observing a smaller subset of it. Predicting where an action is going to take place in the near future is essential to many computer vision based applications such as autonomous driving or surgical robotics. Importantly, it has to be done in real-time and in an online fashion. We propose a Tube Prediction network (TPnet) which jointly predicts the past, present and future bounding boxes along with their action classification scores. At test time TPnet is used in a (temporal) sliding window setting, and its predictions are put into a tube estimation framework to construct/predict the video long action tubes not only for the observed part of the video but also for the unobserved part. Additionally, the proposed action tube predictor helps in completing action tubes for unobserved segments of the video. We quantitatively demonstrate the latter ability, and the fact that TPnet improves state-of-the-art detection performance, on one of the standard action detection benchmarks - J-HMDB-21 dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_11');
INSERT INTO `paper` VALUES (11085, 'Predicting Future Instance Segmentation by Forecasting Convolutional Features', 'Video prediction', 'Instance segmentation', 'Deep learning', 'Convolutional neural networks', '', 'Anticipating future events is an important prerequisite towards intelligent behavior. Video forecasting has been studied as a proxy task towards this goal. Recent work has shown that to predict semantic segmentation of future frames, forecasting at the semantic level is more effective than forecasting RGB frames and then segmenting these. In this paper we consider the more challenging problem of future instance segmentation, which additionally segments out individual objects. To deal with a varying number of output labels per image, we develop a predictive model in the space of fixed-sized convolutional features of the Mask R-CNN instance segmentation model. We apply the “detection head” of Mask R-CNN on the predicted features to produce the instance segmentation of future frames. Experiments show that this approach significantly improves over strong baselines based on optical flow and repurposed instance segmentation architectures.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_36');
INSERT INTO `paper` VALUES (11086, 'Predicting Gaze in Egocentric Video by Learning Task-Dependent Attention Transition', 'Gaze prediction', 'Egocentric video', 'Attention transition', '', '', 'We present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipulation tasks. Our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction. In particular, the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_46');
INSERT INTO `paper` VALUES (11087, 'Predicting Muscle Activity and Joint Angle from Skin Shape', 'Skin shape', 'Muscle activity', 'Joint angle', '', '', 'Muscle of human body can be a clue to recognize the behavior and intention of a person. If the muscle activity is measured only by visual observation, it is useful to estimate the state of the muscle. In this paper, a method of predicting muscle activity and joint angle of human body from skin shape is proposed. Since the muscle activity and the joint angle affect the skin shape, the both factors should be considered simultaneously. The proposed method is a learning-based approach that uses the data set of the skin shape, the muscle activity and the joint angle. It trains a linear regressor for predicting muscle activity and joint angle from skin shape. The deformation of skin shape is calculated as the feature in the active regions, which are extracted from the training data and limits the regions of the skin shape that contribute to the prediction. We acquired a lower limb with simple motion to consider the small number of factors in this paper. In the experiment, the muscle activity and joint angle are predicted even in the case that the both factors change simultaneously. The skin regions that contributes to prediction are given as the result of learning, and the distribution is reasonable from the viewpoint of biomechanics.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_30');
INSERT INTO `paper` VALUES (11088, 'Probabilistic Video Generation Using Holistic Attribute Control', '', '', '', '', '', 'Videos express highly structured spatio-temporal patterns of visual data. A video can be thought of as being governed by two factors: (i) temporally invariant (e.g., person identity), or slowly varying (e.g., activity), attribute-induced appearance, encoding the persistent content of each frame, and (ii) an inter-frame motion or scene dynamics (e.g., encoding evolution of the person executing the action). Based on this intuition, we propose a generative framework for video generation and future prediction. The proposed framework generates a video (short clip) by decoding samples sequentially drawn from a latent space distribution into full video frames. Variational Autoencoders (VAEs) are used as a means of encoding/decoding frames into/from the latent space and RNN as a way to model the dynamics in the latent space. We improve the video generation consistency through temporally-conditional sampling and quality by structuring the latent space with attribute controls; ensuring that attributes can be both inferred and conditioned on during learning/generation. As a result, given attributes and/or the first frame, our model is able to generate diverse but highly consistent sets of video sequences, accounting for the inherent uncertainty in the prediction task. Experimental results on three challenging datasets, along with detailed comparison to the state-of-the-art, verify effectiveness of the framework.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_28');
INSERT INTO `paper` VALUES (11089, 'Product Quantization Network for Fast Image Retrieval', '', '', '', '', '', 'Product quantization has been widely used in fast image retrieval due to its effectiveness of coding high-dimensional visual features. By extending the hard assignment to soft assignment, we make it feasible to incorporate the product quantization as a layer of a convolutional neural network and propose our product quantization network. Meanwhile, we come up with a novel asymmetric triplet loss, which effectively boosts the retrieval accuracy of the proposed product quantization network based on asymmetric similarity. Through the proposed product quantization network, we can obtain a discriminative and compact image representation in an end-to-end manner, which further enables a fast and accurate image retrieval. Comprehensive experiments conducted on public benchmark datasets demonstrate the state-of-the-art performance of the proposed product quantization network.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_12');
INSERT INTO `paper` VALUES (11090, 'Programmable Triangulation Light Curtains', 'Computational imaging', 'Proximity sensors', '', '', '', 'A vehicle on a road or a robot in the field does not need a full-featured 3D depth sensor to detect potential collisions or monitor its blind spot. Instead, it needs to only monitor if any object comes within its near proximity which is an easier task than full depth scanning. We introduce a novel device that monitors the presence of objects on a virtual shell near the device, which we refer to as a light curtain. Light curtains offer a light-weight, resource-efficient and programmable approach to proximity awareness for obstacle avoidance and navigation. They also have additional benefits in terms of improving visibility in fog as well as flexibility in handling light fall-off. Our prototype for generating light curtains works by rapidly rotating a line sensor and a line laser, in synchrony. The device is capable of generating light curtains of various shapes with a range of 20–30 m in sunlight (40 m under cloudy skies and 50 m indoors) and adapts dynamically to the demands of the task. We analyze properties of light curtains and various approaches to optimize their thickness as well as power requirements. We showcase the potential of light curtains using a range of real-world scenarios.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_2');
INSERT INTO `paper` VALUES (11091, 'Progressive Neural Architecture Search', '', '', '', '', '', 'We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_2');
INSERT INTO `paper` VALUES (11092, 'Progressive Structure from Motion', 'Structure From Motion (SfM)', 'Incremental Pipeline', 'Viewgraphs', 'Global Pipelines', 'Reconstruction Pipeline', 'Structure from Motion or the sparse 3D reconstruction out of individual photos is a long studied topic in computer vision. Yet none of the existing reconstruction pipelines fully addresses a progressive scenario where images are only getting available during the reconstruction process and intermediate results are delivered to the user. Incremental pipelines are capable of growing a 3D model but often get stuck in local minima due to wrong (binding) decisions taken based on incomplete information. Global pipelines on the other hand need the access to the complete viewgraph and are not capable of delivering intermediate results. In this paper we propose a new reconstruction pipeline working in a progressive manner rather than in a batch processing scheme. The pipeline is able to recover from failed reconstructions in early stages, avoids to take binding decisions, delivers a progressive output and yet maintains the capabilities of existing pipelines. We demonstrate and evaluate our method on diverse challenging public and dedicated datasets including those with highly symmetric structures and compare to the state of the art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_2');
INSERT INTO `paper` VALUES (11093, 'Propagating LSTM: 3D Pose Estimation Based on Joint Interdependency', '3D human pose estimation', 'Joint interdependency (JI)', 'Long short-term memory (LSTM)', 'Propagating LSTM networks (p-LSTMs)', '', 'We present a novel 3D pose estimation method based on joint interdependency (JI) for acquiring 3D joints from the human pose of an RGB image. The JI incorporates the body part based structural connectivity of joints to learn the high spatial correlation of human posture on our method. Towards this goal, we propose a new long short-term memory (LSTM)-based deep learning architecture named propagating LSTM networks (p-LSTMs), where each LSTM is connected sequentially to reconstruct 3D depth from the centroid to edge joints through learning the intrinsic JI. In the first LSTM, the seed joints of 3D pose are created and reconstructed into the whole-body joints through the connected LSTMs. Utilizing the p-LSTMs, we achieve the higher accuracy of about 11.2% than state-of-the-art methods on the largest publicly available database. Importantly, we demonstrate that the JI drastically reduces the structural errors at body edges, thereby leads to a significant improvement.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_8');
INSERT INTO `paper` VALUES (11094, 'Proximal Dehaze-Net: A Prior Learning-Based Deep Network for Single Image Dehazing', 'Single image dehazing', 'Prior learning', 'Deep neural network', '', '', 'Photos taken in hazy weather are usually covered with white masks and often lose important details. In this paper, we propose a novel deep learning approach for single image dehazing by learning dark channel and transmission priors. First, we build an energy model for dehazing using dark channel and transmission priors and design an iterative optimization algorithm using proximal operators for these two priors. Second, we unfold the iterative algorithm to be a deep network, dubbed as proximal dehaze-net, by learning the proximal operators using convolutional neural networks. Our network combines the advantages of traditional prior-based dehazing methods and deep learning methods by incorporating haze-related prior learning into deep network. Experiments show that our method achieves state-of-the-art performance for single image dehazing.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_43');
INSERT INTO `paper` VALUES (11095, 'Proxy Clouds for Live RGB-D Stream Processing and Consolidation', 'RGB-D stream', '3D geometric primitives', 'Data reinforcement', 'Depth improvement', 'Online processing', 'We propose a new multiplanar superstructure for unified real-time processing of RGB-D data. Modern RGB-D sensors are widely used for indoor 3D capture, with applications ranging from modeling to robotics, through augmented reality. Nevertheless, their use is limited by their low resolution, with frames often corrupted with noise, missing data and temporal inconsistencies. Our approach, named Proxy Clouds, consists in generating and updating through time a single set of compact local statistics parameterized over detected planar proxies, which are fed from raw RGB-D data. Proxy Clouds provide several processing primitives, which improve the quality of the RGB-D stream on-the-fly or lighten further operations. Experimental results confirm that our light weight analysis framework copes well with embedded execution as well as moderate memory and computational capabilities compared to state-of-the-art methods. Processing of RGB-D data with Proxy Clouds includes noise and temporal flickering removal, hole filling and resampling. As a substitute of the observed scene, our proxy cloud can additionally be applied to compression and scene reconstruction. We present experiments performed with our framework in indoor scenes of different natures within a recent open RGB-D dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_16');
INSERT INTO `paper` VALUES (11096, 'PS-FCN: A Flexible Learning Framework for Photometric Stereo', 'Photometric stereo', 'Convolutional neural network', '', '', '', 'This paper addresses the problem of photometric stereo for non-Lambertian surfaces. Existing approaches often adopt simplified reflectance models to make the problem more tractable, but this greatly hinders their applications on real-world objects. In this paper, we propose a deep fully convolutional network, called PS-FCN, that takes an arbitrary number of images of a static object captured under different light directions with a fixed camera as input, and predicts a normal map of the object in a fast feed-forward pass. Unlike the recently proposed learning based method, PS-FCN does not require a pre-defined set of light directions during training and testing, and can handle multiple images and light directions in an order-agnostic manner. Although we train PS-FCN on synthetic data, it can generalize well on real datasets. We further show that PS-FCN can be easily extended to handle the problem of uncalibrated photometric stereo. Extensive experiments on public real datasets show that PS-FCN outperforms existing approaches in calibrated photometric stereo, and promising results are achieved in uncalibrated scenario, clearly demonstrating its effectiveness.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_1');
INSERT INTO `paper` VALUES (11097, 'PSANet: Point-wise Spatial Attention Network for Scene Parsing', 'Point-wise spatial attention', 'Bi-direction information flow', 'Adaptive context aggregation', 'Scene parsing', 'Semantic segmentation', 'We notice information flow in convolutional neural networks is restricted inside local neighborhood regions due to the physical design of convolutional filters, which limits the overall understanding of complex scenes. In this paper, we propose the point-wise spatial attention network (PSANet) to relax the local neighborhood constraint. Each position on the feature map is connected to all the other ones through a self-adaptively learned attention mask. Moreover, information propagation in bi-direction for scene parsing is enabled. Information at other positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of other ones. Our proposed approach achieves top performance on various competitive scene parsing datasets, including ADE20K, PASCAL VOC 2012 and Cityscapes, demonstrating its effectiveness and generality.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_17');
INSERT INTO `paper` VALUES (11098, 'PSDF Fusion: Probabilistic Signed Distance Function for On-the-fly 3D Data Fusion and Scene Reconstruction', 'Signed Distance Function', 'Bayesian updating', '', '', '', 'We propose a novel 3D spatial representation for data fusion and scene reconstruction. Probabilistic Signed Distance Function (Probabilistic SDF, PSDF) is proposed to depict uncertainties in the 3D space. It is modeled by a joint distribution describing SDF value and its inlier probability, reflecting input data quality and surface geometry. A hybrid data structure involving voxel, surfel, and mesh is designed to fully exploit the advantages of various prevalent 3D representations. Connected by PSDF, these components reasonably cooperate in a consistent framework. Given sequential depth measurements, PSDF can be incrementally refined with less ad hoc parametric Bayesian updating. Supported by PSDF and the efficient 3D data representation, high-quality surfaces can be extracted on-the-fly, and in return contribute to reliable data fusion using the geometry information. Experiments demonstrate that our system reconstructs scenes with higher model quality and lower redundancy, and runs faster than existing online mesh generation systems.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_43');
INSERT INTO `paper` VALUES (11099, 'Pyramid Dilated Deeper ConvLSTM for Video Salient Object Detection', '', '', '', '', '', 'This paper proposes a fast video salient object detection model, based on a novel recurrent network architecture, named Pyramid Dilated Bidirectional ConvLSTM (PDB-ConvLSTM). A Pyramid Dilated Convolution (PDC) module is first designed for simultaneously extracting spatial features at multiple scales. These spatial features are then concatenated and fed into an extended Deeper Bidirectional ConvLSTM (DB-ConvLSTM) to learn spatiotemporal information. Forward and backward ConvLSTM units are placed in two layers and connected in a cascaded way, encouraging information flow between the bi-directional streams and leading to deeper feature extraction. We further augment DB-ConvLSTM with a PDC-like structure, by adopting several dilated DB-ConvLSTMs to extract multi-scale spatiotemporal information. Extensive experimental results show that our method outperforms previous video saliency models in a large margin, with a real-time speed of 20 fps on a single GPU. With unsupervised video object segmentation as an example application, the proposed model (with a CRF-based post-process) achieves state-of-the-art results on two popular benchmarks, well demonstrating its superior performance and high applicability.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_44');
INSERT INTO `paper` VALUES (11100, 'PyramidBox: A Context-Assisted Single Shot Face Detector', 'Face detection', 'Context', 'Single shot', 'PyramidBox', '', 'Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named PyramidBox to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: https://github.com/PaddlePaddle/models/tree/develop/fluid/face_detection.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_49');
INSERT INTO `paper` VALUES (11101, 'Quadtree Convolutional Neural Networks', 'Quadtree', 'Neural network', 'Sparse convolution', '', '', 'This paper presents a Quadtree Convolutional Neural Network (QCNN) for efficiently learning from image datasets representing sparse data such as handwriting, pen strokes, freehand sketches, etc. Instead of storing the sparse sketches in regular dense tensors, our method decomposes and represents the image as a linear quadtree that is only refined in the non-empty portions of the image. The actual image data corresponding to non-zero pixels is stored in the finest nodes of the quadtree. Convolution and pooling operations are restricted to the sparse pixels, leading to better efficiency in computation time as well as memory usage. Specifically, the computational and memory costs in QCNN grow linearly in the number of non-zero pixels, as opposed to traditional CNNs where the costs are quadratic in the number of pixels. This enables QCNN to learn from sparse images much faster and process high resolution images without the memory constraints faced by traditional CNNs. We study QCNN on four sparse image datasets for sketch classification and simplification tasks. The results show that QCNN can obtain comparable accuracy with large reduction in computational and memory costs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_34');
INSERT INTO `paper` VALUES (11102, 'Quantifying the Amount of Visual Information Used by Neural Caption Generators', 'Image captioning', 'Sensitivity analysis', 'Explainable AI', '', '', 'This paper addresses the sensitivity of neural image caption generators to their visual input. A sensitivity analysis and omission analysis based on image foils is reported, showing that the extent to which image captioning architectures retain and are sensitive to visual information varies depending on the type of word being generated and the position in the caption as a whole. We motivate this work in the context of broader goals in the field to achieve more explainability in AI.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_11');
INSERT INTO `paper` VALUES (11103, 'Quantization Mimic: Towards Very Tiny CNN for Object Detection', 'Model acceleration', 'Model compression', 'Quantization', 'Mimic', 'Object detection', 'In this paper, we propose a simple and general framework for training very tiny CNNs (e.g. VGG with the number of channels reduced to \\(\\frac{1}{32}\\)) for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this feature of the quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. The quantization operation can help student network to better match the feature maps from teacher network. To evaluate our approach, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_17');
INSERT INTO `paper` VALUES (11104, 'Quantized Densely Connected U-Nets for Efficient Landmark Localization', 'Facial Landmark Localization', 'Connection Density', 'Human Pose Estimation', 'High-precision Operations', 'Memory-efficient Implementation', 'In this paper, we propose quantized densely connected U-Nets for efficient visual landmark localization. The idea is that features of the same semantic meanings are globally reused across the stacked U-Nets. This dense connectivity largely improves the information flow, yielding improved localization accuracy. However, a vanilla dense design would suffer from critical efficiency issue in both training and testing. To solve this problem, we first propose order-K dense connectivity to trim off long-distance shortcuts; then, we use a memory-efficient implementation to significantly boost the training efficiency and investigate an iterative refinement that may slice the model size in half. Finally, to reduce the memory consumption and high precision operations both in training and testing, we further quantize weights, inputs, and gradients of our localization network to low bit-width numbers. We validate our approach in two tasks: human pose estimation and face alignment. The results show that our approach achieves state-of-the-art localization accuracy, but using \\(\\sim \\)70% fewer parameters, \\(\\sim \\)98% less model size and saving \\(\\sim \\)32\\(\\times \\) training memory compared with other benchmark localizers.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_21');
INSERT INTO `paper` VALUES (11105, 'Quaternion Convolutional Neural Networks', 'Quaternion convolutional neural network', 'Quaternion-based layers', 'Color image denoising', 'Color image classification', '', 'Neural networks in the real domain have been studied for a long time and achieved promising results in many vision tasks for recent years. However, the extensions of the neural network models in other number fields and their potential applications are not fully-investigated yet. Focusing on color images, which can be naturally represented as quaternion matrices, we propose a quaternion convolutional neural network (QCNN) model to obtain more representative features. In particular, we re-design the basic modules like convolution layer and fully-connected layer in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks. Moreover, these modules are compatible with almost all deep learning techniques and can be plugged into traditional CNNs easily. We test our QCNN models in both color image classification and denoising tasks. Experimental results show that they outperform the real-valued CNNs with same structures.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_39');
INSERT INTO `paper` VALUES (11106, 'Question Type Guided Attention in Visual Question Answering', 'Visual question answering', 'Attention', 'Question type', 'Feature selection', 'Multi-task', 'Visual Question Answering (VQA) requires integration of feature maps with drastically different structures. Image descriptors have structures at multiple spatial scales, while lexical inputs inherently follow a temporal sequence and naturally cluster into semantically different question types. A lot of previous works use complex models to extract feature representations but neglect to use high-level information summary such as question types in learning. In this work, we propose Question Type-guided Attention (QTA). It utilizes the information of question type to dynamically balance between bottom-up and top-down visual features, respectively extracted from ResNet and Faster R-CNN networks. We experiment with multiple VQA architectures with extensive input ablation studies over the TDIUC dataset and show that QTA systematically improves the performance by more than 5% across multiple question type categories such as “Activity Recognition”, “Utility” and “Counting” on TDIUC dataset compared to the state-of-art. By adding QTA on the state-of-art model MCB, we achieve 3% improvement in overall accuracy. Finally, we propose a multi-task extension to predict question types which generalizes QTA to applications that lack question type, with a minimal performance loss.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_10');
INSERT INTO `paper` VALUES (11107, 'Question-Guided Hybrid Convolution for Visual Question Answering', 'VQA', 'Dynamic parameter prediction', 'Group convolution', '', '', 'In this paper, we propose a novel Question-Guided Hybrid Convolution (QGHC) network for Visual Question Answering (VQA). Most state-of-the-art VQA methods fuse the high-level textual and visual features from the neural network and abandon the visual spatial information when learning multi-modal features. To address these problems, question-guided kernels generated from the input question are designed to convolute with visual features for capturing the textual and visual relationship in the early stage. The question-guided convolution can tightly couple the textual and visual information but also introduce more parameters when learning kernels. We apply the group convolution, which consists of question-independent kernels and question-dependent kernels, to reduce the parameter size and alleviate over-fitting. The hybrid convolution can generate discriminative multi-modal features with fewer parameters. The proposed approach is also complementary to existing bilinear pooling fusion and attention based VQA methods. By integrating with them, our method could further boost the performance. Experiments on VQA datasets validate the effectiveness of QGHC.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_29');
INSERT INTO `paper` VALUES (11108, 'r2p2: A ReparameteRized Pushforward Policy for Diverse, Precise Generative Path Forecasting', 'Trajectory forecasting', 'Imitation learning', 'Generative modeling', 'Self-driving vehicles', '', 'We propose a method to forecast a vehicle’s ego-motion as a distribution over spatiotemporal paths, conditioned on features (e.g., from LIDAR and images) embedded in an overhead map. The method learns a policy inducing a distribution over simulated trajectories that is both “diverse” (produces most of the likely paths) and “precise” (mostly produces likely paths). This balance is achieved through minimization of a symmetrized cross-entropy between the distribution and demonstration data. By viewing the simulated-outcome distribution as the pushforward of a simple distribution under a simulation operator, we obtain expressions for the cross-entropy metrics that can be efficiently evaluated and differentiated, enabling stochastic-gradient optimization. We propose concrete policy architectures for this model, discuss our evaluation metrics relative to previously-used degenerate metrics, and demonstrate the superiority of our method relative to state-of-the-art methods in both the Kitti dataset and a similar but novel and larger real-world dataset explicitly designed for the vehicle forecasting domain.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_47');
INSERT INTO `paper` VALUES (11109, 'RAMCIP Robot: A Personal Robotic Assistant; Demonstration of a Complete Framework', 'Robotic Assistant', 'Integrated framework', 'Manipulation', 'Navigation', 'Perception', 'At the last decades, personal domestic robots are considered as the future for tackling the societal challenge inherent in the growing elderly population. Ageing is typically associated with physical and cognitive decline, altering the way an older person moves around the house, manipulates objects and senses the home environment. This paper aims to demonstrate the RAMCIP robot, which is a Robotic Assistant for patients with Mild Cognitive Impairments (MCI), suitable to provide its services in domestic environments. The use cases that the robot addresses are described herein outlining the necessary requirements that set the basis for the software and hardware architectural components. A short description of the integrated cognitive, perception, manipulation and navigation capabilities of the robot is provided. Robot’s autonomy is enabled through a specific decision making and task planning framework. The robot has been evaluated in ten real home environments of real MCI users exhibiting remarkable performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_7');
INSERT INTO `paper` VALUES (11110, 'Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices', 'Perceptual image enhancement', 'Global feature vector', 'Range scaling layer', '', '', 'Perceptual image enhancement on mobile devices—smart phones in particular—has drawn increasing industrial efforts and academic interests recently. Compared to digital single-lens reflex (DSLR) cameras, cameras on smart phones typically capture lower-quality images due to various hardware constraints. Without additional information, it is a challenging task to enhance the perceptual quality of a single image especially when the computation has to be done on mobile devices. In this paper we present a novel deep learning based approach—the Range Scaling Global U-Net (RSGUNet)—for perceptual image enhancement on mobile devices. Besides the U-Net structure that exploits image features at different resolutions, proposed RSGUNet learns a global feature vector as well as a novel range scaling layer that alleviate artifacts in the enhanced images. Extensive experiments show that the RSGUNet not only outputs enhanced images with higher subjective and objective quality, but also takes less inference time. Our proposal wins the 1st place by a great margin in track B of the Perceptual Image Enhancement on Smartphones Challenge (PRIM2018). Code is available at https://github.com/MTlab/ECCV-PIRM2018.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_15');
INSERT INTO `paper` VALUES (11111, 'RCAA: Relational Context-Aware Agents for Person Search', 'Person search', 'Relational network', '', '', '', 'We aim to search for a target person from a gallery of whole scene images for which the annotations of pedestrian bounding boxes are unavailable. Previous approaches to this problem have relied on a pedestrian proposal net, which may generate redundant proposals and increase the computational burden. In this paper, we address this problem by training relational context-aware agents which learn the actions to localize the target person from the gallery of whole scene images. We incorporate the relational spatial and temporal contexts into the framework. Specifically, we propose to use the target person as the query in the query-dependent relational network. The agent determines the best action to take at each time step by simultaneously considering the local visual information, the relational and temporal contexts, together with the target person. To validate the performance of our approach, we conduct extensive experiments on the large-scale Person Search benchmark dataset and achieve significant improvements over the compared approaches. It is also worth noting that the proposed model even performs better than traditional methods with perfect pedestrian detectors.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_6');
INSERT INTO `paper` VALUES (11112, 'Real-Time Dynamic Object Detection for Autonomous Driving Using Prior 3D-Maps', 'Prior maps', '3D obstacles', 'Inlier rejection', '', '', 'Lidar has become an essential sensor for autonomous driving as it provides reliable depth estimation. Lidar is also the primary sensor used in building 3D maps which can be used even in the case of low-cost systems which do not use Lidar. Computation on Lidar point clouds is intensive as it requires processing of millions of points per second. Additionally there are many subsequent tasks such as clustering, detection, tracking and classification which makes real-time execution challenging. In this paper, we discuss real-time dynamic object detection algorithms which leverages previously mapped Lidar point clouds to reduce processing. The prior 3D maps provide a static background model and we formulate dynamic object detection as a background subtraction problem. Computation and modeling challenges in the mapping and online execution pipeline are described. We propose a rejection cascade architecture to subtract road regions and other 3D regions separately. We implemented an initial version of our proposed algorithm and evaluated the accuracy on CARLA simulator.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_35');
INSERT INTO `paper` VALUES (11113, 'Real-Time Embedded Computer Vision on UAVs', 'Computer vision', 'Real-time', 'UAVs', 'Embedded hardware', 'Deep learning', 'In this paper we present an overview of the contributed work presented at the UAVision2018 ECCV workshop. This workshop focused on real-time image processing on-board of Unmanned Aerial Vehicles (UAVs). For such applications the computational complexity of state-of-the-art computer vision algorithms often conflicts with the need for real-time operation and the extreme resource limitations of the hardware. Apart from a summary of the accepted workshop papers, this work also aims to identify common challenges and concerns which were addressed by multiple authors during the workshop, and their proposed solutions.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_1');
INSERT INTO `paper` VALUES (11114, 'Real-Time Hair Rendering Using Sequential Adversarial Networks', 'Hair rendering', 'GAN', '', '', '', 'We present an adversarial network for rendering photorealistic hair as an alternative to conventional computer graphics pipelines. Our deep learning approach does not require low-level parameter tuning nor ad-hoc asset design. Our method simply takes a strand-based 3D hair model as input and provides intuitive user-control for color and lighting through reference images. To handle the diversity of hairstyles and its appearance complexity, we disentangle hair structure, color, and illumination properties using a sequential GAN architecture and a semi-supervised training approach. We also introduce an intermediate edge activation map to orientation field conversion step to ensure a successful CG-to-photoreal transition, while preserving the hair structures of the original input data. As we only require a feed-forward pass through the network, our rendering performs in real-time. We demonstrate the synthesis of photorealistic hair images on a wide range of intricate hairstyles and compare our technique with state-of-the-art hair rendering methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_7');
INSERT INTO `paper` VALUES (11115, 'Real-Time MDNet', 'Visual tracking', 'Multi-domain learning', 'RoIAlign', 'Instance embedding loss', '', 'We present a fast and accurate visual tracking algorithm based on the multi-domain convolutional neural network (MDNet). The proposed approach accelerates feature extraction procedure and learns more discriminative models for instance classification; it enhances representation quality of target and background by maintaining a high resolution feature map with a large receptive field per activation. We also introduce a novel loss term to differentiate foreground instances across multiple domains and learn a more discriminative embedding of target objects with similar semantics. The proposed techniques are integrated into the pipeline of a well known CNN-based visual tracking algorithm, MDNet. We accomplish approximately 25 times speed-up with almost identical accuracy compared to MDNet. Our algorithm is evaluated in multiple popular tracking benchmark datasets including OTB2015, UAV123, and TempleColor, and outperforms the state-of-the-art real-time tracking methods consistently even without dataset-specific parameter tuning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_6');
INSERT INTO `paper` VALUES (11116, 'Real-Time Point Cloud Alignment for Vehicle Localization in a High Resolution 3D Map', 'Lidar', 'Point cloud', 'Registration', 'Scene understanding', '', 'In this paper we introduce a Lidar based real time and accurate self localization approach for self driving vehicles (SDV) in high resolution 3D point cloud maps of the environment obtained through Mobile Laser Scanning (MLS). Our solution is able to robustly register the sparse point clouds of the SDVs to the dense MLS point cloud data, starting from a GPS based initial position estimation of the vehicle. The main steps of the method are robust object extraction and transformation estimation based on multiple keypoints extracted from the objects, and additional semantic information derived from the MLS based map. We tested our approach on roads with heavy traffic in the downtown of a large city with large GPS positioning errors, and showed that the proposed method enhances the matching accuracy with an order of magnitude. Comparative tests are provided with various keypoint selection strategies, and against a state-of-the-art technique.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_13');
INSERT INTO `paper` VALUES (11117, 'Real-Time ‘Actor-Critic’ Tracking', 'Visual tracking', 'Real-time tracking', 'Reinforcement learning', '', '', 'In this work, we propose a novel tracking algorithm with real-time performance based on the ‘Actor-Critic’ framework. This framework consists of two major components: ‘Actor’ and ‘Critic’. The ‘Actor’ model aims to infer the optimal choice in a continuous action space, which directly makes the tracker move the bounding box to the object’s location in the current frame. For offline training, the ‘Critic’ model is introduced to form a ‘Actor-Critic’ framework with reinforcement learning and outputs a Q-value to guide the learning process of both ‘Actor’ and ‘Critic’ deep networks. Then, we modify the original deep deterministic policy gradient algorithm to effectively train our ‘Actor-Critic’ model for the tracking task. For online tracking, the ‘Actor’ model provides a dynamic search strategy to locate the tracked object efficiently and the ‘Critic’ model acts as a verification module to make our tracker more robust. To the best of our knowledge, this work is the first attempt to exploit the continuous action and ‘Actor-Critic’ framework for visual tracking. Extensive experimental results on popular benchmarks demonstrate that the proposed tracker performs favorably against many state-of-the-art methods, with real-time performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_20');
INSERT INTO `paper` VALUES (11118, 'Real-to-Virtual Domain Unification for End-to-End Autonomous Driving', 'Domain unification', 'End-to-end autonomous driving', '', '', '', 'In the spectrum of vision-based autonomous driving, vanilla end-to-end models are not interpretable and suboptimal in performance, while mediated perception models require additional intermediate representations such as segmentation masks or detection bounding boxes, whose annotation can be prohibitively expensive as we move to a larger scale. More critically, all prior works fail to deal with the notorious domain shift if we were to merge data collected from different sources, which greatly hinders the model generalization ability. In this work, we address the above limitations by taking advantage of virtual data collected from driving simulators, and present DU-drive, an unsupervised real-to-virtual domain unification framework for end-to-end autonomous driving. It first transforms real driving data to its less complex counterpart in the virtual domain, and then predicts vehicle control commands from the generated virtual image. Our framework has three unique advantages: (1) it maps driving data collected from a variety of source distributions into a unified domain, effectively eliminating domain shift; (2) the learned virtual representation is simpler than the input real image and closer in form to the “minimum sufficient statistic” for the prediction task, which relieves the burden of the compression phase while optimizing the information bottleneck tradeoff and leads to superior prediction performance; (3) it takes advantage of annotated virtual data which is unlimited and free to obtain. Extensive experiments on two public driving datasets and two driving simulators demonstrate the performance superiority and interpretive capability of DU-drive.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_33');
INSERT INTO `paper` VALUES (11119, 'Realtime Time Synchronized Event-Based Stereo', 'Event cameras', 'Stereo depth estimation', '3D computer vision', '', '', 'In this work, we propose a novel event based stereo method which addresses the problem of motion blur for a moving event camera. Our method uses the velocity of the camera and a range of disparities to synchronize the positions of the events, as if they were captured at a single point in time. We represent these events using a pair of novel time synchronized event disparity volumes, which we show remove motion blur for pixels at the correct disparity in the volume, while further blurring pixels at the wrong disparity. We then apply a novel matching cost over these time synchronized event disparity volumes, which both rewards similarity between the volumes while penalizing blurriness. We show that our method outperforms more expensive, smoothing based event stereo methods, by evaluating on the Multi Vehicle Stereo Event Camera dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_27');
INSERT INTO `paper` VALUES (11120, 'Recent Advances at the Brain-Driven Computer Vision Workshop 2018', 'Brain-Driven Computer Vision', 'Biologically-inspired machine learning', '', '', '', 'The 1\\(^\\text {st}\\) edition of the Brain-Driven Computer Vision Workshop, held in Munich in conjunction with the European Conference on Computer Vision 2018, aimed at attracting, promoting and inspiring research on paradigms, methods and tools for computer vision driven or inspired by the human brain. While successful, in terms of the quality of received submissions and audience present at the event, the workshop emphasized some of the factors that currently limit research in this field. In this report, we discuss the success points of the workshop, the characteristics of the presented works, and our considerations on the state of current research and future directions of research in this topic.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_37');
INSERT INTO `paper` VALUES (11121, 'Receptive Field Block Net for Accurate and Fast Object Detection', 'Real-time object detection', 'Receptive Field Block (RFB)', '', '', '', 'Current top-performing object detectors depend on deep CNN backbones, such as ResNet-101 and Inception, benefiting from their powerful feature representations but suffering from high computational costs. Conversely, some lightweight model based detectors fulfil real time processing, while their accuracies are often criticized. In this paper, we explore an alternative to build a fast and accurate detector by strengthening lightweight features using a hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs) in human visual systems, we propose a novel RF Block (RFB) module, which takes the relationship between the size and eccentricity of RFs into account, to enhance the feature discriminability and robustness. We further assemble RFB to the top of SSD, constructing the RFB Net detector. To evaluate its effectiveness, experiments are conducted on two major benchmarks and the results show that RFB Net is able to reach the performance of advanced very deep detectors while keeping the real-time speed. Code is available at https://github.com/ruinmessi/RFBNet.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_24');
INSERT INTO `paper` VALUES (11122, 'Recognition in Terra Incognita', 'Recognition', 'Transfer learning', 'Domain adaptation', 'Context', 'Dataset', 'It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_28');
INSERT INTO `paper` VALUES (11123, 'Recognizing People in Blind Spots Based on Surrounding Behavior', 'Action recognition', 'Convolutional Neural Networks', '', '', '', 'Recent advances in computer vision have achieved remarkable performance improvements. These technologies mainly focus on recognition of visible targets. However, there are many invisible targets in blind spots in real situations. Humans may be able to recognize such invisible targets based on contexts (e.g. visible human behavior and environments) around the targets, and used such recognition to predict situations in blind spots on a daily basis. As the first step towards recognizing targets in blind spots captured in videos, we propose a convolutional neural network that recognizes whether or not there is a person in a blind spot. Based on the experiments that used the volleyball dataset, which includes various interactions of players, with artificial occlusions, our proposed method achieved 90.3% accuracy in the recognition.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_42');
INSERT INTO `paper` VALUES (11124, 'Reconstruction-Based Pairwise Depth Dataset for Depth Image Enhancement Using CNN', 'Depth image dataset', 'Depth image enhancement', '3D reconstruction', 'Deep learning', 'Laplacian pyramid network', 'Raw depth images captured by consumer depth cameras suffer from noisy and missing values. Despite the success of CNN-based image processing on color image restoration, similar approaches for depth enhancement have not been much addressed yet because of the lack of raw-clean pairwise dataset. In this paper, we propose a pairwise depth image dataset generation method using dense 3D surface reconstruction with a filtering method to remove low quality pairs. We also present a multi-scale Laplacian pyramid based neural network and structure preserving loss functions to progressively reduce the noise and holes from coarse to fine scales. Experimental results show that our network trained with our pairwise dataset can enhance the input depth images to become comparable with 3D reconstructions obtained from depth streams, and can accelerate the convergence of dense 3D reconstruction results.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_26');
INSERT INTO `paper` VALUES (11125, 'Recovering 3D Planes from a Single Image via Convolutional Neural Networks', '3D reconstruction', 'Plane segmentation', 'Deep learning', '', '', 'In this paper, we study the problem of recovering 3D planar surfaces from a single image of man-made environment. We show that it is possible to directly train a deep neural network to achieve this goal. A novel plane structure-induced loss is proposed to train the network to simultaneously predict a plane segmentation map and the parameters of the 3D planes. Further, to avoid the tedious manual labeling process, we show how to leverage existing large-scale RGB-D dataset to train our network without explicit 3D plane annotations, and how to take advantage of the semantic labels come with the dataset for accurate planar and non-planar classification. Experiment results demonstrate that our method significantly outperforms existing methods, both qualitatively and quantitatively. The recovered planes could potentially benefit many important visual tasks such as vision-based navigation and human-robot interaction.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_6');
INSERT INTO `paper` VALUES (11126, 'Recovering 6D Object Pose: A Review and Multi-modal Analysis', '', '', '', '', '', 'A large number of studies analyse object detection and pose estimation at visual level in 2D, discussing the effects of challenges such as occlusion, clutter, texture, etc., on the performances of the methods, which work in the context of RGB modality. Interpreting the depth data, the study in this paper presents thorough multi-modal analyses. It discusses the above-mentioned challenges for full 6D object pose estimation in RGB-D images comparing the performances of several 6D detectors in order to answer the following questions: What is the current position of the computer vision community for maintaining “automation” in robotic manipulation? What next steps should the community take for improving “autonomy” in robotics while handling objects? Our findings include: (i) reasonably accurate results are obtained on textured-objects at varying viewpoints with cluttered backgrounds. (ii) Heavy existence of occlusion and clutter severely affects the detectors, and similar-looking distractors is the biggest challenge in recovering instances’ 6D. (iii) Template-based methods and random forest-based learning algorithms underlie object detection and 6D pose estimation. Recent paradigm is to learn deep discriminative feature representations and to adopt CNNs taking RGB images as input. (iv) Depending on the availability of large-scale 6D annotated depth datasets, feature representations can be learnt on these datasets, and then the learnt representations can be customized for the 6D problem.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_2');
INSERT INTO `paper` VALUES (11127, 'Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera', 'Human pose', 'Video', 'IMUs', 'Sensor fusion', '2D to 3D', 'In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26 mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW), a new dataset consisting of more than 51, 000 frames with accurate 3D pose in challenging sequences, including walking in the city, going up-stairs, having coffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_37');
INSERT INTO `paper` VALUES (11128, 'Recurrent Fusion Network for Image Captioning', 'Image captioning', 'Encoder-decoder framework', 'Recurrent fusion network (RFNet)', '', '', 'Recently, much advance has been made in image captioning, and an encoder-decoder framework has been adopted by all the state-of-the-art models. Under this framework, an input image is encoded by a convolutional neural network (CNN) and then translated into natural language with a recurrent neural network (RNN). The existing models counting on this framework employ only one kind of CNNs, e.g., ResNet or Inception-X, which describes the image contents from only one specific view point. Thus, the semantic meaning of the input image cannot be comprehensively understood, which restricts improving the performance. In this paper, to exploit the complementary information from multiple encoders, we propose a novel recurrent fusion network (RFNet) for the image captioning task. The fusion process in our model can exploit the interactions among the outputs of the image encoders and generate new compact and informative representations for the decoder. Experiments on the MSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets a new state-of-the-art for image captioning.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_31');
INSERT INTO `paper` VALUES (11129, 'Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining', 'Recurrent neural network', 'Squeeze and excitation block', 'Image deraining', '', '', 'Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. Codes and supplementary material are available at our project webpage: https://xialipku.github.io/RESCAN.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_16');
INSERT INTO `paper` VALUES (11130, 'Recurrent Tubelet Proposal and Recognition Networks for Action Detection', 'Action detection', 'Action recognition', '', '', '', 'Detecting actions in videos is a challenging task as video is an information intensive media with complex variations. Existing approaches predominantly generate action proposals for each individual frame or fixed-length clip independently, while overlooking temporal context across them. Such temporal contextual relations are vital for action detection as an action is by nature a sequence of movements. This motivates us to leverage the localized action proposals in previous frames when determining action regions in the current one. Specifically, we present a novel deep architecture called Recurrent Tubelet Proposal and Recognition (RTPR) networks to incorporate temporal context for action detection. The proposed RTPR consists of two correlated networks, i.e., Recurrent Tubelet Proposal (RTP) networks and Recurrent Tubelet Recognition (RTR) networks. The RTP initializes action proposals of the start frame through a Region Proposal Network and then estimates the movements of proposals in next frame in a recurrent manner. The action proposals of different frames are linked to form the tubelet proposals. The RTR capitalizes on a multi-channel architecture, where in each channel, a tubelet proposal is fed into a CNN plus LSTM to recurrently recognize action in the tubelet. We conduct extensive experiments on four benchmark datasets and demonstrate superior results over state-of-the-art methods. More remarkably, we obtain mAP of 98.6%, 81.3%, 77.9% and 22.3% with gains of 2.9%, 4.3%, 0.7% and 3.9% over the best competitors on UCF-Sports, J-HMDB, UCF-101 and AVA, respectively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_19');
INSERT INTO `paper` VALUES (11131, 'Recycle-GAN: Unsupervised Video Retargeting', 'Video Retargeting', 'Spatio-temporal Constraints', 'Adversarial Loss', 'Preservation Style', 'Origami Bird', 'We introduce a data-driven approach for unsupervised video retargeting that translates content from one domain to another while preserving the style native to a domain, i.e., if contents of John Oliver’s speech were to be transferred to Stephen Colbert, then the generated content/speech should be in Stephen Colbert’s style. Our approach combines both spatial and temporal information along with adversarial losses for content translation and style preservation. In this work, we first study the advantages of using spatiotemporal constraints over spatial constraints for effective retargeting. We then demonstrate the proposed approach for the problems where information in both space and time matters such as face-to-face translation, flower-to-flower, wind and cloud synthesis, sunrise and sunset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_8');
INSERT INTO `paper` VALUES (11132, 'RED: A Simple but Effective Baseline Predictor for the TrajNet Benchmark', 'Trajectory forecasting', 'Path prediction', 'Trajectory-based activity forecasting', '', '', 'In recent years, there is a shift from modeling the tracking problem based on Bayesian formulation towards using deep neural networks. Towards this end, in this paper the effectiveness of various deep neural networks for predicting future pedestrian paths are evaluated. The analyzed deep networks solely rely, like in the traditional approaches, on observed tracklets without human-human interaction information. The evaluation is done on the publicly available TrajNet benchmark dataset [39], which builds up a repository of considerable and popular datasets for trajectory prediction. We show how a Recurrent-Encoder with a Dense layer stacked on top, referred to as RED-predictor, is able to achieve top-rank at the TrajNet 2018 challenge compared to elaborated models. Further, we investigate failure cases and give explanations for observed phenomena, and give some recommendations for overcoming demonstrated shortcomings.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_13');
INSERT INTO `paper` VALUES (11133, 'ReenactGAN: Learning to Reenact Faces via Boundary Transfer', 'Face reenactment', 'Face generation', 'Face alignment', 'GAN', '', 'We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary person’s monocular video input to a target person’s video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source face’s boundary to the target’s boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model are publicly available on our project page (Project Page: https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_37');
INSERT INTO `paper` VALUES (11134, 'Reflecting on How Artworks Are Processed and Analyzed by Computer Vision', 'Computer vision', 'Art history', 'Critical reflection', 'Distant viewing', 'Close reading', 'The intersection between computer vision and art history has resulted in new ways of seeing, engaging and analyzing digital images. Innovative methods and tools have assisted with the evaluation of large datasets, performing tasks such as classification, object detection, image description and style transfer or assisting with a form and content analysis. At this point, in order to progress, past works and established practices must be revisited and evaluated on the ground of their usability for art history. This paper provides a reflection from an art historical perspective to point to erroneous assumptions and where improvements are still needed.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_49');
INSERT INTO `paper` VALUES (11135, 'RefocusGAN: Scene Refocusing Using a Single Image', 'Epsilon focus photography', 'Single image refocusing', '', '', '', 'Post-capture control of the focus position of an image is a useful photographic tool. Changing the focus of a single image involves the complex task of simultaneously estimating the radiance and the defocus radius of all scene points. We introduce RefocusGAN, a deblur-then-reblur approach to single image refocusing. We train conditional adversarial networks for deblurring and refocusing using wide-aperture images created from light-fields. By appropriately conditioning our networks with a focus measure, an in-focus image and a refocus control parameter \\(\\delta \\), we are able to achieve generic free-form refocusing over a single image.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_31');
INSERT INTO `paper` VALUES (11136, 'Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-identification', 'Person re-identification from depth', 'Reinforced temporal attention', 'Split-rate transfer', '', '', 'We address the problem of person re-identification from commodity depth sensors. One challenge for depth-based recognition is data scarcity. Our first contribution addresses this problem by introducing split-rate RGB-to-Depth transfer, which leverages large RGB datasets more effectively than popular fine-tuning approaches. Our transfer scheme is based on the observation that the model parameters at the bottom layers of a deep convolutional neural network can be directly shared between RGB and depth data while the remaining layers need to be fine-tuned rapidly. Our second contribution enhances re-identification for video by implementing temporal attention as a Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is stochastic, the temporal attention parameters are trained using reinforcement learning. Extensive experiments validate the accuracy of our method in person re-identification from depth sequences. Finally, in a scenario where subjects wear unseen clothes, we show large performance gains compared to a state-of-the-art model which relies on RGB data.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_44');
INSERT INTO `paper` VALUES (11137, 'Relaxation-Free Deep Hashing via Policy Gradient', 'Deep hashing', 'Relaxation-free', 'Policy gradient', '', '', 'In this paper, we propose a simple yet effective relaxation-free method to learn more effective binary codes via policy gradient for scalable image search. While a variety of deep hashing methods have been proposed in recent years, most of them are confronted by the dilemma to obtain optimal binary codes in a truly end-to-end manner with non-smooth sign activations. Unlike existing methods which usually employ a general relaxation framework to adapt to the gradient-based algorithms, our approach formulates the non-smooth part of the hashing network as sampling with a stochastic policy, so that the retrieval performance degradation caused by the relaxation can be avoided. Specifically, our method directly generates the binary codes and maximizes the expectation of rewards for similarity preservation, where the network can be trained directly via policy gradient. Hence, the differentiation challenge for discrete optimization can be naturally addressed, which leads to effective gradients and binary codes. Extensive experimental results on three benchmark datasets validate the effectiveness of the proposed method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_9');
INSERT INTO `paper` VALUES (11138, 'Reliable Multilane Detection and Classification by Utilizing CNN as a Regression Network', 'Multilane detection', 'CNN regression', 'Multilane classification', '', '', 'Reliable lane detection is crucial functionality for autonomous driving. Additionally positional information of ego lanes and side lanes is pivotal for critical tasks like overtaking assistants and path planning. In this work we present a CNN based regression approach for detecting multiple lanes as well as positionally classifying them. Present deep learning approaches for lane detection are inherently CNN semantic segmentation networks, which concentrate on classifying each pixel correctly and require post processing operations to infer lane information. We identify that such segmentation approach is not effective for detecting thin and elongated lane boundaries, which occupy relatively few pixels in the scene and is often occluded by vehicles. We pose the lane detection and classification problem as CNN regression task, which relaxes per pixel classification requirement to a few points along lane boundary. Our networks has better accuracy than the recent CNN based segmentation solution, and does not require any post processing or tracking operations. Particularly we observe improved robustness in occlusions and amidst shadows due to over bridge and trees. We have validated the network on our test vehicle using Nvidia’s PX2 platform, where we observe a promising performance of 25 FPS.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_46');
INSERT INTO `paper` VALUES (11139, 'RelocNet: Continuous Metric Learning Relocalisation Using Neural Nets', '', '', '', '', '', 'We propose a method of learning suitable convolutional representations for camera pose retrieval based on nearest neighbour matching and continuous metric learning-based feature descriptors. We introduce information from camera frusta overlaps between pairs of images to optimise our feature embedding network. Thus, the final camera pose descriptor differences represent camera pose changes. In addition, we build a pose regressor that is trained with a geometric loss to infer finer relative poses between a query and nearest neighbour images. Experiments show that our method is able to generalise in a meaningful way, and outperforms related methods across several experiments.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_46');
INSERT INTO `paper` VALUES (11140, 'Remote Photoplethysmography Correspondence Feature for 3D Mask Face Presentation Attack Detection', 'Face presentation attack detection', '3D mask attack', 'Remote photoplethysmography', '', '', '3D mask face presentation attack, as a new challenge in face recognition, has been attracting increasing attention. Recently, remote Photoplethysmography (rPPG) is employed as an intrinsic liveness cue which is independent of the mask appearance. Although existing rPPG-based methods achieve promising results on both intra and cross dataset scenarios, they may not be robust enough when rPPG signals are contaminated by noise. In this paper, we propose a new liveness feature, called rPPG correspondence feature (CFrPPG) to precisely identify the heartbeat vestige from the observed noisy rPPG signals. To further overcome the global interferences, we propose a novel learning strategy which incorporates the global noise within the CFrPPG feature. Extensive experiments indicate that the proposed feature not only outperforms the state-of-the-art rPPG based methods on 3D mask attacks but also be able to handle the practical scenarios with dim light and camera motion.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_34');
INSERT INTO `paper` VALUES (11141, 'Removal of Visual Disruption Caused by Rain Using Cycle-Consistent Generative Adversarial Networks', 'CycleGAN', 'ID-CGAN', 'Generative adversarial network', '', '', 'This paper addresses the problem of removing rain disruption from images for outdoor vision systems. The Cycle-Consistent Generative Adversarial Network (CycleGAN) is proposed as a more promising rain removal algorithm, as compared to the state-of-the-art Image De-raining Conditional Generative Adversarial Network (ID-CGAN). The CycleGAN has an advantage in its ability to learn the underlying relationship between the rain and rain-free domain without the need of paired domain examples. Based on rain physical properties and its various phenomena, five broad categories of real rain distortions are proposed in this paper. For a fair comparison, both networks were trained on the same set of synthesized rain-and-ground-truth image-pairs provided by the ID-CGAN work, and subsequently tested on real rain images which fall broadly under these five categories. The comparison results demonstrated that the CycleGAN is superior in removing real rain distortions.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_34');
INSERT INTO `paper` VALUES (11142, 'Rendering Portraitures from Monocular Camera and Beyond', 'Segmentation Maps', 'Depth Of Field (DoF)', 'Fully Convolutional Neural Network (FCNN)', 'Blur Kernel', 'Google Pixel', 'Shallow Depth-of-Field (DoF) is a desirable effect in photography which renders artistic photos. Usually, it requires single-lens reflex cameras and certain photography skills to generate such effects. Recently, dual-lens on cellphones is used to estimate scene depth and simulate DoF effects for portrait shots. However, this technique cannot be applied to photos already taken and does not work well for whole-body scenes where the subject is at a distance from the cameras. In this work, we introduce an automatic system that achieves portrait DoF rendering for monocular cameras. Specifically, we first exploit Convolutional Neural Networks to estimate the relative depth and portrait segmentation maps from a single input image. Since these initial estimates from a single input are usually coarse and lack fine details, we further learn pixel affinities to refine the coarse estimation maps. With the refined estimation, we conduct depth and segmentation-aware blur rendering to the input image with a Conditional Random Field and image matting. In addition, we train a spatially-variant Recursive Neural Network to learn and accelerate this rendering process. We show that the proposed algorithm can effectively generate portraitures with realistic DoF effects using one single input. Experimental results also demonstrate that our depth and segmentation estimation modules perform favorably against the state-of-the-art methods both quantitatively and qualitatively.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_3');
INSERT INTO `paper` VALUES (11143, 'Rendering Realistic Subject-Dependent Expression Images by Learning 3DMM Deformation Coefficients', '3D morphable model', 'Deformation components learning', 'Facial expression synthesis', '', '', 'Automatic analysis of facial expressions is now attracting an increasing interest, thanks to the many potential applications it can enable. However, collecting images with labeled expression for large sets of images or videos is a quite complicated operation that, in most of the cases, requires substantial human intervention. In this paper, we propose a solution that, starting from a neutral image of a subject, is capable of producing a realistic expressive face image of the same subject. This is possible thanks to the use of a particular 3D morphable model (3DMM) that can effectively and efficiently fit to 2D images, and then deform itself under the action of deformation parameters learned expression-by-expression in a subject-independent manner. Ultimately, the application of such deformation parameters to the neutral model of a subject allows the rendering of realistic expressive images of the subject. Experiments demonstrate that such deformation parameters can be learned from a small set of training data using simple statistical tools; despite this simplicity, very realistic subject-dependent expression renderings can be obtained. Furthermore, robustness to cross dataset tests is also evidenced.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_34');
INSERT INTO `paper` VALUES (11144, 'Repeatability Is Not Enough: Learning Affine Regions via Discriminability', 'Local features', 'Affine shape', 'Loss function', 'Image retrieval', '', 'A method for learning local affine-covariant regions is presented. We show that maximizing geometric repeatability does not lead to local regions, a.k.a features, that are reliably matched and this necessitates descriptor-based learning. We explore factors that influence such learning and registration: the loss function, descriptor type, geometric parametrization and the trade-off between matchability and geometric accuracy and propose a novel hard negative-constant loss function for learning of affine regions. The affine shape estimator – AffNet – trained with the hard negative-constant loss outperforms the state-of-the-art in bag-of-words image retrieval and wide baseline stereo. The proposed training process does not require precisely geometrically aligned patches. The source codes and trained weights are available at https://github.com/ducha-aiki/affnet.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_18');
INSERT INTO `paper` VALUES (11145, 'Residual Stacked RNNs for Action Recognition', 'Action recognition', 'Deep residual learning', 'Two-stream RNN', '', '', 'Action recognition pipelines that use Recurrent Neural Networks (RNN) are currently 5–10% less accurate than Convolutional Neural Networks (CNN). While most works that use RNNs employ a 2D CNN on each frame to extract descriptors for action recognition, we extract spatiotemporal features from a 3D CNN and then learn the temporal relationship of these descriptors through a stacked residual recurrent neural network (Res-RNN). We introduce for the first time residual learning to counter the degradation problem in multi-layer RNNs, which have been successful for temporal aggregation in two-stream action recognition pipelines. Finally, we use a late fusion strategy to combine RGB and optical flow data of the two-stream Res-RNN. Experimental results show that the proposed pipeline achieves competitive results on UCF-101 and state of-the-art results for RNN-like architectures on the challenging HMDB-51 dataset.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_40');
INSERT INTO `paper` VALUES (11146, 'RESOUND: Towards Action Recognition Without Representation Bias', 'Activity Recognition', 'Ground Truth Representation', 'Competitive Diving', 'Dataset Bias', 'Action Recognition Datasets', 'While large datasets have proven to be a key enabler for progress in computer vision, they can have biases that lead to erroneous conclusions. The notion of the representation bias of a dataset is proposed to combat this problem. It captures the fact that representations other than the ground-truth representation can achieve good performance on any given dataset. When this is the case, the dataset is said not to be well calibrated. Dataset calibration is shown to be a necessary condition for the standard state-of-the-art evaluation practice to converge to the ground-truth representation. A procedure, RESOUND, is proposed to quantify and minimize representation bias. Its application to the problem of action recognition shows that current datasets are biased towards static representations (objects, scenes and people). Two versions of RESOUND are studied. An Explicit RESOUND procedure is proposed to assemble new datasets by sampling existing datasets. An implicit RESOUND procedure is used to guide the creation of a new dataset, Diving48, of over 18,000 video clips of competitive diving actions, spanning 48 fine-grained dive classes. Experimental evaluation confirms the effectiveness of RESOUND to reduce the static biases of current datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_32');
INSERT INTO `paper` VALUES (11147, 'Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification', '', '', '', '', '', 'Despite the steady progress in video analysis led by the adoption of convolutional neural networks (CNNs), the relative improvement has been less drastic as that in 2D static image classification. Three main challenges exist including spatial (image) feature representation, temporal information representation, and model/computation complexity. It was recently shown by Carreira and Zisserman that 3D CNNs, inflated from 2D networks and pretrained on ImageNet, could be a promising way for spatial and temporal representation learning. However, as for model/computation complexity, 3D CNNs are much more expensive than 2D CNNs and prone to overfit. We seek a balance between speed and accuracy by building an effective and efficient video classification system through systematic exploration of critical network design choices. In particular, we show that it is possible to replace many of the 3D convolutions by low-cost 2D convolutions. Rather surprisingly, best result (in both speed and accuracy) is achieved when replacing the 3D convolutions at the bottom of the network, suggesting that temporal representation learning on high-level “semantic” features is more useful. Our conclusion generalizes to datasets with very different properties. When combined with several other cost-effective designs including separable spatial/temporal convolution and feature gating, our system results in an effective video classification system that that produces very competitive results on several action classification benchmarks (Kinetics, Something-something, UCF101 and HMDB), as well as two action detection (localization) benchmarks (JHMDB and UCF101-24).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_19');
INSERT INTO `paper` VALUES (11148, 'Rethinking the Form of Latent States in Image Captioning', '', '', '', '', '', 'RNNs and their variants have been widely adopted for image captioning. In RNNs, the production of a caption is driven by a sequence of latent states. Existing captioning models usually represent latent states as vectors, taking this practice for granted. We rethink this choice and study an alternative formulation, namely using two-dimensional maps to encode latent states. This is motivated by the curiosity about a question: how the spatial structures in the latent states affect the resultant captions? Our study on MSCOCO and Flickr30k leads to two significant observations. First, the formulation with 2D states is generally more effective in captioning, consistently achieving higher performance with comparable parameter sizes. Second, 2D states preserve spatial locality. Taking advantage of this, we visually reveal the internal dynamics in the process of caption generation, as well as the connections between input visual domain and output linguistic domain.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_18');
INSERT INTO `paper` VALUES (11149, 'Retrospective Encoders for Video Summarization', 'Video summarization', 'Sequence-to-sequence learning', '', '', '', 'Supervised learning techniques have shown substantial progress on video summarization. State-of-the-art approaches mostly regard the predicted summary and the human summary as two sequences (sets), and minimize discriminative losses that measure element-wise discrepancy. Such training objectives do not explicitly model how well the predicted summary preserves semantic information in the video. Moreover, those methods often demand a large amount of human generated summaries. In this paper, we propose a novel sequence-to-sequence learning model to address these deficiencies. The key idea is to complement the discriminative losses with another loss which measures if the predicted summary preserves the same information as in the original video. To this end, we propose to augment standard sequence learning models with an additional “retrospective encoder” that embeds the predicted summary into an abstract semantic space. The embedding is then compared to the embedding of the original video in the same space. The intuition is that both embeddings ought to be close to each other for a video and its corresponding summary. Thus our approach adds to the discriminative loss a metric learning loss that minimizes the distance between such pairs while maximizing the distances between unmatched ones. One important advantage is that the metric learning loss readily allows learning from videos without human generated summaries. Extensive experimental results show that our model outperforms existing ones by a large margin in both supervised and semi-supervised settings.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_24');
INSERT INTO `paper` VALUES (11150, 'Reverse Attention for Salient Object Detection', 'Salient object detection', 'Reverse attention', 'Side-output residual learning', '', '', 'Benefit from the quick development of deep learning techniques, salient object detection has achieved remarkable progresses recently. However, there still exists following two major challenges that hinder its application in embedded devices, low resolution output and heavy model weight. To this end, this paper presents an accurate yet compact deep network for efficient salient object detection. More specifically, given a coarse saliency prediction in the deepest layer, we first employ residual learning to learn side-output residual features for saliency refinement, which can be achieved with very limited convolutional parameters while keep accuracy. Secondly, we further propose reverse attention to guide such side-output residual learning in a top-down manner. By erasing the current predicted salient regions from side-output features, the network can eventually explore the missing object parts and details which results in high resolution and accuracy. Experiments on six benchmark datasets demonstrate that the proposed approach compares favorably against state-of-the-art methods, and with advantages in terms of simplicity, efficiency (45 FPS) and model size (81 MB).', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_15');
INSERT INTO `paper` VALUES (11151, 'Revisiting Autofocus for Smartphone Cameras', 'Autofocus', 'Focal stack', 'AF platform', 'Low-level computer vision', '', 'Autofocus (AF) on smartphones is the process of determining how to move a camera’s lens such that certain scene content is in focus. The underlying algorithms used by AF systems, such as contrast detection and phase differencing, are well established. However, determining a high-level objective regarding how to best focus a particular scene is less clear. This is evident in part by the fact that different smartphone cameras employ different AF criteria; for example, some attempt to keep items in the center in focus, others give priority to faces while others maximize the sharpness of the entire scene. The fact that different objectives exist raises the research question of whether there is a preferred objective. This becomes more interesting when AF is applied to videos of dynamic scenes. The work in this paper aims to revisit AF for smartphones within the context of temporal image data. As part of this effort, we describe the capture of a new 4D dataset that provides access to a full focal stack at each time point in a temporal sequence. Based on this dataset, we have developed a platform and associated application programming interface (API) that mimic real AF systems, restricting lens motion within the constraints of a dynamic environment and frame capture. Using our platform we evaluated several high-level focusing objectives and found interesting insight into what users prefer. We believe our new temporal focal stack dataset, AF platform, and initial user-study findings will be useful in advancing AF research.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_32');
INSERT INTO `paper` VALUES (11152, 'Revisiting RCNN: On Awakening the Classification Power of Faster RCNN', 'Object detection', '', '', '', '', 'Recent region-based object detectors are usually built with separate classification and localization branches on top of shared feature extraction networks. In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization. We conjecture that: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects. We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. DCR samples hard false positives from the base classifier in Faster RCNN and trains a RCNN-styled strong classifier. Experiments show new state-of-the-art results on PASCAL VOC and COCO without any bells and whistles.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_28');
INSERT INTO `paper` VALUES (11153, 'Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors', 'Inverted Index', 'Neighborhood Centroid', 'Feature Space Partitioning', 'Region Centroid', 'Database Point', 'This work addresses the problem of billion-scale nearest neighbor search. The state-of-the-art retrieval systems for billion-scale databases are currently based on the inverted multi-index, the recently proposed generalization of the inverted index structure. The multi-index provides a very fine-grained partition of the feature space that allows extracting concise and accurate short-lists of candidates for the search queries.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_13');
INSERT INTO `paper` VALUES (11154, 'RGB-D SLAM Based Incremental Cuboid Modeling', 'Geometric shape', 'Cuboid', 'Incrementally structural modeling', 'Point cloud', '', 'This paper present a framework for incremental 3D cuboid modeling combined with RGB-D SLAM. While performing RGB-D SLAM, planes are incrementally reconstructed from point clouds. Then, cuboids are detected in the planes by analyzing the positional relationships between the planes; orthogonality, convexity, and proximity. Finally, the position, pose and size of a cuboid are determined by computing the intersection of three perpendicular planes. In addition, the cuboid shapes are incrementally updated to suppress false detections with sequential measurements. As an application of our framework, an augmented reality based interactive cuboid modeling system is introduced. In the evaluation at a cluttered environment, the precision and recall of the cuboid detection are improved with our framework owing to stable plane detection, compared with a batch based method.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_25');
INSERT INTO `paper` VALUES (11155, 'RIDI: Robust IMU Double Integration', 'Inertial Measurement Unit (IMU)', 'Multiple Human Subjects', 'Linear Acceleration', 'Inertial Navigation', 'Natural Human Motion', 'This paper proposes a novel data-driven approach for inertial navigation, which learns to estimate trajectories of natural human motions just from an inertial measurement unit (IMU) in every smartphone. The key observation is that human motions are repetitive and consist of a few major modes (e.g., standing, walking, or turning). Our algorithm regresses a velocity vector from the history of linear accelerations and angular velocities, then corrects low-frequency bias in the linear accelerations, which are integrated twice to estimate positions. We have acquired training data with ground truth motion trajectories across multiple human subjects and multiple phone placements (e.g., in a bag or a hand). The qualitatively and quantitatively evaluations have demonstrated that our simple algorithm outperforms existing heuristic-based approaches and is even comparable to full Visual Inertial navigation to our surprise. As far as we know, this paper is the first to introduce supervised training for inertial navigation, potentially opening up a new line of research in the domain of data-driven inertial navigation. We will publicly share our code and data to facilitate further research (Project website: https://yanhangpublic.github.io/ridi).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_38');
INSERT INTO `paper` VALUES (11156, 'Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence', '', '', '', '', '', 'Incremental learning (il) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the il problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of il. The main challenge for an il algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, il also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of il algorithms. Furthermore, we present RWalk, a generalization of ewc++ (our efficient version of ewc [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various il algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_33');
INSERT INTO `paper` VALUES (11157, 'Robust 3D Pig Measurement in Pig Farm', 'Pig measurement', 'Computer vision', 'Three dimensional', 'Weight estimation', 'Multiple slits laser', 'On a pig farm, the shipment of pigs of proper weight is very important for increasing profit. However, in order to reduce labor costs, many farmers ship pigs without weighing them. Therefore, an automatic sorting system that selects pigs that have reached the proper weight by measuring the weight of each pig has been developed. In the present paper, a weight estimation system using a camera for pig sorting is introduced. Three-dimensional visual information on a pig captured in a single image is used to estimate its weight. The proposed method is robust and practical for the measurement of a moving animal in a poor environment of pig farms.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_23');
INSERT INTO `paper` VALUES (11158, 'Robust Anchor Embedding for Unsupervised Video Person re-IDentification in the Wild', 'Unsupervised person re-id', 'Robust anchor embedding', '', '', '', 'This paper addresses the scalability and robustness issues of estimating labels from imbalanced unlabeled data for unsupervised video-based person re-identification (re-ID). To achieve it, we propose a novel Robust AnChor Embedding (RACE) framework via deep feature representation learning for large-scale unsupervised video re-ID. Within this framework, anchor sequences representing different persons are firstly selected to formulate an anchor graph which also initializes the CNN model to get discriminative feature representations for later label estimation. To accurately estimate labels from unlabeled sequences with noisy frames, robust anchor embedding is introduced based on the regularized affine hull. Efficiency is ensured with kNN anchors embedding instead of the whole anchor set under manifold assumptions. After that, a robust and efficient top-k counts label prediction strategy is proposed to predict the labels of unlabeled image sequences. With the newly estimated labeled sequences, the unified anchor embedding framework enables the feature learning process to be further facilitated. Extensive experimental results on the large-scale dataset show that the proposed method outperforms existing unsupervised video re-ID methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_11');
INSERT INTO `paper` VALUES (11159, 'Robust Fitting in Computer Vision: Easy or Hard?', 'Robust fitting', 'Consensus maximisation', 'Inlier set maximisation', 'Computational hardness', '', 'Robust model fitting plays a vital role in computer vision, and research into algorithms for robust fitting continues to be active. Arguably the most popular paradigm for robust fitting in computer vision is consensus maximisation, which strives to find the model parameters that maximise the number of inliers. Despite the significant developments in algorithms for consensus maximisation, there has been a lack of fundamental analysis of the problem in the computer vision literature. In particular, whether consensus maximisation is “tractable” remains a question that has not been rigorously dealt with, thus making it difficult to assess and compare the performance of proposed algorithms, relative to what is theoretically achievable. To shed light on these issues, we present several computational hardness results for consensus maximisation. Our results underline the fundamental intractability of the problem, and resolve several ambiguities existing in the literature.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_43');
INSERT INTO `paper` VALUES (11160, 'Robust Image Stitching with Multiple Registrations', '', '', '', '', '', 'Panorama creation is one of the most widely deployed techniques in computer vision. In addition to industry applications such as Google Street View, it is also used by millions of consumers in smartphones and other cameras. Traditionally, the problem is decomposed into three phases: registration, which picks a single transformation of each source image to align it to the other inputs, seam finding, which selects a source image for each pixel in the final result, and blending, which fixes minor visual artifacts [1, 2]. Here, we observe that the use of a single registration often leads to errors, especially in scenes with significant depth variation or object motion. We propose instead the use of multiple registrations, permitting regions of the image at different depths to be captured with greater accuracy. MRF inference techniques naturally extend to seam finding over multiple registrations, and we show here that their energy functions can be readily modified with new terms that discourage duplication and tearing, common problems that are exacerbated by the use of multiple registrations. Our techniques are closely related to layer-based stereo [3, 4], and move image stitching closer to explicit scene modeling. Experimental evidence demonstrates that our techniques often generate significantly better panoramas when there is substantial motion or parallax.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_4');
INSERT INTO `paper` VALUES (11161, 'Robust Optical Flow in Rainy Scenes', 'Optical flow', 'Rain', 'Decomposition', 'Residue channel', '', 'Optical flow estimation in rainy scenes is challenging due to degradation caused by rain streaks and rain accumulation, where the latter refers to the poor visibility of remote scenes due to intense rainfall. To resolve the problem, we introduce a residue channel, a single channel (gray) image that is free from rain, and its colored version, a colored-residue image. We propose to utilize these two rain-free images in computing optical flow. To deal with the loss of contrast and the attendant sensitivity to noise, we decompose each of the input images into a piecewise-smooth structure layer and a high-frequency fine-detail texture layer. We combine the colored-residue images and structure layers in a unified objective function, so that the estimation of optical flow can be more robust. Results on both synthetic and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain. We also provide an optical flow dataset consisting of both synthetic and real rain images.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_18');
INSERT INTO `paper` VALUES (11162, 'Robust Structured Light System Against Subsurface Scattering Effects Achieved by CNN-Based Pattern Detection and Decoding Algorithm', '', '', '', '', '', 'To reconstruct 3D shapes of real objects, a structured-light technique has been commonly used especially for practical purposes, such as inspection, industrial modeling, medical diagnosis, etc., because of simplicity, stability and high precision. Among them, oneshot scanning technique, which requires only single image for reconstruction, becomes important for the purpose of capturing moving objects. One open problem of oneshot scanning technique is its instability, when captured pattern is degraded by some reasons, such as strong specularity, subsurface scattering, inter-reflection and so on. One of important targets for oneshot scan is live animal, which includes human body or tissue of organ, and has subsurface scattering. In this paper, we propose a learning-based approach to solve pattern degradation caused by subsurface scattering for oneshot scan. Since patterns are significantly blurred by subsurface scattering, robust decoding technique is required, which is effectively achieved by separating the decoding process into two parts, such as pattern detection and ID recognition in our technique; both parts are implemented by CNN. To efficiently achieve robust pattern detection, we convert a line detection into segmentation problem. For robust ID recognition, we segment all the region into each ID using U-Net. In the experiments, it is shown that our technique is robust against strong subsurface scattering compared to state of the art technique.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_22');
INSERT INTO `paper` VALUES (11163, 'Role of Group Level Affect to Find the Most Influential Person in Images', 'Important person', 'Group of people', 'Group level affect', '', '', 'Group affect analysis is an important cue for predicting various group traits. Generally, the estimation of the group affect, emotional responses, eye gaze and position of people in images are the important cues to identify an important person from a group of people. The main focus of this paper is to explore the importance of group affect in finding the representative of a group. We call that person the “Most Influential Person” (for the first impression) or “leader” of a group. In order to identify the main visual cues for “Most Influential Person”, we conducted a user survey. Based on the survey statistics, we annotate the “influential persons” in 1000 images of Group AFfect database (GAF 2.0) via LabelMe toolbox and propose the “GAF-personage database”. In order to identify “Most Influential Person”, we proposed a DNN based Multiple Instance Learning (Deep MIL) method which takes deep facial features as input. To leverage the deep facial features, we first predict the individual emotion probabilities via CapsNet and rank the detected faces on the basis of it. Then, we extract deep facial features of the top-3 faces via VGG-16 network. Our method performs better than maximum facial area and saliency-based importance methods and achieves the human-level perception of “Most Influential Person” at group-level.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_39');
INSERT INTO `paper` VALUES (11164, 'Rolling Shutter Pose and Ego-Motion Estimation Using Shape-from-Template', 'Rolling Shutter', 'Pose estimation', 'Shape-from-Template', '', '', 'We propose a new method for the absolute camera pose problem (PnP) which handles Rolling Shutter (RS) effects. Unlike all existing methods which perform 3D-2D registration after augmenting the Global Shutter (GS) projection model with the velocity parameters under various kinematic models, we propose to use local differential constraints. These are established by drawing an analogy with Shape-from-Template (SfT). The main idea consists in considering that RS distortions due to camera ego-motion during image acquisition can be interpreted as virtual deformations of a template captured by a GS camera. Once the virtual deformations have been recovered using SfT, the camera pose and ego-motion are computed by registering the deformed scene on the original template. This 3D-3D registration involves a 3D cost function based on the Euclidean point distance, more physically meaningful than the re-projection error or the algebraic distance based cost functions used in previous work. Results on both synthetic and real data show that the proposed method outperforms existing RS pose estimation techniques in terms of accuracy and stability of performance in various configurations.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_29');
INSERT INTO `paper` VALUES (11165, 'RPNet: An End-to-End Network for Relative Camera Pose Estimation', 'Relative pose estimation', 'Pose estimation', 'PoseNet', '', '', 'This paper addresses the task of relative camera pose estimation from raw image pixels, by means of deep neural networks. The proposed RPNet network takes pairs of images as input and directly infers the relative poses, without the need of camera intrinsic/extrinsic. While state-of-the-art systems based on SIFT + RANSAC, are able to recover the translation vector only up to scale, RPNet is trained to produce the full translation vector, in an end-to-end way. Experimental results on the Cambridge Landmark data set show very promising results regarding the recovery of the full translation vector. They also show that RPNet produces more accurate and more stable results than traditional approaches, especially for hard images (repetitive textures, textureless images, etc.). To the best of our knowledge, RPNet is the first attempt to recover full translation vectors in relative pose estimation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_46');
INSERT INTO `paper` VALUES (11166, 'RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments', 'Gaze estimation', 'Gaze dataset', 'Convolutional neural network', 'Semantic inpainting', 'Eyetracking glasses', 'In this work, we consider the problem of robust gaze estimation in natural environments. Large camera-to-subject distances and high variations in head pose and eye gaze angles are common in such environments. This leads to two main shortfalls in state-of-the-art methods for gaze estimation: hindered ground truth gaze annotation and diminished gaze estimation accuracy as image resolution decreases with distance. We first record a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. We also present a new real-time algorithm involving appearance-based deep convolutional neural networks with increased capacity to cope with the diverse images in the new dataset. Experiments with this network architecture are conducted on a number of diverse eye-gaze datasets including our own, and in cross dataset evaluations. We demonstrate state-of-the-art performance in terms of estimation accuracy in all experiments, and the architecture performs well even on lower resolution images.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_21');
INSERT INTO `paper` VALUES (11167, 'SaaS: Speed as a Supervisor for Semi-supervised Learning', '', '', '', '', '', 'We introduce the SaaS Algorithm for semi-supervised learning, which uses learning speed during stochastic gradient descent in a deep neural network to measure the quality of an iterative estimate of the posterior probability of unknown labels. Training speed in supervised learning correlates strongly with the percentage of correct labels, so we use it as an inference criterion for the unknown labels, without attempting to infer the model parameters at first. Despite its simplicity, SaaS achieves competitive results in semi-supervised learning benchmarks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_10');
INSERT INTO `paper` VALUES (11168, 'SafeUAV: Learning to Estimate Depth and Safe Landing Areas for UAVs from Synthetic Data', 'UAVs', 'CNNs', 'Depth estimation', 'Safe landing', '', 'The emergence of relatively low cost UAVs has prompted a global concern about the safe operation of such devices. Since most of them can ‘autonomously’ fly by means of GPS way-points, the lack of a higher logic for emergency scenarios leads to an abundance of incidents involving property or personal injury. In order to tackle this problem, we propose a small, embeddable ConvNet for both depth and safe landing area estimation. Furthermore, since labeled training data in the 3D aerial field is scarce and ground images are unsuitable, we capture a novel synthetic aerial 3D dataset obtained from 3D reconstructions. We use the synthetic data to learn to estimate depth from in-flight images and segment them into ‘safe-landing’ and ‘obstacle’ regions. Our experiments demonstrate compelling results in practice on both synthetic data and real RGB drone footage.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_4');
INSERT INTO `paper` VALUES (11169, 'Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics', 'Saliency', 'Benchmarking', 'Metrics', 'Fixations', 'Bayesian decision theory', 'Dozens of new models on fixation prediction are published every year and compared on open benchmarks such as MIT300 and LSUN. However, progress in the field can be difficult to judge because models are compared using a variety of inconsistent metrics. Here we show that no single saliency map can perform well under all metrics. Instead, we propose a principled approach to solve the benchmarking problem by separating the notions of saliency models, maps and metrics. Inspired by Bayesian decision theory, we define a saliency model to be a probabilistic model of fixation density prediction and a saliency map to be a metric-specific prediction derived from the model density which maximizes the expected performance on that metric given the model density. We derive these optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC, NSS, CC, SIM, KL-Div) and show that they can be computed analytically or approximated with high precision. We show that this leads to consistent rankings in all metrics and avoids the penalties of using one saliency map for all metrics. Our method allows researchers to have their model compete on many different metrics with state-of-the-art in those metrics: “good” models will perform well in all metrics.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_47');
INSERT INTO `paper` VALUES (11170, 'Saliency Detection in 360\\(^\\circ \\) Videos', 'Spherical convolution', 'Video saliency detection', '\\(360^\\circ \\) VR videos', '', '', 'This paper presents a novel spherical convolutional neural network based scheme for saliency detection for \\(360^\\circ \\) videos. Specifically, in our spherical convolution neural network definition, kernel is defined on a spherical crown, and the convolution involves the rotation of the kernel along the sphere. Considering that the \\(360^\\circ \\) videos are usually stored with equirectangular panorama, we propose to implement the spherical convolution on panorama by stretching and rotating the kernel based on the location of patch to be convolved. Compared with existing spherical convolution, our definition has the parameter sharing property, which would greatly reduce the parameters to be learned. We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net. To validate our approach, we construct a large-scale \\(360^\\circ \\) videos saliency detection benchmark that consists of 104 \\(360^\\circ \\) videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our spherical U-net for \\(360^\\circ \\) video saliency detection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_30');
INSERT INTO `paper` VALUES (11171, 'Saliency Preservation in Low-Resolution Grayscale Images', 'Saliency detection', 'Fully convolutional network', 'Peripheral vision', '', '', 'Visual salience detection originated over 500 million years ago and is one of nature’s most efficient mechanisms. In contrast, many state-of-the-art computational saliency models are complex and inefficient. Most saliency models process high-resolution color images; however, insights into the evolutionary origins of visual salience detection suggest that achromatic low-resolution vision is essential to its speed and efficiency. Previous studies showed that low-resolution color and high-resolution grayscale images preserve saliency information. However, to our knowledge, no one has investigated whether saliency is preserved in low-resolution grayscale (LG) images. In this study, we explain the biological and computational motivation for LG, and show, through a range of human eye-tracking and computational modeling experiments, that saliency information is preserved in LG images. Moreover, we show that using LG images leads to significant speedups in model training and detection times and conclude by proposing LG images for fast and efficient salience detection.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_15');
INSERT INTO `paper` VALUES (11172, 'Saliency-Driven Variational Retargeting for Historical Maps', 'Image retargeting', 'Warping', 'Historical maps', '', '', 'We study the problem of georeferencing artistic historical maps. Since they were primarily conceived as work of art more than an accurate cartographic tool, the common warping approaches implemented in Geographic Application Systems (GIS) usually lead to an overly-stretched image in which the actual pictorial content (like written text, compass roses, buildings, etc.) is un-naturally deformed. On the other hand, domain transformation of images driven by the perceived salient visual content is a well-known topic known as “image retargeting” which has been mostly limited to a change of scale of the image (i.e. changing the width and height) rather than a more general control-points based warping.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_47');
INSERT INTO `paper` VALUES (11173, 'Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground', 'Salient object detection', 'Saliency benchmark', 'Dataset', 'Attribute', '', 'We provide a comprehensive evaluation of salient object detection (SOD) models. Our analysis identifies a serious design bias of existing SOD datasets which assumes that each image contains at least one clearly outstanding salient object in low clutter. The design bias has led to a saturated high performance for state-of-the-art SOD models when evaluated on existing datasets. The models, however, still perform far from being satisfactory when applied to real-world daily scenes. Based on our analyses, we first identify 7 crucial aspects that a comprehensive and balanced dataset should fulfill. Then, we propose a new high quality dataset and update the previous saliency benchmark. Specifically, our SOC (Salient Objects in Clutter) dataset, includes images with salient and non-salient objects from daily object categories. Beyond object category annotations, each salient image is accompanied by attributes that reflect common challenges in real-world scenes. Finally, we report attribute-based performance assessment on our dataset.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_12');
INSERT INTO `paper` VALUES (11174, 'Sampling Algebraic Varieties for Robust Camera Autocalibration', '', '', '', '', '', 'This paper addresses the problem of robustly autocalibrating a moving camera with constant intrinsics. The proposed calibration method uses the Branch-and-Bound (BnB) search paradigm to maximize the consensus of the polynomials. These polynomials are parameterized by the entries of, either the Dual Image of Absolute Conic (DIAC) or the Plane-at-Infinity (PaI). During the BnB search, we exploit the theory of sampling algebraic varieties, to test the positivity of any polynomial within a parameter’s interval, i.e. outliers with certainty. The search process explores the space of exact parameters (i.e. the entries of DIAC or PaI), benefits from the solution of a local method, and converges to the solution satisfied by the largest number of polynomials. Given many polynomials on the sought parameters (with possibly overwhelmingly many from outlier measurements), their consensus for calibration is searched for two cases: simplified Kruppa’s equations and Modulus constraints, expressed in DIAC and PaI, resp. Our approach yields outstanding results in terms of robustness and optimality.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_17');
INSERT INTO `paper` VALUES (11175, 'SAN: Learning Relationship Between Convolutional Features for Multi-scale Object Detection', 'Scale Aware Network', 'Object detection', 'Multi scale', 'Neural network', '', 'Most of the recent successful methods in accurate object detection build on the convolutional neural networks (CNN). However, due to the lack of scale normalization in CNN-based detection methods, the activated channels in the feature space can be completely different according to a scale and this difference makes it hard for the classifier to learn samples. We propose a Scale Aware Network (SAN) that maps the convolutional features from the different scales onto a scale-invariant subspace to make CNN-based detection methods more robust to the scale variation, and also construct a unique learning method which considers purely the relationship between channels without the spatial information for the efficient learning of SAN. To show the validity of our method, we visualize how convolutional features change according to the scale through a channel activation matrix and experimentally show that SAN reduces the feature differences in the scale space. We evaluate our method on VOC PASCAL and MS COCO dataset. We demonstrate SAN by conducting several experiments on structures and parameters. The proposed SAN can be generally applied to many CNN-based detection methods to enhance the detection accuracy with a slight increase in the computing time.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_20');
INSERT INTO `paper` VALUES (11176, 'Scale Aggregation Network for Accurate and Efficient Crowd Counting', 'Crowd counting', 'Crowd density estimation', 'Scale Aggregation Network', 'Local pattern consistency', '', 'In this paper, we propose a novel encoder-decoder network, called Scale Aggregation Network (SANet), for accurate and efficient crowd counting. The encoder extracts multi-scale features with scale aggregation modules and the decoder generates high-resolution density maps by using a set of transposed convolutions. Moreover, we find that most existing works use only Euclidean loss which assumes independence among each pixel but ignores the local correlation in density maps. Therefore, we propose a novel training loss, combining of Euclidean loss and local pattern consistency loss, which improves the performance of the model in our experiments. In addition, we use normalization layers to ease the training process and apply a patch-based test scheme to reduce the impact of statistic shift problem. To demonstrate the effectiveness of the proposed method, we conduct extensive experiments on four major crowd counting datasets and our method achieves superior performance to state-of-the-art methods while with much less parameters.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_45');
INSERT INTO `paper` VALUES (11177, 'Scale Drift Correction of Camera Geo-Localization Using Geo-Tagged Images', '3D reconstruction', 'Localization', 'Street View', '', '', 'Camera geo-localization from a monocular video is a fundamental task for video analysis and autonomous navigation. Although 3D reconstruction is a key technique to obtain camera poses, monocular 3D reconstruction in a large environment tends to result in the accumulation of errors in rotation, translation, and especially in scale: a problem known as scale drift. To overcome these errors, we propose a novel framework that integrates incremental structure from motion (SfM) and a scale drift correction method utilizing geo-tagged images, such as those provided by Google Street View. Our correction method begins by obtaining sparse 6-DoF correspondences between the reconstructed 3D map coordinate system and the world coordinate system, by using geo-tagged images. Then, it corrects scale drift by applying pose graph optimization over \\(\\mathrm {Sim}(3)\\) constraints and bundle adjustment. Experimental evaluations on large-scale datasets show that the proposed framework not only sufficiently corrects scale drift, but also achieves accurate geo-localization in a kilometer-scale environment.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_16');
INSERT INTO `paper` VALUES (11178, 'Scale-Awareness of Light Field Camera Based Visual Odometry', 'Light field', 'Plenoptic camera', 'SLAM', 'Visual odometry', '', 'We propose a novel direct visual odometry algorithm for micro-lens-array-based light field cameras. The algorithm calculates a detailed, semi-dense 3D point cloud of its environment. This is achieved by establishing probabilistic depth hypotheses based on stereo observations between the micro images of different recordings. Tracking is performed in a coarse-to-fine process, working directly on the recorded raw images. The tracking accounts for changing lighting conditions and utilizes a linear motion model to be more robust. A novel scale optimization framework is proposed. It estimates the scene scale, on the basis of keyframes, and optimizes the scale of the entire trajectory by filtering over multiple estimates. The method is tested based on a versatile dataset consisting of challenging indoor and outdoor sequences and is compared to state-of-the-art monocular and stereo approaches. The algorithm shows the ability to recover the absolute scale of the scene and significantly outperforms state-of-the-art monocular algorithms with respect to scale drifts.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_44');
INSERT INTO `paper` VALUES (11179, 'Scale-Recurrent Multi-residual Dense Network for Image Super-Resolution', 'Super-resolution', 'Deep learning', 'Residual networks', 'Dense connections', '', 'Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_9');
INSERT INTO `paper` VALUES (11180, 'Scaling Egocentric Vision: The Open image in new window Dataset', 'Egocentric vision', 'Dataset', 'Benchmarks', 'First-person vision', 'Egocentric object detection', 'First-person vision is gaining interest as it offers a unique viewpoint on people’s interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce Open image in new window , a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict non-scripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55h of video consisting of 11.5M frames, which we densely labelled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_44');
INSERT INTO `paper` VALUES (11181, 'Scene Coordinate Regression with Angle-Based Reprojection Loss for Camera Relocalization', 'Camera relocalization', 'Scene coordinate regression', 'Deep neural networks', '', '', 'Image-based camera relocalization is an important problem in computer vision and robotics. Recent works utilize convolutional neural networks (CNNs) to regress for pixels in a query image their corresponding 3D world coordinates in the scene. The final pose is then solved via a RANSAC-based optimization scheme using the predicted coordinates. Usually, the CNN is trained with ground truth scene coordinates, but it has also been shown that the network can discover 3D scene geometry automatically by minimizing single-view reprojection loss. However, due to the deficiencies of the reprojection loss, the network needs to be carefully initialized. In this paper, we present a new angle-based reprojection loss, which resolves the issues of the original reprojection loss. With this new loss function, the network can be trained without careful initialization, and the system achieves more accurate results. The new loss also enables us to utilize available multi-view constraints, which further improve performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_19');
INSERT INTO `paper` VALUES (11182, 'Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset', 'Video dataset', 'Multi-task', 'Scene', 'Object', 'Action', 'This paper introduces a large-scale, multi-label and multi-task video dataset named Scenes-Objects-Actions (SOA). Most prior video datasets are based on a predefined taxonomy, which is used to define the keyword queries issued to search engines. The videos retrieved by the search engines are then verified for correctness by human annotators. Datasets collected in this manner tend to generate high classification accuracy as search engines typically rank “easy” videos first. The SOA dataset adopts a different approach. We rely on uniform sampling to get a better representation of videos on the Web. Trained annotators are asked to provide free-form text labels describing each video in three different aspects: scene, object and action. These raw labels are then merged, split and renamed to generate a taxonomy for SOA. All the annotations are verified again based on the taxonomy. The final dataset includes 562K videos with 3.64M annotations spanning 49 categories for scenes, 356 for objects, 148 for actions, and naturally captures the long tail distribution of visual concepts in the real world. We show that datasets collected in this way are quite challenging by evaluating existing popular video models on SOA. We provide in-depth analysis about the performance of different models on SOA, and highlight potential new directions in video classification. We compare SOA with existing datasets and discuss various factors that impact the performance of transfer learning. A key-feature of SOA is that it enables the empirical study of correlation among scene, object and action recognition in video. We present results of this study and further analyze the potential of using the information learned from one task to improve the others. We also demonstrate different ways of scaling up SOA to learn better features. We believe that the challenges presented by SOA offer the opportunity for further advancement in video analysis as we progress from single-label classification towards a more comprehensive understanding of video data.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_39');
INSERT INTO `paper` VALUES (11183, 'SConE: Siamese Constellation Embedding Descriptor for Image Matching', 'Feature descriptor', 'Image matching', 'Siamese networks', '', '', 'Numerous computer vision applications rely on local feature descriptors, such as SIFT, SURF or FREAK, for image matching. Although their local character makes image matching processes more robust to occlusions, it often leads to geometrically inconsistent keypoint matches that need to be filtered out, e.g. using RANSAC. In this paper we propose a novel, more discriminative, descriptor that includes not only local feature representation, but also information about the geometric layout of neighbouring keypoints. To that end, we use a Siamese architecture that learns a low-dimensional feature embedding of keypoint constellation by maximizing the distances between non-corresponding pairs of matched image patches, while minimizing it for correct matches. The 48-dimensional floating point descriptor that we train is built on top of the state-of-the-art FREAK descriptor achieves significant performance improvement over the competitors on a challenging TUM dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_24');
INSERT INTO `paper` VALUES (11184, 'SDC-Net: Video Prediction Using Spatially-Displaced Convolution', '3D CNN', 'Sampling kernel', 'Optical flow', 'Frame prediction', '', 'We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we present spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_44');
INSERT INTO `paper` VALUES (11185, 'Seamless Color Mapping for 3D Reconstruction with Consumer-Grade Scanning Devices', 'Texture mapping', 'Markov random field', 'Seamless color optimization', '', '', 'Virtual Reality provides an immersive and intuitive shopping experience for customers. This raises challenging problems of reconstructing real-life products realistically in a cheap way. We present a seamless texturing method for 3D reconstructed objects with inexpensive consumer-grade scanning devices. To this end, we develop a two-step global optimization method to seamlessly texture reconstructed models with color images. We first perform a seam generation optimization based on Markov random field to generate more reasonable seams located at low-frequency color areas. Then, we employ a seam correction optimization that uses local color information around seams to correct the misalignments of images used for texturing. In contrast to previous approaches, the proposed method is more computationally efficient in generating seamless texture maps. Experimental results show that our method can efficiently deliver a seamless and high-quality texture maps even for noisy data.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_39');
INSERT INTO `paper` VALUES (11186, 'Second-Order Democratic Aggregation', 'Second-order features', 'Democratic pooling', 'Matrix power normalization', 'Tensor sketching', '', 'Aggregated second-order features extracted from deep convolutional networks have been shown to be effective for texture generation, fine-grained recognition, material classification, and scene understanding. In this paper, we study a class of orderless aggregation functions designed to minimize interference or equalize contributions in the context of second-order features and we show that they can be computed just as efficiently as their first-order counterparts and they have favorable properties over aggregation by summation. Another line of work has shown that matrix power normalization after aggregation can significantly improve the generalization of second-order representations. We show that matrix power normalization implicitly equalizes contributions during aggregation thus establishing a connection between matrix normalization techniques and prior work on minimizing interference. Based on the analysis we present \\(\\gamma \\)-democratic aggregators that interpolate between sum (\\(\\gamma \\) = 1) and democratic pooling (\\(\\gamma \\) = 0) outperforming both on several classification tasks. Moreover, unlike power normalization, the \\(\\gamma \\)-democratic aggregations can be computed in a low dimensional space by sketching that allows the use of very high-dimensional second-order features. This results in a state-of-the-art performance on several datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_38');
INSERT INTO `paper` VALUES (11187, 'Seeing Deeply and Bidirectionally: A Deep Learning Approach for Single Image Reflection Removal', '', '', '', '', '', 'Reflections often obstruct the desired scene when taking photos through glass panels. Removing unwanted reflection automatically from the photos is highly desirable. Traditional methods often impose certain priors or assumptions to target particular type(s) of reflection such as shifted double reflection, thus have difficulty to generalize to other types. Very recently a deep learning approach has been proposed. It learns a deep neural network that directly maps a reflection contaminated image to a background (target) image (i.e.reflection free image) in an end to end fashion, and outperforms the previous methods. We argue that, to remove reflection truly well, we should estimate the reflection and utilize it to estimate the background image. We propose a cascade deep neural network, which estimates both the background image and the reflection. This significantly improves reflection removal. In the cascade deep network, we use the estimated background image to estimate the reflection, and then use the estimated reflection to estimate the background image, facilitating our idea of seeing deeply and bidirectionally.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_40');
INSERT INTO `paper` VALUES (11188, 'Seeing the World Through Machinic Eyes: Reflections on Computer Vision in the Arts', 'Art', 'Design', 'Perception', 'Computer vision', 'Google Earth', 'Today, computer vision is broadly implemented and operates in the background of many systems. For users of these technologies, there is often no visual feedback, making it hard to understand the mechanisms that drive it. When computer vision is used to generate visual representations like Google Earth, it remains difficult to perceive the particular process and principles that went into its creation. This text examines computer vision as a medium and a system of representation by analyzing the work of design studio Onformative, designer Bernhard Hopfengärtner and artist Clement Valla. By using technical failures and employing computer vision in unforeseen ways, these artists and designers expose the differences between computer vision and human perception. Since computer vision is increasingly used to facilitate (visual) communication, artistic reflections like these help us understand the nature of computer vision and how it shapes our perception of the world.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_50');
INSERT INTO `paper` VALUES (11189, 'Seeing Tree Structure from Vibration', 'Vibration', 'Tree structure', 'Hierarchical Bayesian model', '', '', 'Humans recognize object structure from both their appearance and motion; often, motion helps to resolve ambiguities in object structure that arise when we observe object appearance only. There are particular scenarios, however, where neither appearance nor spatial-temporal motion signals are informative: occluding twigs may look connected and have almost identical movements, though they belong to different, possibly disconnected branches. We propose to tackle this problem through spectrum analysis of motion signals, because vibrations of disconnected branches, though visually similar, often have distinctive natural frequencies. We propose a novel formulation of tree structure based on a physics-based link model, and validate its effectiveness by theoretical analysis, numerical simulation, and empirical experiments. With this formulation, we use nonparametric Bayesian inference to reconstruct tree structure from both spectral vibration signals and appearance cues. Our model performs well in recognizing hierarchical tree structure from real-world videos of trees and vessels.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_46');
INSERT INTO `paper` VALUES (11190, 'SegStereo: Exploiting Semantic Information for Disparity Estimation', 'Disparity estimation', 'Semantic cues', 'Semantic feature embedding', 'Softmax loss regularization', '', 'Disparity estimation for binocular stereo images finds a wide range of applications. Traditional algorithms may fail on featureless regions, which could be handled by high-level clues such as semantic segments. In this paper, we suggest that appropriate incorporation of semantic cues can greatly rectify prediction in commonly-used disparity estimation frameworks. Our method conducts semantic feature embedding and regularizes semantic cues as the loss term to improve learning disparity. Our unified model SegStereo employs semantic features from segmentation and introduces semantic softmax loss, which helps improve the prediction accuracy of disparity maps. The semantic cues work well in both unsupervised and supervised manners. SegStereo achieves state-of-the-art results on KITTI Stereo benchmark and produces decent prediction on both CityScapes and FlyingThings3D datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_39');
INSERT INTO `paper` VALUES (11191, 'Selective Zero-Shot Classification with Augmented Attributes', 'Zero-shot classification', 'Selective classification', 'Defined attributes', 'Residual attributes', 'Risk-coverage trade-off', 'In this paper, we introduce a selective zero-shot classification problem: how can the classifier avoid making dubious predictions? Existing attribute-based zero-shot classification methods are shown to work poorly in the selective classification scenario. We argue the under-complete human defined attribute vocabulary accounts for the poor performance. We propose a selective zero-shot classifier based on both the human defined and the automatically discovered residual attributes. The proposed classifier is constructed by firstly learning the defined and the residual attributes jointly. Then the predictions are conducted within the subspace of the defined attributes. Finally, the prediction confidence is measured by both the defined and the residual attributes. Experiments conducted on several benchmarks demonstrate that our classifier produces a superior performance to other methods under the risk-coverage trade-off metric.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_29');
INSERT INTO `paper` VALUES (11192, 'Self-Calibrating Isometric Non-Rigid Structure-from-Motion', 'NRSfM', 'Self-calibration', 'Uncalibrated camera', 'Differential geometry', 'Metric tensor', 'We present self-calibrating isometric non-rigid structure-from-motion (SCIso-NRSfM), the first method to reconstruct a non-rigid object from at least three monocular images with constant but unknown focal length. The majority of NRSfM methods using the perspective camera simply assume that the calibration is known. SCIso-NRSfM leverages the recent powerful differential approaches to NRSfM, based on formulating local polynomial constraints, where local means correspondence-wise. In NRSfM, the local shape may be solved from these constraints. In SCIso-NRSfM, the difficulty is to also solve for the focal length as a global variable. We propose to eliminate the shape using resultants, obtaining univariate polynomials for the focal length only, whose sum of squares can then be globally minimized. SCIso-NRSfM thus solves for the focal length by integrating the constraints for all correspondences and the whole image set. Once this is done, the local shape is easily recovered. Our experiments show that its performance is very close to the state-of-the-art methods that use a calibrated camera.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_16');
INSERT INTO `paper` VALUES (11193, 'Self-calibration of Cameras with Euclidean Image Plane in Case of Two Views and Known Relative Rotation Angle', 'Multiview geometry', 'Self-calibration', 'Essential matrix', 'Euclidean image plane', 'Relative rotation angle', 'The internal calibration of a pinhole camera is given by five parameters that are combined into an upper-triangular \\(3\\times 3\\) calibration matrix. If the skew parameter is zero and the aspect ratio is equal to one, then the camera is said to have Euclidean image plane. In this paper, we propose a non-iterative self-calibration algorithm for a camera with Euclidean image plane in case the remaining three internal parameters — the focal length and the principal point coordinates — are fixed but unknown. The algorithm requires a set of \\(N \\ge 7\\) point correspondences in two views and also the measured relative rotation angle between the views. We show that the problem generically has six solutions (including complex ones).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_26');
INSERT INTO `paper` VALUES (11194, 'Self-produced Guidance for Weakly-Supervised Object Localization', 'Object localization', 'Weakly Supervised Learning', '', '', '', 'Weakly supervised methods usually generate localization results based on attention maps produced by classification networks. However, the attention maps exhibit the most discriminative parts of the object which are small and sparse. We propose to generate Self-produced Guidance (SPG) masks which separate the foreground i.e., the object of interest, from the background to provide the classification networks with spatial correlation information of pixels. A stagewise approach is proposed to incorporate high confident object regions to learn the SPG masks. The high confident regions within attention maps are utilized to progressively learn the SPG masks. The masks are then used as an auxiliary pixel-level supervision to facilitate the training of classification networks. Extensive experiments on ILSVRC demonstrate that SPG is effective in producing high-quality object localizations maps. Particularly, the proposed SPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC validation set, which is a new state-of-the-art error rate.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_37');
INSERT INTO `paper` VALUES (11195, 'Self-supervised Knowledge Distillation Using Singular Value Decomposition', 'Statistical methods and learning', 'Optimization methods', 'Recognition: detection', 'Categorization', 'Indexing', 'To solve deep neural network (DNN)’s huge training dataset and its high computation issue, so-called teacher-student (T-S) DNN which transfers the knowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN has limited range of use, and the knowledge of T-DNN is insufficiently transferred to S-DNN. To improve the quality of the transferred knowledge from T-DNN, we propose a new knowledge distillation using singular value decomposition (SVD). In addition, we define a knowledge transfer as a self-supervised task and suggest a way to continuously receive information from T-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of the T-DNN can be up to 1.1% better than the T-DNN in terms of classification accuracy. Also assuming the same computational cost, our S-DNN outperforms the S-DNN driven by the state-of-the-art distillation with a performance advantage of 1.79%. code is available on https://github.com/sseung0703/SSKD_SVD.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_21');
INSERT INTO `paper` VALUES (11196, 'Self-Supervised Relative Depth Learning for Urban Scene Understanding', 'Self-supervised learning', 'Unsupervised domain adaptation', 'Urban scene understanding', 'Semantic segmentation', 'Monocular depth estimation', 'As an agent moves through the world, the apparent motion of scene elements is (usually) inversely proportional to their depth (Strictly speaking, this statement is true only after one has compensated for camera rotation, individual object motion, and image position. We address these issues in the paper). It is natural for a learning agent to associate image patterns with the magnitude of their displacement over time: as the agent moves, faraway mountains don’t move much; nearby trees move a lot. This natural relationship between the appearance of objects and their motion is a rich source of information about the world. In this work, we start by training a deep network, using fully automatic supervision, to predict relative scene depth from single images. The relative depth training images are automatically derived from simple videos of cars moving through a scene, using recent motion segmentation techniques, and no human-provided labels. The proxy task of predicting relative depth from a single image induces features in the network that result in large improvements in a set of downstream tasks including semantic segmentation, joint road segmentation and car detection, and monocular (absolute) depth estimation, over a network trained from scratch. The improvement on the semantic segmentation task is greater than that produced by any other automatically supervised methods. Moreover, for monocular depth estimation, our unsupervised pre-training method even outperforms supervised pre-training with ImageNet. In addition, we demonstrate benefits from learning to predict (again, completely unsupervised) relative depth in the specific videos associated with various downstream tasks (e.g., KITTI). We adapt to the specific scenes in those tasks in an unsupervised manner to improve performance. In summary, for semantic segmentation, we present state-of-the-art results among methods that do not use supervised pre-training, and we even exceed the performance of supervised ImageNet pre-trained models for monocular depth estimation, achieving results that are comparable with state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_2');
INSERT INTO `paper` VALUES (11197, 'Self-supervised Segmentation by Grouping Optical-Flow', '', '', '', '', '', 'We propose to self-supervise a convolutional neural network operating on images using temporal information from videos. The task is to learn a representation of single images and the supervision for this is obtained by learning to group image pixels in such a way that their collective motion is “coherent”. This learning by grouping approach is used as a pre-training as well as segmentation strategy. Preliminary results suggest that the segments obtained are reasonable and the representation learned transfers well for classification.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_31');
INSERT INTO `paper` VALUES (11198, 'Selfie Video Stabilization', '', '', '', '', '', 'We propose a novel algorithm for stabilizing selfie videos. Our goal is to automatically generate stabilized video that has optimal smooth motion in the sense of both foreground and background. The key insight is that non-rigid foreground motion in selfie videos can be analyzed using a 3D face model, and background motion can be analyzed using optical flow. We use second derivative of temporal trajectory of selected pixels as the measure of smoothness. Our algorithm stabilizes selfie videos by minimizing the smoothness measure of the background, regularized by the motion of the foreground. Experiments show that our method outperforms state-of-the-art general video stabilization techniques in selfie videos.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_34');
INSERT INTO `paper` VALUES (11199, 'Semantic Match Consistency for Long-Term Visual Localization', 'Visual localization', 'Semantic segmentation', 'Camera pose estimation', 'Outlier rejection', 'Self-driving cars', 'Robust and accurate visual localization across large appearance variations due to changes in time of day, seasons, or changes of the environment is a challenging problem which is of importance to application areas such as navigation of autonomous robots. Traditional feature-based methods often struggle in these conditions due to the significant number of erroneous matches between the image and the 3D model. In this paper, we present a method for scoring the individual correspondences by exploiting semantic information about the query image and the scene. In this way, erroneous correspondences tend to get a low semantic consistency score, whereas correct correspondences tend to get a high score. By incorporating this information in a standard localization pipeline, we show that the localization performance can be significantly improved compared to the state-of-the-art, as evaluated on two challenging long-term localization benchmarks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_24');
INSERT INTO `paper` VALUES (11200, 'Semantic Segmentation of Fisheye Images', 'Semantic segmentation', 'Fisheye images', 'Deep learning', '', '', 'Semantic segmentation of fisheye images (e.g., from action-cameras or smartphones) requires different training approaches and data than those of rectilinear images obtained using central projection. The shape of objects is distorted depending on the distance between the principal point and the object position in the image. Therefore, classical semantic segmentation approaches fall short in terms of performance compared to rectilinear data. A potential solution to this problem is the recording and annotation of a new dataset, however this is expensive and tedious. In this study, an alternative approach that modifies the augmentation stage of deep learning training to re-use rectilinear training data is presented. In this way we obtain a considerably higher semantic segmentation performance on the fisheye images: +18.3% intersection over union (IoU) for action-camera test images, +8.3% IoU for artificially generated fisheye data, and +18.0% IoU for challenging security scenes acquired in bird’s eye view.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_10');
INSERT INTO `paper` VALUES (11201, 'Semantically Aware Urban 3D Reconstruction with Plane-Based Regularization', 'Plane Detection Algorithm', 'Plane Hypothesis', 'Triple Line', 'Poisson Surface', 'Scene Parts', 'We propose a method for urban 3D reconstruction, which incorporates semantic information and plane priors within the reconstruction process in order to generate visually appealing 3D models. We introduce a plane detection algorithm using 3D lines, which detects a more complete and less spurious plane set compared to point-based methods in urban environments. Further, the proposed normalized visibility-based energy formulation eases the combination of several energy terms within a tetrahedra occupancy labeling algorithm and, hence, is well suited for combining it with class specific smoothness terms. As a result, we produce visually appealing and detailed building models (i.e., straight edges and planar surfaces) and a smooth reconstruction of the surroundings.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_29');
INSERT INTO `paper` VALUES (11202, 'Semantically Selective Augmentation for Deep Compact Person Re-Identification', 'Person re-identification', 'Selective augmentation', 'Face filtering', 'Adversarial synthesis', 'Deep compression', 'We present a deep person re-identification approach that combines semantically selective, deep data augmentation with clustering-based network compression to generate high performance, light and fast inference networks. In particular, we propose to augment limited training data via sampling from a deep convolutional generative adversarial network (DCGAN), whose discriminator is constrained by a semantic classifier to explicitly control the domain specificity of the generation process. Thereby, we encode information in the classifier network which can be utilized to steer adversarial synthesis, and which fuels our CondenseNet ID-network training. We provide a quantitative and qualitative analysis of the approach and its variants on a number of datasets, obtaining results that outperform the state-of-the-art on the LIMA dataset for long-term monitoring in indoor living spaces.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_41');
INSERT INTO `paper` VALUES (11203, 'Semi-convolutional Operators for Instance Segmentation', 'Instance embedding', 'Object detection', 'Instance segmentation', 'Coloring', 'Semi-convolutional', 'Object detection and instance segmentation are dominated by region-based methods such as Mask RCNN. However, there is a growing interest in reducing these problems to pixel labeling tasks, as the latter could be more efficient, could be integrated seamlessly in image-to-image network architectures as used in many other tasks, and could be more accurate for objects that are not well approximated by bounding boxes. In this paper we show theoretically and empirically that constructing dense pixel embeddings that can separate object instances cannot be easily achieved using convolutional operators. At the same time, we show that simple modifications, which we call semi-convolutional, have a much better chance of succeeding at this task. We use the latter to show a connection to Hough voting as well as to a variant of the bilateral kernel that is spatially steered by a convolutional network. We demonstrate that these operators can also be used to improve approaches such as Mask RCNN, demonstrating better segmentation of complex biological shapes and PASCAL VOC categories than achievable by Mask RCNN alone.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_6');
INSERT INTO `paper` VALUES (11204, 'Semi-dense 3D Reconstruction with a Stereo Event Camera', 'Camera Effects', 'Simultaneous Localization And Mapping (SLAM)', 'STEREO Observations', 'Inverse Depth', 'Standard Camera', 'Event cameras are bio-inspired sensors that offer several advantages, such as low latency, high-speed and high dynamic range, to tackle challenging scenarios in computer vision. This paper presents a solution to the problem of 3D reconstruction from data captured by a stereo event-camera rig moving in a static scene, such as in the context of stereo Simultaneous Localization and Mapping. The proposed method consists of the optimization of an energy function designed to exploit small-baseline spatio-temporal consistency of events triggered across both stereo image planes. To improve the density of the reconstruction and to reduce the uncertainty of the estimation, a probabilistic depth-fusion strategy is also developed. The resulting method has no special requirements on either the motion of the stereo event-camera rig or on prior knowledge about the scene. Experiments demonstrate our method can deal with both texture-rich scenes as well as sparse scenes, outperforming state-of-the-art stereo methods based on event data image representations.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_15');
INSERT INTO `paper` VALUES (11205, 'Semi-independent Stereo Visual Odometry for Different Field of View Cameras', 'Stereo visual odometry', 'Field of view', '3D reconstruction', '', '', 'This paper presents a pipeline for stereo visual odometry using cameras with different fields of view. It gives a proof of concept about how a constraint on the respective field of view of each camera can lead to both an accurate 3D reconstruction and a robust pose estimation. Indeed, when considering a fixed resolution, a narrow field of view has a higher angular resolution and can preserve image texture details. On the other hand, a wide field of view allows to track features over longer periods since the overlap between two successive frames is more substantial. We propose a semi-independent stereo system where each camera performs individually temporal multi-view optimization but their initial parameters are still jointly optimized in an iterative framework. Furthermore, the concept of lead and follow camera is introduced to adaptively propagate information between the cameras. We evaluate the method qualitatively on two indoor datasets, and quantitatively on a synthetic dataset to allow the comparison across different fields of view.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_26');
INSERT INTO `paper` VALUES (11206, 'Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model', 'Face Images', 'Synthetic Input Image', 'Identity Preservation', 'Domain Adaptation', 'Conservative Ideals', 'We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with a wide range of expressions, poses, and illuminations conditioned by synthetic images sampled from a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with a large volume of paired data or train highly under-constrained two-way generative networks in an unsupervised fashion. We propose a semi-supervised adversarial learning framework to constrain the two-way networks by a small number of paired real and synthetic images, along with a large volume of unpaired data. A set-based loss is also proposed to preserve identity coherence of generated images. Qualitative results show that generated face images of new identities contain pose, lighting and expression diversity. They are also highly constrained by the synthetic input images while adding photorealism and retaining identity information. We combine face images generated by the proposed method with a real data set to train face recognition algorithms and evaluate the model quantitatively on two challenging data sets: LFW and IJB-A. The generated images by our framework consistently improve the performance of deep face recognition networks trained with the Oxford VGG Face dataset, and achieve comparable results to the state-of-the-art.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_14');
INSERT INTO `paper` VALUES (11207, 'Semi-supervised Deep Learning with Memory', 'Semi-supervised learning', 'Neural network with memory', '', '', '', 'We consider the semi-supervised multi-class classification problem of learning from sparse labelled and abundant unlabelled training data. To address this problem, existing semi-supervised deep learning methods often rely on the up-to-date “network-in-training” to formulate the semi-supervised learning objective. This ignores both the discriminative feature representation and the model inference uncertainty revealed by the network in the preceding learning iterations, referred to as the memory of model learning. In this work, we propose a novel Memory-Assisted Deep Neural Network (MA-DNN) capable of exploiting the memory of model learning to enable semi-supervised learning. Specifically, we introduce a memory mechanism into the network training process as an assimilation-accommodation interaction between the network and an external memory module. Experiments demonstrate the advantages of the proposed MA-DNN model over the state-of-the-art semi-supervised deep learning methods on three image classification benchmark datasets: SVHN, CIFAR10, and CIFAR100.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_17');
INSERT INTO `paper` VALUES (11208, 'Semi-supervised FusedGAN for Conditional Image Generation', '', '', '', '', '', 'We present FusedGAN, a deep network for conditional image synthesis with controllable sampling of diverse images. Fidelity, diversity and controllable sampling are the main quality measures of a good image generation model. Most existing models are insufficient in all three aspects. The FusedGAN can perform controllable sampling of diverse images with very high fidelity. We argue that controllability can be achieved by disentangling the generation process into various stages. In contrast to stacked GANs, where multiple stages of GANs are trained separately with full supervision of labeled intermediate images, the FusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike existing methods, which require full supervision with paired conditions and images, the FusedGAN can effectively leverage more abundant images without corresponding conditions in training, to produce more diverse samples with high fidelity. We achieve this by fusing two generators: one for unconditional image generation, and the other for conditional image generation, where the two partly share a common latent space thereby disentangling the generation. We demonstrate the efficacy of the FusedGAN in fine grained image generation tasks such as text-to-image, and attribute-to-face generation.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_41');
INSERT INTO `paper` VALUES (11209, 'Semi-supervised Generative Adversarial Hashing for Image Retrieval', 'Information retrieval', 'Hashing', 'Deep learning', 'GANs', '', 'With explosive growth of image and video data on the Internet, hashing technique has been extensively studied for large-scale visual search. Benefiting from the advance of deep learning, deep hashing methods have achieved promising performance. However, those deep hashing models are usually trained with supervised information, which is rare and expensive in practice, especially class labels. In this paper, inspired by the idea of generative models and the minimax two-player game, we propose a novel semi-supervised generative adversarial hashing (SSGAH) approach. Firstly, we unify a generative model, a discriminative model and a deep hashing model in a framework for making use of triplet-wise information and unlabeled data. Secondly, we design novel structure of the generative model and the discriminative model to learn the distribution of triplet-wise information in a semi-supervised way. In addition, we propose a semi-supervised ranking loss and an adversary ranking loss to learn binary codes which preserve semantic similarity for both labeled data and unlabeled data. Finally, by optimizing the whole model in an adversary training way, the learned binary codes can capture better semantic information of all data. Extensive empirical evaluations on two widely-used benchmark datasets show that our proposed approach significantly outperforms state-of-the-art hashing methods.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_29');
INSERT INTO `paper` VALUES (11210, 'Semi-supervised Semantic Matching', 'Semantic-matching', 'Geometric matching', 'Deep-learning', '', '', 'Convolutional neural networks (CNNs) have been successfully applied to solve the problem of correspondence estimation between semantically related images. Due to non-availability of large training datasets, existing methods resort to self-supervised or unsupervised training paradigm. In this paper we propose a semi-supervised learning framework that imposes cyclic consistency constraint on unlabeled image pairs. Together with the supervised loss the proposed model achieves state-of-the-art on a benchmark semantic matching dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_32');
INSERT INTO `paper` VALUES (11211, 'Separating Reflection and Transmission Images in the Wild', '', '', '', '', '', 'The reflections caused by common semi-reflectors, such as glass windows, can impact the performance of computer vision algorithms. State-of-the-art methods can remove reflections on synthetic data and in controlled scenarios. However, they are based on strong assumptions and do not generalize well to real-world images. Contrary to a common misconception, real-world images are challenging even when polarization information is used. We present a deep learning approach to separate the reflected and the transmitted components of the recorded irradiance, which explicitly uses the polarization properties of light. To train it, we introduce an accurate synthetic data generation pipeline, which simulates realistic reflections, including those generated by curved and non-ideal surfaces, non-static scenes, and high-dynamic-range scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_6');
INSERT INTO `paper` VALUES (11212, 'Sequential Clique Optimization for Video Object Segmentation', 'Video object segmentation', 'Primary object segmentation', 'Salient object detection', 'Sequential clique optimization', '', 'A novel algorithm to segment out objects in a video sequence is proposed in this work. First, we extract object instances in each frame. Then, we select a visually important object instance in each frame to construct the salient object track through the sequence. This can be formulated as finding the maximal weight clique in a complete k-partite graph, which is NP hard. Therefore, we develop the sequential clique optimization (SCO) technique to efficiently determine the cliques corresponding to salient object tracks. We convert these tracks into video object segmentation results. Experimental results show that the proposed algorithm significantly outperforms the state-of-the-art video object segmentation and video salient object detection algorithms on recent benchmark datasets.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_32');
INSERT INTO `paper` VALUES (11213, 'Shape Reconstruction Using Volume Sweeping and Learned Photoconsistency', 'Multi view', 'Stereo reconstruction', 'Learned photoconsistency', 'Performance capture', 'Volume sweeping', 'The rise of virtual and augmented reality fuels an increased need for content suitable to these new technologies including 3D contents obtained from real scenes. We consider in this paper the problem of 3D shape reconstruction from multi-view RGB images. We investigate the ability of learning-based strategies to effectively benefit the reconstruction of arbitrary shapes with improved precision and robustness. We especially target real life performance capture, containing complex surface details that are difficult to recover with existing approaches. A key step in the multi-view reconstruction pipeline lies in the search for matching features between viewpoints in order to infer depth information. We propose to cast the matching on a 3D receptive field along viewing lines and to learn a multi-view photoconsistency measure for that purpose. The intuition is that deep networks have the ability to learn local photometric configurations in a broad way, even with respect to different orientations along various viewing lines of the same surface point. Our results demonstrate this ability, showing that a CNN, trained on a standard static dataset, can help recover surface details on dynamic scenes that are not perceived by traditional 2D feature based methods. Our evaluation also shows that our solution compares on par to state-of-the-art-reconstruction pipelines on standard evaluation datasets, while yielding significantly better results and generalization with realistic performance capture data.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_48');
INSERT INTO `paper` VALUES (11214, 'ShapeCodes: Self-supervised Feature Learning by Lifting Views to Viewgrids', 'Unseen Views', 'Unsupervised Feature Learning', 'Mental Rotation', 'Unknown Viewpoint', 'ShapeNet', 'We introduce an unsupervised feature learning approach that embeds 3D shape information into a single-view image representation. The main idea is a self-supervised training objective that, given only a single 2D image, requires all unseen views of the object to be predictable from learned features. We implement this idea as an encoder-decoder convolutional neural network. The network maps an input image of an unknown category and unknown viewpoint to a latent space, from which a deconvolutional decoder can best “lift” the image to its complete viewgrid showing the object from all viewing angles. Our class-agnostic training procedure encourages the representation to capture fundamental shape primitives and semantic regularities in a data-driven manner—without manual semantic labels. Our results on two widely-used shape datasets show (1) our approach successfully learns to perform “mental rotation” even for objects unseen during training, and (2) the learned latent space is a powerful representation for object recognition, outperforming several existing unsupervised feature learning methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_8');
INSERT INTO `paper` VALUES (11215, 'ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking', 'Intuitive physics', 'Stability prediction', 'Object stacking', '', '', 'Physical intuition is pivotal for intelligent agents to perform complex tasks. In this paper we investigate the passive acquisition of an intuitive understanding of physical principles as well as the active utilisation of this intuition in the context of generalised object stacking. To this end, we provide ShapeStacks (Source code & data are available at http://shapestacks.robots.ox.ac.uk): a simulation-based dataset featuring 20,000 stack configurations composed of a variety of elementary geometric primitives richly annotated regarding semantics and structural stability. We train visual classifiers for binary stability prediction on the ShapeStacks data and scrutinise their learned physical intuition. Due to the richness of the training data our approach also generalises favourably to real-world scenarios achieving state-of-the-art stability prediction on a publicly available benchmark of block towers. We then leverage the physical intuition learned by our model to actively construct stable stacks and observe the emergence of an intuitive notion of stackability - an inherent object affordance - induced by the active stacking task. Our approach performs well exceeding the stack height observed during training and even manages to counterbalance initially unstable structures.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_43');
INSERT INTO `paper` VALUES (11216, 'Shift-Net: Image Inpainting via Deep Feature Rearrangement', 'Inpainting', 'Feature rearrangement', 'Deep learning', '', '', 'Deep convolutional networks (CNNs) have exhibited their potential in image inpainting for producing plausible results. However, in most existing methods, e.g., context encoder, the missing parts are predicted by propagating the surrounding convolutional features through a fully connected layer, which intends to produce semantically plausible but blurry result. In this paper, we introduce a special shift-connection layer to the U-Net architecture, namely Shift-Net, for filling in missing regions of any shape with sharp structures and fine-detailed textures. To this end, the encoder feature of the known region is shifted to serve as an estimation of the missing parts. A guidance loss is introduced on decoder feature to minimize the distance between the decoder feature after fully connected layer and the ground-truth encoder feature of the missing parts. With such constraint, the decoder feature in missing region can be used to guide the shift of encoder feature in known region. An end-to-end learning algorithm is further developed to train the Shift-Net. Experiments on the Paris StreetView and Places datasets demonstrate the efficiency and effectiveness of our Shift-Net in producing sharper, fine-detailed, and visually plausible results. The codes and pre-trained models are available at https://github.com/Zhaoyi-Yan/Shift-Net.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_1');
INSERT INTO `paper` VALUES (11217, 'Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data', 'Image captioning', 'Language and vision', 'Text-image retrieval', '', '', 'The aim of image captioning is to generate captions by machine to describe image contents. Despite many efforts, generating discriminative captions for images remains non-trivial. Most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. In this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. It brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the quality of generated captions. (2) The correspondence between generated captions and images are naturally incorporated in the generation process without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional annotations. We demonstrate the effectiveness of the proposed retrieval-guided method on COCO and Flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_21');
INSERT INTO `paper` VALUES (11218, 'Shuffle-Then-Assemble: Learning Object-Agnostic Visual Relationship Features', '', '', '', '', '', 'Due to the fact that it is prohibitively expensive to completely annotate visual relationships, i.e., the (obj1, rel, obj2) triplets, relationship models are inevitably biased to object classes of limited pairwise patterns, leading to poor generalization to rare or unseen object combinations. Therefore, we are interested in learning object-agnostic visual features for more generalizable relationship models. By “agnostic”, we mean that the feature is less likely biased to the classes of paired objects. To alleviate the bias, we propose a novel Shuffle-Then-Assemble pre-training strategy. First, we discard all the triplet relationship annotations in an image, leaving two unpaired object domains without obj1-obj2 alignment. Then, our feature learning is to recover possible obj1-obj2 pairs. In particular, we design a cycle of residual transformations between the two domains, to capture shared but not object-specific visual patterns. Extensive experiments on two visual relationship benchmarks show that by using our pre-trained features, naive relationship models can be consistently improved and even outperform other state-of-the-art relationship models. Code has been made available at: https://github.com/yangxuntu/vrd.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_3');
INSERT INTO `paper` VALUES (11219, 'ShuffleDet: Real-Time Vehicle Detection Network in On-Board Embedded UAV Imagery', 'UAV imagery', 'Real-time vehicle detection', 'On-board embedded processing', 'Convolutional neural networks', 'Traffic monitoring', 'On-board real-time vehicle detection is of great significance for UAVs and other embedded mobile platforms. We propose a computationally inexpensive detection network for vehicle detection in UAV imagery which we call ShuffleDet. In order to enhance the speed-wise performance, we construct our method primarily using channel shuffling and grouped convolutions. We apply inception modules and deformable modules to consider the size and geometric shape of the vehicles. ShuffleDet is evaluated on CARPK and PUCPR+ datasets and compared against the state-of-the-art real-time object detection networks. ShuffleDet achieves 3.8 GFLOPs while it provides competitive performance on test sets of both datasets. We show that our algorithm achieves real-time performance by running at the speed of 14 frames per second on NVIDIA Jetson TX2 showing high potential for this method for real-time processing in UAVs.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_7');
INSERT INTO `paper` VALUES (11220, 'ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design', 'CNN architecture design', 'Efficiency', 'Practical', '', '', 'Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_8');
INSERT INTO `paper` VALUES (11221, 'Sidekick Policy Learning for Active Visual Exploration', 'Visual exploration', 'Reinforcement learning', '', '', '', 'We consider an active visual exploration scenario, where an agent must intelligently select its camera motions to efficiently reconstruct the full environment from only a limited set of narrow field-of-view glimpses. While the agent has full observability of the environment during training, it has only partial observability once deployed, being constrained by what portions it has seen and what camera motions are permissible. We introduce sidekick policy learning to capitalize on this imbalance of observability. The main idea is a preparatory learning phase that attempts simplified versions of the eventual exploration task, then guides the agent via reward shaping or initial policy supervision. To support interpretation of the resulting policies, we also develop a novel policy visualization technique. Results on active visual exploration tasks with \\(360^{\\circ }\\) scenes and 3D objects show that sidekicks consistently improve performance and convergence rates over existing methods. Code, data and demos are available (Project website: http://vision.cs.utexas.edu/projects/sidekicks/).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_26');
INSERT INTO `paper` VALUES (11222, 'Simple Baselines for Human Pose Estimation and Tracking', 'Human pose estimation', 'Human pose tracking', '', '', '', 'There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_29');
INSERT INTO `paper` VALUES (11223, 'Simultaneous 3D Reconstruction for Water Surface and Underwater Scene', '3D reconstruction', 'Water surface', 'Underwater imaging', '', '', 'This paper presents the first approach for simultaneously recovering the 3D shape of both the wavy water surface and the moving underwater scene. A portable camera array system is constructed, which captures the scene from multiple viewpoints above the water. The correspondences across these cameras are estimated using an optical flow method and are used to infer the shape of the water surface and the underwater scene. We assume that there is only one refraction occurring at the water interface. Under this assumption, two estimates of the water surface normals should agree: one from Snell’s law of light refraction and another from local surface structure. The experimental results using both synthetic and real data demonstrate the effectiveness of the presented approach.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_46');
INSERT INTO `paper` VALUES (11224, 'Simultaneous Edge Alignment and Learning', 'Edge Alignment', 'Learning Edge', 'Fully Convolutional Network (FCNs)', 'Label Alignment', 'Sigmoid Cross Entropy Loss', 'Edge detection is among the most fundamental vision problems for its role in perceptual grouping and its wide applications. Recent advances in representation learning have led to considerable improvements in this area. Many state of the art edge detection models are learned with fully convolutional networks (FCNs). However, FCN-based edge learning tends to be vulnerable to misaligned labels due to the delicate structure of edges. While such problem was considered in evaluation benchmarks, similar issue has not been explicitly addressed in general edge learning. In this paper, we show that label misalignment can cause considerably degraded edge learning quality, and address this issue by proposing a simultaneous edge alignment and learning framework. To this end, we formulate a probabilistic model where edge alignment is treated as latent variable optimization, and is learned end-to-end during network training. Experiments show several applications of this work, including improved edge detection with state of the art performance, and automatic refinement of noisy annotations.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_24');
INSERT INTO `paper` VALUES (11225, 'Single Image Highlight Removal with a Sparse and Low-Rank Reflection Model', 'Highlight removal', 'Low-rank', 'Sparse', 'Diffuse reflection', '', 'We propose a sparse and low-rank reflection model for specular highlight detection and removal using a single input image. This model is motivated by the observation that the specular highlight of a natural image usually has large intensity but is rather sparsely distributed while the remaining diffuse reflection can be well approximated by a linear combination of several distinct colors with a sparse and low-rank weighting matrix. We further impose the non-negativity constraint on the weighting matrix as well as the highlight component to ensure that the model is purely additive. With this reflection model, we reformulate the task of highlight removal as a constrained nuclear norm and \\(l_1\\)-norm minimization problem which can be solved effectively by the augmented Lagrange multiplier method. Experimental results show that our method performs well on both synthetic images and many real-world examples and is competitive with previous methods, especially in some challenging scenarios featuring natural illumination, hue-saturation ambiguity and strong noises.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_17');
INSERT INTO `paper` VALUES (11226, 'Single Image Intrinsic Decomposition Without a Single Intrinsic Image', 'Intrinsic decomposition', 'Unsupervised learning', 'Self-supervised learning', '', '', 'Intrinsic image decomposition—decomposing a natural image into a set of images corresponding to different physical causes—is one of the key and fundamental problems of computer vision. Previous intrinsic decomposition approaches either address the problem in a fully supervised manner, or require multiple images of the same scene as input. These approaches are less desirable in practice, as ground truth intrinsic images are extremely difficult to acquire, and requirement of multiple images pose severe limitation on applicable scenarios. In this paper, we propose to bring the best of both worlds. We present a two stream convolutional neural network framework that is capable of learning the decomposition effectively in the absence of any ground truth intrinsic images, and can be easily extended to a (semi-)supervised setup. At inference time, our model can be easily reduced to a single stream module that performs intrinsic decomposition on a single input image. We demonstrate the effectiveness of our framework through extensive experimental study on both synthetic and real-world datasets, showing superior performance over previous approaches in both single-image and multi-image settings. Notably, our approach outperforms previous state-of-the-art single image methods while using only 50% of ground truth supervision.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_13');
INSERT INTO `paper` VALUES (11227, 'Single Image Water Hazard Detection Using FCN with Reflection Attention Units', 'Water puddle detection', 'Road hazard detection', 'Fully convolutional network', 'Deep learning', 'Reflection attention unit', 'Water bodies, such as puddles and flooded areas, on and off road pose significant risks to autonomous cars. Detecting water from moving camera is a challenging task as water surface is highly refractive, and its appearance varies with viewing angle, surrounding scene, weather conditions. In this paper, we present a water puddle detection method based on a Fully Convolutional Network (FCN) with our newly proposed Reflection Attention Units (RAUs). An RAU is a deep network unit designed to embody the physics of reflection on water surface from sky and nearby scene. To verify the performance of our proposed method, we collect 11455 color stereo images with polarizers, and 985 of left images are annotated and divided into 2 datasets: On Road (ONR) dataset and Off Road (OFR) dataset. We show that FCN-8s with RAUs improves significantly precision and recall metrics as compared to FCN-8s, DeepLab V2 and Gaussian Mixture Model (GMM). We also show that focal loss function can improve the performance of FCN-8s network due to the extreme imbalance of water versus ground classification problem.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_7');
INSERT INTO `paper` VALUES (11228, 'Single Shot Scene Text Retrieval', 'Image retrieval', 'Scene text', 'Word spotting', 'Convolutional neural networks', 'Region proposals networks', 'Textual information found in scene images provides high level semantic information about the image and its context and it can be leveraged for better scene understanding. In this paper we address the problem of scene text retrieval: given a text query, the system must return all images containing the queried text. The novelty of the proposed model consists in the usage of a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them. In this way, the text based image retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the CNN over the entire image database. Our experiments demonstrate that the proposed architecture outperforms previous state-of-the-art while it offers a significant increase in processing speed.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_43');
INSERT INTO `paper` VALUES (11229, 'Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning', 'Skeleton-based action recognition', 'Spatial reasoning', 'Temporal stack learning', 'Clip-based incremental loss', '', 'Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, the representations of skeleton sequences captured by most of the previous methods lack spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton-based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than the state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_7');
INSERT INTO `paper` VALUES (11230, 'SketchyScene: Richly-Annotated Scene Sketches', 'Sketch dataset', 'Scene sketch', 'Sketch segmentation', '', '', 'We contribute the first large-scale dataset of scene sketches, SketchyScene, with the goal of advancing research on sketch understanding at both the object and scene level. The dataset is created through a novel and carefully designed crowdsourcing pipeline, enabling users to efficiently generate large quantities of realistic and diverse scene sketches. SketchyScene contains more than 29,000 scene-level sketches, 7,000+ pairs of scene templates and photos, and 11,000+ object sketches. All objects in the scene sketches have ground-truth semantic and instance masks. The dataset is also highly scalable and extensible, easily allowing augmenting and/or changing scene composition. We demonstrate the potential impact of SketchyScene by training new computational models for semantic segmentation of scene sketches and showing how the new dataset enables several applications including image retrieval, sketch colorization, editing, and captioning, etc. The dataset and code can be found at https://github.com/SketchyScene/SketchyScene.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_26');
INSERT INTO `paper` VALUES (11231, 'SkipNet: Learning Dynamic Routing in Convolutional Networks', 'SkipNet', 'Gating Network', 'Dynamic Skipping', 'Hybrid Reinforcement Learning (HRL)', 'Skipping Policy', 'While deeper convolutional networks are needed to achieve maximum accuracy in visual perception tasks, for many inputs shallower networks are sufficient. We exploit this observation by learning to skip convolutional layers on a per-input basis. We introduce SkipNet, a modified residual network, that uses a gating network to selectively skip convolutional blocks based on the activations of the previous layer. We formulate the dynamic skipping problem in the context of sequential decision making and propose a hybrid learning algorithm that combines supervised learning and reinforcement learning to address the challenges of non-differentiable skipping decisions. We show SkipNet reduces computation by \\(30-90\\%\\) while preserving the accuracy of the original model on four benchmark datasets and outperforms the state-of-the-art dynamic networks and static compression methods. We also qualitatively evaluate the gating policy to reveal a relationship between image scale and saliency and the number of layers skipped.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_25');
INSERT INTO `paper` VALUES (11232, 'Small Defect Detection Using Convolutional Neural Network Features and Random Forests', 'Defect detection', 'Non-destructive evaluation', 'CNN', 'Local features', 'Random Forests', 'We address the problem of identifying small abnormalities in an imaged region, important in applications such as industrial inspection. The goal is to label the pixels corresponding to a defect with a minimum of false positives. A common approach is to run a sliding-window classifier over the image. Recent Fully Convolutional Networks (FCNs), such as U-Net, can be trained to identify pixels corresponding to abnormalities given a suitable training set. However in many application domains it is hard to collect large numbers of defect examples (by their nature they are rare). Although U-Net can work in this scenario, we show that better results can be obtained by replacing the final softmax layer of the network with a Random Forest (RF) using features sampled from the earlier network layers. We also demonstrate that rather than just thresholding the resulting probability image to identify defects it is better to compute Maximally Stable Extremal Regions (MSERs). We apply the approach to the challenging problem of identifying defects in radiographs of aerospace welds.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_35');
INSERT INTO `paper` VALUES (11233, 'Small-Scale Pedestrian Detection Based on Topological Line Localization and Temporal Feature Aggregation', 'Small-scale pedestrian detection', 'Multi-scale', 'Temporal feature aggregation', 'Markov random field', 'Deep learning', 'A critical issue in pedestrian detection is to detect small-scale objects that will introduce feeble contrast and motion blur in images and videos, which in our opinion should partially resort to deep-rooted annotation bias. Motivated by this, we propose a novel method integrated with somatic topological line localization (TLL) and temporal feature aggregation for detecting multi-scale pedestrians, which works particularly well with small-scale pedestrians that are relatively far from the camera. Moreover, a post-processing scheme based on Markov Random Field (MRF) is introduced to eliminate ambiguities in occlusion cases. Applying with these methodologies comprehensively, we achieve best detection performance on Caltech benchmark and improve performance of small-scale objects significantly (miss rate decreases from 74.53% to 60.79%). Beyond this, we also achieve competitive performance on CityPersons dataset and show the existence of annotation bias in KITTI dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_33');
INSERT INTO `paper` VALUES (11234, 'Snap Angle Prediction for 360\\(^{\\circ }\\) Panoramas', '360\\(^{\\circ }\\) panoramas', 'Content-aware projection', 'Foreground objects', '', '', '360\\(^{\\circ }\\) panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama’s content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_1');
INSERT INTO `paper` VALUES (11235, 'SOD-MTGAN: Small Object Detection via Multi-Task Generative Adversarial Network', 'Small object detection', 'Super-resolution', 'Multi-task', 'Generative adversarial network', 'COCO', 'Object detection is a fundamental and important problem in computer vision. Although impressive results have been achieved on large/medium sized objects in large-scale detection benchmarks (e.g. the COCO dataset), the performance on small objects is far from satisfactory. The reason is that small objects lack sufficient detailed appearance information, which can distinguish them from the background or similar objects. To deal with the small object detection problem, we propose an end-to-end multi-task generative adversarial network (MTGAN). In the MTGAN, the generator is a super-resolution network, which can up-sample small blurred images into fine-scale ones and recover detailed information for more accurate detection. The discriminator is a multi-task network, which describes each super-resolved image patch with a real/fake score, object category scores, and bounding box regression offsets. Furthermore, to make the generator recover more details for easier detection, the classification and regression losses in the discriminator are back-propagated into the generator during training. Extensive experiments on the challenging COCO dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a blurred small one, and show that the detection performance, especially for small sized objects, improves over state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_13');
INSERT INTO `paper` VALUES (11236, 'Sparsely Aggregated Convolutional Networks', '', '', '', '', '', 'We explore a key architectural aspect of deep convolutional neural networks: the pattern of internal skip connections used to aggregate outputs of earlier layers for consumption by deeper layers. Such aggregation is critical to facilitate training of very deep networks in an end-to-end manner. This is a primary reason for the widespread adoption of residual networks, which aggregate outputs via cumulative summation. While subsequent works investigate alternative aggregation operations (e.g. concatenation), we focus on an orthogonal question: which outputs to aggregate at a particular point in the network. We propose a new internal connection structure which aggregates only a sparse set of previous outputs at any given depth. Our experiments demonstrate this simple design change offers superior performance with fewer parameters and lower computational requirements. Moreover, we show that sparse aggregation allows networks to scale more robustly to 1000+ layers, thereby opening future avenues for training long-running visual processes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_12');
INSERT INTO `paper` VALUES (11237, 'Spatial-Temporal Attention Res-TCN for Skeleton-Based Dynamic Hand Gesture Recognition', 'Dynamic hand gesture recognition', 'Spatial-Temporal Attention', 'Temporal Convolutional Networks', '', '', 'Dynamic hand gesture recognition is a crucial yet challenging task in computer vision. The key of this task lies in an effective extraction of discriminative spatial and temporal features to model the evolutions of different gestures. In this paper, we propose an end-to-end Spatial-Temporal Attention Residual Temporal Convolutional Network (STA-Res-TCN) for skeleton-based dynamic hand gesture recognition, which learns different levels of attention and assigns them to each spatial-temporal feature extracted by the convolution filters at each time step. The proposed attention branch assists the networks to adaptively focus on the informative time frames and features while exclude the irrelevant ones that often bring in unnecessary noise. Moreover, our proposed STA-Res-TCN is a lightweight model that can be trained and tested in an extremely short time. Experiments on DHG-14/28 Dataset and SHREC’17 Track Dataset show that STA-Res-TCN outperforms state-of-the-art methods on both the 14 gestures setting and the more complicated 28 gestures setting.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_18');
INSERT INTO `paper` VALUES (11238, 'Spatio-temporal Channel Correlation Networks for Action Classification', '', '', '', '', '', 'The work in this paper is driven by the question if spatio-temporal correlations are enough for 3D convolutional neural networks (CNN)? Most of the traditional 3D networks use local spatio-temporal features. We introduce a new block that models correlations between channels of a 3D CNN with respect to temporal and spatial features. This new block can be added as a residual unit to different parts of 3D CNNs. We name our novel block ‘Spatio-Temporal Channel Correlation’ (STC). By embedding this block to the current state-of-the-art architectures such as ResNext and ResNet, we improve the performance by 2–3% on the Kinetics dataset. Our experiments show that adding STC blocks to current state-of-the-art architectures outperforms the state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D CNNs is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D CNNs is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by fine-tuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and fine-tuned on the target datasets, e.g. HMDB51/UCF101.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_18');
INSERT INTO `paper` VALUES (11239, 'Spatio-Temporal Transformer Network for Video Restoration', 'Spatio-temporal transformer network', 'Spatio-temporal flow', 'Spatio-temporal sampler', 'Video super-resolution', 'Video deblurring', 'State-of-the-art video restoration methods integrate optical flow estimation networks to utilize temporal information. However, these networks typically consider only a pair of consecutive frames and hence are not capable of capturing long-range temporal dependencies and fall short of establishing correspondences across several timesteps. To alleviate these problems, we propose a novel Spatio-temporal Transformer Network (STTN) which handles multiple frames at once and thereby manages to mitigate the common nuisance of occlusions in optical flow estimation. Our proposed STTN comprises a module that estimates optical flow in both space and time and a resampling layer that selectively warps target frames using the estimated flow. In our experiments, we demonstrate the efficiency of the proposed network and show state-of-the-art restoration results in video super-resolution and video deblurring.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_7');
INSERT INTO `paper` VALUES (11240, 'Specular-to-Diffuse Translation for Multi-view Reconstruction', 'Generative adversarial network', 'Multi-view reconstruction', 'Multi-view coherence', 'Specular-to-diffuse', 'Image translation', 'Most multi-view 3D reconstruction algorithms, especially when shape-from-shading cues are used, assume that object appearance is predominantly diffuse. To alleviate this restriction, we introduce S2Dnet, a generative adversarial network for transferring multiple views of objects with specular reflection into diffuse ones, so that multi-view reconstruction methods can be applied more effectively. Our network extends unsupervised image-to-image translation to multi-view “specular to diffuse” translation. To preserve object appearance across multiple views, we introduce a Multi-View Coherence loss (MVC) that evaluates the similarity and faithfulness of local patches after the view-transformation. In addition, we carefully design and generate a large synthetic training data set using physically-based rendering. During testing, our network takes only the raw glossy images as input, without extra information such as segmentation masks or lighting estimation. Results demonstrate that multi-view reconstruction can be significantly improved using the images filtered by our network.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_12');
INSERT INTO `paper` VALUES (11241, 'SphereNet: Learning Spherical Representations for Detection and Classification in Omnidirectional Images', '', '', '', '', '', 'Omnidirectional cameras offer great benefits over classical cameras wherever a wide field of view is essential, such as in virtual reality applications or in autonomous robots. Unfortunately, standard convolutional neural networks are not well suited for this scenario as the natural projection surface is a sphere which cannot be unwrapped to a plane without introducing significant distortions, particularly in the polar regions. In this work, we present SphereNet, a novel deep learning framework which encodes invariance against such distortions explicitly into convolutional neural networks. Towards this goal, SphereNet adapts the sampling locations of the convolutional filters, effectively reversing distortions, and wraps the filters around the sphere. By building on regular convolutions, SphereNet enables the transfer of existing perspective convolutional neural network models to the omnidirectional case. We demonstrate the effectiveness of our method on the tasks of image classification and object detection, exploiting two newly created semi-synthetic and real-world omnidirectional datasets.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_32');
INSERT INTO `paper` VALUES (11242, 'SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters', 'Convolutional neural network', 'Parametrized convolutional filters', 'Point clouds', '', '', 'Deep neural networks have enjoyed remarkable success for various vision tasks, however it remains challenging to apply CNNs to domains lacking a regular underlying structures such as 3D point clouds. Towards this we propose a novel convolutional architecture, termed SpiderCNN, to efficiently extract geometric features from point clouds. SpiderCNN is comprised of units called SpiderConv, which extend convolutional operations from regular grids to irregular point sets that can be embedded in \\(\\mathbb {R}^n\\), by parametrizing a family of convolutional filters. We design the filter as a product of a simple step function that captures local geodesic information and a Taylor polynomial that ensures the expressiveness. SpiderCNN inherits the multi-scale hierarchical architecture from classical CNNs, which allows it to extract semantic deep features. Experiments on ModelNet40 demonstrate that SpiderCNN achieves state-of-the-art accuracy \\(92.4 \\%\\) on standard benchmarks, and shows competitive performance on segmentation task.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_6');
INSERT INTO `paper` VALUES (11243, 'SRDA: Generating Instance Segmentation Annotation via Scanning, Reasoning and Domain Adaptation', '3D scanning', 'Physical reasoning', 'Domain adaptation', '', '', 'Instance segmentation is a problem of significance in computer vision. However, preparing annotated data for this task is extremely time-consuming and costly. By combining the advantages of 3D scanning, reasoning, and GAN-based domain adaptation techniques, we introduce a novel pipeline named SRDA to obtain large quantities of training samples with very minor effort. Our pipeline is well-suited to scenes that can be scanned, i.e. most indoor and some outdoor scenarios. To evaluate our performance, we build three representative scenes and a new dataset, with 3D models of various common objects categories and annotated real-world scene images. Extensive experiments show that our pipeline can achieve decent instance segmentation performance given very low human labor cost.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_8');
INSERT INTO `paper` VALUES (11244, 'SRFeat: Single Image Super-Resolution with Feature Discrimination', 'Super-resolution', 'Adversarial network', 'High frequency features', 'Perceptual quality', '', 'Generative adversarial networks (GANs) have recently been adopted to single image super-resolution (SISR) and showed impressive results with realistically synthesized high-frequency textures. However, the results of such GAN-based approaches tend to include less meaningful high-frequency noise that is irrelevant to the input image. In this paper, we propose a novel GAN-based SISR method that overcomes the limitation and produces more realistic results by attaching an additional discriminator that works in the feature domain. Our additional discriminator encourages the generator to produce structural high-frequency features rather than noisy artifacts as it distinguishes synthetic and real images in terms of features. We also design a new generator that utilizes long-range skip connections so that information between distant layers can be transferred more effectively. Experiments show that our method achieves the state-of-the-art performance in terms of both PSNR and perceptual quality compared to recent GAN-based methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_27');
INSERT INTO `paper` VALUES (11245, 'Stacked Cross Attention for Image-Text Matching', 'Attention', 'Multi-modal', 'Visual-semantic embedding', '', '', 'In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior work either simply aggregates the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in a sentence as context and infer image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% relatively and image retrieval by 16.6% relatively (based on Recall@1 using the 5K test set). Code has been made available at: (https://github.com/kuanghuei/SCAN).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_13');
INSERT INTO `paper` VALUES (11246, 'stagNet: An Attentive Semantic RNN for Group Activity Recognition', 'Group activity recognition', 'Spatio-temporal attention', 'Semantic graph', 'Scene understanding', '', 'Group activity recognition plays a fundamental role in a variety of applications, e.g. sports video analysis and intelligent surveillance. How to model the spatio-temporal contextual information in a scene still remains a crucial yet challenging issue. We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph. A semantic graph is explicitly modeled to describe the spatial context of the whole scene, which is further integrated with the temporal factor via structural-RNN. Benefiting from the ‘factor sharing’ and ‘message passing’ mechanisms, our model is capable of extracting discriminative spatio-temporal features and capturing inter-group relationships. Moreover, we adopt a spatio-temporal attention model to attend to key persons/frames for improved performance. Two widely-used datasets are employed for performance evaluation, and the extensive results demonstrate the superiority of our method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_7');
INSERT INTO `paper` VALUES (11247, 'StarMap for Category-Agnostic Keypoint and Viewpoint Estimation', '3D vision', 'Category-agnostic', 'Keypoint estimation', 'Viewpoint estimation', 'Pose estimation', 'Semantic keypoints provide concise abstractions for a variety of visual understanding tasks. Existing methods define semantic keypoints separately for each category with a fixed number of semantic labels in fixed indices. As a result, this keypoint representation is in-feasible when objects have a varying number of parts, e.g. chairs with varying number of legs. We propose a category-agnostic keypoint representation, which combines a multi-peak heatmap (StarMap) for all the keypoints and their corresponding features as 3D locations in the canonical viewpoint (CanViewFeature) defined for each instance. Our intuition is that the 3D locations of the keypoints in canonical object views contain rich semantic and compositional information. Using our flexible representation, we demonstrate competitive performance in keypoint detection and localization compared to category-specific state-of-the-art methods. Moreover, we show that when augmented with an additional depth channel (DepthMap) to lift the 2D keypoints to 3D, our representation can achieve state-of-the-art results in viewpoint estimation. Finally, we show that our category-agnostic keypoint representation can be generalized to novel categories.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_20');
INSERT INTO `paper` VALUES (11248, 'Start, Follow, Read: End-to-End Full-Page Handwriting Recognition', 'Handwriting recognition', 'Document analysis', 'Historical document processing', 'Text detection', 'Text line segmentation', 'Despite decades of research, offline handwriting recognition (HWR) of degraded historical documents remains a challenging problem, which if solved could greatly improve the searchability of online cultural heritage archives. HWR models are often limited by the accuracy of the preceding steps of text detection and segmentation. Motivated by this, we present a deep learning model that jointly learns text detection, segmentation, and recognition using mostly images without detection or segmentation annotations. Our Start, Follow, Read (SFR) model is composed of a Region Proposal Network to find the start position of text lines, a novel line follower network that incrementally follows and preprocesses lines of (perhaps curved) text into dewarped images suitable for recognition by a CNN-LSTM network. SFR exceeds the performance of the winner of the ICDAR2017 handwriting recognition competition, even when not using the provided competition region annotations.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_23');
INSERT INTO `paper` VALUES (11249, 'Statistically-Motivated Second-Order Pooling', 'Second-order descriptors', 'Convolutional neural networks', 'Image classification', '', '', 'Second-order pooling, a.k.a. bilinear pooling, has proven effective for deep learning based visual recognition. However, the resulting second-order networks yield a final representation that is orders of magnitude larger than that of standard, first-order ones, making them memory-intensive and cumbersome to deploy. Here, we introduce a general, parametric compression strategy that can produce more compact representations than existing compression techniques, yet outperform both compressed and uncompressed second-order models. Our approach is motivated by a statistical analysis of the network’s activations, relying on operations that lead to a Gaussian-distributed final representation, as inherently used by first-order deep networks. As evidenced by our experiments, this lets us outperform the state-of-the-art first-order and second-order models on several benchmark recognition datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_37');
INSERT INTO `paper` VALUES (11250, 'Stereo Computation for a Single Mixture Image', 'Stereo computation', 'Image separation', 'Anaglyph', 'Monocular depth estimation', 'Double vision', 'This paper proposes an original problem of stereo computation from a single mixture image – a challenging problem that had not been researched before. The goal is to separate (i.e., unmix) a single mixture image into two constitute image layers, such that the two layers form a left-right stereo image pair, from which a valid disparity map can be recovered. This is a severely illposed problem, from one input image one effectively aims to recover three (i.e., left image, right image and a disparity map). In this work we give a novel deep-learning based solution, by jointly solving the two subtasks of image layer separation as well as stereo matching. Training our deep net is a simple task, as it does not need to have disparity maps. Extensive experiments demonstrate the efficacy of our method.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_27');
INSERT INTO `paper` VALUES (11251, 'Stereo Relative Pose from Line and Point Feature Triplets', 'Minimal solver', 'Stereo visual odometry', 'Generalized camera', 'Relative pose', 'Line features', 'Stereo relative pose problem lies at the core of stereo visual odometry systems that are used in many applications. In this work we present two minimal solvers for the stereo relative pose. We specifically consider the case when a minimal set consists of three point or line features and each of them has three known projections on two stereo cameras. We validate the importance of this formulation for practical purposes in our experiments with motion estimation. We then present a complete classification of minimal cases with three point or line correspondences each having three projections, and present two new solvers that can handle all such cases. We demonstrate a considerable effect from the integration of the new solvers into a visual SLAM system.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_40');
INSERT INTO `paper` VALUES (11252, 'Stereo Vision-Based Semantic 3D Object and Ego-Motion Tracking for Autonomous Driving', 'Semantic SLAM', '3D object localization', 'Visual Odometry', '', '', 'We propose a stereo vision-based approach for tracking the camera ego-motion and 3D semantic objects in dynamic autonomous driving scenarios. Instead of directly regressing the 3D bounding box using end-to-end approaches, we propose to use the easy-to-labeled 2D detection and discrete viewpoint classification together with a light-weight semantic inference method to obtain rough 3D object measurements. Based on the object-aware-aided camera pose tracking which is robust in dynamic environments, in combination with our novel dynamic object bundle adjustment (BA) approach to fuse temporal sparse feature correspondences and the semantic 3D measurement model, we obtain 3D object pose, velocity and anchored dynamic point cloud estimation with instance accuracy and temporal consistency. The performance of our proposed method is demonstrated in diverse scenarios. Both the ego-motion estimation and object localization are compared with the state-of-of-the-art solutions.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_40');
INSERT INTO `paper` VALUES (11253, 'StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth Prediction', 'Stereo matching', 'Depth estimation', 'Edge-aware refinement', 'Cost volume filtering', 'Deep learning', 'This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps. A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches. This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision. Spatial precision is achieved by employing a learned edge-aware upsampling function. Our model uses a Siamese network to extract features from the left and right image. A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks. Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output. We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_35');
INSERT INTO `paper` VALUES (11254, 'Straight to the Facts: Learning Knowledge Base Retrieval for Factual Visual Question Answering', 'Fact based visual question answering', 'Knowledge bases', '', '', '', 'Question answering is an important task for autonomous agents and virtual assistants alike and was shown to support the disabled in efficiently navigating an overwhelming environment. Many existing methods focus on observation-based questions, ignoring our ability to seamlessly combine observed content with general knowledge. To understand interactions with a knowledge base, a dataset has been introduced recently and keyword matching techniques were shown to yield compelling results despite being vulnerable to misconceptions due to synonyms and homographs. To address this issue, we develop a learning-based approach which goes straight to the facts via a learned embedding space. We demonstrate state-of-the-art results on the challenging recently introduced fact-based visual question answering dataset, outperforming competing methods by more than \\(5\\%\\).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_28');
INSERT INTO `paper` VALUES (11255, 'Stroke Controllable Fast Style Transfer with Adaptive Receptive Fields', 'Neural Style Transfer', 'Adaptive receptive fields', '', '', '', 'The Fast Style Transfer methods have been recently proposed to transfer a photograph to an artistic style in real-time. This task involves controlling the stroke size in the stylized results, which remains an open challenge. In this paper, we present a stroke controllable style transfer network that can achieve continuous and spatial stroke size control. By analyzing the factors that influence the stroke size, we propose to explicitly account for the receptive field and the style image scales. We propose a StrokePyramid module to endow the network with adaptive receptive fields, and two training strategies to achieve faster convergence and augment new stroke sizes upon a trained model respectively. By combining the proposed runtime control strategies, our network can achieve continuous changes in stroke sizes and produce distinct stroke sizes in different spatial regions within the same output image.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_15');
INSERT INTO `paper` VALUES (11256, 'Structural Consistency and Controllability for Diverse Colorization', 'Colorization', 'Gaussian-Conditional Random Field', 'VAE', '', '', 'Colorizing a given gray-level image is an important task in the media and advertising industry. Due to the ambiguity inherent to colorization (many shades are often plausible), recent approaches started to explicitly model diversity. However, one of the most obvious artifacts, structural inconsistency, is rarely considered by existing methods which predict chrominance independently for every pixel. To address this issue, we develop a conditional random field based variational auto-encoder formulation which is able to achieve diversity while taking into account structural consistency. Moreover, we introduce a controllability mechanism that can incorporate external constraints from diverse sources including a user interface. Compared to existing baselines, we demonstrate that our method obtains more diverse and globally consistent colorizations on the LFW, LSUN-Church and ILSVRC-2015 datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_37');
INSERT INTO `paper` VALUES (11257, 'Structure-from-Motion-Aware PatchMatch for Adaptive Optical Flow Estimation', '', '', '', '', '', 'Many recent energy-based methods for optical flow estimation rely on a good initialization that is typically provided by some kind of feature matching. So far, however, these initial matching approaches are rather general: They do not incorporate any additional information that could help to improve the accuracy or the robustness of the estimation. In particular, they do not exploit potential cues on the camera poses and the thereby induced rigid motion of the scene. In the present paper, we tackle this problem. To this end, we propose a novel structure-from-motion-aware PatchMatch approach that, in contrast to existing matching techniques, combines two hierarchical feature matching methods: a recent two-frame PatchMatch approach for optical flow estimation (general motion) and a specifically tailored three-frame PatchMatch approach for rigid scene reconstruction (SfM). While the motion PatchMatch serves as baseline with good accuracy, the SfM counterpart takes over at occlusions and other regions with insufficient information. Experiments with our novel SfM-aware PatchMatch approach demonstrate its usefulness. They not only show excellent results for all major benchmarks (KITTI 2012/2015, MPI Sintel), but also improvements up to 50% compared to a PatchMatch approach without structure information.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_35');
INSERT INTO `paper` VALUES (11258, 'Structured Siamese Network for Real-Time Visual Tracking', 'Tracking', 'Deep learning', 'Siamese network', '', '', 'Local structures of target objects are essential for robust tracking. However, existing methods based on deep neural networks mostly describe the target appearance from the global view, leading to high sensitivity to non-rigid appearance change and partial occlusion. In this paper, we circumvent this issue by proposing a local structure learning method, which simultaneously considers the local patterns of the target and their structural relationships for more accurate target tracking. To this end, a local pattern detection module is designed to automatically identify discriminative regions of the target objects. The detection results are further refined by a message passing module, which enforces the structural context among local patterns to construct local structures. We show that the message passing module can be formulated as the inference process of a conditional random field (CRF) and implemented by differentiable operations, allowing the entire model to be trained in an end-to-end manner. By considering various combinations of the local structures, our tracker is able to form various types of structure patterns. Target tracking is finally achieved by a matching procedure of the structure patterns between target template and candidates. Extensive evaluations on three benchmark data sets demonstrate that the proposed tracking algorithm performs favorably against state-of-the-art methods while running at a highly efficient speed of 45 fps.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_22');
INSERT INTO `paper` VALUES (11259, 'Sub-GAN: An Unsupervised Generative Model via Subspaces', '', '', '', '', '', 'The recent years have witnessed significant growth in constructing robust generative models to capture informative distributions of natural data. However, it is difficult to fully exploit the distribution of complex data, like images and videos, due to the high dimensionality of ambient space. Sequentially, how to effectively guide the training of generative models is a crucial issue. In this paper, we present a subspace-based generative adversarial network (Sub-GAN) which simultaneously disentangles multiple latent subspaces and generates diverse samples correspondingly. Since the high-dimensional natural data usually lies on a union of low-dimensional subspaces which contain semantically extensive structure, Sub-GAN incorporates a novel clusterer that can interact with the generator and discriminator via subspace information. Unlike the traditional generative models, the proposed Sub-GAN can control the diversity of the generated samples via the multiplicity of the learned subspaces. Moreover, the Sub-GAN follows an unsupervised fashion to explore not only the visual classes but the latent continuous attributes. We demonstrate that our model can discover meaningful visual attributes which is hard to be annotated via strong supervision, e.g., the writing style of digits, thus avoid the mode collapse problem. Extensive experimental results show the competitive performance of the proposed method for both generating diverse images with satisfied quality and discovering discriminative latent subspaces.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_43');
INSERT INTO `paper` VALUES (11260, 'Subitizing with Variational Autoencoders', 'Object counting', 'Numerosity', 'Variational autoencoders', '', '', 'Numerosity, the number of objects in a set, is a basic property of a given visual scene. Many animals develop the perceptual ability to subitize: the near-instantaneous identification of the numerosity in small sets of visual items. In computer vision, it has been shown that numerosity emerges as a statistical property in neural networks during unsupervised learning from simple synthetic images. In this work, we focus on more complex natural images using unsupervised hierarchical neural networks. Specifically, we show that variational autoencoders are able to spontaneously perform subitizing after training without supervision on a large amount of images from the Salient Object Subitizing dataset. While our method is unable to outperform supervised convolutional networks for subitizing, we observe that the networks learn to encode numerosity as a basic visual property. Moreover, we find that the learned representations are likely invariant to object area; an observation in alignment with studies on biological neural networks in cognitive neuroscience.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_47');
INSERT INTO `paper` VALUES (11261, 'Summarizing First-Person Videos from Third Persons’ Points of Views', 'Video summarization', 'First-person vision', 'Transfer learning', 'Metric learning', '', 'Video highlight or summarization is among interesting topics in computer vision, which benefits a variety of applications like viewing, searching, or storage. However, most existing studies rely on training data of third-person videos, which cannot easily generalize to highlight the first-person ones. With the goal of deriving an effective model to summarize first-person videos, we propose a novel deep neural network architecture for describing and discriminating vital spatiotemporal information across videos with different points of view. Our proposed model is realized in a semi-supervised setting, in which fully annotated third-person videos, unlabeled first-person videos, and a small number of annotated first-person ones are presented during training. In our experiments, qualitative and quantitative evaluations on both benchmarks and our collected first-person video datasets are presented.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_5');
INSERT INTO `paper` VALUES (11262, 'Super-Identity Convolutional Neural Network for Face Hallucination', 'Face hallucination', 'Super identity', 'Domain-integrated training', 'Convolutional neural networks', '', 'Face hallucination is a generative task to super-resolve the facial image with low resolution while human perception of face heavily relies on identity information. However, previous face hallucination approaches largely ignore facial identity recovery. This paper proposes Super-Identity Convolutional Neural Network (SICNN) to recover identity information for generating faces closed to the real identity. Specifically, we define a super-identity loss to measure the identity difference between a hallucinated face and its corresponding high-resolution face within the hypersphere identity metric space. However, directly using this loss will lead to a Dynamic Domain Divergence problem, which is caused by the large margin between the high-resolution domain and the hallucination domain. To overcome this challenge, we present a domain-integrated training approach by constructing a robust identity metric for faces from these two domains. Extensive experimental evaluations demonstrate that the proposed SICNN achieves superior visual quality over the state-of-the-art methods on a challenging task to super-resolve 12 \\(\\times \\) 14 faces with an 8\\(\\times \\) upscaling factor. In addition, SICNN significantly improves the recognizability of ultra-low-resolution faces.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_12');
INSERT INTO `paper` VALUES (11263, 'Super-Resolution and Sparse View CT Reconstruction', 'Super resolution', 'Proximal optimization', 'Tomography', '', '', 'We present a flexible framework for robust computed tomography (CT) reconstruction with a specific emphasis on recovering thin 1D and 2D manifolds embedded in 3D volumes. To reconstruct such structures at resolutions below the Nyquist limit of the CT image sensor, we devise a new 3D structure tensor prior, which can be incorporated as a regularizer into more traditional proximal optimization methods for CT reconstruction. As a second, smaller contribution, we also show that when using such a proximal reconstruction framework, it is beneficial to employ the Simultaneous Algebraic Reconstruction Technique (SART) instead of the commonly used Conjugate Gradient (CG) method in the solution of the data term proximal operator. We show empirically that CG often does not converge to the global optimum for tomography problem even though the underlying problem is convex. We demonstrate that using SART provides better reconstruction results in sparse-view settings using fewer projection images. We provide extensive experimental results for both contributions on both simulated and real data. Moreover, our code will also be made publicly available.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_9');
INSERT INTO `paper` VALUES (11264, 'Superpixel Sampling Networks', 'Superpixels', 'Deep learning', 'Clustering', '', '', 'Superpixels provide an efficient low/mid-level representation of image data, which greatly reduces the number of image primitives for subsequent vision tasks. Existing superpixel algorithms are not differentiable, making them difficult to integrate into otherwise end-to-end trainable deep neural networks. We develop a new differentiable model for superpixel sampling that leverages deep networks for learning superpixel segmentation. The resulting Superpixel Sampling Network (SSN) is end-to-end trainable, which allows learning task-specific superpixels with flexible loss functions and has fast runtime. Extensive experimental analysis indicates that SSNs not only outperform existing superpixel algorithms on traditional segmentation benchmarks, but can also learn superpixels for other tasks. In addition, SSNs can be easily integrated into downstream deep networks resulting in performance improvements.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_22');
INSERT INTO `paper` VALUES (11265, 'Supervising the New with the Old: Learning SFM from SFM', 'Supervisory Signal', 'Unlabeled Video Sequences', 'Brightness Constancy Constraint', 'Monocular Depth Estimation', 'Deep Neural Networks', 'Recent work has demonstrated that it is possible to learn deep neural networks for monocular depth and ego-motion estimation from unlabelled video sequences, an interesting theoretical development with numerous advantages in applications. In this paper, we propose a number of improvements to these approaches. First, since such self-supervised approaches are based on the brightness constancy assumption, which is valid only for a subset of pixels, we propose a probabilistic learning formulation where the network predicts distributions over variables rather than specific values. As these distributions are conditioned on the observed image, the network can learn which scene and object types are likely to violate the model assumptions, resulting in more robust learning. We also propose to build on dozens of years of experience in developing handcrafted structure-from-motion (SFM) algorithms. We do so by using an off-the-shelf SFM system to generate a supervisory signal for the deep neural network. While this signal is also noisy, we show that our probabilistic formulation can learn and account for the defects of SFM, helping to integrate different sources of information and boosting the overall performance of the network.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_43');
INSERT INTO `paper` VALUES (11266, 'SwapNet: Image Based Garment Transfer', '', '', '', '', '', 'We present Swapnet, a framework to transfer garments across images of people with arbitrary body pose, shape, and clothing. Garment transfer is a challenging task that requires (i) disentangling the features of the clothing from the body pose and shape and (ii) realistic synthesis of the garment texture on the new body. We present a neural network architecture that tackles these sub-problems with two task-specific sub-networks. Since acquiring pairs of images showing the same clothing on different bodies is difficult, we propose a novel weakly-supervised approach that generates training pairs from a single image via data augmentation. We present the first fully automatic method for garment transfer in unconstrained images without solving the difficult 3D reconstruction problem. We demonstrate a variety of transfer results and highlight our advantages over traditional image-to-image and analogy pipelines.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_41');
INSERT INTO `paper` VALUES (11267, 'Switchable Temporal Propagation Network', 'Style Energy', 'Advanced Video Processing', 'Segmentation Mask', 'Gray-scale Video', 'Network Steering', 'Videos contain highly redundant information between frames. Such redundancy has been studied extensively in video compression and encoding, but is less explored for more advanced video processing. In this paper, we propose a learnable unified framework for propagating a variety of visual properties of video images, including but not limited to color, high dynamic range (HDR), and segmentation mask, where the properties are available for only a few key-frames. Our approach is based on a temporal propagation network (TPN), which models the transition-related affinity between a pair of frames in a purely data-driven manner. We theoretically prove two essential properties of TPN: (a) by regularizing the global transformation matrix as orthogonal, the “style energy” of the property can be well preserved during propagation; and (b) such regularization can be achieved by the proposed switchable TPN with bi-directional training on pairs of frames. We apply the switchable TPN to three tasks: colorizing a gray-scale video based on a few colored key-frames, generating an HDR video from a low dynamic range (LDR) video and a few HDR frames, and propagating a segmentation mask from the first frame in videos. Experimental results show that our approach is significantly more accurate and efficient than the state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_6');
INSERT INTO `paper` VALUES (11268, 'Synthetically Supervised Feature Learning for Scene Text Recognition', 'Scene text recognition', 'Deep learning', 'Neural networks', 'Feature learning', 'Synthetic data', 'We address the problem of image feature learning for scene text recognition. The image features in the state-of-the-art methods are learned from large-scale synthetic image datasets. However, most methods only rely on outputs of the synthetic data generation process, namely realistically looking images, and completely ignore the rest of the process. We propose to leverage the parameters that lead to the output images to improve image feature learning. Specifically, for every image out of the data generation process, we obtain the associated parameters and render another “clean” image that is free of select distortion factors that are applied to the output image. Because of the absence of distortion factors, the clean image tends to be easier to recognize than the original image which can serve as supervision. We design a multi-task network with an encoder-discriminator-generator architecture to guide the feature of the original image toward that of the clean image. The experiments show that our method significantly outperforms the state-of-the-art methods on standard scene text recognition benchmarks in the lexicon-free category. Furthermore, we show that without explicit handling, our method works on challenging cases where input images contain severe geometric distortion, such as text on a curved path.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_27');
INSERT INTO `paper` VALUES (11269, 'T\\(^2\\)Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth Estimation Tasks', 'Single-image depth estimation', 'Unpaired images', 'Synthetic data', 'Domain adaptation', '', 'Current methods for single-image depth estimation use training datasets with real image-depth pairs or stereo pairs, which are not easy to acquire. We propose a framework, trained on synthetic image-depth pairs and unpaired real images, that comprises an image translation network for enhancing realism of input images, followed by a depth prediction network. A key idea is having the first network act as a wide-spectrum input translator, taking in either synthetic or real images, and ideally producing minimally modified realistic images. This is done via a reconstruction loss when the training input is real, and GAN loss when synthetic, removing the need for heuristic self-regularization. The second network is trained on a task loss for synthetic image-depth pairs, with extra GAN loss to unify real and synthetic feature distributions. Importantly, the framework can be trained end-to-end, leading to good results, even surpassing early deep-learning methods that use real paired data.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_47');
INSERT INTO `paper` VALUES (11270, 'Tackling 3D ToF Artifacts Through Learning and the FLAT Dataset', 'Time-of-Flight', 'MPI artifacts', 'Motion artifacts', '', '', 'Scene motion, multiple reflections, and sensor noise introduce artifacts in the depth reconstruction performed by time-of-flight cameras. We propose a two-stage, deep-learning approach to address all of these sources of artifacts simultaneously. We also introduce FLAT, a synthetic dataset of 2000 ToF measurements that capture all of these nonidealities, and allows to simulate different camera hardware. Using the Kinect 2 camera as a baseline, we show improved reconstruction errors over state-of-the-art methods, on both simulated and real data.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_23');
INSERT INTO `paper` VALUES (11271, 'Target Aware Network Adaptation for Efficient Representation Learning', 'Target-aware', 'Network Adaptation', 'Model compaction', 'Transfer learning', '', 'This paper presents an automatic network adaptation method that finds a ConvNet structure well-suited to a given target task, e.g. image classification, for efficiency as well as accuracy in transfer learning. We call the concept target-aware transfer learning. Given only small-scale labeled data, and starting from an ImageNet pre-trained network, we exploit a scheme of removing its potential redundancy for the target task through iterative operations of filter-wise pruning and network optimization. The basic motivation is that compact networks are on one hand more efficient and should also be more tolerant, being less complex, against the risk of overfitting which would hinder the generalization of learned representations in the context of transfer learning. Further, unlike existing methods involving network simplification, we also let the scheme identify redundant portions across the entire network, which automatically results in a network structure adapted to the task at hand. We achieve this with a few novel ideas: (i) cumulative sum of activation statistics for each layer, and (ii) a priority evaluation of pruning across multiple layers. Experimental results by the method on five datasets (Flower102, CUB200-2011, Dog120, MIT67, and Stanford40) show favorable accuracies over the related state-of-the-art techniques while enhancing the computational and storage efficiency of the transferred model.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_38');
INSERT INTO `paper` VALUES (11272, 'Targeted Kernel Networks: Faster Convolutions with Attentive Regularization', 'Soft attention', 'Region of interest', 'Network acceleration', '', '', 'We propose Attentive Regularization (AR), a method to constrain the activation maps of kernels in Convolutional Neural Networks (CNNs) to specific regions of interest (ROIs). Each kernel learns a location of specialization along with its weights through standard backpropagation. A differentiable attention mechanism requiring no additional supervision is used to optimize the ROIs. Traditional CNNs of different types and structures can be modified with this idea into equivalent Targeted Kernel Networks (TKNs), while keeping the network size nearly identical. By restricting kernel ROIs, we reduce the number of sliding convolutional operations performed throughout the network in its forward pass, speeding up both training and inference. We evaluate our proposed architecture on both synthetic and natural tasks across multiple domains. TKNs obtain significant improvements over baselines, requiring less computation (around an order of magnitude) while achieving superior performance.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_34');
INSERT INTO `paper` VALUES (11273, 'Task-Aware Image Downscaling', 'Image downscaling', 'Image super-resolution', 'Deep learning', '', '', 'Image downscaling is one of the most classical problems in computer vision that aims to preserve the visual appearance of the original image when it is resized to a smaller scale. Upscaling a small image back to its original size is a difficult and ill-posed problem due to information loss that arises in the downscaling process. In this paper, we present a novel technique called task-aware image downscaling to support an upscaling task. We propose an auto-encoder-based framework that enables joint learning of the downscaling network and the upscaling network to maximize the restoration performance. Our framework is efficient, and it can be generalized to handle an arbitrary image resizing operation. Experimental results show that our task-aware downscaled images greatly improve the performance of the existing state-of-the-art super-resolution methods. In addition, realistic images can be recovered by recursively applying our scaling model up to an extreme scaling factor of x128. We also validate our model’s generalization capability by applying it to the task of image colorization.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_25');
INSERT INTO `paper` VALUES (11274, 'Task-Driven Webpage Saliency', 'Webpage analysis', 'Saliency detection', 'Task-specific saliency', '', '', 'In this paper, we present an end-to-end learning framework for predicting task-driven visual saliency on webpages. Given a webpage, we propose a convolutional neural network to predict where people look at it under different task conditions. Inspired by the observation that given a specific task, human attention is strongly correlated with certain semantic components on a webpage (e.g., images, buttons and input boxes), our network explicitly disentangles saliency prediction into two independent sub-tasks: task-specific attention shift prediction and task-free saliency prediction. The task-specific branch estimates task-driven attention shift over a webpage from its semantic components, while the task-free branch infers visual saliency induced by visual features of the webpage. The outputs of the two branches are combined to produce the final prediction. Such a task decomposition framework allows us to efficiently learn our model from a small-scale task-driven saliency dataset with sparse labels (captured under a single task condition). Experimental results show that our method outperforms the baselines and prior works, achieving state-of-the-art performance on a newly collected benchmark dataset for task-driven webpage saliency detection.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_18');
INSERT INTO `paper` VALUES (11275, 'Task-Oriented Hand Motion Retargeting for Dexterous Manipulation Imitation', 'Hand pose estimation', 'Motion retargeting', 'PSO', 'Anthropomorphic hand model', 'Imitation learning', 'Human hand actions are quite complex, especially when they involve object manipulation, mainly due to the high dimensionality of the hand and the vast action space that entails. Imitating those actions with dexterous hand models involves different important and challenging steps: acquiring human hand information, retargeting it to a hand model, and learning a policy from acquired data. In this work, we capture the hand information by using a state-of-the-art hand pose estimator. We tackle the retargeting problem from the hand pose to a 29 DoF hand model by combining inverse kinematics and PSO with a task objective optimisation. This objective encourages the virtual hand to accomplish the manipulation task, relieving the effect of the estimator’s noise and the domain gap. Our approach leads to a better success rate in the grasping task compared to our inverse kinematics baseline, allowing us to record successful human demonstrations. Furthermore, we used these demonstrations to learn a policy network using generative adversarial imitation learning (GAIL) that is able to autonomously grasp an object in the virtual space.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_19');
INSERT INTO `paper` VALUES (11276, 'TBN: Convolutional Neural Network with Ternary Inputs and Binary Weights', 'CNN', 'TBN', 'Acceleration', 'Compression', 'Binary operation', 'Despite the remarkable success of Convolutional Neural Networks (CNNs) on generalized visual tasks, high computational and memory costs restrict their comprehensive applications on consumer electronics (e.g., portable or smart wearable devices). Recent advancements in binarized networks have demonstrated progress on reducing computational and memory costs, however, they suffer from significant performance degradation comparing to their full-precision counterparts. Thus, a highly-economical yet effective CNN that is authentically applicable to consumer electronics is at urgent need. In this work, we propose a Ternary-Binary Network (TBN), which provides an efficient approximation to standard CNNs. Based on an accelerated ternary-binary matrix multiplication, TBN replaces the arithmetical operations in standard CNNs with efficient XOR, AND and bitcount operations, and thus provides an optimal tradeoff between memory, efficiency and performance. TBN demonstrates its consistent effectiveness when applied to various CNN architectures (e.g., AlexNet and ResNet) on multiple datasets of different scales, and provides \\(\\sim \\)32\\(\\times \\) memory savings and \\(40\\times \\) faster convolutional operations. Meanwhile, TBN can outperform XNOR-Network by up to 5.5% (top-1 accuracy) on the ImageNet classification task, and up to 4.4% (mAP score) on the PASCAL VOC object detection task.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_20');
INSERT INTO `paper` VALUES (11277, 'Teaching Machines to Understand Baseball Games: Large-Scale Baseball Video Database for Multiple Video Understanding Tasks', 'Video understanding', 'Large-scale video dataset', 'Action recognition', 'Temporal localization', '', 'A major obstacle in teaching machines to understand videos is the lack of training data, as creating temporal annotations for long videos requires a huge amount of human effort. To this end, we introduce a new large-scale baseball video dataset called the BBDB, which is produced semi-automatically by using play-by-play texts available online. The BBDB contains 4200\\(+\\)hr of baseball game videos with 400k\\(+\\) temporally annotated activity segments. The new dataset has several major challenging factors compared to other datasets: (1) the dataset contains a large number of visually similar segments with different labels. (2) It can be used for many video understanding tasks including video recognition, localization, text-video alignment, video highlight generation, and data imbalance problem. To observe the potential of the BBDB, we conducted extensive experiments by running many different types of video understanding algorithms on our new dataset. The database is available at https://sites.google.com/site/eccv2018bbdb/.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_25');
INSERT INTO `paper` VALUES (11278, 'Teaching UAVs to Race: End-to-End Regression of Agile Controls in Simulation', '', '', '', '', '', 'Automating the navigation of unmanned aerial vehicles (UAVs) in diverse scenarios has gained much attention in recent years. However, teaching UAVs to fly in challenging environments remains an unsolved problem, mainly due to the lack of training data. In this paper, we train a deep neural network to predict UAV controls from raw image data for the task of autonomous UAV racing in a photo-realistic simulation. Training is done through imitation learning with data augmentation to allow for the correction of navigation mistakes. Extensive experiments demonstrate that our trained network (when sufficient data augmentation is used) outperforms state-of-the-art methods and flies more consistently than many human pilots. Additionally, we show that our optimized network architecture can run in real-time on embedded hardware, allowing for efficient on-board processing critical for real-world deployment. From a broader perspective, our results underline the importance of extensive data augmentation techniques to improve robustness in end-to-end learning setups.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_2');
INSERT INTO `paper` VALUES (11279, 'Temporal Attention Mechanism with Conditional Inference for Large-Scale Multi-label Video Classification', 'Multimodal sequential learning', 'Attention', 'Multi-label classification', 'Video understanding', '', 'Here we show neural network based methods, which combine multimodal sequential inputs effectively and classify the inputs into multiple categories. Two key ideas are (1) to select informative frames among a sequence using attention mechanism and (2) to utilize correlation information between labels to solve multi-label classification problems. The attention mechanism is used in both modality (spatio) and sequential (temporal) dimensions to ignore noisy and meaningless frames. Furthermore, to tackle fundamental problems induced by independently predicting each label in conventional multi-label classification methods, the proposed method considers the dependencies among the labels by decomposing joint probability of labels into conditional terms. From the experimental results (5th in the Kaggle competition), we discuss how the suggested methods operate in the YouTube-8M Classification Task, what insights they have, and why they succeed or fail.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_28');
INSERT INTO `paper` VALUES (11280, 'Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos', 'Video retrieval', 'Action recognition', 'Modular networks', '', '', 'A major challenge in computer vision is scaling activity understanding to the long tail of complex activities without requiring collecting large quantities of data for new actions. The task of video retrieval using natural language descriptions seeks to address this through rich, unconstrained supervision about complex activities. However, while this formulation offers hope of leveraging underlying compositional structure in activity descriptions, existing approaches typically do not explicitly model compositional reasoning. In this work, we introduce an approach for explicitly and dynamically reasoning about compositional natural language descriptions of activity in videos. We take a modular neural network approach that, given a natural language query, extracts the semantic structure to assemble a compositional neural network layout and corresponding network modules. We show that this approach is able to achieve state-of-the-art results on the DiDeMo video retrieval dataset.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_34');
INSERT INTO `paper` VALUES (11281, 'Temporal Relational Reasoning in Videos', '', '', '', '', '', 'Temporal relational reasoning, the ability to link meaningful transformations of objects or entities over time, is a fundamental property of intelligent species. In this paper, we introduce an effective and interpretable network module, the Temporal Relation Network (TRN), designed to learn and reason about temporal dependencies between video frames at multiple time scales. We evaluate TRN-equipped networks on activity recognition tasks using three recent video datasets - Something-Something, Jester, and Charades - which fundamentally depend on temporal relational reasoning. Our results demonstrate that the proposed TRN gives convolutional neural networks a remarkable capacity to discover temporal relations in videos. Through only sparsely sampled video frames, TRN-equipped networks can accurately predict human-object interactions in the Something-Something dataset and identify various human gestures on the Jester dataset with very competitive performance. TRN-equipped networks also outperform two-stream networks and 3D convolution networks in recognizing daily activities in the Charades dataset. Further analyses show that the models learn intuitive and interpretable visual common sense knowledge in videos (Code and models are available at http://relation.csail.mit.edu/.).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_49');
INSERT INTO `paper` VALUES (11282, 'Temporally Consistent Depth Estimation in Videos with Recurrent Architectures', 'Convolutional LSTM', 'Recurrent networks', 'Depth estimation', 'Video processing', '', 'Convolutional networks trained on large RGB-D datasets have enabled depth estimation from a single image. Many works on automotive applications rely on such approaches. However, all existing methods work on a frame-by-frame manner when applied to videos, which leads to inconsistent depth estimates over time. In this paper, we introduce for the first time an approach that yields temporally consistent depth estimates over multiple frames of a video. This is done by a dedicated architecture based on convolutional LSTM units and layer normalization. Our approach achieves superior performance on several error metrics when compared to independent frame processing. This also shows in an improved quality of the reconstructed multi-view point clouds.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_52');
INSERT INTO `paper` VALUES (11283, 'TextSnake: A Flexible Representation for Detecting Text of Arbitrary Shapes', 'Scene text detection', 'Deep neural network', 'Curved text', '', '', 'Driven by deep neural networks and large scale datasets, scene text detection methods have progressed substantially over the past years, continuously refreshing the performance records on various standard benchmarks. However, limited by the representations (axis-aligned rectangles, rotated rectangles or quadrangles) adopted to describe text, existing methods may fall short when dealing with much more free-form text instances, such as curved text, which are actually very common in real-world scenarios. To tackle this problem, we propose a more flexible representation for scene text, termed as TextSnake, which is able to effectively represent text instances in horizontal, oriented and curved forms. In TextSnake, a text instance is described as a sequence of ordered, overlapping disks centered at symmetric axes, each of which is associated with potentially variable radius and orientation. Such geometry attributes are estimated via a Fully Convolutional Network (FCN) model. In experiments, the text detector based on TextSnake achieves state-of-the-art or comparable performance on Total-Text and SCUT-CTW1500, the two newly published benchmarks with special emphasis on curved text in natural images, as well as the widely-used datasets ICDAR 2015 and MSRA-TD500. Specifically, TextSnake outperforms the baseline on Total-Text by more than 40% in F-measure.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_2');
INSERT INTO `paper` VALUES (11284, 'Textual Explanations for Self-Driving Vehicles', 'Explainable deep driving', 'BDD-X dataset', '', '', '', 'Deep neural perception and control networks have become key components of self-driving vehicles. User acceptance is likely to benefit from easy-to-interpret textual explanations which allow end-users to understand what triggered a particular behavior. Explanations may be triggered by the neural controller, namely introspective explanations, or informed by the neural controller’s output, namely rationalizations. We propose a new approach to introspective explanations which consists of two parts. First, we use a visual (spatial) attention model to train a convolutional network end-to-end from images to the vehicle control commands, i.e., acceleration and change of course. The controller’s attention identifies image regions that potentially influence the network’s output. Second, we use an attention-based video-to-text model to produce textual explanations of model actions. The attention maps of controller and explanation model are aligned so that explanations are grounded in the parts of the scene that mattered to the controller. We explore two approaches to attention alignment, strong- and weak-alignment. Finally, we explore a version of our model that generates rationalizations, and compare with introspective explanations on the same video segments. We evaluate these models on a novel driving dataset with ground-truth human explanations, the Berkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at https://github.com/JinkyuKimUCB/explainable-deep-driving.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_35');
INSERT INTO `paper` VALUES (11285, 'The 2018 PIRM Challenge on Perceptual Image Super-Resolution', '', '', '', '', '', 'This paper reports on the 2018 PIRM challenge on perceptual super-resolution (SR), held in conjunction with the Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018. In contrast to previous SR challenges, our evaluation methodology jointly quantifies accuracy and perceptual quality, therefore enabling perceptual-driven methods to compete alongside algorithms that target PSNR maximization. Twenty-one participating teams introduced algorithms which well-improved upon the existing state-of-the-art methods in perceptual SR, as confirmed by a human opinion study. We also analyze popular image quality measures and draw conclusions regarding which of them correlates best with human opinion scores. We conclude with an analysis of the current trends in perceptual SR, as reflected from the leading submissions.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_21');
INSERT INTO `paper` VALUES (11286, 'The 2nd YouTube-8M Large-Scale Video Understanding Challenge', 'YouTube', 'Video Classification', 'Video Understanding', '', '', 'We hosted the 2nd YouTube-8M Large-Scale Video Understanding Kaggle Challenge and Workshop at ECCV’18, with the task of classifying videos from frame-level and video-level audio-visual features. In this year’s challenge, we restricted the final model size to 1 GB or less, encouraging participants to explore representation learning or better architecture, instead of heavy ensembles of multiple models. In this paper, we briefly introduce the YouTube-8M dataset and challenge task, followed by participants statistics and result analysis. We summarize proposed ideas by participants, including architectures, temporal aggregation methods, ensembling and distillation, data augmentation, and more.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_18');
INSERT INTO `paper` VALUES (11287, 'The Contextual Loss for Image Transformation with Non-aligned Data', 'Context Loss', 'Loss Function', 'Target Image', 'Style Transfer', 'Puppet Control', 'Feed-forward CNNs trained for image transformation problems rely on loss functions that measure the similarity between the generated image and a target image. Most of the common loss functions assume that these images are spatially aligned and compare pixels at corresponding locations. However, for many tasks, aligned training pairs of images will not be available. We present an alternative loss function that does not require alignment, thus providing an effective and simple solution for a new space of problems. Our loss is based on both context and semantics – it compares regions with similar semantic meaning, while considering the context of the entire image. Hence, for example, when transferring the style of one face to another, it will translate eyes-to-eyes and mouth-to-mouth. Our code can be found at https://www.github.com/roimehrez/contextualLoss.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_47');
INSERT INTO `paper` VALUES (11288, 'The Devil of Face Recognition Is in the Noise', '', '', '', '', '', 'The growing scale of face recognition datasets empowers us to train strong convolutional networks for face recognition. While a variety of architectures and loss functions have been devised, we still have a limited understanding of the source and consequence of label noise inherent in existing datasets. We make the following contributions: (1) We contribute cleaned subsets of popular face databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new large-scale noise-controlled IMDb-Face dataset. (2) With the original datasets and cleaned subsets, we profile and analyze label noise properties of MegaFace and MS-Celeb-1M. We show that a few orders more samples are needed to achieve the same accuracy yielded by a clean subset. (3) We study the association between different types of noise, i.e., label flips and outliers, with the accuracy of face recognition models. (4) We investigate ways to improve data cleanliness, including a comprehensive user study on the influence of data labeling strategies to annotation accuracy. The IMDb-Face dataset has been released on https://github.com/fwang91/IMDb-Face.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_47');
INSERT INTO `paper` VALUES (11289, 'The Mutex Watershed: Efficient, Parameter-Free Image Partitioning', '', '', '', '', '', 'Image partitioning, or segmentation without semantics, is the task of decomposing an image into distinct segments; or equivalently, the task of detecting closed contours in an image. Most prior work either requires seeds, one per segment; or a threshold; or formulates the task as an NP-hard signed graph partitioning problem. Here, we propose an algorithm with empirically linearithmic complexity. Unlike seeded watershed, the algorithm can accommodate not only attractive but also repulsive cues, allowing it to find a previously unspecified number of segments without the need for explicit seeds or a tunable threshold. The algorithm itself, which we dub “Mutex Watershed”, is closely related to a minimal spanning tree computation. It is deterministic and easy to implement. When presented with short-range attractive and long-range repulsive cues from a deep neural network, the Mutex Watershed gives results that currently define the state-of-the-art in the competitive ISBI 2012 EM segmentation benchmark. These results are also better than those obtained from other recently proposed clustering strategies operating on the very same network outputs.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_34');
INSERT INTO `paper` VALUES (11290, 'The Second Workshop on 3D Reconstruction Meets Semantics: Challenge Results Discussion', '3D reconstruction', 'Semantic segmentation', 'Challenge', 'Dataset', '', 'This paper discusses a reconstruction challenge held as a part of the second 3D Reconstruction meets Semantics workshop (3DRMS). The challenge goals and datasets are introduced, including both synthetic and real data from outdoor scenes, here represented by gardens with a variety of bushes, trees, other plants and objects. Both qualitative and quantitative evaluation of the challenge participants’ submissions is given in categories of geometric and semantic accuracy. Finally, comparison of submitted results with baseline methods is given, showing a modest performance increase in some of the categories.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_48');
INSERT INTO `paper` VALUES (11291, 'The Sixth Visual Object Tracking VOT2018 Challenge Results', '', '', '', '', '', 'The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a “real-time” experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_1');
INSERT INTO `paper` VALUES (11292, 'The Sound of Pixels', 'Cross-modal learning', 'Sound separation and localization', '', '', '', 'We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_35');
INSERT INTO `paper` VALUES (11293, 'The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking', 'UAV', 'Object detection', 'Single object tracking', 'Multiple object tracking', '', 'With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used to fuel numerous important applications in computer vision, delivering more efficiency and convenience than surveillance cameras with fixed camera angle, scale and view. However, very limited UAV datasets are proposed, and they focus only on a specific task such as visual tracking or object detection in relatively constrained scenarios. Consequently, it is of great importance to develop an unconstrained UAV benchmark to boost related researches. In this paper, we construct a new UAV benchmark focusing on complex scenarios with new level challenges. Selected from 10 hours raw videos, about 80, 000 representative frames are fully annotated with bounding boxes as well as up to 14 kinds of attributes (e.g., weather condition, flying altitude, camera view, vehicle category, and occlusion) for three fundamental computer vision tasks: object detection, single object tracking, and multiple object tracking. Then, a detailed quantitative study is performed using most recent state-of-the-art algorithms for each task. Experimental results show that the current state-of-the-art methods perform relative worse on our dataset, due to the new challenges appeared in UAV based real scenes, e.g., high density, small object, and camera motion. To our knowledge, our work is the first time to explore such issues in unconstrained scenes comprehensively. The dataset and all the experimental results are available in https://sites.google.com/site/daviddo0323/.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_23');
INSERT INTO `paper` VALUES (11294, 'The Unreasonable Effectiveness of Texture Transfer for Single Image Super-Resolution', 'Single image super resolution', 'Texture transfer', '', '', '', 'While implicit generative models such as GANs have shown impressive results in high quality image reconstruction and manipulation using a combination of various losses, we consider a simpler approach leading to surprisingly strong results. We show that texture loss [1] alone allows the generation of perceptually high quality images. We provide a better understanding of texture constraining mechanism and develop a novel semantically guided texture constraining method for further improvement. Using a recently developed perceptual metric employing “deep features” and termed LPIPS [2], the method obtains state-of-the-art results. Moreover, we show that a texture representation of those deep features better capture the perceptual quality of an image than the original deep features. Using texture information, off-the-shelf deep classification networks (without training) perform as well as the best performing (tuned and calibrated) LPIPS metrics.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_6');
INSERT INTO `paper` VALUES (11295, 'ThermalGAN: Multimodal Color-to-Thermal Image Translation for Person Re-identification in Multispectral Dataset', 'Person re-identification', 'Conditional GAN', 'Thermal images', '', '', 'We propose a ThermalGAN framework for cross-modality color-thermal person re-identification (ReID). We use a stack of generative adversarial networks (GAN) to translate a single color probe image to a multimodal thermal probe set. We use thermal histograms and feature descriptors as a thermal signature. We collected a large-scale multispectral ThermalWorld dataset for extensive training of our GAN model. In total the dataset includes 20216 color-thermal image pairs, 516 person ID, and ground truth pixel-level object annotations. We made the dataset freely available (http://www.zefirus.org/ThermalGAN/). We evaluate our framework on the ThermalWorld dataset to show that it delivers robust matching that competes and surpasses the state-of-the-art in cross-modality color-thermal ReID.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_46');
INSERT INTO `paper` VALUES (11296, 'Tiered Deep Similarity Search for Fashion', 'Fashion search', 'Deep metric learning', 'Multitask learning', '', '', 'How similar are two fashion clothing? Fashion apparels demonstrate diverse visual concepts with their designs, styles and brands. Hence, there exist a hierarchy of similarities between fashion clothing, ranging from exact instance or brand to similar attributes, styles. An effective search method, thus, should be able to represent the tiers of similarities. In this paper, we present a deep learning based fashion search framework for learning the tiers of similarity. We propose a new attribute-guided metric learning (AGML) with multitask CNNs that jointly learns fashion attributes and image embeddings while taking category and brand information into account. The two tasks in the framework are linked with a guiding signal. The guiding signal, first, helps in mining informative training samples. Secondly, it helps in treating training samples by their importance to capture the tiers of similarity. We conduct experiments in a new BrandFashion dataset which is richly annotated at different granularities. Experimental results demonstrate that the proposed method is very effective in capturing a tiered similarity search space and outperforms the state-of-the-art fashion search methods.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_3');
INSERT INTO `paper` VALUES (11297, 'To Learn Image Super-Resolution, Use a GAN to Learn How to Do Image Degradation First', 'Image and face super-resolution', 'Generative Adversarial Networks', 'GANs', '', '', 'This paper is on image and face super-resolution. The vast majority of prior work for this problem focus on how to increase the resolution of low-resolution images which are artificially generated by simple bilinear down-sampling (or in a few cases by blurring followed by down-sampling). We show that such methods fail to produce good results when applied to real-world low-resolution, low quality images. To circumvent this problem, we propose a two-stage process which firstly trains a High-to-Low Generative Adversarial Network (GAN) to learn how to degrade and downsample high-resolution images requiring, during training, only unpaired high and low-resolution images. Once this is achieved, the output of this network is used to train a Low-to-High GAN for image super-resolution using this time paired low- and high-resolution images. Our main result is that this network can be now used to effectively increase the quality of real-world low-resolution images. We have applied the proposed pipeline for the problem of face super-resolution where we report large improvement over baselines and prior work although the proposed method is potentially applicable to other object categories.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_12');
INSERT INTO `paper` VALUES (11298, 'Toward Characteristic-Preserving Image-Based Virtual Try-On Network', 'Virtual try-on', 'Characteristic-preserving', 'Thin plate spline', 'Image alignment', '', 'Image-based virtual try-on systems for fitting a new in-shop clothes into a person image have attracted increasing research attention, yet is still challenging. A desirable pipeline should not only transform the target clothes into the most fitting shape seamlessly but also preserve well the clothes identity in the generated image, that is, the key characteristics (e.g. texture, logo, embroidery) that depict the original clothes. However, previous image-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input image and target clothes. Prior work explicitly tackled spatial deformation using shape context matching, but failed to preserve clothing details due to its coarse-to-fine strategy. In this work, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On Network (CP-VTON) for addressing all real-world challenges in this task. First, CP-VTON learns a thin-plate spline transformation for transforming the in-shop clothes into fitting the body shape of the target person via a new Geometric Matching Module (GMM) rather than computing correspondences of interest points as prior works did. Second, to alleviate boundary artifacts of warped clothes and make the results more realistic, we employ a Try-On Module that learns a composition mask to integrate the warped clothes and the rendered image to ensure smoothness. Extensive experiments on a fashion dataset demonstrate our CP-VTON achieves the state-of-the-art virtual try-on performance both qualitatively and quantitatively.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_36');
INSERT INTO `paper` VALUES (11299, 'Toward Scale-Invariance and Position-Sensitive Region Proposal Networks', 'Object detection', 'Region proposal networks', 'Position-sensitive anchors', '', '', 'Accurately localising object proposals is an important precondition for high detection rate for the state-of-the-art object detection frameworks. The accuracy of an object detection method has been shown highly related to the average recall (AR) of the proposals. In this work, we propose an advanced object proposal network in favour of translation-invariance for objectness classification, translation-variance for bounding box regression, large effective receptive fields for capturing global context and scale-invariance for dealing with a range of object sizes from extremely small to large. The design of the network architecture aims to be simple while being effective and with real-time performance. Without bells and whistles the proposed object proposal network significantly improves the AR at 1,000 proposals by \\(35\\%\\) and \\(45\\%\\) on PASCAL VOC and COCO dataset respectively and has a fast inference time of 44.8 ms for input image size of \\(640^{2}\\). Empirical studies have also shown that the proposed method is class-agnostic to be generalised for general object proposal.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_11');
INSERT INTO `paper` VALUES (11300, 'Towards a Better Match in Siamese Network Based Visual Object Tracker', 'Realtime tracking', 'Siamese network', 'Deep convolutional neural networks', '', '', 'Recently, Siamese network based trackers have received tremendous interest for their fast tracking speed and high performance. Despite the great success, this tracking framework still suffers from several limitations. First, it cannot properly handle large object rotation. Second, tracking gets easily distracted when the background contains salient objects. In this paper, we propose two simple yet effective mechanisms, namely angle estimation and spatial masking, to address these issues. The objective is to extract more representative features so that a better match can be obtained between the same object from different frames. The resulting tracker, named Siam-BM, not only significantly improves the tracking performance, but more importantly maintains the realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM achieves an EAO of 0.335, which makes it the best-performing realtime tracker to date.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_7');
INSERT INTO `paper` VALUES (11301, 'Towards a Fair Evaluation of Zero-Shot Action Recognition Using External Data', 'Action recognition', 'Zero-Shot Learning', '', '', '', 'Zero-shot action recognition aims to classify actions not previously seen during training. This is achieved by learning a visual model for the seen source classes and establishing a semantic relationship to the unseen target classes e.g. through the action labels. In order to draw a clear line between zero-shot and conventional supervised classification, the source and target categories must be disjoint. Ensuring this premise is not trivial, especially when the source dataset is external. In this work, we propose an evaluation procedure that enables fair use of external data for zero-shot action recognition. We empirically show that external sources tend to have actions excessively similar to the target classes, strongly influencing the performance and violating the zero-shot premise. To address this, we propose a corrective method to automatically filter out too similar categories by exploiting the pairwise intra-dataset similarity of the labels. Our experiments on the HMDB-51 dataset demonstrate that the zero-shot models consistently benefit from the external sources even under our realistic evaluation, especially when the source categories of internal and external domains are combined.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_8');
INSERT INTO `paper` VALUES (11302, 'Towards Automated Multiscale Imaging and Analysis in TEM: Glomerulus Detection by Fusion of CNN and LBP Maps', 'Glomerulus detection', 'Transmission Electron Microscopy', 'Convolutional Neural Networks', 'Local binary patterns', 'Digital pathology', 'Glomerulal structures in kidney tissue have to be analysed at a nanometer scale for several medical diagnoses. They are therefore commonly imaged using Transmission Electron Microscopy. The high resolution produces large amounts of data and requires long acquisition time, which makes automated imaging and glomerulus detection a desired option. This paper presents a deep learning approach for Glomerulus detection, using two architectures, VGG16 (with batch normalization) and ResNet50. To enhance the performance over training based only on intensity images, multiple approaches to fuse the input with texture information encoded in local binary patterns of different scales have been evaluated. The results show a consistent improvement in Glomerulus detection when fusing texture-based trained networks with intensity-based ones at a late classification stage.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_36');
INSERT INTO `paper` VALUES (11303, 'Towards Cycle-Consistent Models for Text and Image Retrieval', 'Cross-modal retrieval', 'Cycle consistency', 'Visual-semantic models', '', '', 'Cross-modal retrieval has been recently becoming an hot-spot research, thanks to the development of deeply-learnable architectures. Such architectures generally learn a joint multi-modal embedding space in which text and images could be projected and compared. Here we investigate a different approach, and reformulate the problem of cross-modal retrieval as that of learning a translation between the textual and visual domain. In particular, we propose an end-to-end trainable model which can translate text into image features and vice versa, and regularizes this mapping with a cycle-consistency criterion. Preliminary experimental evaluations show promising results with respect to ordinary visual-semantic models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_58');
INSERT INTO `paper` VALUES (11304, 'Towards End-to-End License Plate Detection and Recognition: A Large Dataset and Baseline', 'Object detection', 'Object recognition', 'Object segmentation', 'Convolutional neural network', '', 'Most current license plate (LP) detection and recognition approaches are evaluated on a small and usually unrepresentative dataset since there are no publicly available large diverse datasets. In this paper, we introduce CCPD, a large and comprehensive LP dataset. All images are taken manually by workers of a roadside parking management company and are annotated carefully. To our best knowledge, CCPD is the largest publicly available LP dataset to date with over 250k unique car images, and the only one provides vertices location annotations. With CCPD, we present a novel network model which can predict the bounding box and recognize the corresponding LP number simultaneously with high speed and accuracy. Through comparative experiments, we demonstrate our model outperforms current object detection and recognition approaches in both accuracy and speed. In real-world applications, our model recognizes LP numbers directly from relatively high-resolution images at over 61 fps and 98.5% accuracy.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_16');
INSERT INTO `paper` VALUES (11305, 'Towards Good Practices for Multi-modal Fusion in Large-Scale Video Classification', 'Video classification', 'Multi-modal learning', 'Bilinear model', '', '', 'Leveraging both visual frames and audio has been experimentally proven effective to improve large-scale video classification. Previous research on video classification mainly focuses on the analysis of visual content among extracted video frames and their temporal feature aggregation. In contrast, multimodal data fusion is achieved by simple operators like average and concatenation. Inspired by the success of bilinear pooling in the visual and language fusion, we introduce multi-modal factorized bilinear pooling (MFB) to fuse visual and audio representations. We combine MFB with different video-level features and explore its effectiveness in video classification. Experimental results on the challenging Youtube-8M v2 dataset demonstrate that MFB significantly outperforms simple fusion methods in large-scale video classification.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_26');
INSERT INTO `paper` VALUES (11306, 'Towards Human-Level License Plate Recognition', 'License Plate Recognition (LPR)', 'Semantic segmentation', 'Convolutional Neural Networks (CNN)', 'Character counting', '', 'License plate recognition (LPR) is a fundamental component of various intelligent transport systems, which is always expected to be accurate and efficient enough. In this paper, we propose a novel LPR framework consisting of semantic segmentation and character counting, towards achieving human-level performance. Benefiting from innovative structure, our method can recognize a whole license plate once rather than conducting character detection or sliding window followed by per-character recognition. Moreover, our method can achieve higher recognition accuracy due to more effectively exploiting global information and avoiding sensitive character detection, and is time-saving due to eliminating one-by-one character recognition. Finally, we experimentally verify the effectiveness of the proposed method on two public datasets (AOLP and Media Lab) and our License Plate Dataset. The results demonstrate our method significantly outperforms the previous state-of-the-art methods, and achieves the accuracies of more than 99% for almost all settings.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_19');
INSERT INTO `paper` VALUES (11307, 'Towards Learning a Realistic Rendering of Human Behavior', '', '', '', '', '', 'Realistic rendering of human behavior is of great interest for applications such as video animations, virtual reality and gaming engines. Commonly animations of persons performing actions are rendered by articulating explicit 3D models based on sequences of coarse body shape representations simulating a certain behavior. While the simulation of natural behavior can be efficiently learned, the corresponding 3D models are typically designed in manual, laborious processes or reconstructed from costly (multi-)sensor data. In this work, we present an approach towards a holistic learning framework for rendering human behavior in which all components are learned from easily available data. To enable control over the generated behavior, we utilize motion capture data and generate realistic motions based on user inputs. Alternatively, we can directly copy behavior from videos and learn a rendering of characters using RGB camera data only. Our experiments show that we can further improve data efficiency by training on multiple characters at the same time. Overall our approach shows a new path towards easily available, personalized avatar creation.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_32');
INSERT INTO `paper` VALUES (11308, 'Towards Privacy-Preserving Visual Recognition via Adversarial Training: A Pilot Study', 'Visual privacy', 'Adversarial training', 'Action recognition', '', '', 'This paper aims to improve privacy-preserving visual recognition, an increasingly demanded feature in smart camera applications, by formulating a unique adversarial training framework. The proposed framework explicitly learns a degradation transform for the original video inputs, in order to optimize the trade-off between target task performance and the associated privacy budgets on the degraded video. A notable challenge is that the privacy budget, often defined and measured in task-driven contexts, cannot be reliably indicated using any single model performance, because a strong protection of privacy has to sustain against any possible model that tries to hack privacy information. Such an uncommon situation has motivated us to propose two strategies, i.e., budget model restarting and ensemble, to enhance the generalization of the learned degradation on protecting privacy against unseen hacker models. Novel training strategies, evaluation protocols, and result visualization methods have been designed accordingly. Two experiments on privacy-preserving action recognition, with privacy budgets defined in various ways, manifest the compelling effectiveness of the proposed framework in simultaneously maintaining high target task (action recognition) performance while suppressing the privacy breach risk. The code is available at https://github.com/wuzhenyusjtu/Privacy-AdversarialLearning.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_37');
INSERT INTO `paper` VALUES (11309, 'Towards Realistic Predictors', 'Hardness score prediction', 'Realistic predictors', '', '', '', 'A new class of predictors, denoted realistic predictors, is defined. These are predictors that, like humans, assess the difficulty of examples, reject to work on those that are deemed too hard, but guarantee good performance on the ones they operate on. In this paper, we talk about a particular case of it, realistic classifiers. The central problem in realistic classification, the design of an inductive predictor of hardness scores, is considered. It is argued that this should be a predictor independent of the classifier itself, but tuned to it, and learned without explicit supervision, so as to learn from its mistakes. A new architecture is proposed to accomplish these goals by complementing the classifier with an auxiliary hardness prediction network (HP-Net). Sharing the same inputs as classifiers, the HP-Net outputs the hardness scores to be fed to the classifier as loss weights. Alternatively, the output of classifiers is also fed to HP-Net in a new defined loss, variant of cross entropy loss. The two networks are trained jointly in an adversarial way where, as the classifier learns to improve its predictions, the HP-Net refines its hardness scores. Given the learned hardness predictor, a simple implementation of realistic classifiers is proposed by rejecting examples with large scores. Experimental results not only provide evidence in support of the effectiveness of the proposed architecture and the learned hardness predictor, but also show that the realistic classifier always improves performance on the examples that it accepts to classify, performing better on these examples than an equivalent nonrealistic classifier. All of these make it possible for realistic classifiers to guarantee a good performance.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_3');
INSERT INTO `paper` VALUES (11310, 'Towards Robust Neural Networks via Random Self-ensemble', '', '', '', '', '', 'Recent studies have revealed the vulnerability of deep neural networks: A small adversarial perturbation that is imperceptible to human can easily make a well-trained deep neural network misclassify. This makes it unsafe to apply neural networks in security-critical applications. In this paper, we propose a new defense algorithm called Random Self-Ensemble (RSE) by combining two important concepts: randomness and ensemble. To protect a targeted model, RSE adds random noise layers to the neural network to prevent the strong gradient-based attacks, and ensembles the prediction over random noises to stabilize the performance. We show that our algorithm is equivalent to ensemble an infinite number of noisy models \\(f_\\epsilon \\) without any additional memory overhead, and the proposed training procedure based on noisy stochastic gradient descent can ensure the ensemble model has a good predictive capability. Our algorithm significantly outperforms previous defense techniques on real data sets. For instance, on CIFAR-10 with VGG network (which has 92% accuracy without any attack), under the strong C&W attack within a certain distortion tolerance, the accuracy of unprotected model drops to less than 10%, the best previous defense technique has \\(48\\%\\) accuracy, while our method still has \\(86\\%\\) prediction accuracy under the same level of attack. Finally, our method is simple and easy to integrate into any neural network.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_23');
INSERT INTO `paper` VALUES (11311, 'Tracking Emerges by Colorizing Videos', 'Colorization', 'Self-supervised learning', 'Tracking', 'Video', '', 'We use large amounts of unlabeled video to learn models for visual tracking without manual human supervision. We leverage the natural temporal coherency of color to create a model that learns to colorize gray-scale videos by copying colors from a reference frame. Quantitative and qualitative experiments suggest that this task causes the model to automatically learn to track visual regions. Although the model is trained without any ground-truth labels, our method learns to track well enough to outperform the latest methods based on optical flow. Moreover, our results suggest that failures to track are correlated with failures to colorize, indicating that advancing video colorization may further improve self-supervised visual tracking.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_24');
INSERT INTO `paper` VALUES (11312, 'TrackingNet: A Large-Scale Dataset and Benchmark for Object Tracking in the Wild', 'Object tracking', 'Dataset', 'Benchmark', 'Deep learning', '', 'Despite the numerous developments in object tracking, further improvement of current tracking algorithms is limited by small and mostly saturated datasets. As a matter of fact, data-hungry trackers based on deep-learning currently rely on object detection datasets due to the scarcity of dedicated large-scale tracking datasets. In this work, we present TrackingNet, the first large-scale dataset and benchmark for object tracking in the wild. We provide more than 30K videos with more than 14 million dense bounding box annotations. Our dataset covers a wide selection of object classes in broad and diverse context. By releasing such a large-scale dataset, we expect deep trackers to further improve and generalize. In addition, we introduce a new benchmark composed of 500 novel videos, modeled with a distribution similar to our training dataset. By sequestering the annotation of the test set and providing an online evaluation server, we provide a fair benchmark for future development of object trackers. Deep trackers fine-tuned on a fraction of our dataset improve their performance by up to 1.6% on OTB100 and up to 1.7% on TrackingNet Test. We provide an extensive benchmark on TrackingNet by evaluating more than 20 trackers. Our results suggest that object tracking in the wild is far from being solved.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_19');
INSERT INTO `paper` VALUES (11313, 'Training Binary Weight Networks via Semi-Binary Decomposition', 'Deep neural networks', 'Binary weight networks', 'Deep network acceleration and compression', '', '', 'Recently binary weight networks have attracted lots of attentions due to their high computational efficiency and small parameter size. Yet they still suffer from large accuracy drops because of their limited representation capacity. In this paper, we propose a novel semi-binary decomposition method which decomposes a matrix into two binary matrices and a diagonal matrix. Since the matrix product of binary matrices has more numerical values than binary matrix, the proposed semi-binary decomposition has more representation capacity. Besides, we propose an alternating optimization method to solve the semi-binary decomposition problem while keeping binary constraints. Extensive experiments on AlexNet, ResNet-18, and ResNet-50 demonstrate that our method outperforms state-of-the-art methods by a large margin (5% higher in top1 accuracy). We also implement binary weight AlexNet on FPGA platform, which shows that our proposed method can achieve \\(\\sim \\)9\\(\\times \\) speed-ups while reducing the consumption of on-chip memory and dedicated multipliers significantly.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_39');
INSERT INTO `paper` VALUES (11314, 'Training Compact Deep Learning Models for Video Classification Using Circulant Matrices', 'Deep learning', 'Computer vision', 'Structured matrices', 'Circulant matrices', '', 'In real world scenarios, model accuracy is hardly the only factor to consider. Large models consume more memory and are computationally more intensive, which make them difficult to train and to deploy, especially on mobile devices. In this paper, we build on recent results at the crossroads of Linear Algebra and Deep Learning which demonstrate how imposing a structure on large weight matrices can be used to reduce the size of the model. Building on these results, we propose very compact models for video classification based on state-of-the-art network architectures such as Deep Bag-of-Frames, NetVLAD and NetFisherVectors. We then conduct thorough experiments using the large YouTube-8M video classification dataset. As we will show, the circulant DBoF embedding achieves an excellent trade-off between size and accuracy.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_25');
INSERT INTO `paper` VALUES (11315, 'Transductive Centroid Projection for Semi-supervised Large-Scale Recognition', 'Person Re-ID', 'Face recognition', 'Deep semi-supervised learning', '', '', 'Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suffer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but effective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the final classification layer (called anchors) converge to the central direction of each class in hyperspace. Specifically, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conflict and enables the unlabelled data functioning as labelled data. We inspect its effectiveness with elaborate ablation study on seven public face/person classification benchmarks. Without any bells and whistles, TCP can achieve significant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_5');
INSERT INTO `paper` VALUES (11316, 'Transductive Semi-Supervised Deep Learning Using Min-Max Features', 'Transductive Semi-Supervised Deep Learning (TSSDL)', 'Min-Max Feature (MMF) regularization', 'Deep Convolutional Neural Network (DCNN)', 'Confidence levels', '', 'In this paper, we propose Transductive Semi-Supervised Deep Learning (TSSDL) method that is effective for training Deep Convolutional Neural Network (DCNN) models. The method applies transductive learning principle to DCNN training, introduces confidence levels on unlabeled image samples to overcome unreliable label estimates on outliers and uncertain samples, and develops the Min-Max Feature (MMF) regularization that encourages DCNN to learn feature descriptors with better between-class separability and within-class compactness. TSSDL method is independent of any DCNN architectures and complementary to the latest Semi-Supervised Learning (SSL) methods. Comprehensive experiments on the benchmark datasets CIFAR10 and SVHN have shown that the DCNN model trained by the proposed TSSDL method can produce image classification accuracies compatible to the state-of-the-art SSL methods, and that combining TSSDL with the Mean Teacher method can produce the best classification accuracies on the two benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_19');
INSERT INTO `paper` VALUES (11317, 'Transferable Adversarial Perturbations', 'Adversarial perturbations', 'Transferability', 'Black-box attacks', '', '', 'State-of-the-art deep neural network classifiers are highly vulnerable to adversarial examples which are designed to mislead classifiers with a very small perturbation. However, the performance of black-box attacks (without knowledge of the model parameters) against deployed models always degrades significantly. In this paper, We propose a novel way of perturbations for adversarial examples to enable black-box transfer. We first show that maximizing distance between natural images and their adversarial examples in the intermediate feature maps can improve both white-box attacks (with knowledge of the model parameters) and black-box attacks. We also show that smooth regularization on adversarial perturbations enables transferring across models. Extensive experimental results show that our approach outperforms state-of-the-art methods both in white-box and black-box attacks.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_28');
INSERT INTO `paper` VALUES (11318, 'Transferring GANs: Generating Images from Limited Data', 'Generative adversarial networks', 'Transfer learning', 'Domain adaptation', 'Image generation', '', 'Transferring knowledge of pre-trained networks to new domains by means of fine-tuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pre-trained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pre-trained model was trained without conditioning. Our results also suggest that density is more important than diversity and a dataset with one or few densely sampled classes is a better source model than more diverse datasets such as ImageNet or Places.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_14');
INSERT INTO `paper` VALUES (11319, 'Triplet Loss in Siamese Network for Object Tracking', 'Siamese network', 'Triplet loss', 'Object tracking', 'Real-time', '', 'Object tracking is still a critical and challenging problem with many applications in computer vision. For this challenge, more and more researchers pay attention to applying deep learning to get powerful feature for better tracking accuracy. In this paper, a novel triplet loss is proposed to extract expressive deep feature for object tracking by adding it into Siamese network framework instead of pairwise loss for training. Without adding any inputs, our approach is able to utilize more elements for training to achieve more powerful feature via the combination of original samples. Furthermore, we propose a theoretical analysis by combining comparison of gradients and back-propagation, to prove the effectiveness of our method. In experiments, we apply the proposed triplet loss for three real-time trackers based on Siamese network. And the results on several popular tracking benchmarks show our variants operate at almost the same frame-rate with baseline trackers and achieve superior tracking performance than them, as well as the comparable accuracy with recent state-of-the-art real-time trackers.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_28');
INSERT INTO `paper` VALUES (11320, 'TS\\(^{2}\\)C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection', 'Weakly-supervised learning', 'Object detection', 'Semantic segmentation', '', '', 'This work provides a simple approach to discover tight object bounding boxes with only image-level supervision, called Tight box mining with Surrounding Segmentation Context (TS2C). We observe that object candidates mined through current multiple instance learning methods are usually trapped to discriminative object parts, rather than the entire object. TS2C leverages surrounding segmentation context derived from weakly-supervised segmentation to suppress such low-quality distracting candidates and boost the high-quality ones. Specifically, TS2C is developed based on two key properties of desirable bounding boxes: (1) high purity, meaning most pixels in the box are with high object response, and (2) high completeness, meaning the box covers high object response pixels comprehensively. With such novel and computable criteria, more tight candidates can be discovered for learning a better object detector. With TS2C, we obtain 48.0% and 44.4% mAP scores on VOC 2007 and 2012 benchmarks, which are the new state-of-the-arts.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_27');
INSERT INTO `paper` VALUES (11321, 'Turning a Blind Eye: Explicit Removal of Biases and Variation from Deep Neural Network Embeddings', 'Dataset bias', 'Face attribute classification', 'Ancestral origin dataset', '', '', 'Neural networks achieve the state-of-the-art in image classification tasks. However, they can encode spurious variations or biases that may be present in the training data. For example, training an age predictor on a dataset that is not balanced for gender can lead to gender biased predicitons (e.g. wrongly predicting that males are older if only elderly males are in the training set). We present two distinct contributions: (1) An algorithm that can remove multiple sources of variation from the feature representation of a network. We demonstrate that this algorithm can be used to remove biases from the feature representation, and thereby improve classification accuracies, when training networks on extremely biased datasets. (2) An ancestral origin database of 14,000 images of individuals from East Asia, the Indian subcontinent, sub-Saharan Africa, and Western Europe. We demonstrate on this dataset, for a number of facial attribute classification tasks, that we are able to remove racial biases from the network feature representation.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_34');
INSERT INTO `paper` VALUES (11322, 'Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net', 'Instance normalization', 'Invariance', 'Generalization', 'CNNs', '', 'Convolutional neural networks (CNNs) have achieved great successes in many computer vision problems. Unlike existing works that designed CNN architectures to improve performance on a single task of a single domain and not generalizable, we present IBN-Net, a novel convolutional architecture, which remarkably enhances a CNN’s modeling ability on one domain (e.g. Cityscapes) as well as its generalization capacity on another domain (e.g. GTA5) without finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch Normalization (BN) as building blocks, and can be wrapped into many advanced deep networks to improve their performances. This work has three key contributions. (1) By delving into IN and BN, we disclose that IN learns features that are invariant to appearance changes, such as colors, styles, and virtuality/reality, while BN is essential for preserving content related information. (2) IBN-Net can be applied to many advanced deep architectures, such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their performance without increasing computational cost. (3) When applying the trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves comparable improvements as domain adaptation methods, even without using data from the target domain. With IBN-Net, we won the 1st place on the WAD 2018 Challenge Drivable Area track, with an mIoU of 86.18%.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_29');
INSERT INTO `paper` VALUES (11323, 'U-PC: Unsupervised Planogram Compliance', 'Planogram compliance', 'Merchandise recognition', '', '', '', 'We present an end-to-end solution for recognizing merchandise displayed in the shelves of a supermarket. Given images of individual products, which are taken under ideal illumination for product marketing, the challenge is to find these products automatically in the images of the shelves. Note that the images of shelves are taken using hand-held camera under store level illumination. We provide a two-layer hypotheses generation and verification model. In the first layer, the model predicts a set of candidate merchandise at a specific location of the shelf while in the second layer, the hypothesis is verified by a novel graph theoretic approach. The performance of the proposed approach on two publicly available datasets is better than the competing approaches by at least 10%.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_36');
INSERT INTO `paper` VALUES (11324, 'UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition', 'UAV', 'Gesture dataset', 'UAV control', 'Gesture recognition', '', 'Current UAV-recorded datasets are mostly limited to action recognition and object tracking, whereas the gesture signals datasets were mostly recorded in indoor spaces. Currently, there is no outdoor recorded public video dataset for UAV commanding signals. Gesture signals can be effectively used with UAVs by leveraging the UAVs visual sensors and operational simplicity. To fill this gap and enable research in wider application areas, we present a UAV gesture signals dataset recorded in an outdoor setting. We selected 13 gestures suitable for basic UAV navigation and command from general aircraft handling and helicopter handling signals. We provide 119 high-definition video clips consisting of 37151 frames. The overall baseline gesture recognition performance computed using Pose-based Convolutional Neural Network (P-CNN) is 91.9%. All the frames are annotated with body joints and gesture classes in order to extend the dataset’s applicability to a wider research area including gesture recognition, action recognition, human pose recognition and situation awareness.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_9');
INSERT INTO `paper` VALUES (11325, 'Uncertainty Estimates and Multi-hypotheses Networks for Optical Flow', 'Convolutional neural networks', 'Optical flow estimation', 'Uncertainty estimation', '', '', 'Optical flow estimation can be formulated as an end-to-end supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoff compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the first time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical flow estimation. Moreover, we introduce a new network architecture and loss function that enforce complementary hypotheses and provide uncertainty estimates efficiently with a single forward pass and without the need for sampling or ensembles. We demonstrate the quality of the uncertainty estimates, which is clearly above previous confidence measures on optical flow and allows for interactive frame rates.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_40');
INSERT INTO `paper` VALUES (11326, 'Understanding Center Loss Based Network for Image Retrieval with Few Training Data', 'Center loss', 'Image retrieval', 'Small training dataset', '', '', 'Performance of convolutional neural network based image retrieval depends on the characteristics and statistics of the data being used for training. We show that for training datasets with a large number of classes but small number of images per class, the combination of cross-entropy loss and center loss works better than either of the losses alone. While cross-entropy loss tries to minimize misclassification of data, center loss minimizes the embedding space distance of each point in a class to its center, bringing together data-points belonging to the same class.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_63');
INSERT INTO `paper` VALUES (11327, 'Understanding Degeneracies and Ambiguities in Attribute Transfer', '', '', '', '', '', 'We study the problem of building models that can transfer selected attributes from one image to another without affecting the other attributes. Towards this goal, we develop analysis and a training methodology for autoencoding models, whose encoded features aim to disentangle attributes. These features are explicitly split into two components: one that should represent attributes in common between pairs of images, and another that should represent attributes that change between pairs of images. We show that achieving this objective faces two main challenges: One is that the model may learn degenerate mappings, which we call shortcut problem, and the other is that the attribute representation for an image is not guaranteed to follow the same interpretation on another image, which we call reference ambiguity. To address the shortcut problem, we introduce novel constraints on image pairs and triplets and show their effectiveness both analytically and experimentally. In the case of the reference ambiguity, we formally prove that a model that guarantees an ideal feature separation cannot be built. We validate our findings on several datasets and show that, surprisingly, trained neural networks often do not exhibit the reference ambiguity.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_43');
INSERT INTO `paper` VALUES (11328, 'Understanding Fake Faces', 'Face recognition', 'False positives', 'Simulacra', '', '', 'Face recognition research is one of the most active topics in computer vision (CV), and deep neural networks (DNN) are now filling the gap between human-level and computer-driven performance levels in face verification algorithms. However, although the performance gap appears to be narrowing in terms of accuracy-based expectations, a curious question has arisen; specifically, Face understanding of AI is really close to that of human? In the present study, in an effort to confirm the brain-driven concept, we conduct image-based detection, classification, and generation using an in-house created fake face database. This database has two configurations: (i) false positive face detections produced using both the Viola Jones (VJ) method and convolutional neural networks (CNN), and (ii) simulacra that have fundamental characteristics that resemble faces but are completely artificial. The results show a level of suggestive knowledge that indicates the continuing existence of a gap between the capabilities of recent vision-based face recognition algorithms and human-level performance. On a positive note, however, we have obtained knowledge that will advance the progress of face-understanding models.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_42');
INSERT INTO `paper` VALUES (11329, 'Understanding Perceptual and Conceptual Fluency at a Large Scale', 'Marketing application', 'Visual design', 'Cognitive information processing', 'Construal level theory', '', 'We create a dataset of 543,758 logo designs spanning 39 industrial categories and 216 countries. We experiment and compare how different deep convolutional neural network (hereafter, DCNN) architectures, pretraining protocols, and weight initializations perform in predicting design memorability and likability. We propose and provide estimation methods based on training DCNNs to extract and evaluate two independent constructs for designs: perceptual distinctiveness (“perceptual fluency” metrics) and ambiguity in meaning (“conceptual fluency” metrics) of each logo. We provide evidences of causal inference that both constructs significantly affect memory for a logo design, consistent with cognitive elaboration theory. The effect on liking, however, is interactive, consistent with processing fluency (e.g., Lee and Labroo (2004), and Landwehr et al. (2011)).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_41');
INSERT INTO `paper` VALUES (11330, 'Unified Perceptual Parsing for Scene Understanding', 'Deep neural network', 'Semantic segmentation', 'Scene understanding', '', '', 'Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes (Models are available at https://github.com/CSAILVision/unifiedparsing).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_26');
INSERT INTO `paper` VALUES (11331, 'Universal Sketch Perceptual Grouping', 'Sketch perceptual grouping', 'Universal grouper', 'Deep grouping model', 'Dataset', '', 'In this work we aim to develop a universal sketch grouper. That is, a grouper that can be applied to sketches of any category in any domain to group constituent strokes/segments into semantically meaningful object parts. The first obstacle to this goal is the lack of large-scale datasets with grouping annotation. To overcome this, we contribute the largest sketch perceptual grouping (SPG) dataset to date, consisting of 20, 000 unique sketches evenly distributed over 25 object categories. Furthermore, we propose a novel deep universal perceptual grouping model. The model is learned with both generative and discriminative losses. The generative losses improve the generalisation ability of the model to unseen object categories and datasets. The discriminative losses include a local grouping loss and a novel global grouping loss to enforce global grouping consistency. We show that the proposed model significantly outperforms the state-of-the-art groupers. Further, we show that our grouper is useful for a number of sketch analysis tasks including sketch synthesis and fine-grained sketch-based image retrieval (FG-SBIR).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_36');
INSERT INTO `paper` VALUES (11332, 'Unpaired Image Captioning by Language Pivoting', 'Image captioning', 'Unpaired learning', '', '', '', 'Image captioning is a multimodal task involving computer vision and natural language processing, where the goal is to learn a mapping from the image to its natural language description. In general, the mapping function is learned from a training set of image-caption pairs. However, for some language, large scale image-caption paired corpus might not be available. We present an approach to this unpaired image captioning problem by language pivoting. Our method can effectively capture the characteristics of an image captioner from the pivot language (Chinese) and align it to the target language (English) using another pivot-target (Chinese-English) sentence parallel corpus. We evaluate our method on two image-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative comparisons against several baseline approaches demonstrate the effectiveness of our method.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_31');
INSERT INTO `paper` VALUES (11333, 'Unpaired Thermal to Visible Spectrum Transfer Using Adversarial Training', 'Thermal imaging', 'Generative Adversarial Networks', 'Unsupervised learning', 'Colorization', '', 'Thermal Infrared (TIR) cameras are gaining popularity in many computer vision applications due to their ability to operate under low-light conditions. Images produced by TIR cameras are usually difficult for humans to perceive visually, which limits their usability. Several methods in the literature were proposed to address this problem by transforming TIR images into realistic visible spectrum (VIS) images. However, existing TIR-VIS datasets suffer from imperfect alignment between TIR-VIS image pairs which degrades the performance of supervised methods. We tackle this problem by learning this transformation using an unsupervised Generative Adversarial Network (GAN) which trains on unpaired TIR and VIS images. When trained and evaluated on KAIST-MS dataset, our proposed methods was shown to produce significantly more realistic and sharp VIS images than the existing state-of-the-art supervised methods. In addition, our proposed method was shown to generalize very well when evaluated on a new dataset of new environments.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_49');
INSERT INTO `paper` VALUES (11334, 'Unsupervised Class-Specific Deblurring', 'Motion blur', 'Deblur', 'Reblur', 'Unsupervised learning', 'GAN', 'In this paper, we present an end-to-end deblurring network designed specifically for a class of data. Unlike the prior supervised deep-learning works that extensively rely on large sets of paired data, which is highly demanding and challenging to obtain, we propose an unsupervised training scheme with unpaired data to achieve the same. Our model consists of a Generative Adversarial Network (GAN) that learns a strong prior on the clean image domain using adversarial loss and maps the blurred image to its clean equivalent. To improve the stability of GAN and to preserve the image correspondence, we introduce an additional CNN module that reblurs the generated GAN output to match with the blurred input. Along with these two modules, we also make use of the blurred image itself to self-guide the network to constrain the solution space of generated clean images. This self-guidance is achieved by imposing a scale-space gradient error with an additional gradient module. We train our model on different classes and observe that adding the reblur and gradient modules helps in better convergence. Extensive experiments demonstrate that our method performs favorably against the state-of-the-art supervised methods on both synthetic and real-world images even in the absence of any supervision.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_22');
INSERT INTO `paper` VALUES (11335, 'Unsupervised CNN-Based Co-saliency Detection with Graphical Optimization', 'Co-saliency detection', 'Unsupervised learning', 'Convolutional neural networks', 'Deep learning', 'Graphical model', 'In this paper, we address co-saliency detection in a set of images jointly covering objects of a specific class by an unsupervised convolutional neural network (CNN). Our method does not require any additional training data in the form of object masks. We decompose co-saliency detection into two sub-tasks, single-image saliency detection and cross-image co-occurrence region discovery corresponding to two novel unsupervised losses, the single-image saliency (SIS) loss and the co-occurrence (COOC) loss. The two losses are modeled on a graphical model where the former and the latter act as the unary and pairwise terms, respectively. These two tasks can be jointly optimized for generating co-saliency maps of high quality. Furthermore, the quality of the generated co-saliency maps can be enhanced via two extensions: map sharpening by self-paced learning and boundary preserving by fully connected conditional random fields. Experiments show that our method achieves superior results, even outperforming many supervised methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_30');
INSERT INTO `paper` VALUES (11336, 'Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency', '3D keypoint estimation', 'Multi-view consistency', 'Domain adaptation', 'Unsupervised learning', '', 'In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan or image. Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other. Such view consistency can provide effective regularization for keypoint prediction on unlabeled instances. In addition, we introduce a geometric alignment term to regularize predictions in the target domain. The resulting loss function can be effectively optimized via alternating minimization. We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art general-purpose domain adaptation techniques.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_9');
INSERT INTO `paper` VALUES (11337, 'Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-training', '', '', '', '', '', 'Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world “wild tasks” where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as “domain gap”, and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training (ST) procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of ST, we also propose a novel class-balanced self-training (CBST) framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_18');
INSERT INTO `paper` VALUES (11338, 'Unsupervised Event-Based Optical Flow Using Motion Compensation', 'Event cameras', 'Unsupervised learning', 'Optical flow', '', '', 'In this work, we propose a novel framework for unsupervised learning for event cameras that learns to predict optical flow from only the event stream. In particular, we propose an input representation of the events in the form of a discretized 3D volume, which we pass through a neural network to predict the optical flow for each event. This optical flow is used to attempt to remove any motion blur in the event image. We then propose a loss function applied to the motion compensated event image that measures the motion blur in this image. We evaluate this network on the Multi Vehicle Stereo Event Camera dataset (MVSEC), along with qualitative results from a variety of different scenes.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_54');
INSERT INTO `paper` VALUES (11339, 'Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation', '3D reconstruction', 'Semi-supervised training', 'Representation learning', 'Monocular human pose reconstruction', '', 'Modern 3D human pose estimation techniques rely on deep networks, which require large amounts of training data. While weakly-supervised methods require less supervision, by utilizing 2D poses or multi-view imagery without annotations, they still need a sufficiently large set of samples with 3D annotations for learning to succeed.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_46');
INSERT INTO `paper` VALUES (11340, 'Unsupervised Hard Example Mining from Videos for Improved Object Detection', 'Object detection', 'Face detection', 'Pedestrian detection', 'Semi-supervised learning', 'Hard negative mining', 'Important gains have recently been obtained in object detection by using training objectives that focus on hard negative examples, i.e., negative examples that are currently rated as positive or ambiguous by the detector. These examples can strongly influence parameters when the network is trained to correct them. Unfortunately, they are often sparse in the training data, and are expensive to obtain. In this work, we show how large numbers of hard negatives can be obtained automatically by analyzing the output of a trained detector on video sequences. In particular, detections that are isolated in time, i.e., that have no associated preceding or following detections, are likely to be hard negatives. We describe simple procedures for mining large numbers of such hard negatives (and also hard positives) from unlabeled video data. Our experiments show that retraining detectors on these automatically obtained examples often significantly improves performance. We present experiments on multiple architectures and multiple data sets, including face detection, pedestrian detection and other object categories.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_19');
INSERT INTO `paper` VALUES (11341, 'Unsupervised Holistic Image Generation from Key Local Patches', 'Image synthesis', 'Generative adversarial networks', '', '', '', 'We introduce a new problem of generating an image based on a small number of key local patches without any geometric prior. In this work, key local patches are defined as informative regions of the target object or scene. This is a challenging problem since it requires generating realistic images and predicting locations of parts at the same time. We construct adversarial networks to tackle this problem. A generator network generates a fake image as well as a mask based on the encoder-decoder framework. On the other hand, a discriminator network aims to detect fake images. The network is trained with three losses to consider spatial, appearance, and adversarial information. The spatial loss determines whether the locations of predicted parts are correct. Input patches are restored in the output image without much modification due to the appearance loss. The adversarial loss ensures output images are realistic. The proposed network is trained without supervisory signals since no labels of key parts are required. Experimental results on seven datasets demonstrate that the proposed algorithm performs favorably on challenging objects and scenes.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_2');
INSERT INTO `paper` VALUES (11342, 'Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks', 'Image-to-image translation', 'Unsupervised learning', 'Genearative adverserial network (GAN)', '', '', 'Recent studies on unsupervised image-to-image translation have made remarkable progress by training a pair of generative adversarial networks with a cycle-consistent loss. However, such unsupervised methods may generate inferior results when the image resolution is high or the two image domains are of significant appearance differences, such as the translations between semantic layouts and natural images in the Cityscapes dataset. In this paper, we propose novel Stacked Cycle-Consistent Adversarial Networks (SCANs) by decomposing a single translation into multi-stage transformations, which not only boost the image translation quality but also enable higher resolution image-to-image translation in a coarse-to-fine fashion. Moreover, to properly exploit the information from the previous stage, an adaptive fusion block is devised to learn a dynamic integration of the current stage’s output and the previous stage’s output. Experiments on multiple datasets demonstrate that our proposed approach can improve the translation quality compared with previous single-stage unsupervised methods.', 'ECCV', '2018', '05 October 2018', 'https://doi.org/10.1007/978-3-030-01240-3_12');
INSERT INTO `paper` VALUES (11343, 'Unsupervised Learning of Multi-Frame Optical Flow with Occlusions', '', '', '', '', '', 'Learning optical flow with neural networks is hampered by the need for obtaining training data with associated ground truth. Unsupervised learning is a promising direction, yet the performance of current unsupervised methods is still limited. In particular, the lack of proper occlusion handling in commonly used data terms constitutes a major source of error. While most optical flow methods process pairs of consecutive frames, more advanced occlusion reasoning can be realized when considering multiple frames. In this paper, we propose a framework for unsupervised learning of optical flow and occlusions over multiple frames. More specifically, we exploit the minimal configuration of three frames to strengthen the photometric loss and explicitly reason about occlusions. We demonstrate that our multi-frame, occlusion-sensitive formulation outperforms existing unsupervised two-frame methods and even produces results on par with some fully supervised methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_42');
INSERT INTO `paper` VALUES (11344, 'Unsupervised Person Re-identification by Deep Learning Tracklet Association', 'Person re-identification', 'Unsupervised learning', 'Tracklet', 'Surveillance video', '', 'Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in practical re-id deployment due to the lack of exhaustive identity labelling of image positive and negative pairs for every camera pair. In this work, we address this problem by proposing an unsupervised re-id deep learning approach capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data from videos in an end-to-end model optimisation. We formulate a Tracklet Association Unsupervised Deep Learning (TAUDL) framework characterised by jointly learning per-camera (within-camera) tracklet association (labelling) and cross-camera tracklet correlation by maximising the discovery of most likely tracklet relationships across camera views. Extensive experiments demonstrate the superiority of the proposed TAUDL model over the state-of-the-art unsupervised and domain adaptation re-id methods using six person re-id benchmarking datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_45');
INSERT INTO `paper` VALUES (11345, 'Unsupervised Video Object Segmentation Using Motion Saliency-Guided Spatio-Temporal Propagation', 'Unsupervised Video Object Segmentation', 'DAVIS Dataset', 'Saliency Estimation', 'Neighborhood Graph', 'Semi-supervised Setting', 'Unsupervised video segmentation plays an important role in a wide variety of applications from object identification to compression. However, to date, fast motion, motion blur and occlusions pose significant challenges. To address these challenges for unsupervised video segmentation, we develop a novel saliency estimation technique as well as a novel neighborhood graph, based on optical flow and edge cues. Our approach leads to significantly better initial foreground-background estimates and their robust as well as accurate diffusion across time. We evaluate our proposed algorithm on the challenging DAVIS, SegTrack v2 and FBMS-59 datasets. Despite the usage of only a standard edge detector trained on 200 images, our method achieves state-of-the-art results outperforming deep learning based methods in the unsupervised setting. We even demonstrate competitive results comparable to deep learning based methods in the semi-supervised setting on the DAVIS dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_48');
INSERT INTO `paper` VALUES (11346, 'Unsupervised Video Object Segmentation with Motion-Based Bilateral Networks', 'Video object segmentation', 'Bilateral networks', 'Instance embeddings', '', '', 'In this work, we study the unsupervised video object segmentation problem where moving objects are segmented without prior knowledge of these objects. First, we propose a motion-based bilateral network to estimate the background based on the motion pattern of non-object regions. The bilateral network reduces false positive regions by accurately identifying background objects. Then, we integrate the background estimate from the bilateral network with instance embeddings into a graph, which allows multiple frame reasoning with graph edges linking pixels from different frames. We classify graph nodes by defining and minimizing a cost function, and segment the video frames based on the node labels. The proposed method outperforms previous state-of-the-art unsupervised video object segmentation methods against the DAVIS 2016 and the FBMS-59 datasets.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_13');
INSERT INTO `paper` VALUES (11347, 'Unveiling the Power of Deep Tracking', '', '', '', '', '', 'In the field of generic object tracking numerous attempts have been made to exploit deep features. Despite all expectations, deep trackers are yet to reach an outstanding level of performance compared to methods solely based on handcrafted features. In this paper, we investigate this key issue and propose an approach to unlock the true potential of deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy. Extensive experiments are performed on four challenging datasets. On VOT2017, our approach significantly outperforms the top performing tracker from the challenge with a relative gain of \\(17\\%\\) in EAO.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_30');
INSERT INTO `paper` VALUES (11348, 'Urban Zoning Using Higher-Order Markov Random Fields on Multi-View Imagery Data', 'Urban zoning', 'Street-view images', 'Satellite images', 'Higher-order Markov random fields', '', 'Urban zoning enables various applications in land use analysis and urban planning. As cities evolve, it is important to constantly update the zoning maps of cities to reflect urban pattern changes. This paper proposes a method for automatic urban zoning using higher-order Markov random fields (HO-MRF) built on multi-view imagery data including street-view photos and top-view satellite images. In the proposed HO-MRF, top-view satellite data is segmented via a multi-scale deep convolutional neural network (MS-CNN) and used in lower-order potentials. Street-view data with geo-tagged information is augmented in higher-order potentials. Various feature types for classifying street-view images were also investigated in our work. We evaluated the proposed method on a number of famous metropolises and provided in-depth analysis on technical issues.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_38');
INSERT INTO `paper` VALUES (11349, 'Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks', 'Adversarial', 'Interference', 'Effective Receptive Field', 'Single-stage network', 'Detection', 'This work shows that it is possible to fool/attack recent state-of-the-art face detectors which are based on the single-stage networks. Successfully attacking face detectors could be a serious malware vulnerability when deploying a smart surveillance system utilizing face detectors. In addition, for the privacy concern, it helps prevent faces being harvested and stored in the server. We show that existing adversarial perturbation methods are not effective to perform such an attack, especially when there are multiple faces in the inut image. This is because the adversarial perturbation specifically generated for one face may disrupt the adversarial perturbation for another face. In this paper, we call this problem the Instance Perturbation Interference (IPI) problem. This IPI problem is addressed by studying the relationship between the deep neural network receptive field and the adversarial perturbation. Besides the single-stage face detector, we find that the IPI problem also exists on the first stage of the Faster-RCNN, the commonly used two-stage object detector. As such, we propose the Localized Instance Perturbation (LIP) that confines the adversarial perturbation inside the Effective Receptive Field (ERF) of a target to perform the attack. Experimental results show the LIP method massively outperforms existing adversarial perturbation generation methods – often by a factor of 2 to 10.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_39');
INSERT INTO `paper` VALUES (11350, 'Using Object Information for Spotting Text', 'Text detection', 'Natural Scenes', 'Deep learning', 'Object detection', 'RCNN', 'Text spotting, also called text detection, is a challenging computer vision task because of cluttered backgrounds, diverse imaging environments, various text sizes and similarity between some objects and characters, e.g., tyre and ‘o’. However, text spotting is a vital step in numerous AI and computer vision systems, such as autonomous robots and systems for visually impaired. Due to its potential applications and commercial values, researchers have proposed various deep architectures and methods for text spotting. These methods and architectures concentrate only on text in images, but neglect other information related to text. There exists a strong relationship between certain objects and the presence of text, such as signboards or the absence of text, such as trees. In this paper, a text spotting algorithm based on text and object dependency is proposed. The proposed algorithm consists of two sub-convolutional neural networks and three training stages. For this study, a new NTU-UTOI dataset containing over 22k non-synthetic images with 277k bounding boxes for text and 42 text-related object classes is established. According to our best knowledge, it is the second largest non-synthetic text image database. Experimental results on three benchmark datasets with clutter backgrounds, COCO-Text, MSRA-TD500 and SVT show that the proposed algorithm provides comparable performance to state-of-the-art text spotting methods. Experiments are also performed on our newly established dataset to investigate the effectiveness of object information for text spotting. The experimental results indicate that the object information contributes significantly on the performance gain.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01270-0_33');
INSERT INTO `paper` VALUES (11351, 'Using Phase Instead of Optical Flow for Action Recognition', 'Motion representation', 'Phase derivatives', 'Eulerian motion representation', 'Action recognition', '', 'Currently, the most common motion representation for action recognition is optical flow. Optical flow is based on particle tracking which adheres to a Lagrangian perspective on dynamics. In contrast to the Lagrangian perspective, the Eulerian model of dynamics does not track, but describes local changes. For video, an Eulerian phase-based motion representation, using complex steerable filters, has been successfully employed recently for motion magnification and video frame interpolation. Inspired by these previous works, here, we proposes learning Eulerian motion representations in a deep architecture for action recognition. We learn filters in the complex domain in an end-to-end manner. We design these complex filters to resemble complex Gabor filters, typically employed for phase-information extraction. We propose a phase-information extraction module, based on these complex filters, that can be used in any network architecture for extracting Eulerian representations. We experimentally analyze the added value of Eulerian motion representations, as extracted by our proposed phase extraction module, and compare with existing motion representations based on optical flow, on the UCF101 dataset.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_51');
INSERT INTO `paper` VALUES (11352, 'Value-Aware Quantization for Training and Inference of Neural Networks', 'Reduced precision', 'Quantization', 'Training', 'Inference', 'Activation', 'We propose a novel value-aware quantization which applies aggressively reduced precision to the majority of data while separately handling a small amount of large values in high precision, which reduces total quantization errors under very low precision. We present new techniques to apply the proposed quantization to training and inference. The experiments show that our method with 3-bit activations (with 2% of large ones) can give the same training accuracy as full-precision one while offering significant (41.6% and 53.7%) reductions in the memory cost of activations in ResNet-152 and Inception-v3 compared with the state-of-the-art method. Our experiments also show that deep networks such as Inception-v3, ResNet-101 and DenseNet-121 can be quantized for inference with 4-bit weights and activations (with 1% 16-bit data) within 1% top-1 accuracy drop.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_36');
INSERT INTO `paper` VALUES (11353, 'Variable Ring Light Imaging: Capturing Transient Subsurface Scattering with an Ordinary Camera', 'Subsurface Scattering', 'Ring Light', 'Transient Image', 'Bounded Path Length', 'Light Transport', 'Subsurface scattering plays a significant role in determining the appearance of real-world surfaces. A light ray penetrating into the subsurface is repeatedly scattered and absorbed by particles along its path before reemerging from the outer interface, which determines its spectral radiance. We introduce a novel imaging method that enables the decomposition of the appearance of a fronto-parallel real-world surface into images of light with bounded path lengths, i.e., transient subsurface light transport. Our key idea is to observe each surface point under a variable ring light: a circular illumination pattern of increasingly larger radius centered on it. We show that the path length of light captured in each of these observations is naturally lower-bounded by the ring light radius. By taking the difference of ring light images of incrementally larger radii, we compute transient images that encode light with bounded path lengths. Experimental results on synthetic and complex real-world surfaces demonstrate that the recovered transient images reveal the subsurface structure of general translucent inhomogeneous surfaces. We further show that their differences reveal the surface colors at different surface depths. The proposed method is the first to enable the unveiling of dense and continuous subsurface structures from steady-state external appearance using ordinary camera and illumination.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_37');
INSERT INTO `paper` VALUES (11354, 'Variational Wasserstein Clustering', 'Clustering', 'Discrete distribution', 'K-means', 'Measure preserving', 'Optimal transportation', 'We propose a new clustering method based on optimal transportation. We discuss the connection between optimal transportation and k-means clustering, solve optimal transportation with the variational principle, and investigate the use of power diagrams as transportation plans for aggregating arbitrary domains into a fixed number of clusters. We drive cluster centroids through the target domain while maintaining the minimum clustering energy by adjusting the power diagram. Thus, we simultaneously pursue clustering and the Wasserstein distance between the centroids and the target domain, resulting in a measure-preserving mapping. We demonstrate the use of our method in domain adaptation, remeshing, and learning representations on synthetic and real data.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_20');
INSERT INTO `paper` VALUES (11355, 'Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes', 'Image synthesis', 'Data augmentation', 'Scene text detection', 'Scene text recognition', '', 'The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes “semantic coherent” synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_16');
INSERT INTO `paper` VALUES (11356, 'Video Compression Through Image Interpolation', '', '', '', '', '', 'An ever increasing amount of our digital communication, media consumption, and content creation revolves around videos. We share, watch, and archive many aspects of our lives through them, all of which are powered by strong video compression. Traditional video compression is laboriously hand designed and hand optimized. This paper presents an alternative in an end-to-end deep learning codec. Our codec builds on one simple idea: Video compression is repeated image interpolation. It thus benefits from recent advances in deep image interpolation and generation. Our deep video codec outperforms today’s prevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with H.264.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_26');
INSERT INTO `paper` VALUES (11357, 'Video Object Detection with an Aligned Spatial-Temporal Memory', 'Aligned spatial-temporal memory', 'Video object detection', '', '', '', 'We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM’s design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs.ucdavis.edu/project/stmn/project.html.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_30');
INSERT INTO `paper` VALUES (11358, 'Video Object Segmentation by Learning Location-Sensitive Embeddings', 'Video object segmentation', 'Location-sensitive embeddings', '', '', '', 'We address the problem of video object segmentation which outputs the masks of a target object throughout a video given only a bounding box in the first frame. There are two main challenges to this task. First, the background may contain similar objects as the target. Second, the appearance of the target object may change drastically over time. To tackle these challenges, we propose an end-to-end training network which accomplishes foreground predictions by leveraging the location-sensitive embeddings which are capable to distinguish the pixels of similar objects. To deal with appearance changes, for a test video, we propose a robust model adaptation method which pre-scans the whole video, generates pseudo foreground/background labels and retrains the model based on the labels. Our method outperforms the state-of-the-art methods on the DAVIS and the SegTrack v2 datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_31');
INSERT INTO `paper` VALUES (11359, 'Video Object Segmentation with Joint Re-identification and Attention-Aware Mask Propagation', 'Mask Generation', 'Video Object Segmentation', 'Template Expansion', 'Pose Variations', 'Deep Recurrent Network', 'The problem of video object segmentation can become extremely challenging when multiple instances co-exist. While each instance may exhibit large scale and pose variations, the problem is compounded when instances occlude each other causing failures in tracking. In this study, we formulate a deep recurrent network that is capable of segmenting and tracking objects in video simultaneously by their temporal continuity, yet able to re-identify them when they re-appear after a prolonged occlusion. We combine temporal propagation and re-identification functionalities into a single framework that can be trained end-to-end. In particular, we present a re-identification module with template expansion to retrieve missing objects despite their large appearance changes. In addition, we contribute an attention-based recurrent mask propagation approach that is robust to distractors not belonging to the target segment. Our approach achieves a new state-of-the-art \\(\\mathcal {G}\\)-mean of 68.2 on the challenging DAVIS 2017 benchmark (test-dev set), outperforming the winning solution. Project Page: http://mmlab.ie.cuhk.edu.hk/projects/DyeNet/.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_6');
INSERT INTO `paper` VALUES (11360, 'Video Object Segmentation with Referring Expressions', '', '', '', '', '', 'Most semi-supervised video object segmentation methods rely on a pixel-accurate mask of a target object provided for the first video frame. However, obtaining a detailed mask is expensive and time-consuming. In this work we explore a more practical and natural way of identifying a target object by employing language referring expressions. Leveraging recent advances of language grounding models designed for images, we propose an approach to extend them to video data, ensuring temporally coherent predictions. To evaluate our approach we augment the popular video object segmentation benchmarks, \\(\\text {DAVIS}_{\\text {16}}\\) and \\(\\text {DAVIS}_{\\text {17}}\\), with language descriptions of target objects. We show that our approach performs on par with the methods which have access to the object mask on \\(\\text {DAVIS}_{\\text {16}}\\) and is competitive to methods using scribbles on challenging \\(\\text {DAVIS}_{\\text {17}}\\).', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_2');
INSERT INTO `paper` VALUES (11361, 'Video Re-localization', 'Video re-localization', 'Cross gating', 'Bilinear matching', '', '', 'Many methods have been developed to help people find the video content they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely video re-localization, to address this need. Video re-localization is an important enabling technology with many applications, such as fast seeking in videos, video copy detection, as well as video surveillance. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence, and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with the localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the baseline methods. Our code is available at: https://github.com/fengyang0317/video_reloc.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_4');
INSERT INTO `paper` VALUES (11362, 'Video Summarization Using Fully Convolutional Sequence Networks', 'Video summarization', 'Fully convolutional neural networks', 'Sequence labeling', '', '', 'This paper addresses the problem of video summarization. Given an input video, the goal is to select a subset of the frames to create a summary video that optimally captures the important information of the input video. With the large amount of videos available online, video summarization provides a useful tool that assists video search, retrieval, browsing, etc. In this paper, we formulate video summarization as a sequence labeling problem. Unlike existing approaches that use recurrent models, we propose fully convolutional sequence models to solve video summarization. We firstly establish a novel connection between semantic segmentation and video summarization, and then adapt popular semantic segmentation networks for video summarization. Extensive experiments and analysis on two benchmark datasets demonstrate the effectiveness of our models.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_22');
INSERT INTO `paper` VALUES (11363, 'VideoMatch: Matching Based Video Object Segmentation', '', '', '', '', '', 'Video object segmentation is challenging yet important in a wide variety of applications for video analysis. Recent works formulate video object segmentation as a prediction task using deep nets to achieve appealing state-of-the-art performance. Due to the formulation as a prediction task, most of these methods require fine-tuning during test time, such that the deep nets memorize the appearance of the objects of interest in the given video. However, fine-tuning is time-consuming and computationally expensive, hence the algorithms are far from real time. To address this issue, we develop a novel matching based algorithm for video object segmentation. In contrast to memorization based classification techniques, the proposed approach learns to match extracted features to a provided template without memorizing the appearance of the objects. We validate the effectiveness and the robustness of the proposed method on the challenging DAVIS-16, DAVIS-17, Youtube-Objects and JumpCut datasets. Extensive results show that our method achieves comparable performance without fine-tuning and is much more favorable in terms of computational time.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01237-3_4');
INSERT INTO `paper` VALUES (11364, 'Videos as Space-Time Region Graphs', '', '', '', '', '', 'How do humans recognize the action “opening a book”? We argue that there are two important cues: modeling temporal shape dynamics and modeling functional relationships between humans and objects. In this paper, we propose to represent videos as space-time region graphs which capture these two important cues. Our graph nodes are defined by the object region proposals from different frames in a long range video. These nodes are connected by two types of relations: (i) similarity relations capturing the long range dependencies between correlated objects and (ii) spatial-temporal relations capturing the interactions between nearby objects. We perform reasoning on this graph representation via Graph Convolutional Networks. We achieve state-of-the-art results on the Charades and Something-Something datasets. Especially for Charades with complex environments, we obtain a huge \\(4.4\\%\\) gain when our model is applied in complex environments.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_25');
INSERT INTO `paper` VALUES (11365, 'View-Graph Selection Framework for SfM', 'View-graph', 'Structure from motion', 'Disambiguation', '', '', 'View-graph selection is a crucial step for accurate and efficient large-scale structure from motion (sfm). Most sfm methods remove undesirable images and pairs using several fixed heuristic criteria, and propose tailor-made solutions to achieve specific reconstruction objectives such as efficiency, accuracy, or disambiguation. In contrast to these disparate solutions, we propose an optimization based formulation that can be used to achieve these different reconstruction objectives with task-specific cost modeling and construct a very efficient network-flow based formulation for its approximate solution. The abstraction brought on by this selection mechanism separates the challenges specific to datasets and reconstruction objectives from the standard sfm pipeline and improves its generalization. This paper mainly focuses on application of this framework with standard sfm pipeline for accurate and ghost-free reconstructions of highly ambiguous datasets. To model selection costs for this task, we introduce new disambiguation priors based on local geometry. We further demonstrate versatility of the method by using it for the general objective of accurate and efficient reconstruction of large-scale Internet datasets using costs based on well-known sfm priors.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_33');
INSERT INTO `paper` VALUES (11366, 'Viewpoint Estimation—Insights and Model', '', '', '', '', '', 'This paper addresses the problem of viewpoint estimation of an object in a given image. It presents five key insights and a CNN that is based on them. The network’s major properties are as follows. (i) The architecture jointly solves detection, classification, and viewpoint estimation. (ii) New types of data are added and trained on. (iii) A novel loss function, which takes into account both the geometry of the problem and the new types of data, is propose. Our network allows a substantial boost in performance: from 36.1% gained by SOTA algorithms to 45.9%.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_16');
INSERT INTO `paper` VALUES (11367, 'VisDrone-DET2018: The Vision Meets Drone Object Detection in Image Challenge Results', 'Performance evaluation', 'Drone', 'Object detection in images', '', '', 'Object detection is a hot topic with various applications in computer vision, e.g., image understanding, autonomous driving, and video surveillance. Much of the progresses have been driven by the availability of object detection benchmark datasets, including PASCAL VOC, ImageNet, and MS COCO. However, object detection on the drone platform is still a challenging task, due to various factors such as view point change, occlusion, and scales. To narrow the gap between current object detection performance and the real-world requirements, we organized the Vision Meets Drone (VisDrone2018) Object Detection in Image challenge in conjunction with the 15th European Conference on Computer Vision (ECCV 2018). Specifically, we release a large-scale drone-based dataset, including 8, 599 images (6, 471 for training, 548 for validation, and 1, 580 for testing) with rich annotations, including object bounding boxes, object categories, occlusion, truncation ratios, etc. Featuring a diverse real-world scenarios, the dataset was collected using various drone models, in different scenarios (across 14 different cities spanned over thousands of kilometres), and under various weather and lighting conditions. We mainly focus on ten object categories in object detection, i.e., pedestrian, person, car, van, bus, truck, motor, bicycle, awning-tricycle, and tricycle. Some rarely occurring special vehicles (e.g., machineshop truck, forklift truck, and tanker) are ignored in evaluation. The dataset is extremely challenging due to various factors, including large scale and pose variations, occlusion, and clutter background. We present the evaluation protocol of the VisDrone-DET2018 challenge and the comparison results of 38 detectors on the released dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We expect the challenge to largely boost the research and development in object detection in images on drone platforms.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_27');
INSERT INTO `paper` VALUES (11368, 'VisDrone-SOT2018: The Vision Meets Drone Single-Object Tracking Challenge Results', 'Performance evaluation', 'Drone', 'Single-object tracking', '', '', 'Single-object tracking, also known as visual tracking, on the drone platform attracts much attention recently with various applications in computer vision, such as filming and surveillance. However, the lack of commonly accepted annotated datasets and standard evaluation platform prevent the developments of algorithms. To address this issue, the Vision Meets Drone Single-Object Tracking (VisDrone-SOT2018) Challenge workshop was organized in conjunction with the 15th European Conference on Computer Vision (ECCV 2018) to track and advance the technologies in such field. Specifically, we collect a dataset, including 132 video sequences divided into three non-overlapping sets, i.e., training (86 sequences with 69, 941 frames), validation (11 sequences with 7, 046 frames), and testing (35 sequences with 29, 367 frames) sets. We provide fully annotated bounding boxes of the targets as well as several useful attributes, e.g., occlusion, background clutter, and camera motion. The tracking targets in these sequences include pedestrians, cars, buses, and animals. The dataset is extremely challenging due to various factors, such as occlusion, large scale, pose variation, and fast motion. We present the evaluation protocol of the VisDrone-SOT2018 challenge and the results of a comparison of 22 trackers on the benchmark dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We hope this challenge largely boosts the research and development in single object tracking on drone platforms.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_28');
INSERT INTO `paper` VALUES (11369, 'VisDrone-VDT2018: The Vision Meets Drone Video Detection and Tracking Challenge Results', 'Drone', 'Benchmark', 'Object detection in videos', 'Multi-object tracking', '', 'Drones equipped with cameras have been fast deployed to a wide range of applications, such as agriculture, aerial photography, fast delivery, and surveillance. As the core steps in those applications, video object detection and tracking attracts much research effort in recent years. However, the current video object detection and tracking algorithms are not usually optimal for dealing with video sequences captured by drones, due to various challenges, such as viewpoint change and scales. To promote and track the development of the detection and tracking algorithms with drones, we organized the Vision Meets Drone Video Detection and Tracking (VisDrone-VDT2018) challenge, which is a subtrack of the Vision Meets Drone 2018 challenge workshop in conjunctiohe 15th European Conference on Computer Vision (ECCV 2018). Specifically, this workshop challenge consists of two tasks, (1) video object detection, and (2) multi-object tracking. We present a large-scale video object detection and tracking dataset, which consists of 79 video clips with about 1.5 million annotated bounding boxes in 33, 366 frames. We also provide rich annotations, including object categories, occlusion, and truncation ratios for better data usage. Being the largest such dataset ever published, the challenge enables extensive evaluation, investigation and tracking the progress of object detection and tracking algorithms on the drone platform. We present the evaluation protocol of the VisDrone-VDT2018 challenge and the results of the algorithms on the benchmark dataset, which are publicly available on the challenge website: http://www.aiskyeye.com/. We hope the challenge largely boost the research and development in related fields.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11021-5_29');
INSERT INTO `paper` VALUES (11370, 'Vision Augmented Robot Feeding', 'Assistive technologies', 'Manipulation aids', 'Computer vision', 'Feeding assistance', '', 'Researchers have over time developed robotic feeding assistants to help at meals so that people with disabilities can live more autonomous lives. Current commercial feeding assistant robots acquire food without feedback on acquisition success and move to a preprogrammed location to deliver the food. In this work, we evaluate how vision can be used to improve both food acquisition and delivery. We show that using visual feedback on whether food was captured increases food acquisition efficiency. We also show how Discriminative Optimization (DO) can be used in tracking so that the food can be effectively brought all the way to the user’s mouth, rather than to a preprogrammed feeding location.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_4');
INSERT INTO `paper` VALUES (11371, 'Visual and Quantitative Comparison of Real and Simulated Biomedical Image Data', 'Feature comparison', 'Validation of simulation', 'Statistical evaluation', 'Similarity visualisation', '', 'The simulations in biomedical image analysis provide a solution when the real image data are difficult to be annotated or if they are available only in small quantities. The progress in simulations rapidly grows in the recent years. Nevertheless, the comparative techniques for the assessment of the plausibility of generated data are still unsatisfactory or none. This paper aims to point out the problem of insufficient comparison of real and synthetic data, which is done in many cases only by visual inspection or based on subjective measurements. The selected texture features are first compared in a univariate manner by quantile-quantile plots and Kolmogorov-Smirnov test. The evaluation is then extended into multivariate assessment using the PCA for a visualization and furthermore for a quantitative measure of similarity by Jaccard index. Two different image datasets were used to show the results and the importance of the validation of simulated data in many aspects.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_28');
INSERT INTO `paper` VALUES (11372, 'Visual Coreference Resolution in Visual Dialog Using Neural Module Networks', '', '', '', '', '', 'Visual dialog entails answering a series of questions grounded in an image, using dialog history as context. In addition to the challenges found in visual question answering (VQA), which can be seen as one-round dialog, visual dialog encompasses several more. We focus on one such problem called visual coreference resolution that involves determining which words, typically noun phrases and pronouns, co-refer to the same entity/object instance in an image. This is crucial, especially for pronouns (e.g., ‘it’), as the dialog agent must first link it to a previous coreference (e.g., ‘boat’), and only then can rely on the visual grounding of the coreference ‘boat’ to reason about the pronoun ‘it’. Prior work (in visual dialog) models visual coreference resolution either (a) implicitly via a memory network over history, or (b) at a coarse level for the entire question; and not explicitly at a phrase level of granularity. In this work, we propose a neural module network architecture for visual dialog by introducing two novel modules—Refer and Exclude—that perform explicit, grounded, coreference resolution at a finer word level. We demonstrate the effectiveness of our model on MNIST Dialog, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on VisDial, a large and challenging visual dialog dataset on real images, where our model outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_10');
INSERT INTO `paper` VALUES (11373, 'Visual Psychophysics for Making Face Recognition Algorithms More Explainable', 'Face recognition', 'Biometrics', 'Explainable AI', 'Visual psychophysics', 'Biometric menagerie', 'Scientific fields that are interested in faces have developed their own sets of concepts and procedures for understanding how a target model system (be it a person or algorithm) perceives a face under varying conditions. In computer vision, this has largely been in the form of dataset evaluation for recognition tasks where summary statistics are used to measure progress. While aggregate performance has continued to improve, understanding individual causes of failure has been difficult, as it is not always clear why a particular face fails to be recognized, or why an impostor is recognized by an algorithm. Importantly, other fields studying vision have addressed this via the use of visual psychophysics: the controlled manipulation of stimuli and careful study of the responses they evoke in a model system. In this paper, we suggest that visual psychophysics is a viable methodology for making face recognition algorithms more explainable. A comprehensive set of procedures is developed for assessing face recognition algorithm behavior, which is then deployed over state-of-the-art convolutional neural networks and more basic, yet still widely used, shallow and handcrafted feature-based approaches.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_16');
INSERT INTO `paper` VALUES (11374, 'Visual Question Answering as a Meta Learning Task', '', '', '', '', '', 'The predominant approach to Visual Question Answering (VQA) demands that the model represents within its weights all of the information required to answer any question about any image. Learning this information from any real training set seems unlikely, and representing it in a reasonable number of weights doubly so. We propose instead to approach VQA as a meta learning task, thus separating the question answering method from the information required. At test time, the method is provided with a support set of example questions/answers, over which it reasons to resolve the given question. The support set is not fixed and can be extended without retraining, thereby expanding the capabilities of the model. To exploit this dynamically provided information, we adapt a state-of-the-art VQA model with two techniques from the recent meta learning literature, namely prototypical networks and meta networks. Experiments demonstrate the capability of the system to learn to produce completely novel answers (i.e. never seen during training) from examples provided at test time. In comparison to the existing state of the art, the proposed method produces qualitatively distinct results with higher recall of rare answers, and a better sample efficiency that allows training with little initial data. More importantly, it represents an important step towards vision-and-language methods that can learn and reason on-the-fly.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_14');
INSERT INTO `paper` VALUES (11375, 'Visual Question Generation for Class Acquisition of Unknown Objects', 'Visual question generation', 'Unknown object recognition', 'Unknown object class acquisition', 'Real world recognition', '', 'Traditional image recognition methods only consider objects belonging to already learned classes. However, since training a recognition model with every object class in the world is unfeasible, a way of getting information on unknown objects (i.e., objects whose class has not been learned) is necessary. A way for an image recognition system to learn new classes could be asking a human about objects that are unknown. In this paper, we propose a method for generating questions about unknown objects in an image, as means to get information about classes that have not been learned. Our method consists of a module for proposing objects, a module for identifying unknown objects, and a module for generating questions about unknown objects. The experimental results via human evaluation show that our method can successfully get information about unknown objects in an image dataset. Our code and dataset are available at https://github.com/mil-tokyo/vqg-unknown.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01258-8_30');
INSERT INTO `paper` VALUES (11376, 'Visual Reasoning with Multi-hop Feature Modulation', 'Deep learning', 'Computer vision', 'Multi-modal learning', 'Natural language', '', 'Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation significantly outperforms prior state-of-the-art on the GuessWhat?! visual dialogue task and matches state-of-the art on the ReferIt object retrieval task, and we provide additional qualitative analysis.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_48');
INSERT INTO `paper` VALUES (11377, 'Visual Relationship Prediction via Label Clustering and Incorporation of Depth Information', 'Relationship prediction', 'Instance segmentation', 'Semantic segmentation', 'Unsupervised clustering', 'Depth information', 'In this paper, we investigate the use of an unsupervised label clustering technique and demonstrate that it enables substantial improvements in visual relationship prediction accuracy on the Person in Context (PIC) dataset. We propose to group object labels with similar patterns of relationship distribution in the dataset into fewer categories. Label clustering not only mitigates both the large classification space and class imbalance issues, but also potentially increases data samples for each clustered category. We further propose to incorporate depth information as an additional feature into the instance segmentation model. The additional depth prediction path supplements the relationship prediction model in a way that bounding boxes or segmentation masks are unable to deliver. We have rigorously evaluated the proposed techniques and performed various ablation analysis to validate the benefits of them.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_43');
INSERT INTO `paper` VALUES (11378, 'Visual Text Correction', '', '', '', '', '', 'This paper introduces a new problem, called Visual Text Correction (VTC), i.e., finding and replacing an inaccurate word in the textual description of a video. We propose a deep network that can simultaneously detect an inaccuracy in a sentence, and fix it by replacing the inaccurate word(s). Our method leverages the semantic interdependence of videos and words, as well as the short-term and long-term relations of the words in a sentence. Our proposed formulation can solve the VTC problem employing an End-to-End network in two steps: (1) Inaccuracy detection, and (2) correct word prediction. In detection step, each word of a sentence is reconstructed such that the reconstruction for the inaccurate word is maximized. We exploit both Short Term and Long Term Dependencies employing respectively Convolutional N-Grams and LSTMs to reconstruct the word vectors. For the correction step, the basic idea is to simply substitute the word with the maximum reconstruction error for a better one. The second step is essentially a classification problem where the classes are the words in the dictionary as replacement options. Furthermore, to train and evaluate our model, we propose an approach to automatically construct a large dataset for the VTC problem. Our experiments and performance analysis demonstrates that the proposed method provides very good results and also highlights the general challenges in solving the VTC problem. To the best of our knowledge, this work is the first of its kind for the Visual Text Correction task.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_10');
INSERT INTO `paper` VALUES (11379, 'Visual Tracking via Spatially Aligned Correlation Filters Network', 'Visual tracking', 'Spatial transformer network', 'Deep learning', 'Correlation filters network', '', 'Correlation filters based trackers rely on a periodic assumption of the search sample to efficiently distinguish the target from the background. This assumption however yields undesired boundary effects and restricts aspect ratios of search samples. To handle these issues, an end-to-end deep architecture is proposed to incorporate geometric transformations into a correlation filters based network. This architecture introduces a novel spatial alignment module, which provides continuous feedback for transforming the target from the border to the center with a normalized aspect ratio. It enables correlation filters to work on well-aligned samples for better tracking. The whole architecture not only learns a generic relationship between object geometric transformations and object appearances, but also learns robust representations coupled to correlation filters in case of various geometric transformations. This lightweight architecture permits real-time speed. Experiments show our tracker effectively handles boundary effects and aspect ratio variations, achieving state-of-the-art tracking results on recent benchmarks.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_29');
INSERT INTO `paper` VALUES (11380, 'Visual-Inertial Object Detection and Mapping', 'Simultaneous Localization And Mapping (SLAM)', 'Semantic Mapping Module', 'Shape Label', 'Current Camera Frame', 'Scene Mesh', 'We present a method to populate an unknown environment with models of previously seen objects, placed in a Euclidean reference frame that is inferred causally and on-line using monocular video along with inertial sensors. The system we implement returns a sparse point cloud for the regions of the scene that are visible but not recognized as a previously seen object, and a detailed object model and its pose in the Euclidean frame otherwise. The system includes bottom-up and top-down components, whereby deep networks trained for detection provide likelihood scores for object hypotheses provided by a nonlinear filter, whose state serves as memory. Additional networks provide likelihood scores for edges, which complements detection networks trained to be invariant to small deformations. We test our algorithm on existing datasets, and also introduce the VISMA dataset, that provides ground truth pose, point-cloud map, and object models, along with time-stamped inertial measurements.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_19');
INSERT INTO `paper` VALUES (11381, 'Visual-Semantic Alignment Across Domains Using a Semi-Supervised Approach', 'Multi-modal retrieval', 'Visual-semantic embeddings', 'Semi-supervised learning', '', '', 'Visual-semantic embeddings have been extensively used as a powerful model for cross-modal retrieval of images and sentences. In this setting, data coming from different modalities can be projected in a common embedding space, in which distances can be used to infer the similarity between pairs of images and sentences. While this approach has shown impressive performances on fully supervised settings, its application to semi-supervised scenarios has been rarely investigated. In this paper we propose a domain adaptation model for cross-modal retrieval, in which the knowledge learned from a supervised dataset can be transferred on a target dataset in which the pairing between images and sentences is not known, or not useful for training due to the limited size of the set. Experiments are performed on two target unsupervised scenarios, respectively related to the fashion and cultural heritage domain. Results show that our model is able to effectively transfer the knowledge learned on ordinary visual-semantic datasets, achieving promising results. As an additional contribution, we collect and release the dataset used for the cultural heritage domain.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_47');
INSERT INTO `paper` VALUES (11382, 'Visually Indicated Sound Generation by Perceptually Optimized Classification', 'Visually indicated sound generation', 'Perceptual loss', '', '', '', 'Visually indicated sound generation aims to predict visually consistent sound from the video content. Previous methods addressed this problem by creating a single generative model that ignores the distinctive characteristics of various sound categories. Nowadays, state-of-the-art sound classification networks are available to capture semantic-level information in audio modality, which can also serve for the purpose of visually indicated sound generation. In this paper, we explore generating fine-grained sound from a variety of sound classes, and leverage pre-trained sound classification networks to improve the audio generation quality. We propose a novel Perceptually Optimized Classification based Audio generation Network (POCAN), which generates sound conditioned on the sound class predicted from visual information. Additionally, a perceptual loss is calculated via a pre-trained sound classification network to align the semantic information between the generated sound and its ground truth during training. Experiments show that POCAN achieves significantly better results in visually indicated sound generation task on two datasets.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_43');
INSERT INTO `paper` VALUES (11383, 'Volumetric Performance Capture from Minimal Camera Viewpoints', 'Multi-view reconstruction', 'Deep autoencoders', 'Visual hull', '', '', 'We present a convolutional autoencoder that enables high fidelity volumetric reconstructions of human performance to be captured from multi-view video comprising only a small set of camera views. Our method yields similar end-to-end reconstruction error to that of a probabilistic visual hull computed using significantly more (double or more) viewpoints. We use a deep prior implicitly learned by the autoencoder trained over a dataset of view-ablated multi-view video footage of a wide range of subjects and actions. This opens up the possibility of high-end volumetric performance capture in on-set and prosumer scenarios where time or cost prohibit a high witness camera count.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_35');
INSERT INTO `paper` VALUES (11384, 'VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions', 'Visual question answering', 'Model with Explanation', '', '', '', 'Most existing works in visual question answering (VQA) are dedicated to improving the accuracy of predicted answers, while disregarding the explanations. We argue that the explanation for an answer is of the same or even more importance compared with the answer itself, since it makes the question answering process more understandable and traceable. To this end, we propose a new task of VQA-E (VQA with Explanation), where the models are required to generate an explanation with the predicted answer. We first construct a new dataset, and then frame the VQA-E problem in a multi-task learning architecture. Our VQA-E dataset is automatically derived from the VQA v2 dataset by intelligently exploiting the available captions. We also conduct a user study to validate the quality of the synthesized explanations. We quantitatively show that the additional supervision from explanations can not only produce insightful textual sentences to justify the answers, but also improve the performance of answer prediction. Our model outperforms the state-of-the-art methods by a clear margin on the VQA v2 dataset.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01234-2_34');
INSERT INTO `paper` VALUES (11385, 'VSO: Visual Semantic Odometry', 'Visual odometry', 'SLAM', 'Semantic segmentation', '', '', 'Robust data association is a core problem of visual odometry, where image-to-image correspondences provide constraints for camera pose and map estimation. Current state-of-the-art direct and indirect methods use short-term tracking to obtain continuous frame-to-frame constraints, while long-term constraints are established using loop closures. In this paper, we propose a novel visual semantic odometry (VSO) framework to enable medium-term continuous tracking of points using semantics. Our proposed framework can be easily integrated into existing direct and indirect visual odometry pipelines. Experiments on challenging real-world datasets demonstrate a significant improvement over state-of-the-art baselines in the context of autonomous driving simply by integrating our semantic constraints.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_15');
INSERT INTO `paper` VALUES (11386, 'W-TALC: Weakly-Supervised Temporal Activity Localization and Classification', 'Weakly-supervised', 'Activity localization', 'Co-activity similarity loss', '', '', 'Most activity localization methods in the literature suffer from the burden of frame-wise annotation requirement. Learning from weak labels may be a potential solution towards reducing such manual labeling effort. Recent years have witnessed a substantial influx of tagged videos on the Internet, which can serve as a rich source of weakly-supervised training data. Specifically, the correlations between videos with similar tags can be utilized to temporally localize the activities. Towards this goal, we present W-TALC, a Weakly-supervised Temporal Activity Localization and Classification framework using only video-level labels. The proposed network can be divided into two sub-networks, namely the Two-Stream based feature extractor network and a weakly-supervised module, which we learn by optimizing two complimentary loss functions. Qualitative and quantitative results on two challenging datasets - Thumos14 and ActivityNet1.2, demonstrate that the proposed method is able to detect activities at a fine granularity and achieve better performance than current state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_35');
INSERT INTO `paper` VALUES (11387, 'WAEF: Weighted Aggregation with Enhancement Filter for Visual Object Tracking', 'Enhancement filter', 'Temporal regression', 'Weighted aggregation', 'Feature prioritization', 'Tikhonov regularization', 'In the recent years, convolutional neural networks (CNN) have been extensively employed in various complex computer vision tasks including visual object tracking. In this paper, we study the efficacy of temporal regression with Tikhonov regularization in generic object tracking. Among other major aspects, we propose a different approach to regress in the temporal domain, based on weighted aggregation of distinctive visual features and feature prioritization with entropy estimation in a recursive fashion. We provide a statistics based ensembler approach for integrating the conventionally driven spatial regression results (such as from ECO), and the proposed temporal regression results to accomplish better tracking. Further, we exploit the obligatory dependency of deep architectures on provided visual information, and present an image enhancement filter that helps to boost the performance on popular benchmarks. Our extensive experimentation shows that the proposed weighted aggregation with enhancement filter (WAEF) tracker outperforms the baseline (ECO) in almost all the challenging categories on OTB50 dataset with a cumulative gain of 14.8%. As per the VOT2016 evaluation, the proposed framework offers substantial improvement of 19.04% in occlusion, 27.66% in illumination change, 33.33% in empty, 10% in size change, and 5.28% in average expected overlap.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11009-3_4');
INSERT INTO `paper` VALUES (11388, 'Wasserstein Divergence for GANs', 'Wasserstein metric', 'Wasserstein divergence', 'GANs', 'Progressive growing', '', 'In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the family of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met). In this paper, we propose a novel Wasserstein divergence (W-div), which is a relaxed version of W-met and does not require the k-Lipschitz constraint. As a concrete application, we introduce a Wasserstein divergence objective for GANs (WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks, showing the superior performance of WGAN-div compared to the state-of-the-art methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_40');
INSERT INTO `paper` VALUES (11389, 'Weakly Supervised Object Detection in Artworks', 'Weakly supervised detection', 'Transfer learning', 'Art analysis', 'Multiple instance learning', '', 'We propose a method for the weakly supervised detection of objects in paintings. At training time, only image-level annotations are needed. This, combined with the efficiency of our multiple-instance learning method, enables one to learn new classes on-the-fly from globally annotated databases, avoiding the tedious task of manually marking objects. We show on several databases that dropping the instance-level annotations only yields mild performance losses. We also introduce a new database, IconArt, on which we perform detection experiments on classes that could not be learned on photographs, such as Jesus Child or Saint Sebastian. To the best of our knowledge, these are the first experiments dealing with the automatic (and in our case weakly supervised) detection of iconographic elements in paintings. We believe that such a method is of great benefit for helping art historians to explore large digital databases.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_53');
INSERT INTO `paper` VALUES (11390, 'Weakly Supervised Region Proposal Network and Object Detection', 'Object detection', 'Region proposal', 'Weakly supervised learning', 'Convolutional neural network', '', 'The Convolutional Neural Network (CNN) based region proposal generation method (i.e. region proposal network), trained using bounding box annotations, is an essential component in modern fully supervised object detectors. However, Weakly Supervised Object Detection (WSOD) has not benefited from CNN-based proposal generation due to the absence of bounding box annotations, and is relying on standard proposal generation methods such as selective search. In this paper, we propose a weakly supervised region proposal network which is trained using only image-level annotations. The weakly supervised region proposal network consists of two stages. The first stage evaluates the objectness scores of sliding window boxes by exploiting the low-level information in CNN and the second stage refines the proposals from the first stage using a region-based CNN classifier. Our proposed region proposal network is suitable for WSOD, can be plugged into a WSOD network easily, and can share its convolutional computations with the WSOD network. Experiments on the PASCAL VOC and ImageNet detection datasets show that our method achieves the state-of-the-art performance for WSOD with performance gain of about \\(3\\%\\) on average.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_22');
INSERT INTO `paper` VALUES (11391, 'Weakly- and Semi-supervised Panoptic Segmentation', 'Weak supervision', 'Instance segmentation', 'Semantic segmentation', 'Scene understanding', '', 'We present a weakly supervised model that jointly performs both semantic- and instance-segmentation – a particularly relevant problem given the substantial cost of obtaining pixel-perfect annotation for these tasks. In contrast to many popular instance segmentation approaches based on object detectors, our method does not predict any overlapping instances. Moreover, we are able to segment both “thing” and “stuff” classes, and thus explain all the pixels in the image. “Thing” classes are weakly-supervised with bounding boxes, and “stuff” with image-level tags. We obtain state-of-the-art results on Pascal VOC, for both full and weak supervision (which achieves about 95% of fully-supervised performance). Furthermore, we present the first weakly-supervised results on Cityscapes for both semantic- and instance-segmentation. Finally, we use our weakly supervised framework to analyse the relationship between annotation quality and predictive performance, which is of interest to dataset creators.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01267-0_7');
INSERT INTO `paper` VALUES (11392, 'Weakly-Supervised 3D Hand Pose Estimation from Monocular RGB Images', '3D hand pose estimation', 'Weakly-supervised methods', 'Depth regularizer', '', '', 'Compared with depth-based 3D hand pose estimation, it is more challenging to infer 3D hand pose from monocular RGB images, due to substantial depth ambiguity and the difficulty of obtaining fully-annotated training data. Different from existing learning-based monocular RGB-input approaches that require accurate 3D annotations for training, we propose to leverage the depth images that can be easily obtained from commodity RGB-D cameras during training, while during testing we take only RGB inputs for 3D joint predictions. In this way, we alleviate the burden of the costly 3D annotations in real-world dataset. Particularly, we propose a weakly-supervised method, adaptating from fully-annotated synthetic dataset to weakly-labeled real-world dataset with the aid of a depth regularizer, which generates depth maps from predicted 3D pose and serves as weak supervision for 3D pose regression. Extensive experiments on benchmark datasets validate the effectiveness of the proposed depth regularizer in both weakly-supervised and fully-supervised settings.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_41');
INSERT INTO `paper` VALUES (11393, 'Weakly-Supervised Video Summarization Using Variational Encoder-Decoder and Web Prior', 'Video summarization', 'Variational autoencoder', '', '', '', 'Video summarization is a challenging under-constrained problem because the underlying summary of a single video strongly depends on users’ subjective understandings. Data-driven approaches, such as deep neural networks, can deal with the ambiguity inherent in this task to some extent, but it is extremely expensive to acquire the temporal annotations of a large-scale video dataset. To leverage the plentiful web-crawled videos to improve the performance of video summarization, we present a generative modelling framework to learn the latent semantic video representations to bridge the benchmark data and web data. Specifically, our framework couples two important components: a variational autoencoder for learning the latent semantics from web videos, and an encoder-attention-decoder for saliency estimation of raw video and summary generation. A loss term to learn the semantic matching between the generated summaries and web videos is presented, and the overall framework is further formulated into a unified conditional variational encoder-decoder, called variational encoder-summarizer-decoder (VESD). Experiments conducted on the challenging datasets CoSum and TVSum demonstrate the superior performance of the proposed VESD to existing state-of-the-art methods. The source code of this work can be found at https://github.com/cssjcai/vesd.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_12');
INSERT INTO `paper` VALUES (11394, 'What Do I Annotate Next? An Empirical Study of Active Learning for Action Localization', 'Video understanding', 'Temporal action localization', 'Active learning', 'Video annotation', '', 'Despite tremendous progress achieved in temporal action localization, state-of-the-art methods still struggle to train accurate models when annotated data is scarce. In this paper, we introduce a novel active learning framework for temporal localization that aims to mitigate this data dependency issue. We equip our framework with active selection functions that can reuse knowledge from previously annotated datasets. We study the performance of two state-of-the-art active selection functions as well as two widely used active learning baselines. To validate the effectiveness of each one of these selection functions, we conduct simulated experiments on ActivityNet. We find that using previously acquired knowledge as a bootstrapping source is crucial for active learners aiming to localize actions. When equipped with the right selection function, our proposed framework exhibits significantly better performance than standard active learning strategies, such as uncertainty sampling. Finally, we employ our framework to augment the newly compiled Kinetics action dataset with ground-truth temporal annotations. As a result, we collect Kinetics-Localization, a novel large-scale dataset for temporal action localization, which contains more than 15K YouTube videos.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_13');
INSERT INTO `paper` VALUES (11395, 'What Was Monet Seeing While Painting? Translating Artworks to Photo-Realistic Images', '', '', '', '', '', 'State of the art Computer Vision techniques exploit the availability of large-scale datasets, most of which consist of images captured from the world as it is. This brings to an incompatibility between such methods and digital data from the artistic domain, on which current techniques under-perform. A possible solution is to reduce the domain shift at the pixel level, thus translating artistic images to realistic copies. In this paper, we present a model capable of translating paintings to photo-realistic images, trained without paired examples. The idea is to enforce a patch level similarity between real and generated images, aiming to reproduce photo-realistic details from a memory bank of real images. This is subsequently adopted in the context of an unpaired image-to-image translation framework, mapping each image from one distribution to a new one belonging to the other distribution. Qualitative and quantitative results are presented on Monet, Cezanne and Van Gogh paintings translation tasks, showing that our approach increases the realism of generated images with respect to the CycleGAN approach.', 'ECCV', '2018', '29 January 2019', 'https://doi.org/10.1007/978-3-030-11012-3_46');
INSERT INTO `paper` VALUES (11396, 'Where and What Am I Eating? Image-Based Food Menu Recognition', 'Multimodal learning', 'Computer vision', 'Food recognition', '', '', 'Food has become a very important aspect of our social activities. Since social networks and websites like Yelp appeared, their users have started uploading photos of their meals to the Internet. This phenomenon opens a whole world of possibilities for developing models for applying food analysis and recognition on huge amounts of real-world data. A clear application could consist in applying image food recognition by using the menu of the restaurants. Our model, based on Convolutional Neural Networks and Recurrent Neural Networks, is able to learn a language model that generalizes on never seen dish names without the need of re-training it. According to the Ranking Loss metric, the results obtained by the model improve the baseline by a 15%.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_45');
INSERT INTO `paper` VALUES (11397, 'Where Are the Blobs: Counting by Localization with Point Supervision', '', '', '', '', '', 'Object counting is an important task in computer vision due to its growing demand in applications such as surveillance, traffic monitoring, and counting everyday objects. State-of-the-art methods use regression-based optimization where they explicitly learn to count the objects of interest. These often perform better than detection-based methods that need to learn the more difficult task of predicting the location, size, and shape of each object. However, we propose a detection-based method that does not need to estimate the size and shape of the objects and that outperforms regression-based methods. Our contributions are three-fold: (1) we propose a novel loss function that encourages the network to output a single blob per object instance using point-level annotations only; (2) we design two methods for splitting large predicted blobs between object instances; and (3) we show that our method achieves new state-of-the-art results on several challenging datasets including the Pascal VOC and the Penguins dataset. Our method even outperforms those that use stronger supervision such as depth features, multi-point annotations, and bounding-box labels.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01216-8_34');
INSERT INTO `paper` VALUES (11398, 'Where Will They Go? Predicting Fine-Grained Adversarial Multi-agent Motion Using Conditional Variational Autoencoders', 'Forecasting', 'Motion prediction', 'Multi-agent tracking', 'Context aware prediction', 'Conditional variational autoencoders', 'Simultaneously and accurately forecasting the behavior of many interacting agents is imperative for computer vision applications to be widely deployed (e.g., autonomous vehicles, security, surveillance, sports). In this paper, we present a technique using conditional variational autoencoder which learns a model that “personalizes” prediction to individual agent behavior within a group representation. Given the volume of data available and its adversarial nature, we focus on the sport of basketball and show that our approach efficiently predicts context-specific agent motions. We find that our model generates results that are three times as accurate as previous state of the art approaches (5.74 ft vs. 17.95 ft).', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_45');
INSERT INTO `paper` VALUES (11399, 'WiCV at ECCV2018: The Fifth Women in Computer Vision Workshop', '', '', '', '', '', 'We report a summary of the Women in Computer Vision Workshop (WiCV) at ECCV 2018. WiCV focuses on creating a more inclusive environment for women researchers, a minority in the currently male dominated field of Computer Vision. In fact, despite the incredible progress of computer vision and machine learning and the growing interest towards these topics, the amount of female researchers is still limited both in academia and industry. The workshop is therefore an opportunity to promote collaborations, increase visibility and inclusion, and provide mentoring. Moreover the workshop offers a venue to discuss gender related biases still present throughout the work environments and are often not discussed with the due importance. This was the fifth WiCV workshop in its fourth year, and also the first WiCV held in Europe, in conjunction with ECCV. We have made changes in our program according to lessons learned from previous workshops and were able to obtain an unprecedented attendance exceeding the room capacity. We report a summary of statistics for presenters and attendees, followed by expectations for future iterations of the workshop.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11018-5_51');
INSERT INTO `paper` VALUES (11400, 'WildDash - Creating Hazard-Aware Benchmarks', 'Test data', 'Autonomous driving', 'Validation', 'Testing', 'Safety analysis', 'Test datasets should contain many different challenging aspects so that the robustness and real-world applicability of algorithms can be assessed. In this work, we present a new test dataset for semantic and instance segmentation for the automotive domain. We have conducted a thorough risk analysis to identify situations and aspects that can reduce the output performance for these tasks. Based on this analysis we have designed our new dataset. Meta-information is supplied to mark which individual visual hazards are present in each test case. Furthermore, a new benchmark evaluation method is presented that uses the meta-information to calculate the robustness of a given algorithm with respect to the individual hazards. We show how this new approach allows for a more expressive characterization of algorithm robustness by comparing three baseline algorithms.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01231-1_25');
INSERT INTO `paper` VALUES (11401, 'Women Also Snowboard: Overcoming Bias in Captioning Models', 'Image description', 'Caption bias', 'Right for the right reasons', '', '', 'Most machine learning methods are known to capture and exploit biases of the training data. While some biases are beneficial for learning, others are harmful. Specifically, image captioning models tend to exaggerate biases present in training data (e.g., if a word is present in 60% of training sentences, it might be predicted in 70% of sentences at test time). This can lead to incorrect captions in domains where unbiased captions are desired, or required, due to over-reliance on the learned prior and image context. In this work we investigate generation of gender-specific caption words (e.g. man, woman) based on the person’s appearance or the image context. We introduce a new Equalizer model that encourages equal gender probability when gender evidence is occluded in a scene and confident predictions when gender evidence is present. The resulting model is forced to look at a person rather than use contextual cues to make a gender-specific prediction. The losses that comprise our model, the Appearance Confusion Loss and the Confident Loss, are general, and can be added to any description model in order to mitigate impacts of unwanted bias in a description dataset. Our proposed model has lower error than prior work when describing images with people and mentioning their gender and more closely matches the ground truth ratio of sentences including women to sentences including men. Finally, we show that our model more often looks at people when predicting their gender (https://people.eecs.berkeley.edu/~lisa anne/snowboard.html).', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_47');
INSERT INTO `paper` VALUES (11402, 'Workshop on Interactive and Adaptive Learning in an Open World', 'Interactive learning', 'Adaptive learning', 'Open set', 'Continuous learning', '', 'Next generation machine learning requires stepping away from classical batch learning towards interactive and adaptive learning. This is essential to cope with demanding machine learning applications we have already today. Our workshop at ECCV 2018 in Munich therefore served as a discussion forum for experts in this field and in the following we give a brief overview. Please note that this discussion paper has not been not peer-reviewed and only contains the subjective summary of the workshop organizers.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_38');
INSERT INTO `paper` VALUES (11403, 'X-Ray Computed Tomography Through Scatter', 'CT', 'Xray', 'Inverse problem', 'Elastic/inelastic scattering', '', 'In current Xray CT scanners, tomographic reconstruction relies only on directly transmitted photons. The models used for reconstruction have regarded photons scattered by the body as noise or disturbance to be disposed of, either by acquisition hardware (an anti-scatter grid) or by the reconstruction software. This increases the radiation dose delivered to the patient. Treating these scattered photons as a source of information, we solve an inverse problem based on a 3D radiative transfer model that includes both elastic (Rayleigh) and inelastic (Compton) scattering. We further present ways to make the solution numerically efficient. The resulting tomographic reconstruction is more accurate than traditional CT, while enabling significant dose reduction and chemical decomposition. Demonstrations include both simulations based on a standard medical phantom and a real scattering tomography experiment.', 'ECCV', '2018', '09 October 2018', 'https://doi.org/10.1007/978-3-030-01264-9_3');
INSERT INTO `paper` VALUES (11404, 'X2Face: A Network for Controlling Face Generation Using Images, Audio, and Pose Codes', 'Pose Code', 'Source Face', 'Source Frame', 'Head Pose Angles', 'Leg Cycling', 'The objective of this paper is a neural network model that controls the pose and expression of a given face, using another face or modality (e.g. audio). This model can then be used for lightweight, sophisticated video and image editing.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01261-8_41');
INSERT INTO `paper` VALUES (11405, 'YOLO3D: End-to-End Real-Time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud', '3D object detection', 'LiDAR', 'Real-time', '', '', 'Object detection and classification in 3D is a key task in Automated Driving (AD). LiDAR sensors are employed to provide the 3D point cloud reconstruction of the surrounding environment, while the task of 3D object bounding box detection in real time remains a strong algorithmic challenge. In this paper, we build on the success of the one-shot regression meta-architecture in the 2D perspective image space and extend it to generate oriented 3D object bounding boxes from LiDAR point cloud. Our main contribution is in extending the loss function of YOLO v2 to include the yaw angle, the 3D box center in Cartesian coordinates and the height of the box as a direct regression problem. This formulation enables real-time performance, which is essential for automated driving. Our results are showing promising figures on KITTI benchmark, achieving real-time performance (40 fps) on Titan X GPU.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11015-4_54');
INSERT INTO `paper` VALUES (11406, 'YouTube-VOS: Sequence-to-Sequence Video Object Segmentation', 'Video object segmentation', 'Large-scale dataset', 'Spatial-temporal information', '', '', 'Learning long-term spatial-temporal features are critical for many video analysis tasks. However, existing video segmentation methods predominantly rely on static image segmentation techniques, and methods capturing temporal dependency for segmentation have to depend on pretrained optical flow models, leading to suboptimal solutions for the problem. End-to-end sequential learning to explore spatial-temporal features for video segmentation is largely limited by the scale of available video segmentation datasets, i.e., even the largest video segmentation dataset only contains 90 short video clips. To solve this problem, we build a new large-scale video object segmentation dataset called YouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains 3,252 YouTube video clips and 78 categories including common objects and human activities (This is the statistics when we submit this paper, see updated statistics on our website). This is by far the largest video object segmentation dataset to our knowledge and we have released it at https://youtube-vos.org. Based on this dataset, we propose a novel sequence-to-sequence network to fully exploit long-term spatial-temporal information in videos for segmentation. We demonstrate that our method is able to achieve the best results on our YouTube-VOS test set and comparable results on DAVIS 2016 compared to the current state-of-the-art methods. Experiments show that the large scale dataset is indeed a key factor to the success of our model.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01228-1_36');
INSERT INTO `paper` VALUES (11407, 'Zero-Annotation Object Detection with Web Knowledge Transfer', 'Object detection', 'Domain adaptation', 'Web knowledge transfer', '', '', 'Object detection is one of the major problems in computer vision, and has been extensively studied. Most of the existing detection works rely on labor-intensive supervision, such as ground truth bounding boxes of objects or at least image-level annotations. On the contrary, we propose an object detection method that does not require any form of human annotation on target tasks, by exploiting freely available web images. In order to facilitate effective knowledge transfer from web images, we introduce a multi-instance multi-label domain adaption learning framework with two key innovations. First of all, we propose an instance-level adversarial domain adaptation network with attention on foreground objects to transfer the object appearances from web domain to target domain. Second, to preserve the class-specific semantic structure of transferred object features, we propose a simultaneous transfer mechanism to transfer the supervision across domains through pseudo strong label generation. With our end-to-end framework that simultaneously learns a weakly supervised detector and transfers knowledge across domains, we achieved significant improvements over baseline methods on the benchmark datasets.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_23');
INSERT INTO `paper` VALUES (11408, 'Zero-Shot Deep Domain Adaptation', 'Zero-shot', 'Domain adaptation', 'Sensor fusion', '', '', 'Domain adaptation is an important tool to transfer knowledge about a task (e.g. classification) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classification tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no task-relevant target-domain data. The underlying principle is not particular to computer vision data, but should be extensible to other domains.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01252-6_47');
INSERT INTO `paper` VALUES (11409, 'Zero-Shot Keyword Spotting for Visual Speech Recognition In-the-wild', 'Visual keyword spotting', 'Visual speech recognition', 'Zero-shot learning', '', '', 'Visual keyword spotting (KWS) is the problem of estimating whether a text query occurs in a given recording using only video information. This paper focuses on visual KWS for words unseen during training, a real-world, practical setting which so far has received no attention by the community. To this end, we devise an end-to-end architecture comprising (a) a state-of-the-art visual feature extractor based on spatiotemporal Residual Networks, (b) a grapheme-to-phoneme model based on sequence-to-sequence neural networks, and (c) a stack of recurrent neural networks which learn how to correlate visual features with the keyword representation. Different to prior works on KWS, which try to learn word representations merely from sequences of graphemes (i.e. letters), we propose the use of a grapheme-to-phoneme encoder-decoder model which learns how to map words to their pronunciation. We demonstrate that our system obtains very promising visual-only KWS results on the challenging LRS2 database, for keywords unseen during training. We also show that our system outperforms a baseline which addresses KWS via automatic speech recognition (ASR), while it drastically improves over other recently proposed ASR-free KWS methods.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01225-0_32');
INSERT INTO `paper` VALUES (11410, 'Zero-Shot Object Detection', 'Zero-shot Classification', 'Visual-semantic Embedding', 'Unseen Classes', 'Background Box', 'Ground Truth Bounding Box', 'We introduce and tackle the problem of zero-shot object detection (ZSD), which aims to detect object classes which are not observed during training. We work with a challenging set of object classes, not restricting ourselves to similar and/or fine-grained categories as in prior works on zero-shot classification. We present a principled approach by first adapting visual-semantic embeddings for ZSD. We then discuss the problems associated with selecting a background class and motivate two background-aware approaches for learning robust detectors. One of these models uses a fixed background class and the other is based on iterative latent assignments. We also outline the challenge associated with using a limited number of training classes and propose a solution based on dense sampling of the semantic label space using auxiliary data with a large number of categories. We propose novel splits of two standard detection datasets – MSCOCO and VisualGenome, and present extensive empirical results in both the traditional and generalized zero-shot settings to highlight the benefits of the proposed methods. We provide useful insights into the algorithm and conclude by posing some open questions to encourage further research.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01246-5_24');
INSERT INTO `paper` VALUES (11411, 'Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition', '', '', '', '', '', 'Recognizing visual relationships \\(\\langle \\)subject-predicate-object\\(\\rangle \\) among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our final Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the effectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 \\(11.42\\%\\) from \\(8.16\\%\\)) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.', 'ECCV', '2018', '07 October 2018', 'https://doi.org/10.1007/978-3-030-01219-9_20');
INSERT INTO `paper` VALUES (11412, '“Factual” or “Emotional”: Stylized Image Captioning with Adaptive Learning and Attention', 'Stylized image captioning', 'Adaptive learning', 'Attention model', '', '', 'Generating stylized captions for an image is an emerging topic in image captioning. Given an image as input, it requires the system to generate a caption that has a specific style (e.g., humorous, romantic, positive, and negative) while describing the image content semantically accurately. In this paper, we propose a novel stylized image captioning model that effectively takes both requirements into consideration. To this end, we first devise a new variant of LSTM, named style-factual LSTM, as the building block of our model. It uses two groups of matrices to capture the factual and stylized knowledge, respectively, and automatically learns the word-level weights of the two groups based on previous context. In addition, when we train the model to capture stylized elements, we propose an adaptive learning approach based on a reference factual model, it provides factual knowledge to the model as the model learns from stylized caption labels, and can adaptively compute how much information to supply at each time step. We evaluate our model on two stylized image captioning datasets, which contain humorous/romantic captions and positive/negative captions, respectively. Experiments shows that our proposed model outperforms the state-of-the-art approaches, without using extra ground truth supervision.', 'ECCV', '2018', '06 October 2018', 'https://doi.org/10.1007/978-3-030-01249-6_32');
INSERT INTO `paper` VALUES (11413, '“What Is Optical Flow For?”: Workshop Results and Summary', '', '', '', '', '', 'Traditionally, computer vision problems have been classified into three levels: low (image to image), middle (image to features), and high (features to analysis) [11]. Some typical low-level vision problems include optical flow [7], stereo [10] and intrinsic image decomposition [1]. The solution to these problems would then be combined to solve higher level problems, such as action recognition and visual question answering.', 'ECCV', '2018', '23 January 2019', 'https://doi.org/10.1007/978-3-030-11024-6_56');
INSERT INTO `paper` VALUES (11414, 'A Broader Study of Cross-Domain Few-Shot Learning', 'Cross-domain', 'Few-shot learning', 'Benchmark', 'Transfer learning', '', 'Recent progress on few-shot learning largely relies on annotated data for meta-learning: base classes sampled from the same domain as the novel classes. However, in many applications, collecting data for meta-learning is infeasible or impossible. This leads to the cross-domain few-shot learning problem, where there is a large shift between base and novel class domains. While investigations of the cross-domain few-shot scenario exist, these works are limited to natural images that still contain a high degree of visual similarity. No work yet exists that examines few-shot learning across different imaging methods seen in real world scenarios, such as aerial and medical imaging. In this paper, we propose the Broader Study of Cross-Domain Few-Shot Learning (BSCD-FSL) benchmark, consisting of image data from a diverse assortment of image acquisition methods. This includes natural images, such as crop disease images, but additionally those that present with an increasing dissimilarity to natural images, such as satellite images, dermatology images, and radiology images. Extensive experiments on the proposed benchmark are performed to evaluate state-of-art meta-learning approaches, transfer learning approaches, and newer methods for cross-domain few-shot learning. The results demonstrate that state-of-art meta-learning methods are surprisingly outperformed by earlier meta-learning approaches, and all meta-learning methods underperform in relation to simple fine-tuning by 12.8% average accuracy. In some cases, meta-learning even underperforms networks with random weights. Performance gains previously observed with methods specialized for cross-domain few-shot learning vanish in this more challenging benchmark. Finally, accuracy of all methods tend to correlate with dataset similarity to natural images, verifying the value of the benchmark to better represent the diversity of data seen in practice and guiding future research. Code for the experiments in this work can be found at https://github.com/IBM/cdfsl-benchmark.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_8');
INSERT INTO `paper` VALUES (11415, 'A Closer Look at Generalisation in RAVEN', 'Visual reasoning', 'Representation learning', 'Scene understanding', 'Raven’s Progressive Matrices', '', 'Humans have a remarkable capacity to draw parallels between concepts, generalising their experience to new domains. This skill is essential to solving the visual problems featured in the RAVEN and PGM datasets, yet, previous papers have scarcely tested how well models generalise across tasks. Additionally, we encounter a critical issue that allows existing models to inadvertently ‘cheat’ problems in RAVEN. We therefore propose a simple workaround to resolve this issue, and focus the conversation on generalisation performance, as this was severely affected in the process. We revise the existing evaluation, and introduce two relational models, Rel-Base and Rel-AIR, that significantly improve this performance. To our knowledge, Rel-AIR is the first method to employ unsupervised scene decomposition in solving abstract visual reasoning problems, and along with Rel-Base, sets states-of-the-art for image-only reasoning and generalisation across both RAVEN and PGM.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_36');
INSERT INTO `paper` VALUES (11416, 'A Closer Look at Local Aggregation Operators in Point Cloud Analysis', '3D point cloud', 'Local aggregation operator', 'Position pooling', '', '', 'Recent advances of network architecture for point cloud processing are mainly driven by new designs of local aggregation operators. However, the impact of these operators to network performance is not carefully investigated due to different overall network architecture and implementation details in each solution. Meanwhile, most of operators are only applied in shallow architectures. In this paper, we revisit the representative local aggregation operators and study their performance using the same deep residual architecture. Our investigation reveals that despite the different designs of these operators, all of these operators make surprisingly similar contributions to the network performance under the same network input and feature numbers and result in the state-of-the-art accuracy on standard benchmarks. This finding stimulate us to rethink the necessity of sophisticated design of local aggregation operator for point cloud processing. To this end, we propose a simple local aggregation operator without learnable weights, named Position Pooling (PosPool), which performs similarly or slightly better than existing sophisticated operators. In particular, a simple deep residual network with PosPool layers achieves outstanding performance on all benchmarks, which outperforms the previous state-of-the methods on the challenging PartNet datasets by a large margin (7.4 mIoU). The code is publicly available at https://github.com/zeliu98/CloserLook3D.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_20');
INSERT INTO `paper` VALUES (11417, 'A Closest Point Proposal for MCMC-based Probabilistic Surface Registration', 'Probabilistic registration', 'Gaussian Process Morphable Model', 'Metropolis-hastings proposal', 'Point distribution model', '', 'We propose to view non-rigid surface registration as a probabilistic inference problem. Given a target surface, we estimate the posterior distribution of surface registrations. We demonstrate how the posterior distribution can be used to build shape models that generalize better and show how to visualize the uncertainty in the established correspondence. Furthermore, in a reconstruction task, we show how to estimate the posterior distribution of missing data without assuming a fixed point-to-point correspondence. We introduce the closest-point proposal for the Metropolis-Hastings algorithm. Our proposal overcomes the limitation of slow convergence compared to a random-walk strategy. As the algorithm decouples inference from modeling the posterior using a propose-and-verify scheme, we show how to choose different distance measures for the likelihood model. All presented results are fully reproducible using publicly available data and our open-source implementation of the registration framework.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_17');
INSERT INTO `paper` VALUES (11418, 'A Competence-Aware Curriculum for Visual Concepts Learning via Question Answering', 'Visual question answering', 'Visual concept learning', 'Curriculum learning', 'Model competence', '', 'Humans can progressively learn visual concepts from easy to hard questions. To mimic this efficient learning ability, we propose a competence-aware curriculum for visual concept learning in a question-answering manner. Specifically, we design a neural-symbolic concept learner for learning the visual concepts and a multi-dimensional Item Response Theory (mIRT) model for guiding the learning process with an adaptive curriculum. The mIRT effectively estimates the concept difficulty and the model competence at each learning step from accumulated model responses. The estimated concept difficulty and model competence are further utilized to select the most profitable training samples. Experimental results on CLEVR show that with a competence-aware curriculum, the proposed method achieves state-of-the-art performances with superior data efficiency and convergence speed. Specifically, the proposed model only uses 40% of training data and converges three times faster compared with other state-of-the-art methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_9');
INSERT INTO `paper` VALUES (11419, 'A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation', '', '', '', '', '', 'Graph convolutional networks (GCNs) have been applied to 3D human pose estimation (HPE) from 2D body joint detections and have shown encouraging performance. One limitation of the vanilla graph convolution is that it models the relationships between neighboring nodes via a shared weight matrix. This is suboptimal for articulated body modeling as the relations between different body joints are different. The objective of this paper is to have a comprehensive and systematic study of weight sharing in GCNs for 3D HPE. We first show there are two different ways to interpret a GCN depending on whether feature transformation occurs before or after feature aggregation. These two interpretations lead to five different weight sharing methods, and three more variants can be derived by decoupling the self-connections with other edges. We conduct extensive ablation study on these weight sharing methods under controlled settings and obtain new conclusions that will benefit the community.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_19');
INSERT INTO `paper` VALUES (11420, 'A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem', 'Perspective-n-point problem', 'Pose estimation', 'Non-linear quadratic program', 'Sequential quadratic programming', 'Global optimality', 'An approach for estimating the pose of a camera given a set of 3D points and their corresponding 2D image projections is presented. It formulates the problem as a non-linear quadratic program and identifies regions in the parameter space that contain unique minima with guarantees that at least one of them will be the global minimum. Each regional minimum is computed with a sequential quadratic programming scheme. These premises result in an algorithm that always determines the global minima of the perspective-n-point problem for any number of input correspondences, regardless of possible coplanar arrangements of the imaged 3D points. For its implementation, the algorithm merely requires ordinary operations available in any standard off-the-shelf linear algebra library. Comparative evaluation demonstrates that the algorithm achieves state-of-the-art results at a consistently low computational cost.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_28');
INSERT INTO `paper` VALUES (11421, 'A Cordial Sync: Going Beyond Marginal Policies for Multi-agent Embodied Tasks', 'Embodied agents', 'Multi-agent reinforcement learning', 'Collaboration', 'Emergent communication', 'AI2-THOR', 'Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task’s difficulty outpaces a single agent’s abilities. While multi-agent collaboration research has flourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 % points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at https://unnat.github.io/cordial-sync.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_28');
INSERT INTO `paper` VALUES (11422, 'A Dataset and Baselines for Visual Question Answering on Art', 'Visual question answering', 'Art dataset', 'External knowledge', '', '', 'Answering questions related to art pieces (paintings) is a difficult task, as it implies the understanding of not only the visual information that is shown in the picture, but also the contextual knowledge that is acquired through the study of the history of art. In this work, we introduce our first attempt towards building a new dataset, coined AQUA (Art QUestion Answering). The question-answer (QA) pairs are automatically generated using state-of-the-art question generation methods based on paintings and comments provided in an existing art understanding dataset. The QA pairs are cleansed by crowdsourcing workers with respect to their grammatical correctness, answerability, and answers’ correctness. Our dataset inherently consists of visual (painting-based) and knowledge (comment-based) questions. We also present a two-branch model as baseline, where the visual and knowledge questions are handled independently. We extensively compare our baseline model against the state-of-the-art models for question answering, and we provide a comprehensive study about the challenges and potential future directions for visual question answering on art.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_8');
INSERT INTO `paper` VALUES (11423, 'A Decoupled Learning Scheme for Real-World Burst Denoising from Raw Images', 'Burst denoising', 'Real-world image denoising', 'Convolutional neural networks', 'Decoupled learning', '', 'The recently developed burst denoising approach, which reduces noise by using multiple frames captured in a short time, has demonstrated much better denoising performance than its single-frame counterparts. However, existing learning based burst denoising methods are limited by two factors. On one hand, most of the models are trained on video sequences with synthetic noise. When applied to real-world raw image sequences, visual artifacts often appear due to the different noise statistics. On the other hand, there lacks a real-world burst denoising benchmark of dynamic scenes because the generation of clean ground-truth is very difficult due to the presence of object motions. In this paper, a novel multi-frame CNN model is carefully designed, which decouples the learning of motion from the learning of noise statistics. Consequently, an alternating learning algorithm is developed to learn how to align adjacent frames from a synthetic noisy video dataset, and learn to adapt to the raw noise statistics from real-world noisy datasets of static scenes. Finally, the trained model can be applied to real-world dynamic sequences for burst denoising. Extensive experiments on both synthetic video datasets and real-world dynamic sequences demonstrate the leading burst denoising performance of our proposed method.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_10');
INSERT INTO `paper` VALUES (11424, 'A Deep Dive into Adversarial Robustness in Zero-Shot Learning', '', '', '', '', '', 'Machine learning (ML) systems have introduced significant advances in various fields, due to the introduction of highly complex models. Despite their success, it has been shown multiple times that machine learning models are prone to imperceptible perturbations that can severely degrade their accuracy. So far, existing studies have primarily focused on models where supervision across all classes were available. In contrast, Zero-shot Learning (ZSL) and Generalized Zero-shot Learning (GZSL) tasks inherently lack supervision across all classes. In this paper, we present a study aimed on evaluating the adversarial robustness of ZSL and GZSL models. We leverage the well-established label embedding model and subject it to a set of established adversarial attacks and defenses across multiple datasets. In addition to creating possibly the first benchmark on adversarial robustness of ZSL models, we also present analyses on important points that require attention for better interpretation of ZSL robustness results. We hope these points, along with the benchmark, will help researchers establish a better understanding what challenges lie ahead and help guide their work.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_1');
INSERT INTO `paper` VALUES (11425, 'A Deep Learning Filter for Visual Drone Single Object Tracking', 'Geometric transformation', 'Context information', 'Visual drone tracking', '', '', 'Object tracking is one of the most important topics in computer vision. In visual drone tracking, it is an extremely challenging due to various factors, such as camera motion, partial occlusion, and full occlusion. In this paper, we propose a deep learning filter method to relieve the above problems, which is to obtain a priori position of the object at the subsequent frame and predict its trajectory to follow up the object during occlusion. Our tracker adopts the geometric transformation of the surrounding of the object to prevent the bounding box of the object lost, and it uses context information to integrate its motion trend thereby tracking the object successfully when it reappears. Experiments on the VisDrone-SOT2018 test dataset and the VisDrone-SOT2020 val dataset illustrate the effectiveness of the proposed approach.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_38');
INSERT INTO `paper` VALUES (11426, 'A Differentiable Recurrent Surface for Asynchronous Event-Based Data', 'Event-based vision', 'Representation learning', 'LSTM', 'Classification', 'Optical flow', 'Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence of pixels subject to brightness changes. Differently from classic vision devices, they produce a sparse representation of the scene. Therefore, to apply standard computer vision algorithms, events need to be integrated into a frame or event-surface. This is usually attained through hand-crafted grids that reconstruct the frame using ad-hoc heuristics. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn end-to-end task-dependent event-surfaces. Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_9');
INSERT INTO `paper` VALUES (11427, 'A Dual Residual Network with Channel Attention for Image Restoration', 'Image restoration', 'Channel attention', 'Dual residual connection', 'Multi-level connection', '', 'Deep learning models have achieved significant performance on image restoration task. However, restoring the images with complicated and combined degradation types still remains a challenge. For this purpose, we proposed a dual residual network with channel attention (DRANet) to address complicated degradation in the real world. We further exploit the potential of encoder-decoder structure. To fuse feature more efficiently, we adopt the channel attention module with skip-connections. To better process low- and high-level information, we introduce the dual residual connection into the network architecture. And we explore the effect of multi-level connection to image restoration. Experimental results demonstrate the superiority of our proposed approach over state-of-the-art methods on the UDC T-OLED dataset.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_27');
INSERT INTO `paper` VALUES (11428, 'A Flexible Recurrent Residual Pyramid Network for Video Frame Interpolation', 'Video frame interpolation', 'Customizable pyramid network', 'Arbitrary resolution and scenes', 'Adjustable calculation', '', 'Video frame interpolation (VFI) aims at synthesizing new video frames in-between existing frames to generate smoother high frame rate videos. Current methods usually use the fixed pre-trained networks to generate interpolated-frames for different resolutions and scenes. However, the fixed pre-trained networks are difficult to be tailored for a variety of cases. Inspired by classical pyramid energy minimization optical flow algorithms, this paper proposes a recurrent residual pyramid network (RRPN) for video frame interpolation. In the proposed network, different pyramid levels share the same weights and base-network, named recurrent residual layer (RRL). In RRL, residual displacements between warped images are detected to gradually refine optical flows rather than directly predict the flows or frames. Owing to the flexible recurrent residual pyramid architecture, we can customize the number of pyramid levels, and make trade-offs between calculations and quality based on the application scenarios. Moreover, occlusion masks are also generated in this recurrent residual way to solve occlusion better. Finally, a refinement network is added to enhance the details for final output with contextual and edge information. Experimental results demonstrate that the RRPN is more flexible and efficient than current VFI networks but has fewer parameters. In particular, the RRPN, which avoid over-reliance on datasets and network structures, shows superior performance for large motion cases.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_29');
INSERT INTO `paper` VALUES (11429, 'A Flow Base Bi-path Network for Cross-Scene Video Crowd Understanding in Aerial View', 'Crowd counting', 'Optical flow', 'Data augmentation', 'Synthetic data', '', 'Drones shooting can be applied in dynamic traffic monitoring, object detecting and tracking, and other vision tasks. The variability of the shooting location adds some intractable challenges to these missions, such as varying scale, unstable exposure, and scene migration. In this paper, we strive to tackle the above challenges and automatically understand the crowd from the visual data collected from drones. First, to alleviate the background noise generated in cross-scene testing, a double-stream crowd counting model is proposed, which extracts optical flow and frame difference information as an additional branch. Besides, to improve the model’s generalization ability at different scales and time, we randomly combine a variety of data transformation methods to simulate some unseen environments. To tackle the crowd density estimation problem under extreme dark environments, we introduce synthetic data generated by game Grand Theft Auto V(GTAV). Experiment results show the effectiveness of the virtual data. Our method wins the challenge with a mean absolute error (MAE) of 12.70\\(^1\\). Moreover, a comprehensive ablation study is conducted to explore each component’s contribution.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_34');
INSERT INTO `paper` VALUES (11430, 'A Generalization of Otsu’s Method and Minimum Error Thresholding', '', '', '', '', '', 'We present Generalized Histogram Thresholding (GHT), a simple, fast, and effective technique for histogram-based image thresholding. GHT works by performing approximate maximum a posteriori estimation of a mixture of Gaussians with appropriate priors. We demonstrate that GHT subsumes three classic thresholding techniques as special cases: Otsu’s method, Minimum Error Thresholding (MET), and weighted percentile thresholding. GHT thereby enables the continuous interpolation between those three algorithms, which allows thresholding accuracy to be improved significantly. GHT also provides a clarifying interpretation of the common practice of coarsening a histogram’s bin width during thresholding. We show that GHT outperforms or matches the performance of all algorithms on a recent challenge for handwritten document image binarization (including deep neural networks trained to produce per-pixel binarizations), and can be implemented in a dozen lines of code or as a trivial modification to Otsu’s method or MET.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_27');
INSERT INTO `paper` VALUES (11431, 'A Generic Graph-Based Neural Architecture Encoding Scheme for Predictor-Based NAS', 'Neural architecture search (NAS)', 'Predictor-based NAS', '', '', '', 'This work proposes a novel Graph-based neural ArchiTecture Encoding Scheme, a.k.a. GATES, to improve the predictor-based neural architecture search. Specifically, different from existing graph-based schemes, GATES models the operations as the transformation of the propagating information, which mimics the actual data processing of neural architecture. GATES is a more reasonable modeling of the neural architectures, and can encode architectures from both the “operation on node” and “operation on edge” cell search spaces consistently. Experimental results on various search spaces confirm GATES’s effectiveness in improving the performance predictor. Furthermore, equipped with the improved performance predictor, the sample efficiency of the predictor-based neural architecture search (NAS) flow is boosted.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_12');
INSERT INTO `paper` VALUES (11432, 'A Generic Visualization Approach for Convolutional Neural Networks', '', '', '', '', '', 'Retrieval networks are essential for searching and indexing. Compared to classification networks, attention visualization for retrieval networks is hardly studied. We formulate attention visualization as a constrained optimization problem. We leverage the unit L2-Norm constraint as an attention filter (L2-CAF) to localize attention in both classification and retrieval networks. Unlike recent literature, our approach requires neither architectural changes nor fine-tuning. Thus, a pre-trained network’s performance is never undermined.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_43');
INSERT INTO `paper` VALUES (11433, 'A Hybrid Approach for 6DoF Pose Estimation', '', '', '', '', '', 'We propose a method for 6DoF pose estimation of rigid objects that uses a state-of-the-art deep learning based instance detector to segment object instances in an RGB image, followed by a point-pair based voting method to recover the object’s pose. We additionally use an automatic method selection that chooses the instance detector and the training set as that with the highest performance on the validation set. This hybrid approach leverages the best of learning and classic approaches, using CNNs to filter highly unstructured data and cut through the clutter, and a local geometric approach with proven convergence for robust pose estimation. The method is evaluated on the BOP core datasets where it significantly exceeds the baseline method and is the best fast method in the BOP 2020 Challenge.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_46');
INSERT INTO `paper` VALUES (11434, 'A Large-Scale Annotated Mechanical Components Benchmark for Classification and Retrieval Tasks with Deep Neural Networks', 'Deep learning', 'Mechanical components', 'Benchmark', '3D objects', 'Classification', 'We introduce a large-scale annotated mechanical components benchmark for classification and retrieval tasks named Mechanical Components Benchmark (MCB): a large-scale dataset of 3D objects of mechanical components. The dataset enables data-driven feature learning for mechanical components. Exploring the shape descriptor for mechanical components is essential to computer vision and manufacturing applications. However, not much attention has been given on creating annotated mechanical components datasets on a large scale. This is because acquiring 3D models is challenging and annotating mechanical components requires engineering knowledge. Our main contributions are the creation of a large-scale annotated mechanical component benchmark, defining hierarchy taxonomy of mechanical components, and benchmarking the effectiveness of deep learning shape classifiers on the mechanical components. We created an annotated dataset and benchmarked seven state-of-the-art deep learning classification methods in three categories, namely: (1) point clouds, (2) volumetric representation in voxel grids, and (3) view-based representation.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_11');
INSERT INTO `paper` VALUES (11435, 'A Machine Learning Approach to Assess Student Group Collaboration Using Individual Level Behavioral Cues', 'Education', 'Collaboration assessment', 'Limited training data', 'Class imbalance', 'Mixup data augmentation', 'K-12 classrooms consistently integrate collaboration as part of their learning experiences. However, owing to large classroom sizes, teachers do not have the time to properly assess each student and give them feedback. In this paper we propose using simple deep-learning-based machine learning models to automatically determine the overall collaboration quality of a group based on annotations of individual roles and individual level behavior of all the students in the group. We come across the following challenges when building these models: (1) Limited training data, (2) Severe class label imbalance. We address these challenges by using a controlled variant of Mixup data augmentation, a method for generating additional data samples by linearly combining different pairs of data samples and their corresponding class labels. Additionally, the label space for our problem exhibits an ordered structure. We take advantage of this fact and also explore using an ordinal-cross-entropy loss function and study its effects with and without Mixup.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_8');
INSERT INTO `paper` VALUES (11436, 'A Metric Learning Reality Check', 'Deep metric learning', '', '', '', '', 'Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best. Code is available at github.com/KevinMusgrave/powerful-benchmarker.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_41');
INSERT INTO `paper` VALUES (11437, 'A Multi-modal Machine Learning Approach and Toolkit to Automate Recognition of Early Stages of Dementia Among British Sign Language Users', 'Hand tracking', 'Facial analysis', 'Convolutional neural network', 'Machine learning', 'Sign language', 'The ageing population trend is correlated with an increased prevalence of acquired cognitive impairments such as dementia. Although there is no cure for dementia, a timely diagnosis helps in obtaining necessary support and appropriate medication. Researchers are working urgently to develop effective technological tools that can help doctors undertake early identification of cognitive disorder. In particular, screening for dementia in ageing Deaf signers of British Sign Language (BSL) poses additional challenges as the diagnostic process is bound up with conditions such as quality and availability of interpreters, as well as appropriate questionnaires and cognitive tests. On the other hand, deep learning based approaches for image and video analysis and understanding are promising, particularly the adoption of Convolutional Neural Network (CNN), which require large amounts of training data. In this paper, however, we demonstrate novelty in the following way: a) a multi-modal machine learning based automatic recognition toolkit for early stages of dementia among BSL users in that features from several parts of the body contributing to the sign envelope, e.g., hand-arm movements and facial expressions, are combined, b) universality in that it is possible to apply our technique to users of any sign language, since it is language independent, c) given the trade-off between complexity and accuracy of machine learning (ML) prediction models as well as the limited amount of training and testing data being available, we show that our approach is not over-fitted and has the potential to scale up.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_20');
INSERT INTO `paper` VALUES (11438, 'A Novel Line Integral Transform for 2D Affine-Invariant Shape Retrieval', 'Shape analysis', 'Affine distortions', 'Affine invariants', 'Radon transform', 'Trace transform', 'Radon transform is a popular mathematical tool for shape analysis. However, it cannot handle affine deformation. Although its extended version, trace transform, allow us to construct affine invariants, they are less informative and computational expensive due to the loss of spatial relationship between trace lines and the extensive repeated calculation of transform. To address this issue, a novel line integral transform is proposed. We first use binding line pairs that have the desirable property of affine preserving as a reference frame to rewrite the diametrical dimension parameters of the lines in a relative manner which make them independent on affine transform. Along polar angle dimension of the line parameters, a moment-based normalization is then conducted to degrade the affine transform to similarity transform which can be easily normalized by Fourier transform. The proposed transform is not only invariant to affine transform, but also preserves the spatial relationship between line integrals which make it very informative. Another advantage of the proposed transform is that it is more efficient than the trace transform. Conducting it one time can allow us to achieve a 2D matrix of affine invariants. While conducting the trace transform once only generates a single feature and multiple trace transforms of different functionals are needed to derive more to make the descriptors informative. The effectiveness of the proposed transform has been validated on two types of standard shape test cases, affinely distorted contour shape dataset and region shape dataset, respectively.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_36');
INSERT INTO `paper` VALUES (11439, 'A Plan for Developing an Auslan Communication Technologies Pipeline', 'Auslan', 'Australian sign language', 'Sign language recognition', 'Sign language production', 'Sign language processing', 'AI techniques for mainstream spoken languages have seen a great deal of progress in recent years, with technologies for transcription, translation and text processing becoming commercially available. However, no such technologies have been developed for sign languages, which, as visual-gestural languages, require multimodal processing approaches. This paper presents a plan to develop an Auslan Communication Technologies Pipeline (Auslan CTP), a prototype AI system enabling Auslan-in, Auslan-out interactions, to demonstrate the feasibility of Auslan-based machine interaction and language processing. Such a system has a range of applications, including gestural human-machine interfaces, educational tools, and translation.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_19');
INSERT INTO `paper` VALUES (11440, 'A Recurrent Transformer Network for Novel View Action Synthesis', 'Novel-view action synthesis', 'Action transformation', 'Video synthesis', '', '', 'In this work, we address the problem of synthesizing human actions from novel views. Given an input video of an actor performing some action, we aim to synthesize a video with the same action performed from a novel view with the help of an appearance prior. We propose an end-to-end deep network to solve this problem. The proposed network utilizes the change in viewpoint to transform the action from the input view to the novel view in feature space. The transformed action is integrated with the target appearance using the proposed recurrent transformer network, which provides a transformed appearance for each time-step in the action sequence. The recurrent transformer network utilize action key-points which are determined in an unsupervised approach using the encoded action features. We also propose a hierarchical structure for the recurrent transformation which further improves the performance. We demonstrate the effectiveness of the proposed method through extensive experiments conducted on a large-scale multi-view action recognition NTU-RGB+D dataset. In addition, we show that the proposed method can transform the action to a novel viewpoint with an entirely different scene or actor. The code is publicly available at https://github.com/schatzkara/cross-view-video.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_25');
INSERT INTO `paper` VALUES (11441, 'A Self-supervised Framework for Human Instance Segmentation', 'Human instance segmentation', 'Prior knowledge', 'Self-supervised', '', '', 'Existing approaches for human-centered tasks such as human instance segmentation are focused on improving the architectures of models, leveraging weak supervision or transforming supervision among related tasks. Nonetheless, the structures are highly specific and the weak supervision is limited by available priors or number of related tasks. In this paper, we present a novel self-supervised framework for human instance segmentation. The framework includes one module which iteratively conducts mutual refinement between segmentation and optical flow estimation, and the other module which iteratively refines pose estimations by exploring the prior knowledge about the consistency in human graph structures from consecutive frames. The results of the proposed framework are employed for fine-tuning segmentation networks in a feedback fashion. Experimental results on the OCHuman and COCOPersons datasets demonstrate that the self-supervised framework achieves current state-of-the-art performance against existing models on the challenging datasets without requiring additional labels. Unlabeled video data is utilized together with prior knowledge to significantly improve performance and reduce the reliance on annotations. Code released at: https://github.com/AllenYLJiang/SSINS.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_33');
INSERT INTO `paper` VALUES (11442, 'A Simple and Effective Framework for Pairwise Deep Metric Learning', 'Deep metric learning', 'Distributed robust learning', 'Data imbalance', '', '', 'Deep metric learning (DML) has received much attention in deep learning due to its wide applications in computer vision. Previous studies have focused on designing complicated losses and hard example mining methods, which are mostly heuristic and lack of theoretical understanding. In this paper, we cast DML as a simple pairwise binary classification problem that classifies a pair of examples as similar or dissimilar. It identifies the most critical issue in this problem—imbalanced data pairs. To tackle this issue, we propose a simple and effective framework to sample pairs in a batch of data for updating the model. The key to this framework is to define a robust loss for all pairs over a mini-batch of data, which is formulated by distributionally robust optimization. The flexibility in constructing the uncertainty decision set of the dual variable allows us to recover state-of-the-art complicated losses and also to induce novel variants. Empirical studies on several benchmark data sets demonstrate that our simple and effective method outperforms the state-of-the-art results.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_23');
INSERT INTO `paper` VALUES (11443, 'A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions', 'Image corruptions', 'Robustness', 'Generalization', 'Adversarial training', '', 'The human visual system is remarkably robust against a wide range of naturally occurring variations and corruptions like rain or snow. In contrast, the performance of modern image recognition models strongly degrades when evaluated on previously unseen corruptions. Here, we demonstrate that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the state of the art on the corruption benchmark ImageNet-C (with ResNet50) and on MNIST-C. We build on top of these strong baseline results and show that an adversarial training of the recognition model against locally correlated worst-case noise distributions leads to an additional increase in performance. This regularization can be combined with previously proposed defense methods for further improvement.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_4');
INSERT INTO `paper` VALUES (11444, 'A Single Stream Network for Robust and Real-Time RGB-D Salient Object Detection', 'RGB-D salient object detection', 'Single stream', 'Depth-enhanced dual attention', 'Lightweight', 'Real-time', 'Existing RGB-D salient object detection (SOD) approaches concentrate on the cross-modal fusion between the RGB stream and the depth stream. They do not deeply explore the effect of the depth map itself. In this work, we design a single stream network to directly use the depth map to guide early fusion and middle fusion between RGB and depth, which saves the feature encoder of the depth stream and achieves a lightweight and real-time model. We tactfully utilize depth information from two perspectives: (1) Overcoming the incompatibility problem caused by the great difference between modalities, we build a single stream encoder to achieve the early fusion, which can take full advantage of ImageNet pre-trained backbone model to extract rich and discriminative features. (2) We design a novel depth-enhanced dual attention module (DEDA) to efficiently provide the fore-/back-ground branches with the spatially filtered features, which enables the decoder to optimally perform the middle fusion. Besides, we put forward a pyramidally attended feature extraction module (PAFE) to accurately localize the objects of different scales. Extensive experiments demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics. Furthermore, this model is 55.5% lighter than the current lightest model and runs at a real-time speed of 32 FPS when processing a \\(384 \\times 384\\) image.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_39');
INSERT INTO `paper` VALUES (11445, 'A Subpixel Residual U-Net and Feature Fusion Preprocessing for Retinal Vessel Segmentation', 'Medical image analysis', 'Retinal Vessel Segmentation', 'Subpixel convolution', 'Residual network', 'U-Net', 'Retinal Image analysis allows medical professionals to inspect the morphology of the retinal vessels for the diagnosis of vascular diseases. Automated extraction of the vessels is vital for computer-aided diagnostic systems to provide a speedy and precise diagnosis. This paper introduces SpruNet, a Subpixel Convolution based Residual U-Net architecture which re-purposes subpixel convolutions as down-sampling and up-sampling method. The proposed subpixel convolution based down-sampling and up-sampling strategy efficiently minimizes the information loss during the encoding and decoding process which in turn increases the sensitivity of the model without hurting the specificity. A feature fusion technique of combining two types of image enhancement algorithms is also introduced. The model is trained and evaluated on three mainstream public benchmark datasets, and detailed analysis and comparison of the results are provided which shows that the model achieves state-of-the-art results with less complexity. The model can make inference on \\(512\\times 512\\) pixel full image in 0.5 s.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_15');
INSERT INTO `paper` VALUES (11446, 'A Unified Framework for Shot Type Classification Based on Subject Centric Lens', '', '', '', '', '', 'Shots are key narrative elements of various videos, e.g. movies, TV series, and user-generated videos that are thriving over the Internet. The types of shots greatly influence how the underlying ideas, emotions, and messages are expressed. The technique to analyze shot types is important to the understanding of videos, which has seen increasing demand in real-world applications in this era. Classifying shot type is challenging due to the additional information required beyond the video content, such as the spatial composition of a frame and camera movement. To address these issues, we propose a learning framework Subject Guidance Network (SGNet) for shot type recognition. SGNet separates the subject and background of a shot into two streams, serving as separate guidance maps for scale and movement type classification respectively. To facilitate shot type analysis and model evaluations, we build a large-scale dataset MovieShots, which contains 46K shots from 7K movie trailers with annotations of their scale and movement types. Experiments show that our framework is able to recognize these two attributes of shot accurately, outperforming all the previous methods.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_2');
INSERT INTO `paper` VALUES (11447, 'A Unified Framework of Surrogate Loss by Refactoring and Interpolation', 'Loss design', 'Image classification', 'Pose estimation', '', '', 'We introduce UniLoss, a unified framework to generate surrogate losses for training deep networks with gradient descent, reducing the amount of manual design of task-specific surrogate losses. Our key observation is that in many cases, evaluating a model with a performance metric on a batch of examples can be refactored into four steps: from input to real-valued scores, from scores to comparisons of pairs of scores, from comparisons to binary variables, and from binary variables to the final performance metric. Using this refactoring we generate differentiable approximations for each non-differentiable step through interpolation. Using UniLoss, we can optimize for different tasks and metrics using one unified framework, achieving comparable performance compared with task-specific losses. We validate the effectiveness of UniLoss on three tasks and four datasets. Code is available at https://github.com/princeton-vl/uniloss.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_17');
INSERT INTO `paper` VALUES (11448, 'A Unifying Mutual Information View of Metric Learning: Cross-Entropy vs. Pairwise Losses', 'Metric learning', 'Deep learning', 'Information theory', '', '', 'Recently, substantial research efforts in Deep Metric Learning (DML) focused on designing complex pairwise-distance losses, which require convoluted schemes to ease optimization, such as sample mining or pair weighting. The standard cross-entropy loss for classification has been largely overlooked in DML. On the surface, the cross-entropy may seem unrelated and irrelevant to metric learning as it does not explicitly involve pairwise distances. However, we provide a theoretical analysis that links the cross-entropy to several well-known and recent pairwise losses. Our connections are drawn from two different perspectives: one based on an explicit optimization insight; the other on discriminative and generative views of the mutual information between the labels and the learned features. First, we explicitly demonstrate that the cross-entropy is an upper bound on a new pairwise loss, which has a structure similar to various pairwise losses: it minimizes intra-class distances while maximizing inter-class distances. As a result, minimizing the cross-entropy can be seen as an approximate bound-optimization (or Majorize-Minimize) algorithm for minimizing this pairwise loss. Second, we show that, more generally, minimizing the cross-entropy is actually equivalent to maximizing the mutual information, to which we connect several well-known pairwise losses. Furthermore, we show that various standard pairwise losses can be explicitly related to one another via bound relationships. Our findings indicate that the cross-entropy represents a proxy for maximizing the mutual information – as pairwise losses do – without the need for convoluted sample-mining heuristics. Our experiments (Code available at: https://github.com/jeromerony/dml_cross_entropy) over four standard DML benchmarks strongly support our findings. We obtain state-of-the-art results, outperforming recent and complex DML methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_33');
INSERT INTO `paper` VALUES (11449, 'A Visual Inductive Priors Framework for Data-Efficient Image Classification', 'Learn from scratch', 'Classification', 'Visual inductive priors', 'DSK-net', 'Induced hierarchy', 'State-of-the-art classifiers rely heavily on large-scale datasets, such as ImageNet, JFT-300M, MSCOCO, Open Images, etc. Besides, the performance may decrease significantly because of insufficient learning on a handful of samples. We present Visual Inductive Priors Framework (VIPF), a framework that can learn classifiers from scratch. VIPF can maximize the effectiveness of limited data. In this work, we propose a novel neural network architecture: DSK-net, which is very effective in training from small data sets. With more discriminative feature extracted from DSK-net, overfitting of network is alleviated. Furthermore, a loss function based on positive class as well as an induced hierarchy are also applied to further improve the VIPF’s capability of learning from scratch. Finally, we won the 1st Place in VIPriors image classification competition.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_35');
INSERT INTO `paper` VALUES (11450, 'AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling', 'Object detection', 'Hyper-parameter optimization', 'Bayesian optimization', 'Sub-sampling', '', 'Most state-of-the-art object detection systems follow an anchor-based diagram. Anchor boxes are densely proposed over the images and the network is trained to predict the boxes position offset as well as the classification confidence. Existing systems pre-define anchor box shapes and sizes and ad-hoc heuristic adjustments are used to define the anchor configurations. However, this might be sub-optimal or even wrong when a new dataset or a new model is adopted. In this paper, we study the problem of automatically optimizing anchor boxes for object detection. We first demonstrate that the number of anchors, anchor scales and ratios are crucial factors for a reliable object detection system. By carefully analyzing the existing bounding box patterns on the feature hierarchy, we design a flexible and tight hyper-parameter space for anchor configurations. Then we propose a novel hyper-parameter optimization method named AABO to determine more appropriate anchor boxes for a certain dataset, in which Bayesian Optimization and sub-sampling method are combined to achieve precise and efficient anchor configuration optimization. Experiments demonstrate the effectiveness of our proposed method on different detectors and datasets, e.g. achieving around 2.4% mAP improvement on COCO, 1.6% on ADE and 1.5% on VG, and the optimal anchors can bring 1.4%–2.4% mAP improvement on SOTA detectors by only optimizing anchor configurations, e.g. boosting Mask RCNN from 40.3% to 42.3%, and HTC detector from 46.8% to 48.2%.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_33');
INSERT INTO `paper` VALUES (11451, 'Abiotic Stress Prediction from RGB-T Images of Banana Plantlets', 'Water stress', 'Thermal images', 'Neural networks', '', '', 'Prediction of stress conditions is important for monitoring plant growth stages, disease detection, and assessment of crop yields. Multi-modal data, acquired from a variety of sensors, offers diverse perspectives and is expected to benefit the prediction process. We present several methods and strategies for abiotic stress prediction in banana plantlets, on a dataset acquired during a two and a half weeks period, of plantlets subject to four separate water and fertilizer treatments. The dataset consists of RGB and thermal images, taken once daily of each plant. Results are encouraging, in the sense that neural networks exhibit high prediction rates (over \\(90\\%\\) amongst four classes), in cases where there are hardly any noticeable features distinguishing the treatments, much higher than field experts can supply.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_20');
INSERT INTO `paper` VALUES (11452, 'Accelerating CNN Training by Pruning Activation Gradients', 'CNN training', 'Acceleration', 'Gradients pruning', '', '', 'Sparsification is an efficient approach to accelerate CNN inference, but it is challenging to take advantage of sparsity in training procedure because the involved gradients are dynamically changed. Actually, an important observation shows that most of the activation gradients in back-propagation are very close to zero and only have a tiny impact on weight-updating. Hence, we consider pruning these very small gradients randomly to accelerate CNN training according to the statistical distribution of activation gradients. Meanwhile, we theoretically analyze the impact of pruning algorithm on the convergence. The proposed approach is evaluated on AlexNet and ResNet-{18, 34, 50, 101, 152} with CIFAR-{10, 100} and ImageNet datasets. Experimental results show that our training approach could substantially achieve up to \\(5.92 \\times \\) speedups at back-propagation stage with negligible accuracy loss.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_20');
INSERT INTO `paper` VALUES (11453, 'Accelerating Deep Learning with Millions of Classes', 'Large-scale learning', 'Deep learning', 'Random projection', 'Acceleration in deep learning', '', 'Deep learning has achieved remarkable success in many classification tasks because of its great power of representation learning for complex data. However, it remains challenging when extending to classification tasks with millions of classes. Previous studies are focused on solving this problem in a distributed fashion or using a sampling-based approach to reduce the computational cost caused by the softmax layer. However, these approaches still need high GPU memory in order to work with large models and it is non-trivial to extend them to parallel settings. To address these issues, we propose an efficient training framework to handle extreme classification tasks based on Random Projection. The key idea is that we first train a slimmed model with a random projected softmax classifier and then we recover it to the original classifier. We also show a theoretical guarantee that this recovered classifier can approximate the original classifier with a small error. Later, we extend our framework to parallel settings by adopting a communication reduction technique. In our experiments, we demonstrate that the proposed framework is able to train deep learning models with millions of classes and achieve above \\(10{\\times }\\) speedup compared to existing approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_42');
INSERT INTO `paper` VALUES (11454, 'Accurate Optimization of Weighted Nuclear Norm for Non-Rigid Structure from Motion', '', '', '', '', '', 'Fitting a matrix of a given rank to data in a least squares sense can be done very effectively using 2nd order methods such as Levenberg-Marquardt by explicitly optimizing over a bilinear parameterization of the matrix. In contrast, when applying more general singular value penalties, such as weighted nuclear norm priors, direct optimization over the elements of the matrix is typically used. Due to non-differentiability of the resulting objective function, first order sub-gradient or splitting methods are predominantly used. While these offer rapid iterations it is well known that they become inefficient near the minimum due to zig-zagging and in practice one is therefore often forced to settle for an approximate solution.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_2');
INSERT INTO `paper` VALUES (11455, 'Accurate Polarimetric BRDF for Real Polarization Scene Rendering', 'Polarization', 'Shape from polarization', 'Polarimetric BRDF', 'Convolutional Neural Network', '', 'Polarization has been used to solve a lot of computer vision tasks such as Shape from Polarization (SfP). But existing methods suffer from ambiguity problems of polarization. To overcome such problems, some research works have suggested to use Convolutional Neural Network (CNN). But acquiring large scale dataset with polarization information is a very difficult task. If there is an accurate model which can describe a complicated phenomenon of polarization, we can easily produce synthetic polarized images with various situations to train CNN.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_14');
INSERT INTO `paper` VALUES (11456, 'Accurate Reconstruction of Oriented 3D Points Using Affine Correspondences', 'Affine correspondences', 'Photoconsistency optimization', 'Tracking', 'Surface normal estimation', '', 'Affine correspondences (ACs) have been an active topic of research, namely for the recovery of surface normals. However, current solutions still suffer from the fact that even state-of-the-art affine feature detectors are inaccurate, and ACs are often contaminated by large levels of noise, yielding poor surface normals. This article provides new formulations for achieving epipolar geometry-consistent ACs, that, besides leading to linear solvers that are up to 30\\(\\times \\) faster than the state-of-the-art alternatives, allow for a fast refinement scheme that significantly improves the quality of the noisy ACs. In addition, a tracker that automatically enforces the epipolar geometry is proposed, with experiments showing that it significantly outperforms competing methods in situations of low texture. This opens the way to application domains where the scenes are typically low textured, such as during arthroscopic procedures.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_33');
INSERT INTO `paper` VALUES (11457, 'Accurate RGB-D Salient Object Detection via Collaborative Learning', '', '', '', '', '', 'Benefiting from the spatial cues embedded in depth images, recent progress on RGB-D saliency detection shows impressive ability on some challenge scenarios. However, there are still two limitations. One hand is that the pooling and upsampling operations in FCNs might cause blur object boundaries. On the other hand, using an additional depth-network to extract depth features might lead to high computation and storage cost. The reliance on depth inputs during testing also limits the practical applications of current RGB-D models. In this paper, we propose a novel collaborative learning framework where edge, depth and saliency are leveraged in a more efficient way, which solves those problems tactfully. The explicitly extracted edge information goes together with saliency to give more emphasis to the salient regions and object boundaries. Depth and saliency learning is innovatively integrated into the high-level feature learning process in a mutual-benefit manner. This strategy enables the network to be free of using extra depth networks and depth inputs to make inference. To this end, it makes our model more lightweight, faster and more versatile. Experiment results on seven benchmark datasets show its superior performance.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_4');
INSERT INTO `paper` VALUES (11458, 'Acquiring Dynamic Light Fields Through Coded Aperture Camera', 'Light field', 'CNN', 'Coded aperture camera', '', '', 'We investigate the problem of compressive acquisition of a dynamic light field. A promising solution for compressive light field acquisition is to use a coded aperture camera, with which an entire light field can be computationally reconstructed from several images captured through differently-coded aperture patterns. With this method, it was assumed that the scene should not move throughout the complete acquisition process, which restricted real applications. In this study, however, we assume that the target scene may change over time, and propose a method for acquiring a dynamic light field (a moving scene) using a coded aperture camera and a convolutional neural network (CNN). To successfully handle scene motions, we develop a new configuration of image observation, called V-shape observation, and train the CNN using a dynamic-light-field dataset with pseudo motions. Our method is validated through experiments using both a computer-generated scene and a real camera.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_22');
INSERT INTO `paper` VALUES (11459, 'Across Scales and Across Dimensions: Temporal Super-Resolution Using Deep Internal Learning', '', '', '', '', '', 'When a very fast dynamic event is recorded with a low-framerate camera, the resulting video suffers from severe motion blur (due to exposure time) and motion aliasing (due to low sampling rate in time). True Temporal Super-Resolution (TSR) is more than just Temporal-Interpolation (increasing framerate). It can also recover new high temporal frequencies beyond the temporal Nyquist limit of the input video, thus resolving both motion-blur and motion-aliasing – effects that temporal frame interpolation (as sophisticated as it may be) cannot undo. In this paper we propose a “Deep Internal Learning” approach for true TSR. We train a video-specific CNN on examples extracted directly from the low-framerate input video. Our method exploits the strong recurrence of small space-time patches inside a single video sequence, both within and across different spatio-temporal scales of the video. We further observe (for the first time) that small space-time patches recur also across-dimensions of the video sequence – i.e., by swapping the spatial and temporal dimensions. In particular, the higher spatial resolution of video frames provides strong examples as to how to increase the temporal resolution of that video. Such internal video-specific examples give rise to strong self-supervision, requiring no data but the input video itself. This results in Zero-Shot Temporal-SR of complex videos, which removes both motion blur and motion aliasing, outperforming previous supervised methods trained on external video datasets.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_4');
INSERT INTO `paper` VALUES (11460, 'Action Localization Through Continual Predictive Learning', 'Action localization', 'Continuous learning', 'Self-supervision', '', '', 'The problem of action localization involves locating the action in the video, both over time and spatially in the image. The current dominant approaches use supervised learning to solve this problem. They require large amounts of annotated training data, in the form of frame-level bounding box annotations around the region of interest. In this paper, we present a new approach based on continual learning that uses feature-level predictions for self-supervision. It does not require any training annotations in terms of frame-level bounding boxes. The approach is inspired by cognitive models of visual event perception that propose a prediction-based approach to event understanding. We use a stack of LSTMs coupled with a CNN encoder, along with novel attention mechanisms, to model the events in the video and use this model to predict high-level features for the future frames. The prediction errors are used to learn the parameters of the models continuously. This self-supervised framework is not complicated as other approaches but is very effective in learning robust visual representations for both labeling and localization. It should be noted that the approach outputs in a streaming fashion, requiring only a single pass through the video, making it amenable for real-time processing. We demonstrate this on three datasets - UCF Sports, JHMDB, and THUMOS’13 and show that the proposed approach outperforms weakly-supervised and unsupervised baselines and obtains competitive performance compared to fully supervised baselines. Finally, we show that the proposed framework can generalize to egocentric videos and achieve state-of-the-art results on the unsupervised gaze prediction task. Code is available on the project page(https://saakur.github.io/Projects/ActionLocalization/.).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_18');
INSERT INTO `paper` VALUES (11461, 'Actions as Moving Points', 'Spatio-temporal action detection', 'Anchor-free detection', '', '', '', 'The existing action tubelet detectors often depend on heuristic anchor design and placement, which might be computationally expensive and sub-optimal for precise localization. In this paper, we present a conceptually simple, computationally efficient, and more precise action tubelet detection framework, termed as MovingCenter Detector (MOC-detector), by treating an action instance as a trajectory of moving points. Based on the insight that movement information could simplify and assist action tubelet detection, our MOC-detector is composed of three crucial head branches: (1) Center Branch for instance center detection and action recognition, (2) Movement Branch for movement estimation at adjacent frames to form trajectories of moving points, (3) Box Branch for spatial extent detection by directly regressing bounding box size at each estimated center. These three branches work together to generate the tubelet detection results, which could be further linked to yield video-level tubes with a matching strategy. Our MOC-detector outperforms the existing state-of-the-art methods for both metrics of frame-mAP and video-mAP on the JHMDB and UCF101-24 datasets. The performance gap is more evident for higher video IoU, demonstrating that our MOC-detector is particularly effective for more precise action detection. We provide the code at https://github.com/MCG-NJU/MOC-Detector.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_5');
INSERT INTO `paper` VALUES (11462, 'Active Class Incremental Learning for Imbalanced Datasets', 'Incremental learning', 'Active learning', 'Imbalanced learning', 'Computer vision', 'Image classification.', 'Incremental Learning (IL) allows AI systems to adapt to streamed data. Most existing algorithms make two strong hypotheses which reduce the realism of the incremental scenario: (1) new data are assumed to be readily annotated when streamed and (2) tests are run with balanced datasets while most real-life datasets are imbalanced. These hypotheses are discarded and the resulting challenges are tackled with a combination of active and imbalanced learning. We introduce sample acquisition functions which tackle imbalance and are compatible with IL constraints. We also consider IL as an imbalanced learning problem instead of the established usage of knowledge distillation against catastrophic forgetting. Here, imbalance effects are reduced during inference through class prediction scaling. Evaluation is done with four visual datasets and compares existing and proposed sample acquisition functions. Results indicate that the proposed contributions have a positive effect and reduce the gap between active and standard IL performance.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_12');
INSERT INTO `paper` VALUES (11463, 'Active Crowd Analysis for Pandemic Risk Mitigation for Blind or Visually Impaired Persons', 'Active crowd analysis', 'Visually impaired', 'Human detection', 'Crowd density', 'Crowd distance', 'During pandemics like COVID-19, social distancing is essential to combat the rise of infections. However, it is challenging for the visually impaired to practice social distancing as their low vision hinders them from maintaining a safe physical distance from other humans. In this paper, we propose a smartphone-based computationally-efficient deep neural network to detect crowds and relay the associated risks to the Blind or Visually Impaired (BVI) user through directional audio alerts. The system first detects humans and estimates their distances from the smartphone’s monocular camera feed. Then, the system clusters humans into crowds to generate density and distance maps from the crowd centers. Finally, the system tracks detections in previous frames creating motion maps predicting the motion of crowds to generate an appropriate audio alert. Active Crowd Analysis is designed for real-time smartphone use, utilizing the phone’s native hardware to ensure the BVI can safely maintain social distancing.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_25');
INSERT INTO `paper` VALUES (11464, 'Active Crowd Counting with Limited Supervision', '', '', '', '', '', 'To learn a reliable people counter from crowd images, head center annotations are normally required. Annotating head centers is however a laborious and tedious process in dense crowds. In this paper, we present an active learning framework which enables accurate crowd counting with limited supervision: given a small labeling budget, instead of randomly selecting images to annotate, we first introduce an active labeling strategy to annotate the most informative images in the dataset and learn the counting model upon them. The process is repeated such that in every cycle we select the samples that are diverse in crowd density and dissimilar to previous selections. In the last cycle when the labeling budget is met, the large amount of unlabeled data are also utilized: a distribution classifier is introduced to align the labeled data with unlabeled data; furthermore, we propose to mix up the distribution labels and latent representations of data in the network to particularly improve the distribution alignment in-between training samples. We follow the popular density estimation pipeline for crowd counting. Extensive experiments are conducted on standard benchmarks i.e. ShanghaiTech, UCF_CC_50, MAll, TRANCOS, and DCC. By annotating limited number of images (e.g. 10% of the dataset), our method reaches levels of performance not far from the state of the art which utilize full annotations of the dataset.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_34');
INSERT INTO `paper` VALUES (11465, 'Active Perception Using Light Curtains for Autonomous Driving', 'Active vision', 'Robotics', 'Autonomous driving', '3D vision', '', 'Most real-world 3D sensors such as LiDARs perform fixed scans of the entire environment, while being decoupled from the recognition system that processes the sensor data. In this work, we propose a method for 3D object recognition using light curtains, a resource-efficient controllable sensor that measures depth at user-specified locations in the environment. Crucially, we propose using prediction uncertainty of a deep learning based 3D point cloud detector to guide active perception. Given a neural network’s uncertainty, we develop a novel optimization algorithm to optimally place light curtains to maximize coverage of uncertain regions. Efficient optimization is achieved by encoding the physical constraints of the device into a constraint graph, which is optimized with dynamic programming. We show how a 3D detector can be trained to detect objects in a scene by sequentially placing uncertainty-guided light curtains to successively improve detection accuracy. Links to code can be found on the project webpage.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_44');
INSERT INTO `paper` VALUES (11466, 'Active Visual Information Gathering for Vision-Language Navigation', 'Vision-language navigation', 'Active exploration', '', '', '', 'Vision-language navigation (VLN) is the task of entailing an agent to carry out navigational instructions inside photo-realistic environments. One of the key challenges in VLN is how to conduct a robust navigation by mitigating the uncertainty caused by ambiguous instructions and insufficient observation of the environment. Agents trained by current approaches typically suffer from this and would consequently struggle to avoid random and inefficient actions at every step. In contrast, when humans face such a challenge, they can still maintain robust navigation by actively exploring the surroundings to gather more information and thus make more confident navigation decisions. This work draws inspiration from human navigation behavior and endows an agent with an active information gathering ability for a more intelligent vision-language navigation policy. To achieve this, we propose an end-to-end framework for learning an exploration policy that decides i) when and where to explore, ii) what information is worth gathering during exploration, and iii) how to adjust the navigation decision after the exploration. The experimental results show promising exploration strategies emerged from training, which leads to significant boost in navigation performance. On the R2R challenge leaderboard, our agent gets promising results all three VLN settings, i.e., single run, pre-exploration, and beam search.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_19');
INSERT INTO `paper` VALUES (11467, 'Adapting Object Detectors with Conditional Domain Normalization', '', '', '', '', '', 'Real-world object detectors are often challenged by the domain gaps between different datasets. In this work, we present the Conditional Domain Normalization (CDN) to bridge the domain distribution gap. CDN is designed to encode different domain inputs into a shared latent space, where the features from different domains carry the same domain attribute. To achieve this, we first disentangle the domain-specific attribute out of the semantic features from source domain via a domain embedding module, which learns a domain-vector to characterize the domain attribute information. Then this domain-vector is used to encode the features from target domain through a conditional normalization, resulting in different domains’ features carrying the same domain attribute. We incorporate CDN into various convolution stages of an object detector to adaptively address the domain shifts of different level’s representation. In contrast to existing adaptation works that conduct domain confusion learning on semantic features to remove domain-specific factors, CDN aligns different domain distributions by modulating the semantic features of target domains conditioned on the learned domain-vector of the source domain. Extensive experiments show that CDN outperforms existing methods remarkably on both real-to-real and synthetic-to-real adaptation benchmarks, including 2D image detection and 3D point cloud detection.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_24');
INSERT INTO `paper` VALUES (11468, 'Adaptive Computationally Efficient Network for Monocular 3D Hand Pose Estimation', '3D hand pose estimation', 'Computation efficiency', 'Dynamic adaption', 'Gaussian gate', '', '3D hand pose estimation is an important task for a wide range of real-world applications. Existing works in this domain mainly focus on designing advanced algorithms to achieve high pose estimation accuracy. However, besides accuracy, the computation efficiency that affects the computation speed and power consumption is also crucial for real-world applications. In this paper, we investigate the problem of reducing the overall computation cost yet maintaining the high accuracy for 3D hand pose estimation from video sequences. A novel model, called Adaptive Computationally Efficient (ACE) network, is proposed, which takes advantage of a Gaussian kernel based Gate Module to dynamically switch the computation between a light model and a heavy network for feature extraction. Our model employs the light model to compute efficient features for most of the frames and invokes the heavy model only when necessary. Combined with the temporal context, the proposed model accurately estimates the 3D hand pose. We evaluate our model on two publicly available datasets, and achieve state-of-the-art performance at 22% of the computation cost compared to traditional temporal models.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_8');
INSERT INTO `paper` VALUES (11469, 'Adaptive Hybrid Composition Based Super-Resolution Network via Fine-Grained Channel Pruning', 'Single image super-resolution', 'Efficient model', 'Channel pruning', '', '', 'In recent years, remarkable progress has been made in single image super-resolution due to the powerful representation capabilities of deep neural networks. However, the superior performance is at the expense of excessive computation costs, limiting the SR application in resource-constrained devices. To address this problem, we firstly propose a hybrid composition block (HCB), which contains asymmetric and shrinked spatial convolution in parallel. Secondly, we build our baseline model based on cascaded HCB with a progressive upsampling method. Besides, feature fusion method is developed which concatenates all of the previous feature maps of HCB. Thirdly, to solve the misalignment problem in pruning residual networks, we propose a fine-grained channel pruning that allows adaptive connections to fully skip the residual block, and any unimportant channel between convolutions can be pruned independently. Finally, we present an adaptive hybrid composition based super-resolution network (AHCSRN) by pruning the baseline model. Extensive experiments demonstrate that the proposed method can achieve better performance than state-of-the-art SR models with ultra-low parameters and Flops.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_7');
INSERT INTO `paper` VALUES (11470, 'Adaptive Margin Diversity Regularizer for Handling Data Imbalance in Zero-Shot SBIR', '', '', '', '', '', 'Data from new categories are continuously being discovered, which has sparked significant amount of research in developing approaches which generalize to previously unseen categories, i.e. zero-shot setting. Zero-shot sketch-based image retrieval (ZS-SBIR) is one such problem in the context of cross-domain retrieval, which has received lot of attention due to its various real-life applications. Since most real-world training data have a fair amount of imbalance; in this work, for the first time in literature, we extensively study the effect of training data imbalance on the generalization to unseen categories, with ZS-SBIR as the application area. We evaluate several state-of-the-art data imbalance mitigating techniques and analyze their results. Furthermore, we propose a novel framework AMDReg (Adaptive Margin Diversity Regularizer), which ensures that the embeddings of the sketches and images in the latent space are not only semantically meaningful, but they are also separated according to their class-representations in the training set. The proposed approach is model-independent, and it can be incorporated seamlessly with several state-of-the-art ZS-SBIR methods to improve their performance under imbalanced condition. Extensive experiments and analysis justify the effectiveness of the proposed AMDReg for mitigating the effect of data imbalance for generalization to unseen classes in ZS-SBIR.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_21');
INSERT INTO `paper` VALUES (11471, 'Adaptive Mixture Regression Network with Local Counting Map for Crowd Counting', 'Crowd counting', 'Local counting map', 'Adaptive mixture regression network', '', '', 'The crowd counting task aims at estimating the number of people located in an image or a frame from videos. Existing methods widely adopt density maps as the training targets to optimize the point-to-point loss. While in testing phase, we only focus on the differences between the crowd numbers and the global summation of density maps, which indicate the inconsistency between the training targets and the evaluation criteria. To solve this problem, we introduce a new target, named local counting map (LCM), to obtain more accurate results than density map based approaches. Moreover, we also propose an adaptive mixture regression framework with three modules in a coarse-to-fine manner to further improve the precision of the crowd estimation: scale-aware module (SAM), mixture regression module (MRM) and adaptive soft interval module (ASIM). Specifically, SAM fully utilizes the context and multi-scale information from different convolutional features; MRM and ASIM perform more precise counting regression on local patches of images. Compared with current methods, the proposed method reports better performances on the typical datasets. The source code is available at https://github.com/xiyang1012/Local-Crowd-Counting.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_15');
INSERT INTO `paper` VALUES (11472, 'Adaptive Object Detection with Dual Multi-label Prediction', 'Cross-domain object detection', 'Auxiliary task', '', '', '', 'In this paper, we propose a novel end-to-end unsupervised deep domain adaptation model for adaptive object detection by exploiting multi-label object recognition as a dual auxiliary task. The model exploits multi-label prediction to reveal the object category information in each image and then uses the prediction results to perform conditional adversarial global feature alignment, such that the multimodal structure of image features can be tackled to bridge the domain divergence at the global feature level while preserving the discriminability of the features. Moreover, we introduce a prediction consistency regularization mechanism to assist object detection, which uses the multi-label prediction results as an auxiliary regularization information to ensure consistent object category discoveries between the object recognition task and the object detection task. Experiments are conducted on a few benchmark datasets and the results show the proposed model outperforms the state-of-the-art comparison methods .', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_4');
INSERT INTO `paper` VALUES (11473, 'Adaptive Offline Quintuplet Loss for Image-Text Matching', 'Image-text matching', 'Triplet loss', 'Hard negative mining', '', '', 'Existing image-text matching approaches typically leverage triplet loss with online hard negatives to train the model. For each image or text anchor in a training mini-batch, the model is trained to distinguish between a positive and the most confusing negative of the anchor mined from the mini-batch (i.e. online hard negative). This strategy improves the model’s capacity to discover fine-grained correspondences and non-correspondences between image and text inputs. However, the above approach has the following drawbacks: (1) the negative selection strategy still provides limited chances for the model to learn from very hard-to-distinguish cases. (2) The trained model has weak generalization capability from the training set to the testing set. (3) The penalty lacks hierarchy and adaptiveness for hard negatives with different “hardness” degrees. In this paper, we propose solutions by sampling negatives offline from the whole training set. It provides “harder” offline negatives than online hard negatives for the model to distinguish. Based on the offline hard negatives, a quintuplet loss is proposed to improve the model’s generalization capability to distinguish positives and negatives. In addition, a novel loss function that combines the knowledge of positives, offline hard negatives and online hard negatives is created. It leverages offline hard negatives as the intermediary to adaptively penalize them based on their distance relations to the anchor. We evaluate the proposed training approach on three state-of-the-art image-text models on the MS-COCO and Flickr30K datasets. Significant performance improvements are observed for all the models, proving the effectiveness and generality of our approach. Code is available at https://github.com/sunnychencool/AOQ.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_33');
INSERT INTO `paper` VALUES (11474, 'Adaptive Task Sampling for Meta-learning', '', '', '', '', '', 'Meta-learning methods have been extensively studied and applied in computer vision, especially for few-shot classification tasks. The key idea of meta-learning for few-shot classification is to mimic the few-shot situations faced at test time by randomly sampling classes in meta-training data to construct few-shot tasks for episodic training. While a rich line of work focuses solely on how to extract meta-knowledge across tasks, we exploit the complementary problem on how to generate informative tasks. We argue that the randomly sampled tasks could be sub-optimal and uninformative (e.g., the task of classifying “dog” from “laptop” is often trivial) to the meta-learner. In this paper, we propose an adaptive task sampling method to improve the generalization performance. Unlike instance based sampling, task based sampling is much more challenging due to the implicit definition of the task in each episode. Therefore, we accordingly propose a greedy class-pair based sampling method, which selects difficult tasks according to class-pair potentials. We evaluate our adaptive task sampling method on two few-shot classification benchmarks, and it achieves consistent improvements across different feature backbones, meta-learning algorithms and datasets.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_44');
INSERT INTO `paper` VALUES (11475, 'Adaptive Text Recognition Through Visual Matching', 'Text recognition', 'Sequence recognition', 'Similarity maps', '', '', 'This work addresses the problems of generalization and flexibility for text recognition in documents. We introduce a new model that exploits the repetitive nature of characters in languages, and decouples the visual decoding and linguistic modelling stages through intermediate representations in the form of similarity maps. By doing this, we turn text recognition into a visual matching problem, thereby achieving generalization in appearance and flexibility in classes.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_4');
INSERT INTO `paper` VALUES (11476, 'Adaptive Variance Based Label Distribution Learning for Facial Age Estimation', 'Age estimation', 'Distribution learning', 'Meta-learning', '', '', 'Estimating age from a single facial image is a classic and challenging topic in computer vision. One of its most intractable issues is label ambiguity, i.e., face images from adjacent age of the same person are often indistinguishable. Some existing methods adopt distribution learning to tackle this issue by exploiting the semantic correlation between age labels. Actually, most of them set a fixed value to the variance of Gaussian label distribution for all the images. However, the variance is closely related to the correlation between adjacent ages and should vary across ages and identities. To model a sample-specific variance, in this paper, we propose an adaptive variance based distribution learning (AVDL) method for facial age estimation. AVDL introduces the data-driven optimization framework, meta-learning, to achieve this. Specifically, AVDL performs a meta gradient descent step on the variable (i.e. variance) to minimize the loss on a clean unbiased validation set. By adaptively learning proper variance for each sample, our method can approximate the true age probability distribution more effectively. Extensive experiments on FG-NET and MORPH II datasets show the superiority of our proposed approach to the existing state-of-the-art methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_23');
INSERT INTO `paper` VALUES (11477, 'Adaptive Video Highlight Detection by Learning from User History', 'Video highlighight detection', 'User-adaptive learning', '', '', '', 'Recently, there is an increasing interest in highlight detection research where the goal is to create a short duration video from a longer video by extracting its interesting moments. However, most existing methods ignore the fact that the definition of video highlight is highly subjective. Different users may have different preferences of highlight for the same input video. In this paper, we propose a simple yet effective framework that learns to adapt highlight detection to a user by exploiting the user’s history in the form of highlights that the user has previously created. Our framework consists of two sub-networks: a fully temporal convolutional highlight detection network H that predicts highlight for an input video and a history encoder network M for user history. We introduce a newly designed temporal-adaptive instance normalization (T-AIN) layer to H where the two sub-networks interact with each other. T-AIN has affine parameters that are predicted from M based on the user history and is responsible for the user-adaptive signal to H. Extensive experiments on a large-scale dataset show that our framework can make more accurate and user-specific highlight predictions.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_16');
INSERT INTO `paper` VALUES (11478, 'Adaptive Virtual Reality Exergame for Individualized Rehabilitation for Persons with Spinal Cord Injury', 'Virtual reality', 'Rehabilitation', 'Spinal cord injury', 'Shoulder torque', 'Exergame', 'Typical exergames used for rehabilitative therapy can be either too difficult to play or monotonous leading to a lack of adherence. Adapting exergames by tuning various gameplay parameters based on the individual’s physiological ability maintains a constant challenge to improve a participant’s level of engagement and to encourage the physical performance of the user to achieve rehabilitation goals. In this paper we developed a pilot exergame using a commercially available virtual reality (VR) system with varied and customizable gameplay parameters and accessible interface. A baseline task VR tool was previously developed to determine an individual player’s initial 3-D spatial range of motion and areas of comfort. We observed the effects of adjusting gameplay parameters on a participant’s physiological performance by measuring velocity of motions and frequency and effort of targeted movements. We calculated joint torques through inverse kinematics to serve as an analysis tool to quantitatively gauge muscular effort. The system can provide an improved rehabilitation experience for persons with tetraplegia in home settings while allowing oversight by clinical therapists through tracking of physiological performance metrics and movement analysis from mixed reality videos.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_31');
INSERT INTO `paper` VALUES (11479, 'Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training', 'Neural network', 'Robustness', 'Common corruptions', 'Adversarial training', 'Mixup', 'Despite their performance, Artificial Neural Networks are not reliable enough for most of industrial applications. They are sensitive to noises, rotations, blurs and adversarial examples. There is a need to build defenses that protect against a wide range of perturbations, covering the most traditional common corruptions and adversarial examples. We propose a new data augmentation strategy called M-TLAT and designed to address robustness in a broad sense. Our approach combines the Mixup augmentation and a new adversarial training algorithm called Targeted Labeling Adversarial Training (TLAT). The idea of TLAT is to interpolate the target labels of adversarial examples with the ground-truth labels. We show that M-TLAT can increase the robustness of image classifiers towards nineteen common corruptions and five adversarial attacks, without reducing the accuracy on clean samples.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_14');
INSERT INTO `paper` VALUES (11480, 'Adversarial Attack on Deepfake Detection Using RL Based Texture Patches', 'Adversarial attack', 'Deepfake', 'FaceForensics++', '', '', 'The advancements in GANs have made creating deepfake videos a relatively easy task. Considering the threat that deepfake videos pose for manipulating political opinion, recent research has focused on ways to better detect deepfake videos. Even though researchers have had some success in detecting deepfake videos, it has been found that these detection systems can be attacked.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_14');
INSERT INTO `paper` VALUES (11481, 'Adversarial Background-Aware Loss for Weakly-Supervised Temporal Activity Localization', 'A2CL-PT', 'Temporal activity localization', 'Adversarial learning', 'Weakly-supervised learning', 'Center loss with a pair of triplets', 'Temporally localizing activities within untrimmed videos has been extensively studied in recent years. Despite recent advances, existing methods for weakly-supervised temporal activity localization struggle to recognize when an activity is not occurring. To address this issue, we propose a novel method named A2CL-PT. Two triplets of the feature space are considered in our approach: one triplet is used to learn discriminative features for each activity class, and the other one is used to distinguish the features where no activity occurs (i.e. background features) from activity-related features for each video. To further improve the performance, we build our network using two parallel branches which operate in an adversarial way: the first branch localizes the most salient activities of a video and the second one finds other supplementary activities from non-localized parts of the video. Extensive experiments performed on THUMOS14 and ActivityNet datasets demonstrate that our proposed method is effective. Specifically, the average mAP of IoU thresholds from 0.1 to 0.9 on the THUMOS14 dataset is significantly improved from 27.9% to 30.0%.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_17');
INSERT INTO `paper` VALUES (11482, 'Adversarial Continual Learning', '', '', '', '', '', 'Continual learning aims to learn new tasks without forgetting previously learned ones. We hypothesize that representations learned to solve each task in a sequence have a shared structure while containing some task-specific properties. We show that shared features are significantly less prone to forgetting and propose a novel hybrid continual learning framework that learns a disjoint representation for task-invariant and task-specific features required to solve a sequence of tasks. Our model combines architecture growth to prevent forgetting of task-specific skills and an experience replay approach to preserve shared skills. We demonstrate our hybrid approach is effective in avoiding forgetting and show it is superior to both architecture-based and memory-based approaches on class incrementally learning of a single dataset as well as a sequence of multiple datasets in image classification. Our code is available at https://github.com/facebookresearch/Adversarial-Continual-Learning.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_23');
INSERT INTO `paper` VALUES (11483, 'Adversarial Data Augmentation via Deformation Statistics', 'Data augmentation', 'Image registration', 'Segmentation', '', '', 'Deep learning models have been successful in computer vision and medical image analysis. However, training these models frequently requires large labeled image sets whose creation is often very time and labor intensive, for example, in the context of 3D segmentations. Approaches capable of training deep segmentation networks with a limited number of labeled samples are therefore highly desirable. Data augmentation or semi-supervised approaches are commonly used to cope with limited labeled training data. However, the augmentation strategies for many existing approaches are either hand-engineered or require computationally demanding searches. To that end, we explore an augmentation strategy which builds statistical deformation models from unlabeled data via principal component analysis and uses the resulting statistical deformation space to augment the labeled training samples. Specifically, we obtain transformations via deep registration models. This allows for an intuitive control over plausible deformation magnitudes via the statistical model and, if combined with an appropriate deformation model, yields spatially regular transformations. To optimally augment a dataset we use an adversarial strategy integrated into our statistical deformation model. We demonstrate the effectiveness of our approach for the segmentation of knee cartilage from 3D magnetic resonance images. We show favorable performance to state-of-the-art augmentation approaches.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_38');
INSERT INTO `paper` VALUES (11484, 'Adversarial Generative Grammars for Human Activity Prediction', '', '', '', '', '', 'In this paper we propose an adversarial generative grammar model for future prediction. The objective is to learn a model that explicitly captures temporal dependencies, providing a capability to forecast multiple, distinct future activities. Our adversarial grammar is designed so that it can learn stochastic production rules from the data distribution, jointly with its latent non-terminal representations. Being able to select multiple production rules during inference leads to different predicted outcomes, thus efficiently modeling many plausible futures. The adversarial generative grammar is evaluated on the Charades, MultiTHUMOS, Human3.6M, and 50 Salads datasets and on two activity prediction tasks: future 3D human pose prediction and future activity prediction. The proposed adversarial grammar outperforms the state-of-the-art approaches, being able to predict much more accurately and further in the future, than prior work. Code will be open sourced.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_30');
INSERT INTO `paper` VALUES (11485, 'Adversarial Learning for Zero-Shot Domain Adaptation', 'Transfer learning', 'Domain adaptation', 'Zero-shot learning', 'Coupled generative adversarial networks', '', 'Zero-shot domain adaptation (ZSDA) is a category of domain adaptation problems where neither data sample nor label is available for parameter learning in the target domain. With the hypothesis that the shift between a given pair of domains is shared across tasks, we propose a new method for ZSDA by transferring domain shift from an irrelevant task (IrT) to the task of interest (ToI). Specifically, we first identify an IrT, where dual-domain samples are available, and capture the domain shift with a coupled generative adversarial networks (CoGAN) in this task. Then, we train a CoGAN for the ToI and restrict it to carry the same domain shift as the CoGAN for IrT does. In addition, we introduce a pair of co-training classifiers to regularize the training procedure of CoGAN in the ToI. The proposed method not only derives machine learning models for the non-available target-domain data, but also synthesizes the data themselves. We evaluate the proposed method on benchmark datasets and achieve the state-of-the-art performances.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_20');
INSERT INTO `paper` VALUES (11486, 'Adversarial Ranking Attack and Defense', '', '', '', '', '', 'Deep Neural Network (DNN) classifiers are vulnerable to adversarial attack, where an imperceptible perturbation could result in misclassification. However, the vulnerability of DNN-based image ranking systems remains under-explored. In this paper, we propose two attacks against deep ranking systems, i.e., Candidate Attack and Query Attack, that can raise or lower the rank of chosen candidates by adversarial perturbations. Specifically, the expected ranking order is first represented as a set of inequalities, and then a triplet-like objective function is designed to obtain the optimal perturbation. Conversely, a defense method is also proposed to improve the ranking system robustness, which can mitigate all the proposed attacks simultaneously. Our adversarial ranking attacks and defense are evaluated on datasets including MNIST, Fashion-MNIST, and Stanford-Online-Products. Experimental results demonstrate that a typical deep ranking system can be effectively compromised by our attacks. Meanwhile, the system robustness can be moderately improved with our defense. Furthermore, the transferable and universal properties of our adversary illustrate the possibility of realistic black-box attack.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_46');
INSERT INTO `paper` VALUES (11487, 'Adversarial Robustness of Open-Set Recognition: Face Recognition and Person Re-identification', 'Adversarial attack', 'Open-world classification', 'Person re-identification', 'Face recognition', '', 'Recent studies show that DNNs are vulnerable to adversarial attacks, in which carefully chosen imperceptible modifications to the inputs lead to incorrect predictions. However most existing attacks focus on closed-set classification, and adversarial attack of open-set recognition has been less investigated. In this paper, we systematically investigate the adversarial robustness of widely used open-set recognition models, namely person re-identification (ReID) and face recognition (FR) models. Specifically, we compare two categories of black-box attacks: transfer-based extensions of standard closed-set attacks and several direct random-search based attacks proposed here. Extensive experiments demonstrate that ReID and FR models are also vulnerable to adversarial attack, and highlight a potential AI trustworthiness problem for these socially important applications.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_9');
INSERT INTO `paper` VALUES (11488, 'Adversarial Robustness on In- and Out-Distribution Improves Explainability', '', '', '', '', '', 'Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art \\(l_2\\)-adversarial robustness on CIFAR10 and maintains better clean accuracy.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_14');
INSERT INTO `paper` VALUES (11489, 'Adversarial Self-supervised Learning for Semi-supervised 3D Action Recognition', 'Semi-supervised 3D action recognition', 'Self-supervised learning', 'Neighborhood consistency', 'Adversarial learning', '', 'We consider the problem of semi-supervised 3D action recognition which has been rarely explored before. Its major challenge lies in how to effectively learn motion representations from unlabeled data. Self-supervised learning (SSL) has been proved very effective at learning representations from unlabeled data in the image domain. However, few effective self-supervised approaches exist for 3D action recognition, and directly applying SSL for semi-supervised learning suffers from misalignment of representations learned from SSL and supervised learning tasks. To address these issues, we present Adversarial Self-Supervised Learning (ASSL), a novel framework that tightly couples SSL and the semi-supervised scheme via neighbor relation exploration and adversarial learning. Specifically, we design an effective SSL scheme to improve the discrimination capability of learned representations for 3D action recognition, through exploring the data relations within a neighborhood. We further propose an adversarial regularization to align the feature distributions of labeled and unlabeled samples. To demonstrate effectiveness of the proposed ASSL in semi-supervised 3D action recognition, we conduct extensive experiments on NTU and N-UCLA datasets. The results confirm its advantageous performance over state-of-the-art semi-supervised methods in the few label regime for 3D action recognition.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_3');
INSERT INTO `paper` VALUES (11490, 'Adversarial Semantic Data Augmentation for Human Pose Estimation', 'Pose estimation', 'Semantic data augmentation', '', '', '', 'Human pose estimation is the task of localizing body keypoints from still images. The state-of-the-art methods suffer from insufficient examples of challenging cases such as symmetric appearance, heavy occlusion and nearby person. To enlarge the amounts of challenging cases, previous methods augmented images by cropping and pasting image patches with weak semantics, which leads to unrealistic appearance and limited diversity. We instead propose Semantic Data Augmentation (SDA), a method that augments images by pasting segmented body parts with various semantic granularity. Furthermore, we propose Adversarial Semantic Data Augmentation (ASDA), which exploits a generative network to dynamically predict tailored pasting configuration. Given off-the-shelf pose estimation network as discriminator, the generator seeks the most confusing transformation to increase the loss of the discriminator while the discriminator takes the generated sample as input and learns from it. The whole pipeline is optimized in an adversarial manner. State-of-the-art results are achieved on challenging benchmarks. The code has been publicly available at https://github.com/Binyr/ASDA.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_36');
INSERT INTO `paper` VALUES (11491, 'Adversarial Shape Perturbations on 3D Point Clouds', 'Adversarial attacks', 'Adversarial defenses', '3D point clouds', 'Robustness', 'Neural networks', 'The importance of training robust neural network grows as 3D data is increasingly utilized in deep learning for vision tasks in robotics, drone control, and autonomous driving. One commonly used 3D data type is 3D point clouds, which describe shape information. We examine the problem of creating robust models from the perspective of the attacker, which is necessary in understanding how neural networks can be exploited. We explore two categories of attacks: distributional attacks that involve imperceptible perturbations to the distribution of points, and shape attacks that involve deforming the shape represented by a point cloud. We explore three possible shape attacks for attacking 3D point cloud classification and show that some of them are able to be effective even against preprocessing steps, like the previously proposed point-removal defenses. (Source code available at https://github.com/Daniel-Liu-c0deb0t/Adversarial-point-perturbations-on-3D-objects).', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_6');
INSERT INTO `paper` VALUES (11492, 'Adversarial T-Shirt! Evading Person Detectors in a Physical World', 'Physical adversarial attack', 'Object detection', 'Deep learning', '', '', 'It is known that deep neural networks (DNNs) are vulnerable to adversarial attacks. The so-called physical adversarial examples deceive DNN-based decision makers by attaching adversarial patches to real objects. However, most of the existing works on physical adversarial attacks focus on static objects such as glass frames, stop signs and images attached to cardboard. In this work, we propose Adversarial T-shirts, a robust physical adversarial example for evading person detectors even if it could undergo non-rigid deformation due to a moving person’s pose changes. To the best of our knowledge, this is the first work that models the effect of deformation for designing physical adversarial examples with respect to non-rigid objects such as T-shirts. We show that the proposed method achieves 74% and 57% attack success rates in the digital and physical worlds respectively against YOLOv2. In contrast, the state-of-the-art physical attack method to fool a person detector only achieves 18% attack success rate. Furthermore, by leveraging min-max optimization, we extend our method to the ensemble attack setting against two object detectors YOLO-v2 and Faster R-CNN simultaneously.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_39');
INSERT INTO `paper` VALUES (11493, 'Adversarial Training Against Location-Optimized Adversarial Patches', '', '', '', '', '', 'Deep neural networks have been shown to be susceptible to adversarial examples – small, imperceptible changes constructed to cause mis-classification in otherwise highly accurate image classifiers. As a practical alternative, recent work proposed so-called adversarial patches: clearly visible, but adversarially crafted rectangular patches in images. These patches can easily be printed and applied in the physical world. While defenses against imperceptible adversarial examples have been studied extensively, robustness against adversarial patches is poorly understood. In this work, we first devise a practical approach to obtain adversarial patches while actively optimizing their location within the image. Then, we apply adversarial training on these location-optimized adversarial patches and demonstrate significantly improved robustness on CIFAR10 and GTSRB. Additionally, in contrast to adversarial training on imperceptible adversarial examples, our adversarial patch training does not reduce accuracy.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_32');
INSERT INTO `paper` VALUES (11494, 'Adversarial Training with Bi-directional Likelihood Regularization for Visual Classification', 'Adversarial training', 'Feature distribution', 'Optimization', '', '', 'Neural networks are vulnerable to adversarial attacks. Practically, adversarial training is by far the most effective approach for enhancing the robustness of neural networks against adversarial examples. The current adversarial training approach aims to maximize the posterior probability for adversarially perturbed training data. However, such a training strategy ignores the fact that the clean data and adversarial examples should have intrinsically different feature distributions despite that they are assigned with the same class label under adversarial training. We propose that this problem can be solved by explicitly modeling the deep feature distribution, for example as a Gaussian Mixture, and then properly introducing the likelihood regularization into the loss function. Specifically, by maximizing the likelihood of features of clean data and minimizing that of adversarial examples simultaneously, the neural network learns a more reasonable feature distribution in which the intrinsic difference between clean data and adversarial examples can be explicitly preserved. We call such a new robust training strategy the adversarial training with bi-directional likelihood regularization (ATBLR) method. Extensive experiments on various datasets demonstrate that the ATBLR method facilitates robust classification of both clean data and adversarial examples, and performs favorably against previous state-of-the-art methods for robust visual classification.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_46');
INSERT INTO `paper` VALUES (11495, 'Adversarial Transfer of Pose Estimation Regression', '', '', '', '', '', 'We address the problem of camera pose estimation in visual localization. Current regression-based methods for pose estimation are trained and evaluated scene-wise. They depend on the coordinate frame of the training dataset and show a low generalization across scenes and datasets. We identify the dataset shift an important barrier to generalization and consider transfer learning as an alternative way towards a better reuse of pose estimation models. We revise domain adaptation techniques for classification and extend them to camera pose estimation, which is a multi-regression task. We develop a deep adaptation network for learning scene-invariant image representations and use adversarial learning to generate such representations for model transfer. We enrich the network with self-supervised learning and use the adaptability theory to validate the existence of scene-invariant representation of images in two given scenes. We evaluate our network on two public datasets, Cambridge Landmarks and 7Scene, demonstrate its superiority over several baselines and compare to the state of the art methods.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_43');
INSERT INTO `paper` VALUES (11496, 'AdvPC: Transferable Adversarial Perturbations on 3D Point Clouds', '', '', '', '', '', 'Deep neural networks are vulnerable to adversarial attacks, in which imperceptible perturbations to their input lead to erroneous network predictions. This phenomenon has been extensively studied in the image domain, and has only recently been extended to 3D point clouds. In this work, we present novel data-driven adversarial attacks against 3D point cloud networks. We aim to address the following problems in current 3D point cloud adversarial attacks: they do not transfer well between different networks, and they are easy to defend against via simple statistical methods. To this extent, we develop a new point cloud attack (dubbed AdvPC) that exploits the input data distribution by adding an adversarial loss, after Auto-Encoder reconstruction, to the objective it optimizes. AdvPC leads to perturbations that are resilient against current defenses, while remaining highly transferable compared to state-of-the-art attacks. We test AdvPC using four popular point cloud networks: PointNet, PointNet++ (MSG and SSG), and DGCNN. Our proposed attack increases the attack success rate by up to 40% for those transferred to unseen networks (transferability), while maintaining a high success rate on the attacked network. AdvPC also increases the ability to break defenses by up to 38% as compared to other baselines on the ModelNet40 dataset. The code is available at https://github.com/ajhamdi/AdvPC.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_15');
INSERT INTO `paper` VALUES (11497, 'AE TextSpotter: Learning Visual and Linguistic Representation for Ambiguous Text Spotting', 'Text spotting', 'Text detection', 'Text recognition', 'Text detection ambiguity', '', 'Scene text spotting aims to detect and recognize the entire word or sentence with multiple characters in natural images. It is still challenging because ambiguity often occurs when the spacing between characters is large or the characters are evenly spread in multiple rows and columns, making many visually plausible groupings of the characters (e.g. “BERLIN” is incorrectly detected as “BERL” and “IN” in Fig. 1(c)). Unlike previous works that merely employed visual features for text detection, this work proposes a novel text spotter, named Ambiguity Eliminating Text Spotter (AE TextSpotter), which learns both visual and linguistic features to significantly reduce ambiguity in text detection. The proposed AE TextSpotter has three important benefits. 1) The linguistic representation is learned together with the visual representation in a framework. To our knowledge, it is the first time to improve text detection by using a language model. 2) A carefully designed language module is utilized to reduce the detection confidence of incorrect text lines, making them easily pruned in the detection stage. 3) Extensive experiments show that AE TextSpotter outperforms other state-of-the-art methods by a large margin. For example, we carefully select a set of extremely ambiguous samples from the IC19-ReCTS dataset, where our approach surpasses other methods by more than 4%.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_27');
INSERT INTO `paper` VALUES (11498, 'AE-OT-GAN: Training GANs from Data Specific Latent Distribution', 'Generative model', 'Optimal transport', 'GAN', 'Continuity', '', 'Though generative adversarial networks (GANs) are prominent models to generate realistic and crisp images, they are unstable to train and suffer from the mode collapse problem. The problems of GANs come from approximating the intrinsic discontinuous distribution transform map with continuous DNNs. The recently proposed AE-OT model addresses the discontinuity problem by explicitly computing the discontinuous optimal transform map in the latent space of the autoencoder. Though have no mode collapse, the generated images by AE-OT are blurry. In this paper, we propose the AE-OT-GAN model to utilize the advantages of the both models: generate high quality images and at the same time overcome the mode collapse problems. Specifically, we firstly embed the low dimensional image manifold into the latent space by autoencoder (AE). Then the extended semi-discrete optimal transport (SDOT) map is used to generate new latent codes. Finally, our GAN model is trained to generate high quality images from the latent distribution induced by the extended SDOT map. The distribution transform map from this dataset related latent distribution to the data distribution will be continuous, and thus can be well approximated by the continuous DNNs. Additionally, the paired data between the latent codes and the real images gives us further restriction about the generator and stabilizes the training process. Experiments on simple MNIST dataset and complex datasets like CIFAR10 and CelebA show the advantages of the proposed method.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_33');
INSERT INTO `paper` VALUES (11499, 'AF2S: An Anchor-Free Two-Stage Tracker Based on a Strong SiamFC Baseline', 'Visual object tracking', 'Siamese network', 'Two-stage tracker', 'Anchor free', '', 'Siamese network based trackers have become a mainstream in visual object tracking. Recently, several high-performance multi-stage trackers have been proposed and some of them adopt SiamRPN for the first-stage region proposal. We argue that an anchor-based region proposal network is not necessary for the tracking task, as a tracker has a strong prior about the location and size of the target. In this paper, we propose a two-stage visual tracker which uses SiamFC for region proposal. SiamFC defines a bounding box by its center, which is a typical anchor-free (AF) network, so we dub our tracker AF2S. As the model size of SiamFC is only about 1/10 that of SiamRPN, AF2S results in a significantly lighter model than its SiamRPN-based counterparts. In the design of AF2S, we first build a strong AlexNet-based SiamFC baseline which improves the AUC on OTB-100 from 0.582 to 0.665. Further, we propose a position-sensitive convolutional layer which can be stacked after SiamFC backbone to increase the robustness of proposals without losing localization precision. Finally, a relation network is used for box refinement. Experimental results show that AF2S achieves the best performance on OTB-100 and VOT-18 among the state-of-the-art trackers which use AlexNet as backbone. On LaSOT-test, AF2S achieves an AUC of 0.480, which is among the first-tier performance even when trackers with more powerful backbone and much larger model size are considered.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_42');
INSERT INTO `paper` VALUES (11500, 'AFOD: Adaptive Focused Discriminative Segmentation Tracker', 'Visual object tracking', 'Adaptive segmentation tracker', 'Discriminative model', '', '', 'Visual object tracking is a fundamental task in computer vision which could be integrated into numerous real-world applications. Traditional object tracking methods focus on providing the bounding box as object position, while some recent trackers start to consider the combination of segmentation module to generate the binary segmentation mask, pursuing more accurate localization. However, how to effectively integrate different information for accurate and robust tracking is an open question. In this paper, we propose a novel Adaptive FOcused Discriminative (AFOD) segmentation tracker with the following advanced components. For localization, a more discriminative light weight online target appearance model is employed to provide robust position estimation. For segmentation, leveraging the backbone semantic feature, the coarse segmentation feature, and the localization feature, an offline trained fine segmentation model with IoU optimization is utilized to generate the accurate high resolution masks. The boundary detection further enhances the segmentation quality. For combination, an adaptive prediction strategy is proposed to better integrate the information from two types of predictions, i.e. box and mask. AFOD achieves leading performance on two tracking benchmarks including the bounding box annotated VOT2018 and the segmentation mask annotated VOT2020, while running close to real-time.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_44');
INSERT INTO `paper` VALUES (11501, 'AgingMapGAN (AMGAN): High-Resolution Controllable Face Aging with Spatially-Aware Conditional GANs', 'Conditional GANs', 'Face aging', 'High-resolution', '', '', 'Existing approaches and datasets for face aging produce results skewed towards the mean, with individual variations and expression wrinkles often invisible or overlooked in favor of global patterns such as the fattening of the face. Moreover, they offer little to no control over the way the faces are aged and can difficultly be scaled to large images, thus preventing their usage in many real-world applications. To address these limitations, we present an approach to change the appearance of a high-resolution image using ethnicity-specific aging information and weak spatial supervision to guide the aging process. We demonstrate the advantage of our proposed method in terms of quality, control, and how it can be used on high-definition images while limiting the computational overhead.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_37');
INSERT INTO `paper` VALUES (11502, 'AIM 2020 Challenge on Efficient Super-Resolution: Methods and Results', '', '', '', '', '', 'This paper reviews the AIM 2020 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The challenge task was to super-resolve an input image with a magnification factor \\(\\times \\)4 based on a set of prior examples of low and corresponding high resolution images. The goal is to devise a network that reduces one or several aspects such as runtime, parameter count, FLOPs, activations, and memory consumption while at least maintaining PSNR of MSRResNet. The track had 150 registered participants, and 25 teams submitted the final results. They gauge the state-of-the-art in efficient single image super-resolution.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_1');
INSERT INTO `paper` VALUES (11503, 'AIM 2020 Challenge on Image Extreme Inpainting', 'Extreme image inpainting', 'Image synthesis', 'Generative modeling', '', '', 'This paper reviews the AIM 2020 challenge on extreme image inpainting. This report focuses on proposed solutions and results for two different tracks on extreme image inpainting: classical image inpainting and semantically guided image inpainting. The goal of track 1 is to inpaint large part of the image with no supervision. Similarly, the goal of track 2 is to inpaint the image by having access to the entire semantic segmentation map of the input. The challenge had 88 and 74 participants, respectively. 11 and 6 teams competed in the final phase of the challenge, respectively. This report gauges current solutions and set a benchmark for future extreme image inpainting methods.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_43');
INSERT INTO `paper` VALUES (11504, 'AIM 2020 Challenge on Learned Image Signal Processing Pipeline', '', '', '', '', '', 'This paper reviews the second AIM learned ISP challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world RAW-to-RGB mapping problem, where to goal was to map the original low-quality RAW images captured by the Huawei P20 device to the same photos obtained with the Canon 5D DSLR camera. The considered task embraced a number of complex computer vision subtasks, such as image demosaicing, denoising, white balancing, color and contrast correction, demoireing, etc. The target metric used in this challenge combined fidelity scores (PSNR and SSIM) with solutions’ perceptual results measured in a user study. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical image signal processing pipeline modeling.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_9');
INSERT INTO `paper` VALUES (11505, 'AIM 2020 Challenge on Real Image Super-Resolution: Methods and Results', '', '', '', '', '', 'This paper introduces the real image Super-Resolution (SR) challenge that was part of the Advances in Image Manipulation (AIM) workshop, held in conjunction with ECCV 2020. This challenge involves three tracks to super-resolve an input image for \\(\\times \\)2, \\(\\times \\)3 and \\(\\times \\)4 scaling factors, respectively. The goal is to attract more attention to realistic image degradation for the SR task, which is much more complicated and challenging, and contributes to real-world image super-resolution applications. 452 participants were registered for three tracks in total, and 24 teams submitted their results. They gauge the state-of-the-art approaches for real image SR in terms of PSNR and SSIM.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_24');
INSERT INTO `paper` VALUES (11506, 'AIM 2020 Challenge on Rendering Realistic Bokeh', '', '', '', '', '', 'This paper reviews the second AIM realistic bokeh effect rendering challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world bokeh simulation problem, where the goal was to learn a realistic shallow focus technique using a large-scale EBB! bokeh dataset consisting of 5K shallow/wide depth-of-field image pairs captured using the Canon 7D DSLR camera. The participants had to render bokeh effect based on only one single frame without any additional data from other cameras or sensors. The target metric used in this challenge combined the runtime and the perceptual quality of the solutions measured in the user study. To ensure the efficiency of the submitted models, we measured their runtime on standard desktop CPUs as well as were running the models on smartphone GPUs. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical bokeh effect rendering problem.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_13');
INSERT INTO `paper` VALUES (11507, 'AIM 2020 Challenge on Video Extreme Super-Resolution: Methods and Results', 'Extreme super-resolution', 'Video restoration', 'Video enhancement', 'Challenge', '', 'This paper reviews the video extreme super-resolution challenge associated with the AIM 2020 workshop at ECCV 2020. Common scaling factors for learned video super-resolution (VSR) do not go beyond factor 4. Missing information can be restored well in this region, especially in HR videos, where the high-frequency content mostly consists of texture details. The task in this challenge is to upscale videos with an extreme factor of 16, which results in more serious degradations that also affect the structural integrity of the videos. A single pixel in the low-resolution (LR) domain corresponds to 256 pixels in the high-resolution (HR) domain. Due to this massive information loss, it is hard to accurately restore the missing information. Track 1 is set up to gauge the state-of-the-art for such a demanding task, where fidelity to the ground truth is measured by PSNR and SSIM. Perceptually higher quality can be achieved in trade-off for fidelity by generating plausible high-frequency content. Track 2 therefore aims at generating visually pleasing results, which are ranked according to human perception, evaluated by a user study. In contrast to single image super-resolution (SISR), VSR can benefit from additional information in the temporal domain. However, this also imposes an additional requirement, as the generated frames need to be consistent along time.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_4');
INSERT INTO `paper` VALUES (11508, 'AIM 2020 Challenge on Video Temporal Super-Resolution', 'Video temporal super-resolution', 'Frame interpolation', '', '', '', 'Videos in the real-world contain various dynamics and motions that may look unnaturally discontinuous in time when the recorded frame rate is low. This paper reports the second AIM challenge on Video Temporal Super-Resolution (VTSR), a.k.a. frame interpolation, with a focus on the proposed solutions, results, and analysis. From low-frame-rate (15 fps) videos, the challenge participants are required to submit higher-frame-rate (30 and 60 fps) sequences by estimating temporally intermediate frames. To simulate realistic and challenging dynamics in the real-world, we employ the REDS_VTSR dataset derived from diverse videos captured in a hand-held camera for training and evaluation purposes. There have been 68 registered participants in the competition, and 5 teams (one withdrawn) have competed in the final testing phase. The winning team proposes the enhanced quadratic video interpolation method and achieves state-of-the-art on the VTSR task.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_2');
INSERT INTO `paper` VALUES (11509, 'AIM 2020: Scene Relighting and Illumination Estimation Challenge', 'Image relighting', 'Illumination estimation', 'Style transfer', '', '', 'We review the AIM 2020 challenge on virtual image relighting and illumination estimation. This paper presents the novel VIDIT dataset used in the challenge and the different proposed solutions and final evaluation results over the 3 challenge tracks. The first track considered one-to-one relighting; the objective was to relight an input photo of a scene with a different color temperature and illuminant orientation (i.e., light source position). The goal of the second track was to estimate illumination settings, namely the color temperature and orientation, from a given image. Lastly, the third track dealt with any-to-any relighting, thus a generalization of the first track. The target color temperature and orientation, rather than being pre-determined, are instead given by a guide image. Participants were allowed to make use of their track 1 and 2 solutions for track 3. The tracks had 94, 52, and 56 registered participants, respectively, leading to 20 confirmed submissions in the final competition stage.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_30');
INSERT INTO `paper` VALUES (11510, 'AiR: Attention with Reasoning Capability', 'Attention', 'Reasoning', 'Eye-tracking dataset', '', '', 'While attention has been an increasingly popular component in deep neural networks to both interpret and boost performance of models, little work has examined how attention progresses to accomplish a task and whether it is reasonable. In this work, we propose an Attention with Reasoning capability (AiR) framework that uses attention to understand and improve the process leading to task outcomes. We first define an evaluation metric based on a sequence of atomic reasoning operations, enabling quantitative measurement of attention that considers the reasoning process. We then collect human eye-tracking and answer correctness data, and analyze various machine and human attentions on their reasoning capability and how they impact task performance. Furthermore, we propose a supervision method to jointly and progressively optimize attention, reasoning, and task performance so that models learn to look at regions of interests by following a reasoning process. We demonstrate the effectiveness of the proposed framework in analyzing and modeling attention with better reasoning capability and task performance. The code and data are available at https://github.com/szzexpoi/AiR.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_6');
INSERT INTO `paper` VALUES (11511, 'ALET (Automated Labeling of Equipment and Tools): A Dataset for Tool Detection and Human Worker Safety Detection', 'Safety detection', 'Tool detection', '', '', '', 'Robots collaborating with humans in realistic environments need to be able to detect the tools that can be used and manipulated. However, there is no available dataset or study that addresses this challenge in real settings. In this paper, we fill this gap with a dataset for detecting farming, gardening, office, stonemasonry, vehicle, woodworking, and workshop tools. The scenes in our dataset are snapshots of sophisticated environments with or without humans using the tools. The scenes we consider introduce several challenges for object detection, including the small scale of the tools, their articulated nature, occlusion, inter-class invariance, etc. Moreover, we train and compare several state of the art deep object detectors (including Faster R-CNN, Cascade R-CNN, YOLOv3, RetinaNet, RepPoint, and FreeAnchor) on our dataset. We observe that the detectors have difficulty in detecting especially small-scale tools or tools that are visually similar to parts of other tools. In addition, we provide a novel, practical safety use case with a deep network which checks whether the human worker is wearing the safety helmet, mask, glass, and glove tools. With the dataset, the code and the trained models, our work provides a basis for further research into tools and their use in robotics applications.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_22');
INSERT INTO `paper` VALUES (11512, 'Aligning Videos in Space and Time', 'Understanding via association', 'Video alignment', 'Visual correspondences', '', '', 'In this paper, we focus on the task of extracting visual correspondences across videos. Given a query video clip from an action class, we aim to align it with training videos in space and time. Obtaining training data for such a fine-grained alignment task is challenging and often ambiguous. Hence, we propose a novel alignment procedure that learns such correspondence in space and time via cross video cycle-consistency. During training, given a pair of videos, we compute cycles that connect patches in a given frame in the first video by matching through frames in the second video. Cycles that connect overlapping patches together are encouraged to score higher than cycles that connect non-overlapping patches. Our experiments on the Penn Action and Pouring datasets demonstrate that the proposed method can successfully learn to correspond semantically similar patches across videos, and learns representations that are sensitive to object and action states.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_16');
INSERT INTO `paper` VALUES (11513, 'All at Once: Temporally Adaptive Multi-frame Interpolation with Advanced Motion Modeling', '', '', '', '', '', 'Recent advances in high refresh rate displays as well as the increased interest in high rate of slow motion and frame up-conversion fuel the demand for efficient and cost-effective multi-frame video interpolation solutions. To that regard, inserting multiple frames between consecutive video frames are of paramount importance for the consumer electronics industry. State-of-the-art methods are iterative solutions interpolating one frame at the time. They introduce temporal inconsistencies and clearly noticeable visual artifacts.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_7');
INSERT INTO `paper` VALUES (11514, 'ALRe: Outlier Detection for Guided Refinement', 'Anchored linear residual', 'Outlier detection', 'Guided refinement', 'Local linear assumption', 'Linear regression', 'Guided refinement is a popular procedure of various image post-processing applications. It produces output image based on input and guided images. Input images are usually flawed estimates containing kinds of noises and outliers, which undermine the edge consistency between input and guided images. As improvements, they are refined into output images with similar intensities of input images and consistent edges of guided images. However, outliers are usually untraceable and simply treated as zero-mean noises, limiting the quality of such refinement. In this paper, we propose a general outlier detection method for guided refinement. We assume local linear relationship between output and guided images to express the expected edge consistency, based on which, the outlier likelihoods of input pixels are measured. The metric is termed as ALRe (anchored linear residual) since it is essentially the residual of local linear regression with an equality constraint exerted on the measured pixel. Valuable features of the ALRe are discussed. Its effectiveness is proven by applications and experiment.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_46');
INSERT INTO `paper` VALUES (11515, 'AMLN: Adversarial-Based Mutual Learning Network for Online Knowledge Distillation', 'Mutual learning network', 'Adversarial-based learning strategy', 'Online knowledge transfer and distillation', '', '', 'Online knowledge distillation has attracted increasing interest recently, which jointly learns teacher and student models or an ensemble of student models simultaneously and collaboratively. On the other hand, existing works focus more on outcome-driven learning according to knowledge like classification probabilities whereas the distilling processes which capture rich and useful intermediate features and information are largely neglected. In this work, we propose an innovative adversarial-based mutual learning network (AMLN) that introduces process-driven learning beyond outcome-driven learning for augmented online knowledge distillation. A block-wise training module is designed which guides the information flow and mutual learning among peer networks adversarially throughout different learning stages, and this spreads until the final network layer which captures more high-level information. AMLN has been evaluated under a variety of network architectures over three widely used benchmark datasets. Extensive experiments show that AMLN achieves superior performance consistently against state-of-the-art knowledge transfer methods.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_10');
INSERT INTO `paper` VALUES (11516, 'Amplifying Key Cues for Human-Object-Interaction Detection', '', '', '', '', '', 'Human-object interaction (HOI) detection aims to detect and recognise how people interact with the objects that surround them. This is challenging as different interaction categories are often distinguished only by very subtle visual differences in the scene. In this paper we introduce two methods to amplify key cues in the image, and also a method to combine these and other cues when considering the interaction between a human and an object. First, we introduce an encoding mechanism for representing the fine-grained spatial layout of the human and object (a subtle cue) and also semantic context (a cue, represented by text embeddings of surrounding objects). Second, we use plausible future movements of humans and objects as a cue to constrain the space of possible interactions. Third, we use a gate and memory architecture as a fusion module to combine the cues. We demonstrate that these three improvements lead to a performance which exceeds prior HOI methods across standard benchmarks by a considerable margin.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_15');
INSERT INTO `paper` VALUES (11517, 'An Analysis of Sketched IRLS for Accelerated Sparse Residual Regression', 'Sparse residual regression', '\\(\\ell _{1}\\) minimization', 'Randomized algorithm', 'Matrix sketching', '', 'This paper studies the problem of sparse residual regression, i.e., learning a linear model using a norm that favors solutions in which the residuals are sparsely distributed. This is a common problem in a wide range of computer vision applications where a linear system has a lot more equations than unknowns and we wish to find the maximum feasible set of equations by discarding unreliable ones. We show that one of the most popular solution methods, iteratively reweighted least squares (IRLS), can be significantly accelerated by the use of matrix sketching. We analyze the convergence behavior of the proposed method and show its efficiency on a range of computer vision applications. The source code for this project can be found at https://github.com/Diwata0909/Sketched_IRLS.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_36');
INSERT INTO `paper` VALUES (11518, 'An Asymmetric Modeling for Action Assessment', '', '', '', '', '', 'Action assessment is a task of assessing the performance of an action. It is widely applicable to many real-world scenarios such as medical treatment and sporting events. However, existing methods for action assessment are mostly limited to individual actions, especially lacking modeling of the asymmetric relations among agents (e.g., between persons and objects); and this limitation undermines their ability to assess actions containingasymmetrically interactive motion patterns, since there always exists subordination between agents in many interactive actions. In this work, we model the asymmetric interactions among agents for action assessment. In particular, we propose an asymmetric interaction module (AIM), to explicitly model asymmetric interactions between intelligent agents within an action, where we group these agents into a primary one (e.g., human) and secondary ones (e.g., objects). We perform experiments on JIGSAWS dataset containing surgical actions, and additionally collect a new dataset, TASD-2, for interactive sporting actions. The experimental results on two interactive action datasets show the effectiveness of our model, and our method achieves state-of-the-art performance. The extended experiment on AQA-7 dataset also demonstrates the generalization capability of our framework to conventional action assessment.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_14');
INSERT INTO `paper` VALUES (11519, 'An Attention-Driven Two-Stage Clustering Method for Unsupervised Person Re-identification', 'Attention', 'Clustering', 'Unsupervised learning', 'Person re-id', '', 'The progressive clustering method and its variants, which iteratively generate pseudo labels for unlabeled data and per form feature learning, have shown great process in unsupervised person re-identification (re-id). However, they have an intrinsic problem of modeling the in-camera variability of images successfully, that is, pedestrian features extracted from the same camera tend to be clustered into the same class. This often results in a non-convergent model in the real world application of clustering based re-id models, leading to degenerated performance. In the present study, we propose an attention-driven two-stage clustering (ADTC) method to solve this problem. Specifically, our method consists of two strategies. Firstly, we use an unsupervised attention kernel to shift the learned features from the image background to the pedestrian foreground, which results in more informative clusters. Secondly, to aid the learning of the attention driven clustering model, we separate the clustering process into two stages. We first use kmeans to generate the centroids of clusters (stage 1) and then apply the k-reciprocal Jaccard distance (KRJD) metric to re-assign data points to each cluster (stage 2). By iteratively learning with the two strategies, the attentive regions are gradually shifted from the background to the foreground and the features become more discriminative. Using two benchmark datasets Market1501 and DukeMTMC, we demonstrate that our model outperforms other state-of-the-art unsupervised approaches for person re-id.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_2');
INSERT INTO `paper` VALUES (11520, 'An Efficient Method for Face Quality Assessment on the Edge', 'Face quality assessment', 'Key frame extraction', 'Face detection', 'Edge processing', '', 'Face recognition applications in practice are composed of two main steps; face detection and feature extraction. In a sole vision-based solution, the first step generates multiple detections for a single identity by ingesting a camera stream. A practical approach on edge devices should prioritize these detections of identities according to their conformity to recognition. In this perspective, we propose a face quality score regression by just appending a single layer to a face landmark detection network. With almost no additional cost, face quality scores are obtained by training this single layer to regress recognition scores with surveillance like augmentations. We implemented the proposed approach on edge GPUs with all face detection pipeline steps, including detection, tracking, and alignment. Comprehensive experiments show the proposed approach’s efficiency through comparison with state-of-the-art face quality regression models on different data sets and real-life scenarios.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_5');
INSERT INTO `paper` VALUES (11521, 'An Efficient Training Framework for Reversible Neural Architectures', 'Reversible neural networks', 'Efficient training', 'Machine learning framework', '', '', 'As machine learning models and dataset escalate in scales rapidly, the huge memory footprint impedes efficient training. Reversible operators can reduce memory consumption by discarding intermediate feature maps in forward computations and recover them via their inverse functions in the backward propagation. They save memory at the cost of computation overhead. However, current implementations of reversible layers mainly focus on saving memory usage with computation overhead neglected. In this work, we formulate the decision problem for reversible operators with training time as the objective function and memory usage as the constraint. By solving this problem, we can maximize the training throughput for reversible neural architectures. Our proposed framework fully automates this decision process, empowering researchers to develop and train reversible neural networks more efficiently.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_17');
INSERT INTO `paper` VALUES (11522, 'An End-to-End OCR Text Re-organization Sequence Learning for Rich-Text Detail Image Comprehension', 'OCR text re-organization', 'Graph neural network', 'Pointer network', '', '', 'Nowadays the description of detailed images helps users know more about the commodities. With the help of OCR technology, the description text can be detected and recognized as auxiliary information to remove the visually impaired users’ comprehension barriers. However, for lack of proper logical structure among these OCR text blocks, it is challenging to comprehend the detailed images accurately. To tackle the above problems, we propose a novel end-to-end OCR text reorganizing model. Specifically, we create a Graph Neural Network with an attention map to encode the text blocks with visual layout features, with which an attention-based sequence decoder inspired by the Pointer Network and a Sinkhorn global optimization will reorder the OCR text into a proper sequence. Experimental results illustrate that our model outperforms the other baselines, and the real experiment of the blind users’ experience shows that our model improves their comprehension.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_6');
INSERT INTO `paper` VALUES (11523, 'An Ensemble Neural Network for Scene Relighting with Light Classification', 'Illumination estimation', 'Scene relighting', 'Convolutional neural network', '', '', 'Illumination is a very important environmental condition. Objects in different illumination environments will present different light and shadow effects. Different kinds of illumination sources will cause different brightness and colors on the surface of the object. The conversion of illumination in two pictures is an interesting and challenging new task, which will be useful in the fields of photography and computer graphics. To solve this problem, we propose a novel solution with three stages: illumination classification, One-to-One Relighting, and Any-to-Any Relighting. Our solution can accurately classify the illumination condition of the input image and can change the direction of the illumination source from any direction to another. We evaluate our methods on VIDIT, a rendered dataset of artificial scenes. The proposed solution produces good results under different light conditions.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_35');
INSERT INTO `paper` VALUES (11524, 'An Ensemble of Epoch-Wise Empirical Bayes for Few-Shot Learning', '', '', '', '', '', 'Few-shot learning aims to train efficient predictive models with a few examples. The lack of training data leads to poor models that perform high-variance or low-confidence predictions. In this paper, we propose to meta-learn the ensemble of epoch-wise empirical Bayes models (E\\(^3\\)BM) to achieve robust predictions. “Epoch-wise” means that each training epoch has a Bayes model whose parameters are specifically learned and deployed. “Empirical” means that the hyperparameters, e.g., used for learning and ensembling the epoch-wise models, are generated by hyperprior learners conditional on task-specific data. We introduce four kinds of hyperprior learners by considering inductive vs. transductive, and epoch-dependent vs. epoch-independent, in the paradigm of meta-learning. We conduct extensive experiments for five-class few-shot tasks on three challenging benchmarks: miniImageNet, tieredImageNet, and FC100, and achieve top performance using the epoch-dependent transductive hyperprior learner, which captures the richest information. Our ablation study shows that both “epoch-wise ensemble” and “empirical” encourage high efficiency and robustness in the model performance (Our code is open-sourced at https://gitlab.mpi-klsb.mpg.de/yaoyaoliu/e3bm).', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_24');
INSERT INTO `paper` VALUES (11525, 'An Exploration of Target-Conditioned Segmentation Methods for Visual Object Trackers', 'Visual object tracking', 'Video object segmentation', 'Target-conditioned segmentation', 'Deep learning', '', 'Visual object tracking is the problem of predicting a target object’s state in a video. Generally, bounding-boxes have been used to represent states, and a surge of effort has been spent by the community to produce efficient causal algorithms capable of locating targets with such representations. As the field is moving towards binary segmentation masks to define objects more precisely, in this paper we propose to extensively explore target-conditioned segmentation methods available in the computer vision community, in order to transform any bounding-box tracker into a segmentation tracker. Our analysis shows that such methods allow trackers to compete with recently proposed segmentation trackers, while performing quasi real-time.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_41');
INSERT INTO `paper` VALUES (11526, 'An Image Enhancing Pattern-Based Sparsity for Real-Time Inference on Mobile Devices', '', '', '', '', '', 'Weight pruning has been widely acknowledged as a straightforward and effective method to eliminate redundancy in Deep Neural Networks (DNN), thereby achieving acceleration on various platforms. However, most of the pruning techniques are essentially trade-offs between model accuracy and regularity which lead to impaired inference accuracy and limited on-device acceleration performance. To solve the problem, we introduce a new sparsity dimension, namely pattern-based sparsity that comprises pattern and connectivity sparsity, and becoming both highly accurate and hardware friendly. With carefully designed patterns, the proposed pruning unprecedentedly and consistently achieves accuracy enhancement and better feature extraction ability on different DNN structures and datasets, and our pattern-aware pruning framework also achieves pattern library extraction, pattern selection, pattern and connectivity pruning and weight training simultaneously. Our approach on the new pattern-based sparsity naturally fits into compiler optimization for highly efficient DNN execution on mobile platforms. To the best of our knowledge, it is the first time that mobile devices achieve real-time inference for the large-scale DNN models thanks to the unique spatial property of pattern-based sparsity and the help of the code generation capability of compilers.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_37');
INSERT INTO `paper` VALUES (11527, 'An Inference Algorithm for Multi-label MRF-MAP Problems with Clique Size 100', 'Submodular minimization', 'Discrete optimization', 'Hybrid methods', 'MRF-MAP', 'Image segmentation', 'In this paper, we propose an algorithm for optimal solutions to submodular higher order multi-label MRF-MAP energy functions which can handle practical computer vision problems with up to 16 labels and cliques of size 100. The algorithm uses a transformation which transforms a multi-label problem to a 2-label problem on a much larger clique. Earlier algorithms based on this transformation could not handle problems larger than 16 labels on cliques of size 4. The proposed algorithm optimizes the resultant 2-label problem using the submodular polyhedron based Min Norm Point algorithm. The task is challenging because the state space of the transformed problem has a very large number of invalid states. For polyhedral based algorithms the presence of invalid states poses a challenge as apart from numerical instability, the transformation also increases the dimension of the polyhedral space making the straightforward use of known algorithms impractical. The approach reported in this paper allows us to bypass the large costs associated with invalid configurations, resulting in a stable, practical, optimal and efficient inference algorithm that, in our experiments, gives high quality outputs on problems like pixel-wise object segmentation and stereo matching.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_16');
INSERT INTO `paper` VALUES (11528, 'An Investigation of Deep Visual Architectures Based on Preprocess Using the Retinal Transform', 'Deep learning', 'Retina', 'Visual cortex', 'CNN', 'Retinal transform', 'This work investigates the utility of a biologically motivated software retina model to pre-process and compress visual information prior to training and classification by means of a deep convolutional neural networks (CNNs) in the context of object recognition in robotics and egocentric perception. We captured a dataset of video clips in a standard office environment by means of a hand-held high-resolution digital camera using uncontrolled illumination. Individual video sequences for each of 20 objects were captured over the observable view hemisphere for each object and several sequences were captured per object to serve training and validation within an object recognition task. A key objective of this project is to investigate appropriate network architectures for processing retina transformed input images and in particular to determine the utility of spatio-temporal CNNs versus simple feed-forward CNNs. A number of different CNN architectures were devised and compared in their classification performance accordingly. The project demonstrated that the image classification task could be conducted with an accuracy exceeding 98% under varying lighting conditions when the object was viewed from distances similar to that when trained.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_32');
INSERT INTO `paper` VALUES (11529, 'An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds', '3D object detection', 'LSTM', 'Point cloud', '', '', 'Detecting objects in 3D LiDAR data is a core technology for autonomous driving and other robotics applications. Although LiDAR data is acquired over time, most of the 3D object detection algorithms propose object bounding boxes independently for each frame and neglect the useful information available in the temporal domain. To address this problem, in this paper we propose a sparse LSTM-based multi-frame 3d object detection algorithm. We use a U-Net style 3D sparse convolution network to extract features for each frame’s LiDAR point-cloud. These features are fed to the LSTM module together with the hidden and memory features from last frame to predict the 3d objects in the current frame as well as hidden and memory features that are passed to the next frame. Experiments on the Waymo Open Dataset show that our algorithm outperforms the traditional frame by frame approach by 7.5% mAP@0.7 and other multi-frame approaches by 1.2% while using less memory and computation per frame. To the best of our knowledge, this is the first work to use an LSTM for 3D object detection in sparse point clouds.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_16');
INSERT INTO `paper` VALUES (11530, 'Anatomy-Aware Siamese Network: Exploiting Semantic Asymmetry for Accurate Pelvic Fracture Detection in X-Ray Images', 'Anatomy-Aware Siamese Network', 'Semantic asymmetry', 'Fracture detection', 'X-ray images', '', 'Visual cues of enforcing bilaterally symmetric anatomies as normal findings are widely used in clinical practice to disambiguate subtle abnormalities from medical images. So far, inadequate research attention has been received on effectively emulating this practice in computer-aided diagnosis (CAD) methods. In this work, we exploit semantic anatomical symmetry or asymmetry analysis in a complex CAD scenario, i.e., anterior pelvic fracture detection in trauma pelvic X-rays (PXRs), where semantically pathological (refer to as fracture) and non-pathological (e.g. pose) asymmetries both occur. Visually subtle yet pathologically critical fracture sites can be missed even by experienced clinicians, when limited diagnosis time is permitted in emergency care. We propose a novel fracture detection framework that builds upon a Siamese network enhanced with a spatial transformer layer to holistically analyze symmetric image features. Image features are spatially formatted to encode bilaterally symmetric anatomies. A new contrastive feature learning component in our Siamese network is designed to optimize the deep image features being more salient corresponding to the underlying semantic asymmetries (caused by pelvic fracture occurrences). Our proposed method have been extensively evaluated on 2,359 PXRs from unique patients (the largest study to-date), and report an area under ROC curve score of 0.9771. This is the highest among state-of-the-art fracture detection methods, with improved clinical indications.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_15');
INSERT INTO `paper` VALUES (11531, 'Angle-Based Search Space Shrinking for Neural Architecture Search', 'Angle', 'Search space shrinking', 'NAS', '', '', 'In this work, we present a simple and general search space shrinking method, called Angle-Based search space Shrinking (ABS), for Neural Architecture Search (NAS). Our approach progressively simplifies the original search space by dropping unpromising candidates, thus can reduce difficulties for existing NAS methods to find superior architectures. In particular, we propose an angle-based metric to guide the shrinking process. We provide comprehensive evidences showing that, in weight-sharing supernet, the proposed metric is more stable and accurate than accuracy-based and magnitude-based metrics to predict the capability of child models. We also show that the angle-based metric can converge fast while training supernet, enabling us to get promising shrunk search spaces efficiently. ABS can easily apply to most of NAS approaches (e.g. SPOS, FairNAS, ProxylessNAS, DARTS and PDARTS). Comprehensive experiments show that ABS can dramatically enhance existing NAS approaches by providing a promising shrunk search space.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_8');
INSERT INTO `paper` VALUES (11532, 'Anti-bandit Neural Architecture Search for Model Defense', 'Neural architecture search (NAS)', 'Bandit', 'Adversarial defense', '', '', 'Deep convolutional neural networks (DCNNs) have dominated as the best performers in machine learning, but can be challenged by adversarial attacks. In this paper, we defend against adversarial attacks using neural architecture search (NAS) which is based on a comprehensive search of denoising blocks, weight-free operations, Gabor filters and convolutions. The resulting anti-bandit NAS (ABanditNAS) incorporates a new operation evaluation measure and search process based on the lower and upper confidence bounds (LCB and UCB). Unlike the conventional bandit algorithm using UCB for evaluation only, we use UCB to abandon arms for search efficiency and LCB for a fair competition between arms. Extensive experiments demonstrate that ABanditNAS is about twice as fast as the state-of-the-art NAS method, while achieving an \\(8.73\\%\\) improvement over prior arts on CIFAR-10 under PGD-7.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_5');
INSERT INTO `paper` VALUES (11533, 'API-Net: Robust Generative Classifier via a Single Discriminator', 'Deep learning', 'Neural networks', 'Adversarial defense', 'Adversarial training', 'Generative classifier', 'Robustness of deep neural network classifiers has been attracting increased attention. As for the robust classification problem, a generative classifier typically models the distribution of inputs and labels, and thus can better handle off-manifold examples at the cost of a concise structure. On the contrary, a discriminative classifier only models the conditional distribution of labels given inputs, but benefits from effective optimization owing to its succinct structure. This work aims for a solution of generative classifiers that can profit from the merits of both. To this end, we propose an Anti-Perturbation Inference (API) method, which searches for anti-perturbations to maximize the lower bound of the joint log-likelihood of inputs and classes. By leveraging the lower bound to approximate Bayes’ rule, we construct a generative classifier Anti-Perturbation Inference Net (API-Net) upon a single discriminator. It takes advantage of the generative properties to tackle off-manifold examples while maintaining a succinct structure for effective optimization. Experiments show that API successfully neutralizes adversarial perturbations, and API-Net consistently outperforms state-of-the-art defenses on prevailing benchmarks, including CIFAR-10, MNIST, and SVHN.(Our code is available at github.com/dongxinshuai/API-Net.).', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_23');
INSERT INTO `paper` VALUES (11534, 'Appearance Consensus Driven Self-supervised Human Mesh Recovery', '', '', '', '', '', 'We present a self-supervised human mesh recovery framework to infer human pose and shape from monocular images in the absence of any paired supervision. Recent advances have shifted the interest towards directly regressing parameters of a parametric human model by supervising them on large-scale datasets with 2D landmark annotations. This limits the generalizability of such approaches to operate on images from unlabeled wild environments. Acknowledging this we propose a novel appearance consensus driven self-supervised objective. To effectively disentangle the foreground (FG) human we rely on image pairs depicting the same person (consistent FG) in varied pose and background (BG) which are obtained from unlabeled wild videos. The proposed FG appearance consistency objective makes use of a novel, differentiable Color-recovery module to obtain vertex colors without the need for any appearance network; via efficient realization of color-picking and reflectional symmetry. We achieve state-of-the-art results on the standard model-based 3D pose estimation benchmarks at comparable supervision levels. Furthermore, the resulting colored mesh prediction opens up the usage of our framework for a variety of appearance-related tasks beyond the pose and shape estimation, thus establishing our superior generalizability.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_46');
INSERT INTO `paper` VALUES (11535, 'Appearance-Preserving 3D Convolution for Video-Based Person Re-identification', 'Video-based person re-identification', 'Temporal appearance misalignment', 'Appearance-Preserving 3D Convolution', '', '', 'Due to the imperfect person detection results and posture changes, temporal appearance misalignment is unavoidable in video-based person re-identification (ReID). In this case, 3D convolution may destroy the appearance representation of person video clips, thus it is harmful to ReID. To address this problem, we propose Appearance-Preserving 3D Convolution (AP3D), which is composed of two components: an Appearance-Preserving Module (APM) and a 3D convolution kernel. With APM aligning the adjacent feature maps in pixel level, the following 3D convolution can model temporal information on the premise of maintaining the appearance representation quality. It is easy to combine AP3D with existing 3D ConvNets by simply replacing the original 3D convolution kernels with AP3Ds. Extensive experiments demonstrate the effectiveness of AP3D for video-based ReID and the results on three widely used datasets surpass the state-of-the-arts. Code is available at: https://github.com/guxinqian/AP3D.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_14');
INSERT INTO `paper` VALUES (11536, 'APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection', 'Adversarial attacks', 'Adversarial defense', 'Datasets and evaluation', 'Object detection', '', 'Physical adversarial attacks threaten to fool object detection systems, but reproducible research on the real-world effectiveness of physical patches and how to defend against them requires a publicly available benchmark dataset. We present APRICOT, a collection of over 1,000 annotated photographs of printed adversarial patches in public locations. The patches target several object categories for three COCO-trained detection models, and the photos represent natural variation in position, distance, lighting conditions, and viewing angle. Our analysis suggests that maintaining adversarial robustness in uncontrolled settings is highly challenging but that it is still possible to produce targeted detections under white-box and sometimes black-box settings. We establish baselines for defending against adversarial patches via several methods, including using a detector supervised with synthetic data and using unsupervised methods such as kernel density estimation, Bayesian uncertainty, and reconstruction error. Our results suggest that adversarial patches can be effectively flagged, both in a high-knowledge, attack-specific scenario and in an unsupervised setting where patches are detected as anomalies in natural images. This dataset and the described experiments provide a benchmark for future research on the effectiveness of and defenses against physical adversarial objects in the wild. The APRICOT project page and dataset are available at apricot.mitre.org.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_3');
INSERT INTO `paper` VALUES (11537, 'AR-Net: Adaptive Frame Resolution for Efficient Action Recognition', 'Efficient action recognition', 'Multi-resolution processing', 'Adaptive learning', '', '', 'Action recognition is an open and challenging problem in computer vision. While current state-of-the-art models offer excellent recognition results, their computational expense limits their impact for many real-world applications. In this paper, we propose a novel approach, called AR-Net (Adaptive Resolution Network), that selects on-the-fly the optimal resolution for each frame conditioned on the input for efficient action recognition in long untrimmed videos. Specifically, given a video frame, a policy network is used to decide what input resolution should be used for processing by the action recognition model, with the goal of improving both accuracy and efficiency. We efficiently train the policy network jointly with the recognition model using standard back-propagation. Extensive experiments on several challenging action recognition benchmark datasets well demonstrate the efficacy of our proposed approach over state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AR-Net.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_6');
INSERT INTO `paper` VALUES (11538, 'Arbitrary-Oriented Object Detection with Circular Smooth Label', 'Oriented object detection', 'Circular smooth label', '', '', '', 'Arbitrary-oriented object detection has recently attracted increasing attention in vision for their importance in aerial imagery, scene text, and face etc. In this paper, we show that existing regression-based rotation detectors suffer the problem of discontinuous boundaries, which is directly caused by angular periodicity or corner ordering. By a careful study, we find the root cause is that the ideal predictions are beyond the defined range. We design a new rotation detection baseline, to address the boundary problem by transforming angular prediction from a regression problem to a classification task with little accuracy loss, whereby high-precision angle classification is devised in contrast to previous works using coarse-granularity in rotation detection. We also propose a circular smooth label (CSL) technique to handle the periodicity of the angle and increase the error tolerance to adjacent angles. We further introduce four window functions in CSL and explore the effect of different window radius sizes on detection performance. Extensive experiments and visual analysis on two large-scale public datasets for aerial images i.e. DOTA, HRSC2016, as well as scene text dataset ICDAR2015 and MLT, show the effectiveness of our approach. The code is public available at https://github.com/Thinklab-SJTU/CSL_RetinaNet_Tensorflow.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_40');
INSERT INTO `paper` VALUES (11539, 'Are Labels Necessary for Neural Architecture Search?', 'Neural architecture search', 'Unsupervised learning', '', '', '', 'Existing neural network architectures in computer vision—whether designed by humans or by machines—were typically found using both images and their associated labels. In this paper, we ask the question: can we find high-quality neural architectures using only images, but no human-annotated labels? To answer this question, we first define a new setup called Unsupervised Neural Architecture Search (UnNAS). We then conduct two sets of experiments. In sample-based experiments, we train a large number (500) of diverse architectures with either supervised or unsupervised objectives, and find that the architecture rankings produced with and without labels are highly correlated. In search-based experiments, we run a well-established NAS algorithm (DARTS) using various unsupervised objectives, and report that the architectures searched without labels can be competitive to their counterparts searched with labels. Together, these results reveal the potentially surprising finding that labels are not necessary, and the image statistics alone may be sufficient to identify good neural architectures.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_46');
INSERT INTO `paper` VALUES (11540, 'AsArcFace: Asymmetric Additive Angular Margin Loss for Fairface Recognition', 'Asymmetric-arc-loss', 'AsArcFace', 'Face recognition', 'Metric learning', '', 'Fairface recognition aims to the mitigate the bias between different attributes in face recognition task while maintaining the state-of-art accurancy. It is a challenging task due to high variances between different attributes and unbalancement of data. In this work, we provide an approach to make a fairface recognition by using asymmetric-arc-loss training and multi-step finetuning. First, we train a general model with an asymmetric-arc-loss, and then, we make a mutli-step finetuning to get higher auc and lower bias. Besides, we propose another viewpoint on reducing the bias and use bag of tricks such as reranking, boundary cut and hard-sample model ensembling to improve the performance. Our approach achieved the first place at ECCV 2020 ChaLearn Looking at People Fair Face Recognition Challenge.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_33');
INSERT INTO `paper` VALUES (11541, 'AssembleNet++: Assembling Modality Representations via Attention Connections', 'Video understanding', 'Activity recognition', 'Attention', '', '', 'We create a family of powerful video models which are able to: (i) learn interactions between semantic object information and raw appearance and motion features, and (ii) deploy attention in order to better learn the importance of features at each convolutional block of the network. A new network component named peer-attention is introduced, which dynamically learns the attention weights using another block or input modality. Even without pre-training, our models outperform the previous work on standard public activity recognition datasets with continuous videos, establishing new state-of-the-art. We also confirm that our findings of having neural connections from the object modality and the use of peer-attention is generally applicable for different existing architectures, improving their performances. We name our model explicitly as AssembleNet++. The code will be available at: https://sites.google.com/corp/view/assemblenet/.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_39');
INSERT INTO `paper` VALUES (11542, 'Assessing Box Merging Strategies and Uncertainty Estimation Methods in Multimodel Object Detection', 'Uncertainty estimation', 'Deep ensembles', 'Object detection', '', '', 'This paper examines the impact of different box merging strategies for sampling-based uncertainty estimation methods in object detection. Also, a comparison between the almost exclusively used softmax confidence scores and the predicted variances on the quality of the final predictions estimates is presented. The results suggest that estimated variances are a stronger predictor for the detection quality. However, variance-based merging strategies do not improve significantly over the confidence-based alternative for the given setup. In contrast, we show that different methods to estimate the uncertainty of the predictions have a significant influence on the quality of the ensembling outcome. Since mAP does not reward uncertainty estimates, such improvements were only noticeable on the resulting PDQ scores.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_1');
INSERT INTO `paper` VALUES (11543, 'Associative Alignment for Few-Shot Image Classification', 'Associative alignment', 'Few-shot image classification', '', '', '', 'Few-shot image classification aims at training a model from only a few examples for each of the “novel” classes. This paper proposes the idea of associative alignment for leveraging part of the base data by aligning the novel training instances to the closely related ones in the base training set. This expands the size of the effective novel training set by adding extra “related base” instances to the few novel ones, thereby allowing a constructive fine-tuning. We propose two associative alignment strategies: 1) a metric-learning loss for minimizing the distance between related base samples and the centroid of novel instances in the feature space, and 2) a conditional adversarial alignment loss based on the Wasserstein distance. Experiments on four standard datasets and three backbones demonstrate that combining our centroid-based alignment loss results in absolute accuracy improvements of 4.4%, 1.2%, and 6.2% in 5-shot learning over the state of the art for object recognition, fine-grained classification, and cross-domain adaptation, respectively .', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_2');
INSERT INTO `paper` VALUES (11544, 'Associative3D: Volumetric Reconstruction from Sparse Views', '3D reconstruction', '', '', '', '', 'This paper studies the problem of 3D volumetric reconstruction from two views of a scene with an unknown camera. While seemingly easy for humans, this problem poses many challenges for computers since it requires simultaneously reconstructing objects in the two views while also figuring out their relationship. We propose a new approach that estimates reconstructions, distributions over the camera/object and camera/camera transformations, as well as an inter-view object affinity matrix. This information is then jointly reasoned over to produce the most likely explanation of the scene. We train and test our approach on a dataset of indoor scenes, and rigorously evaluate the merits of our joint reasoning approach. Our experiments show that it is able to recover reasonable scenes from sparse views, while the problem is still challenging. Project site: https://jasonqsy.github.io/Associative3D.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_9');
INSERT INTO `paper` VALUES (11545, 'Asymmetric Two-Stream Architecture for Accurate RGB-D Saliency Detection', 'Saliency detection', 'Flow ladder', 'Depth attention', '', '', 'Most existing RGB-D saliency detection methods adopt symmetric two-stream architectures for learning discriminative RGB and depth representations. In fact, there is another level of ambiguity that is often overlooked: if RGB and depth data are necessary to fit into the same network. In this paper, we propose an asymmetric two-stream architecture taking account of the inherent differences between RGB and depth data for saliency detection. First, we design a flow ladder module (FLM) for the RGB stream to fully extract global and local information while maintaining the saliency details. This is achieved by constructing four detail-transfer branches, each of which preserves the detail information and receives global location information from representations of other vertical parallel branches in an evolutionary way. Second, we propose a novel depth attention module (DAM) to ensure depth features with high discriminative power in location and spatial structure being effectively utilized when combined with RGB features in challenging scenes. The depth features can also discriminatively guide the RGB features via our proposed DAM to precisely locate the salient objects. Extensive experiments demonstrate that our method achieves superior performance over 13 state-of-the-art RGB-D approaches on the 7 datasets. Our code will be publicly available.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_23');
INSERT INTO `paper` VALUES (11546, 'Asynchronous Interaction Aggregation for Action Detection', 'Action detection', 'Video understanding', 'Interaction', 'Memory', '', 'Understanding interaction is an essential part of video action detection. We propose the Asynchronous Interaction Aggregation network (AIA) that leverages different interactions to boost action detection. There are two key designs in it: one is the Interaction Aggregation structure (IA) adopting a uniform paradigm to model and integrate multiple types of interaction; the other is the Asynchronous Memory Update algorithm (AMU) that enables us to achieve better performance by modeling very long-term interaction dynamically without huge computation cost. We provide empirical evidence to show that our network can gain notable accuracy from the integrative interactions and is easy to train end-to-end. Our method reports the new state-of-the-art performance on AVA dataset, with 3.7 mAP gain (\\(12.6\\%\\) relative improvement) on validation split comparing to our strong baseline. The results on datasets UCF101-24 and EPIC-Kitchens further illustrate the effectiveness of our approach. Source code will be made public at: https://github.com/MVIG-SJTU/AlphAction.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_5');
INSERT INTO `paper` VALUES (11547, 'ATG-PVD: Ticketing Parking Violations on a Drone', '', '', '', '', '', 'In this paper, we introduce a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, our proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by our proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by our investigation module after drone re-localization.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_32');
INSERT INTO `paper` VALUES (11548, 'AtlantaNet: Inferring the 3D Indoor Layout from a Single \\(360^\\circ \\) Image Beyond the Manhattan World Assumption', '3D floor plan recovery', 'Panoramic images', '360 images', 'Data-driven reconstruction', 'Structured indoor reconstruction', 'We introduce a novel end-to-end approach to predict a 3D room layout from a single panoramic image. Compared to recent state-of-the-art works, our method is not limited to Manhattan World environments, and can reconstruct rooms bounded by vertical walls that do not form right angles or are curved – i.e., Atlanta World models. In our approach, we project the original gravity-aligned panoramic image on two horizontal planes, one above and one below the camera. This representation encodes all the information needed to recover the Atlanta World 3D bounding surfaces of the room in the form of a 2D room footprint on the floor plan and a room height. To predict the 3D layout, we propose an encoder-decoder neural network architecture, leveraging Recurrent Neural Networks (RNNs) to capture long-range geometric patterns, and exploiting a customized training strategy based on domain-specific knowledge. The experimental results demonstrate that our method outperforms state-of-the-art solutions in prediction accuracy, in particular in cases of complex wall layouts or curved wall footprints.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_26');
INSERT INTO `paper` VALUES (11549, 'Atlas: End-to-End 3D Scene Reconstruction from Posed Images', 'Multiview stereo', 'TSDF', '3D reconstruction', '', '', 'We present an end-to-end 3D reconstruction method for a scene by directly regressing a truncated signed distance function (TSDF) from a set of posed RGB images. Traditional approaches to 3D reconstruction rely on an intermediate representation of depth maps prior to estimating a full 3D model of a scene. We hypothesize that a direct regression to 3D is more effective. A 2D CNN extracts features from each image independently which are then back-projected and accumulated into a voxel volume using the camera intrinsics and extrinsics. After accumulation, a 3D CNN refines the accumulated features and predicts the TSDF values. Additionally, semantic segmentation of the 3D model is obtained without significant computation. This approach is evaluated on the Scannet dataset where we significantly outperform state-of-the-art baselines (deep multiview stereo followed by traditional TSDF fusion) both quantitatively and qualitatively. We compare our 3D semantic segmentation to prior methods that use a depth sensor since no previous work attempts the problem with only RGB input.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_25');
INSERT INTO `paper` VALUES (11550, 'Attend and Segment: Attention Guided Active Semantic Segmentation', 'Visual attention', 'Active exploration', 'Partial observability', 'Semantic segmentation', '', 'In a dynamic environment, an agent with a limited field of view/resource cannot fully observe the scene before attempting to parse it. The deployment of common semantic segmentation architectures is not feasible in such settings. In this paper we propose a method to gradually segment a scene given a sequence of partial observations. The main idea is to refine an agent’s understanding of the environment by attending the areas it is most uncertain about. Our method includes a self-supervised attention mechanism and a specialized architecture to maintain and exploit spatial memory maps for filling-in the unseen areas in the environment. The agent can select and attend an area while relying on the cues coming from the visited areas to hallucinate the other parts. We reach a mean pixel-wise accuracy of \\(78.1\\%\\), \\(80.9\\%\\) and \\(76.5\\%\\) on CityScapes, CamVid, and Kitti datasets by processing only \\(18\\%\\) of the image pixels (10 retina-like glimpses). We perform an ablation study on the number of glimpses, input image size and effectiveness of retina-like glimpses. We compare our method to several baselines and show that the optimal results are achieved by having access to a very low resolution view of the scene at the first timestep.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_19');
INSERT INTO `paper` VALUES (11551, 'Attention Deeplabv3+: Multi-level Context Attention Mechanism for Skin Lesion Segmentation', 'Medical image segmentation', 'Deeplabv3+', 'Attention mechanism', '', '', 'Skin lesion segmentation is a challenging task due to the large variation of anatomy across different cases. In the last few years, deep learning frameworks have shown high performance in image segmentation. In this paper, we propose Attention Deeplabv3+, an extended version of Deeplabv3+ for skin lesion segmentation by employing the idea of attention mechanism in two stages. We first capture the relationship between the channels of a set of feature maps by assigning a weight for each channel (i.e., channels attention). Channel attention allows the network to emphasize more on the informative and meaningful channels by a context gating mechanism. We also exploit the second level attention strategy to integrate different layers of the atrous convolution. It helps the network to focus on the more relevant field of view to the target. The proposed model is evaluated on three datasets ISIC 2017, ISIC 2018, and \\(PH^2\\), achieving state-of-the-art performance.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_16');
INSERT INTO `paper` VALUES (11552, 'Attention Enhanced Single Stage Multimodal Reasoner', 'Attention', 'Multimodal', 'Self-driving', '', '', 'In this paper, we propose an Attention Enhanced Single Stage Multimodal Reasoner (ASSMR) to tackle the object referral task in the self-driving car scenario. We extract features from each modality and establish attention mechanisms to jointly process them. The Key Words Extractor (KWE) is used to extract the attribute and position/scale information of the target in the command, which are used to score the corresponding features through the Position/Scale Attention Module (P/SAM) and the Object Attention Module (OAM). Based on the attention mechanism, the effective part of the position/scale feature, the object attribute feature and the semantic feature of the command is enhanced. Finally, we map different features to a common embedding space to predict the final result. Our method is based on the simplified version of the Talk2Car dataset, and scored on 66.4 AP50 on the test set, while using the official region proposals.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_5');
INSERT INTO `paper` VALUES (11553, 'Attention Guided Anomaly Localization in Images', 'Guided attention', 'Anomaly localization', 'Convolutional adversarial variational autoencoder', '', '', 'Anomaly localization is an important problem in computer vision which involves localizing anomalous regions within images with applications in industrial inspection, surveillance, and medical imaging. This task is challenging due to the small sample size and pixel coverage of the anomaly in real-world scenarios. Most prior works need to use anomalous training images to compute a class-specific threshold to localize anomalies. Without the need of anomalous training images, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss where we encourage CAVGA to focus on all normal regions in the image. Furthermore, in the weakly-supervised setting we propose a complementary guided attention loss, where we encourage the attention map to focus on all normal regions while minimizing the attention map corresponding to anomalous regions in the image. CAVGA outperforms the state-of-the-art (SOTA) anomaly localization methods on MVTec Anomaly Detection (MVTAD), modified ShanghaiTech Campus (mSTC) and Large-scale Attention based Glaucoma (LAG) datasets in the unsupervised setting and when using only 2% anomalous images in the weakly-supervised setting. CAVGA also outperforms SOTA anomaly detection methods on the MNIST, CIFAR-10, Fashion-MNIST, MVTAD, mSTC and LAG datasets.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_29');
INSERT INTO `paper` VALUES (11554, 'Attention-Based Query Expansion Learning', 'Image retrieval', 'Query expansion learning', 'Attention-based aggregation', '', '', 'Query expansion is a technique widely used in image search consisting in combining highly ranked images from an original query into an expanded query that is then reissued, generally leading to increased recall and precision. An important aspect of query expansion is choosing an appropriate way to combine the images into a new query. Interestingly, despite the undeniable empirical success of query expansion, ad-hoc methods with different caveats have dominated the landscape, and not a lot of research has been done on learning how to do query expansion. In this paper we propose a more principled framework to query expansion, where one trains, in a discriminative manner, a model that learns how images should be aggregated to form the expanded query. Within this framework, we propose a model that leverages a self-attention mechanism to effectively learn how to transfer information between the different images before aggregating them. Our approach obtains higher accuracy than existing approaches on standard benchmarks. More importantly, our approach is the only one that consistently shows high accuracy under different regimes, overcoming caveats of existing methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_11');
INSERT INTO `paper` VALUES (11555, 'Attention-Driven Dynamic Graph Convolutional Network for Multi-label Image Recognition', 'Multi-label image recognition', 'Semantic attention', 'Label dependency', 'Dynamic graph convolutional network', '', 'Recent studies often exploit Graph Convolutional Network (GCN) to model label dependencies to improve recognition accuracy for multi-label image recognition. However, constructing a graph by counting the label co-occurrence possibilities of the training data may degrade model generalizability, especially when there exist occasional co-occurrence objects in test images. Our goal is to eliminate such bias and enhance the robustness of the learnt features. To this end, we propose an Attention-Driven Dynamic Graph Convolutional Network (ADD-GCN) to dynamically generate a specific graph for each image. ADD-GCN adopts a Dynamic Graph Convolutional Network (D-GCN) to model the relation of content-aware category representations that are generated by a Semantic Attention Module (SAM). Extensive experiments on public multi-label benchmarks demonstrate the effectiveness of our method, which achieves mAPs of 85.2%, 96.0%, and 95.5% on MS-COCO, VOC2007, and VOC2012, respectively, and outperforms current state-of-the-art methods with a clear margin.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_39');
INSERT INTO `paper` VALUES (11556, 'AttentionNAS: Spatiotemporal Attention Cell Search for Video Classification', 'Attention', 'Video classification', 'Neural architecture search', '', '', 'Convolutional operations have two limitations: (1) do not explicitly model where to focus as the same filter is applied to all the positions, and (2) are unsuitable for modeling long-range dependencies as they only operate on a small neighborhood. While both limitations can be alleviated by attention operations, many design choices remain to be determined to use attention, especially when applying attention to videos. Towards a principled way of applying attention to videos, we address the task of spatiotemporal attention cell search. We propose a novel search space for spatiotemporal attention cells, which allows the search algorithm to flexibly explore various design choices in the cell. The discovered attention cells can be seamlessly inserted into existing backbone networks, e.g., I3D or S3D, and improve video classification accuracy by more than 2% on both Kinetics-600 and MiT datasets. The discovered attention cells outperform non-local blocks on both datasets, and demonstrate strong generalization across different modalities, backbones, and datasets. Inserting our attention cells into I3D-R50 yields state-of-the-art performance on both datasets.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_27');
INSERT INTO `paper` VALUES (11557, 'Attentive Normalization', '', '', '', '', '', 'In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between the two schema and present Attentive Normalization (AN). Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weighted-sum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging channel-wise feature attention. In experiments, we test the proposed AN using four representative neural architectures in the ImageNet-1000 classification benchmark and the MS-COCO 2017 object detection and instance segmentation benchmark. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5% and 2.7%, and absolute increase up to 1.8% and 2.2% for bounding box and mask AP in MS-COCO respectively. We observe that the proposed AN provides a strong alternative to the widely used Squeeze-and-Excitation (SE) module. The source codes are publicly available at the ImageNet Classification Repo and the MS-COCO Detection and Segmentation Repo.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_5');
INSERT INTO `paper` VALUES (11558, 'Attentive Prototype Few-Shot Learning with Capsule Network-Based Embedding', 'Few-shot learning', 'Meta learning', 'Capsule network', 'Feature embedding', 'Attentive prototype learning', 'Few-shot learning, namely recognizing novel categories with a very small amount of training examples, is a challenging area of machine learning research. Traditional deep learning methods require massive training data to tune the huge number of parameters, which is often impractical and prone to over-fitting. In this work, we further research on the well-known few-shot learning method known as prototypical networks for better performance. Our contributions include (1) a new embedding structure to encode relative spatial relationships between features by applying a capsule network; (2) a new triplet loss designated to enhance the semantic feature embedding where similar samples are close to each other while dissimilar samples are farther apart; and (3) an effective non-parametric classifier termed attentive prototypes in place of the simple prototypes in current few-shot learning. The proposed attentive prototype aggregates all of the instances in a support class which are weighted by their importance, defined by the reconstruction error for a given query. The reconstruction error allows the classification posterior probability to be estimated, which corresponds to the classification confidence score. Extensive experiments on three benchmark datasets demonstrate that our approach is effective for the few-shot classification task.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_15');
INSERT INTO `paper` VALUES (11559, 'AttnGrounder: Talking to Cars with Attention', 'Object detection', 'Visual grounding', 'Attention mechanism', '', '', 'We propose the Attention Grounder (AttnGrounder), a single-stage end-to-end trainable model for the task of visual grounding. Visual grounding aims to localize a specific object in an image based on a given natural language text query. Unlike previous methods that use the same text representation for every image region, we use a visual-text attention module that relates each word in the given query with every region in the corresponding image, constructing a region dependent text representation. Furthermore, to improve the localization ability of our model, we use a visual-text attention module that generates an attention mask around the referred object. The attention mask is trained as an auxiliary task using a rectangular mask generated with the provided ground-truth coordinates. We evaluate the AttnGrounder on the Talk2Car dataset and show an improvement of 3.26% over the existing methods. The code is available at https://github.com/i-m-vivek/AttnGrounder.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_6');
INSERT INTO `paper` VALUES (11560, 'Attract, Perturb, and Explore: Learning a Feature Alignment Network for Semi-supervised Domain Adaptation', 'Domain adaptation', 'Semi-supervised learning', '', '', '', 'Although unsupervised domain adaptation methods have been widely adopted across several computer vision tasks, it is more desirable if we can exploit a few labeled data from new domains encountered in a real application. The novel setting of the semi-supervised domain adaptation (SSDA) problem shares the challenges with the domain adaptation problem and the semi-supervised learning problem. However, a recent study shows that conventional domain adaptation and semi-supervised learning methods often result in less effective or negative transfer in the SSDA problem. In order to interpret the observation and address the SSDA problem, in this paper, we raise the intra-domain discrepancy issue within the target domain, which has never been discussed so far. Then, we demonstrate that addressing the intra-domain discrepancy leads to the ultimate goal of the SSDA problem. We propose an SSDA framework that aims to align features via alleviation of the intra-domain discrepancy. Our framework mainly consists of three schemes, i.e., attraction, perturbation, and exploration. First, the attraction scheme globally minimizes the intra-domain discrepancy within the target domain. Second, we demonstrate the incompatibility of the conventional adversarial perturbation methods with SSDA. Then, we present a domain adaptive adversarial perturbation scheme, which perturbs the given target samples in a way that reduces the intra-domain discrepancy. Finally, the exploration scheme locally aligns features in a class-wise manner complementary to the attraction scheme by selectively aligning unlabeled target features complementary to the perturbation scheme. We conduct extensive experiments on domain adaptation benchmark datasets such as DomainNet, Office-Home, and Office. Our method achieves state-of-the-art performances on all datasets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_35');
INSERT INTO `paper` VALUES (11561, 'Attributional Robustness Training Using Input-Gradient Spatial Alignment', 'Attributional robustness', 'Adversarial robustness', 'Explainable deep learning', '', '', 'Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it has been shown that the explanations could be manipulated easily by adding visually imperceptible perturbations to the input while keeping the model’s prediction intact. In this work, we study the problem of attributional robustness (i.e. models having robust explanations) by showing an upper bound for attributional vulnerability in terms of spatial correlation between the input image and its explanation map. We propose a training methodology that learns robust features by minimizing this upper bound using soft-margin triplet loss. Our methodology of robust attribution training (ART) achieves the new state-of-the-art attributional robustness measure by a margin of \\(\\approx \\)6–18\\(\\%\\) on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust training technique (ART) in the downstream task of weakly supervised object localization by achieving the new state-of-the-art performance on CUB-200 dataset.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_31');
INSERT INTO `paper` VALUES (11562, 'AUTO3D: Novel View Synthesis Through Unsupervisely Learned Variational Viewpoint and Global 3D Representation', 'Unsupervised novel view synthesis', 'Viewer-centered coordinates', 'Variational viewpoints', 'Global 3D representation', '', 'This paper targets on learning-based novel view synthesis from a single or limited 2D images without the pose supervision. In the viewer-centered coordinates, we construct an end-to-end trainable conditional variational framework to disentangle the unsupervisely learned relative-pose/rotation and implicit global 3D representation (shape, texture and the origin of viewer-centered coordinates, etc.). The global appearance of the 3D object is given by several appearance-describing images taken from any number of viewpoints. Our spatial correlation module extracts a global 3D representation from the appearance-describing images in a permutation invariant manner. Our system can achieve implicitly 3D understanding without explicitly 3D reconstruction. With an unsupervisely learned viewer-centered relative-pose/rotation code, the decoder can hallucinate the novel view continuously by sampling the relative-pose in a prior distribution. In various applications, we demonstrate that our model can achieve comparable or even better results than pose/3D model-supervised learning-based novel view synthesis (NVS) methods with any number of input views.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_4');
INSERT INTO `paper` VALUES (11563, 'AutoCount: Unsupervised Segmentation and Counting of Organs in Field Images', 'Computer vision', 'Organ counting', 'Unsupervised segmentation', '', '', 'Counting plant organs such as heads or tassels from outdoor imagery is a popular benchmark computer vision task in plant phenotyping, which has been previously investigated in the literature using state-of-the-art supervised deep learning techniques. However, the annotation of organs in field images is time-consuming and prone to errors. In this paper, we propose a fully unsupervised technique for counting dense objects such as plant organs. We use a convolutional network-based unsupervised segmentation method followed by two post-hoc optimization steps. The proposed technique is shown to provide competitive counting performance on a range of organ counting tasks in sorghum (S. bicolor) and wheat (T. aestivum) with no dataset-dependent tuning or modifications.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_27');
INSERT INTO `paper` VALUES (11564, 'Autoencoder-Based Graph Construction for Semi-supervised Learning', 'Semi-supervised learning', 'Matrix completion', 'Autoencoders', '', '', 'We consider graph-based semi-supervised learning that leverages a similarity graph across data points to better exploit data structure exposed in unlabeled data. One challenge that arises in this problem context is that conventional matrix completion which can serve to construct a similarity graph entails heavy computational overhead, since it re-trains the graph independently whenever model parameters of an interested classifier are updated. In this paper, we propose a holistic approach that employs a parameterized neural-net-based autoencoder for matrix completion, thereby enabling simultaneous training between models of the classifier and matrix completion. We find that this approach not only speeds up training time (around a three-fold improvement over a prior approach), but also offers a higher prediction accuracy via a more accurate graph estimate. We demonstrate that our algorithm obtains state-of-the-art performances by respectful margins on benchmark datasets: Achieving the error rates of 0.57% on MNIST with 100 labels; 3.48% on SVHN with 1000 labels; and 6.87% on CIFAR-10 with 4000 labels.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_30');
INSERT INTO `paper` VALUES (11565, 'Automated Assessment of the Curliness of Collagen Fiber in Breast Cancer', 'Collagen fiber', 'Deep learning', 'Ridge detection', 'Digital pathology', '', 'The growth and spread of breast cancer are influenced by the composition and structural properties of collagen in the extracellular matrix of tumors. Straight alignment of collagen has been attributed to tumor cell migration, which is correlated with tumor progression and metastasis in breast cancer. Thus, there is a need to characterize collagen alignment to study its value as a prognostic biomarker. We present a framework to characterize the curliness of collagen fibers in breast cancer images from DUET (DUal-mode Emission and Transmission) studies on hematoxylin and eosin (H&E) stained tissue samples. Our novel approach highlights the characteristic fiber gradients using a standard ridge detection method before feeding into the convolutional neural network. Experiments were performed on patches of breast cancer images containing straight or curly collagen. The proposed approach outperforms in terms of area under the curve against transfer learning methods trained directly on the original patches. We also explore a feature fusion strategy to combine feature representations of both the original patches and their ridge filter responses.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_17');
INSERT INTO `paper` VALUES (11566, 'Automatic Differentiation of Damaged and Unharmed Grapes Using RGB Images and Convolutional Neural Networks', 'Deep learning', 'Classification', 'Grapevine', 'Precision viticulture', 'Plant phenotyping', 'Knowledge about the damage of grapevine berries in the vineyard is important for breeders and farmers. Damage to berries can be caused for example by mechanical machines during vineyard management, various diseases, parasites or abiotic stress like sun damage. The manual detection of damaged berries in the field is a subjective and labour-intensive task, and automatic detection by machine learning methods is challenging if all variants of damage should be modelled. Our proposed method detects regions of damaged berries in images in an efficient and objective manner using a shallow neural network, where the severeness of the damage is visualized with a heatmap.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_24');
INSERT INTO `paper` VALUES (11567, 'Automatic Segmentation of Sign Language into Subtitle-Units', 'Sign language', 'Segmentation', 'Sentence', 'Subtitle', 'Graph neural network', 'We present baseline results for a new task of automatic segmentation of Sign Language video into sentence-like units. We use a corpus of natural Sign Language video with accurately aligned subtitles to train a spatio-temporal graph convolutional network with a BiLSTM on 2D skeleton data to automatically detect the temporal boundaries of subtitles. In doing so, we segment Sign Language video into subtitle-units that can be translated into phrases in a written language. We achieve a ROC-AUC statistic of 0.87 at the frame level and 92% label accuracy within a time margin of 0.6s of the true labels.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_14');
INSERT INTO `paper` VALUES (11568, 'AutoMix: Mixup Networks for Sample Interpolation via Cooperative Barycenter Learning', 'Image mixing', 'Generative model', 'Image classification', '', '', 'This paper proposes new ways of sample mixing by thinking of the process as generation of barycenter in a metric space for data augmentation. First, we present an optimal-transport-based mixup technique to generate Wasserstein barycenter which works well on images with clean background and is empirically shown complementary to existing mixup methods. Then we generalize mixup to an AutoMix technique by using a learnable network to fit barycenter in a cooperative way between the classifier (a.k.a. discriminator) and generator networks. Experimental results on both multi-class and multi-label prediction tasks show the efficacy of our approach, which is also verified in the presence of unseen categories (open set) and noise.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_37');
INSERT INTO `paper` VALUES (11569, 'Autonomous Car Chasing', 'Autonomous driving', 'Chasing', 'Multi-task convolutional neural network', 'RC car', 'Deep learning', 'We developed an autonomous driving system that can chase another vehicle using only images from a single RGB camera. At the core of the system is a novel dual-task convolutional neural network simultaneously performing object detection as well as coarse semantic segmentation. The system was firstly tested in CARLA simulations. We created a new challenging publicly available CARLA Car Chasing Dataset collected by manually driving the chased car. Using the dataset, we showed that the system that uses the semantic segmentation was able to chase the pursued car on average 16% longer than other versions of the system. Finally, we integrated the system into a sub-scale vehicle platform built on a high-speed RC car and demonstrated its capabilities by autonomously chasing another RC car.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_20');
INSERT INTO `paper` VALUES (11570, 'Autoregressive Unsupervised Image Segmentation', 'Image segmentation', 'Autoregressive models', 'Unsupervised learning', 'Clustering', 'Representation learning', 'In this work, we propose a new unsupervised image segmentation approach based on mutual information maximization between different constructed views of the inputs. Taking inspiration from autoregressive generative models that predict the current pixel from past pixels in a raster-scan ordering created with masked convolutions, we propose to use different orderings over the inputs using various forms of masked convolutions to construct different views of the data. For a given input, the model produces a pair of predictions with two valid orderings, and is then trained to maximize the mutual information between the two outputs. These outputs can either be low-dimensional features for representation learning or output clusters corresponding to semantic labels for clustering. While masked convolutions are used during training, in inference, no masking is applied and we fall back to the standard convolution where the model has access to the full input. The proposed method outperforms current state-of-the-art on unsupervised image segmentation. It is simple and easy to implement, and can be extended to other visual tasks and integrated seamlessly into existing unsupervised learning methods requiring different views of the data.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_9');
INSERT INTO `paper` VALUES (11571, 'AutoSimulate: (Quickly) Learning Synthetic Data Generation', 'Synthetic data', 'Training data distribution', 'Simulator', 'Optimization', 'Rendering', 'Simulation is increasingly being used for generating large labelled datasets in many machine learning problems. Recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task, usually relying on REINFORCE-like gradient estimators. However these approaches are very expensive as they treat the entire data generation, model training, and validation pipeline as a black-box and require multiple costly objective evaluations at each iteration. We propose an efficient alternative for optimal synthetic data generation, based on a novel differentiable approximation of the objective. This allows us to optimize the simulator, which may be non-differentiable, requiring only one objective evaluation at each iteration with a little overhead. We demonstrate on a state-of-the-art photorealistic renderer that the proposed method finds the optimal data distribution faster (up to 50\\(\\times \\)), with significantly reduced training data generation and better accuracy on real-world test datasets than previous methods.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_16');
INSERT INTO `paper` VALUES (11572, 'AutoSTR: Efficient Backbone Search for Scene Text Recognition', 'Scene text recognition', 'Neural architecture search', 'Convolutional neural network', 'Automated machine learning', '', 'Scene text recognition (STR) is challenging due to the diversity of text instances and the complexity of scenes. However, no STR methods can adapt backbones to different diversities and complexities. In this work, inspired by the success of neural architecture search (NAS), we propose automated STR (AutoSTR), which can address the above issue by searching data-dependent backbones. Specifically, we show both choices on operations and the downsampling path are very important in the search space design of NAS. Besides, since no existing NAS algorithms can handle the spatial constraint on the path, we propose a two-step search algorithm, which decouples operations and downsampling path, for an efficient search in the given space. Experiments demonstrate that, by searching data-dependent backbones, AutoSTR can outperform the state-of-the-art approaches on standard benchmarks with much fewer FLOPS and model parameters. (Code is available at https://github.com/AutoML-4Paradigm/AutoSTR.git).', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_44');
INSERT INTO `paper` VALUES (11573, 'AutoTrajectory: Label-Free Trajectory Extraction and Prediction from Videos Using Dynamic Points', '', '', '', '', '', 'Current methods for trajectory prediction operate in supervised manners, and therefore require vast quantities of corresponding ground truth data for training. In this paper, we present a novel, label-free algorithm, AutoTrajectory, for trajectory extraction and prediction to use raw videos directly. To better capture the moving objects in videos, we introduce dynamic points. We use them to model dynamic motions by using a forward-backward extractor to keep temporal consistency and using image reconstruction to keep spatial consistency in an unsupervised manner. Then we aggregate dynamic points to instance points, which stand for moving objects such as pedestrians in videos. Finally, we extract trajectories by matching instance points for prediction training. To the best of our knowledge, our method is the first to achieve unsupervised learning of trajectory extraction and prediction. We evaluate the performance on well-known trajectory datasets and show that our method is effective for real-world videos and can use raw videos to further improve the performance of existing models.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_38');
INSERT INTO `paper` VALUES (11574, 'AWNet: Attentive Wavelet Network for Image ISP', 'Image ISP', 'Discrete wavelet transform', 'Multi-scale CNN', '', '', 'As the revolutionary improvement being made on the performance of smartphones over the last decade, mobile photography becomes one of the most common practices among the majority of smartphone users. However, due to the limited size of camera sensors on phone, the photographed image is still visually distinct to the one taken by the digital single-lens reflex (DSLR) camera. To narrow this performance gap, one is to redesign the camera image signal processor (ISP) to improve the image quality. Owing to the rapid rise of deep learning, recent works resort to the deep convolutional neural network (CNN) to develop a sophisticated data-driven ISP that directly maps the phone-captured image to the DSLR-captured one. In this paper, we introduce a novel network that utilizes the attention mechanism and wavelet transform, dubbed AWNet, to tackle this learnable image ISP problem. By adding the wavelet transform, our proposed method enables us to restore favorable image details from RAW information and achieve a larger receptive field while remaining high efficiency in terms of computational cost. The global context block is adopted in our method to learn the non-local color mapping for the generation of appealing RGB images. More importantly, this block alleviates the influence of image misalignment occurred on the provided dataset. Experimental results indicate the advances of our design in both qualitative and quantitative measurements. The source code is available at https://github.com/Charlie0215/AWNet-Attentive-Wavelet-Network-for-Image-ISP.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_11');
INSERT INTO `paper` VALUES (11575, 'Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation', 'Bottom-up panoptic segmentation', 'Self-attention', '', '', '', 'Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D self-attentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is \\(3.8\\times \\) parameter-efficient and \\(27\\times \\) computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_7');
INSERT INTO `paper` VALUES (11576, 'Backpropagated Gradient Representations for Anomaly Detection', 'Gradient-based representations', 'Anomaly detection', 'Novelty detection', 'Image recognition', '', 'Learning representations that clearly distinguish between normal and abnormal data is key to the success of anomaly detection. Most of existing anomaly detection algorithms use activation representations from forward propagation while not exploiting gradients from backpropagation to characterize data. Gradients capture model updates required to represent data. Anomalies require more drastic model updates to fully represent them compared to normal data. Hence, we propose the utilization of backpropagated gradients as representations to characterize model behavior on anomalies and, consequently, detect such anomalies. We show that the proposed method using gradient-based representations achieves state-of-the-art anomaly detection performance in benchmark image recognition datasets. Also, we highlight the computational efficiency and the simplicity of the proposed method in comparison with other state-of-the-art methods relying on adversarial networks or autoregressive models, which require at least 27 times more model parameters than the proposed method.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_13');
INSERT INTO `paper` VALUES (11577, 'BATS: Binary ArchitecTure Search', 'Binary networks', 'Neural Architecture Search', '', '', '', 'This paper proposes Binary ArchitecTure Search (BATS), a framework that drastically reduces the accuracy gap between binary neural networks and their real-valued counterparts by means of Neural Architecture Search (NAS). We show that directly applying NAS to the binary domain provides very poor results. To alleviate this, we describe, to our knowledge, for the first time, the 3 key ingredients for successfully applying NAS to the binary domain. Specifically, we (1) introduce and design a novel binary-oriented search space, (2) propose a new mechanism for controlling and stabilising the resulting searched topologies, (3) propose and validate a series of new search strategies for binary networks that lead to faster convergence and lower search times. Experimental results demonstrate the effectiveness of the proposed approach and the necessity of searching in the binary space directly. Moreover, (4) we set a new state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and ImageNet datasets. Code will be made available.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_19');
INSERT INTO `paper` VALUES (11578, 'Bayesian Zero-Shot Learning', 'Generalized ZSL', 'Bayesian hierarchical models', '', '', '', 'Object classes that surround us have a natural tendency to emerge at varying levels of abstraction. We propose a Bayesian approach to zero-shot learning (ZSL) that introduces the notion of meta-classes and implements a Bayesian hierarchy around these classes to effectively blend data likelihood with local and global priors. Local priors driven by data from seen classes, i.e., classes available at training time, become instrumental in recovering unseen classes, i.e., classes that are missing at training time, in a generalized ZSL (GZSL) setting. Hyperparameters of the Bayesian model offer a convenient way to optimize the trade-off between seen and unseen class accuracy. We conduct experiments on seven benchmark datasets, including a large scale ImageNet and show that our model produces promising results in the challenging GZSL setting.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_47');
INSERT INTO `paper` VALUES (11579, 'BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network', 'RGB-D saliency detection', 'Bifurcated backbone strategy', '', '', '', 'Multi-level feature fusion is a fundamental topic in computer vision for detecting, segmenting and classifying objects at various scales. When multi-level features meet multi-modal cues, the optimal fusion problem becomes a hot potato. In this paper, we make the first attempt to leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to develop a novel cascaded refinement network. In particular, we 1) propose a bifurcated backbone strategy (BBS) to split the multi-level features into teacher and student features, and 2) utilize a depth-enhanced module (DEM) to excavate informative parts of depth cues from the channel and spatial views. This fuses RGB and depth modalities in a complementary way. Our simple yet efficient architecture, dubbed Bifurcated Backbone Strategy Network (BBS-Net), is backbone independent and outperforms 18 SOTAs on seven challenging datasets using four metrics.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_17');
INSERT INTO `paper` VALUES (11580, 'BCNet: Learning Body and Cloth Shape from a Single Image', 'Clothed body reconstruction', '3D garment shape', '3D body shape', 'Skinning weight', '', 'In this paper, we consider the problem to automatically reconstruct garment and body shapes from a single near-front view RGB image. To this end, we propose a layered garment representation on top of SMPL and novelly make the skinning weight of garment independent of the body mesh, which significantly improves the expression ability of our garment model. Compared with existing methods, our method can support more garment categories and recover more accurate geometry. To train our model, we construct two large scale datasets with ground truth body and garment geometries as well as paired color images. Compared with single mesh or non-parametric representation, our method can achieve more flexible control with separate meshes, makes applications like re-pose, garment transfer, and garment texture mapping possible.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_2');
INSERT INTO `paper` VALUES (11581, 'Behavioural Pattern Discovery from Collections of Egocentric Photo-Streams', 'Behaviour analysis', 'Pattern discovery', 'Egocentric vision', 'Data mining', 'Lifelogging', 'The automatic discovery of behaviour is of high importance when aiming to assess and improve the quality of life of people. Egocentric images offer a rich and objective description of the daily life of the camera wearer. This work proposes a new method to identify a person’s patterns of behaviour from collected egocentric photo-streams. Our model characterizes time-frames based on the context (place, activities and environment objects) that define the images composition. Based on the similarity among the time-frames that describe the collected days for a user, we propose a new unsupervised greedy method to discover the behavioural pattern set based on a novel semantic clustering approach. Moreover, we present a new score metric to evaluate the performance of the proposed algorithm. We validate our method on 104 days and more than 100k images extracted from 7 users. Results show that behavioural patterns can be discovered to characterize the routine of individuals and consequently their lifestyle.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_28');
INSERT INTO `paper` VALUES (11582, 'Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models', '', '', '', '', '', 'Recent Transformer-based large-scale pre-trained models have revolutionized vision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER have significantly lifted state of the art across a wide range of V+L benchmarks. However, little is known about the inner mechanisms that destine their impressive success. To reveal the secrets behind the scene, we present Value (Vision-And-Language Understanding Evaluation), a set of meticulously designed probing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection) generalizable to standard pre-trained V+L models, to decipher the inner workings of multimodal pre-training (e.g., implicit knowledge garnered in individual attention heads, inherent cross-modal alignment learned through contextualized multimodal embeddings). Through extensive analysis of each archetypal model architecture via these probing tasks, our key observations are: (i) Pre-trained models exhibit a propensity for attending over text rather than images during inference. (ii) There exists a subset of attention heads that are tailored for capturing cross-modal interactions. (iii) Learned attention matrix in pre-trained models demonstrates patterns coherent with the latent alignment between image regions and textual words. (iv) Plotted attention patterns reveal visually-interpretable relations among image regions. (v) Pure linguistic knowledge is also effectively encoded in the attention heads. These are valuable insights serving to guide future work towards designing better model architecture and objectives for multimodal pre-training. (Code is available at https://github.com/JizeCao/VALUE).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_34');
INSERT INTO `paper` VALUES (11583, 'Beyond 3DMM Space: Towards Fine-Grained 3D Face Reconstruction', '3D face reconstruction', 'Fine-grained', 'Deep learning', '', '', 'Recently, deep learning based 3D face reconstruction methods have shown promising results in both quality and efficiency. However, most of their training data is constructed by 3D Morphable Model, whose space spanned is only a small part of the shape space. As a result, the reconstruction results lose the fine-grained geometry and look different from real faces. To alleviate this issue, we first propose a solution to construct large-scale fine-grained 3D data from RGB-D images, which are expected to be massively collected as the proceeding of hand-held depth camera. A new dataset Fine-Grained 3D face (FG3D) with 200k samples is constructed to provide sufficient data for neural network training. Secondly, we propose a Fine-Grained reconstruction Network (FGNet) that can concentrate on shape modification by warping the network input and output to the UV space. Through FG3D and FGNet, we successfully generate reconstruction results with fine-grained geometry. The experiments on several benchmarks validate the effectiveness of our method compared to several baselines and other state-of-the-art methods. The proposed method and code will be available at https://github.com/XiangyuZhu-open/Beyond3DMM.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_21');
INSERT INTO `paper` VALUES (11584, 'Beyond Controlled Environments: 3D Camera Re-localization in Changing Indoor Scenes', '', '', '', '', '', 'Long-term camera re-localization is an important task with numerous computer vision and robotics applications. Whilst various outdoor benchmarks exist that target lighting, weather and seasonal changes, far less attention has been paid to appearance changes that occur indoors. This has led to a mismatch between popular indoor benchmarks, which focus on static scenes, and indoor environments that are of interest for many real-world applications. In this paper, we adapt 3RScan – a recently introduced indoor RGB-D dataset designed for object instance re-localization – to create RIO10, a new long-term camera re-localization benchmark focused on indoor scenes. We propose new metrics for evaluating camera re-localization and explore how state-of-the-art camera re-localizers perform according to these metrics. We also examine in detail how different types of scene change affect the performance of different methods, based on novel ways of detecting such changes in a given RGB-D frame. Our results clearly show that long-term indoor re-localization is an unsolved problem. Our benchmark and tools are publicly available at https://www.waldjohannau.github.io/RIO10.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_28');
INSERT INTO `paper` VALUES (11585, 'Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid', '', '', '', '', '', 'In modern computer vision, images are typically represented as a fixed uniform grid with some stride and processed via a deep convolutional neural network. We argue that deforming the grid to better align with the high-frequency image content is a more effective strategy. We introduce Deformable Grid (DefGrid), a learnable neural network module that predicts location offsets of vertices of a 2-dimensional triangular grid, such that the edges of the deformed grid align with image boundaries. We showcase our DefGrid in a variety of use cases, i.e., by inserting it as a module at various levels of processing. We utilize DefGrid as an end-to-end learnable geometric downsampling layer that replaces standard pooling methods for reducing feature resolution when feeding images into a deep CNN. We show significantly improved results at the same grid resolution compared to using CNNs on uniform grids for the task of semantic segmentation. We also utilize DefGrid at the output layers for the task of object mask annotation, and show that reasoning about object boundaries on our predicted polygonal grid leads to more accurate results over existing pixel-wise and curve-based approaches. We finally showcase DefGrid as a standalone module for unsupervised image partitioning, showing superior performance over existing approaches. Project website: http://www.cs.toronto.edu/ jungao/def-grid.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_7');
INSERT INTO `paper` VALUES (11586, 'Beyond Monocular Deraining: Stereo Image Deraining via Semantic Understanding', 'Stereo deraining', 'Semantic understanding', 'Rethinking loop', 'View fusion', 'Deep learning', 'Rain is a common natural phenomenon. Taking images in the rain however often results in degraded quality of images, thus compromises the performance of many computer vision systems. Most existing de-rain algorithms use only one single input image and aim to recover a clean image. Few work has exploited stereo images. Moreover, even for single image based monocular deraining, many current methods fail to complete the task satisfactorily because they mostly rely on per pixel loss functions and ignore semantic information. In this paper, we present a Paired Rain Removal Network (PRRNet), which exploits both stereo images and semantic information. Specifically, we develop a Semantic-Aware Deraining Module (SADM) which solves both tasks of semantic segmentation and deraining of scenes, and a Semantic-Fusion Network (SFNet) and a View-Fusion Network (VFNet) which fuse semantic information and multi-view information respectively. We also propose new stereo based rainy datasets for benchmarking. Experiments on both monocular and the newly proposed stereo rainy datasets demonstrate that the proposed method achieves the state-of-the-art performance.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_5');
INSERT INTO `paper` VALUES (11587, 'Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments', 'Vision-and-Language Navigation', 'Embodied agents', '', '', '', 'We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some transfer, we find significantly lower absolute performance in the continuous setting – suggesting that performance in prior ‘navigation-graph’ settings may be inflated by the strong implicit assumptions. Code at jacobkrantz.github.io/vlnce .', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_7');
INSERT INTO `paper` VALUES (11588, 'Beyond Weak Perspective for Monocular 3D Human Pose Estimation', 'SMPL', 'Pose estimation', 'Perspective projection', '3DPW', '', 'We consider the task of 3D joints location and orientation prediction from a monocular video with the skinned multi-person linear (SMPL) model. We first infer 2D joints locations with an off-the-shelf pose estimation algorithm. We use the SPIN algorithm and estimate initial predictions of body pose, shape and camera parameters from a deep regression neural network. We then adhere to the SMPLify algorithm which receives those initial parameters, and optimizes them so that inferred 3D joints from the SMPL model would fit the 2D joints locations. This algorithm involves a projection step of 3D joints to the 2D image plane. The conventional approach is to follow weak perspective assumptions which use ad-hoc focal length. Through experimentation on the 3D poses in the wild (3DPW) dataset, we show that using full perspective projection, with the correct camera center and an approximated focal length, provides favorable results. Our algorithm has resulted in a winning entry for the 3DPW Challenge, reaching first place in joints orientation accuracy.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_37');
INSERT INTO `paper` VALUES (11589, 'BGGAN: Bokeh-Glass Generative Adversarial Network for Rendering Realistic Bokeh', 'Bokeh', 'Depth-of-field', 'Smartphone GPU', 'GAN', '', 'A photo captured with bokeh effect often means objects in focus are sharp while the out-of-focus areas are all blurred. DSLR can easily render this kind of effect naturally. However, due to the limitation of sensors, smartphones cannot capture images with depth-of-field effects directly. In this paper, we propose a novel generator called Glass-Net, which generates bokeh images not relying on complex hardware. Meanwhile, the GAN-based method and perceptual loss are combined for rendering a realistic bokeh effect in the stage of finetuning the model. Moreover, Instance Normalization(IN) is reimplemented in our network, which ensures our tflite model with IN can be accelerated on smartphone GPU. Experiments show that our method is able to render a high-quality bokeh effect and process one \\(1024 \\times 1536\\) pixel image in 1.9 s on all smartphone chipsets. This approach ranked First in AIM 2020 Rendering Realistic Bokeh Challenge Track 1 & Track 2.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_14');
INSERT INTO `paper` VALUES (11590, 'Bi-Dimensional Feature Alignment for Cross-Domain Object Detection', 'Domain adaptation', 'Object detection', 'Style', 'Attention', '', 'Recently the problem of cross-domain object detection has started drawing attention in the computer vision community. In this paper, we propose a novel unsupervised cross-domain detection model that exploits the annotated data in a source domain to train an object detector for a different target domain. The proposed model mitigates the cross-domain representation divergence for object detection by performing cross-domain feature alignment in two dimensions, the depth dimension and the spatial dimension. In the depth dimension of channel layers, it uses inter-channel information to bridge the domain divergence with respect to image style alignment. In the dimension of spatial layers, it deploys spatial attention modules to enhance detection relevant regions and suppress irrelevant regions with respect to cross-domain feature alignment. Experiments are conducted on a number of benchmark cross-domain detection datasets. The empirical results show the proposed method outperforms the state-of-the-art comparison methods.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_46');
INSERT INTO `paper` VALUES (11591, 'Bi-directional Cross-Modality Feature Propagation with Separation-and-Aggregation Gate for RGB-D Semantic Segmentation', 'RGB-D semantic segmentation', 'Cross-modality feature propagation', '', '', '', 'Depth information has proven to be a useful cue in the semantic segmentation of RGB-D images for providing a geometric counterpart to the RGB representation. Most existing works simply assume that depth measurements are accurate and well-aligned with the RGB pixels and models the problem as a cross-modal feature fusion to obtain better feature representations to achieve more accurate segmentation. This, however, may not lead to satisfactory results as actual depth data are generally noisy, which might worsen the accuracy as the networks go deeper.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_33');
INSERT INTO `paper` VALUES (11592, 'Bias-Based Universal Adversarial Patch Attack for Automatic Check-Out', 'Universal adversarial patch', 'Automatic Check-Out', 'Bias-based attack', '', '', 'Adversarial examples are inputs with imperceptible perturbations that easily misleading deep neural networks (DNNs). Recently, adversarial patch, with noise confined to a small and localized patch, has emerged for its easy feasibility in real-world scenarios. However, existing strategies failed to generate adversarial patches with strong generalization ability. In other words, the adversarial patches were input-specific and failed to attack images from all classes, especially unseen ones during training. To address the problem, this paper proposes a bias-based framework to generate class-agnostic universal adversarial patches with strong generalization ability, which exploits both the perceptual and semantic bias of models. Regarding the perceptual bias, since DNNs are strongly biased towards textures, we exploit the hard examples which convey strong model uncertainties and extract a textural patch prior from them by adopting the style similarities. The patch prior is more close to decision boundaries and would promote attacks. To further alleviate the heavy dependency on large amounts of data in training universal attacks, we further exploit the semantic bias. As the class-wise preference, prototypes are introduced and pursued by maximizing the multi-class margin to help universal training. Taking Automatic Check-out (ACO) as the typical scenario, extensive experiments including white-box/black-box settings in both digital-world (RPC, the largest ACO related dataset) and physical-world scenario (Taobao and JD, the worlds largest online shopping platforms) are conducted. Experimental results demonstrate that our proposed framework outperforms state-of-the-art adversarial patch attack methods.(Our code can be found at https://github.com/liuaishan/ModelBiasedAttack.)', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_24');
INSERT INTO `paper` VALUES (11593, 'Big Transfer (BiT): General Visual Representation Learning', '', '', '', '', '', 'Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes—from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_29');
INSERT INTO `paper` VALUES (11594, 'BigNAS: Scaling up Neural Architecture Search with Big Single-Stage Models', 'Efficient neural architecture search', 'AutoML', '', '', '', 'Neural architecture search (NAS) has shown promising results discovering models that are both accurate and fast. For NAS, training a one-shot model has become a popular strategy to rank the relative quality of different architectures (child models) using a single set of shared weights. However, while one-shot model weights can effectively rank different network architectures, the absolute accuracies from these shared weights are typically far below those obtained from stand-alone training. To compensate, existing methods assume that the weights must be retrained, finetuned, or otherwise post-processed after the search is completed. These steps significantly increase the compute requirements and complexity of the architecture search and model deployment. In this work, we propose BigNAS, an approach that challenges the conventional wisdom that post-processing of the weights is necessary to get good prediction accuracies. Without extra retraining or post-processing steps, we are able to train a single set of shared weights on ImageNet and use these weights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our discovered model family, BigNASModels, achieve top-1 accuracies ranging from 76.5% to 80.9%, surpassing state-of-the-art models in this range including EfficientNets and Once-for-All networks without extra retraining or post-processing. We present ablative study and analysis to further understand the proposed BigNASModels.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_41');
INSERT INTO `paper` VALUES (11595, 'Binarized Neural Network for Single Image Super Resolution', 'Single image super-resolution', 'Model quantization', 'Binary neural network', 'Bit-accumulation mechanism', '', 'Lighter model and faster inference are the focus of current single image super-resolution (SISR) research. However, existing methods are still hard to be applied in real-world applications due to the heavy computation requirement. Model quantization is an effective way to significantly reduce model size and computation time. In this work, we investigate the binary neural network-based SISR problem and propose a novel model binarization method. Specially, we design a bit-accumulation mechanism (BAM) to approximate the full-precision convolution with a value accumulation scheme, which can gradually refine the precision of quantization along the direction of model inference. In addition, we further construct an efficient model structure based on the BAM for lower computational complexity and parameters. Extensive experiments show the proposed model outperforms the state-of-the-art binarization methods by large margins on 4 benchmark datasets, specially by average more than 0.7 dB in terms of Peak Signal-to-Noise Ratio on Set5 dataset.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_6');
INSERT INTO `paper` VALUES (11596, 'BioMetricNet: Deep Unconstrained Face Verification Through Learning of Metrics Regularized onto Gaussian Distributions', 'Biometrics', 'Face verification', 'Biometric authentication', '', '', 'We present BioMetricNet: a novel framework for deep unconstrained face verification which learns a regularized metric to compare facial features. Differently from popular methods such as FaceNet, the proposed approach does not impose any specific metric on facial features; instead, it shapes the decision space by learning a latent representation in which matching and non-matching pairs are mapped onto clearly separated and well-behaved target distributions. In particular, the network jointly learns the best feature representation, and the best metric that follows the target distributions, to be used to discriminate face images. In this paper we present this general framework, first of its kind for facial verification, and tailor it to Gaussian distributions. This choice enables the use of a simple linear decision boundary that can be tuned to achieve the desired trade-off between false alarm and genuine acceptance rate, and leads to a loss function that can be written in closed form. Extensive analysis and experimentation on publicly available datasets such as Labeled Faces in the wild (LFW), Youtube faces (YTF), Celebrities in Frontal-Profile in the Wild (CFP), and challenging datasets like cross-age LFW (CALFW), cross-pose LFW (CPLFW), In-the-wild Age Dataset (AgeDB) show a significant performance improvement and confirms the effectiveness and superiority of BioMetricNet over existing state-of-the-art methods.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_9');
INSERT INTO `paper` VALUES (11597, 'Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual Reality', '', '', '', '', '', 'We present Bionic Tracking, a novel method for solving biological cell tracking problems with eye tracking in virtual reality using commodity hardware. Using gaze data, and especially smooth pursuit eye movements, we are able to track cells in time series of 3D volumetric datasets. The problem of tracking cells is ubiquitous in developmental biology, where large volumetric microscopy datasets are acquired on a daily basis, often comprising hundreds or thousands of time points that span hours or days. The image data, however, is only a means to an end, and scientists are often interested in the reconstruction of cell trajectories and cell lineage trees. Reliably tracking cells in crowded three-dimensional space over many time points remains an open problem, and many current approaches rely on tedious manual annotation or curation. In the Bionic Tracking approach, we substitute the usual 2D point-and-click interface for annotation or curation with eye tracking in a virtual reality headset, where users follow cells with their eyes in 3D space in order to track them. We detail the interaction design of our approach and explain the graph-based algorithm used to connect different time points, also taking occlusion and user distraction into account. We demonstrate Bionic Tracking using examples from two different biological datasets. Finally, we report on a user study with seven cell tracking experts, highlighting the benefits and limitations of Bionic Tracking compared to point-and-click interfaces.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_18');
INSERT INTO `paper` VALUES (11598, 'BIRNAT: Bidirectional Recurrent Neural Networks with Adversarial Training for Video Snapshot Compressive Imaging', 'Snapshot compressive imaging', 'Compressive sensing', 'Deep learning', 'Convolutional neural networks', 'Recurrent Neural Network', 'We consider the problem of video snapshot compressive imaging (SCI), where multiple high-speed frames are coded by different masks and then summed to a single measurement. This measurement and the modulation masks are fed into our Recurrent Neural Network (RNN) to reconstruct the desired high-speed frames. Our end-to-end sampling and reconstruction system is dubbed BIdirectional Recurrent Neural networks with Adversarial Training (BIRNAT). To our best knowledge, this is the first time that recurrent networks are employed to SCI problem. Our proposed BIRNAT outperforms other deep learning based algorithms and the state-of-the-art optimization based algorithm, DeSCI, through exploiting the underlying correlation of sequential video frames. BIRNAT employs a deep convolutional neural network with Resblock and feature map self-attention to reconstruct the first frame, based on which bidirectional RNN is utilized to reconstruct the following frames in a sequential manner. To improve the quality of the reconstructed video, BIRNAT is further equipped with the adversarial training besides the mean square error loss. Extensive results on both simulation and real data (from two SCI cameras) demonstrate the superior performance of our BIRNAT system. The codes are available at https://github.com/BoChenGroup/BIRNAT.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_16');
INSERT INTO `paper` VALUES (11599, 'Black-Box Face Recovery from Identity Features', 'Adversarial', 'Privacy', 'Black-box', 'Arcface', 'Face recognition', 'In this work, we present a novel algorithm based on an iterative sampling of random Gaussian blobs for black-box face recovery, given only an output feature vector of deep face recognition systems. We attack the state-of-the-art face recognition system (ArcFace) to test our algorithm. Another network with different architecture (FaceNet) is used as an independent critic showing that the target person can be identified with the reconstructed image even with no access to the attacked model. Furthermore, our algorithm requires a significantly less number of queries compared to the state-of-the-art solution.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_34');
INSERT INTO `paper` VALUES (11600, 'Blended Grammar Network for Human Parsing', '', '', '', '', '', 'Although human parsing has made great progress, it still faces a challenge, i.e., how to extract the whole foreground from similar or cluttered scenes effectively. In this paper, we propose a Blended Grammar Network (BGNet), to deal with the challenge. BGNet exploits the inherent hierarchical structure of a human body and the relationship of different human parts by means of grammar rules in both cascaded and paralleled manner. In this way, conspicuous parts, which are easily distinguished from the background, can amend the segmentation of inconspicuous ones, improving the foreground extraction. We also design a Part-aware Convolutional Recurrent Neural Network (PCRNN) to pass messages which are generated by grammar rules. To train PCRNNs effectively, we present a blended grammar loss to supervise the training of PCRNNs. We conduct extensive experiments to evaluate BGNet on PASCAL-Person-Part, LIP, and PPSS datasets. BGNet obtains state-of-the-art performance on these human parsing datasets.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_12');
INSERT INTO `paper` VALUES (11601, 'Blind Face Restoration via Deep Multi-scale Component Dictionaries', 'Face hallucination', 'Deep face dictionary', 'Guided image restoration', 'Convolutional neural networks', '', 'Recent reference-based face restoration methods have received considerable attention due to their great capability in recovering high-frequency details on real low-quality images. However, most of these methods require a high-quality reference image of the same identity, making them only applicable in limited scenes. To address this issue, this paper suggests a deep face dictionary network (termed as DFDNet) to guide the restoration process of degraded observations. To begin with, we use K-means to generate deep dictionaries for perceptually significant face components (i.e., left/right eyes, nose and mouth) from high-quality images. Next, with the degraded input, we match and select the most similar component features from their corresponding dictionaries and transfer the high-quality details to the input via the proposed dictionary feature transfer (DFT) block. In particular, component AdaIN is leveraged to eliminate the style diversity between the input and dictionary features (e.g., illumination), and a confidence score is proposed to adaptively fuse the dictionary feature to the input. Finally, multi-scale dictionaries are adopted in a progressive manner to enable the coarse-to-fine restoration. Experiments show that our proposed method can achieve plausible performance in both quantitative and qualitative evaluation, and more importantly, can generate realistic and promising results on real degraded images without requiring an identity-belonging reference. The source code and models are available at https://github.com/csxmli2016/DFDNet.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_23');
INSERT INTO `paper` VALUES (11602, 'BLSM: A Bone-Level Skinned Model of the Human Mesh', '3D human body modelling', 'Graph convolutional networks', '', '', '', 'We introduce BLSM, a bone-level skinned model of the human body mesh where bone scales are set prior to template synthesis, rather than the common, inverse practice. BLSM first sets bone lengths and joint angles to specify the skeleton, then specifies identity-specific surface variation, and finally bundles them together through linear blend skinning. We design these steps by constraining the joint angles to respect the kinematic constraints of the human body and by using accurate mesh convolution-based networks to capture identity-specific surface variation.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_1');
INSERT INTO `paper` VALUES (11603, 'BMBC: Bilateral Motion Estimation with Bilateral Cost Volume for Video Interpolation', 'Video interpolation', 'Bilateral motion', 'Bilateral cost volume', '', '', 'Video interpolation increases the temporal resolution of a video sequence by synthesizing intermediate frames between two consecutive frames. We propose a novel deep-learning-based video interpolation algorithm based on bilateral motion estimation. First, we develop the bilateral motion network with the bilateral cost volume to estimate bilateral motions accurately. Then, we approximate bi-directional motions to predict a different kind of bilateral motions. We then warp the two input frames using the estimated bilateral motions. Next, we develop the dynamic filter generation network to yield dynamic blending filters. Finally, we combine the warped frames using the dynamic blending filters to generate intermediate frames. Experimental results show that the proposed algorithm outperforms the state-of-the-art video interpolation algorithms on several benchmark datasets. The source codes and pre-trained models are available at https://github.com/JunHeum/BMBC.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_7');
INSERT INTO `paper` VALUES (11604, 'Body Shape Privacy in Images: Understanding Privacy and Preventing Automatic Shape Extraction', '', '', '', '', '', 'Modern approaches to pose and body shape estimation have recently achieved strong performance even under challenging real-world conditions. Even from a single image of a clothed person, a realistic looking body shape can be inferred that captures a users’ weight group and body shape type well. This opens up a whole spectrum of applications – in particular in fashion – where virtual try-on and recommendation systems can make use of these new and automatized cues. However, a realistic depiction of the undressed body is regarded highly private and therefore might not be consented by most people. Hence, we ask if the automatic extraction of such information can be effectively evaded. While adversarial perturbations have been shown to be effective for manipulating the output of machine learning models – in particular, end-to-end deep learning approaches – state of the art shape estimation methods are composed of multiple stages. We perform the first investigation of different strategies that can be used to effectively manipulate the automatic shape estimation while preserving the overall appearance of the original image.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_31');
INSERT INTO `paper` VALUES (11605, 'Bokeh Rendering from Defocus Estimation', 'Bokeh rendering', 'Defocus estimation', 'Radiance', 'Upsampling', '', 'In this paper, we study realistic bokeh rendering from a single all-in-focus image. Existing computational bokeh rendering methods generate bokeh effects by adding a simple flat background blur. As a result, the rendering results are different from the real bokeh on DSLR cameras. To address this issue, we propose a multi-stage network to learn shallow depth-of-field from a single bokeh-free image. In particular, our network consists of four modules: defocus estimation, radiance, rendering, and upsampling. The four modules are trained on different sizes to learn global features as well as local details around the boundaries of in-focus objects. Experimental results show that our approach is capable of rendering a pleasing distinctive bokeh effect in complex scenes.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_15');
INSERT INTO `paper` VALUES (11606, 'Boosting Decision-Based Black-Box Adversarial Attacks with Random Sign Flip', 'Adversarial examples', 'Decision-based attacks', '', '', '', 'Decision-based black-box adversarial attacks (decision-based attack) pose a severe threat to current deep neural networks, as they only need the predicted label of the target model to craft adversarial examples. However, existing decision-based attacks perform poorly on the \\( l_\\infty \\) setting and the required enormous queries cast a shadow over the practicality. In this paper, we show that just randomly flipping the signs of a small number of entries in adversarial perturbations can significantly boost the attack performance. We name this simple and highly efficient decision-based \\( l_\\infty \\) attack as Sign Flip Attack. Extensive experiments on CIFAR-10 and ImageNet show that the proposed method outperforms existing decision-based attacks by large margins and can serve as a strong baseline to evaluate the robustness of defensive models. We further demonstrate the applicability of the proposed method on real-world systems.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_17');
INSERT INTO `paper` VALUES (11607, 'Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer', 'Weakly supervised', 'Object detection', 'Transfer learning', 'Semi-supervised', '', 'In this paper, we propose an effective knowledge transfer framework to boost the weakly supervised object detection accuracy with the help of an external fully-annotated source dataset, whose categories may not overlap with the target domain. This setting is of great practical value due to the existence of many off-the-shelf detection datasets. To more effectively utilize the source dataset, we propose to iteratively transfer the knowledge from the source domain by a one-class universal detector and learn the target-domain detector. The box-level pseudo ground truths mined by the target-domain detector in each iteration effectively improve the one-class universal detector. Therefore, the knowledge in the source dataset is more thoroughly exploited and leveraged. Extensive experiments are conducted with Pascal VOC 2007 as the target weakly-annotated dataset and COCO/ImageNet as the source fully-annotated dataset. With the proposed solution, we achieved an mAP of \\(59.7\\%\\) detection performance on the VOC test set and an mAP of \\(60.2\\%\\) after retraining a fully supervised Faster RCNN with the mined pseudo ground truths. This is significantly better than any previously known results in related literature and sets a new state-of-the-art of weakly supervised object detection under the knowledge transfer setting. Code: https://github.com/mikuhatsune/wsod_transfer.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_37');
INSERT INTO `paper` VALUES (11608, 'BOP Challenge 2020 on 6D Object Localization', '', '', '', '', '', 'This paper presents the evaluation methodology, datasets, and results of the BOP Challenge 2020, the third in a series of public competitions organized with the goal to capture the status quo in the field of 6D object pose estimation from an RGB-D image. In 2020, to reduce the domain gap between synthetic training and real test RGB images, the participants were 350K photorealistic training images generated by BlenderProc4BOP, a new open-source and light-weight physically-based renderer (PBR) and procedural data generator. Methods based on deep neural networks have finally caught up with methods based on point pair features, which were dominating previous editions of the challenge. Although the top-performing methods rely on RGB-D image channels, strong results were achieved when only RGB channels were used at both training and test time – out of the 26 evaluated methods, the third method was trained on RGB channels of PBR and real images, while the fifth on RGB channels of PBR images only. Strong data augmentation was identified as a key component of the top-performing CosyPose method, and the photorealism of PBR images was demonstrated effective despite the augmentation. The online evaluation system stays open and is available on the project website: bop.felk.cvut.cz.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_39');
INSERT INTO `paper` VALUES (11609, 'BorderDet: Border Feature for Dense Object Detection', 'Dense object detection', 'Border feature', '', '', '', 'Dense object detectors rely on the sliding-window paradigm that predicts the object over a regular grid of image. Meanwhile, the feature maps on the point of the grid are adopted to generate the bounding box predictions. The point feature is convenient to use but may lack the explicit border information for accurate localization. In this paper, We propose a simple and efficient operator called Border-Align to extract “border features” from the extreme point of the border to enhance the point feature. Based on the BorderAlign, we design a novel detection architecture called BorderDet, which explicitly exploits the border information for stronger classification and more accurate localization. With ResNet-50 backbone, our method improves single-stage detector FCOS by 2.8 AP gains (38.6 v.s. 41.4). With the ResNeXt-101-DCN backbone, our BorderDet obtains 50.3 AP, outperforming the existing state-of-the-art approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_32');
INSERT INTO `paper` VALUES (11610, 'Bottom-Up Temporal Action Localization with Mutual Regularization', 'Action localization', 'Action proposals', 'Mutual regularization', '', '', 'Recently, temporal action localization (TAL), i.e., finding specific action segments in untrimmed videos, has attracted increasing attentions of the computer vision community. State-of-the-art solutions for TAL involves evaluating the frame-level probabilities of three action-indicating phases, i.e. starting, continuing, and ending; and then post-processing these predictions for the final localization. This paper delves deep into this mechanism, and argues that existing methods, by modeling these phases as individual classification tasks, ignored the potential temporal constraints between them. This can lead to incorrect and/or inconsistent predictions when some frames of the video input lack sufficient discriminative information. To alleviate this problem, we introduce two regularization terms to mutually regularize the learning procedure: the Intra-phase Consistency (IntraC) regularization is proposed to make the predictions verified inside each phase; and the Inter-phase Consistency (InterC) regularization is proposed to keep consistency between these phases. Jointly optimizing these two terms, the entire framework is aware of these potential constraints during an end-to-end optimization process. Experiments are performed on two popular TAL datasets, THUMOS14 and ActivityNet1.3. Our approach clearly outperforms the baseline both quantitatively and qualitatively. The proposed regularization also generalizes to other TAL methods (e.g., TSA-Net and PGCN). Code: https://github.com/PeisenZhao/Bottom-Up-TAL-with-MR.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_32');
INSERT INTO `paper` VALUES (11611, 'Boundary Content Graph Neural Network for Temporal Action Proposal Generation', 'Temporal action proposal generation', 'Graph Neural Network', 'Temporal action detection', '', '', 'Temporal action proposal generation plays an important role in video action understanding, which requires localizing high-quality action content precisely. However, generating temporal proposals with both precise boundaries and high-quality action content is extremely challenging. To address this issue, we propose a novel Boundary Content Graph Neural Network (BC-GNN) to model the insightful relations between the boundary and action content of temporal proposals by the graph neural networks. In BC-GNN, the boundaries and content of temporal proposals are taken as the nodes and edges of the graph neural network, respectively, where they are spontaneously linked. Then a novel graph computation operation is proposed to update features of edges and nodes. After that, one updated edge and two nodes it connects are used to predict boundary probabilities and content confidence score, which will be combined to generate a final high-quality proposal. Experiments are conducted on two mainstream datasets: ActivityNet-1.3 and THUMOS14. Without the bells and whistles, BC-GNN outperforms previous state-of-the-art methods in both temporal action proposal and temporal action detection tasks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_8');
INSERT INTO `paper` VALUES (11612, 'Boundary-Aware Cascade Networks for Temporal Action Segmentation', 'Temporal action segmentation', 'Cascade strategy', 'Smoothing operator', 'Untrimmed video', '', 'Identifying human action segments in an untrimmed video is still challenging due to boundary ambiguity and over-segmentation issues. To address these problems, we present a new boundary-aware cascade network by introducing two novel components. First, we devise a new cascading paradigm, called Stage Cascade, to enable our model to have adaptive receptive fields and more confident predictions for ambiguous frames. Second, we design a general and principled smoothing operation, termed as local barrier pooling, to aggregate local predictions by leveraging semantic boundary information. Moreover, these two components can be jointly fine-tuned in an end-to-end manner. We perform experiments on three challenging datasets: 50Salads, GTEA and Breakfast dataset, demonstrating that our framework significantly outperforms the current state-of-the-art methods. The code is available at https://github.com/MCG-NJU/BCN.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_3');
INSERT INTO `paper` VALUES (11613, 'Boundary-Preserving Mask R-CNN', 'Instance segmentation', 'Object detection', 'Boundary-preserving', 'Boundary detection', '', 'Tremendous efforts have been made to improve mask localization accuracy in instance segmentation. Modern instance segmentation methods relying on fully convolutional networks perform pixel-wise classification, which ignores object boundaries and shap, leading coarse and indistinct mask prediction results and imprecise localization. To remedy these problems, we propose a conceptually simple yet effective Boundary-preserving Mask R-CNN (BMask R-CNN) to leverage object boundary information to improve mask localization accuracy. BMask R-CNN contains a boundary-preserving mask head in which object boundary and mask are mutually learned via feature fusion blocks. As a result, the predicted masks are better aligned with object boundaries. Without bells and whistles, BMask R-CNN outperforms Mask R-CNN by a considerable margin on the COCO dataset; in the Cityscapes dataset, there are more accurate boundary groundtruths available, so that BMask R-CNN obtains remarkable improvements over Mask R-CNN. Besides, it is not surprising to observe that BMask R-CNN obtains more obvious improvement when the evaluation criterion requires better localization (e.g.., AP\\(_{75}\\)) as shown in Fig. 1. Code and models are available at https://github.com/hustvl/BMaskR-CNN.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_39');
INSERT INTO `paper` VALUES (11614, 'Bounding-Box Channels for Visual Relationship Detection', 'Bounding-box channels', 'Visual relationship detection', 'Scene graph generation', '', '', 'Recognizing the relationship between multiple objects in an image is essential for a deeper understanding of the meaning of the image. However, current visual recognition methods are still far from reaching human-level accuracy. Recent approaches have tackled this task by combining image features with semantic and spatial features, but the way they relate them to each other is weak, mostly because the spatial context in the image feature is lost. In this paper, we propose the bounding-box channels, a novel architecture capable of relating the semantic, spatial, and image features strongly. Our network learns bounding-box channels, which are initialized according to the position and the label of objects, and concatenated to the image features extracted from such objects. Then, they are input together to the relationship estimator. This allows retaining the spatial information in the image features, and strongly associate them with the semantic and spatial features. This way, our method is capable of effectively emphasizing the features in the object area for a better modeling of the relationships within objects. Our evaluation results show the efficacy of our architecture outperforming previous works in visual relationship detection. In addition, we experimentally show that our bounding-box channels have a high generalization ability.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_40');
INSERT INTO `paper` VALUES (11615, 'Box2Seg: Attention Weighted Loss and Discriminative Feature Learning for Weakly Supervised Segmentation', '', '', '', '', '', 'We propose a weakly supervised approach to semantic segmentation using bounding box annotations. Bounding boxes are treated as noisy labels for the foreground objects. We predict a per-class attention map that saliently guides the per-pixel cross entropy loss to focus on foreground pixels and refines the segmentation boundaries. This avoids propagating erroneous gradients due to incorrect foreground labels on the background. Additionally, we learn pixel embeddings to simultaneously optimize for high intra-class feature affinity while increasing discrimination between features across different classes. Our method, Box2Seg, achieves state-of-the-art segmentation accuracy on PASCAL VOC 2012 by significantly improving the mIOU metric by \\(2.1\\%\\) compared to previous weakly supervised approaches. Our weakly supervised approach is comparable to the recent fully supervised methods when fine-tuned with limited amount of pixel-level annotations. Qualitative results and ablation studies show the benefit of different loss terms on the overall performance.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_18');
INSERT INTO `paper` VALUES (11616, 'Bridging Knowledge Graphs to Generate Scene Graphs', '', '', '', '', '', 'Scene graphs are powerful representations that parse images into their abstract semantic elements, i.e., objects and their interactions, which facilitates visual comprehension and explainable reasoning. On the other hand, commonsense knowledge graphs are rich repositories that encode how the world is structured, and how general concepts interact. In this paper, we present a unified formulation of these two constructs, where a scene graph is seen as an image-conditioned instantiation of a commonsense knowledge graph. Based on this new perspective, we re-formulate scene graph generation as the inference of a bridge between the scene and commonsense graphs, where each entity or predicate instance in the scene graph has to be linked to its corresponding entity or predicate class in the commonsense graph. To this end, we propose a novel graph-based neural network that iteratively propagates information between the two graphs, as well as within each of them, while gradually refining their bridge in each iteration. Our Graph Bridging Network, GB-Net, successively infers edges and nodes, allowing to simultaneously exploit and refine the rich, heterogeneous structure of the interconnected scene and commonsense graphs. Through extensive experimentation, we showcase the superior accuracy of GB-Net compared to the most recent methods, resulting in a new state of the art. We publicly release the source code of our method (https://github.com/alirezazareian/gbnet).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_36');
INSERT INTO `paper` VALUES (11617, 'BroadFace: Looking at Tens of Thousands of People at once for Face Recognition', 'Face recognition', 'Large mini-batch learning', 'Image retrieval', '', '', 'The datasets of face recognition contain an enormous number of identities and instances. However, conventional methods have difficulty in reflecting the entire distribution of the datasets because a mini-batch of small size contains only a small portion of all identities. To overcome this difficulty, we propose a novel method called BroadFace, which is a learning process to consider a massive set of identities, comprehensively. In BroadFace, a linear classifier learns optimal decision boundaries among identities from a large number of embedding vectors accumulated over past iterations. By referring more instances at once, the optimality of the classifier is naturally increased on the entire datasets. Thus, the encoder is also globally optimized by referring the weight matrix of the classifier. Moreover, we propose a novel compensation method to increase the number of referenced instances in the training stage. BroadFace can be easily applied on many existing methods to accelerate a learning process and obtain a significant improvement in accuracy without extra computational burden at inference stage. We perform extensive ablation studies and experiments on various datasets to show the effectiveness of BroadFace, and also empirically prove the validity of our compensation method. BroadFace achieves the state-of-the-art results with significant improvements on nine datasets in 1:1 face verification and 1:N face identification tasks, and is also effective in image retrieval.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_31');
INSERT INTO `paper` VALUES (11618, 'BSL-1K: Scaling Up Co-articulated Sign Language Recognition Using Mouthing Cues', 'Sign language recognition', 'Visual keyword spotting', '', '', '', 'Recent progress in fine-grained gesture and action classification, and machine translation, point to the possibility of automated sign language recognition becoming a reality. A key stumbling block in making progress towards this goal is a lack of appropriate training data, stemming from the high complexity of sign annotation and a limited supply of qualified annotators. In this work, we introduce a new scalable approach to data collection for sign recognition in continuous videos. We make use of weakly-aligned subtitles for broadcast footage together with a keyword spotting method to automatically localise sign-instances for a vocabulary of 1,000 signs in 1,000 h of video. We make the following contributions: (1) We show how to use mouthing cues from signers to obtain high-quality annotations from video data—the result is the BSL-1K dataset, a collection of British Sign Language (BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train strong sign recognition models for co-articulated signs in BSL and that these models additionally form excellent pretraining for other sign languages and benchmarks—we exceed the state of the art on both the MSASL and WLASL benchmarks. Finally, (3) we propose new large-scale evaluation sets for the tasks of sign recognition and sign spotting and provide baselines which we hope will serve to stimulate research in this area.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_3');
INSERT INTO `paper` VALUES (11619, 'BTWD: Bag of Tricks for Wheat Detection', 'Object detection', 'Wheat head', 'Bag of Tricks for Wheat Detection (BTWD)', '', '', 'Accurate detection of wheat heads outdoors is a great challenge. Wheat color and shape distinctions, as well as overlaps and wind blurring in wheat photos, make it difficult to detect wheat heads. We propose a Bag of Tricks for Wheat Detection (BTWD), finding that a reasonable combination of some tricks will bring great improvement to the wheat detection results, and apply it on different networks such as YOLO v5x, YOLO v3, EfficientDet-D5, Faster R-CNN, etc. BTWD has greatly enhanced comparison with the original network without tricks. YOLO v5x with BTWD achieves 77.07% in average mAP, in comparison, only 70.78% without it on the Global Wheat Head Detection (GWHD) dataset.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_31');
INSERT INTO `paper` VALUES (11620, 'Burst Denoising via Temporally Shifted Wavelet Transforms', 'Burst denoising', 'Wavelet transform', 'Deep learning', '', '', 'Mobile photography has made great strides in recent years. However, low light imaging remains a challenge. Long exposures can improve signal-to-noise ratio (SNR) but undesirable motion blur can occur when capturing dynamic scenes. Consequently, imaging pipelines often rely on computational photography to improve SNR by fusing multiple short exposures. Recent deep network-based methods have been shown to generate visually pleasing results by fusing these exposures in a sophisticated manner, but often at a higher computational cost.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_15');
INSERT INTO `paper` VALUES (11621, 'ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images', 'Generative Adversarial Networks', 'Face Attributes Manipulation', 'Face recognition', '', '', 'In this paper, we propose a novel image-to-image GAN framework for eyeglasses removal, called ByeGlassesGAN, which is used to automatically detect the position of eyeglasses and then remove them from face images. Our ByeGlassesGAN consists of an encoder, a face decoder, and a segmentation decoder. The encoder is responsible for extracting information from the source face image, and the face decoder utilizes this information to generate glasses-removed images. The segmentation decoder is included to predict the segmentation mask of eyeglasses and completed face region. The feature vectors generated by the segmentation decoder are shared with the face decoder, which facilitates better reconstruction results. Our experiments show that ByeGlassesGAN can provide visually appealing results in the eyeglasses-removed face images even for semi-transparent color eyeglasses or glasses with glare. Furthermore, we demonstrate significant improvement in face recognition accuracy for face images with glasses by applying our method as a pre-processing step in our face recognition experiment.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_15');
INSERT INTO `paper` VALUES (11622, 'BézierSketch: A Generative Model for Scalable Vector Sketches', 'Sketch generation', 'Scalable graphics', 'Bézier curve', '', '', 'The study of neural generative models of human sketches is a fascinating contemporary modeling problem due to the links between sketch image generation and the human drawing process. The landmark SketchRNN provided breakthrough by sequentially generating sketches as a sequence of waypoints. However this leads to low-resolution image generation, and failure to model long sketches. In this paper we present BézierSketch, a novel generative model for fully vector sketches that are automatically scalable and high-resolution. To this end, we first introduce a novel inverse graphics approach to stroke embedding that trains an encoder to embed each stroke to its best fit Bézier curve. This enables us to treat sketches as short sequences of paramaterized strokes and thus train a recurrent sketch generator with greater capacity for longer sketches, while producing scalable high-resolution results. We report qualitative and quantitative results on the Quick, Draw! benchmark.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_38');
INSERT INTO `paper` VALUES (11623, 'C4AV: Learning Cross-Modal Representations from Transformers', 'Object referral', 'Cross-modal representations', '', '', '', 'In this paper, we focus on the object referral problem in the autonomous driving setting. We propose a novel framework to learn cross-modal representations from transformers. In order to extract the linguistic feature, we feed the input command to the transformer encoder. Meanwhile, we use a resnet as the backbone for the image feature learning. The image features are flattened and used as the query inputs to the transformer decoder. The image feature and the linguistic feature are aggregated in the transformer decoder. A region-of-interest (RoI) alignment is applied to the feature map output from the transformer decoder to crop the RoI features for region proposals. Finally, a multi-layer classifier is used for object referral from the features of proposal regions.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_3');
INSERT INTO `paper` VALUES (11624, 'CA-GAN: Weakly Supervised Color Aware GAN for Controllable Makeup Transfer', 'Image synthesis', 'GANs', 'Weakly supervised learning', 'Makeup style transfer', '', 'While existing makeup style transfer models perform an image synthesis whose results cannot be explicitly controlled, the ability to modify makeup color continuously is a desirable property for virtual try-on applications. We propose a new formulation for the makeup style transfer task, with the objective to learn a color controllable makeup style synthesis. We introduce CA-GAN, a generative model that learns to modify the color of specific objects (e.g. lips or eyes) in the image to an arbitrary target color while preserving background. Since color labels are rare and costly to acquire, our method leverages weakly supervised learning for conditional GANs. This enables to learn a controllable synthesis of complex objects, and only requires a weak proxy of the image attribute that we desire to modify. Finally, we present for the first time a quantitative analysis of makeup style transfer and color control performance.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_17');
INSERT INTO `paper` VALUES (11625, 'CAD-Deform: Deformable Fitting of CAD Models to 3D Scans', 'Scene reconstruction', 'Mesh deformation', '', '', '', 'Shape retrieval and alignment are a promising avenue towards turning 3D scans into lightweight CAD representations that can be used for content creation such as mobile or AR/VR gaming scenarios. Unfortunately, CAD model retrieval is limited by the availability of models in standard 3D shape collections (e.g., ShapeNet). In this work, we address this shortcoming by introducing CAD-Deform (The code for the project: https://github.com/alexeybokhovkin/CAD-Deform), a method which obtains more accurate CAD-to-scan fits by non-rigidly deforming retrieved CAD models. Our key contribution is a new non-rigid deformation model incorporating smooth transformations and preservation of sharp features, that simultaneously achieves very tight fits from CAD models to the 3D scan and maintains the clean, high-quality surface properties of hand-modeled CAD objects. A series of thorough experiments demonstrate that our method achieves significantly tighter scan-to-CAD fits, allowing a more accurate digital replica of the scanned real-world environment while preserving important geometric features present in synthetic CAD environments.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_36');
INSERT INTO `paper` VALUES (11626, 'CAFE-GAN: Arbitrary Face Attribute Editing with Complementary Attention Feature', 'Face attribute editing', 'GAN', 'Complementary attention feature', 'Complementary feature matching', '', 'The goal of face attribute editing is altering a facial image according to given target attributes such as hair color, mustache, gender, etc. It belongs to the image-to-image domain transfer problem with a set of attributes considered as a distinctive domain. There have been some works in multi-domain transfer problem focusing on facial attribute editing employing Generative Adversarial Network (GAN). These methods have reported some successes but they also result in unintended changes in facial regions - meaning the generator alters regions unrelated to the specified attributes. To address this unintended altering problem, we propose a novel GAN model which is designed to edit only the parts of a face pertinent to the target attributes by the concept of Complementary Attention Feature (CAFE). CAFE identifies the facial regions to be transformed by considering both target attributes as well as “complementary attributes”, which we define as those attributes absent in the input facial image. In addition, we introduce a complementary feature matching to help in training the generator for utilizing the spatial information of attributes. Effectiveness of the proposed method is demonstrated by analysis and comparison study with state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_31');
INSERT INTO `paper` VALUES (11627, 'Calibration-Free Structure-from-Motion with Calibrated Radial Trifocal Tensors', '', '', '', '', '', 'In this paper we consider the problem of Structure-from-Motion from images with unknown intrinsic calibration. Instead of estimating the internal camera parameters through some self-calibration procedure, we propose to use a subset of the reprojection constraints that is invariant to radial displacement. This allows us to recover metric 3D reconstructions without explicitly estimating the cameras’ focal length or radial distortion parameters. The weaker projection model makes initializing the reconstruction especially difficult. To handle this additional challenge we propose two novel minimal solvers for radial trifocal tensor estimation. We evaluate our approach on real images and show that even for extreme optical systems, such as fisheye or catadioptric, we are able to get accurate reconstructions without performing any calibration.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_23');
INSERT INTO `paper` VALUES (11628, 'Can You Read Me Now? Content Aware Rectification Using Angle Supervision', '', '', '', '', '', 'The ubiquity of smartphone cameras has led to more and more documents being captured by cameras rather than scanned. Unlike flatbed scanners, photographed documents are often folded and crumpled, resulting in large local variance in text structure. The problem of document rectification is fundamental to the Optical Character Recognition (OCR) process on documents, and its ability to overcome geometric distortions significantly affects recognition accuracy. Despite the great progress in recent OCR systems, most still rely on a pre-process that ensures the text lines are straight and axis aligned. Recent works have tackled the problem of rectifying document images taken in-the-wild using various supervision signals and alignment means. However, they focused on global features that can be extracted from the document’s boundaries, ignoring various signals that could be obtained from the document’s content.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_13');
INSERT INTO `paper` VALUES (11629, 'Caption-Supervised Face Recognition: Training a State-of-the-Art Face Model Without Manual Annotation', '', '', '', '', '', 'The advances over the past several years have pushed the performance of face recognition to an amazing level. This great success, to a large extent, is built on top of millions of annotated samples. However, as we endeavor to take the performance to the next level, the reliance on annotated data becomes a major obstacle. We desire to explore an alternative approach, namely using captioned images for training, as an attempt to mitigate this difficulty. Captioned images are widely available on the web, while the captions often contain the names of the subjects in the images. Hence, an effective method to leverage such data would significantly reduce the need of human annotations. However, an important challenge along this way needs to be tackled: the names in the captions are often noisy and ambiguous, especially when there are multiple names in the captions or multiple people in the photos. In this work, we propose a simple yet effective method, which trains a face recognition model by progressively expanding the labeled set via both selective propagation and caption-driven expansion. We build a large-scale dataset of captioned images, which contain 6.3M faces from 305K subjects. Our experiments show that using the proposed method, we can train a state-of-the-art face recognition model without manual annotation (\\(99.65\\%\\) in LFW). This shows the great potential of caption-supervised face recognition.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_9');
INSERT INTO `paper` VALUES (11630, 'Captioning Images Taken by People Who Are Blind', '', '', '', '', '', 'While an important problem in the vision community is to design algorithms that can automatically caption images, few publicly-available datasets for algorithm development directly address the interests of real users. Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of over 39,000 images originating from people who are blind that are each paired with five captions. We analyze this dataset to (1) characterize the typical captions, (2) characterize the diversity of content found in the images, and (3) compare its content to that found in eight popular vision datasets. We also analyze modern image captioning algorithms to identify what makes this new dataset challenging for the vision community. We publicly-share the dataset with captioning challenge instructions at https://vizwiz.org.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_25');
INSERT INTO `paper` VALUES (11631, 'Cardiac MR Image Sequence Segmentation with Temporal Motion Encoding', 'Cardiac MRI', 'LV segmentation', 'Temporal', 'Motion', '', 'The segmentation of cardiac magnetic resonance (MR) images is a critical step for the accurate assessment of cardiac function and the diagnosis of cardiovascular diseases. In this work, we propose a novel segmentation method that is able to effectively leverage the temporal information in cardiac MR image sequences. Specifically, we construct a Temporal Aggregation Module (TAM) to incorporate the temporal image-based features into a backbone spatial segmentation network (such as a 2D U-Net) with negligible extra computation cost. In addition, we also introduce a novel Motion Encoding Module (MEM) to explicitly encode the motion features of the heart. Experimental results demonstrate that each of the two modules enables clear improvements upon the base spatial network, and their combination leads to further enhanced performance. The proposed method outperforms the previous methods significantly, demonstrating the effectiveness of our design.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_19');
INSERT INTO `paper` VALUES (11632, 'Cascade Graph Neural Networks for RGB-D Salient Object Detection', 'Salient object detection', 'RGB-D perception', 'Graph neural networks', '', '', 'In this paper, we study the problem of salient object detection (SOD) for RGB-D images using both color and depth information. A major technical challenge in performing salient object detection from RGB-D images is how to fully leverage the two complementary data sources. Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors. In this work, we introduce Cascade Graph Neural Networks (Cas-Gnn), a unified framework which is capable of comprehensively distilling and reasoning the mutual benefits between these two data sources through a set of cascade graphs, to learn powerful representations for RGB-D salient object detection. Cas-Gnn processes the two data sources individually and employs a novel Cascade Graph Reasoning (CGR) module to learn powerful dense feature embeddings, from which the saliency map can be easily inferred. Contrast to the previous approaches, the explicitly modeling and reasoning of high-level relations between complementary data sources allows us to better overcome challenges such as occlusions and ambiguities. Extensive experiments demonstrate that Cas-Gnn achieves significantly better performance than all existing RGB-D SOD approaches on several widely-used benchmarks. Code is available at https://github.com/LA30/Cas-Gnn.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_21');
INSERT INTO `paper` VALUES (11633, 'Cascaded Tracking via Pyramid Dense Capsules', 'Visual tracking', 'Pyramid and dense capsules', 'Cascaded architecture', '', '', 'The tracking-by-detection is a two-stage framework including, collecting the candidates around the target object and classifying each candidate as the target object or as background. Despite Convolutional Neural Networks (CNNs) based methods have been successful in tracking-by-detection framework, the own set of flaws of CNNs will still affect the performance. The underlying mechanism of CNNs that are based on the positional invariance (i.e., lose the spatial relationships between features) cannot capture the small affine transformations. This would ultimately result in drift. To solve this problem, we dig into spatial relationships endowed by the Capsule Networks (CapsNets) for tracking-by-detection framework. To strengthen the encoded power of convolutional capsules, we generate the convolutional capsules through a pyramid dense capsules (PDCaps) architecture. Our pyramid dense capsule representation is useful in producing comprehensive spatial relationships within the input. Besides, the critical challenges in the tracking-by-detection framework are how to avoid overfitting and mismatch during training and inference, where a reasonable intersection over union (IoU) threshold that defines the true/false positives is hard to set. To address the issue of the IoU threshold setting, a cascaded PDCaps model is proposed to improve the quality of candidates, and it consists of a sequential PDCaps model trained with increasing IoU thresholds to improve the quality of candidates sequentially. Extensive experiments demonstrate that our tracker performs favorably against state-of-the-art approaches.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_45');
INSERT INTO `paper` VALUES (11634, 'CATCH: Context-Based Meta Reinforcement Learning for Transferrable Architecture Search', 'Neural architecture search', 'Meta reinforcement learning', '', '', '', 'Neural Architecture Search (NAS) achieved many breakthroughs in recent years. In spite of its remarkable progress, many algorithms are restricted to particular search spaces. They also lack efficient mechanisms to reuse knowledge when confronting multiple tasks. These challenges preclude their applicability, and motivate our proposal of CATCH, a novel Context-bAsed meTa reinforcement learning (RL) algorithm for transferrable arChitecture searcH. The combination of meta-learning and RL allows CATCH to efficiently adapt to new tasks while being agnostic to search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, which then guide CATCH’s controller to quickly “catch” top-performing networks. The contexts also assist a network evaluator in filtering inferior candidates and speed up learning. Extensive experiments demonstrate CATCH’s universality and search efficiency over many other widely-recognized algorithms. It is also capable of handling cross-domain architecture search as competitive networks on ImageNet, COCO, and Cityscapes are identified. This is the first work to our knowledge that proposes an efficient transferrable NAS solution while maintaining robustness across various settings.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_12');
INSERT INTO `paper` VALUES (11635, 'Category Level Object Pose Estimation via Neural Analysis-by-Synthesis', 'Category-level object pose', '6DoF pose estimation', '', '', '', 'Many object pose estimation algorithms rely on the analysis-by-synthesis framework which requires explicit representations of individual object instances. In this paper we combine a gradient-based fitting procedure with a parametric neural image synthesis module that is capable of implicitly representing the appearance, shape and pose of entire object categories, thus rendering the need for explicit CAD models per object instance unnecessary. The image synthesis network is designed to efficiently span the pose configuration space so that model capacity can be used to capture the shape and local appearance (i.e., texture) variations jointly. At inference time the synthesized images are compared to the target via an appearance based loss and the error signal is backpropagated through the network to the input parameters. Keeping the network parameters fixed, this allows for iterative optimization of the object pose, shape and appearance in a joint manner and we experimentally show that the method can recover orientation of objects with high accuracy from 2D images alone. When provided with depth measurements, to overcome scale ambiguities, the method can accurately recover the full 6DOF pose successfully.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_9');
INSERT INTO `paper` VALUES (11636, 'CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations', 'Face anti-spoofing', 'Large-scale dataset', '', '', '', 'As facial interaction systems are prevalently deployed, security and reliability of these systems become a critical issue, with substantial research efforts devoted. Among them, face anti-spoofing emerges as an important area, whose objective is to identify whether a presented face is live or spoof. Though promising progress has been achieved, existing works still have difficulty in handling complex spoof attacks and generalizing to real-world scenarios. The main reason is that current face anti-spoofing datasets are limited in both quantity and diversity. To overcome these obstacles, we contribute a large-scale face anti-spoofing dataset, CelebA-Spoof, with the following appealing properties: 1) Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, significantly larger than the existing datasets. 2) Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination conditions) with more than 10 sensors. 3) Annotation Richness: CelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute annotations inherited from the original CelebA dataset. Equipped with CelebA-Spoof, we carefully benchmark existing methods in a unified multi-task framework, Auxiliary Information Embedding Network (AENet), and reveal several valuable observations. Our key insight is that, compared with the commonly-used binary supervision or mid-level geometric representations, rich semantic annotations as auxiliary tasks can greatly boost the performance and generalizability of face anti-spoofing across a wide range of spoof attacks. Through comprehensive studies, we show that CelebA-Spoof serves as an effective training data source. Models trained on CelebA-Spoof (without fine-tuning) exhibit state-of-the-art performance on standard benchmarks such as CASIA-MFSD. The datasets are available at https://github.com/Davidzhangyuanhan/CelebA-Spoof.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_5');
INSERT INTO `paper` VALUES (11637, 'CenterNet Heatmap Propagation for Real-Time Video Object Detection', 'Video object detection', 'Real-time', 'Heatmap propagation', 'One-stage detector', '', 'The existing methods for video object detection mainly depend on two-stage image object detectors. The fact that two-stage detectors are generally slow makes it difficult to apply in real-time scenarios. Moreover, adapting directly existing methods to a one-stage detector is inefficient or infeasible. In this work, we introduce a method based on a one-stage detector called CenterNet. We propagate the previous reliable long-term detection in the form of heatmap to boost results of upcoming image. Our method achieves the online real-time performance on ImageNet VID dataset with 76.7% mAP at 37 FPS and the offline performance 78.4% mAP at 34 FPS.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_14');
INSERT INTO `paper` VALUES (11638, 'CFAD: Coarse-to-Fine Action Detector for Spatiotemporal Action Localization', 'Spatiotemporal action detection', 'Coarse-to-fine paradigm', 'Parameterized modeling', '', '', 'Most current pipelines for spatio-temporal action localization connect frame-wise or clip-wise detection results to generate action proposals, where only local information is exploited and the efficiency is hindered by dense per-frame localization. In this paper, we propose Coarse-to-Fine Action Detector (CFAD), an original end-to-end trainable framework for efficient spatio-temporal action localization. The CFAD introduces a new paradigm that first estimates coarse spatio-temporal action tubes from video streams, and then refines the tubes’ location based on key timestamps. This concept is implemented by two key components, the Coarse and Refine Modules in our framework. The parameterized modeling of long temporal information in the Coarse Module helps obtain accurate initial tube estimation, while the Refine Module selectively adjusts the tube location under the guidance of key timestamps. Against other methods, the proposed CFAD achieves competitive results on action detection benchmarks of UCF101-24, UCFSports and JHMDB-21 with inference speed that is 3.3\\(\\times \\) faster than the nearest competitor.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_30');
INSERT INTO `paper` VALUES (11639, 'Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking', 'Multiple-object Tracking', 'Chained-Tracker', 'End-to-end solution', 'Joint detection and tracking', '', 'Existing Multiple-Object Tracking (MOT) methods either follow the tracking-by-detection paradigm to conduct object detection, feature extraction and data association separately, or have two of the three subtasks integrated to form a partially end-to-end solution. Going beyond these sub-optimal frameworks, we propose a simple online model named Chained-Tracker (CTracker), which naturally integrates all the three subtasks into an end-to-end solution (the first as far as we know). It chains paired bounding boxes regression results estimated from overlapping nodes, of which each node covers two adjacent frames. The paired regression is made attentive by object-attention (brought by a detection module) and identity-attention (ensured by an ID verification module). The two major novelties: chained structure and paired attentive regression, make CTracker simple, fast and effective, setting new MOTA records on MOT16 and MOT17 challenge datasets (67.6 and 66.6, respectively), without relying on any extra training data. The source code of CTracker can be found at: github.com/pjl1995/CTracker.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_9');
INSERT INTO `paper` VALUES (11640, 'Challenge-Aware RGBT Tracking', 'Rgbt tracking', 'Challenge modelling', 'Guidance module', 'Insufficient training data', '', 'RGB and thermal source data suffer from both shared and specific challenges, and how to explore and exploit them plays a critical role to represent the target appearance in RGBT tracking. In this paper, we propose a novel challenge-aware neural network to handle the modality-shared challenges (e.g., fast motion, scale variation and occlusion) and the modality-specific ones (e.g., illumination variation and thermal crossover) for RGBT tracking. In particular, we design several parameter-shared branches in each layer to model the target appearance under the modality-shared challenges, and several parameter-independent branches under the modality-specific ones. Based on the observation that the modality-specific cues of different modalities usually contains the complementary advantages, we propose a guidance module to transfer discriminative features from one modality to another one, which could enhance the discriminative ability of some weak modality. Moreover, all branches are aggregated together in an adaptive manner and parallel embedded in the backbone network to efficiently form more discriminative target representations. These challenge-aware branches are able to model the target appearance under certain challenges so that the target representations can be learnt by a few parameters even in the situation of insufficient training data. From the experimental results we will show that our method operates at a real-time speed while performing well against the state-of-the-art methods on three benchmark datasets.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_14');
INSERT INTO `paper` VALUES (11641, 'Challenges from Fast Camera Motion and Image Blur: Dataset and Evaluation', 'Dataset', 'Fast camera motion', 'Image motion blur', 'SLAM', 'Deblurring', 'To study the impact of the camera motion speed for image/video based tasks, we propose an extendable synthetic dataset based on real image sequences. In our dataset, image sequences with different camera speeds featuring the same scene and the same camera trajectory. To synthesize a photo-realistic image sequence with fast camera motions, we propose an image blur synthesis method that generates blurry images by their sharp images, camera motions and the reconstructed 3D scene model. Experiments show that our synthetic blurry images are more realistic than the ones synthesized by existing methods. Based on our synthetic dataset, one can study the performance of an algorithm in different camera motions. In this paper, we evaluate several mainstream methods of two relevant tasks: visual SLAM and image deblurring. Through our evaluations, we draw some conclusions about the robustness of these methods in the face of different camera speeds and image motion blur.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_16');
INSERT INTO `paper` VALUES (11642, 'Channel Selection Using Gumbel Softmax', 'Network sparsity', 'Channel pruning', 'Dynamic computation', 'Gumbel softmax', '', 'Important applications such as mobile computing require reducing the computational costs of neural network inference. Ideally, applications would specify their preferred tradeoff between accuracy and speed, and the network would optimize this end-to-end, using classification error to remove parts of the network. Increasing speed can be done either during training – e.g., pruning filters – or during inference – e.g., conditionally executing a subset of the layers. We propose a single end-to-end framework that can improve inference efficiency in both settings. We use a combination of batch activation loss and classification loss, and Gumbel reparameterization to learn network structure. We train end-to-end, and the same technique supports pruning as well as conditional computation. We obtain promising experimental results for ImageNet classification with ResNet (45–52% less computation).', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_15');
INSERT INTO `paper` VALUES (11643, 'Character Grounding and Re-identification in Story of Videos and Text Descriptions', '', '', '', '', '', 'We address character grounding and re-identification in multiple story-based videos like movies and associated text descriptions. In order to solve these related tasks in a mutually rewarding way, we propose a model named Character in Story Identification Network (CiSIN). Our method builds two semantically informative representations via joint training of multiple objectives for character grounding, video/text re-identification and gender prediction: Visual Track Embedding from videos and Textual Character Embedding from text context. These two representations are learned to retain rich semantic multimodal information that enables even simple MLPs to achieve the state-of-the-art performance on the target tasks. More specifically, our CiSIN model achieves the best performance in the Fill-in the Characters task of LSMDC 2019 challenges [35]. Moreover, it outperforms previous state-of-the-art models in M-VAD Names dataset [30] as a benchmark of multimodal character grounding and re-identification.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_32');
INSERT INTO `paper` VALUES (11644, 'Character Region Attention for Text Spotting', 'Optical character recognition (OCR)', 'Character Region Attention', 'Text spotting', 'Scene text detection', 'Scene text recognition', 'A scene text spotter is composed of text detection and recognition modules. Many studies have been conducted to unify these modules into an end-to-end trainable model to achieve better performance. A typical architecture places detection and recognition modules into separate branches, and a RoI pooling is commonly used to let the branches share a visual feature. However, there still exists a chance of establishing a more complimentary connection between the modules when adopting recognizer that uses attention-based decoder and detector that represents spatial information of the character regions. This is possible since the two modules share a common sub-task which is to find the location of the character regions. Based on the insight, we construct a tightly coupled single pipeline model. This architecture is formed by utilizing detection outputs in the recognizer and propagating the recognition loss through the detection stage. The use of character score map helps the recognizer attend better to the character center points, and the recognition loss propagation to the detector module enhances the localization of the character regions. Also, a strengthened sharing stage allows feature rectification and boundary localization of arbitrary-shaped text regions. Extensive experiments demonstrate state-of-the-art performance in publicly available straight and curved benchmark dataset.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_30');
INSERT INTO `paper` VALUES (11645, 'Character-Preserving Coherent Story Visualization', 'Story visualization', 'Evaluation metric', 'Foreground segmentation', '', '', 'Story visualization aims at generating a sequence of images to narrate each sentence in a multi-sentence story. Different from video generation that focuses on maintaining the continuity of generated images (frames), story visualization emphasizes preserving the global consistency of characters and scenes across different story pictures, which is very challenging since story sentences only provide sparse signals for generating images. Therefore, we propose a new framework named Character-Preserving Coherent Story Visualization (CP-CSV) to tackle the challenges. CP-CSV effectively learns to visualize the story by three critical modules: story and context encoder (story and sentence representation learning), figure-ground segmentation (auxiliary task to provide information for preserving character and story consistency), and figure-ground aware generation (image sequence generation by incorporating figure-ground information). Moreover, we propose a metric named Fréchet Story Distance (FSD) to evaluate the performance of story visualization. Extensive experiments demonstrate that CP-CSV maintains the details of character information and achieves high consistency among different frames, while FSD better measures the performance of story visualization.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_2');
INSERT INTO `paper` VALUES (11646, 'Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection', 'Pre-training', 'Object detection', 'Acceleration', 'Deep neural networks', 'Deep learning', 'In this paper, we propose a general and efficient pre-training paradigm, Montage pre-training, for object detection. Montage pre-training needs only the target detection dataset while taking only 1/4 computational resources compared to the widely adopted ImageNet pre-training. To build such an efficient paradigm, we reduce the potential redundancy by carefully extracting useful samples from the original images, assembling samples in a Montage manner as input, and using an ERF-adaptive dense classification strategy for model pre-training. These designs include not only a new input pattern to improve the spatial utilization but also a novel learning objective to expand the effective receptive field of the pre-trained model. The efficiency and effectiveness of Montage pre-training are validated by extensive experiments on the MS-COCO dataset, where the results indicate that the models using Montage pre-training are able to achieve on-par or even better detection performances compared with the ImageNet pre-training.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_16');
INSERT INTO `paper` VALUES (11647, 'Circumventing Outliers of AutoAugment with Knowledge Distillation', 'AutoML', 'AutoAugment', 'Knowledge distillation', '', '', 'AutoAugment has been a powerful algorithm that improves the accuracy of many vision tasks, yet it is sensitive to the operator space as well as hyper-parameters, and an improper setting may degenerate network optimization. This paper delves deep into the working mechanism, and reveals that AutoAugment may remove part of discriminative information from the training image and so insisting on the ground-truth label is no longer the best option. To relieve the inaccuracy of supervision, we make use of knowledge distillation that refers to the output of a teacher model to guide network training. Experiments are performed in standard image classification benchmarks, and demonstrate the effectiveness of our approach in suppressing noise of data augmentation and stabilizing training. Upon the cooperation of knowledge distillation and AutoAugment, we claim the new state-of-the-art on ImageNet classification with a top-1 accuracy of \\(\\mathbf {85.8\\%}\\).', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_36');
INSERT INTO `paper` VALUES (11648, 'Class-Imbalanced Domain Adaptation: An Empirical Odyssey', '', '', '', '', '', 'Unsupervised domain adaptation is a promising way to generalize deep models to novel domains. However the current literature assumes that the label distribution is domain-invariant and only aligns the feature distributions or vice versa. In this work, we explore the more realistic task of Class-imbalanced Domain Adaptation: How to align feature distributions across domains while the label distributions of the two domains are also different? Taking a practical step towards this problem, we constructed its first benchmark with 22 cross-domain tasks from 6 real-image datasets. We conducted comprehensive experiments on 10 recent domain adaptation methods and find most of them are very fragile in the face of coexisting feature and label distribution shift. Towards a better solution, we further proposed a feature and label distribution CO-ALignment (COAL) model with a novel combination of existing ideas. COAL is empirically shown to outperform most recent domain adaptation methods on our benchmarks. We believe the provided benchmarks, empirical analysis results, and the COAL baseline could stimulate and facilitate future research towards this important problem.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_38');
INSERT INTO `paper` VALUES (11649, 'Class-Incremental Domain Adaptation', '', '', '', '', '', 'We introduce a practical Domain Adaptation (DA) paradigm called Class-Incremental Domain Adaptation (CIDA). Existing DA methods tackle domain-shift but are unsuitable for learning novel target-domain classes. Meanwhile, class-incremental (CI) methods enable learning of new classes in absence of source training data, but fail under a domain-shift without labeled supervision. In this work, we effectively identify the limitations of these approaches in the CIDA paradigm. Motivated by theoretical and empirical observations, we propose an effective method, inspired by prototypical networks, that enables classification of target samples into both shared and novel (one-shot) target classes, even under a domain-shift. Our approach yields superior performance as compared to both DA and CI methods in the CIDA paradigm.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_4');
INSERT INTO `paper` VALUES (11650, 'Class-Wise Dynamic Graph Convolution for Semantic Segmentation', 'Semantic segmentation', 'Graph convolution', 'Coarse-to-fine framework', '', '', 'Recent works have made great progress in semantic segmentation by exploiting contextual information in a local or global manner with dilated convolutions, pyramid pooling or self-attention mechanism. In order to avoid potential misleading contextual information aggregation in previous works, we propose a class-wise dynamic graph convolution (CDGC) module to adaptively propagate information. The graph reasoning is performed among pixels in the same class. Based on the proposed CDGC module, we further introduce the Class-wise Dynamic Graph Convolution Network (CDGCNet), which consists of two main parts including the CDGC module and a basic segmentation network, formi2ng a coarse-to-fine paradigm. Specifically, the CDGC module takes the coarse segmentation result as class mask to extract node features for graph construction and performs dynamic graph convolutions on the constructed graph to learn the feature aggregation and weight allocation. Then the refined feature and the original feature are fused to get the final prediction. We conduct extensive experiments on three popular semantic segmentation benchmarks including Cityscapes, PASCAL VOC 2012 and COCO Stuff, and achieve state-of-the-art performance on all three benchmarks.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_1');
INSERT INTO `paper` VALUES (11651, 'Classes Matter: A Fine-Grained Adversarial Approach to Cross-Domain Semantic Segmentation', '', '', '', '', '', 'Despite great progress in supervised semantic segmentation, a large performance drop is usually observed when deploying the model in the wild. Domain adaptation methods tackle the issue by aligning the source domain and the target domain. However, most existing methods attempt to perform the alignment from a holistic view, ignoring the underlying class-level data structure in the target domain. To fully exploit the supervision in the source domain, we propose a fine-grained adversarial learning strategy for class-level feature alignment while preserving the internal structure of semantics across domains. We adopt a fine-grained domain discriminator that not only plays as a domain distinguisher, but also differentiates domains at class level. The traditional binary domain labels are also generalized to domain encodings as the supervision signal to guide the fine-grained feature alignment. An analysis with Class Center Distance (CCD) validates that our fine-grained adversarial strategy achieves better class-level alignment compared to other state-of-the-art methods. Our method is easy to implement and its effectiveness is evaluated on three classical domain adaptation tasks, i.e., GTA5\\(\\rightarrow \\)Cityscapes, SYNTHIA\\(\\rightarrow \\)Cityscapes and Cityscapes\\(\\rightarrow \\)Cross-City. Large performance gains show that our method outperforms other global feature alignment based and class-wise alignment based counterparts. The code is publicly available at https://github.com/JDAI-CV/FADA.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_38');
INSERT INTO `paper` VALUES (11652, 'Classifying Nuclei Shape Heterogeneity in Breast Tumors with Skeletons', 'Medial axis transform', 'Breast cancer', 'Digital pathology', 'Computer vision', '', 'In this study, we demonstrate the efficacy of scoring statistics derived from a medial axis transform, for differentiating tumor and non-tumor nuclei, in malignant breast tumor histopathology images. Characterizing nuclei shape is a crucial part of diagnosing breast tumors for human doctors, and these scoring metrics may be integrated into machine perception algorithms which aggregate nuclei information across a region to label whole breast lesions. In particular, we present a low-dimensional representation capturing characteristics of a skeleton extracted from nuclei. We show that this representation outperforms both prior morphological features, as well as CNN features, for classification of tumors. Nuclei and region scoring algorithms such as the one presented here can aid pathologists in the diagnosis of breast tumors.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_20');
INSERT INTO `paper` VALUES (11653, 'CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection', 'Weakly supervised learning', 'Anomaly detection', 'Abnormal event detection', 'Noisy labeled training', 'Event localization', 'Learning to detect real-world anomalous events through video-level labels is a challenging task due to the rare occurrence of anomalies as well as noise in the labels. In this work, we propose a weakly supervised anomaly detection method which has manifold contributions including 1) a random batch based training procedure to reduce inter-batch correlation, 2) a normalcy suppression mechanism to minimize anomaly scores of the normal regions of a video by taking into account the overall information available in one training batch, and 3) a clustering distance based loss to contribute towards mitigating the label noise and to produce better anomaly representations by encouraging our model to generate distinct normal and anomalous clusters. The proposed method obtains 83.03% and 89.67% frame-level AUC performance on the UCF-Crime and ShanghaiTech datasets respectively, demonstrating its superiority over the existing state-of-the-art algorithms.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_22');
INSERT INTO `paper` VALUES (11654, 'CLIFFNet for Monocular Depth Estimation with Hierarchical Embedding Loss', 'Monocular depth estimation', 'Hierarchical loss', 'Hierarchical embedding space', 'Feature fusion', '', 'This paper proposes a hierarchical loss for monocular depth estimation, which measures the differences between the prediction and ground truth in hierarchical embedding spaces of depth maps. In order to find an appropriate embedding space, we design different architectures for hierarchical embedding generators (HEGs) and explore relevant tasks to train their parameters. Compared to conventional depth losses manually defined on a per-pixel basis, the proposed hierarchical loss can be learned in a data-driven manner. As verified by our experiments, the hierarchical loss even learned without additional labels can capture multi-scale context information, is more robust to local outliers, and thus delivers superior performance. To further improve depth accuracy, a cross level identity feature fusion network (CLIFFNet) is proposed, where low-level features with finer details are refined using more reliable high-level cues. Through end-to-end training, CLIFFNet can learn to select the optimal combinations of low-level and high-level features, leading to more effective cross level feature fusion. When trained using the proposed hierarchical loss, CLIFFNet sets a new state of the art on popular depth estimation benchmarks.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_19');
INSERT INTO `paper` VALUES (11655, 'CLNet: A Compact Latent Network for Fast Adjusting Siamese Trackers', 'Siamese tracker', 'Latent feature', 'Sequence-specific', 'Sample mining', 'Fast adjustment', 'In this paper, we provide a deep analysis for Siamese-based trackers and find that the one core reason for their failure on challenging cases can be attributed to the problem of decisive samples missing during offline training. Furthermore, we notice that the samples given in the first frame can be viewed as the decisive samples for the sequence since they contain rich sequence-specific information. To make full use of these sequence-specific samples, we propose a compact latent network to quickly adjust the tracking model to adapt to new scenes. A statistic-based compact latent feature is proposed to efficiently capture the sequence-specific information for the fast adjustment. In addition, we design a new training approach based on a diverse sample mining strategy to further improve the discrimination ability of our compact latent network. To evaluate the effectiveness of our method, we apply it to adjust a recent state-of-the-art tracker, SiamRPN++. Extensive experimental results on five recent benchmarks demonstrate that the adjusted tracker achieves promising improvement in terms of tracking accuracy, with almost the same speed. The code and models are available at https://github.com/xingpingdong/CLNet-tracking.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_23');
INSERT INTO `paper` VALUES (11656, 'CLOTH3D: Clothed 3D Humans', '3D', 'Human', 'Garment', 'Cloth', 'Dataset', 'We present CLOTH3D, the first big scale synthetic dataset of 3D clothed human sequences. CLOTH3D contains a large variability on garment type, topology, shape, size, tightness and fabric. Clothes are simulated on top of thousands of different pose sequences and body shapes, generating realistic cloth dynamics. We provide the dataset with a generative model for cloth generation. We propose a Conditional Variational Auto-Encoder (CVAE) based on graph convolutions (GCVAE) to learn garment latent spaces. This allows for realistic generation of 3D garments on top of SMPL model for any pose and shape.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_21');
INSERT INTO `paper` VALUES (11657, 'Clustering Driven Deep Autoencoder for Video Anomaly Detection', 'Video anomaly detection', 'Spatio-temporal dissociation', 'Deep k-means cluster', '', '', 'Because of the ambiguous definition of anomaly and the complexity of real data, video anomaly detection is one of the most challenging problems in intelligent video surveillance. Since the abnormal events are usually different from normal events in appearance and/or in motion behavior, we address this issue by designing a novel convolution autoencoder architecture to separately capture spatial and temporal informative representation. The spatial part reconstructs the last individual frame (LIF), while the temporal part takes consecutive frames as input and RGB difference as output to simulate the generation of optical flow. The abnormal events which are irregular in appearance or in motion behavior lead to a large reconstruction error. Besides, we design a deep k-means cluster to force the appearance and the motion encoder to extract common factors of variation within the dataset. Experiments on some publicly available datasets demonstrate the effectiveness of our method with the state-of-the-art performance.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_20');
INSERT INTO `paper` VALUES (11658, 'CN: Channel Normalization for Point Cloud Recognition', '3D recognition', 'Point cloud', 'Object detection', 'Classification', '', 'In 3D recognition, to fuse multi-scale structure information, existing methods apply hierarchical frameworks stacked by multiple fusion layers for integrating current relative locations with structure information from the previous level. In this paper, we deeply analyze these point recognition frameworks and present a factor, called difference ratio, to measure the influence of structure information among different levels on the final representation. We discover that structure information in deeper layers is overwhelmed by information in shallower layers in generating the final features, which prevents the model from understanding the point cloud in a global view. Inspired by this observation, we propose a novel channel normalization scheme to balance structure information among different layers and avoid excessive accumulation of shallow information, which benefits the model in exploiting and integrating multilayer structure information. We evaluate our channel normalization in several core 3D recognition tasks including classification, segmentation and detection. Experimental results show that our channel normalization further boosts the performance of state-of-the-art methods effectively.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_35');
INSERT INTO `paper` VALUES (11659, 'Co-heterogeneous and Adaptive Segmentation from Multi-source and Multi-phase CT Imaging Data: A Study on Pathological Liver and Lesion Segmentation', 'Multi-phase segmentation', 'Semi-supervised learning', 'Co-training', 'Domain adaptation', 'Liver and lesion segmentation', 'Within medical imaging, organ/pathology segmentation models trained on current publicly available and fully-annotated datasets usually do not well-represent the heterogeneous modalities, phases, pathologies, and clinical scenarios encountered in real environments. On the other hand, there are tremendous amounts of unlabelled patient imaging scans stored by many modern clinical centers. In this work, we present a novel segmentation strategy, co-heterogenous and adaptive segmentation (CHASe), which only requires a small labeled cohort of single phase data to adapt to any unlabeled cohort of heterogenous multi-phase data with possibly new clinical scenarios and pathologies. To do this, we develop a versatile framework that fuses appearance-based semi-supervision, mask-based adversarial domain adaptation, and pseudo-labeling. We also introduce co-heterogeneous training, which is a novel integration of co-training and hetero-modality learning. We evaluate CHASe using a clinically comprehensive and challenging dataset of multi-phase computed tomography (CT) imaging studies (1147 patients and 4577 3D volumes), with a test set of 100 patients. Compared to previous state-of-the-art baselines, CHASe can further improve pathological liver mask Dice-Sørensen coefficients by ranges of \\(4.2\\%\\) to \\(9.4\\%\\), depending on the phase combinations, e.g., from \\(84.6\\%\\) to \\(94.0\\%\\) on non-contrast CTs.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_27');
INSERT INTO `paper` VALUES (11660, 'COCO-FUNIT: Few-Shot Unsupervised Image Translation with a Content Conditioned Style Encoder', 'Image-to-image translation', 'Generative Adversarial Networks', '', '', '', 'Unsupervised image-to-image translation intends to learn a mapping of an image in a given domain to an analogous image in a different domain, without explicit supervision of the mapping. Few-shot unsupervised image-to-image translation further attempts to generalize the model to an unseen domain by leveraging example images of the unseen domain provided at inference time. While remarkably successful, existing few-shot image-to-image translation models find it difficult to preserve the structure of the input image while emulating the appearance of the unseen domain, which we refer to as the content loss problem. This is particularly severe when the poses of the objects in the input and example images are very different. To address the issue, we propose a new few-shot image translation model, COCO-FUNIT, which computes the style embedding of the example images conditioned on the input image and a new module called the constant style bias. Through extensive experimental validations with comparison to the state-of-the-art, our model shows effectiveness in addressing the content loss problem. Code and pretrained models are available at https://nvlabs.github.io/COCO-FUNIT/.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_23');
INSERT INTO `paper` VALUES (11661, 'Collaboration by Competition: Self-coordinated Knowledge Amalgamation for Multi-talent Student Learning', 'Knowledge amalgamation', 'Competitive collaboration', '', '', '', 'A vast number of well-trained deep networks have been released by developers online for plug-and-play use. These networks specialize in different tasks and in many cases, the data and annotations used to train them are not publicly available. In this paper, we study how to reuse such heterogeneous pre-trained models as teachers, and build a versatile and compact student model, without accessing human annotations. To this end, we propose a self-coordinate knowledge amalgamation network (SOKA-Net) for learning the multi-talent student model. This is achieved via a dual-step adaptive competitive-cooperation training approach, where the knowledge of the heterogeneous teachers are in the first step amalgamated to guide the shared parameter learning of the student network, and followed by a gradient-based competition-balancing strategy to learn the multi-head prediction network as well as the loss weightings of the distinct tasks in the second step. The two steps, which we term as the collaboration and competition step respectively, are performed alternatively until the balance of the competition is reached for the ultimate collaboration. Experimental results demonstrate that, the learned student not only comes with a smaller size but achieves performances on par with or even superior to those of the teachers.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_38');
INSERT INTO `paper` VALUES (11662, 'Collaborative Learning of Gesture Recognition and 3D Hand Pose Estimation with Multi-order Feature Analysis', 'Gesture recognition', '3D hand pose estimation', 'Multi-order multi-stream feature analysis', 'Slow-fast feature analysis', 'Multi-scale relation', 'Gesture recognition and 3D hand pose estimation are two highly correlated tasks, yet they are often handled separately. In this paper, we present a novel collaborative learning network for joint gesture recognition and 3D hand pose estimation. The proposed network exploits joint-aware features that are crucial for both tasks, with which gesture recognition and 3D hand pose estimation boost each other to learn highly discriminative features. In addition, a novel multi-order multi-stream feature analysis method is introduced which learns posture and multi-order motion information from the intermediate feature maps of videos effectively and efficiently. Due to the exploitation of joint-aware features in common, the proposed technique is capable of learning gesture recognition and 3D hand pose estimation even when only gesture or pose labels are available, and this enables weakly supervised network learning with much reduced data labeling efforts. Extensive experiments show that our proposed method achieves superior gesture recognition and 3D hand pose estimation performance as compared with the state-of-the-art.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_45');
INSERT INTO `paper` VALUES (11663, 'Collaborative Learning with Pseudo Labels for Robust Classification in the Presence of Noisy Labels', 'Deep neural network', 'Noisy labels', 'Pseudo labels', '', '', 'Supervised learning depends on labels of dataset to train models with desired properties. Therefore, data containing mislabeled samples (a.k.a. noisy labels) can deteriorate supervised learning performance significantly as it makes models to be trained with wrong targets. There are technics to train models in the presence of noise in data labels, but they usually suffer from the data inefficiency or overhead of additional steps. In this work, we propose a new way to train supervised learning models in the presence of noisy labels. The proposed approach effectively handles noisy labels while maintaining data efficiency by replacing labels of large-loss instances that are likely to be noise with newly generated pseudo labels in the training process. We conducted experiments to demonstrate the effectiveness of the proposed method with public benchmark datasets: CIFAR-10, CIFAR-100 and Tiny-ImageNet. They showed that our method successfully identified correct labels and performed better than other state-of-the-art algorithms for noisy labels.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_13');
INSERT INTO `paper` VALUES (11664, 'Collaborative Training Between Region Proposal Localization and Classification for Domain Adaptive Object Detection', 'Domain adaptation', 'Object detection', 'Transfer learning', '', '', 'Object detectors are usually trained with large amount of labeled data, which is expensive and labor-intensive. Pre-trained detectors applied to unlabeled dataset always suffer from the difference of dataset distribution, also called domain shift. Domain adaptation for object detection tries to adapt the detector from labeled datasets to unlabeled ones for better performance. In this paper, we are the first to reveal that the region proposal network (RPN) and region proposal classifier (RPC) in the endemic two-stage detectors (e.g., Faster RCNN) demonstrate significantly different transferability when facing large domain gap. The region classifier shows preferable performance but is limited without RPN’s high-quality proposals while simple alignment in the backbone network is not effective enough for RPN adaptation. We delve into the consistency and the difference of RPN and RPC, treat them individually and leverage high-confidence output of one as mutual guidance to train the other. Moreover, the samples with low-confidence are used for discrepancy calculation between RPN and RPC and minimax optimization. Extensive experimental results on various scenarios have demonstrated the effectiveness of our proposed method in both domain-adaptive region proposal generation and object detection. Code is available at https://github.com/GanlongZhao/CST_DA_detection.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_6');
INSERT INTO `paper` VALUES (11665, 'Collaborative Video Object Segmentation by Foreground-Background Integration', 'Video Object Segmentation', 'Metric learning', '', '', '', 'This paper investigates the principles of embedding learning to tackle the challenging semi-supervised video object segmentation. Different from previous practices that only explore the embedding learning using pixels from foreground object (s), we consider background should be equally treated and thus propose Collaborative video object segmentation by Foreground-Background Integration (CFBI) approach. Our CFBI implicitly imposes the feature embedding from the target foreground object and its corresponding background to be contrastive, promoting the segmentation results accordingly. With the feature embedding from both foreground and background, our CFBI performs the matching process between the reference and the predicted sequence from both pixel and instance levels, making the CFBI be robust to various object scales. We conduct extensive experiments on three popular benchmarks, i.e., DAVIS 2016, DAVIS 2017, and YouTube-VOS. Our CFBI achieves the performance (\\(\\mathcal {J}\\)&\\(\\mathcal {F}\\)) of 89.4%, 81.9%, and 81.4%, respectively, outperforming all the other state-of-the-art methods. Code: https://github.com/z-x-yang/CFBI.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_20');
INSERT INTO `paper` VALUES (11666, 'Colorization of Depth Map via Disentanglement', 'Depth colorization', 'Disentanglement', 'Image translation', '', '', 'Vision perception is one of the most important components for a computer or robot to understand the surrounding scene and achieve autonomous applications. However, most of the vision models are based on the RGB sensors, which in general are vulnerable to the insufficient lighting condition. In contrast, the depth camera, another widely-used visual sensor, is capable of perceiving 3D information and being more robust to the lack of illumination, but unable to obtain appearance details of the surrounding environment compared to RGB cameras. To make RGB-based vision models workable for the low-lighting scenario, prior methods focus on learning the colorization on depth maps captured by depth cameras, such that the vision models can still achieve reasonable performance on colorized depth maps. However, the colorization produced in this manner is usually unrealistic and constrained to the specific vision model, thus being hard to generalize for other tasks to use. In this paper, we propose a depth map colorization method via disentangling appearance and structure factors, so that our model could 1) learn depth-invariant appearance features from an appearance reference and 2) generate colorized images by combining a given depth map and the appearance feature obtained from any reference. We conduct extensive experiments to show that our colorization results are more realistic and diverse in comparison to several image translation baselines.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_27');
INSERT INTO `paper` VALUES (11667, 'Combining Implicit Function Learning and Parametric Models for 3D Human Reconstruction', '3D human reconstruction', 'Implicit reconstruction', 'Parametric modelling', '', '', 'Implicit functions represented as deep learning approximations are powerful for reconstructing 3D surfaces. However, they can only produce static surfaces that are not controllable, which provides limited ability to modify the resulting model by editing its pose or shape parameters. Nevertheless, such features are essential in building flexible models for both computer graphics and computer vision. In this work, we present methodology that combines detail-rich implicit functions and parametric representations in order to reconstruct 3D models of people that remain controllable and accurate even in the presence of clothing. Given sparse 3D point clouds sampled on the surface of a dressed person, we use an Implicit Part Network (IP-Net) to jointly predict the outer 3D surface of the dressed person, the inner body surface, and the semantic correspondences to a parametric body model. We subsequently use correspondences to fit the body model to our inner surface and then non-rigidly deform it (under a parametric body + displacement model) to the outer surface in order to capture garment, face and hair detail. In quantitative and qualitative experiments with both full body data and hand scans we show that the proposed methodology generalizes, and is effective even given incomplete point clouds collected from single-view depth images. Our models and code will be publicly released (http://virtualhumans.mpi-inf.mpg.de/ipnet).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_19');
INSERT INTO `paper` VALUES (11668, 'Combining Task Predictors via Enhancing Joint Predictability', '', '', '', '', '', 'Predictor combination aims to improve a (target) predictor of a learning task based on the (reference) predictors of potentially relevant tasks, without having access to the internals of individual predictors. We present a new predictor combination algorithm that improves the target by i) measuring the relevance of references based on their capabilities in predicting the target, and ii) strengthening such estimated relevance. Unlike existing predictor combination approaches that only exploit pairwise relationships between the target and each reference, and thereby ignore potentially useful dependence among references, our algorithm jointly assesses the relevance of all references by adopting a Bayesian framework. This also offers a rigorous way to automatically select only relevant references. Based on experiments on seven real-world datasets from visual attribute ranking and multi-class classification scenarios, we demonstrate that our algorithm offers a significant performance gain and broadens the application range of existing predictor combination approaches.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_26');
INSERT INTO `paper` VALUES (11669, 'Commands 4 Autonomous Vehicles (C4AV) Workshop Summary', '', '', '', '', '', 'The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the Commands for Autonomous Vehicles (C4AV) challenge based on the recent Talk2Car dataset. This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_1');
INSERT INTO `paper` VALUES (11670, 'Commands for Autonomous Vehicles by Progressively Stacking Visual-Linguistic Representations', '', '', '', '', '', 'In this work, we focus on the object referral problem in the autonomous driving setting. We use a stacked visual-linguistic BERT model to learn a generic visual-linguistic representation. Each element of the input is either a word or a region of interest from the input image. To train the deep model efficiently, we use a stacking algorithm to transfer knowledge from a shallow BERT model to a deep BERT model.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_2');
INSERT INTO `paper` VALUES (11671, 'Commonality-Parsing Network Across Shape and Appearance for Partially Supervised Instance Segmentation', 'Partially supervised', 'Few-shot', 'Instance segmentation', '', '', 'Partially supervised instance segmentation aims to perform learning on limited mask-annotated categories of data thus eliminating expensive and exhaustive mask annotation. The learned models are expected to be generalizable to novel categories. Existing methods either learn a transfer function from detection to segmentation, or cluster shape priors for segmenting novel categories. We propose to learn the underlying class-agnostic commonalities that can be generalized from mask-annotated categories to novel categories. Specifically, we parse two types of commonalities: 1) shape commonalities which are learned by performing supervised learning on instance boundary prediction; and 2) appearance commonalities which are captured by modeling pairwise affinities among pixels of feature maps to optimize the separability between instance and the background. Incorporating both the shape and appearance commonalities, our model significantly outperforms the state-of-the-art methods on both partially supervised setting and few-shot setting for instance segmentation on COCO dataset. The code is available at https://github.com/fanq15/FewX.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_23');
INSERT INTO `paper` VALUES (11672, 'Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets', '', '', '', '', '', 'A wide range of image captioning models has been developed, achieving significant improvement based on popular metrics, such as BLEU, CIDEr, and SPICE. However, although the generated captions can accurately describe the image, they are generic for similar images and lack distinctiveness, i.e., cannot properly describe the uniqueness of each image. In this paper, we aim to improve the distinctiveness of image captions through training with sets of similar images. First, we propose a distinctiveness metric—between-set CIDEr (CIDErBtw) to evaluate the distinctiveness of a caption with respect to those of similar images. Our metric shows that the human annotations of each image are not equivalent based on distinctiveness. Thus we propose several new training strategies to encourage the distinctiveness of the generated caption for each image, which are based on using CIDErBtw in a weighted loss function or as a reinforcement learning reward. Finally, extensive experiments are conducted, showing that our proposed approach significantly improves both distinctiveness (as measured by CIDErBtw and retrieval metrics) and accuracy (e.g., as measured by CIDEr) for a wide variety of image captioning baselines. These results are further confirmed through a user study. Project page: https://wenjiaxu.github.io/ciderbtw/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_22');
INSERT INTO `paper` VALUES (11673, 'Component Divide-and-Conquer for Real-World Image Super-Resolution', 'Real-world image super-resolution', 'Image degradation', 'Corner point', 'Component divide-and-conquer', 'Gradient-weighted loss', 'In this paper, we present a large-scale Diverse Real-world image Super-Resolution dataset, i.e., DRealSR, as well as a divide-and-conquer Super-Resolution (SR) network, exploring the utility of guiding SR model with low-level image components. DRealSR establishes a new SR benchmark with diverse real-world degradation processes, mitigating the limitations of conventional simulated image degradation. In general, the targets of SR vary with image regions with different low-level image components, e.g., smoothness preserving for flat regions, sharpening for edges, and detail enhancing for textures. Learning an SR model with conventional pixel-wise loss usually is easily dominated by flat regions and edges, and fails to infer realistic details of complex textures. We propose a Component Divide-and-Conquer (CDC) model and a Gradient-Weighted (GW) loss for SR. Our CDC parses an image with three components, employs three Component-Attentive Blocks (CABs) to learn attentive masks and intermediate SR predictions with an intermediate supervision learning strategy, and trains an SR model following a divide-and-conquer learning principle. Our GW loss also provides a feasible way to balance the difficulties of image components for SR. Extensive experiments validate the superior performance of our CDC and the challenging aspects of our DRealSR dataset related to diverse real-world scenarios. Our dataset and codes are publicly available at https://github.com/xiezw5/Component-Divide-and-Conquer-for-Real-World-Image-Super-Resolution.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_7');
INSERT INTO `paper` VALUES (11674, 'Comprehensive Image Captioning via Scene Graph Decomposition', 'Image captioning', 'Scene graph', 'Graph neural networks', '', '', 'We address the challenging problem of image captioning by revisiting the representation of image scene graph. At the core of our method lies the decomposition of a scene graph into a set of sub-graphs, with each sub-graph capturing a semantic component of the input image. We design a deep model to select important sub-graphs, and to decode each selected sub-graph into a single target sentence. By using sub-graphs, our model is able to attend to different components of the image. Our method thus accounts for accurate, diverse, grounded and controllable captioning at the same time. We present extensive experiments to demonstrate the benefits of our comprehensive captioning model. Our method establishes new state-of-the-art results in caption diversity, grounding, and controllability, and compares favourably to latest methods in caption quality. Our project website can be found at http://pages.cs.wisc.edu/~yiwuzhong/Sub-GC.html.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_13');
INSERT INTO `paper` VALUES (11675, 'Conditional Adversarial Camera Model Anonymization', 'Camera model anonymization', 'Conditional generative adversarial nets', 'Adversarial training', 'Non-interactive black-box attacks', 'Image editing/manipulation', 'The model of camera that was used to capture a particular photographic image (model attribution) is typically inferred from high-frequency model-specific artifacts present within the image. Model anonymization is the process of transforming these artifacts such that the apparent capture model is changed. We propose a conditional adversarial approach for learning such transformations. In contrast to previous works, we cast model anonymization as the process of transforming both high and low spatial frequency information. We augment the objective with the loss from a pre-trained dual-stream model attribution classifier, which constrains the generative network to transform the full range of artifacts. Quantitative comparisons demonstrate the efficacy of our framework in a restrictive non-interactive black-box setting.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_13');
INSERT INTO `paper` VALUES (11676, 'Conditional Convolutions for Instance Segmentation', 'Conditional convolutions', 'Instance segmentation', '', '', '', 'We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: (1) Instance segmentation is solved by a fully convolutional network, eliminating the need for ROI cropping and feature alignment. (2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask R-CNN baselines, without longer training schedules needed. Code is available: https://git.io/AdelaiDet.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_17');
INSERT INTO `paper` VALUES (11677, 'Conditional Entropy Coding for Efficient Video Compression', '', '', '', '', '', 'We propose a very simple and efficient video compression framework that only focuses on modeling the conditional entropy between frames. Unlike prior learning-based approaches, we reduce complexity by not performing any form of explicit transformations between frames and assume each frame is encoded with an independent state-of-the-art deep image compressor. We first show that a simple architecture modeling the entropy between the image latent codes is as competitive as other neural video compression works and video codecs while being much faster and easier to implement. We then propose a novel internal learning extension on top of this architecture that brings an additional \\(\\sim \\)10% bitrate savings without trading off decoding speed. Importantly, we show that our approach outperforms H.265 and other deep learning baselines in MS-SSIM on higher bitrate UVG video, and against all video codecs on lower framerates, while being thousands of times faster in decoding than deep models utilizing an autoregressive entropy model.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_27');
INSERT INTO `paper` VALUES (11678, 'Conditional Image Repainting via Semantic Bridge and Piecewise Value Function', 'Image generation', 'Semantic', 'Compositing', 'Adversarial', '', 'We study conditional image repainting where a model is trained to generate visual content conditioned on user inputs, and composite the generated content seamlessly onto a user provided image while preserving the semantics of users’ inputs. The content generation community has been pursuing to lower the skill barriers. The usage of human language is the rose among thorns for this purpose, because the language is friendly to users but poses great difficulties for the model in associating relevant words with the semantically ambiguous regions. To resolve this issue, we propose a delicate mechanism which bridges the semantic chasm between the language input and the generated visual content. The state-of-the-art image compositing techniques pose a latent ceiling of fidelity for the composited content during the adversarial training process. In this work, we improve the compositing by breaking through the latent ceiling using a novel piecewise value function. We demonstrate on two datasets that the proposed techniques can better assist tackling conditional image repainting compared to the existing ones.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_27');
INSERT INTO `paper` VALUES (11679, 'Conditional Sequential Modulation for Efficient Global Image Retouching', '', '', '', '', '', 'Photo retouching aims at enhancing the aesthetic visual quality of images that suffer from photographic defects such as over/under exposure, poor contrast, inharmonious saturation. Practically, photo retouching can be accomplished by a series of image processing operations. In this paper, we investigate some commonly-used retouching operations and mathematically find that these pixel-independent operations can be approximated or formulated by multi-layer perceptrons (MLPs). Based on this analysis, we propose an extremely light-weight framework - Conditional Sequential Retouching Network (CSRNet) - for efficient global image retouching. CSRNet consists of a base network and a condition network. The base network acts like an MLP that processes each pixel independently and the condition network extracts the global features of the input image to generate a condition vector. To realize retouching operations, we modulate the intermediate features using Global Feature Modulation (GFM), of which the parameters are transformed by condition vector. Benefiting from the utilization of \\(1\\times 1\\) convolution, CSRNet only contains less than 37 k trainable parameters, which is orders of magnitude smaller than existing learning-based methods. Extensive experiments show that our method achieves state-of-the-art performance on the benchmark MIT-Adobe FiveK dataset quantitively and qualitatively. Code is available at https://github.com/hejingwenhejingwen/CSRNet.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_40');
INSERT INTO `paper` VALUES (11680, 'CONFIG: Controllable Neural Face Image Generation', 'Neural rendering', 'Face image manipulation', 'GAN', '', '', 'Our ability to sample realistic natural images, particularly faces, has advanced by leaps and bounds in recent years, yet our ability to exert fine-tuned control over the generative process has lagged behind. If this new technology is to find practical uses, we need to achieve a level of control over generative networks which, without sacrificing realism, is on par with that seen in computer graphics and character animation. To this end we propose ConfigNet, a neural face model that allows for controlling individual aspects of output images in semantically meaningful ways and that is a significant step on the path towards finely-controllable neural rendering. ConfigNet is trained on real face images as well as synthetic face renders. Our novel method uses synthetic data to factorize the latent space into elements that correspond to the inputs of a traditional rendering pipeline, separating aspects such as head pose, facial expression, hair style, illumination, and many others which are very hard to annotate in real data. The real images, which are presented to the network without labels, extend the variety of the generated images and encourage realism. Finally, we propose an evaluation criterion using an attribute detection network combined with a user study and demonstrate state-of-the-art individual control over attributes in the output images.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_18');
INSERT INTO `paper` VALUES (11681, 'Connecting the Dots: Detecting Adversarial Perturbations Using Context Inconsistency', 'Object detection', 'Adversarial perturbation', 'Context', '', '', 'There has been a recent surge in research on adversarial perturbations that defeat Deep Neural Networks (DNNs) in machine vision; most of these perturbation-based attacks target object classifiers. Inspired by the observation that humans are able to recognize objects that appear out of place in a scene or along with other unlikely objects, we augment the DNN with a system that learns context consistency rules during training and checks for the violations of the same during testing. Our approach builds a set of auto-encoders, one for each object class, appropriately trained so as to output a discrepancy between the input and output if an added adversarial perturbation violates context consistency rules. Experiments on PASCAL VOC and MS COCO show that our method effectively detects various adversarial attacks and achieves high ROC-AUC (over 0.95 in most cases); this corresponds to over 20% improvement over a state-of-the-art context-agnostic method.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_24');
INSERT INTO `paper` VALUES (11682, 'Connecting Vision and Language with Localized Narratives', '', '', '', '', '', 'We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_38');
INSERT INTO `paper` VALUES (11683, 'Consensus-Aware Visual-Semantic Embedding for Image-Text Matching', 'Image-text matching', 'Visual-semantic embedding', 'Consensus', '', '', 'Image-text matching plays a central role in bridging vision and language. Most existing approaches only rely on the image-text instance pair to learn their representations, thereby exploiting their matching relationships and making the corresponding alignments. Such approaches only exploit the superficial associations contained in the instance pairwise data, with no consideration of any external commonsense knowledge, which may hinder their capabilities to reason the higher-level relationships between image and text. In this paper, we propose a Consensus-aware Visual-Semantic Embedding (CVSE) model to incorporate the consensus information, namely the commonsense knowledge shared between both modalities, into image-text matching. Specifically, the consensus information is exploited by computing the statistical co-occurrence correlations between the semantic concepts from the image captioning corpus and deploying the constructed concept correlation graph to yield the consensus-aware concept (CAC) representations. Afterwards, CVSE learns the associations and alignments between image and text based on the exploited consensus as well as the instance-level representations for both modalities. Extensive experiments conducted on two public datasets verify that the exploited consensus makes significant contributions to constructing more meaningful visual-semantic embeddings, with the superior performances over the state-of-the-art approaches on the bidirectional image and text retrieval task. Our code of this paper is available at: https://github.com/BruceW91/CVSE.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_2');
INSERT INTO `paper` VALUES (11684, 'Consistency Guided Scene Flow Estimation', 'Scene flow', 'Disparity estimation', 'Stereo video', 'Geometric constraints', 'Self-supervised learning', 'Consistency Guided Scene Flow Estimation (CGSF) is a self-supervised framework for the joint reconstruction of 3D scene structure and motion from stereo video. The model takes two temporal stereo pairs as input, and predicts disparity and scene flow. The model self-adapts at test time by iteratively refining its predictions. The refinement process is guided by a consistency loss, which combines stereo and temporal photo-consistency with a geometric term that couples disparity and 3D motion. To handle inherent modeling error in the consistency loss (e.g. Lambertian assumptions) and for better generalization, we further introduce a learned, output refinement network, which takes the initial predictions, the loss, and the gradient as input, and efficiently predicts a correlated output update. In multiple experiments, including ablation studies, we show that the proposed model can reliably predict disparity and scene flow in challenging imagery, achieves better generalization than the state-of-the-art, and adapts quickly and robustly to unseen domains.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_8');
INSERT INTO `paper` VALUES (11685, 'Consistency-Based Semi-supervised Active Learning: Towards Minimizing Labeling Cost', 'Active learning', 'Semi-supervised learning', 'Consistency-based sample selection', '', '', 'Active learning (AL) combines data labeling and model training to minimize the labeling cost by prioritizing the selection of high value data that can best improve model performance. In pool-based active learning, accessible unlabeled data are not used for model training in most conventional methods. Here, we propose to unify unlabeled sample selection and model training towards minimizing labeling cost, and make two contributions towards that end. First, we exploit both labeled and unlabeled data using semi-supervised learning (SSL) to distill information from unlabeled data during the training stage. Second, we propose a consistency-based sample selection metric that is coherent with the training objective such that the selected samples are effective at improving model performance. We conduct extensive experiments on image classification tasks. The experimental results on CIFAR-10, CIFAR-100 and ImageNet demonstrate the superior performance of our proposed method with limited labeled data, compared to the existing methods and the alternative AL and SSL combinations. Additionally, we also study an important yet under-explored problem – “When can we start learning-based AL selection?”. We propose a measure that is empirically correlated with the AL target loss and is potentially useful for determining the proper starting point of learning-based AL methods .', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_30');
INSERT INTO `paper` VALUES (11686, 'Contact and Human Dynamics from Monocular Video', '', '', '', '', '', 'Existing deep models predict 2D and 3D kinematic poses from video that are approximately accurate, but contain visible errors that violate physical constraints, such as feet penetrating the ground and bodies leaning at extreme angles. In this paper, we present a physics-based method for inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input. We first estimate ground contact timings with a novel prediction network which is trained without hand-labeled data. A physics-based trajectory optimization then solves for a physically-plausible motion, based on the inputs. We show this process produces motions that are significantly more realistic than those from purely kinematic methods, substantially improving quantitative measures of both kinematic and dynamic plausibility. We demonstrate our method on character animation and pose estimation tasks on dynamic motions of dancing and sports with complex contact patterns.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_5');
INSERT INTO `paper` VALUES (11687, 'ContactPose: A Dataset of Grasps with Object Contact and Hand Pose', 'Contact modeling', 'Hand-object contact', 'Functional grasping', '', '', 'Grasping is natural for humans. However, it involves complex hand configurations and soft tissue deformation that can result in complicated regions of contact between the hand and the object. Understanding and modeling this contact can potentially improve hand models, AR/VR experiences, and robotic grasping. Yet, we currently lack datasets of hand-object contact paired with other data modalities, which is crucial for developing and evaluating contact modeling techniques. We introduce ContactPose, the first dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. Analysis of ContactPose data reveals interesting relationships between hand pose and contact. We use this data to rigorously evaluate various data representations, heuristics from the literature, and learning methods for contact modeling. Data, code, and trained models are available at https://contactpose.cc.gatech.edu.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_22');
INSERT INTO `paper` VALUES (11688, 'Content Adaptive and Error Propagation Aware Deep Video Compression', '', '', '', '', '', 'Recently, learning based video compression methods attract increasing attention. However, the previous works suffer from error propagation due to the accumulation of reconstructed error in inter predictive coding. Meanwhile, the previous learning based video codecs are also not adaptive to different video contents. To address these two problems, we propose a content adaptive and error propagation aware video compression system. Specifically, our method employs a joint training strategy by considering the compression performance of multiple consecutive frames instead of a single frame. Based on the learned long-term temporal information, our approach effectively alleviates error propagation in reconstructed frames. More importantly, instead of using the hand-crafted coding modes in the traditional compression systems, we design an online encoder updating scheme in our system. The proposed approach updates the parameters for encoder according to the rate-distortion criterion but keeps the decoder unchanged in the inference stage. Therefore, the encoder is adaptive to different video contents and achieves better compression performance by reducing the domain gap between the training and testing datasets. Our method is simple yet effective and outperforms the state-of-the-art learning based video codecs on benchmark datasets without increasing the model size or decreasing the decoding speed.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_27');
INSERT INTO `paper` VALUES (11689, 'Content-Aware Unsupervised Deep Homography Estimation', 'Homography', 'Deep homography', 'Image alignment', 'RANSAC', '', 'Homography estimation is a basic image alignment method in many applications. It is usually conducted by extracting and matching sparse feature points, which are error-prone in low-light and low-texture images. On the other hand, previous deep homography approaches use either synthetic images for supervised learning or aerial images for unsupervised learning, both ignoring the importance of handling depth disparities and moving objects in real world applications. To overcome these problems, in this work we propose an unsupervised deep homography method with a new architecture design. In the spirit of the RANSAC procedure in traditional methods, we specifically learn an outlier mask to only select reliable regions for homography estimation. We calculate loss with respect to our learned deep features instead of directly comparing image content as did previously. To achieve the unsupervised training, we also formulate a novel triplet loss customized for our network. We verify our method by conducting comprehensive comparisons on a new dataset that covers a wide range of scenes with varying degrees of difficulties for the task. Experimental results reveal that our method outperforms the state-of-the-art including deep solutions and feature-based solutions.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_38');
INSERT INTO `paper` VALUES (11690, 'Content-Consistent Matching for Domain Adaptive Semantic Segmentation', 'Semantic segmentation', 'Domain adaptation', '', '', '', 'This paper considers the adaptation of semantic segmentation from the synthetic source domain to the real target domain. Different from most previous explorations that often aim at developing adversarial-based domain alignment solutions, we tackle this challenging task from a new perspective, i.e., content-consistent matching (CCM). The target of CCM is to acquire those synthetic images that share similar distribution with the real ones in the target domain, so that the domain gap can be naturally alleviated by employing the content-consistent synthetic images for training. To be specific, we facilitate the CCM from two aspects, i.e., semantic layout matching and pixel-wise similarity matching. First, we use all the synthetic images from the source domain to train an initial segmentation model, which is then employed to produce coarse pixel-level labels for the unlabeled images in the target domain. With the coarse/accurate label maps for real/synthetic images, we construct their semantic layout matrixes from both horizontal and vertical directions and perform the matrixes matching to find out the synthetic images with similar semantic layout to real images. Second, we choose those predicted labels with high confidence to generate feature embeddings for all classes in the target domain, and further perform the pixel-wise matching on the mined layout-consistent synthetic images to harvest the appearance-consistent pixels. With the proposed CCM, only those content-consistent synthetic images are taken into account for learning the segmentation model, which can effectively alleviate the domain bias caused by those content-irrelevant synthetic images. Extensive experiments are conducted on two popular domain adaptation tasks, i.e., GTA5\\(\\xrightarrow {}\\)Cityscapes and SYNTHIA\\(\\xrightarrow {}\\)Cityscapes. Our CCM yields consistent improvements over the baselines and performs favorably against previous state-of-the-arts.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_26');
INSERT INTO `paper` VALUES (11691, 'Context-Aware RCNN: A Baseline for Action Detection in Videos', 'Action detection', 'Context-Aware RCNN', 'Baseline', '', '', 'Video action detection approaches usually conduct actor-centric action recognition over RoI-pooled features following the standard pipeline of Faster-RCNN. In this work, we first empirically find the recognition accuracy is highly correlated with the bounding box size of an actor, and thus higher resolution of actors contributes to better performance. However, video models require dense sampling in time to achieve accurate recognition. To fit in GPU memory, the frames to backbone network must be kept low-resolution, resulting in a coarse feature map in RoI-Pooling layer. Thus, we revisit RCNN for actor-centric action recognition via cropping and resizing image patches around actors before feature extraction with I3D deep network. Moreover, we found that expanding actor bounding boxes slightly and fusing the context features can further boost the performance. Consequently, we develop a surprisingly effective baseline (Context-Aware RCNN) and it achieves new state-of-the-art results on two challenging action detection benchmarks of AVA and JHMDB. Our observations challenge the conventional wisdom of RoI-Pooling based pipeline and encourage researchers rethink the importance of resolution in actor-centric action recognition. Our approach can serve as a strong baseline for video action detection and is expected to inspire new ideas for this filed. The code is available at https://github.com/MCG-NJU/CRCNN-Action.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_27');
INSERT INTO `paper` VALUES (11692, 'Context-Gated Convolution', 'Convolutional neural network', 'Context-gated convolution', 'Global context information', '', '', 'As the basic building block of Convolutional Neural Networks (CNNs), the convolutional layer is designed to extract local patterns and lacks the ability to model global context in its nature. Many efforts have been recently devoted to complementing CNNs with the global modeling ability, especially by a family of works on global feature interaction. In these works, the global context information is incorporated into local features before they are fed into convolutional layers. However, research on neuroscience reveals that the neurons’ ability of modifying their functions dynamically according to context is essential for the perceptual tasks, which has been overlooked in most of CNNs. Motivated by this, we propose one novel Context-Gated Convolution (CGC) to explicitly modify the weights of convolutional layers adaptively under the guidance of global context. As such, being aware of the global context, the modulated convolution kernel of our proposed CGC can better extract representative local patterns and compose discriminative features. Moreover, our proposed CGC is lightweight and applicable with modern CNN architectures, and consistently improves the performance of CNNs according to extensive experiments on image classification, action recognition, and machine translation. Our code of this paper is available at https://github.com/XudongLinthu/context-gated-convolution.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_41');
INSERT INTO `paper` VALUES (11693, 'Contextual Diversity for Active Learning', '', '', '', '', '', 'Requirement of large annotated datasets restrict the use of deep convolutional neural networks (CNNs) for many practical applications. The problem can be mitigated by using active learning (AL) techniques which, under a given annotation budget, allow to select a subset of data that yields maximum accuracy upon fine tuning. State of the art AL approaches typically rely on measures of visual diversity or prediction uncertainty, which are unable to effectively capture the variations in spatial context. On the other hand, modern CNN architectures make heavy use of spatial context for achieving highly accurate predictions. Since the context is difficult to evaluate in the absence of ground-truth labels, we introduce the notion of contextual diversity that captures the confusion associated with spatially co-occurring classes. Contextual Diversity (CD) hinges on a crucial observation that the probability vector predicted by a CNN for a region of interest typically contains information from a larger receptive field. Exploiting this observation, we use the proposed CD measure within two AL frameworks: (1) a core-set based strategy and (2) a reinforcement learning based policy, for active frame selection. Our extensive empirical evaluation establish state of the art results for active learning on benchmark datasets of Semantic Segmentation, Object Detection and Image classification. Our ablation studies show clear advantages of using contextual diversity for active learning. The source code and additional results are available at https://github.com/sharat29ag/CDAL.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_9');
INSERT INTO `paper` VALUES (11694, 'Contextual Heterogeneous Graph Network for Human-Object Interaction Detection', 'Human-object interaction', 'Heterogeneous graph', 'Neural network', '', '', 'Human-object interaction (HOI) detection is an important task for understanding human activity. Graph structure is appropriate to denote the HOIs in the scene. Since there is an subordination between human and object—human play subjective role and object play objective role in HOI, the relations between homogeneous entities and heterogeneous entities in the scene should also not be equally the same. However, previous graph models regard human and object as the same kind of nodes and do not consider that the messages are not equally the same between different entities. In this work, we address such a problem for HOI task by proposing a heterogeneous graph network that models humans and objects as different kinds of nodes and incorporates intra-class messages between homogeneous nodes and inter-class messages between heterogeneous nodes. In addition, a graph attention mechanism based on the intra-class context and inter-class context is exploited to improve the learning. Extensive experiments on the benchmark datasets V-COCO and HICO-DET verify the effectiveness of our method and demonstrate the importance to extract intra-class and inter-class messages which are not equally the same in HOI detection.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_15');
INSERT INTO `paper` VALUES (11695, 'Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation', 'Semantic segmentation', 'Unsupervised domain adaptation', 'Contextual-relation consistent', '', '', 'Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 \\(\\rightarrow \\) Cityscapes and SYNTHIA \\(\\rightarrow \\) Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_42');
INSERT INTO `paper` VALUES (11696, 'Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections', '', '', '', '', '', 'In interactive object segmentation a user collaborates with a computer vision model to segment an object. Recent works employ convolutional neural networks for this task: Given an image and a set of corrections made by the user as input, they output a segmentation mask. These approaches achieve strong performance by training on large datasets but they keep the model parameters unchanged at test time. Instead, we recognize that user corrections can serve as sparse training examples and we propose a method that capitalizes on that idea to update the model parameters on-the-fly to the data at hand. Our approach enables the adaptation to a particular object and its background, to distributions shifts in a test set, to specific object classes, and even to large domain changes, where the imaging modality changes between training and testing. We perform extensive experiments on 8 diverse datasets and show: Compared to a model with frozen parameters, our method reduces the required corrections (i) by 9%–30% when distribution shifts are small between training and testing; (ii) by 12%–44% when specializing to a specific class; (iii) and by 60% and 77% when we completely change domain between training and testing.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_34');
INSERT INTO `paper` VALUES (11697, 'Contrastive Learning for Unpaired Image-to-Image Translation', 'Contrastive learning', 'Noise contrastive estimation', 'Mutual information', 'Image generation', '', 'In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so – maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each “domain” is only a single image.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_19');
INSERT INTO `paper` VALUES (11698, 'Contrastive Learning for Weakly Supervised Phrase Grounding', 'Mutual information', 'InfoNCE', 'Grounding', 'Attention', '', 'Phrase grounding, the problem of associating image regions to caption words, is a crucial component of vision-language tasks. We show that phrase grounding can be learned by optimizing word-region attention to maximize a lower bound on mutual information between images and caption words. Given pairs of images and captions, we maximize compatibility of the attention-weighted regions and the words in the corresponding caption, compared to non-corresponding pairs of images and captions. A key idea is to construct effective negative captions for learning through language model guided word substitutions. Training with our negatives yields a \\(\\sim 10\\%\\) absolute gain in accuracy over randomly-sampled negatives from the training data. Our weakly supervised phrase grounding model trained on COCO-Captions shows a healthy gain of \\(5.7\\%\\) to achieve \\(76.7\\%\\) accuracy on Flickr30K Entities benchmark. Our code and project material will be available at http://tanmaygupta.info/info-ground.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_44');
INSERT INTO `paper` VALUES (11699, 'Contrastive Multiview Coding', '', '', '', '', '', 'Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a “dog” can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_45');
INSERT INTO `paper` VALUES (11700, 'Controllable Image Synthesis via SegVAE', '', '', '', '', '', 'Flexible user controls are desirable for content creation and image editing. A semantic map is commonly used intermediate representation for conditional image generation. Compared to the operation on raw RGB pixels, the semantic map enables simpler user modification. In this work, we specifically target at generating semantic maps given a label-set consisting of desired categories. The proposed framework, SegVAE, synthesizes semantic maps in an iterative manner using conditional variational autoencoder. Quantitative and qualitative experiments demonstrate that the proposed model can generate realistic and diverse semantic maps. We also apply an off-the-shelf image-to-image translation model to generate realistic RGB images to better understand the quality of the synthesized semantic maps. Finally, we showcase several real-world image-editing applications including object removal, insertion, and replacement.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_10');
INSERT INTO `paper` VALUES (11701, 'Controlling Style and Semantics in Weakly-Supervised Image Generation', '', '', '', '', '', 'We propose a weakly-supervised approach for conditional image generation of complex scenes where a user has fine control over objects appearing in the scene. We exploit sparse semantic maps to control object shapes and classes, as well as textual descriptions or attributes to control both local and global style. In order to condition our model on textual descriptions, we introduce a semantic attention module whose computational cost is independent of the image resolution. To further augment the controllability of the scene, we propose a two-step generation scheme that decomposes background and foreground. The label maps used to train our model are produced by a large-vocabulary object detector, which enables access to unlabeled data and provides structured instance information. In such a setting, we report better FID scores compared to fully-supervised settings where the model is trained on ground-truth semantic maps. We also showcase the ability of our model to manipulate a scene on complex datasets such as COCO and Visual Genome.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_29');
INSERT INTO `paper` VALUES (11702, 'Convolutional Occupancy Networks', '', '', '', '', '', 'Recently, implicit neural representations have gained popularity for learning-based 3D reconstruction. While demonstrating promising results, most implicit approaches are limited to comparably simple geometry of single objects and do not scale to more complicated or large-scale scenes. The key limiting factor of implicit methods is their simple fully-connected network architecture which does not allow for integrating local information in the observations or incorporating inductive biases such as translational equivariance. In this paper, we propose Convolutional Occupancy Networks, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes. By combining convolutional encoders with implicit occupancy decoders, our model incorporates inductive biases, enabling structured reasoning in 3D space. We investigate the effectiveness of the proposed representation by reconstructing complex geometry from noisy point clouds and low-resolution voxel representations. We empirically find that our method enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_31');
INSERT INTO `paper` VALUES (11703, 'CooGAN: A Memory-Efficient Framework for High-Resolution Facial Attribute Editing', 'Generative adversarial networks', 'Conditional GANs', 'Face attributes editing', '', '', 'In contrast to great success of memory-consuming face editing methods at a low resolution, to manipulate high-resolution (HR) facial images, i.e., typically larger than \\(768^2\\) pixels, with very limited memory is still challenging. This is due to the reasons of 1) intractable huge demand of memory; 2) inefficient multi-scale features fusion. To address these issues, we propose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for HR facial image editing. This framework features a local path for fine-grained local facial patch generation (i.e., patch-level HR, LOW memory) and a global path for global low-resolution (LR) facial structure monitoring (i.e., image-level LR, LOW memory), which largely reduce memory requirements. Both paths work in a cooperative manner under a local-to-global consistency objective (i.e., for smooth stitching). In addition, we propose a lighter selective transfer unit for more efficient multi-scale features fusion, yielding higher fidelity facial attributes manipulation. Extensive experiments on CelebA-HQ well demonstrate the memory efficiency as well as the high image generation quality of the proposed framework.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_39');
INSERT INTO `paper` VALUES (11704, 'CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image', '', '', '', '', '', 'Advances in deep learning techniques have allowed recent work to reconstruct the shape of a single object given only one RBG image as input. Building on common encoder-decoder architectures for this task, we propose three extensions: (1) ray-traced skip connections that propagate local 2D information to the output 3D volume in a physically correct manner; (2) a hybrid 3D volume representation that enables building translation equivariant models, while at the same time encoding fine object details without an excessive memory footprint; (3) a reconstruction loss tailored to capture overall object geometry. Furthermore, we adapt our model to address the harder task of reconstructing multiple objects from a single image. We reconstruct all objects jointly in one pass, producing a coherent reconstruction, where all objects live in a single consistent 3D coordinate frame relative to the camera and they do not intersect in 3D space. We also handle occlusions and resolve them by hallucinating the missing object parts in the 3D volume. We validate the impact of our contributions experimentally both on synthetic data from ShapeNet as well as real images from Pix3D. Our method improves over the state-of-the-art single-object methods on both datasets. Finally, we evaluate performance quantitatively on multiple object reconstruction with synthetic scenes assembled from ShapeNet objects.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_22');
INSERT INTO `paper` VALUES (11705, 'Corner Proposal Network for Anchor-Free, Two-Stage Object Detection', 'Object detection', 'Anchor-free detector', 'Two-stage detector', 'Corner keypoints', 'Object proposals', 'The goal of object detection is to determine the class and location of objects in an image. This paper proposes a novel anchor-free, two-stage framework which first extracts a number of object proposals by finding potential corner keypoint combinations and then assigns a class label to each proposal by a standalone classification stage. We demonstrate that these two stages are effective solutions for improving recall and precision, respectively, and they can be integrated into an end-to-end network. Our approach, dubbed Corner Proposal Network (CPN), enjoys the ability to detect objects of various scales and also avoids being confused by a large number of false-positive proposals. On the MS-COCO dataset, CPN achieves an AP of \\(49.2\\%\\) which is competitive among state-of-the-art object detection methods. CPN also fits the scenario of computational efficiency, which achieves an AP of \\(41.6\\%\\)/\\(39.7\\%\\) at 26.2/43.3 FPS, surpassing most competitors with the same inference speed. Code is available at https://github.com/Duankaiwen/CPNDet.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_24');
INSERT INTO `paper` VALUES (11706, 'CorNet: Unsupervised Deep Homography Estimation for Agricultural Aerial Imagery', 'Unsupervised convolutional neural network', 'Homography estimation', 'UAV', 'Freely flown', 'Maize', 'Efficient and accurate estimation of homographies among images is the first step in mosaicking crop fields for phenotyping. The current strategy uses sophisticated vehicles that have excellent telemetry to hover over a grid of waypoints, imaging each one. This approach simplifies homography estimation, but precludes more flexible, adaptive protocols that can collect richer information. It also makes aerial phenotyping impractical for many researchers and farmers. We are developing an alternative strategy that uses consumer-grade vehicles, freely flown over a variety of trajectories, to collect video. We have developed an unsupervised deep learning network that estimates the sequence of planar homography matrices of our corn fields from imagery, without using any metadata to correct estimation errors. The vehicle was freely flown using a variety of trajectories and camera views. Our system, CorNet, performed faster than and with comparable accuracy to the gold standard ASIFT algorithm in many challenging cases.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_28');
INSERT INTO `paper` VALUES (11707, 'Cosine Meets Softmax: A Tough-to-beat Baseline for Visual Grounding', 'Autonomous driving', 'Visual grounding', 'Deep learning', '', '', 'In this paper, we present a simple baseline for visual grounding for autonomous driving which outperforms the state of the art methods, while retaining minimal design choices. Our framework minimizes the cross-entropy loss over the cosine distance between multiple image ROI features with a text embedding (representing the given sentence/phrase). We use pre-trained networks for obtaining the initial embeddings and learn a transformation layer on top of the text embedding. We perform experiments on the Talk2Car dataset and achieve 68.7% AP50 accuracy, improving upon the previous state of the art by 8.6%. Our investigation suggests reconsideration towards more approaches employing sophisticated attention mechanisms or multi-stage reasoning or complex metric learning loss functions by showing promise in simpler alternatives.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_4');
INSERT INTO `paper` VALUES (11708, 'CosyPose: Consistent Multi-view Multi-object 6D Pose Estimation', '', '', '', '', '', 'We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. First, we present a single-view single-object 6D pose estimation method, which we use to generate 6D object pose hypotheses. Second, we develop a robust method for matching individual 6D object pose hypotheses across different input images in order to jointly estimate camera viewpoints and 6D poses of all objects in a single consistent scene. Our approach explicitly handles object symmetries, does not require depth measurements, is robust to missing or incorrect object hypotheses, and automatically recovers the number of objects in the scene. Third, we develop a method for global scene refinement given multiple object hypotheses and their correspondences across views. This is achieved by solving an object-level bundle adjustment problem that refines the poses of cameras and objects to minimize the reprojection error in all views. We demonstrate that the proposed method, dubbed CosyPose, outperforms current state-of-the-art results for single-view and multi-view 6D object pose estimation by a large margin on two challenging benchmarks: the YCB-Video and T-LESS datasets. Code and pre-trained models are available on the project webpage. (https://www.di.ens.fr/willow/research/cosypose/.)', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_34');
INSERT INTO `paper` VALUES (11709, 'CoTeRe-Net: Discovering Collaborative Ternary Relations in Videos', 'Video understanding', 'Action recognition', 'Relation model', '', '', 'Modeling relations is crucial to understand videos for action and behavior recognition. Current relation models mainly reason about relations of invisibly implicit cues, while important relations of visually explicit cues are rarely considered, and the collaboration between them is usually ignored. In this paper, we propose a novel relation model that discovers relations of both implicit and explicit cues as well as their collaboration in videos. Our model concerns Collaborative Ternary Relations (CoTeRe), where the ternary relation involves channel (C, for implicit), temporal (T, for implicit), and spatial (S, for explicit) relation (R). We devise a flexible and effective CTSR module to collaborate ternary relations for 3D-CNNs, and then construct CoTeRe-Nets for action recognition. Extensive experiments on both ablation study and performance evaluation demonstrate that our CTSR module is significantly effective with approximate \\(3\\%\\) gains and our CoTeRe-Nets outperform state-of-the-art approaches on three popular benchmarks. Boosts analysis and relations visualization also validate that relations of both implicit and explicit cues are discovered with efficacy by our method. Our code is available at https://github.com/zhenglab/cotere-net.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_23');
INSERT INTO `paper` VALUES (11710, 'Count- and Similarity-Aware R-CNN for Pedestrian Detection', 'Pedestrian detection', 'Human instance segmentation', '', '', '', 'Recent pedestrian detection methods generally rely on additional supervision, such as visible bounding-box annotations, to handle heavy occlusions. We propose an approach that leverages pedestrian count and proposal similarity information within a two-stage pedestrian detection framework. Both pedestrian count and proposal similarity are derived from standard full-body annotations commonly used to train pedestrian detectors. We introduce a count-weighted detection loss function that assigns higher weights to the detection errors occurring at highly overlapping pedestrians. The proposed loss function is utilized at both stages of the two-stage detector. We further introduce a count-and-similarity branch within the two-stage detection framework, which predicts pedestrian count as well as proposal similarity. Lastly, we introduce a count and similarity-aware NMS strategy to identify distinct proposals. Our approach requires neither part information nor visible bounding-box annotations. Experiments are performed on the CityPersons and CrowdHuman datasets. Our method sets a new state-of-the-art on both datasets. Further, it achieves an absolute gain of 2.4% over the current state-of-the-art, in terms of log-average miss rate, on the heavily occluded (HO) set of CityPersons test set. Finally, we demonstrate the applicability of our approach for the problem of human instance segmentation. Code and models are available at: https://github.com/Leotju/CaSe.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_6');
INSERT INTO `paper` VALUES (11711, 'Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler', '', '', '', '', '', 'Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model’s ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_5');
INSERT INTO `paper` VALUES (11712, 'Coupling Explicit and Implicit Surface Representations for Generative 3D Modeling', '', '', '', '', '', 'We propose a novel neural architecture for representing 3D surfaces, which harnesses two complementary shape representations: (i) an explicit representation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an implicit-function representation, i.e., a scalar function over the 3D volume, with its levels denoting surfaces. We make these two representations synergistic by introducing novel consistency losses that ensure that the surface created from the atlas aligns with the level-set of the implicit function. Our hybrid architecture outputs results which are superior to the output of the two equivalent single-representation networks, yielding smoother explicit surfaces with more accurate normals, and a more accurate implicit occupancy function. Additionally, our surface reconstruction step can directly leverage the explicit atlas-based representation. This process is computationally efficient, and can be directly used by differentiable rasterizers, enabling training our hybrid representation with image-based losses.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_39');
INSERT INTO `paper` VALUES (11713, 'CPGAN: Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis', 'Text-to-image synthesis', 'Content-Parsing', 'Generative Adversarial Networks', 'Memory structure', 'Cross-modality', 'Typical methods for text-to-image synthesis seek to design effective generative architecture to model the text-to-image mapping directly. It is fairly arduous due to the cross-modality translation. In this paper we circumvent this problem by focusing on parsing the content of both the input text and the synthesized image thoroughly to model the text-to-image consistency in the semantic level. Particularly, we design a memory structure to parse the textual content by exploring semantic correspondence between each word in the vocabulary to its various visual contexts across relevant images during text encoding. Meanwhile, the synthesized image is parsed to learn its semantics in an object-aware manner. Moreover, we customize a conditional discriminator to model the fine-grained correlations between words and image sub-regions to push for the text-image semantic alignment. Extensive experiments on COCO dataset manifest that our model advances the state-of-the-art performance significantly (from 35.69 to 52.73 in Inception Score).', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_29');
INSERT INTO `paper` VALUES (11714, 'Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral Super-Resolution', 'Coupled unmixing', 'Cross-attention', 'Deep learning', 'Hyperspectral super-resolution', 'Multispectral', 'The recent advancement of deep learning techniques has made great progress on hyperspectral image super-resolution (HSI-SR). Yet the development of unsupervised deep networks remains challenging for this task. To this end, we propose a novel coupled unmixing network with a cross-attention mechanism, CUCaNet for short, to enhance the spatial resolution of HSI by means of higher-spatial-resolution multispectral image (MSI). Inspired by coupled spectral unmixing, a two-stream convolutional autoencoder framework is taken as backbone to jointly decompose MS and HS data into a spectrally meaningful basis and corresponding coefficients. CUCaNet is capable of adaptively learning spectral and spatial response functions from HS-MS correspondences by enforcing reasonable consistency assumptions on the networks. Moreover, a cross-attention module is devised to yield more effective spatial-spectral information transfer in networks. Extensive experiments are conducted on three widely-used HS-MS datasets in comparison with state-of-the-art HSI-SR models, demonstrating the superiority of the CUCaNet in the HSI-SR application. Furthermore, the codes and datasets are made available at: https://github.com/danfenghong/ECCV2020_CUCaNet.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_13');
INSERT INTO `paper` VALUES (11715, 'Cross-Domain Cascaded Deep Translation', 'Unpaired image-to-image translation', 'Image generation', '', '', '', 'In recent years we have witnessed tremendous progress in unpaired image-to-image translation, propelled by the emergence of DNNs and adversarial training strategies. However, most existing methods focus on transfer of style and appearance, rather than on shape translation. The latter task is challenging, due to its intricate non-local nature, which calls for additional supervision. We mitigate this by descending the deep layers of a pre-trained network, where the deep features contain more semantics, and applying the translation between these deep features. Our translation is performed in a cascaded, deep-to-shallow, fashion, along the deep feature hierarchy: we first translate between the deepest layers that encode the higher-level semantic content of the image, proceeding to translate the shallower layers, conditioned on the deeper ones. We further demonstrate the effectiveness of using pre-trained deep features in the context of unconditioned image generation. Our code and trained models will be made publicly available.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_40');
INSERT INTO `paper` VALUES (11716, 'Cross-Identity Motion Transfer for Arbitrary Objects Through Pose-Attentive Video Reassembling', 'Motion transfer', 'Generative adversarial network', 'Video to video translation', '', '', 'We propose an attention-based networks for transferring motions between arbitrary objects. Given a source image(s) and a driving video, our networks animate the subject in the source images according to the motion in the driving video. In our attention mechanism, dense similarities between the learned keypoints in the source and the driving images are computed in order to retrieve the appearance information from the source images. Taking a different approach from the well-studied warping based models, our attention-based model has several advantages. By reassembling non-locally searched pieces from the source contents, our approach can produce more realistic outputs. Furthermore, our system can make use of multiple observations of the source appearance (e.g. front and sides of faces) to make the results more accurate. To reduce the training-testing discrepancy of the self-supervised learning, a novel cross-identity training scheme is additionally introduced. With the training scheme, our networks is trained to transfer motions between different subjects, as in the real testing scenario. Experimental results validate that our method produces visually pleasing results in various object domains, showing better performances compared to previous works.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_18');
INSERT INTO `paper` VALUES (11717, 'Cross-Modal Weighting Network for RGB-D Salient Object Detection', 'RGB-D salient object detection', 'Cross-Modal Weighting', 'Depth-to-RGB weighting', 'RGB-to-RGB weighting', '', 'Depth maps contain geometric clues for assisting Salient Object Detection (SOD). In this paper, we propose a novel Cross-Modal Weighting (CMW) strategy to encourage comprehensive interactions between RGB and depth channels for RGB-D SOD. Specifically, three RGB-depth interaction modules, named CMW-L, CMW-M and CMW-H, are developed to deal with respectively low-, middle- and high-level cross-modal information fusion. These modules use Depth-to-RGB Weighing (DW) and RGB-to-RGB Weighting (RW) to allow rich cross-modal and cross-scale interactions among feature layers generated by different network blocks. To effectively train the proposed Cross-Modal Weighting Network (CMWNet), we design a composite loss function that summarizes the errors between intermediate predictions and ground truth over different scales. With all these novel components working together, CMWNet effectively fuses information from RGB and depth channels, and meanwhile explores object localization and details across scales. Thorough evaluations demonstrate CMWNet consistently outperforms 15 state-of-the-art RGB-D SOD methods on seven popular benchmarks.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_39');
INSERT INTO `paper` VALUES (11718, 'Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition', 'Cross-task transfer', 'Aerial scene classification', 'Geotagged sound', 'Multimodal learning', 'Remote sensing', 'Aerial scene recognition is a fundamental task in remote sensing and has recently received increased interest. While the visual information from overhead images with powerful models and efficient algorithms yields considerable performance on scene recognition, it still suffers from the variation of ground objects, lighting conditions etc. Inspired by the multi-channel perception theory in cognition science, in this paper, for improving the performance on the aerial scene recognition, we explore a novel audiovisual aerial scene recognition task using both images and sounds as input. Based on an observation that some specific sound events are more likely to be heard at a given geographic location, we propose to exploit the knowledge from the sound events to improve the performance on the aerial scene recognition. For this purpose, we have constructed a new dataset named AuDio Visual Aerial sceNe reCognition datasEt (ADVANCE). With the help of this dataset, we evaluate three proposed approaches for transferring the sound event knowledge to the aerial scene recognition task in a multimodal learning framework, and show the benefit of exploiting the audio information for the aerial scene recognition. The source code is publicly available for reproducibility purposes. (https://github.com/DTaoo/Multimodal-Aerial-Scene-Recognition)', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_5');
INSERT INTO `paper` VALUES (11719, 'Crowdsampling the Plenoptic Function', '', '', '', '', '', 'Many popular tourist landmarks are captured in a multitude of online, public photos. These photos represent a sparse and unstructured sampling of the plenoptic function for a particular scene. In this paper, we present a new approach to novel view synthesis under time-varying illumination from such data. Our approach builds on the recent multi-plane image (MPI) format for representing local light fields under fixed viewing conditions. We introduce a new DeepMPI representation, motivated by observations on the sparsity structure of the plenoptic function, that allows for real-time synthesis of photorealistic views that are continuous in both space and across changes in lighting. Our method can synthesize the same compelling parallax and view-dependent effects as previous MPI methods, while simultaneously interpolating along changes in reflectance and illumination with time. We show how to learn a model of these effects in an unsupervised way from an unstructured collection of photos without temporal registration, demonstrating significant improvements over recent work in neural rendering. More information can be found at crowdsampling.io.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_11');
INSERT INTO `paper` VALUES (11720, 'CSCL: Critical Semantic-Consistent Learning for Unsupervised Domain Adaptation', 'Unsupervised domain adaptation', 'Semantic segmentation', 'Adversarial learning', 'Reinforcement learning', 'Pseudo label', 'Unsupervised domain adaptation without consuming annotation process for unlabeled target data attracts appealing interests in semantic segmentation. However, 1) existing methods neglect that not all semantic representations across domains are transferable, which cripples domain-wise transfer with untransferable knowledge; 2) they fail to narrow category-wise distribution shift due to category-agnostic feature alignment. To address above challenges, we develop a new Critical Semantic-Consistent Learning (CSCL) model, which mitigates the discrepancy of both domain-wise and category-wise distributions. Specifically, a critical transfer based adversarial framework is designed to highlight transferable domain-wise knowledge while neglecting untransferable knowledge. Transferability-critic guides transferability-quantizer to maximize positive transfer gain under reinforcement learning manner, although negative transfer of untransferable knowledge occurs. Meanwhile, with the help of confidence-guided pseudo labels generator of target samples, a symmetric soft divergence loss is presented to explore inter-class relationships and facilitate category-wise distribution alignment. Experiments on several datasets demonstrate the superiority of our model.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_44');
INSERT INTO `paper` VALUES (11721, 'Curriculum DeepSDF', '', '', '', '', '', 'When learning to sketch, beginners start with simple and flexible shapes, and then gradually strive for more complex and accurate ones in the subsequent training sessions. In this paper, we design a “shape curriculum” for learning continuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF. Inspired by how humans learn, Curriculum DeepSDF organizes the learning task in ascending order of difficulty according to the following two criteria: surface accuracy and sample difficulty. The former considers stringency in supervising with ground truth, while the latter regards the weights of hard training samples near complex geometry and fine structure. More specifically, Curriculum DeepSDF learns to reconstruct coarse shapes at first, and then gradually increases the accuracy and focuses more on complex local details. Experimental results show that a carefully-designed curriculum leads to significantly better shape reconstructions with the same training data, training epochs and network architecture as DeepSDF. We believe that the application of shape curricula can benefit the training process of a wide variety of 3D shape representation learning methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_4');
INSERT INTO `paper` VALUES (11722, 'Curriculum Manager for Source Selection in Multi-source Domain Adaptation', 'Unsupervised domain adaptation', 'Multi-source', 'Curriculum learning', 'Adversarial training', '', 'The performance of Multi-Source Unsupervised Domain Adaptation depends significantly on the effectiveness of transfer from labeled source domain samples. In this paper, we proposed an adversarial agent that learns a dynamic curriculum for source samples, called Curriculum Manager for Source Selection (CMSS). The Curriculum Manager, an independent network module, constantly updates the curriculum during training, and iteratively learns which domains or samples are best suited for aligning to the target. The intuition behind this is to force the Curriculum Manager to constantly re-measure the transferability of latent domains over time to adversarially raise the error rate of the domain discriminator. CMSS does not require any knowledge of the domain labels, yet it outperforms other methods on four well-known benchmarks by significant margins. We also provide interpretable results that shed light on the proposed method.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_36');
INSERT INTO `paper` VALUES (11723, 'CurveLane-NAS: Unifying Lane-Sensitive Architecture Search and Adaptive Point Blending', 'Lane detection', 'Autonomous driving', 'Benchmark dataset', 'Neural architecture search', 'Curve lane', 'We address the curve lane detection problem which poses more realistic challenges than conventional lane detection for better facilitating modern assisted/autonomous driving systems. Current hand-designed lane detection methods are not robust enough to capture the curve lanes especially the remote parts due to the lack of modeling both long-range contextual information and detailed curve trajectory. In this paper, we propose a novel lane-sensitive architecture search framework named CurveLane-NAS to automatically capture both long-ranged coherent and accurate short-range curve information. It consists of three search modules: a) a feature fusion search module to find a better fusion of the local and global context for multi-level hierarchy features; b) an elastic backbone search module to explore an efficient feature extractor with good semantics and latency; c) an adaptive point blending module to search a multi-level post-processing refinement strategy to combine multi-scale head prediction. Furthermore, we also steer forward to release a more challenging benchmark named CurveLanes for addressing the most difficult curve lanes. It consists of 150K images with 680K labels (The new dataset can be downloaded at http://www.noahlab.com.hk/opensource/vega/#curvelanes). Experiments on the new CurveLanes show that the SOTA lane detection methods suffer substantial performance drop while our model can still reach an 80+% F1-score. Extensive experiments on traditional lane benchmarks such as CULane also demonstrate the superiority of our CurveLane-NAS, e.g. achieving a new SOTA 74.8% F1-score on CULane.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_41');
INSERT INTO `paper` VALUES (11724, 'CycAs: Self-supervised Cycle Association for Learning Re-identifiable Descriptions', 'Self-supervised', 'Cycle consistency', 'person re-ID', '', '', 'This paper proposes a self-supervised learning method for the person re-identification (re-ID) problem, where existing unsupervised methods usually rely on pseudo labels, such as those from video tracklets or clustering. A potential drawback of using pseudo labels is that errors may accumulate and it is challenging to estimate the number of pseudo IDs. We introduce a different unsupervised method that allows us to learn pedestrian embeddings from raw videos, without resorting to pseudo labels. The goal is to construct a self-supervised pretext task that matches the person re-ID objective. Inspired by the data association concept in multi-object tracking, we propose the Cycle Association (CycAs) task: after performing data association between a pair of video frames forward and then backward, a pedestrian instance is supposed to be associated to itself. To fulfill this goal, the model must learn a meaningful representation that can well describe correspondences between instances in frame pairs. We adapt the discrete association process to a differentiable form, such that end-to-end training becomes feasible. Experiments are conducted in two aspects: We first compare our method with existing unsupervised re-ID methods on seven benchmarks and demonstrate CycAs’ superiority. Then, to further validate the practical value of CycAs in real-world applications, we perform training on self-collected videos and report promising performance on standard test sets.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_5');
INSERT INTO `paper` VALUES (11725, 'Cyclic Functional Mapping: Self-supervised Correspondence Between Non-isometric Deformable Shapes', 'Dense shape correspondence', 'Self-supervision', 'One-shot learning', 'Spectral decomposition', '3D alignment', 'We present the first spatial-spectral joint consistency network for self-supervised dense correspondence mapping between non-isometric shapes. The task of alignment in non-Euclidean domains is one of the most fundamental and crucial problems in computer vision. As 3D scanners can generate highly complex and dense models, the mission of finding dense mappings between those models is vital. The novelty of our solution is based on a cyclic mapping between metric spaces, where the distance between a pair of points should remain invariant after the full cycle. As the same learnable rules that generate the point-wise descriptors apply in both directions, the network learns invariant structures without any labels while coping with non-isometric deformations. We show here state-of-the-art-results by a large margin for a variety of tasks compared to known self-supervised and supervised methods .', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_3');
INSERT INTO `paper` VALUES (11726, 'DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search', 'Data adapted pruning', 'Neural Architecture Search', 'Search cost', '', '', 'Efficient search is a core issue in Neural Architecture Search (NAS). It is difficult for conventional NAS algorithms to directly search the architectures on large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS grows with regard to training dataset size and candidate set size. One common way is searching on a smaller proxy dataset (e.g., CIFAR-10) and then transferring to the target task (e.g., ImageNet). These architectures optimized on proxy data are not guaranteed to be optimal on the target task. Another common way is learning with a smaller candidate set, which may require expert knowledge and indeed betrays the essence of NAS. In this paper, we present DA-NAS that can directly search the architecture for large-scale target tasks while allowing a large candidate set in a more efficient manner. Our method is based on an interesting observation that the learning speed for blocks in deep neural networks is related to the difficulty of recognizing distinct categories. We carefully design a progressive data adapted pruning strategy for efficient architecture search. It will quickly trim low performed blocks on a subset of target dataset (e.g., easy classes), and then gradually find the best blocks on the whole target dataset. At this time, the original candidate set becomes as compact as possible, providing a faster search in the target task. Experiments on ImageNet verify the effectiveness of our approach. It is \\(\\mathbf{2} \\varvec{\\times }\\) faster than previous methods while the accuracy is currently state-of-the-art, at 76.2% under small FLOPs constraint. It supports an argument search space (i.e., more candidate blocks) to efficiently search the best-performing architecture.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_35');
INSERT INTO `paper` VALUES (11727, 'DA4AD: End-to-End Deep Attention-Based Visual Localization for Autonomous Driving', '', '', '', '', '', 'We present a visual localization framework based on novel deep attention aware features for autonomous driving that achieves centimeter level localization accuracy. Conventional approaches to the visual localization problem rely on handcrafted features or human-made objects on the road. They are known to be either prone to unstable matching caused by severe appearance or lighting changes, or too scarce to deliver constant and robust localization results in challenging scenarios. In this work, we seek to exploit the deep attention mechanism to search for salient, distinctive and stable features that are good for long-term matching in the scene through a novel end-to-end deep neural network. Furthermore, our learned feature descriptors are demonstrated to be competent to establish robust matches and therefore successfully estimate the optimal camera poses with high precision. We comprehensively validate the effectiveness of our method using a freshly collected dataset with high-quality ground truth trajectories and hardware synchronization between sensors. Results demonstrate that our method achieves a competitive localization accuracy when compared to the LiDAR-based localization solutions under various challenging circumstances, leading to a potential low-cost localization solution for autonomous driving.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_17');
INSERT INTO `paper` VALUES (11728, 'DanbooRegion: An Illustration Region Dataset', 'Artistic creation', 'Fine art', 'Cartoon', 'Region processing', '', 'Region is a fundamental element of various cartoon animation techniques and artistic painting applications. Achieving satisfactory region is essential to the success of these techniques. Motivated to assist diversiform region-based cartoon applications, we invite artists to annotate regions for in-the-wild cartoon images with several application-oriented goals: (1) To assist image-based cartoon rendering, relighting, and cartoon intrinsic decomposition literature, artists identify object outlines and eliminate lighting-and-shadow boundaries. (2) To assist cartoon inking tools, cartoon structure extraction applications, and cartoon texture processing techniques, artists clean-up texture or deformation patterns and emphasize cartoon structural boundary lines. (3) To assist region-based cartoon digitalization, clip-art vectorization, and animation tracking applications, artists inpaint and reconstruct broken or blurred regions in cartoon images. Given the typicality of these involved applications, this dataset is also likely to be used in other cartoon techniques. We detail the challenges in achieving this dataset and present a human-in-the-loop workflow namely Feasibility-based Assignment Recommendation (FAR) to enable large-scale annotating. The FAR tends to reduce artist trails-and-errors and encourage their enthusiasm during annotating. Finally, we present a dataset that contains a large number of artistic region compositions paired with corresponding cartoon illustrations. We also invite multiple professional artists to assure the quality of each annotation.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_9');
INSERT INTO `paper` VALUES (11729, 'Data Augmentation Techniques for the Video Question Answering Task', 'Vision and language', 'Video Question Answering', 'Egocentric vision', 'Data augmentation', '', 'Video Question Answering (VideoQA) is a task that requires a model to analyze and understand both the visual content given by the input video and the textual part given by the question, and the interaction between them in order to produce a meaningful answer. In our work we focus on the Egocentric VideoQA task, which exploits first-person videos, because of the importance of such task which can have impact on many different fields, such as those pertaining the social assistance and the industrial training. Recently, an Egocentric VideoQA dataset, called EgoVQA, has been released. Given its small size, models tend to overfit quickly. To alleviate this problem, we propose several augmentation techniques which give us a +5.5% improvement on the final accuracy over the considered baseline.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_33');
INSERT INTO `paper` VALUES (11730, 'DataMix: Efficient Privacy-Preserving Edge-Cloud Inference', '', '', '', '', '', 'Deep neural networks are widely deployed on edge devices (e.g., for computer vision and speech recognition). Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run inference remotely (i.e., cloud-based). However, both solutions have their limitations: edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, DataMix. We off-load the majority of the computations to the cloud and leverage a pair of mixing and de-mixing operation, inspired by mixup, to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as the mixing cannot be inverted without the user’s private mixing coefficients. Second, our framework is accuracy-preserving because our framework takes advantage of the space spanned by images, and we train the model in a mixing-aware manner to maintain accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and our mixing and de-mixing processes introduce very few extra computations. Also, our framework introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple computer vision and speech recognition datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_34');
INSERT INTO `paper` VALUES (11731, 'DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural Networks', 'Deep learning', 'Quantization', 'Low-complexity neural networks', '', '', 'Deep neural networks have achieved state-of-the art performance on various computer vision tasks. However, their deployment on resource-constrained devices has been hindered due to their high computational and storage complexity. While various complexity reduction techniques, such as lightweight network architecture design and parameter quantization, have been successful in reducing the cost of implementing these networks, these methods have often been considered orthogonal. In reality, existing quantization techniques fail to replicate their success on lightweight architectures such as MobileNet. To this end, we present a novel fully differentiable non-uniform quantizer that can be seamlessly mapped onto efficient ternary-based dot product engines. We conduct comprehensive experiments on CIFAR-10, ImageNet, and Visual Wake Words datasets. The proposed quantizer (DBQ) successfully tackles the daunting task of aggressively quantizing lightweight networks such as MobileNetV1, MobileNetV2, and ShuffleNetV2. DBQ achieves state-of-the art results with minimal training overhead and provides the best (pareto-optimal) accuracy-complexity trade-off.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_6');
INSERT INTO `paper` VALUES (11732, 'DDGCN: A Dynamic Directed Graph Convolutional Network for Action Recognition', 'Action modeling and recognition', 'Graph Convolutional Network', 'Dynamic Spatiotemporal Graph', '', '', 'We propose a Dynamic Directed Graph Convolutional Network (DDGCN) to model spatial and temporal features of human actions from their skeletal representations. The DDGCN consists of three new feature modeling modules: (1) Dynamic Convolutional Sampling (DCS), (2) Dynamic Convolutional Weight (DCW) assignment, and (3) Directed Graph Spatial-Temporal (DGST) feature extraction. Comprehensive experiments show that the DDGCN outperforms existing state-of-the-art action recognition approaches in various testing datasets.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_45');
INSERT INTO `paper` VALUES (11733, 'Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition', 'Skeleton-based action recognition', 'Decoupling GCN', 'DropGraph', '', '', 'In skeleton-based action recognition, graph convolutional networks (GCNs) have achieved remarkable success. Nevertheless, how to efficiently model the spatial-temporal skeleton graph without introducing extra computation burden is a challenging problem for industrial deployment. In this paper, we rethink the spatial aggregation in existing GCN-based skeleton action recognition methods and discover that they are limited by coupling aggregation mechanism. Inspired by the decoupling aggregation mechanism in CNNs, we propose decoupling GCN to boost the graph modeling ability with no extra computation, no extra latency, no extra GPU memory cost, and less than 10% extra parameters. Another prevalent problem of GCNs is over-fitting. Although dropout is a widely used regularization technique, it is not effective for GCNs, due to the fact that activation units are correlated between neighbor nodes. We propose DropGraph to discard features in correlated nodes, which is particularly effective on GCNs. Moreover, we introduce an attention-guided drop mechanism to enhance the regularization effect. All our contributions introduce zero extra computation burden at deployment. We conduct experiments on three datasets (NTU-RGBD, NTU-RGBD-120, and Northwestern-UCLA) and exceed the state-of-the-art performance with less computation cost.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_32');
INSERT INTO `paper` VALUES (11734, 'Deep Adaptive Inference Networks for Single Image Super-Resolution', 'Single image super-resolution', 'Convolutional neural network', 'Adaptive inference', '', '', 'Recent years have witnessed tremendous progress in single image super-resolution (SISR) owing to the deployment of deep convolutional neural networks (CNNs). For most existing methods, the computational cost of each SISR model is irrelevant to local image content, hardware platform and application scenario. Nonetheless, content and resource adaptive model is more preferred, and it is encouraging to apply simpler and efficient networks to the easier regions with less details and the scenarios with restricted efficiency constraints. In this paper, we take a step forward to address this issue by leveraging the adaptive inference networks for deep SISR (AdaDSR). In particular, our AdaDSR involves an SISR model as backbone and a lightweight adapter module which takes image features and resource constraint as input and predicts a map of local network depth. Adaptive inference can then be performed with the support of efficient sparse convolution, where only a fraction of the layers in the backbone is performed at a given position according to its predicted depth. The network learning can be formulated as joint optimization of reconstruction and network depth losses. In the inference stage, the average depth can be flexibly tuned to meet a range of efficiency constraints. Experiments demonstrate the effectiveness and adaptability of our AdaDSR in contrast to its counterparts (e.g., EDSR and RCAN). Code is available at https://github.com/csmliu/AdaDSR.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_8');
INSERT INTO `paper` VALUES (11735, 'Deep Atrous Guided Filter for Image Restoration in Under Display Cameras', 'Under-display camera', 'Image restoration', 'Image enhancement', '', '', 'Under Display Cameras present a promising opportunity for phone manufacturers to achieve bezel-free displays by positioning the camera behind semi-transparent OLED screens. Unfortunately, such imaging systems suffer from severe image degradation due to light attenuation and diffraction effects. In this work, we present Deep Atrous Guided Filter (DAGF), a two-stage, end-to-end approach for image restoration in UDC systems. A Low-Resolution Network first restores image quality at low-resolution, which is subsequently used by the Guided Filter Network as a filtering input to produce a high-resolution output. Besides the initial downsampling, our low-resolution network uses multiple, parallel atrous convolutions to preserve spatial resolution and emulates multi-scale processing. Our approach’s ability to directly train on megapixel images results in significant performance improvement. We additionally propose a simple simulation scheme to pre-train our model and boost performance. Our overall framework ranks 2nd and 5th in the RLQ-TOD’20 UDC Challenge for POLED and TOLED displays, respectively.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_29');
INSERT INTO `paper` VALUES (11736, 'Deep Complementary Joint Model for Complex Scene Registration and Few-Shot Segmentation on Medical Images', '', '', '', '', '', 'Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions’ disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge [42] show great advantages of our DeepRS that outperforms the existing state-of-the-art models.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_45');
INSERT INTO `paper` VALUES (11737, 'Deep Credible Metric Learning for Unsupervised Domain Adaptation Person Re-identification', 'Credible learning', 'Metric learning', 'Unsupervised domain adaptation', 'Person re-identification', '', 'The trained person re-identification systems fundamentally need to be deployed on different target environments. Learning the cross-domain model has great potential for the scalability of real-world applications. In this paper, we propose a deep credible metric learning (DCML) method for unsupervised domain adaptation person re-identification. Unlike existing methods that directly finetune the model in the target domain with pseudo labels generated by the source pre-trained model, our DCML method adaptively mines credible samples for training to avoid the misleading from noise labels. Specifically, we design two credibility metrics for sample mining including the k-Nearest Neighbor similarity for density evaluation and the prototype similarity for centrality evaluation. As the increasing of the pseudo label credibility, we progressively adjust the sampling strategy in the training process. In addition, we propose an instance margin spreading loss to further increase instance-wise discrimination. Experimental results demonstrate that our DCML method explores credible and valuable training data and improves the performance of unsupervised domain adaptation.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_38');
INSERT INTO `paper` VALUES (11738, 'Deep Cross-Species Feature Learning for Animal Face Recognition via Residual Interspecies Equivariant Network', 'Animal face recognition', 'Interspecies', 'Fine-tuning', 'Feature equivariant', 'Feature fusion', 'Although human face recognition has achieved exceptional success driven by deep learning, animal face recognition (AFR) is still a research field that received less attention. Due to the big challenge in collecting large-scale animal face datasets, it is difficult to train a high-precision AFR model from scratch. In this work, we propose a novel Residual InterSpecies Equivariant Network (RiseNet) to deal with the animal face recognition task with limited training samples. First, we formulate a module called residual inter-species feature equivariant to make the feature distribution of animals face closer to the human. Second, according to the structural characteristic of animal face, the features of the upper and lower half faces are learned separately. We present an animal facial feature fusion module to treat the features of the lower half face as additional information, which improves the proposed RiseNet performance. Besides, an animal face alignment strategy is designed for the preprocessing of the proposed network, which further aligns with the human face image. Extensive experiments on two benchmarks show that our method is effective and outperforms the state-of-the-arts.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_40');
INSERT INTO `paper` VALUES (11739, 'Deep Cyclic Generative Adversarial Residual Convolutional Networks for Real Image Super-Resolution', 'Real image super-resolution', 'Cyclic GAN', 'Image restoration', 'Convex optimization', 'Deep convolutional neural networks', 'Recent deep learning based single image super-resolution (SISR) methods mostly train their models in a clean data domain where the low-resolution (LR) and the high-resolution (HR) images come from noise-free settings (same domain) due to the bicubic down-sampling assumption. However, such degradation process is not available in real-world settings. We consider a deep cyclic network structure to maintain the domain consistency between the LR and HR data distributions, which is inspired by the recent success of CycleGAN in the image-to-image translation applications. We propose the Super-Resolution Residual Cyclic Generative Adversarial Network (SRResCycGAN) by training with a generative adversarial network (GAN) framework for the LR to HR domain translation in an end-to-end manner. We demonstrate our proposed approach in the quantitative and qualitative experiments that generalize well to the real image super-resolution and it is easy to deploy for the mobile/embedded devices. In addition, our SR results on the AIM 2020 Real Image SR Challenge datasets demonstrate that the proposed SR approach achieves comparable results as the other state-of-art methods.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_29');
INSERT INTO `paper` VALUES (11740, 'Deep Decomposition Learning for Inverse Imaging Problems', 'Decomposition learning', 'Physics', 'Inverse problems', '', '', 'Deep learning is emerging as a new paradigm for solving inverse imaging problems. However, the deep learning methods often lack the assurance of traditional physics-based methods due to the lack of physical information considerations in neural network training and deploying. The appropriate supervision and explicit calibration by the information of the physic model can enhance the neural network learning and its practical performance. In this paper, inspired by the geometry that data can be decomposed by two components from the null-space of the forward operator and the range space of its pseudo-inverse, we train neural networks to learn the two components and therefore learn the decomposition, i.e. we explicitly reformulate the neural network layers as learning range-nullspace decomposition functions with reference to the layer inputs, instead of learning unreferenced functions. We empirically show that the proposed framework demonstrates superior performance over recent deep residual learning, unrolled learning and nullspace learning on tasks including compressive sensing medical imaging and natural image super-resolution. Our code is available at https://github.com/edongdongchen/DDN.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_31');
INSERT INTO `paper` VALUES (11741, 'Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images', '', '', '', '', '', 'High-fidelity clothing reconstruction is the key to achieving photorealism in a wide range of applications including human digitization, virtual try-on, etc. Recent advances in learning-based approaches have accomplished unprecedented accuracy in recovering unclothed human shape and pose from single images, thanks to the availability of powerful statistical models, e.g. SMPL, learned from a large number of body scans. In contrast, modeling and recovering clothed human and 3D garments remains notoriously difficult, mostly due to the lack of large-scale clothing models available for the research community. We propose to fill this gap by introducing Deep Fashion3D, the largest collection to date of 3D garment models, with the goal of establishing a novel benchmark and dataset for the evaluation of image-based garment reconstruction systems. Deep Fashion3D contains 2078 models reconstructed from real garments, which covers 10 different categories and 563 garment instances. It provides rich annotations including 3D feature lines, 3D body pose and the corresponded multi-view real images. In addition, each garment is randomly posed to enhance the variety of real clothing deformations. To demonstrate the advantage of Deep Fashion3D, we propose a novel baseline approach for single-view garment reconstruction, which leverages the merits of both mesh and implicit representations. A novel adaptable template is proposed to enable the learning of all types of clothing in a single network. Extensive experiments have been conducted on the proposed dataset to verify its significance and usefulness.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_30');
INSERT INTO `paper` VALUES (11742, 'Deep Feedback Inverse Problem Solver', '', '', '', '', '', 'We present an efficient, effective, and generic approach towards solving inverse problems. The key idea is to leverage the feedback signal provided by the forward process and learn an iterative update model. Specifically, at each iteration, the neural network takes the feedback as input and outputs an update on current estimation. Our approach does not have any restrictions on the forward process; it does not require any prior knowledge either. Through the feedback information, our model not only can produce accurate estimations that are coherent to the input observation but also is capable of recovering from early incorrect predictions. We verify the performance of our model over a wide range of inverse problems, including 6-DoF pose estimation, illumination estimation, as well as inverse kinematics. Comparing to traditional optimization-based methods, we can achieve comparable or better performance while being two to three orders of magnitude faster. Compared to deep learning-based approaches, our model consistently improves the performance on all metrics.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_14');
INSERT INTO `paper` VALUES (11743, 'Deep FusionNet for Point Cloud Semantic Segmentation', '', '', '', '', '', 'Many point cloud segmentation methods rely on transferring irregular points into a voxel-based regular representation. Although voxel-based convolutions are useful for feature aggregation, they produce ambiguous or wrong predictions if a voxel contains points from different classes. Other approaches (such as PointNets and point-wise convolutions) can take irregular points for feature learning. But their high memory and computational costs (such as for neighborhood search and ball-querying) limit their ability and accuracy for large-scale point cloud processing. To address these issues, we propose a deep fusion network architecture (FusionNet) with a unique voxel-based “mini-PointNet” point cloud representation and a new feature aggregation module (fusion module) for large-scale 3D semantic segmentation. Our FusionNet can learn more accurate point-wise predictions when compared to voxel-based convolutional networks. It can realize more effective feature aggregations with lower memory and computational complexity for large-scale point cloud segmentation when compared to the popular point-wise convolutions. Our experimental results show that FusionNet can take more than one million points on one GPU for training to achieve state-of-the-art accuracy on large-scale Semantic KITTI benchmark.The code will be available at https://github.com/feihuzhang/LiDARSeg.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_38');
INSERT INTO `paper` VALUES (11744, 'Deep Graph Matching via Blackbox Differentiation of Combinatorial Solvers', 'Deep graph matching', 'Keypoint correspondence', 'Combinatorial optimization', '', '', 'Building on recent progress at the intersection of combinatorial optimization and deep learning, we propose an end-to-end trainable architecture for deep graph matching that contains unmodified combinatorial solvers. Using the presence of heavily optimized combinatorial solvers together with some improvements in architecture design, we advance state-of-the-art on deep graph matching benchmarks for keypoint correspondence. In addition, we highlight the conceptual advantages of incorporating solvers into deep learning architectures, such as the possibility of post-processing with a strong multi-graph matching solver or the indifference to changes in the training setting. Finally, we propose two new challenging experimental setups.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_25');
INSERT INTO `paper` VALUES (11745, 'Deep Hashing with Active Pairwise Supervision', 'Active learning', 'Deep hashing', 'Structural risk minimization', '', '', 'In this paper, we propose a Deep Hashing method with Active Pairwise Supervision (DH-APS). Conventional methods with passive pairwise supervision obtain labeled data for training and require large amount of annotations to reach their full potential, which are not feasible in realistic retrieval tasks. On the contrary, we actively select a small quantity of informative samples for annotation to provide effective pairwise supervision so that discriminative hash codes can be obtained with limited annotation budget. Specifically, we generalize the structural risk minimization principle and obtain three criteria for the pairwise supervision acquisition: uncertainty, representativeness and diversity. Accordingly, samples involved in the following training pairs should be labeled: pairs with most uncertain similarity, pairs that minimize the discrepancy between labeled and unlabeled data, and pairs which are most different from the annotated data, so that the discriminality and generalization ability of the learned hash codes are significantly strengthened. Moreover, our DH-APS can also be employed as a plug-and-play module for semi-supervised hashing methods to further enhance the performance. Experiments demonstrate that the presented DH-APS achieves the accuracy of supervised hashing methods with only \\(30\\%\\) labeled training samples and improves the semi-supervised binary codes by a sizable margin.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_31');
INSERT INTO `paper` VALUES (11746, 'Deep Hough Transform for Semantic Line Detection', 'Straight line detection', 'Hough transform', 'CNN', '', '', 'In this paper, we put forward a simple yet effective method to detect meaningful straight lines, a.k.a. semantic lines, in given scenes. Prior methods take line detection as a special case of object detection, while neglect the inherent characteristics of lines, leading to less efficient and suboptimal results. We propose a one-shot end-to-end framework by incorporating the classical Hough transform into deeply learned representations. By parameterizing lines with slopes and biases, we perform Hough transform to translate deep representations to the parametric space and then directly detect lines in the parametric space. More concretely, we aggregate features along candidate lines on the feature map plane and then assign the aggregated features to corresponding locations in the parametric domain. Consequently, the problem of detecting semantic lines in the spatial domain is transformed to spotting individual points in the parametric domain, making the post-processing steps, i.e. non-maximal suppression, more efficient. Furthermore, our method makes it easy to extract contextual line features, that are critical to accurate line detection. Experimental results on a public dataset demonstrate the advantages of our method over state-of-the-arts. Codes are available at https://mmcheng.net/dhtline/.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_15');
INSERT INTO `paper` VALUES (11747, 'Deep Hough-Transform Line Priors', 'Hough transform', 'Global line prior', 'Line segment detection', '', '', 'Classical work on line segment detection is knowledge-based; it uses carefully designed geometric priors using either image gradients, pixel groupings, or Hough transform variants. Instead, current deep learning methods do away with all prior knowledge and replace priors by training deep networks on large manually annotated datasets. Here, we reduce the dependency on labeled data by building on the classic knowledge-based priors while using deep networks to learn features. We add line priors through a trainable Hough transform block into a deep network. Hough transform provides the prior knowledge about global line parameterizations, while the convolutional layers can learn the local gradient-like line features. On the Wireframe (ShanghaiTech) and York Urban datasets we show that adding prior knowledge improves data efficiency as line priors no longer need to be learned from data.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_20');
INSERT INTO `paper` VALUES (11748, 'Deep Image Clustering with Category-Style Representation', 'Image clustering', 'Deep learning', 'Unsupervised learning', '', '', 'Deep clustering which adopts deep neural networks to obtain optimal representations for clustering has been widely studied recently. In this paper, we propose a novel deep image clustering framework to learn a category-style latent representation in which the category information is disentangled from image style and can be directly used as the cluster assignment. To achieve this goal, mutual information maximization is applied to embed relevant information in the latent representation. Moreover, augmentation-invariant loss is employed to disentangle the representation into category part and style part. Last but not least, a prior distribution is imposed on the latent representation to ensure the elements of the category vector can be used as the probabilities over clusters. Comprehensive experiments demonstrate that the proposed approach outperforms state-of-the-art methods significantly on five public datasets (Project address: https://github.com/sKamiJ/DCCS).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_4');
INSERT INTO `paper` VALUES (11749, 'Deep Image Compression Using Decoder Side Information', 'Deep Distributed Source Coding', 'Deep Neural Networks', 'Deep Learning', 'Image reconstruction', '', 'We present a Deep Image Compression neural network that relies on side information, which is only available to the decoder. We base our algorithm on the assumption that the image available to the encoder and the image available to the decoder are correlated, and we let the network learn these correlations in the training phase.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_41');
INSERT INTO `paper` VALUES (11750, 'Deep k-NN Defense Against Clean-Label Data Poisoning Attacks', 'Machine learning', 'Adversarial attacks', 'Clean label poisoning', 'Deep k-NN', '', 'Targeted clean-label data poisoning is a type of adversarial attack on machine learning systems in which an adversary injects a few correctly-labeled, minimally-perturbed samples into the training data, causing a model to misclassify a particular test sample during inference. Although defenses have been proposed for general poisoning attacks, no reliable defense for clean-label attacks has been demonstrated, despite the attacks’ effectiveness and realistic applications. In this work, we propose a simple, yet highly-effective Deep k-NN defense against both feature collision and convex polytope clean-label attacks on the CIFAR-10 dataset. We demonstrate that our proposed strategy is able to detect over 99% of poisoned examples in both attacks and remove them without compromising model performance. Additionally, through ablation studies, we discover simple guidelines for selecting the value of k as well as for implementing the Deep k-NN defense on real-world datasets with class imbalance. Our proposed defense shows that current clean-label poisoning attack strategies can be annulled, and serves as a strong yet simple-to-implement baseline defense to test future clean-label poisoning attacks. Our code is available on GitHub.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_4');
INSERT INTO `paper` VALUES (11751, 'Deep Learning-Based Pupil Center Detection for Fast and Accurate Eye Tracking System', 'Remote eye tracking', 'Mobile applications', '', '', '', 'In augmented reality (AR) or virtual reality (VR) systems, eye tracking is a key technology and requires significant accuracy as well as real-time operation. Many techniques for detecting pupil centers with error range of iris radius have been developed, but few techniques have precise performance with error range of pupil radius. In addition, the conventional methods rarely guarantee real-time pupil center detection in a general-purpose computer environment due to high complexity. Thus, we propose more accurate pupil center detection by improving the representation quality of the network in charge of pupil center detection. This is realized by representation learning based on mutual information. Also, the latency of the entire system is greatly reduced by using non-local block and self-attention block with large receptive field, which makes it accomplish real-time operation. The proposed system not only shows real-time performance of 52 FPS in a general-purpose computer environment but also provides state-of-the-art accuracy in terms of fine level index of 96.71%, 99.84% and 96.38% for BioID, GI4E and Talking Face Video datasets, respectively.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_3');
INSERT INTO `paper` VALUES (11752, 'Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction', '', '', '', '', '', 'Efficiently reconstructing complex and intricate surfaces at scale is a long-standing goal in machine perception. To address this problem we introduce Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements. DeepLS replaces the dense volumetric signed distance function (SDF) representation used in traditional surface reconstruction systems with a set of locally learned continuous SDFs defined by a neural network, inspired by recent work such as DeepSDF. Unlike DeepSDF, which represents an object-level SDF with a neural network and a single latent code, we store a grid of independent latent codes, each responsible for storing information about surfaces in a small local neighborhood. This decomposition of scenes into local shapes simplifies the prior distribution that the network must learn, and also enables efficient inference. We demonstrate the effectiveness and generalization power of DeepLS by showing object shape encoding and reconstructions of full scenes, where DeepLS delivers high compression, accuracy, and local shape completion.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_36');
INSERT INTO `paper` VALUES (11753, 'Deep Material Recognition in Light-Fields via Disentanglement of Spatial and Angular Information', 'Light field', 'Material recognition', 'Angular registration', '', '', 'Light-field cameras capture sub-views from multiple perspectives simultaneously, with possibly reflectance variations that can be used to augment material recognition in remote sensing, autonomous driving, etc. Existing approaches for light-field based material recognition suffer from the entanglement between angular and spatial domains, leading to inefficient training which in turn limits their performances. In this paper, we propose an approach that achieves decoupling of angular and spatial information by establishing correspondences in the angular domain, then employs regularization to enforce a rotational invariance. As opposed to relying on the Lambertian surface assumption, we align the angular domain by estimating sub-pixel displacements using the Fourier transform. The network takes sparse inputs, i.e. sub-views along particular directions, to gain structural information about the angular domain. A novel regularization technique further improves generalization by weight sharing and max-pooling among different directions. The proposed approach outperforms any previously reported method on multiple datasets. The accuracy gain over 2D images is improved by a factor of 1.5. Ablation studies are conducted to demonstrate the significance of each component.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_39');
INSERT INTO `paper` VALUES (11754, 'Deep Multi Depth Panoramas for View Synthesis', '360\\(^\\circ \\) panoramas', 'View synthesis', 'Image-based rendering', 'Virtual reality', '', 'We propose a learning-based approach for novel view synthesis for multi-camera 360\\(^\\circ \\) panorama capture rigs. Previous work constructs RGBD panoramas from such data, allowing for view synthesis with small amounts of translation, but cannot handle the disocclusions and view-dependent effects that are caused by large translations. To address this issue, we present a novel scene representation—Multi Depth Panorama (MDP)—that consists of multiple RGBD\\(\\alpha \\) panoramas that represent both scene geometry and appearance. We demonstrate a deep neural network-based method to reconstruct MDPs from multi-camera 360\\(^\\circ \\) images. MDPs are more compact than previous 3D scene representations and enable high-quality, efficient new view rendering. We demonstrate this via experiments on both synthetic and real data and comparisons with previous state-of-the-art methods spanning both learning-based approaches and classical RGBD-based methods.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_20');
INSERT INTO `paper` VALUES (11755, 'Deep Near-Light Photometric Stereo for Spatially Varying Reflectances', 'Near-light photometric stereo', '', '', '', '', 'This paper presents a near-light photometric stereo method for spatially varying reflectances. Recent studies in photometric stereo proposed learning-based approaches to handle diverse real-world reflectances and achieve high accuracy compared to conventional methods. However, they assume distant (i.e., parallel) lights, which can in practical settings only be approximately realized, and they fail in near-light conditions. Near-light photometric stereo methods address near-light conditions but previous works are limited to over-simplified reflectances, such as Lambertian reflectance. The proposed method takes a hybrid approach of distant- and near-light models, where the surface normal of a small area (corresponding to a pixel) is computed locally with a distant light assumption, and the reconstruction error is assessed based on a near-light image formation model. This paper is the first work to solve unknown, spatially varying, diverse reflectances in near-light photometric stereo.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_9');
INSERT INTO `paper` VALUES (11756, 'Deep Novel View Synthesis from Colored 3D Point Clouds', 'Image synthesis', '3D point clouds', 'Virtual views', '', '', 'We propose a new deep neural network which takes a colored 3D point cloud of a scene as input, and synthesizes a photo-realistic image from a novel viewpoint. Key contributions of this work include a deep point feature extraction module, an image synthesis module, and a refinement module. Our PointEncoder network extracts discriminative features from the point cloud that contain both local and global contextual information about the scene. Next, the multi-level point features are aggregated to form multi-layer feature maps, which are subsequently fed into an ImageDecoder network to generate a synthetic RGB image. Finally, the output of the ImageDecoder network is refined using a RefineNet module, providing finer details and suppressing unwanted visual artifacts. W rotate and translate the 3D point cloud in order to synthesize new images from a novel perspective. We conduct numerous experiments on public datasets to validate the method in terms of quality of the synthesized views.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_1');
INSERT INTO `paper` VALUES (11757, 'Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches', 'Image editing', 'Sketch-to-image translation', 'User control', '', '', 'Sketch-based image editing aims to synthesize and modify photos based on the structural information provided by the human-drawn sketches. Since sketches are difficult to collect, previous methods mainly use edge maps instead of sketches to train models (referred to as edge-based models). However, human-drawn sketches display great structural discrepancy with edge maps, thus failing edge-based models. Moreover, sketches often demonstrate huge variety among different users, demanding even higher generalizability and robustness for the editing model to work. In this paper, we propose Deep Plastic Surgery, a novel, robust and controllable image editing framework that allows users to interactively edit images using hand-drawn sketch inputs. We present a sketch refinement strategy, as inspired by the coarse-to-fine drawing process of the artists, which we show can help our model well adapt to casual and varied sketches without the need for real sketch training data. Our model further provides a refinement level control parameter that enables users to flexibly define how “reliable” the input sketch should be considered for the final output, balancing between sketch faithfulness and output verisimilitude (as the two goals might contradict if the input sketch is drawn poorly). To achieve the multi-level refinement, we introduce a style-based module for level conditioning, which allows adaptive feature representations for different levels in a singe network. Extensive experimental results demonstrate the superiority of our approach in improving the visual quality and user controllablity of image editing over the state-of-the-art methods. Our project and code are available at https://github.com/TAMU-VITA/DeepPS.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_36');
INSERT INTO `paper` VALUES (11758, 'Deep Plug-and-Play Video Super-Resolution', 'Deep learning', 'Video super-resolution', 'Single image super-resolution', 'CNN', 'LSTM', 'Video super-resolution (VSR) has been drawing increasing research attention due to its wide practical applications. Despite the unprecedented success of deep single image super-resolution (SISR), recent deep VSR methods devote much effort to designing modules for spatial alignment and feature fusion of multiple adjacent frames while failing to leverage the progress in SISR. In this paper, we propose a plug-and-play VSR framework, through which the state-of-the-art SISR models can be readily employed without re-training, and the proposed temporal consistency refinement network (TCRNet) can enhance the temporal consistency and visual quality. In particular, an SISR model is firstly adopted to super-resolve low-resolution video in a frame-by-frame manner. Instead of using multiple frames, our TCRNet only takes two adjacent frames as input. To alleviate the issue of spatial misalignments, we present an iterative residual refinement module for motion offset estimation. Furthermore, a deformable convolutional LSTM is proposed to exploit long-distance temporal information. The proposed TCRNet can be easily and stably trained using \\(\\ell _2\\) loss function. Moreover, the VSR performance is further boosted by a bidirectional process. On popular benchmark datasets, our TCRNet can significantly enhance the temporal consistency when collaborating with various SISR models, and is superior to or at least on par with state-of-the-art VSR methods in terms of quantitative metrics and visually quality.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_7');
INSERT INTO `paper` VALUES (11759, 'Deep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis', 'Rotation-invariance', 'Point cloud', 'Deep feature learning', '', '', 'In this paper we propose a rotation-invariant deep network for point clouds analysis. Point-based deep networks are commonly designed to recognize roughly aligned 3D shapes based on point coordinates, but suffer from performance drops with shape rotations. Some geometric features, e.g., distances and angles of points as inputs of network, are rotation-invariant but lose positional information of points. In this work, we propose a novel deep network for point clouds by incorporating positional information of points as inputs while yielding rotation-invariance. The network is hierarchical and relies on two modules: a positional feature embedding block and a relational feature embedding block. Both modules and the whole network are proven to be rotation-invariant when processing point clouds as input. Experiments show state-of-the-art classification and segmentation performances on benchmark datasets, and ablation studies demonstrate effectiveness of the network design .', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_13');
INSERT INTO `paper` VALUES (11760, 'Deep Reflectance Volumes: Relightable Reconstructions from Multi-view Photometric Images', 'View synthesis', 'Relighting', 'Appearance acquisition', 'Neural rendering', '', 'We present a deep learning approach to reconstruct scene appearance from unstructured images captured under collocated point lighting. At the heart of Deep Reflectance Volumes is a novel volumetric scene representation consisting of opacity, surface normal and reflectance voxel grids. We present a novel physically-based differentiable volume ray marching framework to render these scene volumes under arbitrary viewpoint and lighting. This allows us to optimize the scene volumes to minimize the error between their rendered images and the captured images. Our method is able to reconstruct real scenes with challenging non-Lambertian reflectance and complex geometry with occlusions and shadowing. Moreover, it accurately generalizes to novel viewpoints and lighting, including non-collocated lighting, rendering photorealistic images that are significantly better than state-of-the-art mesh-based methods. We also show that our learned reflectance volumes are editable, allowing for modifying the materials of the captured scenes.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_18');
INSERT INTO `paper` VALUES (11761, 'Deep Reinforced Attention Learning for Quality-Aware Visual Recognition', 'Convolutional Neural Networks', 'Attention modules', 'Reinforcement learning', 'Visual recognition', '', 'In this paper, we build upon the weakly-supervised generation mechanism of intermediate attention maps in any convolutional neural networks and disclose the effectiveness of attention modules more straightforwardly to fully exploit their potential. Given an existing neural network equipped with arbitrary attention modules, we introduce a meta critic network to evaluate the quality of attention maps in the main network. Due to the discreteness of our designed reward, the proposed learning method is arranged in a reinforcement learning setting, where the attention actors and recurrent critics are alternately optimized to provide instant critique and revision for the temporary attention representation, hence coined as Deep REinforced Attention Learning (DREAL). It could be applied universally to network architectures with different types of attention modules and promotes their expressive ability by maximizing the relative gain of the final recognition performance arising from each individual attention module, as demonstrated by extensive experiments on both category and instance recognition benchmarks.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_29');
INSERT INTO `paper` VALUES (11762, 'Deep Relighting Networks for Image Light Source Manipulation', 'Image relighting', 'Back-projection theory', 'Deep learning', '', '', 'Manipulating the light source of given images is an interesting task and useful in various applications, including photography and cinematography. Existing methods usually require additional information like the geometric structure of the scene, which may not be available for most images. In this paper, we formulate the single image relighting task and propose a novel Deep Relighting Network (DRN) with three parts: 1) scene reconversion, which aims to reveal the primary scene structure through a deep auto-encoder network, 2) shadow prior estimation, to predict light effect from the new light direction through adversarial learning, and 3) re-renderer, to combine the primary structure with the reconstructed shadow view to form the required estimation under the target light source. Experiments show that the proposed method outperforms other possible methods, both qualitatively and quantitatively. Specifically, the proposed DRN has achieved the best PSNR in the “AIM2020 - Any to one relighting challenge” of the 2020 ECCV conference.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_33');
INSERT INTO `paper` VALUES (11763, 'Deep Shape from Polarization', 'Shape from Polarization', '3D reconstruction', 'Physics-based deep learning', '', '', 'This paper makes a first attempt to bring the Shape from Polarization (SfP) problem to the realm of deep learning. The previous state-of-the-art methods for SfP have been purely physics-based. We see value in these principled models, and blend these physical models as priors into a neural network architecture. This proposed approach achieves results that exceed the previous state-of-the-art on a challenging dataset we introduce. This dataset consists of polarization images taken over a range of object textures, paints, and lighting conditions. We report that our proposed method achieves the lowest test error on each tested condition in our dataset, showing the value of blending data-driven and physics-driven approaches.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_33');
INSERT INTO `paper` VALUES (11764, 'Deep Space-Time Video Upsampling Networks', 'Video super-resolution', 'Video frame interpolation', 'Joint space-time upsampling', '', '', 'Video super-resolution (VSR) and frame interpolation (FI) are traditional computer vision problems, and the performance have been improving by incorporating deep learning recently. In this paper, we investigate the problem of jointly upsampling videos both in space and time, which is becoming more important with advances in display systems. One solution for this is to run VSR and FI, one by one, independently. This is highly inefficient as heavy deep neural networks (DNN) are involved in each solution. To this end, we propose an end-to-end DNN framework for the space-time video upsampling by efficiently merging VSR and FI into a joint framework. In our framework, a novel weighting scheme is proposed to fuse all input frames effectively without explicit motion compensation for efficient processing of videos. The results show better results both quantitatively and qualitatively, while reducing the computation time (\\(\\times \\)7 faster) and the number of parameters (30%) compared to baselines. Our source code is available at https://github.com/JaeYeonKang/STVUN-Pytorch.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_41');
INSERT INTO `paper` VALUES (11765, 'Deep Spatial-Angular Regularization for Compressive Light Field Reconstruction over Coded Apertures', 'Light field', 'Coded aperture', 'Deep learning', 'Regularization', 'Observation model', 'Coded aperture is a promising approach for capturing the 4-D light field (LF), in which the 4-D data are compressively modulated into 2-D coded measurements that are further decoded by reconstruction algorithms. The bottleneck lies in the reconstruction algorithms, resulting in rather limited reconstruction quality. To tackle this challenge, we propose a novel learning-based framework for the reconstruction of high-quality LFs from acquisitions via learned coded apertures. The proposed method incorporates the measurement observation into the deep learning framework elegantly to avoid relying entirely on data-driven priors for LF reconstruction. Specifically, we first formulate the compressive LF reconstruction as an inverse problem with an implicit regularization term. Then, we construct the regularization term with an efficient deep spatial-angular convolutional sub-network to comprehensively explore the signal distribution free from the limited representation ability and inefficiency of deterministic mathematical modeling. Experimental results show that the reconstructed LFs not only achieve much higher PSNR/SSIM but also preserve the LF parallax structure better, compared with state-of-the-art methods on both real and synthetic LF benchmarks. In addition, experiments show that our method is efficient and robust to noise, which is an essential advantage for a real camera system. The code is publicly available at https://github.com/angmt2008/LFCA.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_17');
INSERT INTO `paper` VALUES (11766, 'Deep Spiking Neural Network: Energy Efficiency Through Time Based Coding', 'Spiking Neural Network', 'ANN-SNN conversion', 'Temporal coding', 'Energy efficiency', 'Deep learning', 'Spiking Neural Networks (SNNs) are promising for enabling low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained deep learning Analog Neural Network (ANN) composed of Rectified Linear Unit (ReLU) activation to SNN consisting of Integrate-and-Fire (IF) neurons with “proper” firing thresholds. However, this has come at the cost of accuracy loss and higher inference latency due to lack of a notion of time. In this work, we propose an ANN to SNN conversion methodology that uses a time-based coding scheme, named Temporal-Switch-Coding (TSC), and a corresponding TSC spiking neuron model. Each input image pixel is presented using two spikes and the timing between the two spiking instants is proportional to the pixel intensity. The real-valued ReLU activations in ANN are encoded using the spike-times of the TSC neurons in the converted TSC-SNN. At most two memory accesses and two addition operations are performed for each synapse during the whole inference, which significantly improves the SNN energy efficiency. We demonstrate the proposed TSC-SNN for VGG-16, ResNet-20 and ResNet-34 SNNs on datasets including CIFAR-10 (93.63% top-1), CIFAR-100 (70.97% top-1) and ImageNet (73.46% top-1 accuracy). It surpasses the best inference accuracy of the converted rate-encoded SNN with 7–14.5\\(\\times \\) lesser inference latency, and 30–60\\(\\times \\) fewer addition operations and memory accesses per inference across datasets.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_23');
INSERT INTO `paper` VALUES (11767, 'Deep Surface Normal Estimation on the 2-Sphere with Confidence Guided Semantic Attention', 'Normal estimation', '2-sphere', 'Confidence guided semantic attention', 'Mutual feature fusion', '', 'We propose a deep convolutional neural network (CNN) to estimate surface normal from a single color image accompanied with a low-quality depth channel. Unlike most previous works, we predict the normal on the 2-sphere rather than the 3D Euclidean space, which produces naturally normalized values and makes the training stable. Although the depth information is beneficial for normal estimation, the raw data contain missing values and noises. To alleviate this problem, we employ a confidence guided semantic attention (CGSA) module to progressively improve the quality of depth channel during training. The continuously refined depth features are fused with the normal features at multiple scales with the mutual feature fusion (MFF) modules to fully exploit the correlations between normals and depth, resulting in high quality normals and depth with fine details. Extensive experiments on multiple benchmark datasets prove the superiority of the proposed method.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_43');
INSERT INTO `paper` VALUES (11768, 'Deep Transferring Quantization', 'Quantization', 'Deep transfer', 'Knowledge distillation', '', '', 'Network quantization is an effective method for network compression. Existing methods train a low-precision network by fine-tuning from a pre-trained model. However, training a low-precision network often requires large-scale labeled data to achieve superior performance. In many real-world scenarios, only limited labeled data are available due to expensive labeling costs or privacy protection. With limited training data, fine-tuning methods may suffer from the overfitting issue and substantial accuracy loss. To alleviate these issues, we introduce transfer learning into network quantization to obtain an accurate low-precision model. Specifically, we propose a method named deep transferring quantization (DTQ) to effectively exploit the knowledge in a pre-trained full-precision model. To this end, we propose a learnable attentive transfer module to identify the informative channels for alignment. In addition, we introduce the Kullback-Leibler (KL) divergence to further help train a low-precision model. Extensive experiments on both image classification and face recognition demonstrate the effectiveness of DTQ.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_37');
INSERT INTO `paper` VALUES (11769, 'Deep Vectorization of Technical Drawings', 'Transformer network', 'Vectorization', 'Floor plans', 'Technical drawings', '', 'We present a new method for vectorization of technical line drawings, such as floor plans, architectural drawings, and 2D CAD images. Our method includes (1) a deep learning-based cleaning stage to eliminate the background and imperfections in the image and fill in missing parts, (2) a transformer-based network to estimate vector primitives, and (3) optimization procedure to obtain the final primitive configurations. We train the networks on synthetic data, renderings of vector line drawings, and manually vectorized scans of line drawings. Our method quantitatively and qualitatively outperforms a number of existing techniques on a collection of representative technical drawings.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_35');
INSERT INTO `paper` VALUES (11770, 'DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares', 'Normal estimation', 'Surface fitting', 'Least squares', 'Unstructured 3D point clouds', '3D point cloud deep learning', 'We propose a surface fitting method for unstructured 3D point clouds. This method, called DeepFit, incorporates a neural network to learn point-wise weights for weighted least squares polynomial surface fitting. The learned weights act as a soft selection for the neighborhood of surface points thus avoiding the scale selection required of previous methods. To train the network we propose a novel surface consistency loss that improves point weight estimation. The method enables extracting normal vectors and other geometrical properties, such as principal curvatures, the latter were not presented as ground truth during training. We achieve state-of-the-art results on a benchmark normal and curvature estimation dataset, demonstrate robustness to noise, outliers and density variations, and show its application on noise removal.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_2');
INSERT INTO `paper` VALUES (11771, 'DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting', 'Image inpainting', 'Attention', 'Back projection', '', '', 'The degree of difficulty in image inpainting depends on the types and sizes of the missing parts. Existing image inpainting approaches usually encounter difficulties in completing the missing parts in the wild with pleasing visual and contextual results as they are trained for either dealing with one specific type of missing patterns (mask) or unilaterally assuming the shapes and/or sizes of the masked areas. We propose a deep generative inpainting network, named DeepGIN, to handle various types of masked images. We design a Spatial Pyramid Dilation (SPD) ResNet block to enable the use of distant features for reconstruction. We also employ Multi-Scale Self-Attention (MSSA) mechanism and Back Projection (BP) technique to enhance our inpainting results. Our DeepGIN outperforms the state-of-the-art approaches generally, including two publicly available datasets (FFHQ and Oxford Buildings), both quantitatively and qualitatively. We also demonstrate that our model is capable of completing masked images in the wild.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_1');
INSERT INTO `paper` VALUES (11772, 'DeepGMR: Learning Latent Gaussian Mixture Models for Registration', 'Point cloud registration', 'Gaussian Mixture Model', '', '', '', 'Point cloud registration is a fundamental problem in 3D computer vision, graphics and robotics. For the last few decades, existing registration algorithms have struggled in situations with large transformations, noise, and time constraints. In this paper, we introduce Deep Gaussian Mixture Registration (DeepGMR), the first learning-based registration method that explicitly leverages a probabilistic registration paradigm by formulating registration as the minimization of KL-divergence between two probability distributions modeled as mixtures of Gaussians. We design a neural network that extracts pose-invariant correspondences between raw point clouds and Gaussian Mixture Model (GMM) parameters and two differentiable compute blocks that recover the optimal transformation from matched GMM parameters. This construction allows the network learn an SE(3)-invariant feature space, producing a global registration method that is real-time, generalizable, and robust to noise. Across synthetic and real-world data, our proposed method shows favorable performance when compared with state-of-the-art geometry-based and learning-based registration methods.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_43');
INSERT INTO `paper` VALUES (11773, 'DeepHandMesh: A Weakly-Supervised Deep Encoder-Decoder Framework for High-Fidelity Hand Mesh Modeling', '', '', '', '', '', 'Human hands play a central role in interacting with other people and objects. For realistic replication of such hand motions, high-fidelity hand meshes have to be reconstructed. In this study, we firstly propose DeepHandMesh, a weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling. We design our system to be trained in an end-to-end and weakly-supervised manner; therefore, it does not require groundtruth meshes. Instead, it relies on weaker supervisions such as 3D joint coordinates and multi-view depth maps, which are easier to get than groundtruth meshes and do not dependent on the mesh topology. Although the proposed DeepHandMesh is trained in a weakly-supervised way, it provides significantly more realistic hand mesh than previous fully-supervised hand models. Our newly introduced penetration avoidance loss further improves results by replicating physical interaction between hand parts. Finally, we demonstrate that our system can also be applied successfully to the 3D hand mesh estimation from general images. Our hand model, dataset, and codes are publicly available(https://mks0601.github.io/DeepHandMesh/).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_26');
INSERT INTO `paper` VALUES (11774, 'DeepLandscape: Adversarial Modeling of Landscape Videos', '', '', '', '', '', 'We build a new model of landscape videos that can be trained on a mixture of static landscape images as well as landscape animations. Our architecture extends StyleGAN model by augmenting it with parts that allow to model dynamic changes in a scene. Once trained, our model can be used to generate realistic time-lapse landscape videos with moving objects and time-of-the-day changes. Furthermore, by fitting the learned models to a static landscape image, the latter can be reenacted in a realistic way. We propose simple but necessary modifications to StyleGAN inversion procedure, which lead to in-domain latent codes and allow to manipulate real images. Quantitative comparisons and user studies suggest that our model produces more compelling animations of given photographs than previously proposed methods. The results of our approach including comparisons with prior art can be seen in supplementary materials and on the project page https://saic-mdal.github.io/deep-landscape/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_16');
INSERT INTO `paper` VALUES (11775, 'DeepSFM: Structure from Motion via Deep Bundle Adjustment', '', '', '', '', '', 'Structure from motion (SfM) is an essential computer vision problem which has not been well handled by deep learning. One of the promising trends is to apply explicit structural constraint, e.g. 3D cost volume, into the network. However, existing methods usually assume accurate camera poses either from GT or other methods, which is unrealistic in practice. In this work, we design a physical driven architecture, namely DeepSFM, inspired by traditional Bundle Adjustment (BA), which consists of two cost volume based architectures for depth and pose estimation respectively, iteratively running to improve both. The explicit constraints on both depth (structure) and pose (motion), when combined with the learning components, bring the merit from both traditional BA and emerging deep learning technology. Extensive experiments on various datasets show that our model achieves the state-of-the-art performance on both depth and pose estimation with superior robustness against less number of inputs and the noise in initialization.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_14');
INSERT INTO `paper` VALUES (11776, 'Defense Against Adversarial Attacks via Controlling Gradient Leaking on Embedded Manifolds', 'Gradient leaking', 'DNNs', 'Adversarial robustness', '', '', 'Deep neural networks are vulnerable to adversarial attacks. Though various attempts have been made, it is still largely open to fully understand the existence of adversarial samples and thereby develop effective defense strategies. In this paper, we present a new perspective, namely gradient leaking hypothesis, to understand the existence of adversarial examples and to further motivate effective defense strategies. Specifically, we consider the low dimensional manifold structure of natural images, and empirically verify that the leakage of the gradient (w.r.t input) along the (approximately) perpendicular direction to the tangent space of data manifold is a reason for the vulnerability over adversarial attacks. Based on our investigation, we further present a new robust learning algorithm which encourages a larger gradient component in the tangent space of data manifold, suppressing the gradient leaking phenomenon consequently. Experiments on various tasks demonstrate the effectiveness of our algorithm despite its simplicity.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_45');
INSERT INTO `paper` VALUES (11777, 'Defenses Against Multi-sticker Physical Domain Attacks on Classifiers', 'Real-world adversarial attacks', 'Defenses', 'Classifiers', 'Deep learning', '', 'Recently, physical domain adversarial attacks have drawn significant attention from the machine learning community. One important attack proposed by Eykholt et al. can fool a classifier by placing black and white stickers on an object such as a road sign. While this attack may pose a significant threat to visual classifiers, there are currently no defenses designed to protect against this attack. In this paper, we propose new defenses that can protect against multi-sticker attacks. We present defensive strategies capable of operating when the defender has full, partial, and no prior information about the attack. By conducting extensive experiments, we show that our proposed defenses can outperform existing defenses against physical attacks when presented with a multi-sticker attack.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_13');
INSERT INTO `paper` VALUES (11778, 'Defocus Blur Detection via Depth Distillation', 'Defocus blur detection', 'Attention module', 'Knowledge distillation', '', '', 'Defocus Blur Detection (DBD) aims to separate in-focus and out-of-focus regions from a single image pixel-wisely. This task has been paid much attention since bokeh effects are widely used in digital cameras and smartphone photography. However, identifying obscure homogeneous regions and borderline transitions in partially defocus images is still challenging. To solve these problems, we introduce depth information into DBD for the first time. When the camera parameters are fixed, we argue that the accuracy of DBD is highly related to scene depth. Hence, we consider the depth information as the approximate soft label of DBD and propose a joint learning framework inspired by knowledge distillation. In detail, we learn the defocus blur from ground truth and the depth distilled from a well-trained depth estimation network at the same time. Thus, the sharp region will provide a strong prior for depth estimation while the blur detection also gains benefits from the distilled depth. Besides, we propose a novel decoder in the fully convolutional network (FCN) as our network structure. In each level of the decoder, we design the Selective Reception Field Block (SRFB) for merging multi-scale features efficiently and reuse the side outputs as Supervision-guided Attention Block (SAB). Unlike previous methods, the proposed decoder builds reception field pyramids and emphasizes salient regions simply and efficiently. Experiments show that our approach outperforms 11 other state-of-the-art methods on two popular datasets. Our method also runs at over 30 fps on a single GPU, which is 2x faster than previous works. The code is available at: https://github.com/vinthony/depth-distillation.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_44');
INSERT INTO `paper` VALUES (11779, 'Defocus Deblurring Using Dual-Pixel Data', 'Defocus blur', 'Extended depth of field', 'Dual-pixel sensors', '', '', 'Defocus blur arises in images that are captured with a shallow depth of field due to the use of a wide aperture. Correcting defocus blur is challenging because the blur is spatially varying and difficult to estimate. We propose an effective defocus deblurring method that exploits data available on dual-pixel (DP) sensors found on most modern cameras. DP sensors are used to assist a camera’s auto-focus by capturing two sub-aperture views of the scene in a single image shot. The two sub-aperture images are used to calculate the appropriate lens position to focus on a particular scene region and are discarded afterwards. We introduce a deep neural network (DNN) architecture that uses these discarded sub-aperture images to reduce defocus blur. A key contribution of our effort is a carefully captured dataset of 500 scenes (2000 images) where each scene has: (i) an image with defocus blur captured at a large aperture; (ii) the two associated DP sub-aperture views; and (iii) the corresponding all-in-focus image captured with a small aperture. Our proposed DNN produces results that are significantly better than conventional single image methods in terms of both quantitative and perceptual metrics – all from data that is already available on the camera but ignored.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_7');
INSERT INTO `paper` VALUES (11780, 'Deformable Kernel Convolutional Network for Video Extreme Super-Resolution', 'Video super-resolution', 'Deep learning', 'Deformable kernels', 'Deformable convolution network', 'Attention mechanism', 'Video super-resolution, which attempts to reconstruct high-resolution video frames from their corresponding low-resolution versions, has received increasingly more attention in recent years. Most existing approaches opt to use deformable convolution to temporally align neighboring frames and apply traditional spatial attention mechanism (convolution based) to enhance reconstructed features. However, such spatial-only strategies cannot fully utilize temporal dependency among video frames. In this paper, we propose a novel deep learning based VSR algorithm, named Deformable Kernel Spatial Attention Network (DKSAN). Thanks to newly designed Deformable Kernel Convolution Alignment (DKC_Align) and Deformable Kernel Spatial Attention (DKSA) modules, DKSAN can better exploit both spatial and temporal redundancies to facilitate the information propagation across different layers. We have tested DKSAN on AIM2020 Video Extreme Super-Resolution Challenge to super-resolve videos with a scale factor as large as 16. Experimental results demonstrate that our proposed DKSAN can achieve both better subjective and objective performance compared with the existing state-of-the-art EDVR on Vid3oC and IntVID datasets.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_5');
INSERT INTO `paper` VALUES (11781, 'Deformable Style Transfer', 'Neural style transfer', 'Geometric deformation', 'Differentiable image warping', '', '', 'Both geometry and texture are fundamental aspects of visual style. Existing style transfer methods, however, primarily focus on texture, almost entirely ignoring geometry. We propose deformable style transfer (DST), an optimization-based approach that jointly stylizes the texture and geometry of a content image to better match a style image. Unlike previous geometry-aware stylization methods, our approach is neither restricted to a particular domain (such as human faces), nor does it require training sets of matching style/content pairs. We demonstrate our method on a diverse set of content and style images including portraits, animals, objects, scenes, and paintings. Code has been made publicly available at https://github.com/sunniesuhyoung/DST.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_15');
INSERT INTO `paper` VALUES (11782, 'Deformation-Aware 3D Model Embedding and Retrieval', '3D model retrieval', 'Deformation-aware embedding', 'Non-metric embedding', '', '', 'We introduce a new problem of retrieving 3D models that are deformable to a given query shape and present a novel deep deformation-aware embedding to solve this retrieval task. 3D model retrieval is a fundamental operation for recovering a clean and complete 3D model from a noisy and partial 3D scan. However, given a finite collection of 3D shapes, even the closest model to a query may not be satisfactory. This motivates us to apply 3D model deformation techniques to adapt the retrieved model so as to better fit the query. Yet, certain restrictions are enforced in most 3D deformation techniques to preserve important features of the original model that prevent a perfect fitting of the deformed model to the query. This gap between the deformed model and the query induces asymmetric relationships among the models, which cannot be handled by typical metric learning techniques. Thus, to retrieve the best models for fitting, we propose a novel deep embedding approach that learns the asymmetric relationships by leveraging location-dependent egocentric distance fields. We also propose two strategies for training the embedding network. We demonstrate that both of these approaches outperform other baselines in our experiments with both synthetic and real data. Our project page can be found at deformscan2cad.github.io.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_24');
INSERT INTO `paper` VALUES (11783, 'DELTAS: Depth Estimation by Learning Triangulation and Densification of Sparse Points', '3D from multi-view and sensors', 'Stereo depth estimation', 'Multi-task learning', '', '', 'Multi-view stereo (MVS) is the golden mean between the accuracy of active depth sensing and the practicality of monocular depth estimation. Cost volume based approaches employing 3D convolutional neural networks (CNNs) have considerably improved the accuracy of MVS systems. However, this accuracy comes at a high computational cost which impedes practical adoption. Distinct from cost volume approaches, we propose an efficient depth estimation approach by first (a) detecting and evaluating descriptors for interest points, then (b) learning to match and triangulate a small set of interest points, and finally (c) densifying this sparse set of 3D points using CNNs. An end-to-end network efficiently performs all three steps within a deep learning framework and trained with intermediate 2D image and 3D geometric supervision, along with depth supervision. Crucially, our first step complements pose estimation using interest point detection and descriptor learning. We demonstrate state-of-the-art results on depth estimation with lower compute for different scene lengths. Furthermore, our method generalizes to newer environments and the descriptors output by our network compare favorably to strong baselines.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_7');
INSERT INTO `paper` VALUES (11784, 'DEMEA: Deep Mesh Autoencoders for Non-rigidly Deforming Objects', 'Auto-encoding', 'Embedded deformation', 'Non-rigid tracking', '', '', 'Mesh autoencoders are commonly used for dimensionality reduction, sampling and mesh modeling. We propose a general-purpose DEep MEsh Autoencoder (DEMEA) which adds a novel embedded deformation layer to a graph-convolutional mesh autoencoder. The embedded deformation layer (EDL) is a differentiable deformable geometric proxy which explicitly models point displacements of non-rigid deformations in a lower dimensional space and serves as a local rigidity regularizer. DEMEA decouples the parameterization of the deformation from the final mesh resolution since the deformation is defined over a lower dimensional embedded deformation graph. We perform a large-scale study on four different datasets of deformable objects. Reasoning about the local rigidity of meshes using EDL allows us to achieve higher-quality results for highly deformable objects, compared to directly regressing vertex positions. We demonstrate multiple applications of DEMEA, including non-rigid 3D reconstruction from depth and shading cues, non-rigid surface tracking, as well as the transfer of deformations over different meshes.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_35');
INSERT INTO `paper` VALUES (11785, 'Demographic Influences on Contemporary Art with Unsupervised Style Embeddings', 'Unsupervised analysis', 'Contemporary art', 'Social networks', '', '', 'Computational art analysis has, through its reliance on classification tasks, prioritised historical datasets in which the artworks are already well sorted with the necessary annotations. Art produced today, on the other hand, is numerous and easily accessible, through the internet and social networks that are used by professional and amateur artists alike to display their work. Although this art—yet unsorted in terms of style and genre—is less suited for supervised analysis, the data sources come with novel information that may help frame the visual content in equally novel ways. As a first step in this direction, we present contempArt, a multi-modal dataset of exclusively contemporary artworks. contempArt is a collection of paintings and drawings, a detailed graph network based on social connections on Instagram and additional socio-demographic information; all attached to 442 artists at the beginning of their career. We evaluate three methods suited for generating unsupervised style embeddings of images and correlate them with the remaining data. We find no connections between visual style on the one hand and social proximity, gender, and nationality on the other.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_10');
INSERT INTO `paper` VALUES (11786, 'DenoiSeg: Joint Denoising and Segmentation', 'Segmentation', 'Denoising', 'Co-learning', '', '', 'Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain. Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. We achieve this by extending Noise2Void, a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense 3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network. The network becomes a denoising expert by seeing all available raw data, while co-learning to segment, even if only a few segmentation labels are available. This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. This renders the denoising-task non-trivial and unleashes the desired co-learning effect. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables learning of dense segmentations when only very limited amounts of segmentation labels are available.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_21');
INSERT INTO `paper` VALUES (11787, 'Dense Hybrid Recurrent Multi-view Stereo Net with Dynamic Consistency Checking', 'Multi-view stereo', 'Deep learning', 'Dense hybrid recurrent-MVSNet', 'Dynamic consistency checking', '', 'In this paper, we propose an efficient and effective dense hybrid recurrent multi-view stereo net with dynamic consistency checking, namely \\(D^{2}\\)HC-RMVSNet, for accurate dense point cloud reconstruction. Our novel hybrid recurrent multi-view stereo net consists of two core modules: 1) a light DRENet (Dense Reception Expanded) module to extract dense feature maps of original size with multi-scale context information, 2) a HU-LSTM (Hybrid U-LSTM) to regularize 3D matching volume into predicted depth map, which efficiently aggregates different scale information by coupling LSTM and U-Net architecture. To further improve the accuracy and completeness of reconstructed point clouds, we leverage a dynamic consistency checking strategy instead of prefixed parameters and strategies widely adopted in existing methods for dense point cloud reconstruction. In doing so, we dynamically aggregate geometric consistency matching error among all the views. Our method ranks \\(1^{st}\\) on the complex outdoor Tanks and Temples benchmark over all the methods. Extensive experiments on the in-door DTU dataset show our method exhibits competitive performance to the state-of-the-art method while dramatically reduces memory consumption, which costs only \\(19.4\\%\\) of R-MVSNet memory consumption. The codebase is available at https://github.com/yhw-yhw/D2HC-RMVSNet.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_39');
INSERT INTO `paper` VALUES (11788, 'Dense RepPoints: Representing Visual Objects with Dense Point Sets', '', '', '', '', '', 'We present a new object representation, called Dense RepPoints, that utilizes a large set of points to describe an object at multiple levels, including both box level and pixel level. Techniques are proposed to efficiently process these dense points, maintaining near-constant complexity with increasing point numbers. Dense RepPoints is shown to represent and learn object segments well, with the use of a novel distance transform sampling method combined with set-to-set supervision. The distance transform sampling combines the strengths of contour and grid representations, leading to performance that surpasses counterparts based on contours or grids. Code is available at https://github.com/justimyhxu/Dense-RepPoints.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_14');
INSERT INTO `paper` VALUES (11789, 'Densely Connecting Depth Maps for Monocular Depth Estimation', 'Monocular depth estimation', 'Dense connection mechanism', 'Diversity enhancement device', '', '', 'Predicting depth map from a single RGB image is beneficial for many three-dimensional applications. Although recent monocular depth estimation methods have achieved impressive accuracy, the preference on high-level features or low-level features prevents them from balancing sharpness and fidelity of depth maps. In this work, we propose a dense connection mechanism that connects diverse sub-depth maps produced by the sub-predictors to the final depth map to contribute information from features at different levels. Besides, two kinds of diversity enhancement devices are proposed to increase the number and diversity of the sub-depth maps collected by the dense connection mechanism. Experimental results on KITTI and NYU Depth V2 datasets shows that, by fusing the dense connection mechanism and diversity enhancement devices, our proposed method achieves state-of-the-art accuracy and predicts sharp depth maps that restore reliable object structures.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_9');
INSERT INTO `paper` VALUES (11790, 'Describing Textures Using Natural Language', '', '', '', '', '', 'Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_4');
INSERT INTO `paper` VALUES (11791, 'Describing Unseen Videos via Multi-modal Cooperative Dialog Agents', 'Video description', 'Dialog agents', 'Multi-modal', '', '', 'With the arising concerns for the AI systems provided with direct access to abundant sensitive information, researchers seek to develop more reliable AI with implicit information sources. To this end, in this paper, we introduce a new task called video description via two multi-modal cooperative dialog agents, whose ultimate goal is for one conversational agent to describe an unseen video based on the dialog and two static frames. Specifically, one of the intelligent agents - Q-BOT - is given two static frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has already seen the entire video, assists Q-BOT to accomplish the goal by providing answers to those questions. We propose a QA-Cooperative Network with a dynamic dialog history update learning mechanism to transfer knowledge from A-BOT to Q-BOT, thus helping Q-BOT to better describe the video. Extensive experiments demonstrate that Q-BOT can effectively learn to describe an unseen video by the proposed model and the cooperative learning method, achieving the promising performance where Q-BOT is given the full ground truth history dialog. Codes and models are available at https://github.com/L-YeZhu/Video-Description-via-Dialog-Agents-ECCV2020.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_10');
INSERT INTO `paper` VALUES (11792, 'Design and Interpretation of Universal Adversarial Patches in Face Detection', '', '', '', '', '', 'We consider universal adversarial patches for faces—small visual elements whose addition to a face image reliably destroys the performance of face detectors. Unlike previous work that mostly focused on the algorithmic design of adversarial examples in terms of improving the success rate as an attacker, in this work we show an interpretation of such patches that can prevent the state-of-the-art face detectors from detecting the real faces. We investigate a phenomenon: patches designed to suppress real face detection appear face-like. This phenomenon holds generally across different initialization, locations, scales of patches, backbones and face detection frameworks. We propose new optimization-based approaches to automatic design of universal adversarial patches for varying goals of the attack, including scenarios in which true positives are suppressed without introducing false positives. Our proposed algorithms perform well on real-world datasets, deceiving state-of-the-art face detectors in terms of multiple precision/recall metrics and transferability.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_11');
INSERT INTO `paper` VALUES (11793, 'Detail Preserved Point Cloud Completion via Separated Feature Aggregation', 'Point cloud', 'Shape completion', 'Deep learning', '', '', 'Point cloud shape completion is a challenging problem in 3D vision and robotics. Existing learning-based frameworks leverage encoder-decoder architectures to recover the complete shape from a highly encoded global feature vector. Though the global feature can approximately represent the overall shape of 3D objects, it would lead to the loss of shape details during the completion process. In this work, instead of using a global feature to recover the whole complete surface, we explore the functionality of multi-level features and aggregate different features to represent the known part and the missing part separately. We propose two different feature aggregation strategies, named global & local feature aggregation (GLFA) and residual feature aggregation (RFA), to express the two kinds of features and reconstruct coordinates from their combination. In addition, we also design a refinement component to prevent the generated point cloud from non-uniform distribution and outliers. Extensive experiments have been conducted on the ShapeNet and KITTI dataset. Qualitative and quantitative evaluations demonstrate that our proposed network outperforms current state-of-the art methods especially on detail preservation.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_31');
INSERT INTO `paper` VALUES (11794, 'Detecting Faces, Visual Medium Types, and Gender in Historical Advertisements, 1950–1995', 'Face detection', 'Heritage', 'Historical advertisements', 'Medium detection', 'Gender detection', 'Libraries, museums, and other heritage institutions are digitizing large parts of their archives. Computer vision techniques enable scholars to query, analyze, and enrich the visual sources in these archives. However, it remains unclear how well algorithms trained on modern photographs perform on historical material. This study evaluates and adapts existing algorithms. We show that we can detect faces, visual media types, and gender with high accuracy in historical advertisements. It remains difficult to detect gender when faces are either of low quality or relatively small or large. Further optimization of scaling might solve the latter issue, while the former might be ameliorated using upscaling. We show how computer vision can produce meta-data information, which can enrich historical collections. This information can be used for further analysis of the historical representation of gender.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_7');
INSERT INTO `paper` VALUES (11795, 'Detecting Human-Object Interactions with Action Co-occurrence Priors', '', '', '', '', '', 'A common problem in human-object interaction (HOI) detection task is that numerous HOI classes have only a small number of labeled examples, resulting in training sets with a long-tailed distribution. The lack of positive labels can lead to low classification accuracy for these classes. Towards addressing this issue, we observe that there exist natural correlations and anti-correlations among human-object interactions. In this paper, we model the correlations as action co-occurrence matrices and present techniques to learn these priors and leverage them for more effective training, especially on rare classes. The utility of our approach is demonstrated experimentally, where the performance of our approach exceeds the state-of-the-art methods on both of the two leading HOI detection benchmark datasets, HICO-Det and V-COCO.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_43');
INSERT INTO `paper` VALUES (11796, 'Detecting Natural Disasters, Damage, and Incidents in the Wild', 'Image classification', 'Visual recognition', 'Scene understanding', 'Image dataset', 'Social media', 'Responding to natural disasters, such as earthquakes, floods, and wildfires, is a laborious task performed by on-the-ground emergency responders and analysts. Social media has emerged as a low-latency data source to quickly understand disaster situations. While most studies on social media are limited to text, images offer more information for understanding disaster and incident scenes. However, no large-scale image datasets for incident detection exists. In this work, we present the Incidents Dataset, which contains 446,684 images annotated by humans that cover 43 incidents across a variety of scenes. We employ a baseline classification model that mitigates false-positive errors and we perform image filtering experiments on millions of social media images from Flickr and Twitter. Through these experiments, we show how the Incidents Dataset can be used to detect images with incidents in the wild. Code, data, and models are available online at http://incidentsdataset.csail.mit.edu.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_20');
INSERT INTO `paper` VALUES (11797, 'Detection in Agricultural Contexts: Are We Close to Human Level?', 'Detection', 'Precision agriculture', 'Human performance', '', '', 'We consider detection accuracy in agricultural contexts. Five challenging datasets were collected and benchmarked, with three recent networks tested. Based on an initial analysis showing the importance of image resolution, models were trained and tested with a multiple-resolution procedure. Detection results were compared to human performance, judged based on the consistency of multiple annotators. A quantitative analysis was made highlighting the role of object scale and occlusion as detection failure causes. Finally, novel detection accuracy metrics were suggested based on the needs of agriculture tasks, and used in detector performance evaluation.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_26');
INSERT INTO `paper` VALUES (11798, 'Determining the Relevance of Features for Deep Neural Networks', 'Explainable-AI', 'Structural causal model', 'Deep learning', 'Causality', '', 'Deep neural networks are tremendously successful in many applications, but end-to-end trained networks often result in hard to understand black-box classifiers or predictors. In this work, we present a novel method to identify whether a specific feature is relevant to a classifier’s decision or not. This relevance is determined at the level of the learned mapping, instead of for a single example. The approach does neither need retraining of the network nor information on intermediate results or gradients. The key idea of our approach builds upon concepts from causal inference. We interpret machine learning in a structural causal model and use Reichenbach’s common cause principle to infer whether a feature is relevant. We demonstrate empirically that the method is able to successfully evaluate the relevance of given features on three real-life data sets, namely MS COCO, CUB200 and HAM10000.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_20');
INSERT INTO `paper` VALUES (11799, 'DH3D: Deep Hierarchical 3D Descriptors for Robust Large-Scale 6DoF Relocalization', 'Point clouds', '3D deep learning', 'Relocalization', '', '', 'For relocalization in large-scale point clouds, we propose the first approach that unifies global place recognition and local 6DoF pose refinement. To this end, we design a Siamese network that jointly learns 3D local feature detection and description directly from raw 3D points. It integrates FlexConv and Squeeze-and-Excitation (SE) to assure that the learned local descriptor captures multi-level geometric information and channel-wise relations. For detecting 3D keypoints we predict the discriminativeness of the local descriptors in an unsupervised manner. We generate the global descriptor by directly aggregating the learned local descriptors with an effective attention mechanism. In this way, local and global 3D descriptors are inferred in one single forward pass. Experiments on various benchmarks demonstrate that our method achieves competitive results for both global point cloud retrieval and local point cloud registration in comparison to state-of-the-art approaches. To validate the generalizability and robustness of our 3D keypoints, we demonstrate that our method also performs favorably without fine-tuning on the registration of point clouds that were generated by a visual SLAM system. Code and related materials are available at https://vision.in.tum.de/research/vslam/dh3d.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_43');
INSERT INTO `paper` VALUES (11800, 'DHP: Differentiable Meta Pruning via HyperNetworks', 'Network pruning', 'Hyperneworks', 'Meta learning', 'Differentiable optimization', 'Proximal gradient', 'Network pruning has been the driving force for the acceleration of neural networks and the alleviation of model storage/transmission burden. With the advent of AutoML and neural architecture search (NAS), pruning has become topical with automatic mechanism and searching based architecture optimization. Yet, current automatic designs rely on either reinforcement learning or evolutionary algorithm. Due to the non-differentiability of those algorithms, the pruning algorithm needs a long searching stage before reaching the convergence.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_36');
INSERT INTO `paper` VALUES (11801, 'Differentiable Automatic Data Augmentation', 'AutoML', 'Data augmentation', 'Differentiable optimization', '', '', 'Data augmentation (DA) techniques aim to increase data variability, and thus train deep networks with better generalisation. The pioneering AutoAugment automated the search for optimal DA policies with reinforcement learning. However, AutoAugment is extremely computationally expensive, limiting its wide applicability. Followup works such as Population Based Augmentation (PBA) and Fast AutoAugment improved efficiency, but their optimization speed remains a bottleneck. In this paper, we propose Differentiable Automatic Data Augmentation (DADA) which dramatically reduces the cost. DADA relaxes the discrete DA policy selection to a differentiable optimization problem via Gumbel-Softmax. In addition, we introduce an unbiased gradient estimator, RELAX, leading to an efficient and effective one-pass optimization strategy to learn an efficient and accurate DA policy. We conduct extensive experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate the value of Auto DA in pre-training for downstream detection problems. Results show our DADA is at least one order of magnitude faster than the state-of-the-art while achieving very comparable accuracy. The code is available at https://github.com/VDIGPKU/DADA.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_35');
INSERT INTO `paper` VALUES (11802, 'Differentiable Feature Aggregation Search for Knowledge Distillation', 'Knowledge distillation', 'Feature aggregation', 'Differentiable architecture search', '', '', 'Knowledge distillation has become increasingly important in model compression. It boosts the performance of a miniaturized student network with the supervision of the output distribution and feature maps from a sophisticated teacher network. Some recent works introduce multi-teacher distillation to provide more supervision to the student network. However, the effectiveness of multi-teacher distillation methods are accompanied by costly computation resources. To tackle with both the efficiency and the effectiveness of knowledge distillation, we introduce the feature aggregation to imitate the multi-teacher distillation in the single-teacher distillation framework by extracting informative supervision from multiple teacher feature maps. Specifically, we introduce DFA, a two-stage Differentiable Feature Aggregation search method that motivated by DARTS in neural architecture search, to efficiently find the aggregations. In the first stage, DFA formulates the searching problem as a bi-level optimization and leverages a novel bridge loss, which consists of a student-to-teacher path and a teacher-to-student path, to find appropriate feature aggregations. The two paths act as two players against each other, trying to optimize the unified architecture parameters to the opposite directions while guaranteeing both expressivity and learnability of the feature aggregation simultaneously. In the second stage, DFA performs knowledge distillation with the derived feature aggregation. Experimental results show that DFA outperforms existing distillation methods on CIFAR-100 and CINIC-10 datasets under various teacher-student settings, verifying the effectiveness and robustness of the design.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_28');
INSERT INTO `paper` VALUES (11803, 'Differentiable Hierarchical Graph Grouping for Multi-person Pose Estimation', 'Human pose estimation', 'Graph neural network', 'Grouping', '', '', 'Multi-person pose estimation is challenging because it localizes body keypoints for multiple persons simultaneously. Previous methods can be divided into two streams, i.e. top-down and bottom-up methods. The top-down methods localize keypoints after human detection, while the bottom-up methods localize keypoints directly and then cluster/group them for different persons, which are generally more efficient than top-down methods. However, in existing bottom-up methods, the keypoint grouping is usually solved independently from keypoint detection, making them not end-to-end trainable and have sub-optimal performance. In this paper, we investigate a new perspective of human part grouping and reformulate it as a graph clustering task. Especially, we propose a novel differentiable Hierarchical Graph Grouping (HGG) method to learn the graph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is easily embedded into main-stream bottom-up methods. It takes human keypoint candidates as graph nodes and clusters keypoints in a multi-layer graph neural network model. The modules of HGG can be trained end-to-end with the keypoint detection network and is able to supervise the grouping process in a hierarchical manner. To improve the discrimination of the clustering, we add a set of edge discriminators and macro-node discriminators. Extensive experiments on both COCO and OCHuman datasets demonstrate that the proposed method improves the performance of bottom-up pose estimation methods.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_42');
INSERT INTO `paper` VALUES (11804, 'Differentiable Joint Pruning and Quantization for Hardware Efficiency', 'Joint optimization', 'Model compression', 'Mixed precision', 'Bit-restriction', 'Variational information bottleneck', 'We present a differentiable joint pruning and quantization (DJPQ) scheme. We frame neural network compression as a joint gradient-based optimization problem, trading off between model pruning and quantization automatically for hardware efficiency. DJPQ incorporates variational information bottleneck based structured pruning and mixed-bit precision quantization into a single differentiable loss function. In contrast to previous works which consider pruning and quantization separately, our method enables users to find the optimal trade-off between both in a single training procedure. To utilize the method for more efficient hardware inference, we extend DJPQ to integrate structured pruning with power-of-two bit-restricted quantization. We show that DJPQ significantly reduces the number of Bit-Operations (BOPs) for several networks while maintaining the top-1 accuracy of original floating-point models (e.g., 53\\(\\times \\) BOPs reduction in ResNet18 on ImageNet, 43\\(\\times \\) in MobileNetV2). Compared to the conventional two-stage approach, which optimizes pruning and quantization independently, our scheme outperforms in terms of both accuracy and BOPs. Even when considering bit-restricted quantization, DJPQ achieves larger compression ratios and better accuracy than the two-stage approach.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_16');
INSERT INTO `paper` VALUES (11805, 'Differentiable Programming for Hyperspectral Unmixing Using a Physics-Based Dispersion Model', 'Hyperspectral imaging', 'Spectral unmixing', 'Differentiable programming', '', '', 'Hyperspectral unmixing is an important remote sensing task with applications including material identification and analysis. Characteristic spectral features make many pure materials identifiable from their visible-to-infrared spectra, but quantifying their presence within a mixture is a challenging task due to nonlinearities and factors of variation. In this paper, spectral variation is considered from a physics-based approach and incorporated into an end-to-end spectral unmixing algorithm via differentiable programming. The dispersion model is introduced to simulate realistic spectral variation, and an efficient method to fit the parameters is presented. Then, this dispersion model is utilized as a generative model within an analysis-by-synthesis spectral unmixing algorithm. Further, a technique for inverse rendering using a convolutional neural network to predict parameters of the generative model is introduced to enhance performance and speed when training data is available. Results achieve state-of-the-art on both infrared and visible-to-near-infrared (VNIR) datasets, and show promise for the synergy between physics-based models and deep learning in hyperspectral unmixing in the future.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_39');
INSERT INTO `paper` VALUES (11806, 'Diffraction Line Imaging', 'Line sensor', 'Diffraction grating', '3D sensing', 'Motion capture', 'Computational imaging', 'We present a novel computational imaging principle that combines diffractive optics with line (1D) sensing. When light passes through a diffraction grating, it disperses as a function of wavelength. We exploit this principle to recover 2D and even 3D positions from only line images. We derive a detailed image formation model and a learning-based algorithm for 2D position estimation. We show several extensions of our system to improve the accuracy of the 2D positioning and expand the effective field of view. We demonstrate our approach in two applications: (a) fast passive imaging of sparse light sources like street lamps, headlights at night and LED-based motion capture, and (b) structured light 3D scanning with line illumination and line sensing. Line imaging has several advantages over 2D sensors: high frame rate, high dynamic range, high fill-factor with additional on-chip computation, low cost beyond the visible spectrum, and high energy efficiency when used with line illumination. Thus, our system is able to achieve high-speed and high-accuracy 2D positioning of light sources and 3D scanning of scenes.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_1');
INSERT INTO `paper` VALUES (11807, 'Directional Temporal Modeling for Action Recognition', 'Action recognition', 'Temporal modeling', 'Directional convolution', '', '', 'Many current activity recognition models use 3D convolutional neural networks (e.g. I3D, I3D-NL) to generate local spatial-temporal features. However, such features do not encode clip-level ordered temporal information. In this paper, we introduce a channel independent directional convolution (CIDC) operation, which learns to model the temporal evolution among local features. By applying multiple CIDC units we construct a light-weight network that models the clip-level temporal evolution across multiple spatial scales. Our CIDC network can be attached to any activity recognition backbone network. We evaluate our method on four popular activity recognition datasets and consistently improve upon state-of-the-art techniques. We further visualize the activation map of our CIDC network and show that it is able to focus on more meaningful, action related parts of the frame.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_17');
INSERT INTO `paper` VALUES (11808, 'Disambiguating Monocular Depth Estimation with a Single Transient', 'Depth estimation', 'Time-of-flight imaging', '', '', '', 'Monocular depth estimation algorithms successfully predict the relative depth order of objects in a scene. However, because of the fundamental scale ambiguity associated with monocular images, these algorithms fail at correctly predicting true metric depth. In this work, we demonstrate how a depth histogram of the scene, which can be readily captured using a single-pixel time-resolved detector, can be fused with the output of existing monocular depth estimation algorithms to resolve the depth ambiguity problem. We validate this novel sensor fusion technique experimentally and in extensive simulation. We show that it significantly improves the performance of several state-of-the-art monocular depth estimation algorithms.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_9');
INSERT INTO `paper` VALUES (11809, 'DisCont: Self-Supervised Visual Attribute Disentanglement Using Context Vectors', 'Self-supervision', 'Contrastive learning', 'Disentanglement', '', '', 'Disentangling underlying feature attributes within an image with no prior supervision is a challenging task. Models that can disentangle attributes well, provide greater interpretability and control. In this paper, we propose a self-supervised framework: DisCont to disentangle multiple attributes by exploiting structural inductive biases within images. Motivated by a recent surge in contrastive learning frameworks, our model bridges the gap between self-supervised contrastive learning algorithms and unsupervised disentanglement. We evaluate the efficacy of our approach, qualitatively and quantitatively, on four benchmark datasets. The code is available at https://github.com/sarthak268/DisCont.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_38');
INSERT INTO `paper` VALUES (11810, 'Discrete Point Flow Networks for Efficient Point Cloud Generation', 'Generative modeling', 'Normalizing flows', '3D shape modeling', 'Point cloud generation', 'Single view reconstruction', 'Generative models have proven effective at modeling 3D shapes and their statistical variations. In this paper we investigate their application to point clouds, a 3D shape representation widely used in computer vision for which, however, only few generative models have yet been proposed. We introduce a latent variable model that builds on normalizing flows with affine coupling layers to generate 3D point clouds of an arbitrary size given a latent shape representation. To evaluate its benefits for shape modeling we apply this model for generation, autoencoding, and single-view shape reconstruction tasks. We improve over recent GAN-based models in terms of most metrics that assess generation and autoencoding. Compared to recent work based on continuous flows, our model offers a significant speedup in both training and inference times for similar or better performance. For single-view shape reconstruction we also obtain results on par with state-of-the-art voxel, point cloud, and mesh-based methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_41');
INSERT INTO `paper` VALUES (11811, 'Discriminability Distillation in Group Representation Learning', 'Group representation learning', 'Set-to-set matching', '', '', '', 'Learning group representation is a commonly concerned issue in tasks where the basic unit is a group, set, or sequence. Previously, the research community tries to tackle it by aggregating the elements in a group based on an indicator either defined by humans such as the quality and saliency, or generated by a black box such as the attention score. This article provides a more essential and explicable view. We claim the most significant indicator to show whether the group representation can be benefited from one of its element is not the quality or an inexplicable score, but the discriminability w.r.t. the model. We explicitly design the discrimiability using embedded class centroids on a proxy set. We show the discrimiability knowledge has good properties that can be distilled by a light-weight distillation network and can be generalized on the unseen target set. The whole procedure is denoted as discriminability distillation learning (DDL). The proposed DDL can be flexibly plugged into many group-based recognition tasks without influencing the original training procedures. Comprehensive experiments on various tasks have proven the effectiveness of DDL for both accuracy and efficiency. Moreover, it pushes forward the state-of-the-art results on these tasks by an impressive margin.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_1');
INSERT INTO `paper` VALUES (11812, 'Discriminative Partial Domain Adversarial Network', 'Partial domain adaptation', 'Adversarial learning', 'Discriminative learning', '', '', 'Domain adaptation (DA) has been a fundamental building block for Transfer Learning (TL) which assumes that source and target domain share the same label space. A more general and realistic setting is that the label space of target domain is a subset of the source domain, as termed by Partial domain adaptation (PDA). Previous methods typically match the whole source domain to target domain, which causes negative transfer due to the source-negative classes in source domain that does not exist in target domain. In this paper, a novel Discriminative Partial Domain Adversarial Network (DPDAN) is developed. We first propose to use hard binary weighting to differentiate the source-positive and source-negative samples in the source domain. The source-positive samples are those with labels shared by two domains, while the rest in the source domain are treated as source-negative samples. Based on the above binary relabeling strategy, our algorithm maximizes the distribution divergence between source-negative samples and all the others (source-positive and target samples), meanwhile minimizes domain shift between source-positive samples and target domain to obtain discriminative domain-invariant features. We empirically verify DPDAN can effectively reduce the negative transfer caused by source-negative classes, and also theoretically show it decreases negative transfer caused by domain shift. Experiments on four benchmark domain adaptation datasets show DPDAN consistently outperforms state-of-the-art methods.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_38');
INSERT INTO `paper` VALUES (11813, 'Disentangled Image Generation for Unsupervised Domain Adaptation', '', '', '', '', '', 'We explore the use of generative modeling in unsupervised domain adaptation (UDA), where annotated real images are only available in the source domain, and pseudo images are generated in a manner that allows independent control of class (content) and nuisance variability (style). The proposed method differs from existing generative UDA models in that we explicitly disentangle the content and nuisance features at different layers of the generator network. We demonstrate the effectiveness of (pseudo)-conditional generation by showing that it improves upon baseline methods. Moreover, we outperform the previous state-of-the-art with significant margins in recently introduced multi-source domain adaptation (MSDA) tasks, achieving significant error reduction rates of \\(50.27 \\%\\), \\(89.54 \\%\\), \\(75.35 \\%\\), \\(27.46 \\%\\) and \\(94.3 \\%\\) in all 5 tasks.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_44');
INSERT INTO `paper` VALUES (11814, 'Disentangled Non-local Neural Networks', '', '', '', '', '', 'The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of every pixel. We also observe that the two terms trained alone tend to model different visual clues, e.g. the whitened pairwise term learns within-region relationships while the unary term learns salient boundaries. However, the two terms are tightly coupled in the non-local block, which hinders the learning of each. Based on these findings, we present the disentangled non-local block, where the two terms are decoupled to facilitate learning for both terms. We demonstrate the effectiveness of the decoupled design on various tasks, such as semantic segmentation on Cityscapes, ADE20K and PASCAL Context, object detection on COCO, and action recognition on Kinetics. Code is available at https://github.com/yinmh17/DNL-Semantic-Segmentation and https://github.com/Howal/DNL-Object-Detection', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_12');
INSERT INTO `paper` VALUES (11815, 'Disentangling Multiple Features in Video Sequences Using Gaussian Processes in Variational Autoencoders', '', '', '', '', '', 'We introduce MGP-VAE (Multi-disentangled-features Gaussian Processes Variational AutoEncoder), a variational autoencoder which uses Gaussian processes (GP) to model the latent space for the unsupervised learning of disentangled representations in video sequences. We improve upon previous work by establishing a framework by which multiple features, static or dynamic, can be disentangled. Specifically we use fractional Brownian motions (fBM) and Brownian bridges (BB) to enforce an inter-frame correlation structure in each independent channel, and show that varying this structure enables one to capture different factors of variation in the data. We demonstrate the quality of our representations with experiments on three publicly available datasets, and also quantify the improvement using a video prediction task. Moreover, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold to improve learning. Our experiments show that the combination of the improved representations with the novel loss function enable MGP-VAE to outperform the baselines in video prediction.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_7');
INSERT INTO `paper` VALUES (11816, 'Disguised Face Verification Using Inverse Disguise Quality', 'Disguised face verification', 'Biometric fusion', '', '', '', 'Research in face recognition has evolved over the past few decades. With initial research focusing heavily on constrained images, recent research has focused more on unconstrained images captured in-the-wild settings. Faces captured in unconstrained settings with disguise accessories persist to be a challenge for automated face verification. To this effect, this research proposes a novel deep learning framework for disguised face verification. A novel Inverse Disguise Quality metric is proposed for evaluating amount of disguise in the input image, which is utilized in likelihood ratio as a quality score for enhanced verification performance. The proposed framework is model-agnostic and can be applied in conjunction with existing state-of-the-art face verification models for obtaining improved performance. Experiments have been performed on the Disguised Faces in Wild (DFW) 2018 and DFW 2019 datasets, with three state-of-the-art deep learning models, where it demonstrates substantial improvement compared to the base model.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_36');
INSERT INTO `paper` VALUES (11817, 'Disrupting Deepfakes: Adversarial Attacks Against Conditional Image Translation Networks and Facial Manipulation Systems', 'Adversarial attacks', 'Image translation', 'Face modification', 'Deepfake', 'Generative models', 'Face modification systems using deep learning have become increasingly powerful and accessible. Given images of a person’s face, such systems can generate new images of that same person under different expressions and poses. Some systems can also modify targeted attributes such as hair color or age. This type of manipulated images and video have been coined Deepfakes. In order to prevent a malicious user from generating modified images of a person without their consent we tackle the new problem of generating adversarial attacks against such image translation systems, which disrupt the resulting output image. We call this problem disrupting deepfakes. Most image translation architectures are generative models conditioned on an attribute (e.g. put a smile on this person’s face). We are first to propose and successfully apply (1) class transferable adversarial attacks that generalize to different classes, which means that the attacker does not need to have knowledge about the conditioning class, and (2) adversarial training for generative adversarial networks (GANs) as a first step towards robust image translation networks. Finally, in our scenario, the deepfaker can adaptively blur the image and potentially mount a successful defense against disruption. We present a spread-spectrum adversarial attack, which evades blur defenses. We open-source our code.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_14');
INSERT INTO `paper` VALUES (11818, 'Distance-Normalized Unified Representation for Monocular 3D Object Detection', 'Monocular 3D object detection', 'Unified representation across different distance ranges', 'Distance-guided NMS', 'Fully convolutional cascaded point regression', '', 'Monocular 3D object detection plays an important role in autonomous driving and still remains challenging. To achieve fast and accurate monocular 3D object detection, we introduce a single-stage and multi-scale framework to learn a unified representation for objects within different distance ranges, termed as UR3D. UR3D formulates different tasks of detection by exploiting the scale information, to reduce model capacity requirement and achieve accurate monocular 3D object detection. Besides, distance estimation is enhanced by a distance-guided NMS, which automatically selects candidate boxes with better distance estimates. In addition, an efficient fully convolutional cascaded point regression method is proposed to infer accurate locations of the projected 2D corners and centers of 3D boxes, which can be used to recover object physical size and orientation by a projection-consistency loss. Experimental results on the challenging KITTI autonomous driving dataset show that UR3D achieves accurate monocular 3D object detection with a compact architecture.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_6');
INSERT INTO `paper` VALUES (11819, 'Distilling Visual Priors from Self-Supervised Learning', 'Self-supervised learning', 'Knowledge-distillation', '', '', '', 'Convolutional Neural Networks (CNNs) are prone to overfit small training datasets. We present a novel two-phase pipeline that leverages self-supervised learning and knowledge distillation to improve the generalization ability of CNN models for image classification under the data-deficient setting. The first phase is to learn a teacher model which possesses rich and generalizable visual representations via self-supervised learning, and the second phase is to distill the representations into a student model in a self-distillation manner, and meanwhile fine-tune the student model for the image classification task. We also propose a novel margin loss for the self-supervised contrastive learning proxy task to better learn the representation under the data-deficient scenario. Together with other tricks, we achieve competitive performance in the VIPriors image classification challenge.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_29');
INSERT INTO `paper` VALUES (11820, 'Distribution-Balanced Loss for Multi-label Classification in Long-Tailed Datasets', 'Multi-label classification', 'Long-tailed data', 'Distribution-balanced loss', '', '', 'We present a new loss function called Distribution-Balanced Loss for the multi-label recognition problems that exhibit long-tailed class distributions. Compared to conventional single-label classification problem, multi-label recognition problems are often more challenging due to two significant issues, namely the co-occurrence of labels and the dominance of negative labels (when treated as multiple binary classification problems). The Distribution-Balanced Loss tackles these issues through two key modifications to the standard binary cross-entropy loss: 1) a new way to re-balance the weights that takes into account the impact caused by label co-occurrence, and 2) a negative tolerant regularization to mitigate the over-suppression of negative labels. Experiments on both Pascal VOC and COCO show that the models trained with this new loss function achieve significant performance gains over existing methods. Code and models are available at: https://github.com/wutong16/DistributionBalancedLoss.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_10');
INSERT INTO `paper` VALUES (11821, 'DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning', 'Deep metric learning', 'Generalization', 'Self-supervision', '', '', 'Visual similarity plays an important role in many computer vision applications. Deep metric learning (DML) is a powerful framework for learning such similarities which not only generalize from training data to identically distributed test distributions, but in particular also translate to unknown test classes. However, its prevailing learning paradigm is class-discriminative supervised training, which typically results in representations specialized in separating training classes. For effective generalization, however, such an image representation needs to capture a diverse range of data characteristics. To this end, we propose and study multiple complementary learning tasks, targeting conceptually different data relationships by only resorting to the available training samples and labels of a standard DML setting. Through simultaneous optimization of our tasks we learn a single model to aggregate their training signals, resulting in strong generalization and state-of-the-art performance on multiple established DML benchmark datasets.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_35');
INSERT INTO `paper` VALUES (11822, 'Dive Deeper into Box for Object Detection', '', '', '', '', '', 'Anchor free methods have defined the new frontier in state-of-the-art object detection researches where accurate bounding box estimation is the key to the success of these methods. However, even the bounding box has the highest confidence score, it is still far from perfect at localization. To this end, we propose a box reorganization method (DDBNet), which can dive deeper into the box for more accurate localization. At the first step, drifted boxes are filtered out because the contents in these boxes are inconsistent with target semantics. Next, the selected boxes are broken into boundaries, and the well-aligned boundaries are searched and grouped into a sort of optimal boxes toward tightening instances more precisely. Experimental results show that our method is effective which leads to state-of-the-art performance for object detection.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_25');
INSERT INTO `paper` VALUES (11823, 'Diverse and Admissible Trajectory Forecasting Through Multimodal Context Understanding', 'Trajectory forecasting', 'Diversity', 'Admissibility', 'Generative modeling', 'Autonomous driving', 'Multi-agent trajectory forecasting in autonomous driving requires an agent to accurately anticipate the behaviors of the surrounding vehicles and pedestrians, for safe and reliable decision-making. Due to partial observability in these dynamical scenes, directly obtaining the posterior distribution over future agent trajectories remains a challenging problem. In realistic embodied environments, each agent’s future trajectories should be both diverse since multiple plausible sequences of actions can be used to reach its intended goals, and admissible since they must obey physical constraints and stay in drivable areas. In this paper, we propose a model that synthesizes multiple input signals from the multimodal world|the environment’s scene context and interactions between multiple surrounding agents|to best model all diverse and admissible trajectories. We compare our model with strong baselines and ablations across two public datasets and show a significant performance improvement over previous state-of-the-art methods. Lastly, we offer new metrics incorporating admissibility criteria to further study and evaluate the diversity of predictions. Codes are at: https://github.com/kami93/CMU-DATF.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_17');
INSERT INTO `paper` VALUES (11824, 'Diversified Mutual Learning for Deep Metric Learning', 'Model diversification', 'Mutual learning', 'Deep metric learning', 'Inductive transfer learning', '', 'Mutual learning is an ensemble training strategy to improve generalization by transferring individual knowledge to each other while simultaneously training multiple models. In this work, we propose an effective mutual learning method for deep metric learning, called Diversified Mutual Metric Learning, which enhances embedding models with diversified mutual learning. We transfer relational knowledge for deep metric learning by leveraging three kinds of diversities in mutual learning: (1) model diversity from different initializations of models (2) temporal diversity from different frequencies of parameter update, and (3) view diversity from different augmentations of inputs. Our method is particularly adequate for inductive transfer learning at the lack of large-scale data, where the embedding model is initialized with a pretrained model and then fine-tuned on a target dataset. Extensive experiments show that our method significantly improves individual models as well as their ensemble. Finally, the proposed method with a conventional triplet loss achieves the state-of-the-art performance of Recall@1 on standard datasets: 69.9 on CUB-200–2011 and 89.1 on CARS-196.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_49');
INSERT INTO `paper` VALUES (11825, 'DLow: Diversifying Latent Flows for Diverse Human Motion Prediction', 'Generative models', 'Diversity', 'Human motion forecasting', '', '', 'Deep generative models are often used for human motion prediction as they are able to model multi-modal data distributions and characterize diverse human behavior. While much care has been taken into designing and learning deep generative models, how to efficiently produce diverse samples from a deep generative model after it has been trained is still an under-explored problem. To obtain samples from a pretrained generative model, most existing generative human motion prediction methods draw a set of independent Gaussian latent codes and convert them to motion samples. Clearly, this random sampling strategy is not guaranteed to produce diverse samples for two reasons: (1) The independent sampling cannot force the samples to be diverse; (2) The sampling is based solely on likelihood which may only produce samples that correspond to the major modes of the data distribution. To address these problems, we propose a novel sampling method, Diversifying Latent Flows (DLow), to produce a diverse set of samples from a pretrained deep generative model. Unlike random (independent) sampling, the proposed DLow sampling method samples a single random variable and then maps it with a set of learnable mapping functions to a set of correlated latent codes. The correlated latent codes are then decoded into a set of correlated samples. During training, DLow uses a diversity-promoting prior over samples as an objective to optimize the latent mappings to improve sample diversity. The design of the prior is highly flexible and can be customized to generate diverse motions with common features (e.g., similar leg motion but diverse upper-body motion). Our experiments demonstrate that DLow outperforms state-of-the-art baseline methods in terms of sample diversity and accuracy (Code: https://github.com/Khrylx/DLow. Video: https://youtu.be/64OEdSadb00).', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_20');
INSERT INTO `paper` VALUES (11826, 'DMD: A Large-Scale Multi-modal Driver Monitoring Dataset for Attention and Alertness Analysis', 'Driver Monitoring Dataset', 'Driver actions', 'Driver behaviour recognition', 'Driver state analysis', 'Multi-modal fusion', 'Vision is the richest and most cost-effective technology for Driver Monitoring Systems (DMS), especially after the recent success of Deep Learning (DL) methods. The lack of sufficiently large and comprehensive datasets is currently a bottleneck for the progress of DMS development, crucial for the transition of automated driving from SAE Level-2 to SAE Level-3. In this paper, we introduce the Driver Monitoring Dataset (DMD), an extensive dataset which includes real and simulated driving scenarios: distraction, gaze allocation, drowsiness, hands-wheel interaction and context data, in 41 h of RGB, depth and IR videos from 3 cameras capturing face, body and hands of 37 drivers. A comparison with existing similar datasets is included, which shows the DMD is more extensive, diverse, and multi-purpose. The usage of the DMD is illustrated by extracting a subset of it, the dBehaviourMD dataset, containing 13 distraction activities, prepared to be used in DL training processes. Furthermore, we propose a robust and real-time driver behaviour recognition system targeting a real-world application that can run on cost-efficient CPU-only platforms, based on the dBehaviourMD. Its performance is evaluated with different types of fusion strategies, which all reach enhanced accuracy still providing real-time response.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_23');
INSERT INTO `paper` VALUES (11827, 'Do Not Disturb Me: Person Re-identification Under the Interference of Other Pedestrians', 'Person re-identification', 'Pedestrian-Interference', 'Location accuracy', 'Feature distinctiveness', 'Query-guided attention', 'In the conventional person Re-ID setting, it is assumed that cropped images are the person images within the bounding box for each individual. However, in a crowded scene, off-shelf-detectors may generate bounding boxes involving multiple people, where the large proportion of background pedestrians or human occlusion exists. The representation extracted from such cropped images, which contain both the target and the interference pedestrians, might include distractive information. This will lead to wrong retrieval results. To address this problem, this paper presents a novel deep network termed Pedestrian-Interference Suppression Network (PISNet). PISNet leverages a Query-Guided Attention Block (QGAB) to enhance the feature of the target in the gallery, under the guidance of the query. Furthermore, the involving Guidance Reversed Attention Module and the Multi-Person Separation Loss promote QGAB to suppress the interference of other pedestrians. Our method is evaluated on two new pedestrian-interference datasets and the results show that the proposed method performs favorably against existing Re-ID methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_39');
INSERT INTO `paper` VALUES (11828, 'Do Not Mask What You Do Not Need to Mask: A Parser-Free Virtual Try-On', 'Virtual try-on', 'Teacher-student', 'Model distillation', '', '', 'The 2D virtual try-on task has recently attracted a great interest from the research community, for its direct potential applications in online shopping as well as for its inherent and non-addressed scientific challenges. This task requires fitting an in-shop cloth image on the image of a person, which is highly challenging because it involves cloth warping, image compositing, and synthesizing. Casting virtual try-on into a supervised task faces a difficulty: available datasets are composed of pairs of pictures (cloth, person wearing the cloth). Thus, we have no access to ground-truth when the cloth on the person changes. State-of-the-art models solve this by masking the cloth information on the person with both a human parser and a pose estimator. Then, image synthesis modules are trained to reconstruct the person image from the masked person image and the cloth image. This procedure has several caveats: firstly, human parsers are prone to errors; secondly, it is a costly pre-processing step, which also has to be applied at inference time; finally, it makes the task harder than it is since the mask covers information that should be kept such as hands or accessories. In this paper, we propose a novel student-teacher paradigm where the teacher is trained in the standard way (reconstruction) before guiding the student to focus on the initial task (changing the cloth). The student additionally learns from an adversarial loss, which pushes it to follow the distribution of the real images. Consequently, the student exploits information that is masked to the teacher. A student trained without the adversarial loss would not use this information. Also, getting rid of both human parser and pose estimator at inference time allows obtaining a real-time virtual try-on.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_37');
INSERT INTO `paper` VALUES (11829, 'Document Structure Extraction Using Prior Based High Resolution Hierarchical Semantic Segmentation', 'Documents structure extraction', 'Hierarchical semantic segmentation', 'High resolution semantic segmentation', '', '', 'Structure extraction from document images has been a long-standing research topic due to its high impact on a wide range of practical applications. In this paper, we share our findings on employing a hierarchical semantic segmentation network for this task of structure extraction. We propose a prior based deep hierarchical CNN network architecture that enables document structure extraction using very high resolution (\\(1800 \\times 1000\\)) images. We divide the document image into overlapping horizontal strips such that the network segments a strip and uses its prediction mask as prior for predicting the segmentation of the subsequent strip. We perform experiments establishing the effectiveness of our strip based network architecture through ablation methods and comparison with low-resolution variations. Further, to demonstrate our network’s capabilities, we train it on only one type of documents (Forms) and achieve state-of-the-art results over other general document datasets. We introduce our new human-annotated forms dataset and show that our method significantly outperforms different segmentation baselines on this dataset in extracting hierarchical structures. Our method is currently being used in Adobe’s AEM Forms for automated conversion of paper and PDF forms to modern HTML based forms.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_39');
INSERT INTO `paper` VALUES (11830, 'Domain Adaptation for Eye Segmentation', 'Domain adaptation', 'Eye segmentation', '', '', '', 'Domain adaptation (DA) has been widely investigated as a framework to alleviate the laborious task of data annotation for image segmentation. Most DA investigations operate under the unsupervised domain adaptation (UDA) setting, where the modeler has access to a large cohort of source domain labeled data and target domain data with no annotations. UDA techniques exhibit poor performance when the domain gap, i.e., the distribution overlap between the data in source and target domain is large. We hypothesize that the DA performance gap can be improved with the availability of a small subset of labeled target domain data. In this paper, we systematically investigate the impact of varying amounts of labeled target domain data on the performance gap for DA. We specifically focus on the problem of segmenting eye-regions from eye images collected using two different head mounted display systems. Source domain is comprised of 12,759 eye images with annotations and target domain is comprised of 4,629 images with varying amounts of annotations. Experiments are performed to compare the impact on DA performance gap under three schemes: unsupervised (UDA), supervised (SDA) and semi-supervised (SSDA) domain adaptation. We evaluate these schemes by measuring the mean intersection-over-union (mIoU) metric. Using only 200 samples of labeled target data under SDA and SSDA schemes, we show an improvement in mIoU of 5.4% and 6.6% respectively, over mIoU of 81.7% under UDA. By using all available labeled target data, models trained under SSDA achieve a competitive mIoU score of 89.8%. Overall, we conclude that availability of a small subset of target domain data with annotations can substantially improve DA performance.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_36');
INSERT INTO `paper` VALUES (11831, 'Domain Adaptation Through Task Distillation', 'Domain adaptation', 'Autonomous driving', 'Sim-to-real', '', '', 'Deep networks devour millions of precisely annotated images to build their complex and powerful representations. Unfortunately, tasks like autonomous driving have virtually no real-world training data. Repeatedly crashing a car into a tree is simply too expensive. The commonly prescribed solution is simple: learn a representation in simulation and transfer it to the real world. However, this transfer is challenging since simulated and real-world visual experiences vary dramatically. Our core observation is that for certain tasks, such as image recognition, datasets are plentiful. They exist in any interesting domain, simulated or real, and are easy to label and extend. We use these recognition datasets to link up a source and target domain to transfer models between them in a task distillation framework. Our method can successfully transfer navigation policies between drastically different simulators: ViZDoom, SuperTuxKart, and CARLA. Furthermore, it shows promising results on standard domain adaptation benchmarks.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_40');
INSERT INTO `paper` VALUES (11832, 'Domain Adaptive Object Detection via Asymmetric Tri-Way Faster-RCNN', 'Object detection', 'Transfer learning', 'Deep learning', '', '', 'Conventional object detection models inevitably encounter a performance drop as the domain disparity exists. Unsupervised domain adaptive object detection is proposed recently to reduce the disparity between domains, where the source domain is label-rich while the target domain is label-agnostic. The existing models follow a parameter shared siamese structure for adversarial domain alignment, which, however, easily leads to the collapse and out-of-control risk of the source domain and brings negative impact to feature adaption. The main reason is that the labeling unfairness (asymmetry) between source and target makes the parameter sharing mechanism unable to adapt. Therefore, in order to avoid the source domain collapse risk caused by parameter sharing, we propose an asymmetric tri-way Faster-RCNN (ATF) for domain adaptive object detection. Our ATF model has two distinct merits: 1) A ancillary net supervised by source label is deployed to learn ancillary target features and simultaneously preserve the discrimination of source domain, which enhances the structural discrimination (object classification vs. bounding box regression) of domain alignment. 2) The asymmetric structure consisting of a chief net and an independent ancillary net essentially overcomes the parameter sharing aroused source risk collapse. The adaption safety of the proposed ATF detector is guaranteed. Extensive experiments on a number of datasets, including Cityscapes, Foggy-cityscapes, KITTI, Sim10k, Pascal VOC, Clipart and Watercolor, demonstrate the SOTA performance of our method.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_19');
INSERT INTO `paper` VALUES (11833, 'Domain Adaptive Semantic Segmentation Using Weak Labels', '', '', '', '', '', 'Learning semantic segmentation models requires a huge amount of pixel-wise labeling. However, labeled data may only be available abundantly in a domain different from the desired target domain, which only has minimal or no annotations. In this work, we propose a novel framework for domain adaptation in semantic segmentation with image-level weak labels in the target domain. The weak labels may be obtained based on a model prediction for unsupervised domain adaptation (UDA), or from a human annotator in a new weakly-supervised domain adaptation (WDA) paradigm for semantic segmentation. Using weak labels is both practical and useful, since (i) collecting image-level target annotations is comparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the opportunity for category-wise domain alignment. Our framework uses weak labels to enable the interplay between feature alignment and pseudo-labeling, improving both in the process of domain adaptation. Specifically, we develop a weak-label classification module to enforce the network to attend to certain categories, and then use such training signals to guide the proposed category-wise alignment method. In experiments, we show considerable improvements with respect to the existing state-of-the-arts in UDA and present a new benchmark in the WDA setting. Project page is at http://www.nec-labs.com/~mas/WeakSegDA.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_33');
INSERT INTO `paper` VALUES (11834, 'Domain Generalization Using Shape Representation', '', '', '', '', '', 'CNN-based representations have greatly advanced the state of the art in visual recognition, but the community has primarily focused on the setting where training and test set belong to the same dataset/distribution', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_45');
INSERT INTO `paper` VALUES (11835, 'Domain Generalization vs Data Augmentation: An Unbiased Perspective', 'Domain generalization', 'Data augmentation', 'Style transfer', '', '', 'In domain generalization the target domain is not known at training time. We show that a style transfer based data augmentation strategy can be implemented easily and outperforms the current state of the art domain generalization methods. Moreover, we observe that those methods, even if combined with the described data augmentation, do not take advantage of it, indicating the need of new generalization solutions.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_50');
INSERT INTO `paper` VALUES (11836, 'Domain-Invariant Stereo Matching Networks', '', '', '', '', '', 'State-of-the-art stereo matching networks have difficulties in generalizing to new unseen environments due to significant domain differences, such as color, illumination, contrast, and texture. In this paper, we aim at designing a domain-invariant stereo matching network (DSMNet) that generalizes well to unseen scenes. To achieve this goal, we propose i) a novel “domain normalization” approach that regularizes the distribution of learned representations to allow them to be invariant to domain differences, and ii) an end-to-end trainable structure-preserving graph-based filter for extracting robust structural and geometric representations that can further enhance domain-invariant generalizations. When trained on synthetic data and generalized to real test sets, our model performs significantly better than all state-of-the-art models. It even outperforms some deep neural network models (e.g. MC-CNN [61]) fine-tuned with test-domain data. The code is available at https://github.com/feihuzhang/DSMNet.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_25');
INSERT INTO `paper` VALUES (11837, 'Domain-Specific Mappings for Generative Adversarial Style Transfer', '', '', '', '', '', 'Style transfer generates an image whose content comes from one image and style from the other. Image-to-image translation approaches with disentangled representations have been shown effective for style transfer between two image categories. However, previous methods often assume a shared domain-invariant content space, which could compromise the content representation power. For addressing this issue, this paper leverages domain-specific mappings for remapping latent features in the shared content space to domain-specific content spaces. This way, images can be encoded more properly for style transfer. Experiments show that the proposed method outperforms previous style transfer methods, particularly on challenging scenarios that would require semantic correspondences between images. Code and results are available at https://github.com/acht7111020/DSMAP.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_34');
INSERT INTO `paper` VALUES (11838, 'Domain2Vec: Domain Embedding for Unsupervised Domain Adaptation', 'Unsupervised domain adaptation', 'Domain vectorization', '', '', '', 'Conventional unsupervised domain adaptation (UDA) studies the knowledge transfer between a limited number of domains. This neglects the more practical scenario where data are distributed in numerous different domains in the real world. A technique to measure domain similarity is critical for domain adaptation performance. To describe and learn relations between different domains, we propose a novel Domain2Vec model to provide vectorial representations of visual domains based on joint learning of feature disentanglement and Gram matrix. To evaluate the effectiveness of our Domain2Vec model, we create two large-scale cross-domain benchmarks. The first one is TinyDA, which contains 54 domains and about one million MNIST-style images. The second benchmark is DomainBank, which is collected from 56 existing vision datasets. We demonstrate that our embedding is capable of predicting domain similarities that match our intuition about visual relations between different domains. Extensive experiments are conducted to demonstrate the power of our new datasets in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model (Data and code are available at https://github.com/VisionLearningGroup/Domain2Vec).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_45');
INSERT INTO `paper` VALUES (11839, 'DOPE: Distillation of Part Experts for Whole-Body 3D Pose Estimation in the Wild', 'Human pose estimation', 'Human pose detection', '3D pose estimation', '2D pose estimation', 'Body pose estimation', 'We introduce DOPE, the first method to detect and estimate whole-body 3D human poses, including bodies, hands and faces, in the wild. Achieving this level of details is key for a number of applications that require understanding the interactions of the people with each other or with the environment. The main challenge is the lack of in-the-wild data with labeled whole-body 3D poses. In previous work, training data has been annotated or generated for simpler tasks focusing on bodies, hands or faces separately. In this work, we propose to take advantage of these datasets to train independent experts for each part, namely a body, a hand and a face expert, and distill their knowledge into a single deep network designed for whole-body 2D-3D pose detection. In practice, given a training image with partial or no annotation, each part expert detects its subset of keypoints in 2D and 3D and the resulting estimations are combined to obtain whole-body pseudo ground-truth poses. A distillation loss encourages the whole-body predictions to mimic the experts’ outputs. Our results show that this approach significantly outperforms the same whole-body model trained without distillation while staying close to the performance of the experts. Importantly, DOPE is computationally less demanding than the ensemble of experts and can achieve real-time performance. Test code and models are available at https://europe.naverlabs.com/research/computer-vision/dope.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_23');
INSERT INTO `paper` VALUES (11840, 'DoubleU-Net: Colorectal Cancer Diagnosis and Gland Instance Segmentation with Text-Guided Feature Control', 'Cancer diagnosis', 'Gland segmentation', 'Multi-domain learning', 'Morphological feature guidance', '', 'With the rapid therapeutic advancement in personalized medicine, the role of pathologists for colorectal cancer has greatly expanded from morphologists to clinical consultants. In addition to cancer diagnosis, pathologists are responsible for multiple assessments based on glandular morphology statistics, like selecting appropriate tissue sections for mutation analysis [6]. Therefore, we propose DoubleU-Net that determines the initial gland segmentation and diagnoses the histologic grades simultaneously, and then incorporates the diagnosis text data to produce more accurate final segmentation. Our DoubleU-Net shows three advantages: (1) Besides the initial segmentation, it offers histologic grade diagnosis and enhanced segmentation for full-scale assistance. (2) The textual features extracted from diagnosis data provide high-level guidance related to gland morphology, and boost the performance of challenging cases with seriously deformed glands. (3) It can be extended to segmentation tasks with text data like key clinical phrases or pathology descriptions. The model is evaluated on two public colon gland datasets and achieves state-of-the-art performance.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_22');
INSERT INTO `paper` VALUES (11841, 'DPDist: Comparing Point Clouds Using Deep Point Cloud Distance', '3D point clouds', '3D computer vision', '3D deep learning', 'Distance', 'Registration', 'We introduce a new deep learning method for point cloud comparison. Our approach, named Deep Point Cloud Distance (DPDist), measures the distance between the points in one cloud and the estimated surface from which the other point cloud is sampled. The surface is estimated locally using the 3D modified Fisher vector representation. The local representation reduces the complexity of the surface, enabling effective learning, which generalizes well between object categories. We test the proposed distance in challenging tasks, such as similar object comparison and registration, and show that it provides significant improvements over commonly used distances such as Chamfer distance, Earth mover’s distance, and others.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_32');
INSERT INTO `paper` VALUES (11842, 'DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape Reconstruction', '3D Reconstruction', 'Visual similarity metric', 'Differentiablity', '', '', 'We introduce a differential visual similarity metric to train deep neural networks for 3D reconstruction, aimed at improving reconstruction quality. The metric compares two 3D shapes by measuring distances between multi-view images differentiably rendered from the shapes. Importantly, the image-space distance is also differentiable and measures visual similarity, rather than pixel-wise distortion. Specifically, the similarity is defined by mean-squared errors over HardNet features computed from probabilistic keypoint maps of the compared images. Our differential visual shape similarity metric can be easily plugged into various 3D reconstruction networks, replacing their distortion-based losses, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstructions with better structural fidelity and visual quality. We demonstrate this both objectively, using well-known shape metrics for retrieval and classification tasks that are independent from our new metric, and subjectively through a perceptual study.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_18');
INSERT INTO `paper` VALUES (11843, 'DRG: Dual Relation Graph for Human-Object Interaction Detection', '', '', '', '', '', 'We tackle the challenging problem of human-object interaction (HOI) detection. Existing methods either recognize the interaction of each human-object pair in isolation or perform joint inference based on complex appearance-based features. In this paper, we leverage an abstract spatial-semantic representation to describe each human-object pair and aggregate the contextual information of the scene via a dual relation graph (one human-centric and one object-centric). Our proposed dual relation graph effectively captures discriminative cues from the scene to resolve ambiguity from local predictions. Our model is conceptually simple and leads to favorable results compared to the state-of-the-art HOI detection algorithms on two large-scale benchmark datasets.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_41');
INSERT INTO `paper` VALUES (11844, 'DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose Estimation via a Smooth Silhouette Loss', '3D pose estimation', 'Dataset generation', 'UAV', 'Differentiable rendering', '', 'In this work we consider UAVs as cooperative agents supporting human users in their operations. In this context, the 3D localisation of the UAV assistant is an important task that can facilitate the exchange of spatial information between the user and the UAV. To address this in a data-driven manner, we design a data synthesis pipeline to create a realistic multimodal dataset that includes both the exocentric user view, and the egocentric UAV view. We then exploit the joint availability of photorealistic and synthesized inputs to train a single-shot monocular pose estimation model. During training we leverage differentiable rendering to supplement a state-of-the-art direct regression objective with a novel smooth silhouette loss. Our results demonstrate its qualitative and quantitative performance gains over traditional silhouette objectives. Our data and code are available at https://vcl3d.github.io/DronePose.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_44');
INSERT INTO `paper` VALUES (11845, 'DSA: More Efficient Budgeted Pruning via Differentiable Sparsity Allocation', 'Budgeted pruning', 'Structured pruning', 'Model compression', '', '', 'Budgeted pruning is the problem of pruning under resource constraints. In budgeted pruning, how to distribute the resources across layers (i.e., sparsity allocation) is the key problem. Traditional methods solve it by discretely searching for the layer-wise pruning ratios, which lacks efficiency. In this paper, we propose Differentiable Sparsity Allocation (DSA), an efficient end-to-end budgeted pruning flow. Utilizing a novel differentiable pruning process, DSA finds the layer-wise pruning ratios with gradient-based optimization. It allocates sparsity in continuous space, which is more efficient than methods based on discrete evaluation and search. Furthermore, DSA could work in a pruning-from-scratch manner, whereas traditional budgeted pruning methods are applied to pre-trained models. Experimental results on CIFAR-10 and ImageNet show that DSA could achieve superior performance than current iterative budgeted pruning methods, and shorten the time cost of the overall pruning process by at least 1.5\\(\\times \\) in the meantime.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_35');
INSERT INTO `paper` VALUES (11846, 'DSDNet: Deep Structured Self-driving Network', 'Autonomous driving', 'Motion prediction', 'Motion planning', '', '', 'In this paper, we propose the Deep Structured self-Driving Network (DSDNet), which performs object detection, motion prediction, and motion planning with a single neural network. Towards this goal, we develop a deep structured energy based model which considers the interactions between actors and produces socially consistent multimodal future predictions. Furthermore, DSDNet explicitly exploits the predicted future distributions of actors to plan a safe maneuver by using a structured planning cost. Our sample-based formulation allows us to overcome the difficulty in probabilistic inference of continuous random variables. Experiments on a number of large-scale self driving datasets demonstrate that our model significantly outperforms the state-of-the-art.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_10');
INSERT INTO `paper` VALUES (11847, 'DTVNet: Dynamic Time-Lapse Video Generation via Single Still Image', 'Generative adversarial network', 'Optical flow encoding', 'Time-Lapse video generation', '', '', 'This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image, which are conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: Optical Flow Encoder (OFE) and Dynamic Video Generator (DVG). The OFE maps a sequence of optical flow maps to a normalized motion vector that encodes the motion information inside the generated video. The DVG contains motion and content streams that learn from the motion vector and the single image respectively, as well as an encoder and a decoder to learn shared content features and construct video frames with corresponding motion respectively. Specifically, the motion stream introduces multiple adaptive instance normalization (AdaIN) layers to integrate multi-level motion information that are processed by linear layers. In the testing stage, videos with the same content but various motion information can be generated by different normalized motion vectors based on only one input image. We further conduct experiments on Sky Time-lapse dataset, and the results demonstrate the superiority of our approach over the state-of-the-art methods for generating high-quality and dynamic videos, as well as the variety for generating videos with various motion information (https://github.com/zhangzjn/DTVNet).', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_18');
INSERT INTO `paper` VALUES (11848, 'Du2Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels', 'Dual-pixels', 'Stereo matching', 'Depth estimation', 'Computational photography', '', 'Computational stereo has reached a high level of accuracy, but degrades in the presence of occlusions, repeated textures, and correspondence errors along edges. We present a novel approach based on neural networks for depth estimation that combines stereo from dual cameras with stereo from a dual-pixel sensor, which is increasingly common on consumer cameras. Our network uses a novel architecture to fuse these two sources of information and can overcome the above-mentioned limitations of pure binocular stereo matching. Our method provides a dense depth map with sharp edges, which is crucial for computational photography applications like synthetic shallow-depth-of-field or 3D Photos. Additionally, we avoid the inherent ambiguity due to the aperture problem in stereo cameras by designing the stereo baseline to be orthogonal to the dual-pixel baseline. We present experiments and comparisons with state-of-the-art approaches to show that our method offers a substantial improvement over previous works.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_34');
INSERT INTO `paper` VALUES (11849, 'Dual Adversarial Network for Deep Active Learning', 'Active learning', 'Generative adversarial network', 'Unsupervised video summarization', 'Deep learning', '', 'Active learning, reducing the cost and workload of annotations, attracts increasing attentions from the community. Current active learning approaches commonly adopted uncertainty-based acquisition functions for the data selection due to their effectiveness. However, data selection based on uncertainty suffers from the overlapping problem, i.e., the top-K samples ranked by the uncertainty are similar. In this paper, we investigate the overlapping problem of recent uncertainty-based approaches and propose to alleviate the issue by taking representativeness into consideration. In particular, we propose a dual adversarial network, namely DAAL, for this purpose. Different from previous hybrid active learning methods requiring multi-stage data selections i.e., step-by-step evaluating the uncertainty and representativeness using different acquisition functions, our DAAL learns to select the most uncertain and representative data points in one-stage. Extensive experiments conducted on three publicly available datasets, i.e., CIFAR10/100 and Cityscapes, demonstrate the effectiveness of our method—a new state-of-the-art accuracy is achieved.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_40');
INSERT INTO `paper` VALUES (11850, 'Dual Adversarial Network: Toward Real-World Noise Removal and Noise Generation', 'Real-world', 'Denoising', 'Generation', 'Metric', '', 'Real-world image noise removal is a long-standing yet very challenging task in computer vision. The success of deep neural network in denoising stimulates the research of noise generation, aiming at synthesizing more clean-noisy image pairs to facilitate the training of deep denoisers. In this work, we propose a novel unified framework to simultaneously deal with the noise removal and noise generation tasks. Instead of only inferring the posteriori distribution of the latent clean image conditioned on the observed noisy image in traditional MAP framework, our proposed method learns the joint distribution of the clean-noisy image pairs. Specifically, we approximate the joint distribution with two different factorized forms, which can be formulated as a denoiser mapping the noisy image to the clean one and a generator mapping the clean image to the noisy one. The learned joint distribution implicitly contains all the information between the noisy and clean images, avoiding the necessity of manually designing the image priors and noise assumptions as traditional. Besides, the performance of our denoiser can be further improved by augmenting the original training dataset with the learned generator. Moreover, we propose two metrics to assess the quality of the generated noisy image, for which, to the best of our knowledge, such metrics are firstly proposed along this research line. Extensive experiments have been conducted to demonstrate the superiority of our method over the state-of-the-arts both in the real noise removal and generation tasks. The training and testing code is available at https://github.com/zsyOAOA/DANet.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_3');
INSERT INTO `paper` VALUES (11851, 'Dual Grid Net: Hand Mesh Vertex Regression from Single Depth Maps', '', '', '', '', '', 'We aim to recover the dense 3D surface of the hand from depth maps and propose a network that can predict mesh vertices, transformation matrices for every joint and joint coordinates in a single forward pass. Use fully convolutional architectures, we first map depth image features to the mesh grid and then regress the mesh coordinates into real world 3D coordinates. The final mesh is found by sampling from the mesh grid refit in closed-form based on an articulated template mesh. When trained with supervision from sparse key-points, our accuracy is comparable with state-of-the-art on the NYU dataset for key point localization, all while recovering mesh vertices and dense correspondences. Under multi-view settings for training, our framework can also learn through self-supervision by minimizing a set of data-fitting terms and kinematic priors. Our approach is competitive with strongly supervised methods and showcases the potential for self-supervision in dense mesh estimation.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_27');
INSERT INTO `paper` VALUES (11852, 'Dual Mixup Regularized Learning for Adversarial Domain Adaptation', 'Domain adaptation', 'Mixup', 'Regularization', '', '', 'Recent advances on unsupervised domain adaptation (UDA) rely on adversarial learning to disentangle the explanatory and transferable features for domain adaptation. However, there are two issues with the existing methods. First, the discriminability of the latent space cannot be fully guaranteed without considering the class-aware information in the target domain. Second, samples from the source and target domains alone are not sufficient for domain-invariant feature extracting in the latent space. In order to alleviate the above issues, we propose a dual mixup regularized learning (DMRL) method for UDA, which not only guides the classifier in enhancing consistent predictions in-between samples, but also enriches the intrinsic structures of the latent space. The DMRL jointly conducts category and domain mixup regularizations on pixel level to improve the effectiveness of models. A series of empirical studies on four domain adaptation benchmarks demonstrate that our approach can achieve the state-of-the-art.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_32');
INSERT INTO `paper` VALUES (11853, 'Dual Refinement Underwater Object Detection Network', 'Underwater object detection', 'Feature enhancement', 'Anchor refinement', 'Underwater dataset', '', 'Due to the complex underwater environment, underwater imaging often encounters some problems such as blur, scale variation, color shift, and texture distortion. Generic detection algorithms can not work well when we use them directly in the underwater scene. To address these problems, we propose an underwater detection framework with feature enhancement and anchor refinement. It has a composite connection backbone to boost the feature representation and introduces a receptive field augmentation module to exploit multi-scale contextual features. The developed underwater object detection framework also provides a prediction refinement scheme according to six prediction layers, it can refine multi-scale features to better align with anchors by learning from offsets, which solve the problem of sample imbalance to a certain extent. We also construct a new underwater detection dataset, denoted as UWD, which has more than 10,000 train-val and test underwater images. The extensive experiments on PASCAL VOC and UWD demonstrate the favorable performance of the proposed underwater detection framework against the states-of-the-arts methods in terms of accuracy and robustness. Source code and models are available at: https://github.com/Peterchen111/FERNet.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_17');
INSERT INTO `paper` VALUES (11854, 'Duality Diagram Similarity: A Generic Framework for Initialization Selection in Task Transfer Learning', 'Transfer learning', 'Deep neural network similarity', 'Duality diagram similarity', 'Representational similarity analysis', '', 'In this paper, we tackle an open research question in transfer learning, which is selecting a model initialization to achieve high performance on a new task, given several pre-trained models. We propose a new highly efficient and accurate approach based on duality diagram similarity (DDS) between deep neural networks (DNNs). DDS is a generic framework to represent and compare data of different feature dimensions. We validate our approach on the Taskonomy dataset by measuring the correspondence between actual transfer learning performance rankings on 17 taskonomy tasks and predicted rankings. Computing DDS based ranking for \\(17\\times 17\\) transfers requires less than 2 min and shows a high correlation (0.86) with actual transfer learning rankings, outperforming state-of-the-art methods by a large margin (\\(10\\%\\)) on the Taskonomy benchmark. We also demonstrate the robustness of our model selection approach to a new task, namely Pascal VOC semantic segmentation. Additionally, we show that our method can be applied to select the best layer locations within a DNN for transfer learning on 2D, 3D and semantic tasks on NYUv2 and Pascal VOC datasets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_30');
INSERT INTO `paper` VALUES (11855, 'DVI: Depth Guided Video Inpainting for Autonomous Driving', 'Video inpainting', 'Autonomous driving', 'Depth', 'Image synthesis', 'Simulation', 'To get clear street-view and photo-realistic simulation in autonomous driving, we present an automatic video inpainting algorithm that can remove traffic agents from videos and synthesize missing regions with the guidance of depth/point cloud. By building a dense 3D map from stitched point clouds, frames within a video are geometrically correlated via this common 3D map. In order to fill a target inpainting area in a frame, it is straightforward to transform pixels from other frames into the current one with correct occlusion. Furthermore, we are able to fuse multiple videos through 3D point cloud registration, making it possible to inpaint a target video with multiple source videos. The motivation is to solve the long-time occlusion problem where an occluded area has never been visible in the entire video. To our knowledge, we are the first to fuse multiple videos for video inpainting. To verify the effectiveness of our approach, we build a large inpainting dataset in the real urban road environment with synchronized images and Lidar data including many challenge scenes, e.g., long time occlusion. The experimental results show that the proposed approach outperforms the state-of-the-art approaches for all the criteria, especially the RMSE (Root Mean Squared Error) has been reduced by about \\(\\mathbf{13} \\%\\).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_1');
INSERT INTO `paper` VALUES (11856, 'Dynamic and Static Context-Aware LSTM for Multi-agent Motion Prediction', 'Motion prediction', 'Trajectory forecasting', 'Social model', '', '', 'Multi-agent motion prediction is challenging because it aims to foresee the future trajectories of multiple agents (e.g. pedestrians) simultaneously in a complicated scene. Existing work addressed this challenge by either learning social spatial interactions represented by the positions of a group of pedestrians, while ignoring their temporal coherence (i.e. dependencies between different long trajectories), or by understanding the complicated scene layout (e.g. scene segmentation) to ensure safe navigation. However, unlike previous work that isolated the spatial interaction, temporal coherence, and scene layout, this paper designs a new mechanism, i.e., Dynamic and Static Context-aware Motion Predictor (DSCMP), to integrates these rich information into the long-short-term-memory (LSTM). It has three appealing benefits. (1) DSCMP models the dynamic interactions between agents by learning both their spatial positions and temporal coherence, as well as understanding the contextual scene layout. (2) Different from previous LSTM models that predict motions by propagating hidden features frame by frame, limiting the capacity to learn correlations between long trajectories, we carefully design a differentiable queue mechanism in DSCMP, which is able to explicitly memorize and learn the correlations between long trajectories. (3) DSCMP captures the context of scene by inferring latent variable, which enables multimodal predictions with meaningful semantic scene layout. Extensive experiments show that DSCMP outperforms state-of-the-art methods by large margins, such as 9.05% and 7.62% relative improvements on the ETH-UCY and SDD datasets respectively.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_33');
INSERT INTO `paper` VALUES (11857, 'Dynamic Dual-Attentive Aggregation Learning for Visible-Infrared Person Re-identification', 'Person re-identification', 'Graph attention', 'Cross-modality', '', '', 'Visible-infrared person re-identification (VI-ReID) is a challenging cross-modality pedestrian retrieval problem. Due to the large intra-class variations and cross-modality discrepancy with large amount of sample noise, it is difficult to learn discriminative part features. Existing VI-ReID methods instead tend to learn global representations, which have limited discriminability and weak robustness to noisy images. In this paper, we propose a novel dynamic dual-attentive aggregation (DDAG) learning method by mining both intra-modality part-level and cross-modality graph-level contextual cues for VI-ReID. We propose an intra-modality weighted-part attention module to extract discriminative part-aggregated features, by imposing the domain knowledge on the part relationship mining. To enhance robustness against noisy samples, we introduce cross-modality graph structured attention to reinforce the representation with the contextual relations across the two modalities. We also develop a parameter-free dynamic dual aggregation learning strategy to adaptively integrate the two components in a progressive joint training manner. Extensive experiments demonstrate that DDAG outperforms the state-of-the-art methods under various settings.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_14');
INSERT INTO `paper` VALUES (11858, 'Dynamic Group Convolution for Accelerating Convolutional Neural Networks', 'Group convolution', 'Dynamic execution', 'Efficient network architecture', '', '', 'Replacing normal convolutions with group convolutions can significantly increase the computational efficiency of modern deep convolutional networks, which has been widely adopted in compact network architecture designs. However, existing group convolutions undermine the original network structures by cutting off some connections permanently resulting in significant accuracy degradation. In this paper, we propose dynamic group convolution (DGC) that adaptively selects which part of input channels to be connected within each group for individual samples on the fly. Specifically, we equip each group with a small feature selector to automatically select the most important input channels conditioned on the input images. Multiple groups can adaptively capture abundant and complementary visual/semantic features for each input image. The DGC preserves the original network structure and has similar computational efficiency as the conventional group convolution simultaneously. Extensive experiments on multiple image classification benchmarks including CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority over the existing group convolution techniques and dynamic execution methods. The code is available at https://github.com/zhuogege1943/dgc.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_9');
INSERT INTO `paper` VALUES (11859, 'Dynamic Image for 3D MRI Image Alzheimer’s Disease Classification', 'Dynamic image', '2D CNN', 'MRI image', 'Alzheimer’s disease', '', 'We propose to apply a 2D CNN architecture to 3D MRI image Alzheimer’s disease classification. Training a 3D convolutional neural network (CNN) is time-consuming and computationally expensive. We make use of approximate rank pooling to transform the 3D MRI image volume into a 2D image to use as input to a 2D CNN. We show our proposed CNN model achieves \\(9.5\\%\\) better Alzheimer’s disease classification accuracy than the baseline 3D models. We also show that our method allows for efficient training, requiring only \\(20\\%\\) of the training time compared to 3D CNN models. The code is available online: https://github.com/UkyVision/alzheimer-project.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_23');
INSERT INTO `paper` VALUES (11860, 'Dynamic Low-Light Imaging with Quanta Image Sensors', 'Quanta Image Sensors', 'Low light', 'Burst photography', '', '', 'Imaging in low light is difficult because the number of photons arriving at the sensor is low. Imaging dynamic scenes in low-light environments is even more difficult because as the scene moves, pixels in adjacent frames need to be aligned before they can be denoised. Conventional CMOS image sensors (CIS) are at a particular disadvantage in dynamic low-light settings because the exposure cannot be too short lest the read noise overwhelms the signal. We propose a solution using Quanta Image Sensors (QIS) and present a new image reconstruction algorithm. QIS are single-photon image sensors with photon counting capabilities. Studies over the past decade have confirmed the effectiveness of QIS for low-light imaging but reconstruction algorithms for dynamic scenes in low light remain an open problem. We fill the gap by proposing a student-teacher training protocol that transfers knowledge from a motion teacher and a denoising teacher to a student network. We show that dynamic scenes can be reconstructed from a burst of frames at a photon level of 1 photon per pixel per frame. Experimental results confirm the advantages of the proposed method compared to existing methods.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_8');
INSERT INTO `paper` VALUES (11861, 'Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training', 'Dynamic training', 'High quality object detection', '', '', '', 'Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP\\(_{90}\\) on the MS COCO dataset with no extra overhead. Codes and models are available at https://github.com/hkzhang95/DynamicRCNN.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_16');
INSERT INTO `paper` VALUES (11862, 'Dynamic ReLU', 'ReLU', 'Convolutional Neural Networks', 'Dynamic', '', '', 'Rectified linear units (ReLU) are commonly used in deep neural networks. So far ReLU and its generalizations (non-parametric or parametric) are static, performing identically for all input samples. In this paper, we propose Dynamic ReLU (DY-ReLU), a dynamic rectifier of which parameters are generated by a hyper function over all input elements. The key insight is that DY-ReLU encodes the global context into the hyper function, and adapts the piecewise linear activation function accordingly. Compared to its static counterpart, DY-ReLU has negligible extra computational cost, but significantly more representation capability, especially for light-weight neural networks. By simply using DY-ReLU for MobileNetV2, the top-1 accuracy on ImageNet classification is boosted from 72.0% to 76.2% with only 5% additional FLOPs.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_21');
INSERT INTO `paper` VALUES (11863, 'EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning', 'Model compression', 'Neural network pruning', '', '', '', 'Finding out the computational redundant part of a trained Deep Neural Network (DNN) is the key question that pruning algorithms target on. Many algorithms try to predict model performance of the pruned sub-nets by introducing various evaluation methods. But they are either inaccurate or very complicated for general application. In this work, we present a pruning method called EagleEye, in which a simple yet efficient evaluation component based on adaptive batch normalization is applied to unveil a strong correlation between different pruned DNN structures and their final settled accuracy. This strong correlation allows us to fast spot the pruned candidates with highest potential accuracy without actually fine-tuning them. This module is also general to plug-in and improve some existing pruning algorithms. EagleEye achieves better pruning performance than all of the studied pruning algorithms in our experiments. Concretely, to prune MobileNet V1 and ResNet-50, EagleEye outperforms all compared methods by up to 3.8%. Even in the more challenging experiments of pruning the compact model of MobileNet V1, EagleEye achieves the highest accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. All accuracy results are Top-1 ImageNet classification accuracy. Source code and models are accessible to open-source community (https://github.com/anonymous47823493/EagleEye).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_38');
INSERT INTO `paper` VALUES (11864, 'Early Exit or Not: Resource-Efficient Blind Quality Enhancement for Compressed Images', 'Blind quality enhancement', 'Compressed images', 'Resource-efficient', 'Early-exit', '', 'Lossy image compression is pervasively conducted to save communication bandwidth, resulting in undesirable compression artifacts. Recently, extensive approaches have been proposed to reduce image compression artifacts at the decoder side; however, they require a series of architecture-identical models to process images with different quality, which are inefficient and resource-consuming. Besides, it is common in practice that compressed images are with unknown quality and it is intractable for existing approaches to select a suitable model for blind quality enhancement. In this paper, we propose a resource-efficient blind quality enhancement (RBQE) approach for compressed images. Specifically, our approach blindly and progressively enhances the quality of compressed images through a dynamic deep neural network (DNN), in which an early-exit strategy is embedded. Then, our approach can automatically decide to terminate or continue enhancement according to the assessed quality of enhanced images. Consequently, slight artifacts can be removed in a simpler and faster process, while the severe artifacts can be further removed in a more elaborate process. Extensive experiments demonstrate that our RBQE approach achieves state-of-the-art performance in terms of both blind quality enhancement and resource efficiency.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_17');
INSERT INTO `paper` VALUES (11865, 'Edge-Aware Graph Representation Learning and Reasoning for Face Parsing', 'Face parsing', 'Graph representation', 'Attention mechanism', 'Graph reasoning', '', 'Face parsing infers a pixel-wise label to each facial component, which has drawn much attention recently. Previous methods have shown their efficiency in face parsing, which however overlook the correlation among different face regions. The correlation is a critical clue about the facial appearance, pose, expression, etc., and should be taken into account for face parsing. To this end, we propose to model and reason the region-wise relations by learning graph representations, and leverage the edge information between regions for optimized abstraction. Specifically, we encode a facial image onto a global graph representation where a collection of pixels (“regions”) with similar features are projected to each vertex. Our model learns and reasons over relations between the regions by propagating information across vertices on the graph. Furthermore, we incorporate the edge information to aggregate the pixel-wise features onto vertices, which emphasizes on the features around edges for fine segmentation along edges. The finally learned graph representation is projected back to pixel grids for parsing. Experiments demonstrate that our model outperforms state-of-the-art methods on the widely used Helen dataset, and also exhibits the superior performance on the large-scale CelebAMask-HQ and LaPa dataset. The code is available at https://github.com/tegusi/EAGRNet.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_16');
INSERT INTO `paper` VALUES (11866, 'EEDNet: Enhanced Encoder-Decoder Network for AutoISP', 'ISP', 'RAW-to-RGB', 'LRF hypothesis', 'ClipL1', '', 'Image Signal Processor (ISP) plays a core rule in camera systems. However, ISP tuning is highly complicated and requires professional skills and advanced imaging experiences. To skip the painful ISP tuning process, we introduce EEDNet in this paper, which directly transforms an image in the raw space to an image in the sRGB space (RAW-to-RGB). Data-driven RAW-to-RGB mapping is a grand new low-level vision task. In this work, we propose a hypothesis of the receptive field that large receptive field (LRF) is essential in high-level computer vision tasks, but not crucial in low-level pixel-to-pixel tasks. Besides, we present a ClipL1 loss, which simultaneously considers easy examples and outliers during the optimization process. Benefiting from the LRF hypothesis and ClipL1 loss, EEDNet can generate high-quality pictures with more details. Our method achieves promising results on Zurich RAW2RGB (ZRR) dataset and won the first place in AIM2020 ISP challenging.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_10');
INSERT INTO `paper` VALUES (11867, 'Effective Feature Enhancement and Model Ensemble Strategies in Tiny Object Detection', 'Data augmentation', 'Feature pyramid', 'Model ensemble', '', '', 'We introduce a novel tiny-object detection network that achieves better accuracy than existing detectors on TinyPerson dataset. It is an end-to-end detection framework developed on PaddlePaddle. A suit of strategies are developed to improve the detectors performance including: 1) data augmentation based on scale-match that aligns the object scales between the existing large-scale dataset and TinyPerson; 2) comprehensive training methods to further improve detection performance by a large margin; 3) model refinement based on the enhanced PAFPN module to fully utilize semantic information; 4) a hierarchical coarse-to-fine ensemble strategy to improve detection performance based on a well-designed model pond.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_24');
INSERT INTO `paper` VALUES (11868, 'Efficiency in Real-Time Webcam Gaze Tracking', '', '', '', '', '', 'Efficiency and ease of use are essential for practical applications of camera based eye/gaze-tracking. Gaze tracking involves estimating where a person is looking on a screen based on face images from a computer-facing camera. In this paper we investigate two complementary forms of efficiency in gaze tracking: 1. The computational efficiency of the system which is dominated by the inference speed of a CNN predicting gaze-vectors; 2. The usability efficiency which is determined by the tediousness of the mandatory calibration of the gaze-vector to a computer screen. To do so, we evaluate the computational speed/accuracy trade-off for the CNN and the calibration effort/accuracy trade-off for screen calibration. For the CNN, we evaluate the full face, two-eyes, and single eye input. For screen calibration, we measure the number of calibration points needed and evaluate three types of calibration: 1. pure geometry, 2. pure machine learning, and 3. hybrid geometric regression. Results suggest that a single eye input and geometric regression calibration achieve the best trade-off.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_34');
INSERT INTO `paper` VALUES (11869, 'Efficient Adversarial Attacks for Visual Object Tracking', 'Adversarial attack', 'Visual object tracking', 'Deep learning', '', '', 'Visual object tracking is an important task that requires the tracker to find the objects quickly and accurately. The existing state-of-the-art object trackers, i.e., Siamese based trackers, use DNNs to attain high accuracy. However, the robustness of visual tracking models is seldom explored. In this paper, we analyze the weakness of object trackers based on the Siamese network and then extend adversarial examples to visual object tracking. We present an end-to-end network FAN (Fast Attack Network) that uses a novel drift loss combined with the embedded feature loss to attack the Siamese network based trackers. Under a single GPU, FAN is efficient in the training speed and has a strong attack performance. The FAN can generate an adversarial example at 10ms, achieve effective targeted attack (at least 40% drop rate on OTB) and untargeted attack (at least 70% drop rate on OTB).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_3');
INSERT INTO `paper` VALUES (11870, 'Efficient Approximation of Filters for High-Accuracy Binary Convolutional Neural Networks', 'Binary neural network', 'Convolutional neural networks', 'Filter approximation', 'Scaling factors', '', 'In this paper, we propose an efficient design to convert full-precision convolutional networks into binary neural networks. Our method approximates a full-precision convolutional filter by sum of binary filters with multiplicative and additive scaling factors. We present closed form solutions to the proposed methods. We perform experiments on binary neural networks with binary activations and pre-trained neural networks with full-precision activations. The results show an increase in accuracy compared to previous binary neural networks. Furthermore, to reduce the complexity, we prune scaling factors considering the accuracy. We show that up to a certain degree of threshold, we can prune scaling factors while maintaining accuracy comparable to full-precision convolutional neural networks.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_6');
INSERT INTO `paper` VALUES (11871, 'Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs', 'Visual dialog', 'Attention', 'Multimodality', '', '', 'It has been a primary concern in recent studies of vision and language tasks to design an effective attention mechanism dealing with interactions between the two modalities. The Transformer has recently been extended and applied to several bi-modal tasks, yielding promising results. For visual dialog, it becomes necessary to consider interactions between three or more inputs, i.e., an image, a question, and a dialog history, or even its individual dialog components. In this paper, we present a neural architecture named Light-weight Transformer for Many Inputs (LTMI) that can efficiently deal with all the interactions between multiple such inputs in visual dialog. It has a block structure similar to the Transformer and employs the same design of attention computation, whereas it has only a small number of parameters, yet has sufficient representational power for the purpose. Assuming a standard setting of visual dialog, a layer built upon the proposed attention block has less than one-tenth of parameters as compared with its counterpart, a natural Transformer extension. The experimental results on the VisDial datasets validate the effectiveness of the proposed approach, showing improvements of the best NDCG score on the VisDial v1.0 dataset from 57.59 to 60.92 with a single model, from 64.47 to 66.53 with ensemble models, and even to 74.88 with additional finetuning.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_14');
INSERT INTO `paper` VALUES (11872, 'Efficient Image Super-Resolution Using Pixel Attention', 'Super resolution', 'Deep neural networks', '', '', '', 'This work aims at designing a lightweight convolutional neural network for image super resolution (SR). With simplicity bare in mind, we construct a pretty concise and effective network with a newly proposed pixel attention scheme. Pixel attention (PA) is similar as channel attention and spatial attention in formulation. The difference is that PA produces 3D attention maps instead of a 1D attention vector or a 2D map. This attention scheme introduces fewer additional parameters but generates better SR results. On the basis of PA, we propose two building blocks for the main branch and the reconstruction branch, respectively. The first one—SC-PA block has the same structure as the Self-Calibrated convolution but with our PA layer. This block is much more efficient than conventional residual/dense blocks, for its two-branch architecture and attention scheme. While the second one—U-PA block combines the nearest-neighbor upsampling, convolution and PA layers. It improves the final reconstruction quality with little parameter cost. Our final model—PAN could achieve similar performance as the lightweight networks—SRResNet and CARN, but with only 272K parameters (17.92% of SRResNet and 17.09% of CARN). The effectiveness of each proposed component is also validated by ablation study. The code is available at https://github.com/zhaohengyuan1/PAN.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_3');
INSERT INTO `paper` VALUES (11873, 'Efficient Neighbourhood Consensus Networks via Submanifold Sparse Convolutions', 'Image matching', 'Neighbourhood consensus', 'Sparse CNN', '', '', 'In this work we target the problem of estimating accurately localized correspondences between a pair of images. We adopt the recent Neighbourhood Consensus Networks that have demonstrated promising performance for difficult correspondence problems and propose modifications to overcome their main limitations: large memory consumption, large inference time and poorly localized correspondences. Our proposed modifications can reduce the memory footprint and execution time more than \\(10\\times \\), with equivalent results. This is achieved by sparsifying the correlation tensor containing tentative matches, and its subsequent processing with a 4D CNN using submanifold sparse convolutions. localization accuracy is significantly improved by processing the input images in higher resolution, which is possible due to the reduced memory footprint, and by a novel two-stage correspondence relocalization module. The proposed Sparse-NCNet method obtains state-of-the-art results on the HPatches Sequences and InLoc visual localization benchmarks, and competitive results on the Aachen Day-Night benchmark.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_35');
INSERT INTO `paper` VALUES (11874, 'Efficient Non-Line-of-Sight Imaging from Transient Sinograms', 'Computational imaging', 'Non-line-of-sight imaging', '', '', '', 'Non-line-of-sight (NLOS) imaging techniques use light that diffusely reflects off of visible surfaces (e.g., walls) to see around corners. One approach involves using pulsed lasers and ultrafast sensors to measure the travel time of multiply scattered light. Unlike existing NLOS techniques that generally require densely raster scanning points across the entirety of a relay wall, we explore a more efficient form of NLOS scanning that reduces both acquisition times and computational requirements. We propose a circular and confocal non-line-of-sight (\\(\\text {C}^2\\text {NLOS}\\)) scan that involves illuminating and imaging a common point, and scanning this point in a circular path along a wall. We observe that (1) these \\(\\text {C}^2\\text {NLOS}\\) measurements consist of a superposition of sinusoids, which we refer to as a transient sinogram, (2) there exists computationally efficient reconstruction procedures that transform these sinusoidal measurements into 3D positions of hidden scatterers or NLOS images of hidden objects, and (3) despite operating on an order of magnitude fewer measurements than previous approaches, these \\(\\text {C}^2\\text {NLOS}\\) scans provide sufficient information about the hidden scene to solve these different NLOS imaging tasks. We show results from both simulated and real \\(\\text {C}^2\\text {NLOS}\\) scans (Project page: https://marikoisogawa.github.io/project/c2nlos).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_12');
INSERT INTO `paper` VALUES (11875, 'Efficient Outdoor 3D Point Cloud Semantic Segmentation for Critical Road Objects and Distributed Contexts', '3D semantic segmentation', 'Attention', 'Point convolution', 'Point clouds', '', 'Large-scale point cloud semantic understanding is an important problem in self-driving cars and autonomous robotics navigation. However, such problem involves many challenges, such as i) critical road objects (e.g., pedestrians, barriers) with diverse and varying input shapes; ii) distributed contextual information across large spatial range; iii) efficient inference time. Failing to deal with such challenges may weaken the mission-critical performance of self-driving car, e.g, LiDAR road objects perception. In this work, we propose a novel neural network model called Attention-based Dynamic Convolution Network with Self-Attention Global Contexts(ADConvnet-SAGC), which i) applies attention mechanism to adaptively focus on the most task-related neighboring points for learning the point features of 3D objects, especially for small objects with diverse shapes; ii) applies self-attention module for efficiently capturing long-range distributed contexts from the input; iii) a more reasonable and compact architecture for efficient inference. Extensive experiments on point cloud semantic segmentation validate the effectiveness of the proposed ADConvnet-SAGC model and show significant improvements over state-of-the-art methods.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_30');
INSERT INTO `paper` VALUES (11876, 'Efficient Residue Number System Based Winograd Convolution', '', '', '', '', '', 'Prior research has shown that Winograd algorithm can reduce the computational complexity of convolutional neural networks (CNN) with weights and activations represented in floating point. However it is difficult to apply the scheme to the inference of low-precision quantized (e.g. INT8) networks. Our work extends the Winograd algorithm to Residue Number System (RNS). The minimal complexity convolution is computed precisely over large transformation tile (e.g. \\(10 \\times 10\\) to \\(16 \\times 16\\)) of filters and activation patches using the Winograd transformation and low cost (e.g. 8-bit) arithmetic without degrading the prediction accuracy of the networks during inference. The arithmetic complexity reduction is up to \\(7.03\\times \\) while the performance improvement is up to \\(2.30\\times \\) to \\(4.69\\times \\) for \\(3\\times 3\\) and \\(5 \\times 5\\) filters respectively.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_4');
INSERT INTO `paper` VALUES (11877, 'Efficient Scale-Permuted Backbone with Learned Resource Distribution', 'Scale-permuted model', 'Object detection', '', '', '', 'Recently, SpineNet has demonstrated promising results on object detection and image classification over ResNet model. However, it is unclear if the improvement adds up when combining scale-permuted backbone with advanced efficient operations and compound scaling. Furthermore, SpineNet is built with a uniform resource distribution over operations. While this strategy seems to be prevalent for scale-decreased models, it may not be an optimal design for scale-permuted models. In this work, we propose a simple technique to combine efficient operations and compound scaling with a previously learned scale-permuted architecture. We demonstrate the efficiency of scale-permuted model can be further improved by learning a resource distribution over the entire network. The resulting efficient scale-permuted models outperform state-of-the-art EfficientNet-based models on object detection and achieve competitive performance on image classification and semantic segmentation.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_34');
INSERT INTO `paper` VALUES (11878, 'Efficient Semantic Video Segmentation with Per-Frame Inference', 'Semantic video segmentation', 'Temporal consistency', '', '', '', 'For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results when tested on a video sequence. A few methods take the correlations in the video sequence into account, e.g., by propagating the results to the neighbouring frames using optical flow, or extracting frame representations using multi-frame information, which may lead to inaccurate results or unbalanced latency. In contrast, here we explicitly consider the temporal consistency among frames as extra constraints during training and process each frame independently in the inference phase. Thus no computation overhead is introduced for inference. Compact models are employed for real-time execution. To narrow the performance gap between compact models and large models, new temporal knowledge distillation methods are designed. Weighing among accuracy, temporal smoothness and efficiency, our proposed method outperforms previous keyframe based methods and corresponding baselines which are trained with each frame independently on benchmark datasets including Cityscapes and Camvid. Code is available at: https://git.io/vidseg.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_21');
INSERT INTO `paper` VALUES (11879, 'Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring', 'Video deblurring', 'RNN', 'Network efficiency', 'Attention', 'Dataset', 'Real-time video deblurring still remains a challenging task due to the complexity of spatially and temporally varying blur itself and the requirement of low computational cost. To improve the network efficiency, we adopt residual dense blocks into RNN cells, so as to efficiently extract the spatial features of the current frame. Furthermore, a global spatio-temporal attention module is proposed to fuse the effective hierarchical features from past and future frames to help better deblur the current frame. For evaluation, we also collect a novel dataset with paired blurry/sharp video clips by using a co-axis beam splitter system. Through experiments on synthetic and realistic datasets, we show that our proposed method can achieve better deblurring performance both quantitatively and qualitatively with less computational cost against state-of-the-art video deblurring methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_12');
INSERT INTO `paper` VALUES (11880, 'Efficient Super-Resolution Using MobileNetV3', 'Super-resolution', 'Efficient CNN', 'Mobile digital zoom', '', '', 'Deep learning methods for super-resolution (SR) have been dominating in terms of performance in recent years. Such methods can potentially improve the digital zoom capabilities of most modern mobile phones, but are not directly applicable on device, due to hardware constraints. In this work, we adapt MobileNetV3 blocks, shown to work well for classification, detection and segmentation, to the task of super-resolution. The proposed models with the modified MobileNetV3 block are shown to be efficient enough to run on modern mobile phones with an accuracy approaching that of the much heavier, state-of-the-art (SOTA) super-resolution approaches.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_5');
INSERT INTO `paper` VALUES (11881, 'Efficient Transfer Learning via Joint Adaptation of Network Architecture and Weight', 'Neural architecture search', 'Transfer learning', 'Weight sharing', '', '', 'Transfer learning can boost the performance on the target task by leveraging the knowledge of the source domain. Recent works in neural architecture search (NAS), especially one-shot NAS, can aid transfer learning by establishing sufficient network search space. However, existing NAS methods tend to approximate huge search spaces by explicitly building giant super-networks with multiple sub-paths, and discard super-network weights after a child structure is found. Both the characteristics of existing approaches causes repetitive network training on source tasks in transfer learning. To remedy the above issues, we reduce the super-network size by randomly dropping connection between network blocks while embedding a larger search space. Moreover, we reuse super-network weights to avoid redundant training by proposing a novel framework consisting of two modules, the neural architecture search module for architecture transfer and the neural weight search module for weight transfer. These two modules conduct search on the target task based on a reduced super-networks, so we only need to train once on the source task. We experiment our framework on both MS-COCO and CUB-200 for the object detection and fine-grained image classification tasks, and show promising improvements with only \\(O(C^{N})\\) super-network complexity.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_28');
INSERT INTO `paper` VALUES (11882, 'EfficientFCN: Holistically-Guided Decoding for Semantic Segmentation', 'Semantic segmentation', 'Encoder-decoder', 'Dilated convolution', 'Holistic features', '', 'Both performance and efficiency are important to semantic segmentation. State-of-the-art semantic segmentation algorithms are mostly based on dilated Fully Convolutional Networks (dilatedFCN), which adopt dilated convolutions in the backbone networks to extract high-resolution feature maps for achieving high-performance segmentation performance. However, due to many convolution operations are conducted on the high-resolution feature maps, such dilatedFCN-based methods result in large computational complexity and memory consumption. To balance the performance and efficiency, there also exist encoder-decoder structures that gradually recover the spatial information by combining multi-level feature maps from the encoder. However, the performances of existing encoder-decoder methods are far from comparable with the dilatedFCN-based methods. In this paper, we propose the EfficientFCN, whose backbone is a common ImageNet pretrained network without any dilated convolution. A holistically-guided decoder is introduced to obtain the high-resolution semantic-rich feature maps via the multi-scale features from the encoder. The decoding task is converted to novel codebook generation and codeword assembly task, which takes advantages of the high-level and low-level features from the encoder. Such a framework achieves comparable or even better performance than state-of-the-art methods with only 1/3 of the computational cost. Extensive experiments on PASCAL Context, PASCAL VOC, ADE20K validate the effectiveness of the proposed EfficientFCN.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_1');
INSERT INTO `paper` VALUES (11883, 'Efficiently Detecting Plausible Locations for Object Placement Using Masked Convolutions', '', '', '', '', '', 'Being able to insert new objects into images is an important problem for both artistic image editing and for data augmentation. For a successful image manipulation, the plausible placement and the blending of the new objects in the image are critical. In this paper, we propose a fast method for the automatic selection of plausible locations for object insertion into images. Like previous work, we approach the object placement problem as a detection problem – given a bounding box, we evaluate whether an object is present inside the box based only on the neighborhood of the box. However, previous work requires a forward pass for each potential bounding box location. We propose instead to make use of masked convolutions to compute featuremaps for left, right, top and bottom contexts just once per image. Combining these features in such a way that no information from inside a bounding box is propagated to the final classifier allows the model to evaluate a grid of proposals on the featuremaps rather than on the image, speeding up inference dramatically. We validate that our model can generate plausible placements using experiments on the COCO dataset and on a user study. Our method trades off speed for performance, as compared to a patch based approach.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_15');
INSERT INTO `paper` VALUES (11884, 'EGDCL: An Adaptive Curriculum Learning Framework for Unbiased Glaucoma Diagnosis', 'Curriculum learning', 'Unbiased diagnosis', 'Sample imbalance', 'Hard sample', 'Computer-aided diagnosis', 'Today’s computer-aided diagnosis (CAD) model is still far from the clinical practice of glaucoma detection, mainly due to the training bias originating from 1) the normal-abnormal class imbalance and 2) the rare but significant hard samples in fundus images. However, debiasing in CAD is not trivial because existing methods cannot cure the two types of bias to categorize fundus images. In this paper, we propose a novel curriculum learning paradigm (EGDCL) to train an unbiased glaucoma diagnosis model with the adaptive dual-curriculum. Innovatively, the dual-curriculum is designed with the guidance of evidence maps to build a training criterion, which gradually cures the bias in training data. In particular, the dual-curriculum emphasizes unbiased training contributions of data from easy to hard, normal to abnormal, and the dual-curriculum is optimized jointly with model parameters to obtain the optimal solution. In comparison to baselines, EGDCL significantly improves the convergence speed of the training process and obtains the top performance in the test procedure. Experimental results on challenging glaucoma datasets show that our EGDCL delivers unbiased diagnosis (0.9721 of Sensitivity, 0.9707 of Specificity, 0.993 of AUC, 0.966 of F2-score) and outperform the other methods. It endows our EGDCL a great advantage to handle the unbiased CAD in clinical application.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_12');
INSERT INTO `paper` VALUES (11885, 'Embedding Propagation: Smoother Manifold for Few-Shot Classification', 'Few-shot', 'Classification', 'Semi-supervised learning', 'Metalearning', '', 'Few-shot classification is challenging because the data distribution of the training set can be widely different to the test set as their classes are disjoint. This distribution shift often results in poor generalization. Manifold smoothing has been shown to address the distribution shift problem by extending the decision boundaries and reducing the noise of the class representations. Moreover, manifold smoothness is a key factor for semi-supervised learning and transductive learning algorithms. In this work, we propose to use embedding propagation as an unsupervised non-parametric regularizer for manifold smoothing in few-shot classification. Embedding propagation leverages interpolations between the extracted features of a neural network based on a similarity graph. We empirically show that embedding propagation yields a smoother embedding manifold. We also show that applying embedding propagation to a transductive classifier achieves new state-of-the-art results in miniImagenet, tieredImagenet, Imagenet-FS, and CUB. Furthermore, we show that embedding propagation consistently improves the accuracy of the models in multiple semi-supervised learning scenarios by up to 16% points. The proposed embedding propagation operation can be easily integrated as a non-parametric layer into a neural network. We provide the training code and usage examples at https://github.com/ElementAI/embedding-propagation.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_8');
INSERT INTO `paper` VALUES (11886, 'Emotion Embedded Pose Generation', 'Emotion recognition', 'Body pose', 'Conditional GAN', '', '', 'Body poses are a rich source of information in the field of sentiment analysis and they complement existing facial emotion recognition tasks by adding significant value especially when faces are not easily available. CCTV recordings and other non-human interfaces are applications where capturing facial expression is challenging and these interfaces can benefit from body pose emotion recognition to conduct context analysis. Another roadblock in this direction is the limited availability of collected and curated datasets of body poses with emotion labels. Addressing these issues, we propose two end-to-end pipelines to generate emotion conditioned human poses corresponding to specific emotion labels. An auxiliary conditional GAN network is presented for pose images and pose skeleton pipelines. The generated images improved emotion classification accuracy by an average of 5.40% across 4 different networks compared to images that were traditionally augmented. Additionally, through image and skeletal augmentation, we achieve state-of-the-art emotion classification results for the BEAST dataset.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_54');
INSERT INTO `paper` VALUES (11887, 'Emotion Understanding in Videos Through Body, Context, and Visual-Semantic Embedding Loss', 'Emotion', 'Body', 'Context', 'Visual-semantic', 'BEEU challenge', 'We present our winning submission to the First International Workshop on Bodily Expressed Emotion Understanding (BEEU) challenge. Based on recent literature on the effect of context/environment on emotion, as well as visual representations with semantic meaning using word embeddings, we extend the framework of Temporal Segment Network to accommodate these. Our method is verified on the validation set of the Body Language Dataset (BoLD) and achieves 0.26235 Emotion Recognition Score on the test set, surpassing the previous best result of 0.2530.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_52');
INSERT INTO `paper` VALUES (11888, 'Employing Multi-estimations for Weakly-Supervised Semantic Segmentation', 'Weakly-supervised learning', 'Semantic segmentation', '', '', '', 'Image-level label based weakly-supervised semantic segmentation (WSSS) aims to adopt image-level labels to train semantic segmentation models, saving vast human labors for costly pixel-level annotations. A typical pipeline for this problem is first to adopt class activation maps (CAM) with image-level labels to generate pseudo-masks (a.k.a. seeds) and then use them for training segmentation models. The main difficulty is that seeds are usually sparse and incomplete. Related works typically try to alleviate this problem by adopting many bells and whistles to enhance the seeds. Instead of struggling to refine a single seed, we propose a novel approach to alleviate the inaccurate seed problem by leveraging the segmentation model’s robustness to learn from multiple seeds. We managed to generate many different seeds for each image, which are different estimates of the underlying ground truth. The segmentation model simultaneously exploits these seeds to learn and automatically decides the confidence of each seed. Extensive experiments on Pascal VOC 2012 demonstrate the advantage of this multi-seeds strategy over previous state-of-the-art.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_20');
INSERT INTO `paper` VALUES (11889, 'Empowering Relational Network by Self-attention Augmented Conditional Random Fields for Group Activity Recognition', 'Bidirectional universal transformer encoder', 'Self-attention mechanism', 'Conditional random field', 'Graph cliques', 'Group activity', 'This paper presents a novel relational network for group activity recognition. The core of our network is to augment the conditional random fields (CRF), amenable to learning inter-dependency of correlated observations, with the newly devised temporal and spatial self-attention to learn the temporal evolution and spatial relational contexts of every actor in videos. Such a combination utilizes the global receptive fields of self-attention to construct a spatio-temporal graph topology to address the temporal dependency and non-local relationships of the actors. The network first uses the temporal self-attention along with the spatial self-attention, which considers multiple cliques with different scales of locality to account for the diversity of the actors’ relationships in group activities, to model the pairwise energy of CRF. Afterward, to accommodate the distinct characteristics of each video, a new mean-field inference algorithm with dynamic halting is also addressed. Finally, a bidirectional universal transformer encoder (UTE), which combines both of the forward and backward temporal context information, is used to aggregate the relational contexts and scene information for group activity recognition. Simulations show that the proposed approach surpasses the state-of-the-art methods on the widespread Volleyball and Collective Activity datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_5');
INSERT INTO `paper` VALUES (11890, 'Enabling Deep Residual Networks for Weakly Supervised Object Detection', '', '', '', '', '', 'Weakly supervised object detection (WSOD) has attracted extensive research attention due to its great flexibility of exploiting large-scale image-level annotation for detector training. Whilst deep residual networks such as ResNet and DenseNet have become the standard backbones for many computer vision tasks, the cutting-edge WSOD methods still rely on plain networks, e.g., VGG, as backbones. It is indeed not trivial to employ deep residual networks for WSOD, which even shows significant deterioration of detection accuracy and non-convergence. In this paper, we discover the intrinsic root with sophisticated analysis and propose a sequence of design principles to take full advantages of deep residual learning for WSOD from the perspectives of adding redundancy, improving robustness and aligning features. First, a redundant adaptation neck is key for effective object instance localization and discriminative feature learning. Second, small-kernel convolutions and MaxPool down-samplings help improve the robustness of information flow, which gives finer object boundaries and make the detector more sensitivity to small objects. Third, dilated convolution is essential to align the proposal features and exploit diverse local information by extracting high-resolution feature maps. Extensive experiments show that the proposed principles enable deep residual networks to establishes new state-of-the-arts on PASCAL VOC and MS COCO.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_8');
INSERT INTO `paper` VALUES (11891, 'Encoding Structure-Texture Relation with P-Net for Anomaly Detection in Retinal Images', 'Structure-texture relation', 'Anomaly detection', 'Novel class discovery', '', '', 'Anomaly detection in retinal image refers to the identification of abnormality caused by various retinal diseases/lesions, by only leveraging normal images in training phase. Normal images from healthy subjects often have regular structures (e.g., the structured blood vessels in the fundus image, or structured anatomy in optical coherence tomography image). On the contrary, the diseases and lesions often destroy these structures. Motivated by this, we propose to leverage the relation between the image texture and structure to design a deep neural network for anomaly detection. Specifically, we first extract the structure of the retinal images, then we combine both the structure features and the last layer features extracted from original health image to reconstruct the original input healthy image. The image feature provides the texture information and guarantees the uniqueness of the image recovered from the structure. In the end, we further utilize the reconstructed image to extract the structure and measure the difference between structure extracted from original and the reconstructed image. On the one hand, minimizing the reconstruction difference behaves like a regularizer to guarantee that the image is corrected reconstructed. On the other hand, such structure difference can also be used as a metric for normality measurement. The whole network is termed as P-Net because it has a “P” shape. Extensive experiments on RESC dataset and iSee dataset validate the effectiveness of our approach for anomaly detection in retinal images. Further, our method also generalizes well to novel class discovery in retinal images and anomaly detection in real-world images.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_22');
INSERT INTO `paper` VALUES (11892, 'End-to-end Dynamic Matching Network for Multi-view Multi-person 3D Pose Estimation', '3d human pose estimation', 'End-to-end', 'Multi-view multi-person', 'Dynamic matching', '', 'As an important computer vision task, 3d human pose estimation in a multi-camera, multi-person setting has received widespread attention and many interesting applications have been derived from it. Traditional approaches use a 3d pictorial structure model to handle this task. However, these models suffer from high computation costs and result in low accuracy in joint detection. Recently, especially since the introduction of Deep Neural Networks, one popular approach is to build a pipeline that involves three separate steps: (1) 2d skeleton detection in each camera view, (2) identification of matched 2d skeletons and (3) estimation of the 3d poses. Many existing works operate by feeding the 2d images and camera parameters through the three modules in a cascade fashion. However, all three operations can be highly correlated. For example, the 3d generation results may affect the results of detection in step 1, as does the matching algorithm in step 2. To address this phenomenon, we propose a novel end-to-end training scheme that brings the three separate modules into a single model. However, one outstanding problem of doing so is that the matching algorithm in step 2 appears to disjoint the pipeline. Therefore, we take our inspiration from the recent success in Capsule Networks, in which its Dynamic Routing step is also disjointed, but plays a crucial role in deciding how gradients are flowed from the upper to the lower layers. Similarly, a dynamic matching module in our work also decides the paths in which gradients flow from step 3 to step 1. Furthermore, as a large number of cameras are present, the existing matching algorithm either fails to deliver a robust performance or can be very inefficient. Thus, we additionally propose a novel matching algorithm that can match 2d poses from multiple views efficiently. The algorithm is robust and able to deal with situations of incomplete and false 2d detection as well.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_29');
INSERT INTO `paper` VALUES (11893, 'End-to-end Interpretable Learning of Non-blind Image Deblurring', 'Non-blind deblurring', 'Preconditioned fixed-point method', 'End-to-end learning', '', '', 'Non-blind image deblurring is typically formulated as a linear least-squares problem regularized by natural priors on the corresponding sharp picture’s gradients, which can be solved, for example, using a half-quadratic splitting method with Richardson fixed-point iterations for its least-squares updates and a proximal operator for the auxiliary variable updates. We propose to precondition the Richardson solver using approximate inverse filters of the (known) blur and natural image prior kernels. Using convolutions instead of a generic linear preconditioner allows extremely efficient parameter sharing across the image, and leads to significant gains in accuracy and/or speed compared to classical FFT and conjugate-gradient methods. More importantly, the proposed architecture is easily adapted to learning both the preconditioner and the proximal operator using CNN embeddings. This yields a simple and efficient algorithm for non-blind image deblurring which is fully interpretable, can be learned end to end, and whose accuracy matches or exceeds the state of the art, quite significantly, in the non-uniform case.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_19');
INSERT INTO `paper` VALUES (11894, 'End-to-End Low Cost Compressive Spectral Imaging with Spatial-Spectral Self-Attention', 'Compressive spectral imaging', 'Spatial-Spectral Self-Attention', 'Large-scale real data', '', '', 'Coded aperture snapshot spectral imaging (CASSI) is an effective tool to capture real-world 3D hyperspectral images. While a number of existing work has been conducted for hardware and algorithm design, we make a step towards the low-cost solution that enjoys video-rate high-quality reconstruction. To make solid progress on this challenging yet under-investigated task, we reproduce a stable single disperser (SD) CASSI system to gather large-scale real-world CASSI data and propose a novel deep convolutional network to carry out the real-time reconstruction by using self-attention. In order to jointly capture the self-attention across different dimensions in hyperspectral images (i.e., channel-wise spectral correlation and non-local spatial regions), we propose Spatial-Spectral Self-Attention (TSA) to process each dimension sequentially, yet in an order-independent manner. We employ TSA in an encoder-decoder network, dubbed TSA-Net, to reconstruct the desired 3D cube. Furthermore, we investigate how noise affects the results and propose to add shot noise in model training, which improves the real data results significantly. We hope our large-scale CASSI data serve as a benchmark in future research and our TSA model as a baseline in deep learning based reconstruction algorithms. Our code and data are available at https://github.com/mengziyi64/TSA-Net.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_12');
INSERT INTO `paper` VALUES (11895, 'End-to-End Object Detection with Transformers', '', '', '', '', '', 'We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_13');
INSERT INTO `paper` VALUES (11896, 'End-to-End Trainable Deep Active Contour Models for Automated Image Segmentation: Delineating Buildings in Aerial Imagery', 'Computer vision', 'Image segmentation', 'Active contour models', 'Convolutional neural networks', 'Building delineation', 'The automated segmentation of buildings in remote sensing imagery is a challenging task that requires the accurate delineation of multiple building instances over typically large image areas. Manual methods are often laborious and current deep-learning-based approaches fail to delineate all building instances and do so with adequate accuracy. As a solution, we present Trainable Deep Active Contours (TDACs), an automatic image segmentation framework that intimately unites Convolutional Neural Networks (CNNs) and Active Contour Models (ACMs). The Eulerian energy functional of the ACM component includes per-pixel parameter maps that are predicted by the backbone CNN, which also initializes the ACM. Importantly, both the ACM and CNN components are fully implemented in TensorFlow and the entire TDAC architecture is end-to-end automatically differentiable and backpropagation trainable without user intervention. TDAC yields fast, accurate, and fully automatic simultaneous delineation of arbitrarily many buildings in the image. We validate the model on two publicly available aerial image datasets for building segmentation, and our results demonstrate that TDAC establishes a new state-of-the-art performance.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_43');
INSERT INTO `paper` VALUES (11897, 'Energy-Based Models for Deep Probabilistic Regression', '', '', '', '', '', 'While deep learning-based classification is generally tackled using standardized approaches, a wide variety of techniques are employed for regression. In computer vision, one particularly popular such technique is that of confidence-based regression, which entails predicting a confidence value for each input-target pair (x, y). While this approach has demonstrated impressive results, it requires important task-dependent design choices, and the predicted confidences lack a natural probabilistic meaning. We address these issues by proposing a general and conceptually simple regression method with a clear probabilistic interpretation. In our proposed approach, we create an energy-based model of the conditional target density p(y|x), using a deep neural network to predict the un-normalized density from (x, y). This model of p(y|x) is trained by directly minimizing the associated negative log-likelihood, approximated using Monte Carlo sampling. We perform comprehensive experiments on four computer vision regression tasks. Our approach outperforms direct regression, as well as other probabilistic and confidence-based methods. Notably, our model achieves a \\(2.2\\%\\) AP improvement over Faster-RCNN for object detection on the COCO dataset, and sets a new state-of-the-art on visual tracking when applied for bounding box estimation. In contrast to confidence-based methods, our approach is also shown to be directly applicable to more general tasks such as age and head-pose estimation. Code is available at https://github.com/fregu856/ebms_regression.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_20');
INSERT INTO `paper` VALUES (11898, 'Enhanced Adaptive Dense Connection Single Image Super-Resolution', 'SISR', 'Dense connection', 'Sub-pixel reconstruction', 'Convolutional neural network', 'Deep learning', 'Increasing model size often results in improved performance on super-resolution reconstruction. However, at some point large model cannot SR huge images due to GPU/TPU memory limitations. In this paper, to address this problem, we present Block-Reconstruction(BR) strategy to improve the reconstruction quality of large images, which lower memory consumption. Meanwhile, we propose an enhanced adaptive dense connection super resolution reconstruction network(EDCSR) that has 89M parameters. In AIM2020 Real Image Super-Resolution Challenge, we won the second place in Track 1 and Track 2, and the third place in Track 3.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_26');
INSERT INTO `paper` VALUES (11899, 'Enhanced Quadratic Video Interpolation', 'Video frame interpolation', 'Least squares method', 'Rectified quadratic flow prediction', 'Contextual information', 'Multi-scale fusion', 'With the prosperity of digital video industry, video frame interpolation has arisen continuous attention in computer vision community and become a new upsurge in industry. Many learning-based methods have been proposed and achieved progressive results. Among them, a recent algorithm named quadratic video interpolation (QVI) achieves appealing performance. It exploits higher-order motion information (e.g. acceleration) and successfully models the estimation of interpolated flow. However, its produced intermediate frames still contain some unsatisfactory ghosting, artifacts and inaccurate motion, especially when large and complex motion occurs. In this work, we further improve the performance of QVI from three facets and propose an enhanced quadratic video interpolation (EQVI) model. In particular, we adopt a rectified quadratic flow prediction (RQFP) formulation with least squares method to estimate the motion more accurately. Complementary with image pixel-level blending, we introduce a residual contextual synthesis network (RCSN) to employ contextual information in high-dimensional feature space, which could help the model handle more complicated scenes and motion patterns. Moreover, to further boost the performance, we devise a novel multi-scale fusion network (MS-Fusion) which can be regarded as a learnable augmentation process. The proposed EQVI model won the first place in the AIM2020 Video Temporal Super-Resolution Challenge. Codes are available at https://github.com/lyh-18/EQVI.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_3');
INSERT INTO `paper` VALUES (11900, 'Enhanced Sparse Model for Blind Deblurring', 'Blind deblurring', 'Noise model', 'Enhanced sparse model', '', '', 'Existing arts have shown promising efforts to deal with the blind deblurring task. However, most of the recent works assume the additive noise involved in the blurring process to be simple-distributed (i.e. Gaussian or Laplacian), while the real-world case is proved to be much more complicated. In this paper, we develop a new term to better fit the complex natural noise. Specifically, we use a combination of a dense function (i.e. \\(l_2\\)) and a newly designed enhanced sparse model termed as \\(l_e\\), which is developed from two sparse models (i.e. \\(l_1\\) and \\(l_0\\)), to fulfill the task. Moreover, we further suggest using \\(l_e\\) to regularize image gradients. Compared to the widely-adopted \\(l_0\\) sparse term, \\(l_e\\) can penalize more insignificant image details (Fig. 1). Based on the half-quadratic splitting method, we provide an effective scheme to optimize the overall formulation. Comprehensive evaluations on public datasets and real-world images demonstrate the superiority of the proposed method against state-of-the-art methods in terms of both speed and accuracy.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_38');
INSERT INTO `paper` VALUES (11901, 'Enhancing Robot-Assisted WEEE Disassembly Through Optimizing Automated Detection of Small Components', 'Small object detection', 'WEEE disassembly', '', '', '', 'Automated detection of small objects poses additional challenges, compared to bigger-sized ones, due to the former’s limited resolution for extracting discriminative information. In such cases, even a slight misalignment between a candidate region and its ground truth target has a huge impact on their IoU which significantly increases the amount of noisy information. Given the fact that state of the art two-stage detection algorithms generate predefined shaped and sized candidate regions in pixel-level interval, the aforementioned misalignments are very likely to occur. In this work, a scalable object detection approach is introduced -specifically dedicated to small object parts- incorporating both learnable and handcrafted features. In particular, a set of simplified Gabor waveforms (SGWs) is applied to the raw data, ultimately producing an improved set of anchors for the region proposal network. These Gabor filters are further utilized generating a soft attention mask. Additionally, the interaction of a human with the object is also exploited by taking advantage of affordance-based information for further improvement of detection performance. Experiments have been conducted in a newly introduced device disassembly segmentation dataset, demonstrating the robustness of the method in detection of small device components.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_26');
INSERT INTO `paper` VALUES (11902, 'Entropy Minimisation Framework for Event-Based Vision Model Estimation', 'Event-based vision', 'Optimisation framework', 'Model estimation', 'Entropy Minimisation', '', 'We propose a novel Entropy Minimisation (EMin) framework for event-based vision model estimation. The framework extends previous event-based motion compensation algorithms to handle models whose outputs have arbitrary dimensions. The main motivation comes from estimating motion from events directly in 3D space (e.g. events augmented with depth), without projecting them onto an image plane. This is achieved by modelling the event alignment according to candidate parameters and minimising the resultant dispersion. We provide a family of suitable entropy loss functions and an efficient approximation whose complexity is only linear with the number of events (e.g. the complexity does not depend on the number of image pixels). The framework is evaluated on several motion estimation problems, including optical flow and rotational motion. As proof of concept, we also test our framework on 6-DOF estimation by performing the optimisation directly in 3D space.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_10');
INSERT INTO `paper` VALUES (11903, 'Environment-Agnostic Multitask Learning for Natural Language Grounded Navigation', 'Vision-and-language navigation', 'Natural language grounding', 'Multitask learning', 'Agnostic learning', '', 'Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog . However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. To close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that environment-agnostic multitask learning significantly reduces the performance gap between seen and unseen environments, and the navigation agent trained so outperforms baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH. Our submission to the CVDN leaderboard establishes a new state-of-the-art for the NDH task on the holdout test set. Code is available at https://github.com/google-research/valan.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_25');
INSERT INTO `paper` VALUES (11904, 'EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection', '3D object detection', 'Point cloud', 'Multiple sensors', '', '', 'In this paper, we aim at addressing two critical issues in the 3D detection task, including the exploitation of multiple sensors (namely LiDAR point cloud and camera image), as well as the inconsistency between the localization and classification confidence. To this end, we propose a novel fusion module to enhance the point features with semantic image features in a point-wise manner without any image annotations. Besides, a consistency enforcing loss is employed to explicitly encourage the consistency of both the localization and classification confidence. We design an end-to-end learnable framework named EPNet to integrate these two components. Extensive experiments on the KITTI and SUN-RGBD datasets demonstrate the superiority of EPNet over the state-of-the-art methods. Codes and models are available at: https://github.com/happinesslz/EPNet.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_3');
INSERT INTO `paper` VALUES (11905, 'EPrOD: Evolved Probabilistic Object Detector with Diverse Samples', 'Probabilistic object detector', 'Albedo extraction', 'Monte Carlo method', 'Non-maximum suppression', '', 'Even small errors in object detection algorithms can lead to serious accidents in critical fields such as factories and autonomous vehicles. Thus, a so-called probabilistic object detector (PrOD) has been proposed. However, the PrOD still has problems of underestimating the uncertainty of results and causing biased approximation due to limited information. To solve the above-mentioned problems, this paper proposes a new PrOD composed of four key techniques, i.e., albedo extraction, soft-DropBlock, stacked NMS, and inter-frame processing. In terms of Probability-based Detection Quality (PDQ) metric, the proposed object detector achieved 22.46, which is 4.46 higher than a backbone method, for the Australian Centre for Robotic Vision (ACRV) validation dataset consisting of four videos.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_6');
INSERT INTO `paper` VALUES (11906, 'Erasing Appearance Preservation in Optimization-Based Smoothing', 'Image smoothing', '', '', '', '', 'Optimization-based Image smoothing is routinely formulated as the game between a smoothing energy and an appearance preservation energy. Achieving adequate smoothing is a fundamental goal of these Image smoothing algorithms. We show that partially “erasing” the appearance preservation facilitate adequate Image smoothing. In this paper, we call this manipulation as Erasing Appearance Preservation (EAP). We conduct an user study, allowing users to indicate the “erasing” positions by drawing scribbles interactively, to verify the correctness and effectiveness of EAP. We observe the characteristics of human-indicated “erasing” positions, and then formulate a simple and effective 0-1 knapsack to automatically synthesize the “erasing” positions. We test our synthesized erasing positions in a majority of Image smoothing methods. Experimental results and large-scale perceptual human judgments show that the EAP solution tends to encourage the pattern separation or elimination capabilities of Image smoothing algorithms. We further study the performance of the EAP solution in many image decomposition problems to decompose textures, shadows, and the challenging specular reflections. We also present examinations of diversiform image manipulation applications like texture removal, retexturing, intrinsic decomposition, layer extraction, recoloring, material manipulation, etc. Due to the widespread applicability of Image smoothing, the EAP is also likely to be used in more image editing applications.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_4');
INSERT INTO `paper` VALUES (11907, 'Estimating People Flows to Better Count Them in Crowded Scenes', 'Crowd counting', 'Grid flow model', 'Temporal consistency', '', '', 'Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_43');
INSERT INTO `paper` VALUES (11908, 'ETH-XGaze: A Large Scale Dataset for Gaze Estimation Under Extreme Head Pose and Gaze Variation', '', '', '', '', '', 'Gaze estimation is a fundamental task in many applications of computer vision, human computer interaction and robotics. Many state-of-the-art methods are trained and tested on custom datasets, making comparison across methods challenging. Furthermore, existing gaze estimation datasets have limited head pose and gaze variations, and the evaluations are conducted using different protocols and metrics. In this paper, we propose a new gaze estimation dataset called ETH-XGaze, consisting of over one million high-resolution images of varying gaze under extreme head poses. We collect this dataset from 110 participants with a custom hardware setup including 18 digital SLR cameras and adjustable illumination conditions, and a calibrated system to record ground truth gaze targets. We show that our dataset can significantly improve the robustness of gaze estimation methods across different head poses and gaze angles. Additionally, we define a standardized experimental protocol and evaluation metric on ETH-XGaze, to better unify gaze estimation research going forward. The dataset and benchmark website are available at https://ait.ethz.ch/projects/2020/ETH-XGaze.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_22');
INSERT INTO `paper` VALUES (11909, 'Evaluating Input Perturbation Methods for Interpreting CNNs and Saliency Map Comparison', 'Saliency methods', 'Saliency maps', 'Saliency metrics', 'Perturbation methods', 'Baseline image', 'Input perturbation methods occlude parts of an input to a function and measure the change in the function’s output. Recently, input perturbation methods have been applied to generate and evaluate saliency maps from convolutional neural networks. In practice, neutral baseline images are used for the occlusion, such that the baseline image’s impact on the classification probability is minimal. However, in this paper we show that arguably neutral baseline images still impact the generated saliency maps and their evaluation with input perturbations. We also demonstrate that many choices of hyperparameters lead to the divergence of saliency maps generated by input perturbations. We experimentally reveal inconsistencies among a selection of input perturbation methods and find that they lack robustness for generating saliency maps and for evaluating saliency maps as saliency metrics.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_8');
INSERT INTO `paper` VALUES (11910, 'Event Enhanced High-Quality Image Recovery', 'Event camera', 'Intensity reconstruction', 'Denoising', 'Deblurring', 'Super resolution', 'With extremely high temporal resolution, event cameras have a large potential for robotics and computer vision. However, their asynchronous imaging mechanism often aggravates the measurement sensitivity to noises and brings a physical burden to increase the image spatial resolution. To recover high-quality intensity images, one should address both denoising and super-resolution problems for event cameras. Since events depict brightness changes, with the enhanced degeneration model by the events, the clear and sharp high-resolution latent images can be recovered from the noisy, blurry and low-resolution intensity observations. Exploiting the framework of sparse learning, the events and the low-resolution intensity observations can be jointly considered. Based on this, we propose an explainable network, an event-enhanced sparse learning network (eSL-Net), to recover the high-quality images from event cameras. After training with a synthetic dataset, the proposed eSL-Net can largely improve the performance of the state-of-the-art by 7–12 dB. Furthermore, without additional training process, the proposed eSL-Net can be easily extended to generate continuous frames with frame-rate as high as the events.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_10');
INSERT INTO `paper` VALUES (11911, 'Event-Based Asynchronous Sparse Convolutional Networks', 'Deep Learning: Applications', 'Methodology', 'and Theory', 'Low-level Vision', '', 'Event cameras are bio-inspired sensors that respond to per-pixel brightness changes in the form of asynchronous and sparse “events”. Recently, pattern recognition algorithms, such as learning-based methods, have made significant progress with event cameras by converting events into synchronous dense, image-like representations and applying traditional machine learning methods developed for standard cameras. However, these approaches discard the spatial and temporal sparsity inherent in event data at the cost of higher computational complexity and latency. In this work, we present a general framework for converting models trained on synchronous image-like event representations into asynchronous models with identical output, thus directly leveraging the intrinsic asynchronous and sparse nature of the event data. We show both theoretically and experimentally that this drastically reduces the computational complexity and latency of high-capacity, synchronous neural networks without sacrificing accuracy. In addition, our framework has several desirable characteristics: (i) it exploits spatio-temporal sparsity of events explicitly, (ii) it is agnostic to the event representation, network architecture, and task, and (iii) it does not require any train-time change, since it is compatible with the standard neural networks’ training process. We thoroughly validate the proposed framework on two computer vision tasks: object detection and object recognition. In these tasks, we reduce the computational complexity up to 20 times with respect to high-latency neural networks. At the same time, we outperform state-of-the-art asynchronous approaches up to \\(24\\%\\) in prediction accuracy.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_25');
INSERT INTO `paper` VALUES (11912, 'Every Pixel Matters: Center-Aware Feature Alignment for Domain Adaptive Object Detector', '', '', '', '', '', 'A domain adaptive object detector aims to adapt itself to unseen domains that may contain variations of object appearance, viewpoints or backgrounds. Most existing methods adopt feature alignment either on the image level or instance level. However, image-level alignment on global features may tangle foreground/background pixels at the same time, while instance-level alignment using proposals may suffer from the background noise. Different from existing solutions, we propose a domain adaptation framework that accounts for each pixel via predicting pixel-wise objectness and centerness. Specifically, the proposed method carries out center-aware alignment by paying more attention to foreground pixels, hence achieving better adaptation across domains. We demonstrate our method on numerous adaptation settings with extensive experimental results and show favorable performance against existing state-of-the-art algorithms. Source codes and models are available at https://github.com/chengchunhsu/EveryPixelMatters.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_42');
INSERT INTO `paper` VALUES (11913, 'Example-Guided Image Synthesis Using Masked Spatial-Channel Attention and Self-supervision', 'Example-guided image synthesis', 'Self-supervised learning', 'Correspondence modeling', 'Efficient attention', '', 'Example-guided image synthesis has recently been attempted to synthesize an image from a semantic label map and an exemplary image. In the task, the additional exemplar image provides the style guidance that controls the appearance of the synthesized output. Despite the controllability advantage, the existing models are designed on datasets with specific and roughly aligned objects. In this paper, we tackle a more challenging and general task, where the exemplar is a scene image that is semantically different from the given label map. To this end, we first propose a Masked Spatial-Channel Attention (MSCA) module which models the correspondence between two scenes via efficient decoupled attention. Next, we propose an end-to-end network for joint global and local feature alignment and synthesis. Finally, we propose a novel self-supervision task to enable training. Experiments on the large-scale and more diverse COCO-stuff dataset show significant improvements over the existing methods. Moreover, our approach provides interpretability and can be readily extended to other content manipulation tasks including style and spatial interpolation or extrapolation.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_25');
INSERT INTO `paper` VALUES (11914, 'Exchangeable Deep Neural Networks for Set-to-Set Matching and Learning', 'Set to set matching', 'Deep learning', 'Permutation invariance', '', '', 'Matching two different sets of items, called heterogeneous set-to-set matching problem, has recently received attention as a promising problem. The difficulties are to extract features to match a correct pair of different sets and also preserve two types of exchangeability required for set-to-set matching: the pair of sets, as well as the items in each set, should be exchangeable. In this study, we propose a novel deep learning architecture to address the abovementioned difficulties and also an efficient training framework for set-to-set matching. We evaluate the methods through experiments based on two industrial applications: fashion set recommendation and group re-identification. In these experiments, we show that the proposed method provides significant improvements and results compared with the state-of-the-art methods, thereby validating our architecture for the heterogeneous set matching problem.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_37');
INSERT INTO `paper` VALUES (11915, 'ExchNet: A Unified Hashing Network for Large-Scale Fine-Grained Image Retrieval', 'Fine-Grained Image Retrieval', 'Learning to hash', 'Feature alignment', 'Large-scale image search', '', 'Retrieving content relevant images from a large-scale fine-grained dataset could suffer from intolerably slow query speed and highly redundant storage cost, due to high-dimensional real-valued embeddings which aim to distinguish subtle visual differences of fine-grained objects. In this paper, we study the novel fine-grained hashing topic to generate compact binary codes for fine-grained images, leveraging the search and storage efficiency of hash learning to alleviate the aforementioned problems. Specifically, we propose a unified end-to-end trainable network, termed as ExchNet. Based on attention mechanisms and proposed attention constraints, ExchNet can firstly obtain both local and global features to represent object parts and the whole fine-grained objects, respectively. Furthermore, to ensure the discriminative ability and semantic meaning’s consistency of these part-level features across images, we design a local feature alignment approach by performing a feature exchanging operation. Later, an alternating learning algorithm is employed to optimize the whole ExchNet and then generate the final binary hash codes. Validated by extensive experiments, our ExchNet consistently outperforms state-of-the-art generic hashing methods on five fine-grained datasets. Moreover, compared with other approximate nearest neighbor methods, ExchNet achieves the best speed-up and storage reduction, revealing its efficiency and practicality.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_12');
INSERT INTO `paper` VALUES (11916, 'Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition', 'Face recognition', 'Knowledge distillation', 'Weight exclusivity', 'Feature consistency', '', 'Knowledge distillation is an effective tool to compress large pre-trained Convolutional Neural Networks (CNNs) or their ensembles into models applicable to mobile and embedded devices. The success of which mainly comes from two aspects: the designed student network and the exploited knowledge. However, current methods usually suffer from the low-capability of mobile-level student network and the unsatisfactory knowledge for distillation. In this paper, we propose a novel position-aware exclusivity to encourage large diversity among different filters of the same layer to alleviate the low-capability of student network. Moreover, we investigate the effect of several prevailing knowledge for face recognition distillation and conclude that the knowledge of feature consistency is more flexible and preserves much more information than others. Experiments on a variety of face recognition benchmarks have revealed the superiority of our method over the state-of-the-arts.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_20');
INSERT INTO `paper` VALUES (11917, 'Expanding CNN-Based Plant Phenotyping Systems to Larger Environments', 'Plant phenotyping', 'Convolutional Neural Networks', 'Integration of domain-specific knowledge', '', '', 'Plant phenotyping systems strive to maintain high categorization accuracy when expanding their scopes to larger environments. In this paper, we discuss problems associated with expanding the plant categorization scope. These problems are particularly complicated due to the increase in the number of species and the inter-species similarity. In our approach, we modify previously trained Convolutional Neural Networks (CNNs) and integrate domain-specific knowledge in the fine-tuning process of these models to maintain high accuracy while expanding the scope. This process is the key idea behind our CNN-based expanding approach resulting in plant-expert models. Experiments described in this paper compare the accuracy of an expanded phenotyping system using different plant-related datasets during the training of the CNN categorization models. Although it takes much longer to train these models, our approach achieves better performance compared to models trained without the integration of domain-specific knowledge, especially when the number of species increases significantly.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_29');
INSERT INTO `paper` VALUES (11918, 'Explainable Face Recognition', '', '', '', '', '', 'Explainable face recognition (XFR) is the problem of explaining the matches returned by a facial matcher, in order to provide insight into why a probe was matched with one identity over another. In this paper, we provide the first comprehensive benchmark and baseline evaluation for XFR. We define a new evaluation protocol called the “inpainting game”, which is a curated set of 3648 triplets (probe, mate, nonmate) of 95 subjects, which differ by synthetically inpainting a chosen facial characteristic like the nose, eyebrows or mouth creating an inpainted nonmate. An XFR algorithm is tasked with generating a network attention map which best explains which regions in a probe image match with a mated image, and not with an inpainted nonmate for each triplet. This provides ground truth for quantifying what image regions contribute to face matching. Finally, we provide a comprehensive benchmark on this dataset comparing five state-of-the-art XFR algorithms on three facial matchers. This benchmark includes two new algorithms called subtree EBP and Density-based Input Sampling for Explanation (DISE) which outperform the state-of-the-art XFR by a wide margin.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_15');
INSERT INTO `paper` VALUES (11919, 'Explaining Image Classifiers Using Statistical Fault Localization', 'Deep learning', 'Explainability', 'Statistical fault localization', 'Software testing', '', 'The black-box nature of deep neural networks (DNNs) makes it impossible to understand why a particular output is produced, creating demand for “Explainable AI”. In this paper, we show that statistical fault localization (SFL) techniques from software engineering deliver high quality explanations of the outputs of DNNs, where we define an explanation as a minimal subset of features sufficient for making the same decision as for the original input. We present an algorithm and a tool called DeepCover, which synthesizes a ranking of the features of the inputs using SFL and constructs explanations for the decisions of the DNN based on this ranking. We compare explanations produced by DeepCover with those of the state-of-the-art tools gradcam, lime, shap, rise and extremal and show that explanations generated by DeepCover are consistently better across a broad set of experiments. On a benchmark set with known ground truth, DeepCover achieves \\(76.7\\%\\) accuracy, which is \\(6\\%\\) better than the second best extremal.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_24');
INSERT INTO `paper` VALUES (11920, 'Explanation-Based Weakly-Supervised Learning of Visual Relations with Graph Networks', '', '', '', '', '', 'Visual relationship detection is fundamental for holistic image understanding. However, the localization and classification of (subject, predicate, object) triplets remain challenging tasks, due to the combinatorial explosion of possible relationships, their long-tailed distribution in natural images, and an expensive annotation process.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_37');
INSERT INTO `paper` VALUES (11921, 'Exploiting 3D Hand Pose Estimation in Deep Learning-Based Sign Language Recognition from RGB Videos', 'Sign language recognition', '3D hand pose', '2D body skeleton', 'Attention-based encoder-decoder', 'Convolutional neural network', 'In this paper, we investigate the benefit of 3D hand skeletal information to the task of sign language (SL) recognition from RGB videos, within a state-of-the-art, multiple-stream, deep-learning recognition system. As most SL datasets are available in traditional RGB-only video lacking depth information, we propose to infer 3D coordinates of the hand joints from RGB data via a powerful architecture that has been primarily introduced in the literature for the task of 3D human pose estimation. We then fuse these estimates with additional SL informative streams, namely 2D skeletal data, as well as convolutional neural network-based hand- and mouth-region representations, and employ an attention-based encoder-decoder for recognition. We evaluate our proposed approach on a corpus of isolated signs of Greek SL and a dataset of continuous finger-spelling in American SL, reporting significant gains by the inclusion of 3D hand pose information, while also outperforming the state-of-the-art on both databases. Further, we evaluate the 3D hand pose estimation technique as standalone.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_18');
INSERT INTO `paper` VALUES (11922, 'Exploiting Deep Generative Prior for Versatile Image Restoration and Manipulation', '', '', '', '', '', 'Learning a good image prior is a long-term goal for image restoration and manipulation. While existing methods like deep image prior (DIP) capture low-level image statistics, there are still gaps toward an image prior that captures rich image semantics including color, spatial coherence, textures, and high-level concepts. This work presents an effective way to exploit the image prior captured by a generative adversarial network (GAN) trained on large-scale natural images. As shown in Fig. 1, the deep generative prior (DGP) provides compelling results to restore missing semantics, e.g., color, patch, resolution, of various degraded images. It also enables diverse image manipulation including random jittering, image morphing, and category transfer. Such highly flexible effects are made possible through relaxing the assumption of existing GAN-inversion methods, which tend to fix the generator. Notably, we allow the generator to be fine-tuned on-the-fly in a progressive manner regularized by feature distance obtained by the discriminator in GAN. We show that these easy-to-implement and practical changes help preserve the reconstruction to remain in the manifold of nature image, and thus lead to more precise and faithful reconstruction for real images. Code is at https://github.com/XingangPan/deep-generative-prior.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_16');
INSERT INTO `paper` VALUES (11923, 'Exploiting Scene-Specific Features for Object Goal Navigation', 'Visual Navigation', 'ObjectGoal Navigation', 'Reinforcement Learning', '', '', 'Can the intrinsic relation between an object and the room in which it is usually located help agents in the Visual Navigation Task? We study this question in the context of Object Navigation, a problem in which an agent has to reach an object of a specific class while moving in a complex domestic environment. In this paper, we introduce a new reduced dataset that speeds up the training of navigation models, a notoriously complex task. Our proposed dataset permits the training of models that do not exploit online-built maps in reasonable times even without the use of huge computational resources. Therefore, this reduced dataset guarantees a significant benchmark and it can be used to identify promising models that could be then tried on bigger and more challenging datasets. Subsequently, we propose the SMTSC model, an attention-based model capable of exploiting the correlation between scenes and objects contained in them, highlighting quantitatively how the idea is correct.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_24');
INSERT INTO `paper` VALUES (11924, 'Exploiting Temporal Coherence for Self-Supervised One-Shot Video Re-identification', 'Video person re-identification', 'Temporal consistency', 'One-shot learning', 'Semi-supervised learning', '', 'While supervised techniques in re-identification are extremely effective, the need for large amounts of annotations makes them impractical for large camera networks. One-shot re-identification, which uses a singular labeled tracklet for each identity along with a pool of unlabeled tracklets, is a potential candidate towards reducing this labeling effort. Current one-shot re-identification methods function by modeling the inter-relationships amongst the labeled and the unlabeled data, but fail to fully exploit such relationships that exist within the pool of unlabeled data itself. In this paper, we propose a new framework named Temporal Consistency Progressive Learning, which uses temporal coherence as a novel self-supervised auxiliary task in the one-shot learning paradigm to capture such relationships amongst the unlabeled tracklets. Optimizing two new losses, which enforce consistency on a local and global scale, our framework can learn richer and more discriminative representations. Extensive experiments on two challenging video re-identification datasets - MARS and DukeMTMC-VideoReID - demonstrate that our proposed method is able to estimate the true labels of the unlabeled data more accurately by up to \\(8\\%\\), and obtain significantly better re-identification performance compared to the existing state-of-the-art techniques .', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_16');
INSERT INTO `paper` VALUES (11925, 'Exploring Effective Methods to Improve the Performance of Tiny Object Detection', 'Pedestrian detection', 'Tiny Object Detection', '', '', '', 'In this paper, we present our solution of the 1st Tiny Object Detection (TOD) Challenge. The purpose of the challenge is to detect tiny person objects (2–20 pixels) in large-scale images. Due to the extreme small object size and low signal-to-noise ratio, the detection of tiny objects is much more challenging than objects in other datasets such as COCO and CityPersons. Based on Faster R-CNN, we explore some effective and general methods to improve the detection performance of tiny objects. Since the model architectures will not be changed, these methods are easy to implement. Accordingly, we obtain the 2nd place with the \\(A P_{50}^{{\\text {tiny}}}\\) score of 71.53 in the challenge.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_25');
INSERT INTO `paper` VALUES (11926, 'Expressive Telepresence via Modular Codec Avatars', 'Virtual reality', 'Telepresence', 'Codec Avatar', '', '', 'VR telepresence consists of interacting with another human in a virtual space represented by an avatar. Today most avatars are cartoon-like, but soon the technology will allow video-realistic ones. This paper aims in this direction, and presents Modular Codec Avatars (MCA), a method to generate hyper-realistic faces driven by the cameras in the VR headset. MCA extends traditional Codec Avatars (CA) by replacing the holistic models with a learned modular representation. It is important to note that traditional person-specific CAs are learned from few training samples, and typically lack robustness as well as limited expressiveness when transferring facial expressions. MCAs solve these issues by learning a modulated adaptive blending of different facial components as well as an exemplar-based latent alignment. We demonstrate that MCA achieves improved expressiveness and robustness w.r.t to CA in a variety of real-world datasets and practical scenarios. Finally, we showcase new applications in VR telepresence enabled by the proposed model.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_20');
INSERT INTO `paper` VALUES (11927, 'Extending and Analyzing Self-supervised Learning Across Domains', '', '', '', '', '', 'Self-supervised representation learning has achieved impressive results in recent years, with experiments primarily coming on ImageNet or other similarly large internet imagery datasets. There has been little to no work with these methods on other smaller domains, such as satellite, textural, or biological imagery. We experiment with several popular methods on an unprecedented variety of domains. We discover, among other findings, that Rotation is the most semantically meaningful task, while much of the performance of Jigsaw is attributable to the nature of its induced distribution rather than semantic understanding. Additionally, there are several areas, such as fine-grain classification, where all tasks underperform. We quantitatively and qualitatively diagnose the reasons for these failures and successes via novel experiments studying pretext generalization, random labelings, and implicit dimensionality. Code and models are available at https://github.com/BramSW/Extending_SSRL_Across_Domains/.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_43');
INSERT INTO `paper` VALUES (11928, 'Extract and Merge: Superpixel Segmentation with Regional Attributes', 'Superpixel', 'Power-window', 'Boundary clearness', 'Graph model', '', 'For a certain object in an image, the relationship between its central region and the peripheral region is not well utilized in existing superpixel segmentation methods. In this work, we propose the concept of regional attribute, which indicates the location of a certain region in the object. Based on the regional attributes, we propose a novel superpixel method called Extract and Merge (EAM). In the extracting stage, we design square windows with a side length of a power of two, named power-window, to extract regional attributes by calculating boundary clearness of objects in the window. The larger windows are for the central regions and the smaller ones correspond to the peripheral regions. In the merging stage, power-windows are merged according to the defined attraction between them. Specifically, we build a graph model and propose an efficient method to make the large windows merge the small ones strategically, regarding power-windows as vertices and the adjacencies between them as edges. We demonstrate that our superpixels have fine boundaries and are superior to the respective state-of-the-art algorithms on multiple benchmarks.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_10');
INSERT INTO `paper` VALUES (11929, 'Eyeglasses 3D Shape Reconstruction from a Single Face Image', '', '', '', '', '', 'A complete 3D face reconstruction requires to explicitly model the eyeglasses on the face, which is less investigated in the literature. In this paper, we present an automatic system that recovers the 3D shape of eyeglasses from a single face image with an arbitrary head pose. To achieve this goal, we first trains a neural network to jointly perform glasses landmark detection and segmentation, which carry the sparse and dense glasses shape information respectively for 3D glasses pose estimation and shape recovery. To solve the ambiguity in 2D to 3D reconstruction, our system fully explores the prior knowledge including the relative motion constraint between face and glasses and the planar and symmetric shape prior feature of glasses. From the qualitative and quantitative experiments, we see that our system reconstructs promising 3D shapes of eyeglasses for various poses.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_23');
INSERT INTO `paper` VALUES (11930, 'EyeSeg: Fast and Efficient Few-Shot Semantic Segmentation', 'Semantic segmentation', 'Eye tracking', 'Computer vision', 'OpenEDS2020', '', 'Semantic segmentation is a key component in eye- and gaze- tracking for virtual reality (VR) and augmented reality (AR) applications. While it is a well-studied computer vision problem, most state-of-the-art models require large amounts of labeled data, which is limited in this specific domain. An additional consideration in eye tracking is the capacity for real-time predictions, necessary for responsive AR/VR interfaces. In this work, we propose EyeSeg, an encoder-decoder architecture designed for accurate pixel-wise few-shot semantic segmentation with limited annotated data. We report results from the OpenEDS2020 Challenge, yielding a 94.5% mean Intersection Over Union (mIOU) score, which is a 10.5% score increase over the baseline approach. The experimental results demonstrate state-of-the-art performance while preserving a low latency framework. Source code is available: http://www.cs.utsa.edu/~fernandez/segmentation.html.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_37');
INSERT INTO `paper` VALUES (11931, 'Face Anti-Spoofing via Disentangled Representation Learning', 'Face anti-spoofing', 'Generative model', 'Disentangled representation', '', '', 'Face anti-spoofing is crucial to security of face recognition systems. Previous approaches focus on developing discriminative models based on the features extracted from images, which may be still entangled between spoof patterns and real persons. In this paper, motivated by the disentangled representation learning, we propose a novel perspective of face anti-spoofing that disentangles the liveness features and content features from images, and the liveness features is further used for classification. We also put forward a Convolutional Neural Network (CNN) architecture with the process of disentanglement and combination of low-level and high-level supervision to improve the generalization capabilities. We evaluate our method on public benchmark datasets and extensive experimental results demonstrate the effectiveness of our method against the state-of-the-art competitors. Finally, we further visualize some results to help understand the effect and advantage of disentanglement.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_38');
INSERT INTO `paper` VALUES (11932, 'Face Anti-Spoofing with Human Material Perception', 'Face anti-spoofing', 'Material perception', 'Bilateral filtering', '', '', 'Face anti-spoofing (FAS) plays a vital role in securing the face recognition systems from presentation attacks. Most existing FAS methods capture various cues (e.g., texture, depth and reflection) to distinguish the live faces from the spoofing faces. All these cues are based on the discrepancy among physical materials (e.g., skin, glass, paper and silicone). In this paper we rephrase face anti-spoofing as a material recognition problem and combine it with classical human material perception, intending to extract discriminative and robust features for FAS. To this end, we propose the Bilateral Convolutional Networks (BCN), which is able to capture intrinsic material-based patterns via aggregating multi-level bilateral macro- and micro- information. Furthermore, Multi-level Feature Refinement Module (MFRM) and multi-head supervision are utilized to learn more robust features. Comprehensive experiments are performed on six benchmark datasets, and the proposed method achieves superior performance on both intra- and cross-dataset testings. One highlight is that we achieve overall 11.3 ± 9.5% EER for cross-type testing in SiW-M dataset, which significantly outperforms previous results. We hope this work will facilitate future cooperation between FAS and material communities.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_33');
INSERT INTO `paper` VALUES (11933, 'Face Mask Invariant End-to-End Face Recognition', 'Face mask', 'Face recognition', 'End-to-end network', 'Face alignment', '', 'This paper introduces an end-to-end face recognition network that is invariant to face images with face masks. Conventional face recognition networks have degraded performance on images with face masks due to inaccurate landmark prediction and alignment results. Thus, an end-to-end network is proposed to solve the problem. We generate face mask synthesized datasets by properly aligning the face mask to images on available public datasets, such as CASIA-Webface, LFW, CALFW, CPLFW, and CFP. Then, we utilize those datasets as training and testing datasets. Second, we introduce a network that contains two modules: alignment and feature extraction modules. These modules are trained end-to-end, which makes the network invariant to face images with a face mask. Experimental results show that the proposed method achieves significant improvement from state-of-the-art face recognition network in face mask synthesized datasets.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_19');
INSERT INTO `paper` VALUES (11934, 'Face Super-Resolution Guided by 3D Facial Priors', 'Face super-resolution', '3D facial priors', 'Facial structures and identity knowledge', '', '', 'State-of-the-art face super-resolution methods employ deep convolutional neural networks to learn a mapping between low- and high-resolution facial patterns by exploring local appearance knowledge. However, most of these methods do not well exploit facial structures and identity information, and struggle to deal with facial images that exhibit large pose variations. In this paper, we propose a novel face super-resolution method that explicitly incorporates 3D facial priors which grasp the sharp facial structures. Our work is the first to explore 3D morphable knowledge based on the fusion of parametric descriptions of face attributes (e.g., identity, facial expression, texture, illumination, and face pose). Furthermore, the priors can easily be incorporated into any network and are extremely efficient in improving the performance and accelerating the convergence speed. Firstly, a 3D face rendering branch is set up to obtain 3D priors of salient facial structures and identity knowledge. Secondly, the Spatial Attention Module is used to better exploit this hierarchical information (i.e., intensity similarity, 3D facial structure, and identity content) for the super-resolution problem. Extensive experiments demonstrate that the proposed 3D priors achieve superior face super-resolution results over the state-of-the-arts.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_44');
INSERT INTO `paper` VALUES (11935, 'Face-Image Source Generator Identification', 'Image source identification', 'Image forensics', 'Fake vs Real', 'Auto-encoder', 'Generative adversarial networks', 'Recent advances in deep networks and specifically, Generative Adversarial Networks, have introduced new ways of manipulating and synthesizing “fake” images. Concerns have been raised as to the sinister use of these images, and accordingly challenges have been raised to detect “fake” from “real” images. In this study we address a slightly different problem in image forensics. Rather than discriminating real from fake, we attempt to perform “Source Generator Identification”, i.e. determine the source generator of the synthesized image. In this study we focus on face images. We exploit the specific characteristics associated with each fake face image generator and introduce a face generator representation space (the profile space) which allows a study of the distribution of face generators, their distinctions as well as allows estimating probability of images arising from the same generator.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_37');
INSERT INTO `paper` VALUES (11936, 'Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search', 'Differentiable neural architecture search', 'Image classification', 'Failure of DARTS', '', '', 'Differentiable Architecture Search (DARTS) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair DARTS where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation’s architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on CIFAR-10 and ImageNet (Code is available here: https://github.com/xiaomi-automl/FairDARTS).', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_28');
INSERT INTO `paper` VALUES (11937, 'Fair Face Recognition Using Data Balancing, Enhancement and Fusion', 'Racial bias', 'Face detector', 'Data re-sampling', 'Data enhancement', 'Linear-combination strategy', 'Racial bias is an important issue in biometrics, while has not been thoroughly studied in deep face recognition. By reducing the influence of gender and skin colour, this paper proposes a fair face recognition system with low bias. First, multiple preprocessing methods are added to improve the dual shot face detector for obtaining target face from a given test image. Then, a data re-sampling approach is employed to balance the data distribution and reduce the bias based on the analysis of training data. Moreover, multiple data enhancement methods are used to increase the accuracy performance. Finally, a linear-combination strategy is adopted to benefit from mutil-model fusion. ChaLearn Looking at People Fair Face Recognition challenge is supported by ECCV 2020. Our team (ustc-nelslip) ranked 1st in the development stage and 2nd in the test stage of this challenge. The code is available at https://github.com/HaoSir/ECCV-2020-Fair-Face-Recognition-challenge_2nd_place_solution-ustc-nelslip-.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_34');
INSERT INTO `paper` VALUES (11938, 'FairALM: Augmented Lagrangian Method for Training Fair Models with Little Regret', '', '', '', '', '', 'Algorithmic decision making based on computer vision and machine learning methods continues to permeate our lives. But issues related to biases of these models and the extent to which they treat certain segments of the population unfairly, have led to legitimate concerns. There is agreement that because of biases in the datasets we present to the models, a fairness-oblivious training will lead to unfair models. An interesting topic is the study of mechanisms via which the de novo design or training of the model can be informed by fairness measures. Here, we study strategies to impose fairness concurrently while training the model. While many fairness based approaches in vision rely on training adversarial modules together with the primary classification/regression task, in an effort to remove the influence of the protected attribute or variable, we show how ideas based on well-known optimization concepts can provide a simpler alternative. In our proposal, imposing fairness just requires specifying the protected attribute and utilizing our routine. We provide a detailed technical analysis and present experiments demonstrating that various fairness measures can be reliably imposed on a number of training tasks in vision in a manner that is interpretable.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_22');
INSERT INTO `paper` VALUES (11939, 'FairFace Challenge at ECCV 2020: Analyzing Bias in Face Recognition', 'Face verification', 'Face recognition', 'Fairness', 'Bias', '', 'This work summarizes the 2020 ChaLearn Looking at People Fair Face Recognition and Analysis Challenge and provides a description of the top-winning solutions and analysis of the results. The aim of the challenge was to evaluate accuracy and bias in gender and skin colour of submitted algorithms on the task of 1:1 face verification in the presence of other confounding attributes. Participants were evaluated using an in-the-wild dataset based on reannotated IJB-C, further enriched 12.5K new images and additional labels. The dataset is not balanced, which simulates a real world scenario where AI-based models supposed to present fair outcomes are trained and evaluated on imbalanced data. The challenge attracted 151 participants, who made more 1.8K submissions in total. The final phase of the challenge attracted 36 active teams out of which 10 exceeded 0.999 AUC-ROC while achieving very low scores in the proposed bias metrics. Common strategies by the participants were face pre-processing, homogenization of data distributions, the use of bias aware loss functions and ensemble models. The analysis of top-10 teams shows higher false positive rates (and lower false negative rates) for females with dark skin tone as well as the potential of eyeglasses and young age to increase the false positive rates too.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_32');
INSERT INTO `paper` VALUES (11940, 'Fairness by Learning Orthogonal Disentangled Representations', 'Representation learning', 'Disentangled representation', 'Fairness in machine learning', '', '', 'Learning discriminative powerful representations is a crucial step for machine learning systems. Introducing invariance against arbitrary nuisance or sensitive attributes while performing well on specific tasks is an important problem in representation learning. This is mostly approached by purging the sensitive information from learned representations. In this paper, we propose a novel disentanglement approach to invariant representation problem. We disentangle the meaningful and sensitive representations by enforcing orthogonality constraints as a proxy for independence. We explicitly enforce the meaningful representation to be agnostic to sensitive information by entropy maximization. The proposed approach is evaluated on five publicly available datasets and compared with state of the art methods for learning fairness and invariance achieving the state of the art performance on three datasets and comparable performance on the rest. Further, we perform an ablative study to evaluate the effect of each component.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_44');
INSERT INTO `paper` VALUES (11941, 'FamilyGAN: Generating Kin Face Images Using Generative Adversarial Networks', 'Kinship', 'Image generation', 'Generative adversarial networks', 'Deep learning', '', 'Automatic kinship verification using face images involves analyzing features and computing similarities between two input images to establish kin-relationship. It has gained significant interest from the research community and several approaches including deep learning architectures are proposed. One of the law enforcement applications of kinship analysis involves predicting the kin image given an input image. In other words, the question posed here is: “given an input image, can we generate a kin-image?” This paper attempts to generate kin-images using Generative Adversarial Learning for multiple kin-relations. The proposed FamilyGAN model incorporates three information, kin-gender, kinship loss, and reconstruction loss, in a GAN model to generate kin images. FamilyGAN is the first model capable of generating kin-images for multiple relations such as parent-child and siblings from a single model. On the WVU Kinship Video database, the proposed model shows very promising results for generating kin images. Experimental results show 71.34% kinship verification accuracy using the images generated via FamilyGAN.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_18');
INSERT INTO `paper` VALUES (11942, 'FAN: Frequency Aggregation Network for Real Image Super-Resolution', 'Frequency aggregation network (FAN)', 'Real image super-resoltion (RealSR)', 'AIM 2020 challenge', '', '', 'Single image super-resolution (SISR) aims to recover the high-resolution (HR) image from its low-resolution (LR) input image. With the development of deep learning, SISR has achieved great progress. However, It is still a challenge to restore the real-world LR image with complicated authentic degradations. Therefore, we propose FAN, a frequency aggregation network, to address the real-world image super-resolu-tion problem. Specifically, we extract different frequencies of the LR image and pass them to a channel attention-grouped residual dense network (CA-GRDB) individually to output corresponding feature maps. And then aggregating these residual dense feature maps adaptively to recover the HR image with enhanced details and textures. We conduct extensive experiments quantitatively and qualitatively to verify that our FAN performs well on the real image super-resolution task of AIM 2020 challenge. According to the released final results, our team SR-IM achieves the fourth place on the X4 track with PSNR of 31.1735 and SSIM of 0.8728.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_28');
INSERT INTO `paper` VALUES (11943, 'Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards', 'Fashion', 'Captioning', 'Reinforcement Learning', 'Semantics', '', 'Generating accurate descriptions for online fashion items is important not only for enhancing customers’ shopping experiences, but also for the increase of online sales. Besides the need of correctly presenting the attributes of items, the expressions in an enchanting style could better attract customer interests. The goal of this work is to develop a novel learning framework for accurate and expressive fashion captioning. Different from popular work on image captioning, it is hard to identify and describe the rich attributes of fashion items. We seed the description of an item by first identifying its attributes, and introduce attribute-level semantic (ALS) reward and sentence-level semantic (SLS) reward as metrics to improve the quality of text descriptions. We further integrate the training of our model with maximum likelihood estimation (MLE), attribute embedding, and Reinforcement Learning (RL). To facilitate the learning, we build a new FAshion CAptioning Dataset (FACAD), which contains 993K images and 130K corresponding enchanting and diverse descriptions. Experiments on FACAD demonstrate the effectiveness of our model (Code and data: https://github.com/xuewyang/Fashion_Captioning).', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_1');
INSERT INTO `paper` VALUES (11944, 'Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset', 'Dataset', 'Ontology', 'Instance segmentation', 'Fine-grained', 'Attribute', 'In this work we explore the task of instance segmentation with attribute localization, which unifies instance segmentation (detect and segment each object instance) and fine-grained visual attribute categorization (recognize one or multiple attributes). The proposed task requires both localizing an object and describing its properties. To illustrate the various aspects of this task, we focus on the domain of fashion and introduce Fashionpedia as a step toward mapping out the visual aspects of the fashion world. Fashionpedia consists of two parts: (1) an ontology built by fashion experts containing 27 main apparel categories, 19 apparel parts, 294 fine-grained attributes and their relationships; (2) a dataset with everyday and celebrity event fashion images annotated with segmentation masks and their associated per-mask fine-grained attributes, built upon the Fashionpedia ontology. In order to solve this challenging task, we propose a novel Attribute-Mask R-CNN model to jointly perform instance segmentation and localized attribute recognition, and provide a novel evaluation metric for the task. Fashionpedia is available at: https://fashionpedia.github.io/home/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_19');
INSERT INTO `paper` VALUES (11945, 'Fast Adaptation to Super-Resolution Networks via Meta-learning', 'Deep learning', 'Meta-learning', 'Single-image super-resolution', 'Patch recurrence', '', 'Conventional supervised super-resolution (SR) approaches are trained with massive external SR datasets but fail to exploit desirable properties of the given test image. On the other hand, self-supervised SR approaches utilize the internal information within a test image but suffer from computational complexity in run-time. In this work, we observe the opportunity for further improvement of the performance of single-image super-resolution (SISR) without changing the architecture of conventional SR networks by practically exploiting additional information given from the input image. In the training stage, we train the network via meta-learning; thus, the network can quickly adapt to any input image at test time. Then, in the test stage, parameters of this meta-learned network are rapidly fine-tuned with only a few iterations by only using the given low-resolution image. The adaptation at the test time takes full advantage of patch-recurrence property observed in natural images. Our method effectively handles unknown SR kernels and can be applied to any existing model. We demonstrate that the proposed model-agnostic approach consistently improves the performance of conventional SR networks on various benchmark SR datasets.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_45');
INSERT INTO `paper` VALUES (11946, 'Fast Bi-Layer Neural Synthesis of One-Shot Realistic Head Avatars', 'Neural avatars', 'Talking heads', 'Neural rendering', 'Head synthesis', 'Head animation', 'We propose a neural rendering-based system that creates head avatars from a single photograph. Our approach models a person’s appearance by decomposing it into two layers. The first layer is a pose-dependent coarse image that is synthesized by a small neural network. The second layer is defined by a pose-independent texture image that contains high-frequency details. The texture image is generated offline, warped and added to the coarse image to ensure a high effective resolution of synthesized head views. We compare our system to analogous state-of-the-art systems in terms of visual quality and speed. The experiments show significant inference speedup over previous neural head avatar models for a given visual quality. We also report on a real-time smartphone-based implementation of our system.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_31');
INSERT INTO `paper` VALUES (11947, 'Fast Light-Weight Network for Extreme Image Inpainting Challenge', 'Inpainting', 'Fast and light-weight model', 'Equilibrium convolution', 'Mask-wise Gated Convolution', '', 'Image inpainting has a wide range of applications. However, to this challenge existing inpainting models that usually have a large model size can hardly run fast, as memory and supported operations are much limited. In this paper, we propose a novel light-weight inpainting model in which we design three novel operations named Equilibrium Conv Mask-wise Gated Conv, Difference Conv and define a new loss function based on SN-patchGAN. In specific, the incorporation of Equilibrium Conv and Mask-wise Gated Conv not only reduces the model size and improve the efficiency, but also keeps comparative performance. For Difference Conv, it is benefit to handle big mask problem. Moreover, our proposed loss results in a better performance in recovering images with rich textures. Experimental results demonstrate our model is 1.43\\(\\times \\) speeding up and reduces the size by 2.37\\(\\times \\) compared with the state-of-the-art model.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_44');
INSERT INTO `paper` VALUES (11948, 'Fast Video Object Segmentation Using the Global Context Module', 'Video object segmentation', 'Global context module', '', '', '', 'We developed a real-time, high-quality semi-supervised video object segmentation algorithm. Its accuracy is on par with the most accurate, time-consuming online-learning model, while its speed is similar to the fastest template-matching method with sub-optimal accuracy. The core component of the model is a novel global context module that effectively summarizes and propagates information through the entire video. Compared to previous approaches that only use one frame or a few frames to guide the segmentation of the current frame, the global context module uses all past frames. Unlike the previous state-of-the-art space-time memory network that caches a memory at each spatio-temporal position, the global context module uses a fixed-size feature representation. Therefore, it uses constant memory regardless of the video length and costs substantially less memory and computation. With the novel module, our model achieves top performance on standard benchmarks at a real-time speed.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_43');
INSERT INTO `paper` VALUES (11949, 'Faster AutoAugment: Learning Augmentation Strategies Using Backpropagation', '', '', '', '', '', 'Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as a differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented and original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior methods without a performance drop.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_1');
INSERT INTO `paper` VALUES (11950, 'Faster Person Re-identification', '', '', '', '', '', 'Fast person re-identification (ReID) aims to search person images quickly and accurately. The main idea of recent fast ReID methods is the hashing algorithm, which learns compact binary codes and performs fast Hamming distance and counting sort. However, a very long code is needed for high accuracy (e.g. 2048), which compromises search speed. In this work, we introduce a new solution for fast ReID by formulating a novel Coarse-to-Fine (CtF) hashing code search strategy, which complementarily uses short and long codes, achieving both faster speed and better accuracy. It uses shorter codes to coarsely rank broad matching similarities and longer codes to refine only a few top candidates for more accurate instance ReID. Specifically, we design an All-in-One (AiO) framework together with a Distance Threshold Optimization (DTO) algorithm. In AiO, we simultaneously learn and enhance multiple codes of different lengths in a single model. It learns multiple codes in a pyramid structure, and encourage shorter codes to mimic longer codes by self-distillation. DTO solves a complex threshold search problem by a simple optimization process, and the balance between accuracy and speed is easily controlled by a single parameter. It formulates the optimization target as a \\(F_{\\beta }\\) score that can be optimised by Gaussian cumulative distribution functions. Experimental results on 2 datasets show that our proposed method (CtF) is not only \\(8\\%\\) more accurate but also \\(5\\times \\) faster than contemporary hashing ReID methods. Compared with non-hashing ReID methods, CtF is \\(50\\times \\) faster with comparable accuracy. Code is available at https://github.com/wangguanan/light-reid.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_17');
INSERT INTO `paper` VALUES (11951, 'FeatMatch: Feature-Based Augmentation for Semi-supervised Learning', 'Semi-supervised learning', 'Feature-based augmentation', 'Consistency regularization', '', '', 'Recent state-of-the-art semi-supervised learning (SSL) methods use a combination of image-based transformations and consistency regularization as core components. Such methods, however, are limited to simple transformations such as traditional data augmentation or convex combinations of two images. In this paper, we propose a novel learned feature-based refinement and augmentation method that produces a varied set of complex transformations. Importantly, these transformations also use information from both within-class and across-class prototypical representations that we extract through clustering. We use features already computed across iterations by storing them in a memory bank, obviating the need for significant extra computation. These transformations, combined with traditional image-based augmentation, are then used as part of the consistency-based regularization loss. We demonstrate that our method is comparable to current state of art for smaller datasets (CIFAR-10 and SVHN) while being able to scale up to larger datasets such as CIFAR-100 and mini-Imagenet where we achieve significant gains over the state of art (e.g., absolute 17.44% gain on mini-ImageNet). We further test our method on DomainNet, demonstrating better robustness to out-of-domain unlabeled data, and perform rigorous ablations and analysis to validate the method. Code is available here: https://sites.google.com/view/chiawen-kuo/home/featmatch.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_28');
INSERT INTO `paper` VALUES (11952, 'Feature Normalized Knowledge Distillation for Image Classification', 'Label noise', 'Knowledge distillation', 'Image classification', '', '', 'Knowledge Distillation (KD) transfers the knowledge from a cumbersome teacher model to a lightweight student network. Since a single image may reasonably relate to several categories, the one-hot label would inevitably introduce the encoding noise. From this perspective, we systematically analyze the distillation mechanism and demonstrate that the \\(L_2\\)-norm of the feature in penultimate layer would be too large under the influence of label noise, and the temperature T in KD could be regarded as a correction factor for \\(L_2\\)-norm to suppress the impact of noise. Noticing different samples suffer from varying intensities of label noise, we further propose a simple yet effective feature normalized knowledge distillation which introduces the sample specific correction factor to replace the unified temperature T for better reducing the impact of noise. Extensive experiments show that the proposed method surpasses standard KD as well as self-distillation significantly on Cifar-100, CUB-200-2011 and Stanford Cars datasets. The codes are in https://github.com/aztc/FNKD', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_40');
INSERT INTO `paper` VALUES (11953, 'Feature Pyramid Transformer', 'Feature pyramid', 'Visual context', 'Transformer', 'Object detection', 'Instance segmentation', 'Feature interactions across space and scales underpin modern visual recognition systems because they introduce beneficial visual contexts. Conventionally, spatial contexts are passively hidden in the CNN’s increasing receptive fields or actively encoded by non-local convolution. Yet, the non-local spatial interactions are not across scales, and thus they fail to capture the non-local contexts of objects (or parts) residing in different scales. To this end, we propose a fully active feature interaction across both space and scales, called Feature Pyramid Transformer (FPT). It transforms any feature pyramid into another feature pyramid of the same size but with richer contexts, by using three specially designed transformers in self-level, top-down, and bottom-up interaction fashion. FPT serves as a generic visual backbone with fair computational overhead. We conduct extensive experiments in both instance-level (i.e., object detection and instance segmentation) and pixel-level segmentation tasks, using various backbones and head networks, and observe consistent improvement over all the baselines and the state-of-the-art methods (Code is open-sourced at https://github.com/ZHANGDONG-NJUST).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_20');
INSERT INTO `paper` VALUES (11954, 'Feature Representation Matters: End-to-End Learning for Reference-Based Image Super-Resolution', 'Super-resolution', 'Reference-based', 'Feature matching', 'Feature swapping', 'CUFED5', 'In this paper, we are aiming for a general reference-based super-resolution setting: it does not require the low-resolution image and the high-resolution reference image to be well aligned or with a similar texture. Instead, we only intend to transfer the relevant textures from reference images to the output super-resolution image. To this end, we engaged neural texture transfer to swap texture features between the low-resolution image and the high-resolution reference image. We identified the importance of designing a super-resolution task-specific features rather than classification oriented features for neural texture transfer, making the feature extractor more compatible with the image synthesis task. We develop an end-to-end training framework for the reference-based super-resolution task, where the feature encoding network prior to matching and swapping is jointly trained with the image synthesis network. We also discovered that learning the high-frequency residual is an effective way for the reference-based super-resolution task. Without bells and whistles, the proposed method E2ENT\\(^2\\) achieved better performance than state-of-the method (i.e., SRNTT with five loss functions) with only two basic loss functions. Extensive experimental results on several datasets demonstrate that the proposed method E2ENT\\(^2\\) can achieve superior performance to existing best models both quantitatively and qualitatively.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_14');
INSERT INTO `paper` VALUES (11955, 'Feature Space Augmentation for Long-Tailed Data', '', '', '', '', '', 'Real-world data often follow a long-tailed distribution as the frequency of each class is typically different. For example, a dataset can have a large number of under-represented classes and a few classes with more than sufficient data. However, a model to represent the dataset is usually expected to have reasonably homogeneous performances across classes. Introducing class-balanced loss and advanced methods on data re-sampling and augmentation are among the best practices to alleviate the data imbalance problem. However, the other part of the problem about the under-represented classes will have to rely on additional knowledge to recover the missing information.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_41');
INSERT INTO `paper` VALUES (11956, 'Feature-Metric Loss for Self-supervised Learning of Depth and Egomotion', '', '', '', '', '', 'Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by \\(\\delta _1\\) for depth estimation, and significantly outperforms previous method for visual odometry.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_34');
INSERT INTO `paper` VALUES (11957, 'Federated Visual Classification with Real-World Data Distribution', '', '', '', '', '', 'Federated Learning enables visual models to be trained on-device, bringing advantages for user privacy (data need never leave the device), but challenges in terms of data diversity and quality. Whilst typical models in the datacenter are trained using data that are independent and identically distributed (IID), data at source are typically far from IID. Furthermore, differing quantities of data are typically available at each device (imbalance). In this work, we characterize the effect these real-world data distributions have on distributed learning, using as a benchmark the standard Federated Averaging (FedAvg) algorithm. To do so, we introduce two new large-scale datasets for species and landmark classification, with realistic per-user data splits that simulate real-world edge learning scenarios. We also develop two new algorithms (FedVC, FedIR) that intelligently resample and reweight over the client pool, bringing large improvements in accuracy and stability in training. The datasets are made available online.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_5');
INSERT INTO `paper` VALUES (11958, 'Feed-Forward On-Edge Fine-Tuning Using Static Synthetic Gradient Modules', 'Synthetic gradients', 'Feed-forward training', 'One-edge fine-tuning', '', '', 'Training deep learning models on embedded devices is typically avoided since this requires more memory, computation and power over inference. In this work, we focus on lowering the amount of memory needed for storing all activations, which are required during the backward pass to compute the gradients. Instead, during the forward pass, static Synthetic Gradient Modules (SGMs) predict gradients for each layer. This allows training the model in a feed-forward manner without having to store all activations. We tested our method on a robot grasping scenario where a robot needs to learn to grasp new objects given only a single demonstration. By first training the SGMs in a meta-learning manner on a set of common objects, during fine-tuning, the SGMs provided the model with accurate gradients to successfully learn to grasp new objects. We have shown that our method has comparable results to using standard backpropagation.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_11');
INSERT INTO `paper` VALUES (11959, 'Feedback Attention for Cell Image Segmentation', 'Cell image', 'Semantic segmentation', 'Attention mechanism', 'Feedback mechanism', '', 'In this paper, we address cell image segmentation task by Feedback Attention mechanism like feedback processing. Unlike conventional neural network models of feedforward processing, we focused on the feedback processing in human brain and assumed that the network learns like a human by connecting feature maps from deep layers to shallow layers. We propose some Feedback Attentions which imitate human brain and feeds back the feature maps of output layer to close layer to the input. U-Net with Feedback Attention showed better result than the conventional methods using only feedforward processing.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_24');
INSERT INTO `paper` VALUES (11960, 'Few-Shot Action Recognition with Permutation-Invariant Attention', '', '', '', '', '', 'Many few-shot learning models focus on recognising images. In contrast, we tackle a challenging task of few-shot action recognition from videos. We build on a C3D encoder for spatio-temporal video blocks to capture short-range action patterns. Such encoded blocks are aggregated by permutation-invariant pooling to make our approach robust to varying action lengths and long-range temporal dependencies whose patterns are unlikely to repeat even in clips of the same class. Subsequently, the pooled representations are combined into simple relation descriptors which encode so-called query and support clips. Finally, relation descriptors are fed to the comparator with the goal of similarity learning between query and support clips. Importantly, to re-weight block contributions during pooling, we exploit spatial and temporal attention modules and self-supervision. In naturalistic clips (of the same class) there exists a temporal distribution shift–the locations of discriminative temporal action hotspots vary. Thus, we permute blocks of a clip and align the resulting attention regions with similarly permuted attention regions of non-permuted clip to train the attention mechanism invariant to block (and thus long-term hotspot) permutations. Our method outperforms the state of the art on the HMDB51, UCF101, miniMIT datasets.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_31');
INSERT INTO `paper` VALUES (11961, 'Few-Shot Compositional Font Generation with Dual Memory', '', '', '', '', '', 'Generating a new font library is a very labor-intensive and time-consuming job for glyph-rich scripts. Despite the remarkable success of existing font generation methods, they have significant drawbacks; they require a large number of reference images to generate a new font set, or they fail to capture detailed styles with only a few samples. In this paper, we focus on compositional scripts, a widely used letter system in the world, where each glyph can be decomposed by several components. By utilizing the compositionality of compositional scripts, we propose a novel font generation framework, named Dual Memory-Augmented Font Generation Network (DM-Font), which enables us to generate a high-quality font library with only a few samples. We employ memory components and global-context awareness in the generator to take advantage of the compositionality. In the experiments on Korean-handwriting fonts and Thai-printing fonts, we observe that our method generates a significantly better quality of samples with faithful stylization compared to the state-of-the-art generation methods quantitatively and qualitatively. Source code is available at https://github.com/clovaai/dmfont.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_43');
INSERT INTO `paper` VALUES (11962, 'Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild', 'Few-shot learning', 'Meta learning', 'Object detection', 'Viewpoint estimation', '', 'Detecting objects and estimating their viewpoint in images are key tasks of 3D scene understanding. Recent approaches have achieved excellent results on very large benchmarks for object detection and viewpoint estimation. However, performances are still lagging behind for novel object categories with few samples. In this paper, we tackle the problems of few-shot object detection and few-shot viewpoint estimation. We propose a meta-learning framework that can be applied to both tasks, possibly including 3D data. Our models improve the results on objects of novel classes by leveraging on rich feature information originating from base classes with many samples. A simple joint feature embedding module is proposed to make the most of this feature sharing. Despite its simplicity, our method outperforms state-of-the-art methods by a large margin on a range of datasets, including PASCAL VOC and MS COCO for few-shot object detection, and Pascal3D+ and ObjectNet3D for few-shot viewpoint estimation. And for the first time, we tackle the combination of both few-shot tasks, on ObjectNet3D, showing promising results.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_12');
INSERT INTO `paper` VALUES (11963, 'Few-Shot Scene-Adaptive Anomaly Detection', 'Anomaly detection', 'Few-shot learning', 'Meta-learning', '', '', 'We address the problem of anomaly detection in videos. The goal is to identify unusual behaviours automatically by learning exclusively from normal videos. Most existing approaches are usually data-hungry and have limited generalization abilities. They usually need to be trained on a large number of videos from a target scene to achieve good results in that scene. In this paper, we propose a novel few-shot scene-adaptive anomaly detection problem to address the limitations of previous approaches. Our goal is to learn to detect anomalies in a previously unseen scene with only a few frames. A reliable solution for this new problem will have huge potential in real-world applications since it is expensive to collect a massive amount of data for each target scene. We propose a meta-learning based approach for solving this new problem; extensive experimental results demonstrate the effectiveness of our proposed method. All codes are released in https://github.com/yiweilu3/Few-shot-Scene-adaptive-Anomaly-Detection.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_8');
INSERT INTO `paper` VALUES (11964, 'Few-Shot Semantic Segmentation with Democratic Attention Networks', 'Few-shot segmentation', 'Graph attention', 'Democratic attention network', 'Multi-scale guidance', '', 'Few-shot segmentation has recently generated great popularity, addressing the challenging yet important problem of segmenting objects from unseen categories with scarce annotated support images. The crux of few-shot segmentation is to extract object information from the support image and then propagate it to guide the segmentation of query images. In this paper, we propose the Democratic Attention Network (DAN) for few-shot semantic segmentation. We introduce the democratized graph attention mechanism, which can activate more pixels on the object to establish a robust correspondence between support and query images. Thus, the network is able to propagate more guiding information of foreground objects from support to query images, enhancing its robustness and generalizability to new objects. Furthermore, we propose multi-scale guidance by designing a refinement fusion unit to fuse features from intermediate layers for the segmentation of the query image. This offers an efficient way of leveraging multi-level semantic information to achieve more accurate segmentation. Extensive experiments on three benchmarks demonstrate that the proposed DAN achieves the new state-of-the-art performance, surpassing the previous methods by large margins. The thorough ablation studies further reveal its great effectiveness for few-shot semantic segmentation.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_43');
INSERT INTO `paper` VALUES (11965, 'Few-Shot Single-View 3-D Object Reconstruction with Compositional Priors', '3D reconstruction', 'Few-shot learning', 'Compositionality', '', '', 'The impressive performance of deep convolutional neural networks in single-view 3D reconstruction suggests that these models perform non-trivial reasoning about the 3D structure of the output space. Recent work has challenged this belief, showing that complex encoder-decoder architectures perform similarly to nearest-neighbor baselines or simple linear decoder models that exploit large amounts of per-category data, in standard benchmarks. A more realistic setting, however, involves inferring 3D shapes for categories with few available training examples; this requires a model that can successfully generalize to novel object classes. In this work we experimentally demonstrate that naive baselines fail in this few-shot learning setting, where the network must learn informative shape priors for inference of new categories. We propose three ways to learn a class-specific global shape prior, directly from data. Using these techniques, our learned prior is able to capture multi-scale information about the 3D shape, and account for intra-class variability by virtue of an implicit compositional structure. Experiments on the popular ShapeNet dataset show that our method outperforms a zero-shot baseline by over \\(50\\%\\) and the current state-of-the-art by over \\(10\\%\\) in terms of relative performance, in the few-shot setting.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_37');
INSERT INTO `paper` VALUES (11966, 'FHDe2Net: Full High Definition Demoireing Network', 'Low-level vision', 'Moiré pattern', 'Image restoration', '', '', 'Frequency aliasing in the digital capture of display screens leads to the moiré pattern, appearing as stripe-shaped distortions in images. Efforts to demoiréing have been made recently in a learning fashion due to the complexity and diversity of the pattern appearance. However, existing methods cannot satisfy the practical demand of demoiréing on camera phone capturing more pixels than a full high definition (FHD) image, which poses additional challenges of wider pattern scale range and fine detail preservation. We propose the Full High Definition Demoiréing Network (FHDe\\(^{2}\\)Net) to solve such problems. The framework consists of a global to local cascaded removal branch to eradicate multi-scale moiré patterns and a frequency based high-resolution content separation branch to retain fine details. We further collect an FHD moiré image dataset as a new benchmark for training and evaluation. Comparison experiments and ablation studies have verified the effectiveness of the proposed framework and each functional module both quantitatively and qualitatively in practical application scenarios.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_43');
INSERT INTO `paper` VALUES (11967, 'Filter Style Transfer Between Photos', 'Photorealistic style transfer', 'Filter style transfer', 'Image-to-image translation', '', '', 'Over the past few years, image-to-image style transfer has risen to the frontiers of neural image processing. While conventional methods were successful in various tasks such as color and texture transfer between images, none could effectively work with the custom filter effects that are applied by users through various platforms like Instagram. In this paper, we introduce a new concept of style transfer, Filter Style Transfer (FST). Unlike conventional style transfer, new technique FST can extract and transfer custom filter style from a filtered style image to a content image. FST first infers the original image from a filtered reference via image-to-image translation. Then it estimates filter parameters from the difference between them. To resolve the ill-posed nature of reconstructing the original image from the reference, we represent each pixel color of an image to class mean and deviation. Besides, to handle the intra-class color variation, we propose an uncertainty based weighted least square method for restoring an original image. To the best of our knowledge, FST is the first style transfer method that can transfer custom filter effects between FHD image under 2 ms on a mobile device without any textual context loss.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_7');
INSERT INTO `paper` VALUES (11968, 'Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning', 'Image captioning', 'Change captioning', 'Attention', 'Reinforcement learning', '', 'Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets .', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_34');
INSERT INTO `paper` VALUES (11969, 'Finding Non-uniform Quantization Schemes Using Multi-task Gaussian Processes', 'Quantization', 'Bayesian Optimization', 'Gaussian Process', '', '', 'We propose a novel method for neural network quantization that casts the neural architecture search problem as one of hyperparameter search to find non-uniform bit distributions throughout the layers of a CNN. We perform the search assuming a Multi-Task Gaussian Processes prior, which splits the problem to multiple tasks, each corresponding to different number of training epochs, and explore the space by sampling those configurations that yield maximum information. We then show that with significantly lower precision in the last layers we achieve a minimal loss of accuracy with appreciable memory savings. We test our findings on the CIFAR10 and ImageNet datasets using the VGG, ResNet and GoogLeNet architectures.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_23');
INSERT INTO `paper` VALUES (11970, 'Finding Your (3D) Center: 3D Object Detection Using a Learned Loss', '3D learning', '3D point clouds', '3D object detection', 'Unsupervised', '', 'Massive semantically labeled datasets are readily available for 2D images, however, are much harder to achieve for 3D scenes. Objects in 3D repositories like ShapeNet are labeled, but regrettably only in isolation, so without context. 3D scenes can be acquired by range scanners on city-level scale, but much fewer with semantic labels. Addressing this disparity, we introduce a new optimization procedure, which allows training for 3D detection with raw 3D scans while using as little as 5% of the object labels and still achieve comparable performance. Our optimization uses two networks. A scene network maps an entire 3D scene to a set of 3D object centers. As we assume the scene not to be labeled by centers, no classic loss, such as Chamfer can be used to train it. Instead, we use another network to emulate the loss. This loss network is trained on a small labeled subset and maps a non-centered 3D object in the presence of distractions to its own center. This function is very similar – and hence can be used instead of – the gradient the supervised loss would provide. Our evaluation documents competitive fidelity at a much lower level of supervision, respectively higher quality at comparable supervision. Supplementary material can be found at: dgriffiths3.github.io.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_5');
INSERT INTO `paper` VALUES (11971, 'Fine-Grained Visual Classification via Progressive Multi-granularity Training of Jigsaw Patches', '', '', '', '', '', 'Fine-grained visual classification (FGVC) is much more challenging than traditional classification tasks due to the inherently subtle intra-class object variations. Recent works are mainly part-driven (either explicitly or implicitly), with the assumption that fine-grained information naturally rests within the parts. In this paper, we take a different stance, and show that part operations are not strictly necessary – the key lies with encouraging the network to learn at different granularities and progressively fusing multi-granularity features together. In particular, we propose: (i) a progressive training strategy that effectively fuses features from different granularities, and (ii) a random jigsaw patch generator that encourages the network to learn features at specific granularities. We evaluate on several standard FGVC benchmark datasets, and show the proposed method consistently outperforms existing alternatives or delivers competitive results. The code is available at https://github.com/PRIS-CV/PMG-Progressive-Multi-Granularity-Training.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_10');
INSERT INTO `paper` VALUES (11972, 'Fixing Localization Errors to Improve Image Classification', '', '', '', '', '', 'Deep neural networks are generally considered black-box models that offer less interpretability for their decision process. To address this limitation, Class Activation Map (CAM) provides an attractive solution that visualizes class-specific discriminative regions in an input image. The remarkable ability of CAMs to locate class discriminating regions has been exploited in weakly-supervised segmentation and localization tasks. In this work, we explore a new direction towards the possible use of CAM in deep network learning process. We note that such visualizations lend insights into the workings of deep CNNs and could be leveraged to introduce additional constraints during the learning stage. Specifically, the CAMs for negative classes (negative CAMs) often have false activations even though those classes are absent from an image. Thereby, we propose a loss function that seeks to minimize peaks within the negative CAMs, called ‘Homogeneous Negative CAM’ loss. This way, in an effort to fix localization errors, our loss provides an extra supervisory signal that helps the model to better discriminate between similar classes. Our designed loss function is easy to implement and can be readily integrated into existing DNNs. We evaluate it on a number of classification tasks including large-scale recognition, multi-label classification and fine-grained recognition. Our loss provides better performance compared to other loss functions across the studied tasks. Additionally, we show that the proposed loss function provides higher robustness against adversarial attacks and noisy labels.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_17');
INSERT INTO `paper` VALUES (11973, 'Flexible Example-Based Image Enhancement with Task Adaptive Global Feature Self-guided Network', '', '', '', '', '', 'We propose the first practical multitask image enhancement network, that is able to learn one-to-many and many-to-one image mappings. We show that our model outperforms the current state of the art in learning a single enhancement mapping, while having significantly fewer parameters than its competitors. Furthermore, the model achieves even higher performance on learning multiple mappings simultaneously, by taking advantage of shared representations. Our network is based on the recently proposed SGN architecture, with modifications targeted at incorporating global features and style adaption. Finally, we present an unpaired learning method for multitask image enhancement, that is based on generative adversarial networks (GANs).', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_21');
INSERT INTO `paper` VALUES (11974, 'FLOT: Scene Flow on Point Clouds Guided by Optimal Transport', '', '', '', '', '', 'We propose and study a method called FLOT that estimates scene flow on point clouds. We start the design of FLOT by noticing that scene flow estimation on point clouds reduces to estimating a permutation matrix in a perfect world. Inspired by recent works on graph matching, we build a method to find these correspondences by borrowing tools from optimal transport. Then, we relax the transport constraints to take into account real-world imperfections. The transport cost between two points is given by the pairwise similarity between deep features extracted by a neural network trained under full supervision using synthetic datasets. Our main finding is that FLOT can perform as well as the best existing methods on synthetic and real-world datasets while requiring much less parameters and without using multiscale analysis. Our second finding is that, on the training datasets considered, most of the performance can be explained by the learned transport cost. This yields a simpler method, FLOT\\(_0\\), which is obtained using a particular choice of optimal transport parameters and performs nearly as well as FLOT.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_32');
INSERT INTO `paper` VALUES (11975, 'Flow-edge Guided Video Completion', '', '', '', '', '', 'We present a new flow-based video completion algorithm. Previous flow completion methods are often unable to retain the sharpness of motion boundaries. Our method first extracts and completes motion edges, and then uses them to guide piecewise-smooth flow completion with sharp edges. Existing methods propagate colors among local flow connections between adjacent frames. However, not all missing regions in a video can be reached in this way because the motion boundaries form impenetrable barriers. Our method alleviates this problem by introducing non-local flow connections to temporally distant frames, enabling propagating video content over motion boundaries. We validate our approach on the DAVIS dataset. Both visual and quantitative results show that our method compares favorably against the state-of-the-art algorithms.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_42');
INSERT INTO `paper` VALUES (11976, 'Foley Music: Learning to Generate Music from Videos', 'Audio-visual', 'Sound generation', 'Pose', 'Foley', '', 'In this paper, we introduce Foley Music, a system that can synthesize plausible music for a silent video clip about people playing musical instruments. We first identify two key intermediate representations for a successful video to music generator: body keypoints from videos and MIDI events from audio recordings. We then formulate music generation from videos as a motion-to-MIDI translation problem. We present a Graph−Transformer framework that can accurately predict MIDI event sequences in accordance with the body movements. The MIDI event can then be converted to realistic music using an off-the-shelf music synthesizer tool. We demonstrate the effectiveness of our models on videos containing a variety of music performances. Experimental results show that our model outperforms several existing systems in generating music that is pleasant to listen to. More importantly, the MIDI representations are fully interpretable and transparent, thus enabling us to perform music editing flexibly. We encourage the readers to watch the supplementary video with audio turned on to experience the results.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_44');
INSERT INTO `paper` VALUES (11977, 'Forecasting Human-Object Interaction: Joint Prediction of Motor Attention and Actions in First Person Video', 'First Person Vision', 'Action anticipation', 'Motor attention', '', '', 'We address the challenging task of anticipating human-object interaction in first person videos. Most existing methods either ignore how the camera wearer interacts with objects, or simply considers body motion as a separate modality. In contrast, we observe that the intentional hand movement reveals critical information about the future activity. Motivated by this observation, we adopt intentional hand movement as a feature representation, and propose a novel deep network that jointly models and predicts the egocentric hand motion, interaction hotspots and future action. Specifically, we consider the future hand motion as the motor attention, and model this attention using probabilistic variables in our deep model. The predicted motor attention is further used to select the discriminative spatial-temporal visual features for predicting actions and interaction hotspots. We present extensive experiments demonstrating the benefit of the proposed joint model. Importantly, our model produces new state-of-the-art results for action anticipation on both EGTEA Gaze+ and the EPIC-Kitchens datasets. Our project page is available at https://aptx4869lm.github.io/ForecastingHOI/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_41');
INSERT INTO `paper` VALUES (11978, 'Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations', 'Forgetting', 'Data removal', 'Neural tangent kernel', 'Information theory', '', 'We describe a procedure for removing dependency on a cohort of training data from a trained deep network that improves upon and generalizes previous methods to different readout functions, and can be extended to ensure forgetting in the final activations of the network. We introduce a new bound on how much information can be extracted per query about the forgotten cohort from a black-box network for which only the input-output behavior is observed. The proposed forgetting procedure has a deterministic part derived from the differential equations of a linearized version of the model, and a stochastic part that ensures information destruction by adding noise tailored to the geometry of the loss landscape. We exploit the connections between the final activations and weight dynamics of a DNN inspired by Neural Tangent Kernels to compute the information in the final activations.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_23');
INSERT INTO `paper` VALUES (11979, 'ForkGAN: Seeing into the Rainy Night', 'Light illumination', 'Image-to-image translation', 'Image synthesis', 'Generative adversarial networks', '', 'We present a ForkGAN for task-agnostic image translation that can boost multiple vision tasks in adverse weather conditions. Three tasks of image localization/retrieval, semantic image segmentation, and object detection are evaluated. The key challenge is achieving high-quality image translation without any explicit supervision, or task awareness. Our innovation is a fork-shape generator with one encoder and two decoders that disentangles the domain-specific and domain-invariant information. We force the cyclic translation between the weather conditions to go through a common encoding space, and make sure the encoding features reveal no information about the domains. Experimental results show our algorithm produces state-of-the-art image synthesis results and boost three vision tasks’ performances in adverse weathers.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_10');
INSERT INTO `paper` VALUES (11980, 'Free View Synthesis', 'View synthesis', 'Image-based rendering', '', '', '', 'We present a method for novel view synthesis from input images that are freely distributed around a scene. Our method does not rely on a regular arrangement of input views, can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. We calibrate the input images via SfM and erect a coarse geometric scaffold via MVS. This scaffold is used to create a proxy depth map for a novel view of the scene. Based on this depth map, a recurrent encoder-decoder network processes reprojected features from nearby views and synthesizes the new view. Our network does not need to be optimized for a given scene. After training on a dataset, it works in previously unseen environments with no fine-tuning or per-scene optimization. We evaluate the presented approach on challenging real-world datasets, including Tanks and Temples, where we demonstrate successful view synthesis for the first time and substantially outperform prior and concurrent work.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_37');
INSERT INTO `paper` VALUES (11981, 'FreeCam3D: Snapshot Structured Light 3D with Freely-Moving Cameras', 'Computational photography', '3D reconstruction', 'Coded aperture', 'Structured light', '', 'A 3D imaging and mapping system that can handle both multiple-viewers and dynamic-objects is attractive for many applications. We propose a freeform structured light system that does not rigidly constrain camera(s) to the projector. By introducing an optimized phase-coded aperture in the projector, we transform the projector pattern to encode depth in its defocus robustly; this allows a camera to estimate depth, in projector coordinates, using local information. Additionally, we project a Kronecker-multiplexed pattern that provides global context to establish correspondence between camera and projector pixels. Together with aperture coding and projected pattern, the projector offers a unique 3D labeling for every location of the scene. The projected pattern can be observed in part or full by any camera, to reconstruct both the 3D map of the scene and the camera pose in the projector coordinates. This system is optimized using a fully differentiable rendering model and a CNN-based reconstruction. We build a prototype and demonstrate high-quality 3D reconstruction with an unconstrained camera, for both dynamic scenes and multi-camera systems.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_19');
INSERT INTO `paper` VALUES (11982, 'Frequency-Tuned Universal Adversarial Perturbations', 'Universal adverarial attack', 'Frequency tuning', 'Just-noticeable difference (JND)', 'Discrete Cosine Transform (DCT)', '', 'The predictions of a convolutional neural network (CNN) for an image set can be severely altered by one single image-agnostic perturbation, or universal perturbation, even when the perturbation is small to restrict its perceptibility. Such universal perturbations are typically generated and added to an image in the spatial domain. However, it is well known that human perception is affected by local visual frequency characteristics. Based on this, we propose a frequency-tuned universal attack method to compute universal perturbations in the frequency domain. We show that our method can realize a good balance between perceptibility and effectiveness in terms of fooling rate by adapting the perturbations to the local frequency content. Compared with existing universal adversarial attack techniques, our frequency-tuned attack method can achieve cutting-edge quantitative results. We demonstrate that our approach can significantly improve the performance of the baseline on both white-box and black-box attacks.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_36');
INSERT INTO `paper` VALUES (11983, 'From Image to Stability: Learning Dynamics from Human Pose', 'Stability', 'Center of Pressure', 'Base of support', 'Foot pressure estimation', '3D human pose', 'We propose and validate two end-to-end deep learning architectures to learn foot pressure distribution maps (dynamics) from 2D or 3D human pose (kinematics). The networks are trained using 1.36 million synchronized pose+pressure data pairs from 10 subjects performing multiple takes of a 5-min long choreographed Taiji sequence. Using leave-one-subject-out cross validation, we demonstrate reliable and repeatable foot pressure prediction, setting the first baseline for solving a non-obvious pose to pressure cross-modality mapping problem in computer vision. Furthermore, we compute and quantitatively validate Center of Pressure (CoP) and Base of Support (BoS), two key components for stability analysis, from the predicted foot pressure distributions.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_32');
INSERT INTO `paper` VALUES (11984, 'From Shadow Segmentation to Shadow Removal', 'Shadow removal', 'GAN', 'Weakly-supervised', 'Illumination model', 'Unpaired', 'The requirement for paired shadow and shadow-free images limits the size and diversity of shadow removal datasets and hinders the possibility of training large-scale, robust shadow removal algorithms. We propose a shadow removal method that can be trained using only shadow and non-shadow patches cropped from the shadow images themselves. Our method is trained via an adversarial framework, following a physical model of shadow formation. Our central contribution is a set of physics-based constraints that enables this adversarial training. Our method achieves competitive shadow removal results compared to state-of-the-art methods that are trained with fully paired shadow and shadow-free images. The advantages of our training regime are even more pronounced in shadow removal for videos. Our method can be fine-tuned on a testing video with only the shadow masks generated by a pre-trained shadow detector and outperforms state-of-the-art methods on this challenging test. We illustrate the advantages of our method on our proposed video shadow removal dataset.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_16');
INSERT INTO `paper` VALUES (11985, 'FTL: A Universal Framework for Training Low-Bit DNNs via Feature Transfer', 'Low-bit DNN', 'Feature Transfer', 'Space mismatch', '', '', 'Low-bit Deep Neural Networks (low-bit DNNs) have recently received significant attention for their high efficiency. However, low-bit DNNs are often difficult to optimize due to the saddle points in loss surfaces. Here we introduce a novel feature-based knowledge transfer framework, which utilizes a 32-bit DNN to guide the training of a low-bit DNN via feature maps. It is challenge because feature maps from two branches lie in continuous and discrete space respectively, and such mismatch has not been handled properly by existing feature transfer frameworks. In this paper, we propose to directly transfer information-rich continuous-space feature to the low-bit branch. To alleviate the negative impacts brought by the feature quantizer during the transfer process, we make two branches interact via centered cosine distance rather than the widely-used p-norms. Extensive experiments are conducted on Cifar10/100 and ImageNet. Compared with low-bit models trained directly, the proposed framework brings 0.5% to 3.4% accuracy gains to three different quantization schemes. Besides, the proposed framework can also be combined with other techniques, e.g. logits transfer, for further enhacement.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_42');
INSERT INTO `paper` VALUES (11986, 'Full-Body Awareness from Partial Observations', 'Human pose estimation', '', '', '', '', 'There has been great progress in human 3D mesh recovery and great interest in learning about the world from consumer video data. Unfortunately current methods for 3D human mesh recovery work rather poorly on consumer video data, since on the Internet, unusual camera viewpoints and aggressive truncations are the norm rather than a rarity. We study this problem and make a number of contributions to address it: (i) we propose a simple but highly effective self-training framework that adapts human 3D mesh recovery systems to consumer videos and demonstrate its application to two recent systems; (ii) we introduce evaluation protocols and keypoint annotations 13 K frames across four consumer video datasets for studying this task, including evaluations on out-of-image keypoints; and (iii) we show that our method substantially improves PCK and human-subject judgments compared to baselines, both on test videos from the dataset it was trained on, as well as on three other datasets without further adaptation.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_31');
INSERT INTO `paper` VALUES (11987, 'Full-Time Monocular Road Detection Using Zero-Distribution Prior of Angle of Polarization', 'Road detection', 'Polarization prior', 'Angle of Polarization', 'LWIR DoFP sensor', '', 'This paper presents a road detection technique based on long-wave infrared (LWIR) polarization imaging for autonomous navigation regardless of illumination conditions, day and night. Division of Focal Plane (DoFP) imaging technology enables acquisition of infrared polarization images in real time using a monocular camera. Zero-distribution prior embodies the zero-distribution of Angle of Polarization (AoP) of a road scene image, which provides a significant contrast between the road and the background. This paper combines zero-distribution of AoP, the difference of Degree of linear Polarization (DoP), and the edge information to segment the road region in the scene. We developed a LWIR DoFP Dataset of Road Scene (LDDRS) consisting of 2,113 annotated images. Experiment results on the LDDRS dataset demonstrate the merits of the proposed road detection method based on the zero-distribution prior. The LDDRS dataset is available at https://github.com/polwork/LDDRS.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_28');
INSERT INTO `paper` VALUES (11988, 'Fully Convolutional Networks for Continuous Sign Language Recognition', 'Continuous sign language recognition', 'Fully convolutional network', 'Joint training', 'Online recognition', '', 'Continuous sign language recognition (SLR) is a challenging task that requires learning on both spatial and temporal dimensions of signing frame sequences. Most recent work accomplishes this by using CNN and RNN hybrid networks. However, training these networks is generally non-trivial, and most of them fail in learning unseen sequence patterns, causing an unsatisfactory performance for online recognition. In this paper, we propose a fully convolutional network (FCN) for online SLR to concurrently learn spatial and temporal features from weakly annotated video sequences with only sentence-level annotations given. A gloss feature enhancement (GFE) module is introduced in the proposed network to enforce better sequence alignment learning. The proposed network is end-to-end trainable without any pre-training. We conduct experiments on two large scale SLR datasets. Experiments show that our method for continuous SLR is effective and performs well in online recognition.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_41');
INSERT INTO `paper` VALUES (11989, 'Fully Embedding Fast Convolutional Networks on Pixel Processor Arrays', 'Low-level vision', 'PPA', 'CNN', 'Vision sensor', 'Edge computing', 'We present a novel method of CNN inference for pixel processor array (PPA) vision sensors, designed to take advantage of their massive parallelism and analog compute capabilities. PPA sensors consist of an array of processing elements (PEs), with each PE capable of light capture, data storage and computation, allowing various computer vision processes to be executed directly upon the sensor device. The key idea behind our approach is storing network weights “in-pixel” within the PEs of the PPA sensor itself to allow various computations, such as multiple different image convolutions, to be carried out in parallel. Our approach can perform convolutional layers, max pooling, ReLu, and a final fully connected layer entirely upon the PPA sensor, while leaving no untapped computational resources. This is in contrast to previous works that only use a sensor-level processing to sequentially compute image convolutions, and must transfer data to an external digital processor to complete the computation. We demonstrate our approach on the SCAMP-5 vision system, performing inference in a MNIST digit classification network at over 3000 frames per second and over 93% classification accuracy. This is the first work demonstrating CNN inference conducted entirely upon a PPA vision sensor, requiring no external processing.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_29');
INSERT INTO `paper` VALUES (11990, 'Fully Trainable and Interpretable Non-local Sparse Models for Image Restoration', 'Sparse coding', 'Image processing', 'Structured sparsity', '', '', 'Non-local self-similarity and sparsity principles have proven to be powerful priors for natural image modeling. We propose a novel differentiable relaxation of joint sparsity that exploits both principles and leads to a general framework for image restoration which is (1) trainable end to end, (2) fully interpretable, and (3) much more compact than competing deep learning architectures. We apply this approach to denoising, blind denoising, jpeg deblocking, and demosaicking, and show that, with as few as 100 K parameters, its performance on several standard benchmarks is on par or better than state-of-the-art methods that may have an order of magnitude or more parameters.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_15');
INSERT INTO `paper` VALUES (11991, 'Funnel Activation for Visual Recognition', 'Funnel activation', 'Visual recognition', 'CNN', '', '', 'We present a conceptually simple but effective funnel activation for image recognition tasks, called Funnel activation (FReLU), that extends ReLU and PReLU to a 2D activation by adding a negligible overhead of spatial condition. The forms of ReLU and PReLU are \\(y=max(x,0)\\) and \\(y=max(x,px)\\), respectively, while FReLU is in the form of \\(y=max(x, \\mathbb {T}(x))\\), where \\(\\mathbb {T}(\\cdot )\\) is the 2D spatial condition. Moreover, the spatial condition achieves a pixel-wise modeling capacity in a simple way, capturing complicated visual layouts with regular convolutions. We conduct experiments on ImageNet, COCO detection, and semantic segmentation tasks, showing great improvements and robustness of FReLU in the visual recognition tasks. Code is available at https://github.com/megvii-model/FunnelAct.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_21');
INSERT INTO `paper` VALUES (11992, 'G-LBM: Generative Low-Dimensional Background Model Estimation from Video Sequences', 'Background estimation', 'Foreground segmentation', 'Non-linear manifold learning', 'Deep neural network', 'Variational auto-encoding', 'In this paper, we propose a computationally tractable and theoretically supported non-linear low-dimensional generative model to represent real-world data in the presence of noise and sparse outliers. The non-linear low-dimensional manifold discovery of data is done through describing a joint distribution over observations, and their low-dimensional representations (i.e. manifold coordinates). Our model, called generative low-dimensional background model (G-LBM) admits variational operations on the distribution of the manifold coordinates and simultaneously generates a low-rank structure of the latent manifold given the data. Therefore, our probabilistic model contains the intuition of the non-probabilistic low-dimensional manifold learning. G-LBM selects the intrinsic dimensionality of the underling manifold of the observations, and its probabilistic nature models the noise in the observation data. G-LBM has direct application in the background scenes model estimation from video sequences and we have evaluated its performance on SBMnet-2016 and BMC2012 datasets, where it achieved a performance higher or comparable to other state-of-the-art methods while being agnostic to different scenes. Besides, in challenges such as camera jitter and background motion, G-LBM is able to robustly estimate the background by effectively modeling the uncertainties in video observations in these scenarios. (The code and models are available at: https://github.com/brezaei/G-LBM.)', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_18');
INSERT INTO `paper` VALUES (11993, 'Gabor Layers Enhance Network Robustness', 'Gabor', 'Robustness', 'Adversarial attacks', 'Regularizer', '', 'We revisit the benefits of merging classical vision concepts with deep learning models. In particular, we explore the effect of replacing the first layers of various deep architectures with Gabor layers (i.e. convolutional layers with filters that are based on learnable Gabor parameters) on robustness against adversarial attacks. We observe that architectures with Gabor layers gain a consistent boost in robustness over regular models and maintain high generalizing test performance. We then exploit the analytical expression of Gabor filters to derive a compact expression for a Lipschitz constant of such filters, and harness this theoretical result to develop a regularizer we use during training to further enhance network robustness. We conduct extensive experiments with various architectures (LeNet, AlexNet, VGG16, and WideResNet) on several datasets (MNIST, SVHN, CIFAR10 and CIFAR100) and demonstrate large empirical robustness gains. Furthermore, we experimentally show how our regularizer provides consistent robustness improvements.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_26');
INSERT INTO `paper` VALUES (11994, 'Gait Lateral Network: Learning Discriminative and Compact Representations for Gait Recognition', 'Gait recognition', 'Lateral connections', 'Discriminative representations', 'Compact representations', '', 'Gait recognition aims at identifying different people by the walking patterns, which can be conducted at a long distance without the cooperation of subjects. A key challenge for gait recognition is to learn representations from the silhouettes that are invariant to the factors such as clothing, carrying conditions and camera viewpoints. Besides being discriminative for identification, the gait representations should also be compact for storage to keep millions of subjects registered in the gallery. In this work, we propose a novel network named Gait Lateral Network (GLN) which can learn both discriminative and compact representations from the silhouettes for gait recognition. Specifically, GLN leverages the inherent feature pyramid in deep convolutional neural networks to enhance the gait representations. The silhouette-level and set-level features extracted by different stages are merged with the lateral connections in a top-down manner. Besides, GLN is equipped with a Compact Block which can significantly reduce the dimension of the gait representations without hindering the accuracy. Extensive experiments on CASIA-B and OUMVLP show that GLN can achieve state-of-the-art performance using the 256-dimensional representations. Under the most challenging condition of walking in different clothes on CASIA-B, our method improves the rank-1 accuracy by \\(6.45\\%\\).', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_22');
INSERT INTO `paper` VALUES (11995, 'Gait Recognition from a Single Image Using a Phase-Aware Gait Cycle Reconstruction Network', 'Gait cycle reconstruction', 'Gait recognition', 'Single image', '', '', 'We propose a method of gait recognition just from a single image for the first time, which enables latency-free gait recognition. To mitigate large intra-subject variations caused by a phase (gait pose) difference between a matching pair of input single images, we first reconstruct full gait cycles of image sequences from the single images using an auto-encoder framework, and then feed them into a state-of-the-art gait recognition network for matching. Specifically, a phase estimation network is introduced for the input single image, and the gait cycle reconstruction network exploits the estimated phase to mitigate the dependence of an encoded feature on the phase of that single image. This is called phase-aware gait cycle reconstructor (PA-GCR). In the training phase, the PA-GCR and recognition network are simultaneously optimized to achieve a good trade-off between reconstruction and recognition accuracies. Experiments on three gait datasets demonstrate the significant performance improvement of this method.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_23');
INSERT INTO `paper` VALUES (11996, 'GAN Slimming: All-in-One GAN Compression by a Unified Optimization Framework', '', '', '', '', '', 'Generative adversarial networks (GANs) have gained increasing popularity in various computer vision applications, and recently start to be deployed to resource-constrained mobile devices. Similar to other deep models, state-of-the-art GANs suffer from high parameter complexities. That has recently motivated the exploration of compressing GANs (usually generators). Compared to the vast literature and prevailing success in compressing deep classifiers, the study of GAN compression remains in its infancy, so far leveraging individual compression techniques instead of more sophisticated combinations. We observe that due to the notorious instability of training GANs, heuristically stacking different compression techniques will result in unsatisfactory results. To this end, we propose the first unified optimization framework combining multiple compression means for GAN compression, dubbed GAN Slimming (GS). GS seamlessly integrates three mainstream compression techniques: model distillation, channel pruning and quantization, together with the GAN minimax objective, into one unified optimization form, that can be efficiently optimized from end to end. Without bells and whistles, GS largely outperforms existing options in compressing image-to-image translation GANs. Specifically, we apply GS to compress CartoonGAN, a state-of-the-art style transfer network, by up to \\(\\mathbf {{47}{\\times }}\\) times, with minimal visual quality degradation. Codes and pre-trained models can be found at https://github.com/TAMU-VITA/GAN-Slimming.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_4');
INSERT INTO `paper` VALUES (11997, 'GAN-Based Anomaly Detection In Imbalance Problems', 'Imbalance problems', 'Anomaly detection', 'GAN', 'Defects inspection', 'Patch loss', 'Imbalance pre one of the key issues that affect the performance greatly. Our focus in this work is to address an imbalance problem arising from defect detection in industrial inspections, including the different number of defect and non-defect dataset, the gap of distribution among defect classes, and various sizes of defects. To this end, we adopt the anomaly detection method that is to identify unusual patterns to address such challenging problems. Especially generative adversarial network (GAN) and autoencoder-based approaches have shown to be effective in this field. In this work, (1) we propose a novel GAN-based anomaly detection model which consists of an autoencoder as the generator and two separate discriminators for each of normal and anomaly input; and (2) we also explore a way to effectively optimize our model by proposing new loss functions: Patch loss and Anomaly adversarial loss, and further combining them to jointly train the model. In our experiment, we evaluate our model on conventional benchmark datasets such as MNIST, Fashion MNIST, CIFAR 10/100 data as well as on real-world industrial dataset – smartphone case defects. Finally, experimental results demonstrate the effectiveness of our approach by showing the results of outperforming the current State-Of-The-Art approaches in terms of the average area under the ROC curve (AUROC).', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_11');
INSERT INTO `paper` VALUES (11998, 'GAN-Based Garment Generation Using Sewing Pattern Images', '', '', '', '', '', 'The generation of realistic apparel model has become increasingly popular as a result of the rapid pace of change in fashion trends and the growing need for garment models in various applications such as virtual try-on. For such application requirements, it is important to have a general cloth model that can represent a diverse set of garments. Previous studies often make certain assumptions about the garment, such as the topology or suited body shape. We propose a unified method using the generative network. Our model is applicable to different garment topologies with different sewing patterns and fabric materials. We also develop a novel image representation of garment models, and a reliable mapping algorithm between the general garment model and the image representation that can regularize the data representation of the cloth. Using this special intermediate image representation, the generated garment model can be easily retargeted to another body, enabling garment customization. In addition, a large garment appearance dataset is provided for use in garment reconstruction, garment capturing, and other applications. We demonstrate that our generative model has high reconstruction accuracy and can provide rich variations of virtual garments.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_14');
INSERT INTO `paper` VALUES (11999, 'GANHopper: Multi-hop GAN for Unsupervised Image-to-Image Translation', 'Unsupervised learning', 'Adversarial learning', 'Image translation', '', '', 'We introduce GANHopper, an unsupervised image-to-image translation network that transforms images gradually between two domains, through multiple hops. Instead of executing translation directly, we steer the translation by requiring the network to produce in-between images that resemble weighted hybrids between images from the input domains. Our network is trained on unpaired images from the two domains only, without any in-between images. All hops are produced using a single generator along each direction. In addition to the standard cycle-consistency and adversarial losses, we introduce a new hybrid discriminator, which is trained to classify the intermediate images produced by the generator as weighted hybrids, with weights based on a predetermined hop count. We also add a smoothness term to constrain the magnitude of each hop, further regularizing the translation. Compared to previous methods, GANHopper excels at image translations involving domain-specific image features and geometric variations while also preserving non-domain-specific features such as general color schemes.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_22');
INSERT INTO `paper` VALUES (12000, 'GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images', 'Generative Adversarial Networks', 'Style and content conditioning', 'Handwritten word images', '', '', 'Although current image generation methods have reached impressive quality levels, they are still unable to produce plausible yet diverse images of handwritten words. On the contrary, when writing by hand, a great variability is observed across different writers, and even when analyzing words scribbled by the same individual, involuntary variations are conspicuous. In this work, we take a step closer to producing realistic and varied artificially rendered handwriting. We propose a novel method that is able to produce credible handwritten word images by conditioning the generative process with both calligraphic style features and textual content. Our generator is guided by three complementary learning objectives: to produce realistic images, to imitate a certain handwriting style and to convey a specific textual content. Our model is unconstrained to any predefined vocabulary, being able to render whatever input word. Given a sample writer, it is also able to mimic its calligraphic features in a few-shot setup. We significantly advance over prior art and demonstrate with qualitative, quantitative and human-based evaluations the realistic aspect of our synthetically produced images.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_17');
INSERT INTO `paper` VALUES (12001, 'GATCluster: Self-supervised Gaussian-Attention Network for Image Clustering', '', '', '', '', '', 'We propose a self-supervised Gaussian ATtention network for image Clustering (GATCluster). Rather than extracting intermediate features first and then performing traditional clustering algorithms, GATCluster directly outputs semantic cluster labels without further post-processing. We give a Label Feature Theorem to guarantee that the learned features are one-hot encoded vectors and the trivial solutions are avoided. Based on this theorem, we design four self-learning tasks with the constraints of transformation invariance, separability maximization, entropy analysis, and attention mapping. Specifically, the transformation invariance and separability maximization tasks learn the relations between samples. The entropy analysis task aims to avoid trivial solutions. To capture the object-oriented semantics, we design a self-supervised attention mechanism that includes a Gaussian attention module and a soft-attention loss. Moreover, we design a two-step learning algorithm that is memory-efficient for clustering large-size images. Extensive experiments demonstrate the superiority of our proposed method in comparison with the state-of-the-art image clustering benchmarks.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_44');
INSERT INTO `paper` VALUES (12002, 'Gated Texture CNN for Efficient and Configurable Image Denoising', 'Image denoising', 'Texture gating mechanisms', 'Interactive modification', '', '', 'Convolutional neural network (CNN)-based image denoising methods typically estimate the noise component contained in a noisy input image and restore a clean image by subtracting the estimated noise from the input. However, previous denoising methods tend to remove high-frequency information (e.g., textures) from the input. It caused by intermediate feature maps of CNN contains texture information. A straightforward approach to this problem is stacking numerous layers, which leads to a high computational cost. To achieve high performance and computational efficiency, we propose a gated texture CNN (GTCNN), which is designed to carefully exclude the texture information from each intermediate feature map of the CNN by incorporating gating mechanisms. Our GTCNN achieves state-of-the-art performance with 4.8 times fewer parameters than previous state-of-the-art methods. Furthermore, the GTCNN allows us to interactively control the texture strength in the output image without any additional modules, training, or computational costs.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_40');
INSERT INTO `paper` VALUES (12003, 'GCF-Net: Gated Clip Fusion Network for Video Action Recognition', 'Video action recognition', '3D-CNNs', 'Dense slip sampling', 'Clip fusion', '', 'In recent years, most of the accuracy gains for video action recognition have come from the newly designed CNN architectures (e.g., 3D-CNNs). These models are trained by applying a deep CNN on single clip of fixed temporal length. Since each video segment are processed by the 3D-CNN module separately, the corresponding clip descriptor is local and the inter-clip relationships are inherently implicit. Common method that directly averages the clip-level outputs as a video-level prediction is prone to fail due to the lack of mechanism that can extract and integrate relevant information to represent the video.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_46');
INSERT INTO `paper` VALUES (12004, 'GDumb: A Simple Approach that Questions Our Progress in Continual Learning', '', '', '', '', '', 'We discuss a general formulation for the Continual Learning (CL) problem for classification—a learning task where a stream provides samples to a learner and the goal of the learner, depending on the samples it receives, is to continually upgrade its knowledge about the old classes and learn new ones. Our formulation takes inspiration from the open-set recognition problem where test scenarios do not necessarily belong to the training distribution. We also discuss various quirks and assumptions encoded in recently proposed approaches for CL. We argue that some oversimplify the problem to an extent that leaves it with very little practical importance, and makes it extremely easy to perform well on. To validate this, we propose GDumb that (1) greedily stores samples in memory as they come and; (2) at test time, trains a model from scratch using samples only in the memory. We show that even though GDumb is not specifically designed for CL problems, it obtains state-of-the-art accuracies (often with large margins) in almost all the experiments when compared to a multitude of recently proposed algorithms. Surprisingly, it outperforms approaches in CL formulations for which they were specifically designed. This, we believe, raises concerns regarding our progress in CL for classification. Overall, we hope our formulation, characterizations and discussions will help in designing realistically useful CL algorithms, and GDumb will serve as a strong contender for the same.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_31');
INSERT INTO `paper` VALUES (12005, 'GeLaTO: Generative Latent Textured Objects', '3D modeling', '3D reconstruction', 'Generative modeling', '', '', 'Accurate modeling of 3D objects exhibiting transparency, reflections and thin structures is an extremely challenging problem. Inspired by billboards and geometric proxies used in computer graphics, this paper proposes Generative Latent Textured Objects (GeLaTO), a compact representation that combines a set of coarse shape proxies defining low frequency geometry with learned neural textures, to encode both medium and fine scale geometry as well as view-dependent appearance. To generate the proxies’ textures, we learn a joint latent space allowing category-level appearance and geometry interpolation. The proxies are independently rasterized with their corresponding neural texture and composited using a U-Net, which generates an output photorealistic image including an alpha map. We demonstrate the effectiveness of our approach by reconstructing complex objects from a sparse set of views. We show results on a dataset of real images of eyeglasses frames, which are particularly challenging to reconstruct using classical methods. We also demonstrate that these coarse proxies can be handcrafted when the underlying object geometry is easy to model, like eyeglasses, or generated using a neural network for more complex categories, such as cars.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_15');
INSERT INTO `paper` VALUES (12006, 'Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection', '3D lane detection', 'Geometry-guided anchor', 'Two-stage framework', 'Monocular camera', 'Unified network', 'We present a generalized and scalable method, called Gen-LaneNet, to detect 3D lanes from a single image. The method, inspired by the latest state-of-the-art 3D-LaneNet, is a unified framework solving image encoding, spatial transform of features and 3D lane prediction in a single network. However, we propose unique designs for Gen-LaneNet in two folds. First, we introduce a new geometry-guided lane anchor representation in a new coordinate frame and apply a specific geometric transformation to directly calculate real 3D lane points from the network output. We demonstrate that aligning the lane points with the underlying top-view features in the new coordinate frame is critical towards a generalized method in handling unfamiliar scenes. Second, we present a scalable two-stage framework that decouples the learning of image segmentation subnetwork and geometry encoding subnetwork. Compared to 3D-LaneNet, the proposed Gen-LaneNet drastically reduces the amount of 3D lane labels required to achieve a robust solution in real-world applications. Moreover, we release a new synthetic dataset and its construction strategy to encourage the development and evaluation of 3D lane detection methods. In experiments, we conduct extensive ablation study to substantiate the proposed Gen-LaneNet significantly outperforms 3D-LaneNet in average precision (AP) and F-measure.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_40');
INSERT INTO `paper` VALUES (12007, 'General 3D Room Layout from a Single View by Render-and-Compare', 'Room layout', '3D geometry', 'Analysis-by-synthesis', '', '', 'We present a novel method to reconstruct the 3D layout of a room—walls, floors, ceilings—from a single perspective view in challenging conditions, by contrast with previous single-view methods restricted to cuboid-shaped layouts. This input view can consist of a color image only, but considering a depth map results in a more accurate reconstruction. Our approach is formalized as solving a constrained discrete optimization problem to find the set of 3D polygons that constitute the layout. In order to deal with occlusions between components of the layout, which is a problem ignored by previous works, we introduce an analysis-by-synthesis method to iteratively refine the 3D layout estimate. As no dataset was available to evaluate our method quantitatively, we created one together with several appropriate metrics. Our dataset consists of 293 images from ScanNet, which we annotated with precise 3D layouts. It offers three times more samples than the popular NYUv2 303 benchmark, and a much larger variety of layouts.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_12');
INSERT INTO `paper` VALUES (12008, 'Generalized Many-Way Few-Shot Video Classification', '', '', '', '', '', 'Few-shot learning methods operate in low data regimes. The aim is to learn with few training examples per class. Although significant progress has been made in few-shot image classification, few-shot video recognition is relatively unexplored and methods based on 2D CNNs are unable to learn temporal information. In this work we thus develop a simple 3D CNN baseline, surpassing existing methods by a large margin. To circumvent the need of labeled examples, we propose to leverage weakly-labeled videos from a large dataset using tag retrieval followed by selecting the best clips with visual similarities, yielding further improvement. Our results saturate current 5-way benchmarks for few-shot video classification and therefore we propose a new challenging benchmark involving more classes and a mixture of classes with varying supervision.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_10');
INSERT INTO `paper` VALUES (12009, 'Generalizing Person Re-Identification by Camera-Aware Invariance Learning and Cross-Domain Mixup', 'Domain adaptation', 'Person re-identification', 'Camera-aware invariance learning', 'Cross-domain mixup', '', 'Despite the impressive performance under the single-domain setup, current fully-supervised models for person re-identification (re-ID) degrade significantly when deployed to an unseen domain. According to the characteristics of cross-domain re-ID, such degradation is mainly attributed to the dramatic variation within the target domain and the severe shift between the source and target domain. To achieve a model that generalizes well to the target domain, it is desirable to take both issues into account. In terms of the former issue, one of the most successful solutions is to enforce consistency between nearest-neighbors in the embedding space. However, we find that the search of neighbors is highly biased due to the discrepancy across cameras. To this end, we improve the vanilla neighborhood invariance approach by imposing the constraint in a camera-aware manner. As for the latter issue, we propose a novel cross-domain mixup scheme. It alleviates the abrupt transfer by introducing the interpolation between the two domains as a transition state. Extensive experiments on three public benchmarks demonstrate the superiority of our method. Without any auxiliary data or models, it outperforms existing state-of-the-arts by a large margin. The code is available at https://github.com/LuckyDC/generalizing-reid.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_14');
INSERT INTO `paper` VALUES (12010, 'Generate to Adapt: Resolution Adaption Network for Surveillance Face Recognition', 'Surveillance face recognition', 'Generative adversarial networks', 'Feature adaption', '', '', 'Although deep learning techniques have largely improved face recognition, unconstrained surveillance face recognition is still an unsolved challenge, due to the limited training data and the gap of domain distribution. Previous methods mostly match low-resolution and high-resolution faces in different domains, which tend to deteriorate the original feature space in the common recognition scenarios. To avoid this problem, we propose resolution adaption network (RAN) which contains Multi-Resolution Generative Adversarial Networks (MR-GAN) followed by a feature adaption network. MR-GAN learns multi-resolution representations and randomly selects one resolution to generate realistic low-resolution (LR) faces that can avoid the artifacts of down-sampled faces. A novel feature adaption network with translation gate is developed to fuse the discriminative information of LR faces into backbone network, while preserving the discrimination ability of original face representations. The experimental results on IJB-C TinyFace, SCface, QMUL-SurvFace datasets have demonstrated the superiority of our method compared with state-of-the-art surveillance face recognition methods, while showing stable performance on the common recognition scenarios.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_44');
INSERT INTO `paper` VALUES (12011, 'Generating Handwriting via Decoupled Style Descriptors', '', '', '', '', '', 'Representing a space of handwriting stroke styles includes the challenge of representing both the style of each character and the overall style of the human writer. Existing VRNN approaches to representing handwriting often do not distinguish between these different style components, which can reduce model capability. Instead, we introduce the Decoupled Style Descriptor (DSD) model for handwriting, which factors both character- and writer-level styles and allows our model to represent an overall greater space of styles. This approach also increases flexibility: given a few examples, we can generate handwriting in new writer styles, and also now generate handwriting of new characters across writer styles. In experiments, our generated results were preferred over a state of the art baseline method 88% of the time, and in a writer identification task on 20 held-out writers, our DSDs achieved 89.38% accuracy from a single sample word. Overall, DSDs allows us to improve both the quality and flexibility over existing handwriting stroke generation approaches.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_45');
INSERT INTO `paper` VALUES (12012, 'Generating Videos of Zero-Shot Compositions of Actions and Objects', 'Video generation', 'Compositionality in videos', '', '', '', 'Human activity videos involve rich, varied interactions between people and objects. In this paper we develop methods for generating such videos – making progress toward addressing the important, open problem of video generation in complex scenes. In particular, we introduce the task of generating human-object interaction videos in a zero-shot compositional setting, i.e., generating videos for action-object compositions that are unseen during training, having seen the target action and target object separately. This setting is particularly important for generalization in human activity video generation, obviating the need to observe every possible action-object combination in training and thus avoiding the combinatorial explosion involved in modeling complex scenes. To generate human-object interaction videos, we propose a novel adversarial framework HOI-GAN which includes multiple discriminators focusing on different aspects of a video. To demonstrate the effectiveness of our proposed framework, we perform extensive quantitative and qualitative evaluation on two challenging datasets: EPIC-Kitchens and 20BN-Something-Something v2.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_23');
INSERT INTO `paper` VALUES (12013, 'Generating Visual and Semantic Explanations with Multi-task Network', 'Multi-task learning', 'Explainable AI', '', '', '', 'Explaining deep models is desirable especially for improving the user trust and experience. Much progress has been done recently towards visually and semantically explaining deep models. However, establishing the most effective explanation is often human-dependent, which suffers from the bias of the annotators. To address this issue, we propose a multitask learning network (MTL-Net) that generates saliency-based visual explanation as well as attribute-based semantic explanation. Via an integrated evaluation mechanism, our model quantitatively evaluates the quality of the generated explanations. First, we introduce attributes to the image classification process and rank the attribute contribution with gradient weighted mapping, then generate semantic explanations with those attributes. Second, we propose a fusion classification mechanism (FCM) to evaluate three recent saliency-based visual explanation methods by their influence on the classification. Third, we conduct user studies, quantitative and qualitative evaluations. According to our results on three benchmark datasets with varying size and granularity, our attribute-based semantic explanations are not only helpful to the user but they also improve the classification accuracy of the model, and our ranking framework detects the best performing visual explanation method in agreement with the users.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_40');
INSERT INTO `paper` VALUES (12014, 'Generative Low-Bitwidth Data Free Quantization', 'Data free compression', 'Low-bitwidth quantization', 'Knowledge matching generator', '', '', 'Neural network quantization is an effective way to compress deep models and improve their execution latency and energy efficiency, so that they can be deployed on mobile or embedded devices. Existing quantization methods require original data for calibration or fine-tuning to get better performance. However, in many real-world scenarios, the data may not be available due to confidential or private issues, thereby making existing quantization methods not applicable. Moreover, due to the absence of original data, the recently developed generative adversarial networks (GANs) cannot be applied to generate data. Although the full-precision model may contain rich data information, such information alone is hard to exploit for recovering the original data or generating new meaningful data. In this paper, we investigate a simple-yet-effective method called Generative Low-bitwidth Data Free Quantization (GDFQ) to remove the data dependence burden. Specifically, we propose a knowledge matching generator to produce meaningful fake data by exploiting classification boundary knowledge and distribution information in the pre-trained model. With the help of generated data, we can quantize a model by learning knowledge from the pre-trained model. Extensive experiments on three data sets demonstrate the effectiveness of our method. More critically, our method achieves much higher accuracy on 4-bit quantization than the existing data free quantization method. Code is available at https://github.com/xushoukai/GDFQ.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_1');
INSERT INTO `paper` VALUES (12015, 'Generative Sparse Detection Networks for 3D Single-Shot Object Detection', 'Single shot detection', '3D object detection', 'Generative sparse network', 'Point cloud', '', '3D object detection has been widely studied due to its potential applicability to many promising areas such as robotics and augmented reality. Yet, the sparse nature of the 3D data poses unique challenges to this task. Most notably, the observable surface of the 3D point clouds is disjoint from the center of the instance to ground the bounding box prediction on. To this end, we propose Generative Sparse Detection Network (GSDN), a fully-convolutional single-shot sparse detection network that efficiently generates the support for object proposals. The key component of our model is a generative sparse tensor decoder, which uses a series of transposed convolutions and pruning layers to expand the support of sparse tensors while discarding unlikely object centers to maintain minimal runtime and memory footprint. GSDN can process unprecedentedly large-scale inputs with a single fully-convolutional feed-forward pass, thus does not require the heuristic post-processing stage that stitches results from sliding windows as other previous methods have. We validate our approach on three 3D indoor datasets including the large-scale 3D indoor reconstruction dataset where our method outperforms the state-of-the-art methods by a relative improvement of 7.14% while being 3.78 times faster than the best prior work.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_18');
INSERT INTO `paper` VALUES (12016, 'Generative View-Correlation Adaptation for Semi-supervised Multi-view Learning', 'Multi-view learning', 'Data augmentation', 'Semi-supervised learning', '', '', 'Multi-view learning (MVL) explores the data extracted from multiple resources. It assumes that the complementary information between different views could be revealed to further improve the learning performance. There are two challenges. First, it is difficult to effectively combine the different view data while still fully preserve the view-specific information. Second, multi-view datasets are usually small, which means the model can be easily overfitted. To address the challenges, we propose a novel View-Correlation Adaptation (VCA) framework in semi-supervised fashion. A semi-supervised data augmentation me-thod is designed to generate extra features and labels based on both labeled and unlabeled samples. In addition, a cross-view adversarial training strategy is proposed to explore the structural information from one view and help the representation learning of the other view. Moreover, an effective and simple fusion network is proposed for the late fusion stage. In our model, all networks are jointly trained in an end-to-end fashion. Extensive experiments demonstrate that our approach is effective and stable compared with other state-of-the-art methods (Code is available on: https://github.com/wenwen0319/GVCA).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_19');
INSERT INTO `paper` VALUES (12017, 'Genetic-GAN: Synthesizing Images Between Two Domains by Genetic Crossover', 'Image to image translation', 'Unsupervised learning', 'Genetic crossover', 'Generative adversarial network', '', 'Synthesizing an interpolated image between two real images can be achieved by a simple interpolation on the latent space of the images, so that the resulting image inherits features from both. The task becomes more difficult when two images are in different domains, because an interpolated image whose latent representation lies near the middle of two distant input images may not be realistic and may end up in either domain. In this paper, we present a novel technique called Genetic-GAN that solves a novel problem of synthesizing a set of images that inherit features from both of the domains, while at the same time allowing control of which domain the resulting images fall into. We experiment on human face images using female and male genders as two different domains. We show that our method can take two images with very different attributes and synthesize images between them, and can perform domain transformations.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_19');
INSERT INTO `paper` VALUES (12018, 'GeoGraph: Graph-Based Multi-view Object Detection with Geometric Cues End-to-End', 'Object detection', 'Re-identification', 'Graph Neural Networks', 'Urban objects', 'Multi-view', 'In this paper we propose an end-to-end learnable approach that detects static urban objects from multiple views, re-identifies instances, and finally assigns a geographic position per object. Our method relies on a Graph Neural Network (GNN) to, detect all objects and output their geographic positions given images and approximate camera poses as input. Our GNN simultaneously models relative pose and image evidence, and is further able to deal with an arbitrary number of input views. Our method is robust to occlusion, with similar appearance of neighboring objects, and severe changes in viewpoints by jointly reasoning about visual image appearance and relative pose. Experimental evaluation on two challenging, large-scale datasets and comparison with state-of-the-art methods show significant and systematic improvements both in accuracy and efficiency, with 2–6% gain in detection and re-ID average precision as well as 8x reduction of training time.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_29');
INSERT INTO `paper` VALUES (12019, 'Geographically Local Representation Learning with a Spatial Prior for Visual Localization', 'Visual localization', 'Cross-view image matching', 'Image retrieval', 'End-to-end representation learning', '', 'We revisit end-to-end representation learning for cross-view self-localization, the task of retrieving for a query camera image the closest satellite image in a database by matching them in a shared image representation space. Previous work tackles this task as a global localization problem, i.e. assuming no prior knowledge on the location, thus the learned image representation must distinguish far apart areas of the map. However, in many practical applications such as self-driving vehicles, it is already possible to discard distant locations through well-known localization techniques using temporal filters and GNSS/GPS sensors. We argue that learned features should therefore be optimized to be discriminative within the geographic local neighborhood, instead of globally. We propose a simple but effective adaptation to the common triplet loss used in previous work to consider a prior localization estimate already in the training phase. We evaluate our approach on the existing CVACT dataset, and on a novel localization benchmark based on the Oxford RobotCar dataset which tests generalization across multiple traversals and days in the same area. For the Oxford benchmarks we collected corresponding satellite images. With a localization prior, our approach improves recall@1 by 9% points on CVACT, and reduces the median localization error by 2.45 m on the Oxford benchmark, compared to a state-of-the-art baseline approach. Qualitative results underscore that with our approach the network indeed captures different aspects of the local surroundings compared to the global baseline.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_38');
INSERT INTO `paper` VALUES (12020, 'GeoLayout: Geometry Driven Room Layout Estimation Based on Depth Maps of Planes', 'Room layout estimation', 'Plane segmentation', 'Dataset', '', '', 'The task of room layout estimation is to locate the wall-floor, wall-ceiling, and wall-wall boundaries. Most recent methods solve this problem based on edge/keypoint detection or semantic segmentation. However, these approaches have shown limited attention on the geometry of the dominant planes and the intersection between them, which has significant impact on room layout. In this work, we propose to incorporate geometric reasoning to deep learning for layout estimation. Our approach learns to infer the depth maps of the dominant planes in the scene by predicting the pixel-level surface parameters, and the layout can be generated by the intersection of the depth maps. Moreover, we present a new dataset with pixel-level depth annotation of dominant planes. It is larger than the existing datasets and contains both cuboid and non-cuboid rooms. Experimental results show that our approach produces considerable performance gains on both 2D and 3D datasets.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_37');
INSERT INTO `paper` VALUES (12021, 'Geolocating Time: Digitisation and Reverse Engineering of a Roman Sundial', 'Computational archaeology', '3D reconstruction', 'Reverse engineering', '', '', 'The sundial of Euporus was discovered in 1878 within the ancient Roman city of Aquileia (Italy), in a quite unusual location at the centre of the city’s horse race track. Studies have tried to demonstrate that the sundial had been made for a more southern location than the one it was found at, although no specific alternative positions have been suggested. This paper showcases both the workflow designed to fully digitise it in 3D and analyses on the use of the artefact undertaken from it. The final 3D reconstruction achieves accuracies of a few millimetres, thus offering the opportunity to analyse small details of its surface and to perform non-trivial measurements. We also propose a mathematical approach to compute the object’s optimal working latitude as well as the gnomon position and orientation. The algorithm is designed as an optimization problem where the sundial’s inscriptions and the Sun positions during daytime are considered to obtain the optimal configuration. The complete 3D model of the object is used to get all the geometrical information needed to validate the results of computations.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_11');
INSERT INTO `paper` VALUES (12022, 'Geometric Correspondence Fields: Learned Differentiable Rendering for 3D Pose Refinement in the Wild', '', '', '', '', '', 'We present a novel 3D pose refinement approach based on differentiable rendering for objects of arbitrary categories in the wild. In contrast to previous methods, we make two main contributions: First, instead of comparing real-world images and synthetic renderings in the RGB or mask space, we compare them in a feature space optimized for 3D pose refinement. Second, we introduce a novel differentiable renderer that learns to approximate the rasterization backward pass from data instead of relying on a hand-crafted algorithm. For this purpose, we predict deep cross-domain correspondences between RGB images and 3D model renderings in the form of what we call geometric correspondence fields. These correspondence fields serve as pixel-level gradients which are analytically propagated backward through the rendering pipeline to perform a gradient-based optimization directly on the 3D pose. In this way, we precisely align 3D models to objects in RGB images which results in significantly improved 3D pose estimates. We evaluate our approach on the challenging Pix3D dataset and achieve up to 55% relative improvement compared to state-of-the-art refinement methods in multiple metrics.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_7');
INSERT INTO `paper` VALUES (12023, 'Geometric Estimation via Robust Subspace Recovery', 'Geometric estimation', 'Robust model fitting', '3D vision', 'Robust subspace recovery', '', 'Geometric estimation from image point correspondences is the core procedure of many 3D vision problems, which is prevalently accomplished by random sampling techniques. In this paper, we consider the problem from an optimization perspective, to exploit the intrinsic linear structure of point correspondences to assist estimation. We generalize the conventional method to a robust one and extend the previous analysis for linear structure to develop several new algorithms. The proposed solutions essentially address the estimation problem by solving a subspace recovery problem to identify the inliers. Experiments on real-world image datasets for both fundamental matrix and homography estimation demonstrate the superiority of our method over the state-of-the-art in terms of both robustness and accuracy.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_28');
INSERT INTO `paper` VALUES (12024, 'Geometry Constrained Weakly Supervised Object Localization', '', '', '', '', '', 'We propose a geometry constrained network, termed GC-Net, for weakly supervised object localization (WSOL). GC-Net consists of three modules: a detector, a generator and a classifier. The detector predicts the object location defined by a set of coefficients describing a geometric shape (i.e. ellipse or rectangle), which is geometrically constrained by the mask produced by the generator. The classifier takes the resulting masked images as input and performs two complementary classification tasks for the object and background. To make the mask more compact and more complete, we propose a novel multi-task loss function that takes into account area of the geometric shape, the categorical cross-entropy and the negative entropy. In contrast to previous approaches, GC-Net is trained end-to-end and predict object location without any post-processing (e.g. thresholding) that may require additional tuning. Extensive experiments on the CUB-200-2011 and ILSVRC2012 datasets show that GC-Net outperforms state-of-the-art methods by a large margin. Our source code is available at https://github.com/lwzeng/GC-Net.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_29');
INSERT INTO `paper` VALUES (12025, 'Germination Detection of Seedlings in Soil: A System, Dataset and Challenge', 'Transplantation guidance', 'High-throughput plant phenotyping', 'Automation', 'Arabidopsis', 'Two leaf stadium', 'In phenotyping experiments plants are often germinated in high numbers, and in a manual transplantation step selected and moved to single pots. Selection is based on visually derived germination date, visual size, or health inspection. Such values are often inaccurate, as evaluating thousands of tiny seedlings is tiring. We address these issues by quantifying germination detection with an automated, imaging-based device, and by a visual support system for inspection and transplantation. While this is a great help and reduces the need for visual inspection, accuracy of seedling detection is not yet sufficient to allow skipping the inspection step. We therefore present a new dataset and challenge containing 19.5k images taken by our germination detection system and manually verified labels. We describe in detail the involved automated system and handling setup. As baseline we report the performances of the currently applied color-segmentation based algorithm and of five transfer-learned deep neural networks.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_25');
INSERT INTO `paper` VALUES (12026, 'GIA-Net: Global Information Aware Network for Low-Light Imaging', '', '', '', '', '', 'It is extremely challenging to acquire perceptually plausible images under low-light conditions due to low SNR. Most recently, U-Nets have shown promising results for low-light imaging. However, vanilla U-Nets generate images with artifacts such as color inconsistency due to the lack of global color information. In this paper, we propose a global information aware (GIA) module, which is capable of extracting and integrating the global information into the network to improve the performance of low-light imaging. The GIA module can be inserted into a vanilla U-Net with negligible extra learnable parameters or computational cost. Moreover, a GIA-Net is constructed, trained and evaluated on a large scale real-world low-light imaging dataset. Experimental results show that the proposed GIA-Net outperforms the state-of-the-art methods in terms of four metrics, including deep metrics that measure perceptual similarities. Extensive ablation studies have been conducted to verify the effectiveness of the proposed GIA-Net for low-light imaging by utilizing global information.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_20');
INSERT INTO `paper` VALUES (12027, 'GINet: Graph Interaction Network for Scene Parsing', 'Scene parsing', 'Context reasoning', 'Graph interaction', '', '', 'Recently, context reasoning using image regions beyond local convolution has shown great potential for scene parsing. In this work, we explore how to incorperate the linguistic knowledge to promote context reasoning over image regions by proposing a Graph Interaction unit (GI unit) and a Semantic Context Loss (SC-loss). The GI unit is capable of enhancing feature representations of convolution networks over high-level semantics and learning the semantic coherency adaptively to each sample. Specifically, the dataset-based linguistic knowledge is first incorporated in the GI unit to promote context reasoning over the visual graph, then the evolved representations of the visual graph are mapped to each local representation to enhance the discriminated capability for scene parsing. GI unit is further improved by the SC-loss to enhance the semantic representations over the exemplar-based semantic graph. We perform full ablation studies to demonstrate the effectiveness of each component in our approach. Particularly, the proposed GINet outperforms the state-of-the-art approaches on the popular benchmarks, including Pascal-Context and COCO Stuff.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_3');
INSERT INTO `paper` VALUES (12028, 'GIQA: Generated Image Quality Assessment', 'Generative model', 'Generative adversarial networks', 'Image quality assessment', '', '', 'Generative adversarial networks (GANs) achieve impressive results today, but not all generated images are perfect. A number of quantitative criteria have recently emerged for generative models, but none of them are designed for a single generated image. In this paper, we propose a new research topic, Generated Image Quality Assessment (GIQA), which quantitatively evaluates the quality of each generated image. We introduce three GIQA algorithms from two perspectives: learning-based and data-based. We evaluate a number of images generated by various recent GAN models on different datasets and demonstrate that they are consistent with human assessments. Furthermore, GIQA is available for many applications, like separately evaluating the realism and diversity of generative models, and enabling online hard negative mining (OHEM) in the training of GANs to improve the results.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_22');
INSERT INTO `paper` VALUES (12029, 'Global and Local Enhancement Networks for Paired and Unpaired Image Enhancement', 'Image enhancement', 'Unpaired learning', 'Generative adversarial network', '', '', 'A novel approach for paired and unpaired image enhancement is proposed in this work. First, we develop global enhancement network (GEN) and local enhancement network (LEN), which can faithfully enhance images. The proposed GEN performs the channel-wise intensity transforms that can be trained easier than the pixel-wise prediction. The proposed LEN refines GEN results based on spatial filtering. Second, we propose different training schemes for paired learning and unpaired learning to train GEN and LEN. Especially, we propose a two-stage training scheme based on generative adversarial networks for unpaired learning. Experimental results demonstrate that the proposed algorithm outperforms the state-of-the-arts in paired and unpaired image enhancement. Notably, the proposed unpaired image enhancement algorithm provides better results than recent state-of-the-art paired image enhancement algorithms. The source codes and trained models are available at https://github.com/hukim1124/GleNet.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_21');
INSERT INTO `paper` VALUES (12030, 'Global Distance-Distributions Separation for Unsupervised Person Re-identification', 'Unsupervised learning', 'Person re-identification', 'Global distance-distributions separation', 'Momentum update', 'Hard mining', 'Supervised person re-identification (ReID) often has poor scalability and usability in real-world deployments due to domain gaps and the lack of annotations for the target domain data. Unsupervised person ReID through domain adaptation is attractive yet challenging. Existing unsupervised ReID approaches often fail in correctly identifying the positive samples and negative samples through the distance-based matching/ranking. The two distributions of distances for positive sample pairs (Pos-distr) and negative sample pairs (Neg-distr) are often not well separated, having large overlap. To address this problem, we introduce a global distance-distributions separation (GDS) constraint over the two distributions to encourage the clear separation of positive and negative samples from a global view. We model the two global distance distributions as Gaussian distributions and push apart the two distributions while encouraging their sharpness in the unsupervised training process. Particularly, to model the distributions from a global view and facilitate the timely updating of the distributions and the GDS related losses, we leverage a momentum update mechanism for building and maintaining the distribution parameters (mean and variance) and calculate the loss on the fly during the training. Distribution-based hard mining is proposed to further promote the separation of the two distributions. We validate the effectiveness of the GDS constraint in unsupervised ReID networks. Extensive experiments on multiple ReID benchmark datasets show our method leads to significant improvement over the baselines and achieves the state-of-the-art performance.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_43');
INSERT INTO `paper` VALUES (12031, 'Global-and-Local Relative Position Embedding for Unsupervised Video Summarization', 'Video summarization', 'Relative position embedding', 'Unsupervised Learning', '', '', 'In order to summarize a content video properly, it is important to grasp the sequential structure of video as well as the long-term dependency between frames. The necessity of them is more obvious, especially for unsupervised learning. One possible solution is to utilize a well-known technique in the field of natural language processing for long-term dependency and sequential property: self-attention with relative position embedding (RPE). However, compared to natural language processing, video summarization requires capturing a much longer length of the global context. In this paper, we therefore present a novel input decomposition strategy, which samples the input both globally and locally. This provides an effective temporal window for RPE to operate and improves overall computational efficiency significantly. By combining both Global-and-Local input decomposition and RPE together, we come up with GL-RPE. Our approach allows the network to capture both local and global interdependencies between video frames effectively. Since GL-RPE can be easily integrated into the existing methods, we apply it to two different unsupervised backbones. We provide extensive ablation studies and visual analysis to verify the effectiveness of the proposals. We demonstrate our approach achieves new state-of-the-art performance using the recently proposed rank order-based metrics: Kendall’s \\(\\tau \\) and Spearman’s \\(\\rho \\). Furthermore, despite our method is unsupervised, we show ours perform on par with the fully-supervised method.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_11');
INSERT INTO `paper` VALUES (12032, 'Globally Optimal and Efficient Vanishing Point Estimation in Atlanta World', '', '', '', '', '', 'Atlanta world holds for the scenes composed of a vertical dominant direction and several horizontal dominant directions. Vanishing point (VP) is the intersection of the image lines projected from parallel 3D lines. In Atlanta world, given a set of image lines, we aim to cluster them by the unknown-but-sought VPs whose number is unknown. Existing approaches are prone to missing partial inliers, rely on prior knowledge of the number of VPs, and/or lead to low efficiency. To overcome these limitations, we propose the novel mine-and-stab (MnS) algorithm and embed it in the branch-and-bound (BnB) algorithm. Different from BnB that iteratively branches the full parameter intervals, our MnS directly mines the narrow sub-intervals and then stabs them by probes. We simultaneously search for the vertical VP by BnB and horizontal VPs by MnS. The proposed collaboration between BnB and MnS guarantees global optimality in terms of maximizing the number of inliers. It can also automatically determine the number of VPs. Moreover, its efficiency is suitable for practical applications. Experiments on synthetic and real-world datasets showed that our method outperforms state-of-the-art approaches in terms of accuracy and/or efficiency.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_10');
INSERT INTO `paper` VALUES (12033, 'Globally-Optimal Event Camera Motion Estimation', 'Event cameras', 'Motion estimation', 'Contrast maximisation', 'Global optimality', 'Branch and bound', 'Event cameras are bio-inspired sensors that perform well in HDR conditions and have high temporal resolution. However, different from traditional frame-based cameras, event cameras measure asynchronous pixel-level brightness changes and return them in a highly discretised format, hence new algorithms are needed. The present paper looks at fronto-parallel motion estimation of an event camera. The flow of the events is modeled by a general homographic warping in a space-time volume, and the objective is formulated as a maximisation of contrast within the image of unwarped events. However, in stark contrast to prior art, we derive a globally optimal solution to this generally non-convex problem, and thus remove the dependency on a good initial guess. Our algorithm relies on branch-and-bound optimisation for which we derive novel, recursive upper and lower bounds for six different contrast estimation functions. The practical validity of our approach is supported by a highly successful application to AGV motion estimation with a downward facing event camera, a challenging scenario in which the sensor experiences fronto-parallel motion in front of noisy, fast moving textures.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_4');
INSERT INTO `paper` VALUES (12034, 'GMNet: Graph Matching Network for Large Scale Part Semantic Segmentation in the Wild', 'Part parsing', 'Semantic segmentation', 'Graph matching', 'Deep learning', '', 'The semantic segmentation of parts of objects in the wild is a challenging task in which multiple instances of objects and multiple parts within those objects must be detected in the scene. This problem remains nowadays very marginally explored, despite its fundamental importance towards detailed object understanding. In this work, we propose a novel framework combining higher object-level context conditioning and part-level spatial relationships to address the task. To tackle object-level ambiguity, a class-conditioning module is introduced to retain class-level semantics when learning parts-level semantics. In this way, mid-level features carry also this information prior to the decoding stage. To tackle part-level ambiguity and localization we propose a novel adjacency graph-based module that aims at matching the relative spatial relationships between ground truth and predicted parts. The experimental evaluation on the Pascal-Part dataset shows that we achieve state-of-the-art results on this task.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_24');
INSERT INTO `paper` VALUES (12035, 'GRAB: A Dataset of Whole-Body Human Grasping of Objects', '', '', '', '', '', 'Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_34');
INSERT INTO `paper` VALUES (12036, 'Gradient Centralization: A New Optimization Technique for Deep Neural Networks', 'Deep network optimization', 'Gradient descent', '', '', '', 'Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and it can be embedded into existing gradient based DNN optimizers with only one line of code. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_37');
INSERT INTO `paper` VALUES (12037, 'Gradient-Induced Co-Saliency Detection', 'Co-saliency detection', 'New dataset', 'Gradient inducing', 'Jigsaw training', '', 'Co-saliency detection (Co-SOD) aims to segment the common salient foreground in a group of relevant images. In this paper, inspired by human behavior, we propose a gradient-induced co-saliency detection (GICD) method. We first abstract a consensus representation for a group of images in the embedding space; then, by comparing the single image with consensus representation, we utilize the feedback gradient information to induce more attention to the discriminative co-salient features. In addition, due to the lack of Co-SOD training data, we design a jigsaw training strategy, with which Co-SOD networks can be trained on general saliency datasets without extra pixel-level annotations. To evaluate the performance of Co-SOD methods on discovering the co-salient object among multiple foregrounds, we construct a challenging CoCA dataset, where each image contains at least one extraneous foreground along with the co-salient object. Experiments demonstrate that our GICD achieves state-of-the-art performance. Our codes and dataset are available at https://mmcheng.net/gicd/.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_27');
INSERT INTO `paper` VALUES (12038, 'Graph Convolutional Networks for Learning with Few Clean and Many Noisy Labels', '', '', '', '', '', 'In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier, which learns to discriminate clean from noisy examples using a weighted binary cross-entropy loss function. The GCN-inferred “clean” probability is then exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data, as well as standard few-shot classification where only few clean examples are used.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_17');
INSERT INTO `paper` VALUES (12039, 'Graph Edit Distance Reward: Learning to Edit Scene Graph', 'Scene graph editing', 'Policy gradient', 'Graph matching', '', '', 'Scene Graph, as a vital tool to bridge the gap between language domain and image domain, has been widely adopted in the cross-modality task like VQA. In this paper, we propose a new method to edit the scene graph according to the user instructions, which has never been explored. To be specific, in order to learn editing scene graphs as the semantics given by texts, we propose a Graph Edit Distance Reward, which is based on the Policy Gradient and Graph Matching algorithm, to optimize neural symbolic model. In the context of text-editing image retrieval, we validate the effectiveness of our method in CSS and CRIR dataset. Besides, CRIR is a new synthetic dataset generated by us, which we will publish soon for future use.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_32');
INSERT INTO `paper` VALUES (12040, 'Graph Wasserstein Correlation Analysis for Movie Retrieval', 'Graph Wasserstein metric', 'Graph correlation analysis', 'Movie retrieval', '', '', 'Movie graphs play an important role to bridge heterogenous modalities of videos and texts in human-centric retrieval. In this work, we propose Graph Wasserstein Correlation Analysis (GWCA) to deal with the core issue therein, i.e, cross heterogeneous graph comparison. Spectral graph filtering is introduced to encode graph signals, which are then embedded as probability distributions in a Wasserstein space, called graph Wasserstein metric learning. Such a seamless integration of graph signal filtering together with metric learning results in a surprise consistency on both learning processes, in which the goal of metric learning is just to optimize signal filters or vice versa. Further, we derive the solution of the graph comparison model as a classic generalized eigenvalue decomposition problem, which has an exactly closed-form solution. Finally, GWCA together with movie/text graphs generation are unified into the framework of movie retrieval to evaluate our proposed method. Extensive experiments on MovieGrpahs dataset demonstrate the effectiveness of our GWCA as well as the entire framework.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_26');
INSERT INTO `paper` VALUES (12041, 'Graph-Based Social Relation Reasoning', 'Social relation reasoning', 'Paradigm shift', 'Graph neural networks', 'Social relation graph', '', 'Human beings are fundamentally sociable—that we generally organize our social lives in terms of relations with other people. Understanding social relations from an image has great potential for intelligent systems such as social chatbots and personal assistants. In this paper, we propose a simpler, faster, and more accurate method named graph relational reasoning network (GR\\(^2\\)N) for social relation recognition. Different from existing methods which process all social relations on an image independently, our method considers the paradigm of jointly inferring the relations by constructing a social relation graph. Furthermore, the proposed GR\\(^2\\)N constructs several virtual relation graphs to explicitly grasp the strong logical constraints among different types of social relations. Experimental results illustrate that our method generates a reasonable and consistent social relation graph and improves the performance in both accuracy and efficiency.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_2');
INSERT INTO `paper` VALUES (12042, 'Graph-PCNN: Two Stage Human Pose Estimation with Graph Pose Refinement', 'Human pose estimation', 'Keypoint localization', 'Two stage', 'Graph pose refinement', '', 'Recently, most of the state-of-the-art human pose estimation methods are based on heatmap regression. The final coordinates of keypoints are obtained by decoding heatmap directly. In this paper, we aim to find a better approach to get more accurate localization results. We mainly put forward two suggestions for improvement: 1) different features and methods should be applied for rough and accurate localization, 2) relationship between keypoints should be considered. Specifically, we propose a two-stage graph-based and model-agnostic framework, called Graph-PCNN, with a localization subnet and a graph pose refinement module added onto the original heatmap regression network. In the first stage, heatmap regression network is applied to obtain a rough localization result, and a set of proposal keypoints, called guided points, are sampled. In the second stage, for each guided point, different visual feature is extracted by the localization subnet. The relationship between guided points is explored by the graph pose refinement module to get more accurate localization results. Experiments show that Graph-PCNN can be used in various backbones to boost the performance by a large margin. Without bells and whistles, our best model can achieve a new state-of-the-art 76.8% AP on COCO test-dev split.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_29');
INSERT INTO `paper` VALUES (12043, 'GRNet: Gridding Residual Network for Dense Point Cloud Completion', 'Point cloud completion', 'Gridding', 'Cubic feature sampling', '', '', 'Estimating the complete 3D point cloud from an incomplete one is a key problem in many vision and robotics applications. Mainstream methods (e.g., PCN and TopNet) use Multi-layer Perceptrons (MLPs) to directly process point clouds, which may cause the loss of details because the structural and context of point clouds are not fully considered. To solve this problem, we introduce 3D grids as intermediate representations to regularize unordered point clouds and propose a novel Gridding Residual Network (GRNet) for point cloud completion. In particular, we devise two novel differentiable layers, named Gridding and Gridding Reverse, to convert between point clouds and 3D grids without losing structural information. We also present the differentiable Cubic Feature Sampling layer to extract features of neighboring points, which preserves context information. In addition, we design a new loss function, namely Gridding Loss, to calculate the L1 distance between the 3D grids of the predicted and ground truth point clouds, which is helpful to recover details. Experimental results indicate that the proposed GRNet performs favorably against state-of-the-art methods on the ShapeNet, Completion3D, and KITTI benchmarks.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_21');
INSERT INTO `paper` VALUES (12044, 'GroSS: Group-Size Series Decomposition for Grouped Architecture Search', 'Group convolution', 'Network acceleration', 'Architecture search', '', '', 'We present a novel approach which is able to explore the configuration of grouped convolutions within neural networks. Group-size Series (GroSS) decomposition is a mathematical formulation of tensor factorisation into a series of approximations of increasing rank terms. GroSS allows for dynamic and differentiable selection of factorisation rank, which is analogous to a grouped convolution. Therefore, to the best of our knowledge, GroSS is the first method to enable simultaneous training of differing numbers of groups within a single layer, as well as all possible combinations between layers. In doing so, GroSS is able to train an entire grouped convolution architecture search-space concurrently. We demonstrate this through architecture searches with performance objectives on multiple datasets and networks. GroSS enables more effective and efficient search for grouped convolutional architectures.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_2');
INSERT INTO `paper` VALUES (12045, 'Grounded Situation Recognition', 'Situation recognition', 'Scene understanding', 'Grounding', '', '', 'We introduce Grounded Situation Recognition (GSR), a task that requires producing structured semantic summaries of images describing: the primary activity, entities engaged in the activity with their roles (e.g. agent, tool), and bounding-box groundings of entities. GSR presents important technical challenges: identifying semantic saliency, categorizing and localizing a large and diverse set of entities, overcoming semantic sparsity, and disambiguating roles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To study this new task we create the Situations With Groundings (SWiG) dataset which adds 278,336 bounding-box groundings to the 11,538 entity classes in the imSitu dataset. We propose a Joint Situation Localizer and find that jointly predicting situations and groundings with end-to-end training handily outperforms independent training on the entire grounding metric suite with relative gains between 8% and 32%. Finally, we show initial findings on three exciting future directions enabled by our models: conditional querying, visual chaining, and grounded semantic aware image retrieval. Code and data available at https://prior.allenai.org/projects/gsr.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_19');
INSERT INTO `paper` VALUES (12046, 'Group Activity Prediction with Sequential Relational Anticipation Model', 'Activity prediction', 'Group activity', 'Structured prediction', 'Relational model.', '', 'In this paper, we propose a novel approach to predict group activities given the beginning frames with incomplete activity executions. Existing action (We define action as the behavior performed by a single person, and define activity as the behavior performed by a group of people) prediction approaches learn to enhance the representation power of the partial observation (We define partial observation as the beginning frames with incomplete activity execution, and full observation as the one with complete activity execution). However, for group activity prediction, the relation evolution of people’s activity and their positions over time is an important cue for predicting group activity. To this end, we propose a sequential relational anticipation model (SRAM) that summarizes the relational dynamics in the partial observation and progressively anticipates the group representations with rich discriminative information. Our model explicitly anticipates both activity features and positions by two graph auto-encoders, aiming to learn a discriminative group representation for group activity prediction. Experimental results on two popularly used datasets demonstrate that our approach significantly outperforms the state-of-the-art activity prediction methods.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_35');
INSERT INTO `paper` VALUES (12047, 'GSIR: Generalizable 3D Shape Interpretation and Reconstruction', 'Shape interpretation', '3D reconstruction', '', '', '', '3D shape interpretation and reconstruction are closely related to each other but have long been studied separately and often end up with priors that are highly biased towards the training classes. In this paper, we present an algorithm, Generalizable 3D Shape Interpretation and Reconstruction (GSIR), designed to jointly learn these two tasks to capture generic, class-agnostic shape priors for a better understanding of 3D geometry. We propose to recover 3D shape structures as cuboids from partial reconstruction and use the predicted structures to further guide full 3D reconstruction. The unified framework is trained simultaneously offline to learn a generic notion and can be fine-tuned online for specific objects without any annotations. Extensive experiments on both synthetic and real data demonstrate that introducing 3D shape interpretation improves the performance of single image 3D reconstruction and vice versa, achieving the state-of-the-art performance on both tasks for objects in both seen and unseen categories.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_30');
INSERT INTO `paper` VALUES (12048, 'GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-Aware Supervision', 'Vehicle pose and shape reconstruction', '3D traffic scene understanding', '', '', '', 'We present a novel end-to-end framework named as GSNet (Geometric and Scene-aware Network), which jointly estimates 6DoF poses and reconstructs detailed 3D car shapes from single urban street view. GSNet utilizes a unique four-way feature extraction and fusion scheme and directly regresses 6DoF poses and shapes in a single forward pass. Extensive experiments show that our diverse feature extraction and fusion scheme can greatly improve model performance. Based on a divide-and-conquer 3D shape representation strategy, GSNet reconstructs 3D vehicle shape with great detail (1352 vertices and 2700 faces). This dense mesh representation further leads us to consider geometrical consistency and scene context, and inspires a new multi-objective loss function to regularize network training, which in turn improves the accuracy of 6D pose estimation and validates the merit of jointly performing both tasks. We evaluate GSNet on the largest multi-task ApolloCar3D benchmark and achieve state-of-the-art performance both quantitatively and qualitatively. Project page is available at https://lkeab.github.io/gsnet/.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_31');
INSERT INTO `paper` VALUES (12049, 'Guessing State Tracking for Visual Dialogue', 'Visual dialogue', 'Visual grounding', 'Guessing state tracking', 'GuessWhat?!', '', 'The Guesser is a task of visual grounding in GuessWhat?! like visual dialogue. It locates the target object in an image supposed by an Oracle oneself over a question-answer based dialogue between a Questioner and the Oracle. Most existing guessers make one and only one guess after receiving all question-answer pairs in a dialogue with the predefined number of rounds. This paper proposes a guessing state for the Guesser, and regards guess as a process with change of guessing state through a dialogue. A guessing state tracking based guess model is therefore proposed. The guessing state is defined as a distribution on objects in the image. With that in hand, two loss functions are defined as supervisions to guide the guessing state in model training. Early supervision brings supervision to Guesser at early rounds, and incremental supervision brings monotonicity to the guessing state. Experimental results on GuessWhat?! dataset show that our model significantly outperforms previous models, achieves new state-of-the-art, especially the success rate of guessing 83.3% is approaching the human-level accuracy of 84.4%.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_40');
INSERT INTO `paper` VALUES (12050, 'Guidance and Evaluation: Semantic-Aware Image Inpainting for Mixed Scenes', 'Image inpainting', 'Semantic guidance', 'Segmentation confidence evaluation', 'Mixed scene', '', 'Completing a corrupted image with correct structures and reasonable textures for a mixed scene remains an elusive challenge. Since the missing hole in a mixed scene of a corrupted image often contains various semantic information, conventional two-stage approaches utilizing structural information often lead to the problem of unreliable structural prediction and ambiguous image texture generation. In this paper, we propose a Semantic Guidance and Evaluation Network (SGE-Net) to iteratively update the structural priors and the inpainted image in an interplay framework of semantics extraction and image inpainting. It utilizes semantic segmentation map as guidance in each scale of inpainting, under which location-dependent inferences are re-evaluated, and, accordingly, poorly-inferred regions are refined in subsequent scales. Extensive experiments on real-world images of mixed scenes demonstrated the superiority of our proposed method over state-of-the-art approaches, in terms of clear boundaries and photo-realistic textures.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_41');
INSERT INTO `paper` VALUES (12051, 'Guided Collaborative Training for Pixel-Wise Semi-Supervised Learning', 'Semi-supervised learning', 'Pixel-wise vision tasks', '', '', '', 'We investigate the generalization of semi-supervised learning (SSL) to diverse pixel-wise tasks. Although SSL methods have achieved impressive results in image classification, the performances of applying them to pixel-wise tasks are unsatisfactory due to their need for dense outputs. In addition, existing pixel-wise SSL approaches are only suitable for certain tasks as they usually require to use task-specific properties. In this paper, we present a new SSL framework, named Guided Collaborative Training (GCT), for pixel-wise tasks, with two main technical contributions. First, GCT addresses the issues caused by the dense outputs through a novel flaw detector. Second, the modules in GCT learn from unlabeled data collaboratively through two newly proposed constraints that are independent of task-specific properties. As a result, GCT can be applied to a wide range of pixel-wise tasks without structural adaptation. Our extensive experiments on four challenging vision tasks, including semantic segmentation, real image denoising, portrait image matting, and night image enhancement, show that GCT outperforms state-of-the-art SSL methods by a large margin.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_26');
INSERT INTO `paper` VALUES (12052, 'Guided Deep Decoder: Unsupervised Image Pair Fusion', 'Deep image prior', 'Deep decoder', 'Image fusion', 'Hyperspectral image', 'Super-resolution', 'The fusion of input and guidance images that have a tradeoff in their information (e.g., hyperspectral and RGB image fusion or pansharpening) can be interpreted as one general problem. However, previous studies applied a task-specific handcrafted prior and did not address the problems with a unified approach. To address this limitation, in this study, we propose a guided deep decoder network as a general prior. The proposed network is composed of an encoder-decoder network that exploits multi-scale features of a guidance image and a deep decoder network that generates an output image. The two networks are connected by feature refinement units to embed the multi-scale features of the guidance image into the deep decoder network. The proposed network allows the network parameters to be optimized in an unsupervised way without training data. Our results show that the proposed network can achieve state-of-the-art performance in various image fusion problems.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_6');
INSERT INTO `paper` VALUES (12053, 'Guided Saliency Feature Learning for Person Re-identification in Crowded Scenes', 'Person re-identification', 'Guided saliency feature learning', 'Guided adaptive spatial matching', '', '', 'Person Re-identification (Re-ID) in crowed scenes is a challenging problem, where people are frequently partially occluded by objects and other people. However, few studies have provided flexible solutions to re-identifying people in an image containing a partial occlusion body part. In this paper, we propose a simple occlusion-aware approach to address the problem. The proposed method first leverages a fully convolutional network to generate spatial features. And then we design a combination of a pose-guided and mask-guided layer to generate saliency heatmap to further guide discriminative feature learning. More importantly, we propose a new matching approach, called Guided Adaptive Spatial Matching (GASM), which expects that each spatial feature in the query can find the most similar spatial features of a person in a gallery to match. Especially, We use the saliency heatmap to guide the adaptive spatial matching by assigning the foreground human parts with larger weights adaptively. The effectiveness of the proposed GASM is demonstrated on two occluded person datasets: Crowd REID (51.52%) and Occluded REID (80.25%) and three benchmark person datasets: Market1501 (95.31%), DukeMTMC-reID (88.12%) and MSMT17 (79.52%). Additionally, GASM achieves good performance on cross-domain person Re-ID. The code and models are available at: https://github.com/JDAI-CV/fast-reid/blob/master/projects/CrowdReID.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_22');
INSERT INTO `paper` VALUES (12054, 'Guided Semantic Flow', 'Dense semantic correspondence', 'Matching confidence', 'Moving least square', '', '', 'Establishing dense semantic correspondences requires dealing with large geometric variations caused by the unconstrained setting of images. To address such severe matching ambiguities, we introduce a novel approach, called guided semantic flow, based on the key insight that sparse yet reliable matches can effectively capture non-rigid geometric variations, and these confident matches can guide adjacent pixels to have similar solution spaces, reducing the matching ambiguities significantly. We realize this idea with learning-based selection of confident matches from an initial set of all pairwise matching scores and their propagation by a new differentiable upsampling layer based on moving least square concept. We take advantage of the guidance from reliable matches to refine the matching hypotheses through Gaussian parametric model in the subsequent matching pipeline. With the proposed method, state-of-the-art performance is attained on several standard benchmarks for semantic correspondence.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_38');
INSERT INTO `paper` VALUES (12055, 'Guiding Monocular Depth Estimation Using Depth-Attention Volume', 'Monocular depth', 'Attention mechanism', 'Depth estimation', '', '', 'Recovering the scene depth from a single image is an ill-posed problem that requires additional priors, often referred to as monocular depth cues, to disambiguate different 3D interpretations. In recent works, those priors have been learned in an end-to-end manner from large datasets by using deep neural networks. In this paper, we propose guiding depth estimation to favor planar structures that are ubiquitous especially in indoor environments. This is achieved by incorporating a non-local coplanarity constraint to the network with a novel attention mechanism called depth-attention volume (DAV). Experiments on two popular indoor datasets, namely NYU-Depth-v2 and ScanNet, show that our method achieves state-of-the-art depth estimation results while using only a fraction of the number of parameters needed by the competing methods. Code is available at: https://github.com/HuynhLam/DAV.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_35');
INSERT INTO `paper` VALUES (12056, 'H3DNet: 3D Object Detection Using Hybrid Geometric Primitives', '3D deep learning', 'Geometric deep learning', '3D point clouds', '3D bounding boxes', '3D object detection', 'We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D. Our code is open-sourced at here.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_19');
INSERT INTO `paper` VALUES (12057, 'Hallucinating Visual Instances in Total Absentia', '', '', '', '', '', 'In this paper, we investigate a new visual restoration task, termed as hallucinating visual instances in total absentia (HVITA). Unlike conventional image inpainting task that works on images with only part of a visual instance missing, HVITA concerns scenarios where an object is completely absent from the scene. This seemingly minor difference in fact makes the HVITA a much challenging task, as the restoration algorithm would have to not only infer the category of the object in total absentia, but also hallucinate an object of which the appearance is consistent with the background. Towards solving HVITA, we propose an end-to-end deep approach that explicitly looks into the global semantics within the image. Specifically, we transform the input image to a semantic graph, wherein each node corresponds to a detected object in the scene. We then adopt a Graph Convolutional Network on top of the scene graph to estimate the category of the missing object in the masked region, and finally introduce a Generative Adversarial Module to carry out the hallucination. Experiments on COCO, Visual Genome and NYU Depth v2 datasets demonstrate that the proposed approach yields truly encouraging and visually plausible results.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_16');
INSERT INTO `paper` VALUES (12058, 'HALO: Hardware-Aware Learning to Optimize', 'On-device learning', 'Learning to optimize', 'Meta learning', 'Efficient training', 'Internet-of-Things', 'There has been an explosive demand for bringing machine learning (ML) powered intelligence into numerous Internet-of-Things (IoT) devices. However, the effectiveness of such intelligent functionality requires in-situ continuous model adaptation for adapting to new data and environments, while the on-device computing and energy resources are usually extremely constrained. Neither traditional hand-crafted (e.g., SGD, Adagrad, and Adam) nor existing meta optimizers are specifically designed to meet those challenges, as the former requires tedious hyper-parameter tuning while the latter are often costly due to the meta algorithms’ own overhead. To this end, we propose hardware-aware learning to optimize (HALO), a practical meta optimizer dedicated to resource-efficient on-device adaptation. Our HALO optimizer features the following highlights: (1) faster adaptation speed (i.e., taking fewer data or iterations to reach a specified accuracy) by introducing a new regularizer to promote empirical generalization; and (2) lower per-iteration complexity, thanks to a stochastic structural sparsity regularizer being enforced. Furthermore, the optimizer itself is designed as a very light-weight RNN and thus incurs negligible overhead. Ablation studies and experiments on five datasets, six optimizees, and two state-of-the-art (SOTA) edge AI devices validate that, while always achieving a better accuracy (\\(\\uparrow \\)0.46% - \\(\\uparrow \\)20.28%), HALO can greatly trim down the energy cost (up to \\(\\downarrow \\)60%) in adaptation, quantified using an IoT device or SOTA simulator. Codes and pre-trained models are at https://github.com/RICE-EIC/HALO.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_29');
INSERT INTO `paper` VALUES (12059, 'Hamiltonian Dynamics for Real-World Shape Interpolation', 'Shape interpolation', 'Registration', '3D computer vision', '', '', 'We revisit the classical problem of 3D shape interpolation and propose a novel, physically plausible approach based on Hamiltonian dynamics. While most prior work focuses on synthetic input shapes, our formulation is designed to be applicable to real-world scans with imperfect input correspondences and various types of noise. To that end, we use recent progress on dynamic thin shell simulation and divergence-free shape deformation and combine them to address the inverse problem of finding a plausible intermediate sequence for two input shapes. In comparison to prior work that mainly focuses on small distortion of consecutive frames, we explicitly model volume preservation and momentum conservation, as well as an anisotropic local distortion model. We argue that, in order to get a robust interpolation for imperfect inputs, we need to model the input noise explicitly which results in an alignment based formulation. Finally, we show a qualitative and quantitative improvement over prior work on a broad range of synthetic and scanned data. Besides being more robust to noisy inputs, our method yields exactly volume preserving intermediate shapes, avoids self-intersections and is scalable to high resolution scans.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_11');
INSERT INTO `paper` VALUES (12060, 'Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation', '3D hand pose estimation', 'Structured learning', 'Attention', 'Non-autoregressive transformer', '', '3D hand pose estimation is still far from a well-solved problem mainly due to the highly nonlinear dynamics of hand pose and the difficulties of modeling its inherent structural dependencies. To address this issue, we connect this structured output learning problem with the structured modeling framework in sequence transduction field. Standard transduction models like Transformer adopt an autoregressive connection to capture dependencies from previously generated tokens and further correlate this information with the input sequence in order to prioritize the set of relevant input tokens for current token generation. To borrow wisdom from this structured learning framework while avoiding the sequential modeling for hand pose, taking a 3D point set as input, we propose to leverage the Transformer architecture with a novel non-autoregressive structured decoding mechanism. Specifically, instead of using previously generated results, our decoder utilizes a reference hand pose to provide equivalent dependencies among hand joints for each output joint generation. By imposing the reference structural dependencies, we can correlate the information with the input 3D points through a multi-head attention mechanism, aiming to discover informative points from different perspectives, towards each hand joint localization. We demonstrate our model’s effectiveness over multiple challenging hand pose datasets, comparing with several state-of-the-art methods.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_2');
INSERT INTO `paper` VALUES (12061, 'Handcrafted Outlier Detection Revisited', 'Low-level vision', 'Matching', 'Spatial matching', 'Spatial consistency', 'Spatial verification', 'Local feature matching is a critical part of many computer vision pipelines, including among others Structure-from-Motion, SLAM, and Visual Localization. However, due to limitations in the descriptors, raw matches are often contaminated by a majority of outliers. As a result, outlier detection is a fundamental problem in computer vision and a wide range of approaches, from simple checks based on descriptor similarity to geometric verification, have been proposed over the last decades. In recent years, deep learning-based approaches to outlier detection have become popular. Unfortunately, the corresponding works rarely compare with strong classical baselines. In this paper we revisit handcrafted approaches to outlier filtering. Based on best practices, we propose a hierarchical pipeline for effective outlier detection as well as integrate novel ideas which in sum lead to an efficient and competitive approach to outlier rejection. We show that our approach, although not relying on learning, is more than competitive to both recent learned works as well as handcrafted approaches, both in terms of efficiency and effectiveness. The code is available at https://github.com/cavalli1234/AdaLAM.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_45');
INSERT INTO `paper` VALUES (12062, 'Hard Negative Examples are Hard, but Useful', 'Hard negative', 'Deep metric learning', 'Triplet loss', '', '', 'Triplet loss is an extremely common approach to distance metric learning. Representations of images from the same class are optimized to be mapped closer together in an embedding space than representations of images from different classes. Much work on triplet losses focuses on selecting the most useful triplets of images to consider, with strategies that select dissimilar examples from the same class or similar examples from different classes. The consensus of previous research is that optimizing with the hardest negative examples leads to bad training behavior. That’s a problem – these hardest negatives are literally the cases where the distance metric fails to capture semantic similarity. In this paper, we characterize the space of triplets and derive why hard negatives make triplet loss training fail. We offer a simple fix to the loss function and show that, with this fix, optimizing with hard negative examples becomes feasible. This leads to more generalizable features, and image retrieval results that outperform state of the art for datasets with high intra-class variance. Code is available at: https://github.com/littleredxh/HardNegative.git', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_8');
INSERT INTO `paper` VALUES (12063, 'Hard Occlusions in Visual Object Tracking', 'Visual object tracking', 'Occlusion', 'Benchmarks', 'Metrics', 'Dataset', 'Visual object tracking is among the hardest problems in computer vision, as trackers have to deal with many challenging circumstances such as illumination changes, fast motion, occlusion, among others. A tracker is assessed to be good or not based on its performance on the recent tracking datasets, e.g., VOT2019, and LaSOT. We argue that while the recent datasets contain large sets of annotated videos that to some extent provide a large bandwidth for training data, the hard scenarios such as occlusion and in-plane rotation are still underrepresented. For trackers to be brought closer to the real-world scenarios and deployed in safety-critical devices, even the rarest hard scenarios must be properly addressed. In this paper, we particularly focus on hard occlusion cases and benchmark the performance of recent state-of-the-art trackers (SOTA) on them. We created a small-scale dataset (Dataset can be accessed at https://github.com/ThijsKuipers1995/HTB2020) containing different categories within hard occlusions, on which the selected trackers are evaluated. Results show that hard occlusions remain a very challenging problem for SOTA trackers. Furthermore, it is observed that tracker performance varies wildly between different categories of hard occlusions, where a top-performing tracker on one category performs significantly worse on a different category. The varying nature of tracker performance based on specific categories suggests that the common tracker rankings using averaged single performance scores are not adequate to gauge tracker performance in real-world scenarios.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_22');
INSERT INTO `paper` VALUES (12064, 'HARD-Net: Hardness-AwaRe Discrimination Network for 3D Early Activity Prediction', 'Early activity prediction', 'Action/gesture understanding', '3D skeleton data', 'Hardness-aware learning', '', 'Predicting the class label from the partially observed activity sequence is a very hard task, as the observed early segments of different activities can be very similar. In this paper, we propose a novel Hardness-AwaRe Discrimination Network (HARD-Net) to specifically investigate the relationships between the similar activity pairs that are hard to be discriminated. Specifically, a Hard Instance-Interference Class (HI-IC) bank is designed, which dynamically records the hard similar pairs. Based on the HI-IC bank, a novel adversarial learning scheme is proposed to train our HARD-Net, which thus grants our network with the strong capability in mining subtle discrimination information for 3D early activity prediction. We evaluate our proposed HARD-Net on two public activity datasets and achieve state-of-the-art performance.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_25');
INSERT INTO `paper` VALUES (12065, 'HardGAN: A Haze-Aware Representation Distillation GAN for Single Image Dehazing', 'Image dehazing', 'Generative Adversarial Network (GAN)', 'Image restoration', 'Deep learning', '', 'In this paper, we present a Haze-Aware Representation Distillation Generative Adversarial Network (HardGAN) for single-image dehazing. Unlike previous studies that intend to model the transmission map and global atmospheric light jointly to restore a clear image, we approach this restoration problem by using a multi-scale structure neural network composed of our proposed haze-aware representation distillation layers. Moreover, we re-introduce to utilize the normalization layer skillfully instead of stacking with the convolutional layers directly as before to avoid useful information wash away, as claimed in many image quality enhancement studies. Extensive experiments on several synthetic benchmark datasets as well as the NTIRE 2020 real-world images show our proposed HardGAN performs favorably against the state-of-the-art methods in terms of PSNR, SSIM, LPIPS, and individual subjective evaluation.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_43');
INSERT INTO `paper` VALUES (12066, 'Hardware Architecture of Embedded Inference Accelerator and Analysis of Algorithms for Depthwise and Large-Kernel Convolutions', 'Convolutional neural networks (CNNs)', 'Embedded vision', 'Depthwise convolution', 'Hardware utilization', '', 'In order to handle modern convolutional neural networks (CNNs) efficiently, a hardware architecture of CNN inference accelerator is proposed to handle depthwise convolutions and regular convolutions, which are both essential building blocks for embedded-computer-vision algorithms. Different from related works, the proposed architecture can support filter kernels with different sizes with high flexibility since it does not require extra costs for intra-kernel parallelism, and it can generate convolution results faster than the architecture of the related works.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_1');
INSERT INTO `paper` VALUES (12067, 'HDNet: Human Depth Estimation for Multi-person Camera-Space Localization', 'Human Depth Estimation', 'Multi-person pose estimation', 'Camera coordinate space', '', '', 'Current works on multi-person 3D pose estimation mainly focus on the estimation of the 3D joint locations relative to the root joint and ignore the absolute locations of each pose. In this paper, we propose the Human Depth Estimation Network (HDNet), an end-to-end framework for absolute root joint localization in the camera coordinate space. Our HDNet first estimates the 2D human pose with heatmaps of the joints. These estimated heatmaps serve as attention masks for pooling features from image regions corresponding to the target person. A skeleton-based Graph Neural Network (GNN) is utilized to propagate features among joints. We formulate the target depth regression as a bin index estimation problem, which can be transformed with a soft-argmax operation from the classification output of our HDNet. We evaluate our HDNet on the root joint localization and root-relative 3D pose estimation tasks with two benchmark datasets, i.e., Human3.6M and MuPoTS-3D. The experimental results show that we outperform the previous state-of-the-art consistently under multiple evaluation metrics. Our source code is available at: https://github.com/jiahaoLjh/HumanDepth.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_37');
INSERT INTO `paper` VALUES (12068, 'HGNet: Hybrid Generative Network for Zero-Shot Domain Adaptation', 'Deep learning', 'Domain adaptation', 'Generative learning', '', '', 'Domain Adaptation as an important tool aims to explore a generalized model trained on well-annotated source knowledge to address learning issue on target domain with insufficient or even no annotation. Current approaches typically incorporate data from source and target domains for training stage to deal with domain shift. However, most domain adaptation tasks generally suffer from the problem that measuring the domain shift tends to be impossible when target data is inaccessible. In this paper, we propose a novel algorithm, Hybrid Generative Network (HGNet) for Zero-shot Domain Adaptation, which embeds an adaptive feature separation (AFS) module into generative architecture. Specifically, AFS module can adaptively distinguish classification-relevant features from classification-irrelevant ones to learn domain-invariant and discriminative representations when task-relevant target instances are invisible. To learn high-quality feature representation, we also develop hybrid generative strategy to ensure the uniqueness of feature separation and completeness of semantic information. Extensive experimental results on several benchmarks illustrate that our method achieves more promising results than state-of-the-art approaches.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_4');
INSERT INTO `paper` VALUES (12069, 'Hidden Footprints: Learning Contextual Walkability from 3D Human Trails', 'Scene understanding', 'Context', 'Human analysis', '', '', 'Predicting where people can walk in a scene is important for many tasks, including autonomous driving systems and human behavior analysis. Yet learning a computational model for this purpose is challenging due to semantic ambiguity and a lack of labeled data: current datasets only tell you where people are, not where they could be. We tackle this problem by leveraging information from existing datasets, without additional labeling. We first augment the set of valid, labeled walkable regions by propagating person observations between images, utilizing 3D information to create what we call hidden footprints. However, this augmented data is still sparse. We devise a training strategy designed for such sparse labels, combining a class-balanced classification loss with a contextual adversarial loss. Using this strategy, we demonstrate a model that learns to predict a walkability map from a single image. We evaluate our model on the Waymo and Cityscapes datasets, demonstrating superior performance compared to baselines and state-of-the-art models.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_12');
INSERT INTO `paper` VALUES (12070, 'Hierarchical Context Embedding for Region-Based Object Detection', 'Object detection', 'Context embedding', 'Region-based CNNs', '', '', 'State-of-the-art two-stage object detectors apply a classifier to a sparse set of object proposals, relying on region-wise features extracted by RoIPool or RoIAlign as inputs. The region-wise features, in spite of aligning well with the proposal locations, may still lack the crucial context information which is necessary for filtering out noisy background detections, as well as recognizing objects possessing no distinctive appearances. To address this issue, we present a simple but effective Hierarchical Context Embedding (HCE) framework, which can be applied as a plug-and-play component, to facilitate the classification ability of a series of region-based detectors by mining contextual cues. Specifically, to advance the recognition of context-dependent object categories, we propose an image-level categorical embedding module which leverages the holistic image-level context to learn object-level concepts. Then, novel RoI features are generated by exploiting hierarchically embedded context information beneath both whole images and interested regions, which are also complementary to conventional RoI features. Moreover, to make full use of our hierarchical contextual RoI features, we propose the early-and-late fusion strategies (i.e., feature fusion and confidence fusion), which can be combined to boost the classification accuracy of region-based detectors. Comprehensive experiments demonstrate that our HCE framework is flexible and generalizable, leading to significant and consistent improvements upon various region-based detectors, including FPN, Cascade R-CNN and Mask R-CNN.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_38');
INSERT INTO `paper` VALUES (12071, 'Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection', 'RGB-D Salient Object Detection', 'Cross-modal fusion', 'Dynamic dilated pyramid module', 'Hybrid enhanced loss', '', 'The main purpose of RGB-D salient object detection (SOD) is how to better integrate and utilize cross-modal fusion information. In this paper, we explore these issues from a new perspective. We integrate the features of different modalities through densely connected structures and use their mixed features to generate dynamic filters with receptive fields of different sizes. In the end, we implement a kind of more flexible and efficient multi-scale cross-modal feature processing, i.e. dynamic dilated pyramid module. In order to make the predictions have sharper edges and consistent saliency regions, we design a hybrid enhanced loss function to further optimize the results. This loss function is also validated to be effective in the single-modal RGB SOD task. In terms of six metrics, the proposed method outperforms the existing twelve methods on eight challenging benchmark datasets. A large number of experiments verify the effectiveness of the proposed module and loss function. Our code, model and results are available at https://github.com/lartpang/HDFNet.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_15');
INSERT INTO `paper` VALUES (12072, 'Hierarchical Face Aging Through Disentangled Latent Characteristics', 'Facial age analysis', 'Variational autoencoder', '', '', '', 'Current age datasets lie in a long-tailed distribution, which brings difficulties to describe the aging mechanism for the imbalance ages. To alleviate it, we design a novel facial age prior to guide the aging mechanism modeling. To explore the age effects on facial images, we propose a Disentangled Adversarial Autoencoder (DAAE) to disentangle the facial images into three independent factors: age, identity and extraneous information. To avoid the “wash away” of age and identity information in face aging process, we propose a hierarchical conditional generator by passing the disentangled identity and age embeddings to the high-level and low-level layers with class-conditional BatchNorm. Finally, a disentangled adversarial learning mechanism is introduced to boost the image quality for face aging. In this way, when manipulating the age distribution, DAAE can achieve face aging with arbitrary ages. Further, given an input face image, the mean value of the learned age posterior distribution can be treated as an age estimator. These indicate that DAAE can efficiently and accurately estimate the age distribution in a disentangling manner. DAAE is the first attempt to achieve facial age analysis tasks, including face aging with arbitrary ages, exemplar-based face aging and age estimation, in a universal framework. The qualitative and quantitative experiments demonstrate the superiority of DAAE on five popular datasets, including CACD2000, Morph, UTKFace, FG-NET and AgeDB.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_6');
INSERT INTO `paper` VALUES (12073, 'Hierarchical HMM for Eye Movement Classification', 'Hidden Markov Model', 'Eye movement', 'Fixation', 'Smooth pursuit', 'Saccade', 'In this work, we tackle the problem of ternary eye movement classification, which aims to separate fixations, saccades and smooth pursuits from the raw eye positional data. The efficient classification of these different types of eye movements helps to better analyze and utilize the eye tracking data. Different from the existing methods that detect eye movement by several pre-defined threshold values, we propose a hierarchical Hidden Markov Model (HMM) statistical algorithm for detecting fixations, saccades and smooth pursuits. The proposed algorithm leverages different features from the recorded raw eye tracking data with a hierarchical classification strategy, separating one type of eye movement each time. Experimental results demonstrate the effectiveness and robustness of the proposed method by achieving competitive or better performance compared to the state-of-the-art methods.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_35');
INSERT INTO `paper` VALUES (12074, 'Hierarchical Kinematic Human Mesh Recovery', '', '', '', '', '', 'We consider the problem of estimating a parametric model of 3D human mesh from a single image. While there has been substantial recent progress in this area with direct regression of model parameters, these methods only implicitly exploit the human body kinematic structure, leading to sub-optimal use of the model prior. In this work, we address this gap by proposing a new technique for regression of human parametric model that is explicitly informed by the known hierarchical structure, including joint interdependencies of the model. This results in a strong prior-informed design of the regressor architecture and an associated hierarchical optimization that is flexible to be used in conjunction with the current standard frameworks for 3D human mesh recovery. We demonstrate these aspects by means of extensive experiments on standard benchmark datasets, showing how our proposed new design outperforms several existing and popular methods, establishing new state-of-the-art results. By considering joint interdependencies, our method is equipped to infer joints even under data corruptions, which we demonstrate by conducting experiments under varying degrees of occlusion.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_45');
INSERT INTO `paper` VALUES (12075, 'Hierarchical Style-Based Networks for Motion Synthesis', 'Long-range motion generation', 'Motion style transfer', '', '', '', 'Generating diverse and natural human motion is one of the long-standing goals for creating intelligent characters in the animated world. In this paper, we propose an unsupervised method for generating long-range, diverse and plausible behaviors to achieve a specific goal location. Our proposed method learns to model the motion of human by decomposing a long-range generation task in a hierarchical manner. Given the starting and ending states, a memory bank is used to retrieve motion references as source material for short-range clip generation. We first propose to explicitly disentangle the provided motion material into style and content counterparts via bi-linear transformation modelling, where diverse synthesis is achieved by free-form combination of these two components. The short-range clips are then connected to form a long-range motion sequence. Without ground truth annotation, we propose a parameterized bi-directional interpolation scheme to guarantee the physical validity and visual naturalness of generated results. On large-scale skeleton dataset, we show that the proposed method is able to synthesise long-range, diverse and plausible motion, which is also generalizable to unseen motion data during testing. Moreover, we demonstrate the generated sequences are useful as subgoals for actual physical execution in the animated world. Please refer to our project page for more synthesised results (https://sites.google.com/view/hsnms/home).', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_11');
INSERT INTO `paper` VALUES (12076, 'Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language', 'Temporal Activity Localization via Language', 'Hierarchical Visual-Textual Graph', 'Visual-textual alignment', '', '', 'Temporal Activity Localization via Language (TALL) in video is a recently proposed challenging vision task, and tackling it requires fine-grained understanding of the video content, however, this is overlooked by most of the existing works. In this paper, we propose a novel TALL method which builds a Hierarchical Visual-Textual Graph to model interactions between the objects and words as well as among the objects to jointly understand the video contents and the language. We also design a convolutional network with cross-channel communication mechanism to further encourage the information passing between the visual and textual modalities. Finally, we propose a loss function that enforces alignment of the visual representation of the localized activity and the sentence representation, so that the model can predict more accurate temporal boundaries. We evaluated our proposed method on two popular benchmark datasets: Charades-STA and ActivityNet Captions, and achieved state-of-the-art performances on both datasets. Code is available at https://github.com/forwchen/HVTG.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_36');
INSERT INTO `paper` VALUES (12077, 'High Resolution Zero-Shot Domain Adaptation of Synthetically Rendered Face Images', '', '', '', '', '', 'Generating photorealistic images of human faces at scale remains a prohibitively difficult task using computer graphics approaches. This is because these require the simulation of light to be photorealistic, which in turn requires physically accurate modelling of geometry, materials, and light sources, for both the head and the surrounding scene. Non-photorealistic renders however are increasingly easy to produce. In contrast to computer graphics approaches, generative models learned from more readily available 2D image data have been shown to produce samples of human faces that are hard to distinguish from real data. The process of learning usually corresponds to a loss of control over the shape and appearance of the generated images. For instance, even simple disentangling tasks such as modifying the hair independently of the face, which is trivial to accomplish in a computer graphics approach, remains an open research question. In this work, we propose an algorithm that matches a non-photorealistic, synthetically generated image to a latent vector of a pretrained StyleGAN2 model which, in turn, maps the vector to a photorealistic image of a person of the same pose, expression, hair, and lighting. In contrast to most previous work, we require no synthetic training data. To the best of our knowledge, this is the first algorithm of its kind to work at a resolution of 1K and represents a significant leap forward in visual realism.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_14');
INSERT INTO `paper` VALUES (12078, 'High-Fidelity Synthesis with Disentangled Representation', '', '', '', '', '', 'Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance (i.e., improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (\\(1024\\times 1024\\) pixels) for the first time using the disentangled representations. Our code is available at https://www.github.com/1Konny/idgan.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_10');
INSERT INTO `paper` VALUES (12079, 'High-Quality Single-Model Deep Video Compression with Frame-Conv3D and Multi-frame Differential Modulation', '', '', '', '', '', 'Deep learning (DL) methods have revolutionized the paradigm of computer vision tasks and DL-based video compression is becoming a hot topic. This paper proposes a deep video compression method to simultaneously encode multiple frames with Frame-Conv3D and differential modulation. We first adopt Frame-Conv3D instead of traditional Channel-Conv3D for efficient multi-frame fusion. When generating the binary representation, the multi-frame differential modulation is utilized to alleviate the effect of quantization noise. By analyzing the forward and backward computing flow of the modulator, we identify that this technique can make full use of past frames’ information to remove the redundancy between multiple frames, thus achieves better performance. A dropout scheme combined with the differential modulator is proposed to enable bit rate optimization within a single model. Experimental results show that the proposed approach outperforms the H.264 and H.265 codecs in the region of low bit rate. Compared with recent DL-based methods, our model also achieves competitive performance.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_15');
INSERT INTO `paper` VALUES (12080, 'High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling', '', '', '', '', '', 'Existing image inpainting methods often produce artifacts when dealing with large holes in real applications. To address this challenge, we propose an iterative inpainting method with a feedback mechanism. Specifically, we introduce a deep generative model which not only outputs an inpainting result but also a corresponding confidence map. Using this map as feedback, it progressively fills the hole by trusting only high-confidence pixels inside the hole at each iteration and focuses on the remaining pixels in the next iteration. As it reuses partial predictions from the previous iterations as known pixels, this process gradually improves the result. In addition, we propose a guided upsampling network to enable generation of high-resolution inpainting results. We achieve this by extending the Contextual Attention module [39] to borrow high-resolution feature patches in the input image. Furthermore, to mimic real object removal scenarios, we collect a large object mask dataset and synthesize more realistic training data that better simulates user inputs. Experiments show that our method significantly outperforms existing methods in both quantitative and qualitative evaluations. More results and Web APP are available at https://zengxianyu.github.io/iic.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_1');
INSERT INTO `paper` VALUES (12081, 'Highly Efficient Salient Object Detection with 100K Parameters', 'Salient object detection', 'Highly efficient', '', '', '', 'Salient object detection models often demand a considerable amount of computation cost to make precise prediction for each pixel, making them hardly applicable on low-power devices. In this paper, we aim to relieve the contradiction between computation cost and model performance by improving the network efficiency to a higher degree. We propose a flexible convolutional module, namely generalized OctConv (gOctConv), to efficiently utilize both in-stage and cross-stages multi-scale features, while reducing the representation redundancy by a novel dynamic weight decay scheme. The effective dynamic weight decay scheme stably boosts the sparsity of parameters during training, supports learnable number of channels for each scale in gOctConv, allowing 80% of parameters reduce with negligible performance drop. Utilizing gOctConv, we build an extremely light-weighted model, namely CSNet, which achieves comparable performance with \\({\\sim }0.2\\%\\) parameters (100k) of large models on popular salient object detection benchmarks. The source code is publicly available at https://mmcheng.net/sod100k/.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_42');
INSERT INTO `paper` VALUES (12082, 'History Repeats Itself: Human Motion Prediction via Motion Attention', 'Human motion prediction', 'Motion attention', '', '', '', 'Human motion prediction aims to forecast future human poses given a past motion. Whether based on recurrent or feed-forward neural networks, existing methods fail to model the observation that human motion tends to repeat itself, even for complex sports actions and cooking activities. Here, we introduce an attention-based feed-forward network that explicitly leverages this observation. In particular, instead of modeling frame-wise attention via pose similarity, we propose to extract motion attention to capture the similarity between the current motion context and the historical motion sub-sequences. Aggregating the relevant past motions and processing the result with a graph convolutional network allows us to effectively exploit motion patterns from the long-term history to predict the future poses. Our experiments on Human3.6M, AMASS and 3DPW evidence the benefits of our approach for both periodical and non-periodical actions. Thanks to our attention model, it yields state-of-the-art results on all three datasets. Our code is available at https://github.com/wei-mao-2019/HisRepItself.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_28');
INSERT INTO `paper` VALUES (12083, 'HMOR: Hierarchical Multi-person Ordinal Relations for Monocular Multi-person 3D Pose Estimation', '3D human pose', 'Ordinal relations', 'Integrated model', '', '', 'Remarkable progress has been made in 3D human pose estimation from a monocular RGB camera. However, only a few studies explored 3D multi-person cases. In this paper, we attempt to address the lack of a global perspective of the top-down approaches by introducing a novel form of supervision - Hierarchical Multi-person Ordinal Relations (HMOR). The HMOR encodes interaction information as the ordinal relations of depths and angles hierarchically, which captures the body-part and joint level semantic and maintains global consistency at the same time. In our approach, an integrated top-down model is designed to leverage these ordinal relations in the learning process. The integrated model estimates human bounding boxes, human depths, and root-relative 3D poses simultaneously, with a coarse-to-fine architecture to improve the accuracy of depth estimation. The proposed method significantly outperforms state-of-the-art methods on publicly available multi-person 3D pose datasets. In addition to superior performance, our method costs lower computation complexity and fewer model parameters.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_15');
INSERT INTO `paper` VALUES (12084, 'HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs', 'Deep neural networks', 'Model compression', 'Quantization', '', '', 'Recent work in network quantization produced state-of-the-art results using mixed precision quantization. An imperative requirement for many efficient edge device hardware implementations is that their quantizers are uniform and with power-of-two thresholds. In this work, we introduce the Hardware Friendly Mixed Precision Quantization Block (HMQ) in order to meet this requirement. The HMQ is a mixed precision quantization block that repurposes the Gumbel-Softmax estimator into a smooth estimator of a pair of quantization parameters, namely, bit-width and threshold. HMQs use this to search over a finite space of quantization schemes. Empirically, we apply HMQs to quantize classification models trained on CIFAR10 and ImageNet. For ImageNet, we quantize four different architectures and show that, in spite of the added restrictions to our quantization scheme, we achieve competitive and, in some cases, state-of-the-art results.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_27');
INSERT INTO `paper` VALUES (12085, 'HoughNet: Integrating Near and Long-Range Evidence for Bottom-Up Object Detection', 'Object detection', 'Voting', 'Bottom-up recognition', 'Hough transform', 'Image-to-image translation', 'This paper presents HoughNet, a one-stage, anchor-free, voting-based, bottom-up object detection method. Inspired by the Generalized Hough Transform, HoughNet determines the presence of an object at a certain location by the sum of the votes cast on that location. Votes are collected from both near and long-distance locations based on a log-polar vote field. Thanks to this voting mechanism, HoughNet is able to integrate both near and long-range, class-conditional evidence for visual recognition, thereby generalizing and enhancing current object detection methodology, which typically relies on only local evidence. On the COCO dataset, HoughNet’s best model achieves 46.4 AP (and 65.1 \\(AP_{50}\\)), performing on par with the state-of-the-art in bottom-up object detection and outperforming most major one-stage and two-stage methods. We further validate the effectiveness of our proposal in another task, namely, “labels to photo” image generation by integrating the voting module of HoughNet to two different GAN models and showing that the accuracy is significantly improved in both cases. Code is available at https://github.com/nerminsamet/houghnet.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_25');
INSERT INTO `paper` VALUES (12086, 'House-GAN: Relational Generative Adversarial Networks for Graph-Constrained House Layout Generation', 'GAN', 'Graph-constrained', 'Layout', 'Generation', 'Floorplan', 'This paper proposes a novel graph-constrained generative adversarial network, whose generator and discriminator are built upon relational architecture. The main idea is to encode the constraint into the graph structure of its relational networks. We have demonstrated the proposed architecture for a new house layout generation problem, whose task is to take an architectural constraint as a graph (i.e., the number and types of rooms with their spatial adjacency) and produce a set of axis-aligned bounding boxes of rooms. We measure the quality of generated house layouts with the three metrics: the realism, the diversity, and the compatibility with the input graph constraint. Our qualitative and quantitative evaluations over 117,000 real floorplan images demonstrate that the proposed approach outperforms existing methods and baselines. We will publicly share all our code and data.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_10');
INSERT INTO `paper` VALUES (12087, 'How Can I See My Future? FvTraj: Using First-Person View for Pedestrian Trajectory Prediction', 'Deep learning', 'Human behavior', 'Trajectory prediction', 'Crowd simulation', 'Multi-head attention', 'This work presents a novel First-person View based Trajectory predicting model (FvTraj) to estimate the future trajectories of pedestrians in a scene given their observed trajectories and the corresponding first-person view images. First, we render first-person view images using our in-house built First-person View Simulator (FvSim), given the ground-level 2D trajectories. Then, based on multi-head attention mechanisms, we design a social-aware attention module to model social interactions between pedestrians, and a view-aware attention module to capture the relations between historical motion states and visual features from the first-person view images. Our results show the dynamic scene contexts with ego-motions captured by first-person view images via FvSim are valuable and effective for trajectory prediction. Using this simulated first-person view images, our well structured FvTraj model achieves state-of-the-art performance.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_34');
INSERT INTO `paper` VALUES (12088, 'How Does Lipschitz Regularization Influence GAN Training?', 'Generative adversarial network (GAN)', 'Lipschitz regularization', 'Loss functions', '', '', 'Despite the success of Lipschitz regularization in stabilizing GAN training, the exact reason of its effectiveness remains poorly understood. The direct effect of K-Lipschitz regularization is to restrict the L2-norm of the neural network gradient to be smaller than a threshold K (e.g., \\(K=1\\)) such that \\(\\Vert \\nabla f\\Vert \\le K\\). In this work, we uncover an even more important effect of Lipschitz regularization by examining its impact on the loss function: It degenerates GAN loss functions to almost linear ones by restricting their domain and interval of attainable gradient values. Our analysis shows that loss functions are only successful if they are degenerated to almost linear ones. We also show that loss functions perform poorly if they are not degenerated and that a wide range of functions can be used as loss function as long as they are sufficiently degenerated by regularization. Basically, Lipschitz regularization ensures that all loss functions effectively work in the same way. Empirically, we verify our proposition on the MNIST, CIFAR10 and CelebA datasets.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_19');
INSERT INTO `paper` VALUES (12089, 'How to Track Your Dragon: A Multi-attentional Framework for Real-Time RGB-D 6-DOF Object Pose Tracking', 'Pose', 'Tracking', 'Attention', 'Geodesic', 'Multi-task', 'We present a novel multi-attentional convolutional architecture to tackle the problem of real-time RGB-D 6D object pose tracking of single, known objects. Such a problem poses multiple challenges originating both from the objects’ nature and their interaction with their environment, which previous approaches have failed to fully address. The proposed framework encapsulates methods for background clutter and occlusion handling by integrating multiple parallel soft spatial attention modules into a multitask Convolutional Neural Network (CNN) architecture. Moreover, we consider the special geometrical properties of both the object’s 3D model and the pose space, and we use a more sophisticated approach for data augmentation during training. The provided experimental results confirm the effectiveness of the proposed multi-attentional architecture, as it improves the State-of-the-Art (SoA) tracking performance by an average score of 34.03\\(\\%\\) for translation and 40.01\\(\\%\\) for rotation, when tested on the most complete dataset designed, up to date, for the problem of RGB-D object tracking. Code will be available in: https://github.com/ismarou/How_to_track_your_Dragon.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_45');
INSERT INTO `paper` VALUES (12090, 'HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalization', 'Hand texture model', 'Appearance modeling', 'Hand tracking', '3D hand reconstruction', '', '3D hand reconstruction from images is a widely-studied problem in computer vision and graphics, and has a particularly high relevance for virtual and augmented reality. Although several 3D hand reconstruction approaches leverage hand models as a strong prior to resolve ambiguities and achieve more robust results, most existing models account only for the hand shape and poses and do not model the texture. To fill this gap, in this work we present HTML, the first parametric texture model of human hands. Our model spans several dimensions of hand appearance variability (e.g., related to gender, ethnicity, or age) and only requires a commodity camera for data acquisition. Experimentally, we demonstrate that our appearance model can be used to tackle a range of challenging problems such as 3D hand reconstruction from a single monocular image. Furthermore, our appearance model can be used to define a neural rendering layer that enables training with a self-supervised photometric loss. We make our model publicly available.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_4');
INSERT INTO `paper` VALUES (12091, 'Human Body Model Fitting by Learned Gradient Descent', 'Human body fitting', '3D human pose', 'Inverse problem', '', '', 'We propose a novel algorithm for the fitting of 3D human shape to images. Combining the accuracy and refinement capabilities of iterative gradient-based optimization techniques with the robustness of deep neural networks, we propose a gradient descent algorithm that leverages a neural network to predict the parameter update rule for each iteration. This per-parameter and state-aware update guides the optimizer towards a good solution in very few steps, converging in typically few steps. During training our approach only requires MoCap data of human poses, parametrized via SMPL. From this data the network learns a subspace of valid poses and shapes in which optimization is performed much more efficiently. The approach does not require any hard to acquire image-to-3D correspondences. At test time we only optimize the 2D joint re-projection error without the need for any further priors or regularization terms. We show empirically that this algorithm is fast (avg. 120ms convergence), robust to initialization and dataset, and achieves state-of-the-art results on public evaluation datasets including the challenging 3DPW in-the-wild benchmark (improvement over SMPLify (\\(45\\%\\)) and also approaches using image-to-3D correspondences).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_44');
INSERT INTO `paper` VALUES (12092, 'Human Correspondence Consensus for 3D Object Semantic Understanding', '', '', '', '', '', 'Semantic understanding of 3D objects is crucial in many applications such as object manipulation. However, it is hard to give a universal definition of point-level semantics that everyone would agree on. We observe that people have a consensus on semantic correspondences between two areas from different objects, but are less certain about the exact semantic meaning of each area. Therefore, we argue that by providing human labeled correspondences between different objects from the same category instead of explicit semantic labels, one can recover rich semantic information of an object. In this paper, we introduce a new dataset named CorresPondenceNet. Based on this dataset, we are able to learn dense semantic embeddings with a novel geodesic consistency loss. Accordingly, several state-of-the-art networks are evaluated on this correspondence benchmark. We further show that CorresPondenceNet could not only boost fine-grained understanding of heterogeneous objects but also cross-object registration and partial object matching.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_30');
INSERT INTO `paper` VALUES (12093, 'Human Interaction Learning on 3D Skeleton Point Clouds for Video Violence Recognition', '', '', '', '', '', 'This paper introduces a new method for recognizing violent behavior by learning contextual relationships between related people from human skeleton points. Unlike previous work, we first formulate 3D skeleton point clouds from human skeleton sequences extracted from videos and then perform interaction learning on these 3D skeleton point clouds. A novel Skeleton Points Interaction Learning (SPIL) module, is proposed to model the interactions between skeleton points. Specifically, by constructing a specific weight distribution strategy between local regional points, SPIL aims to selectively focus on the most relevant parts of them based on their features and spatial-temporal position information. In order to capture diverse types of relation information, a multi-head mechanism is designed to aggregate different features from independent heads to jointly handle different types of relationships between points. Experimental results show that our model outperforms the existing networks and achieves new state-of-the-art performance on video violence datasets.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_5');
INSERT INTO `paper` VALUES (12094, 'Human Motion Transfer from Poses in the Wild', '', '', '', '', '', 'In this paper, we tackle the problem of human motion transfer, where we synthesize novel motion video for a target person that imitates the movement from a reference video. It is a video-to-video translation task in which the estimated poses are used to bridge two domains. Despite substantial progress on the topic, there exist several problems with the previous methods. First, there is a domain gap between training and testing pose sequences–the model is tested on poses it has not seen during training, such as difficult dancing moves. Furthermore, pose detection errors are inevitable, making the job of the generator harder. Finally, generating realistic pixels from sparse poses is challenging in a single step. To address these challenges, we introduce a novel pose-to-video translation framework for generating high-quality videos that are temporally coherent even for in-the-wild pose sequences unseen during training. We propose a pose augmentation method to minimize the training-test gap, a unified paired and unpaired learning strategy to improve the robustness to detection errors, and two-stage network architecture to achieve superior texture quality. To further boost research on the topic, we build two human motion datasets. Finally, we show the superiority of our approach over the state-of-the-art studies through extensive experiments and evaluations on different datasets.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_16');
INSERT INTO `paper` VALUES (12095, 'Hybrid Models for Open Set Recognition', 'Flow-based model', 'Density estimation', 'Image classification', '', '', 'Open set recognition requires a classifier to detect samples not belonging to any of the classes in its training set. Existing methods fit a probability distribution to the training samples on their embedding space and detect outliers according to this distribution. The embedding space is often obtained from a discriminative classifier. However, such discriminative representation focuses only on known classes, which may not be critical for distinguishing the unknown classes. We argue that the representation space should be jointly learned from the inlier classifier and the density estimator (served as an outlier detector). We propose the OpenHybrid framework, which is composed of an encoder to encode the input data into a joint embedding space, a classifier to classify samples to inlier classes, and a flow-based density estimator to detect whether a sample belongs to the unknown category. A typical problem of existing flow-based models is that they may assign a higher likelihood to outliers. However, we empirically observe that such an issue does not occur in our experiments when learning a joint representation for discriminative and generative components. Experiments on standard open set benchmarks also reveal that an end-to-end trained OpenHybrid model significantly outperforms state-of-the-art methods and flow-based baselines.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_7');
INSERT INTO `paper` VALUES (12096, 'i-Walk Intelligent Assessment System: Activity, Mobility, Intention, Communication', 'Intelligent assessment system', 'Human-robot interaction', 'Activity recognition', '3D pose estimation', 'Speech understanding', 'We present the i-Walk system, a novel framework for intelligent mobility assistance applications. The proposed system is capable of automatically understanding human activity, assessing mobility and rehabilitation progress, recognizing human intentions and communicating with the patients by giving meaningful feedback. To this end, multiple sensors, i.e. cameras, microphones, lasers, provide multimodal data in order to allow for user monitoring, while state-of-the-art and beyond algorithms have been developed and integrated into the system to enable recognition, interaction and assessment. More specifically, i-Walk performs in real-time and consists of four main sub-modules that interact automatically to provide speech understanding, activity recognition, mobility analysis and multimodal communication for seamless HRI. The i-Walk assessment system is evaluated on a database of healthy subjects and patients, who participated in carefully designed experimental scenarios that cover essential needs of rehabilitation. The presented results highlight the efficacy of the proposed framework to endow personal assistants with intelligence.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_30');
INSERT INTO `paper` VALUES (12097, 'I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image', '', '', '', '', '', 'Most of the previous image-based 3D human pose and mesh estimation methods estimate parameters of the human mesh model from an input image. However, directly regressing the parameters from the input image is a highly non-linear mapping because it breaks the spatial relationship between pixels in the input image. In addition, it cannot model the prediction uncertainty, which can make training harder. To resolve the above issues, we propose I2L-MeshNet, an image-to-lixel (line+pixel) prediction network. The proposed I2L-MeshNet predicts the per-lixel likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Our lixel-based 1D heatmap preserves the spatial relationship in the input image and models the prediction uncertainty. We demonstrate the benefit of the image-to-lixel prediction and show that the proposed I2L-MeshNet outperforms previous methods. The code is publicly available (https://github.com/mks0601/I2L-MeshNet_RELEASE).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_44');
INSERT INTO `paper` VALUES (12098, 'iCaps: An Interpretable Classifier via Disentangled Capsule Networks', 'Capsule Networks', 'Interpretable neural networks', 'Class-supervised disentanglement', 'Generative Adversarial Networks (GANs)', '', 'We propose an interpretable Capsule Network, \\(\\textit{iCaps}\\), for image classification. A capsule is a group of neurons nested inside each layer, and the one in the last layer is called a class capsule, which is a vector whose norm indicates a predicted probability for the class. Using the class capsule, existing Capsule Networks already provide some level of interpretability. However, there are two limitations which degrade its interpretability: 1) the class capsule also includes classification-irrelevant information, and 2) entities represented by the class capsule overlap. In this work, we address these two limitations using a novel class-supervised disentanglement algorithm and an additional regularizer, respectively. Through quantitative and qualitative evaluations on three datasets, we demonstrate that the resulting classifier, \\(\\textit{iCaps}\\), provides a prediction along with clear rationales behind it with no performance degradation.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_19');
INSERT INTO `paper` VALUES (12099, 'Identity-Aware Multi-sentence Video Description', '', '', '', '', '', 'Standard video and movie description tasks abstract away from person identities, thus failing to link identities across sentences. We propose a multi-sentence Identity-Aware Video Description task, which overcomes this limitation and requires to re-identify persons locally within a set of consecutive clips. We introduce an auxiliary task of Fill-in the Identity , that aims to predict persons’ IDs consistently within a set of clips, when the video descriptions are given. Our proposed approach to this task leverages a Transformer architecture allowing for coherent joint prediction of multiple IDs. One of the key components is a gender-aware textual representation as well an additional gender prediction objective in the main model. This auxiliary task allows us to propose a two-stage approach to Identity-Aware Video Description . We first generate multi-sentence video descriptions, and then apply our Fill-in the Identity model to establish links between the predicted person entities. To be able to tackle both tasks, we augment the Large Scale Movie Description Challenge (LSMDC) benchmark with new annotations suited for our problem statement. Experiments show that our proposed Fill-in the Identity model is superior to several baselines and recent works, and allows us to generate descriptions with locally re-identified people.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_22');
INSERT INTO `paper` VALUES (12100, 'Identity-Guided Human Semantic Parsing for Person Re-identification', 'Person re-ID', 'Weakly-supervised human parsing', 'Aligned representation learning', '', '', 'Existing alignment-based methods have to employ the pre-trained human parsing models to achieve the pixel-level alignment, and cannot identify the personal belongings (e.g., backpacks and reticule) which are crucial to person re-ID. In this paper, we propose the identity-guided human semantic parsing approach (ISP) to locate both the human body parts and personal belongings at pixel-level for aligned person re-ID only with person identity labels. We design the cascaded clustering on feature maps to generate the pseudo-labels of human parts. Specifically, for the pixels of all images of a person, we first group them to foreground or background and then group the foreground pixels to human parts. The cluster assignments are subsequently used as pseudo-labels of human parts to supervise the part estimation and ISP iteratively learns the feature maps and groups them. Finally, local features of both human body parts and personal belongings are obtained according to the self-learned part estimation, and only features of visible parts are utilized for the retrieval. Extensive experiments on three widely used datasets validate the superiority of ISP over lots of state-of-the-art methods. Our code is available at https://github.com/CASIA-IVA-Lab/ISP-reID.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_21');
INSERT INTO `paper` VALUES (12101, 'IdleSR: Efficient Super-Resolution Network with Multi-scale IdleBlocks', 'Image super-resolution', 'Compact model', 'Multi-scale IdleBlock', 'Gradient scaling', '', 'In recent years, deep learning approaches have achieved impressive results in single image super-resolution (SISR). However, most of these models require high computational and memory resources beyond the capability of most mobile and embedded devices. How to significantly reduce the number of operations and parameters while maintaining the performance is a meaningful and challenging problem. To address this problem, we propose an efficient super-resolution network with multi-scale IdleBlocks called IdleSR. Firstly, inspired by information multi-distillation blocks and hybrid composition of IdleBlocks, we construct efficient multi-scale IdleBlocks at the granularity of residual block. Secondly, we replace two 3 \\(\\times \\) 3 kernels in residual blocks by a 5 \\(\\times \\) 1 kernel and a 1 \\(\\times \\) 5 kernel, decreasing parameters and operations dramatically. Thirdly, we use gradient scaling, large input patch size and extra data during training phase to compensate dropped performance. The experiments show that IdleSR can achieve a much better tradeoff among parameter, runtime and performance than start-of-the-art methods.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_8');
INSERT INTO `paper` VALUES (12102, 'Image Classification in the Dark Using Quanta Image Sensors', 'Quanta image sensors', 'Low light', 'Classification', '', '', 'State-of-the-art image classifiers are trained and tested using well-illuminated images. These images are typically captured by CMOS image sensors with at least tens of photons per pixel. However, in dark environments when the photon flux is low, image classification becomes difficult because the measured signal is suppressed by noise. In this paper, we present a new low-light image classification solution using Quanta Image Sensors (QIS). QIS are a new type of image sensors that possess photon-counting ability without compromising on pixel size and spatial resolution. Numerous studies over the past decade have demonstrated the feasibility of QIS for low-light imaging, but their usage for image classification has not been studied. This paper fills the gap by presenting a student-teacher learning scheme which allows us to classify the noisy QIS raw data. We show that with student-teacher learning, we can achieve image classification at a photon level of one photon per pixel or lower. Experimental results verify the effectiveness of the proposed method compared to existing solutions.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_29');
INSERT INTO `paper` VALUES (12103, 'Image Stitching and Rectification for Hand-Held Cameras', 'Rolling Shutter', 'Image rectification', 'Image stitching', 'Differential homography', 'Homography field', 'In this paper, we derive a new differential homography that can account for the scanline-varying camera poses in Rolling Shutter (RS) cameras, and demonstrate its application to carry out RS-aware image stitching and rectification at one stroke. Despite the high complexity of RS geometry, we focus in this paper on a special yet common input—two consecutive frames from a video stream, wherein the inter-frame motion is restricted from being arbitrarily large. This allows us to adopt simpler differential motion model, leading to a straightforward and practical minimal solver. To deal with non-planar scene and camera parallax in stitching, we further propose an RS-aware spatially-varying homogarphy field in the principle of As-Projective-As-Possible (APAP). We show superior performance over state-of-the-art methods both in RS image stitching and rectification, especially for images captured by hand-held shaking cameras.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_15');
INSERT INTO `paper` VALUES (12104, 'Image-Based Table Recognition: Data, Model, and Evaluation', 'Table recognition', 'Dual decoder', 'Dataset', 'Evaluation', '', 'Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g. Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop and release the largest publicly available table recognition dataset PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet.), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central™ Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_34');
INSERT INTO `paper` VALUES (12105, 'Image-to-Voxel Model Translation for 3D Scene Reconstruction and Segmentation', 'Single photo 3D reconstruction', '3D semantic segmentation', '', '', '', 'Objects class, depth, and shape are instantly reconstructed by a human looking at a 2D image. While modern deep models solve each of these challenging tasks separately, they struggle to perform simultaneous scene 3D reconstruction and segmentation. We propose a single shot image-to-semantic voxel model translation framework. We train a generator adversarially against a discriminator that verifies the object’s poses. Furthermore, trapezium-shaped voxels, volumetric residual blocks, and 2D-to-3D skip connections facilitate our model learning explicit reasoning about 3D scene structure. We collected a SemanticVoxels dataset with 116k images, ground-truth semantic voxel models, depth maps, and 6D object poses. Experiments on ShapeNet and our SemanticVoxels datasets demonstrate that our framework achieves and surpasses state-of-the-art in the reconstruction of scenes with multiple non-rigid objects of different classes. We made our model and dataset publicly available (http://www.zefirus.org/SSZ).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_7');
INSERT INTO `paper` VALUES (12106, 'Imaging Behind Occluders Using Two-Bounce Light', 'Non-line-of-sight imaging', 'Computational photography', '', '', '', 'We introduce the new non-line-of-sight imaging problem of imaging behind an occluder. The behind-an-occluder problem can be solved if the hidden space is flanked by opposing visible surfaces. We illuminate one surface and observe light that scatters off of the opposing surface after traveling through the hidden space. Hidden objects attenuate light that passes through the hidden space, leaving an observable signature that can be used to reconstruct their shape. Our method uses a simple capture setup—we use an eye-safe laser pointer as a light source and off-the-shelf RGB or RGB-D cameras to estimate the geometry of relay surfaces and observe two-bounce light. We analyze the photometric and geometric challenges of this new imaging problem, and develop a robust method that produces high-quality 3D reconstructions in uncontrolled settings where relay surfaces may be non-planar.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_34');
INSERT INTO `paper` VALUES (12107, 'Imbalanced Continual Learning with Partitioning Reservoir Sampling', 'Imbalanced learning', 'Continual learning', 'Multi-label classification', 'Long-tailed distribution', 'Online learning', 'Continual learning from a sequential stream of data is a crucial challenge for machine learning research. Most studies have been conducted on this topic under the single-label classification setting along with an assumption of balanced label distribution. This work expands this research horizon towards multi-label classification. In doing so, we identify unanticipated adversity innately existent in many multi-label datasets, the long-tailed distribution. We jointly address the two independently solved problems, Catastropic Forgetting and the long-tailed label distribution by first empirically showing a new challenge of destructive forgetting of the minority concepts on the tail. Then, we curate two benchmark datasets, COCOseq and NUS-WIDEseq, that allow the study of both intra- and inter-task imbalances. Lastly, we propose a new sampling strategy for replay-based approach named Partitioning Reservoir Sampling (PRS), which allows the model to maintain a balanced knowledge of both head and tail classes. We publicly release the dataset and the code in our project page.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_25');
INSERT INTO `paper` VALUES (12108, 'Impact of Base Dataset Design on Few-Shot Image Classification', 'Dataset labeling', 'Few-shot classification', 'Meta-learning', 'Weakly-supervised learning', '', 'The quality and generality of deep image features is crucially determined by the data they have been trained on, but little is known about this often overlooked effect. In this paper, we systematically study the effect of variations in the training data by evaluating deep features trained on different image sets in a few-shot classification setting. The experimental protocol we define allows to explore key practical questions. What is the influence of the similarity between base and test classes? Given a fixed annotation budget, what is the optimal trade-off between the number of images per class and the number of classes? Given a fixed dataset, can features be improved by splitting or combining different classes? Should simple or diverse classes be annotated? In a wide range of experiments, we provide clear answers to these questions on the miniImageNet, ImageNet and CUB-200 benchmarks. We also show how the base dataset design can improve performance in few-shot classification more drastically than replacing a simple baseline by an advanced state of the art algorithm.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_35');
INSERT INTO `paper` VALUES (12109, 'Implementing Planning KL-Divergence', '', '', '', '', '', 'Variants of accuracy and precision are the gold-standard by which the computer vision community measures progress of perception algorithms. One reason for the ubiquity of these metrics is that they are largely task-agnostic; we in general seek to detect zero false negatives or positives. The downside of these metrics is that, at worst, they penalize all incorrect detections equally without conditioning on the task or scene, and at best, heuristics need to be chosen to ensure that different mistakes count differently. In this paper, we revisit “Planning KL-Divergence”, a principled metric for 3D object detection specifically for the task of self-driving. The core idea behind PKL is to isolate the task of object detection and measure the impact the produced detections would induce on the downstream task of driving. We summarize functionality provided by our python package planning-centric-metrics that implements PKL. nuScenes is in the process of incorporating PKL into their detection leaderboard and we hope that the convenience of our implementation encourages other leaderboards to follow suit.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_2');
INSERT INTO `paper` VALUES (12110, 'Implicit Feature Networks for Texture Completion from Partial 3D Data', 'Implicit representation', 'Implicit function learning', 'Texture field', 'Implicit feature networks', 'Texture completion', 'Prior work to infer 3D texture use either texture atlases, which require uv-mappings and hence have discontinuities, or colored voxels, which are memory inefficient and limited in resolution. Recent work, predicts RGB color at every XYZ coordinate forming a texture field, but focus on completing texture given a single 2D image. Instead, we focus on 3D texture and geometry completion from partial and incomplete 3D scans. IF-Nets [2] have recently achieved state-of-the-art results on 3D geometry completion using a multi-scale deep feature encoding, but the outputs lack texture. In this work, we generalize IF-Nets to texture completion from partial textured scans of humans and arbitrary objects. Our key insight is that 3D texture completion benefits from incorporating local and global deep features extracted from both the 3D partial texture and completed geometry. Specifically, given the partial 3D texture and the 3D geometry completed with IF-Nets, our model successfully in-paints the missing texture parts in consistence with the completed geometry. Our model won the SHARP ECCV’20 challenge, achieving highest performance on all challenges.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_48');
INSERT INTO `paper` VALUES (12111, 'Implicit Latent Variable Model for Scene-Consistent Motion Forecasting', '', '', '', '', '', 'In order to plan a safe maneuver an autonomous vehicle must accurately perceive its environment, and understand the interactions among traffic participants. In this paper, we aim to learn scene-consistent motion forecasts of complex urban traffic directly from sensor data. In particular, we propose to characterize the joint distribution over future trajectories via an implicit latent variable model. We model the scene as an interaction graph and employ powerful graph neural networks to learn a distributed latent representation of the scene. Coupled with a deterministic decoder, we obtain trajectory samples that are consistent across traffic participants, achieving state-of-the-art results in motion forecasting and interaction understanding. Last but not least, we demonstrate that our motion forecasts result in safer and more comfortable motion planning.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_37');
INSERT INTO `paper` VALUES (12112, 'Improved Adversarial Training via Learned Optimizer', 'Optimization', 'Adversarial training', 'Learning to learn', '', '', 'Adversarial attack has recently become a tremendous threat to deep learning models. To improve the robustness of machine learning models, adversarial training, formulated as a minimax optimization problem, has been recognized as one of the most effective defense mechanisms. However, the non-convex and non-concave property poses a great challenge to the minimax training. In this paper, we empirically demonstrate that the commonly used PGD attack may not be optimal for inner maximization, and improved inner optimizer can lead to a more robust model. Then we leverage a learning-to-learn (L2L) framework to train an optimizer with recurrent neural networks, providing update directions and steps adaptively for the inner problem. By co-training optimizer’s parameters and model’s weights, the proposed framework consistently improves over PGD-based adversarial training and TRADES.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_6');
INSERT INTO `paper` VALUES (12113, 'Improved Robustness to Open Set Inputs via Tempered Mixup', '', '', '', '', '', 'Supervised classification methods often assume that evaluation data is drawn from the same distribution as training data and that all classes are present for training. However, real-world classifiers must handle inputs that are far from the training distribution including samples from unknown classes. Open set robustness refers to the ability to properly label samples from previously unseen categories as novel and avoid high-confidence, incorrect predictions. Existing approaches have focused on either novel inference methods, unique training architectures, or supplementing the training data with additional background samples. Here, we propose a simple regularization technique easily applied to existing convolutional neural network architectures that improves open set robustness without a background dataset. Our method achieves state-of-the-art results on open set classification baselines and easily scales to large-scale open set classification problems.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_12');
INSERT INTO `paper` VALUES (12114, 'Improving 3D Object Detection Through Progressive Population Based Augmentation', 'Progressive population based augmentation', 'Data augmentation', 'Point cloud', '3D object detection', 'Data efficiency', 'Data augmentation has been widely adopted for object detection in 3D point clouds. However, all previous related efforts have focused on manually designing specific data augmentation methods for individual architectures. In this work, we present the first attempt to automate the design of data augmentation policies for 3D object detection. We introduce the Progressive Population Based Augmentation (PPBA) algorithm, which learns to optimize augmentation strategies by narrowing down the search space and adopting the best parameters discovered in previous iterations. On the KITTI 3D detection test set, PPBA improves the StarNet detector by substantial margins on the moderate difficulty category of cars, pedestrians, and cyclists, outperforming all current state-of-the-art single-stage detection models. Additional experiments on the Waymo Open Dataset indicate that PPBA continues to effectively improve the StarNet and PointPillars detectors on a 20x larger dataset compared to KITTI. The magnitude of the improvements may be comparable to advances in 3D perception architectures and the gains come without an incurred cost at inference time. In subsequent experiments, we find that PPBA may be up to 10x more data efficient than baseline 3D detection models without augmentation, highlighting that 3D detection models may achieve competitive accuracy with far fewer labeled examples.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_17');
INSERT INTO `paper` VALUES (12115, 'Improving Adversarial Robustness by Enforcing Local and Global Compactness', 'Adversarial robustness', 'Local compactness', 'Global compactness', 'Clustering assumption', '', 'The fact that deep neural networks are susceptible to crafted perturbations severely impacts the use of deep learning in certain domains of application. Among many developed defense models against such attacks, adversarial training emerges as the most successful method that consistently resists a wide range of attacks. In this work, based on an observation from a previous study that the representations of a clean data example and its adversarial examples become more divergent in higher layers of a deep neural net, we propose the Adversary Divergence Reduction Network which enforces local/global compactness and the clustering assumption over an intermediate layer of a deep neural network. We conduct comprehensive experiments to understand the isolating behavior of each component (i.e., local/global compactness and the clustering assumption) and compare our proposed model with state-of-the-art adversarial training methods. The experimental results demonstrate that augmenting adversarial training with our proposed components can further improve the robustness of the network, leading to higher unperturbed and adversarial predictive performances.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_13');
INSERT INTO `paper` VALUES (12116, 'Improving Blind Spot Denoising for Microscopy', 'Denoising', 'CNN', 'Light microscopy', 'Deconvolution', '', 'Many microscopy applications are limited by the total amount of usable light and are consequently challenged by the resulting levels of noise in the acquired images. This problem is often addressed via (supervised) deep learning based denoising. Recently, by making assumptions about the noise statistics, self-supervised methods have emerged. Such methods are trained directly on the images that are to be denoised and do not require additional paired training data. While achieving remarkable results, self-supervised methods can produce high-frequency artifacts and achieve inferior results compared to supervised approaches. Here we present a novel way to improve the quality of self-supervised denoising. Considering that light microscopy images are usually diffraction-limited, we propose to include this knowledge in the denoising process. We assume the clean image to be the result of a convolution with a point spread function (PSF) and explicitly include this operation at the end of our neural network. As a consequence, we are able to eliminate high-frequency artifacts and achieve self-supervised results that are very close to the ones achieved with traditional supervised methods.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_25');
INSERT INTO `paper` VALUES (12117, 'Improving Deep Video Compression by Resolution-Adaptive Flow Coding', '', '', '', '', '', 'In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_12');
INSERT INTO `paper` VALUES (12118, 'Improving Face Recognition by Clustering Unlabeled Faces in the Wild', '', '', '', '', '', 'While deep face recognition has benefited significantly from large-scale labeled data, current research is focused on leveraging unlabeled data to further boost performance, reducing the cost of human annotation. Prior work has mostly been in controlled settings, where the labeled and unlabeled data sets have no overlapping identities by construction. This is not realistic in large-scale face recognition, where one must contend with such overlaps, the frequency of which increases with the volume of data. Ignoring identity overlap leads to significant labeling noise, as data from the same identity is split into multiple clusters. To address this, we propose a novel identity separation method based on extreme value theory. It is formulated as an out-of-distribution detection algorithm, and greatly reduces the problems caused by overlapping-identity label noise. Considering cluster assignments as pseudo-labels, we must also overcome the labeling noise from clustering errors. We propose a modulation of the cosine loss, where the modulation weights correspond to an estimate of clustering uncertainty. Extensive experiments on both controlled and real settings demonstrate our method’s consistent improvements over supervised baselines, e.g., 11.6% improvement on IJB-A verification.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_8');
INSERT INTO `paper` VALUES (12119, 'Improving Face Recognition from Hard Samples via Distribution Distillation Loss', 'Face recognition', 'Loss function', 'Distribution distillation', '', '', 'Large facial variations are the main challenge in face recognition. To this end, previous variation-specific methods make full use of task-related prior to design special network losses, which are typically not general among different tasks and scenarios. In contrast, the existing generic methods focus on improving the feature discriminability to minimize the intra-class distance while maximizing the inter-class distance, which perform well on easy samples but fail on hard samples. To improve the performance on hard samples, we propose a novel Distribution Distillation Loss to narrow the performance gap between easy and hard samples, which is simple, effective and generic for various types of facial variations. Specifically, we first adopt state-of-the-art classifiers such as Arcface to construct two similarity distributions: a teacher distribution from easy samples and a student distribution from hard samples. Then, we propose a novel distribution-driven loss to constrain the student distribution to approximate the teacher distribution, which thus leads to smaller overlap between the positive and negative pairs in the student distribution. We have conducted extensive experiments on both generic large-scale face benchmarks and benchmarks with diverse variations on race, resolution and pose. The quantitative results demonstrate the superiority of our method over strong baselines, e.g., Arcface and Cosface. Code will be available at https://github.com/HuangYG123/DDL.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_9');
INSERT INTO `paper` VALUES (12120, 'Improving Keyword Search Performance in Sign Language with Hand Shape Features', 'Sign language recognition', 'Keyword search', 'Handshape recognition', '', '', 'Handshapes and human pose estimation are among the most used pretrained features in sign language recognition. In this study, we develop a handshape based keyword search (KWS) system for sign language and compare different pose based and handshape based encoders for the task of large vocabulary sign retrieval. We improved KWS performance in sign language by 3.5% mAP score for gloss search and 1.6% for cross-lingual KWS by combining pose and handshape based KWS models in a late fusion approach.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_23');
INSERT INTO `paper` VALUES (12121, 'Improving Knowledge Distillation via Category Structure', 'Knowledge distillation', 'Intra-category structure', 'Inter-category structure', 'Structured relation', '', 'Most previous knowledge distillation frameworks train the student to mimic the teacher’s output of each sample or transfer cross-sample relations from the teacher to the student. Nevertheless, they neglect the structured relations at a category level. In this paper, a novel Category Structure is proposed to transfer category-level structured relations for knowledge distillation. It models two structured relations, including intra-category structure and inter-category structure, which are intrinsic natures in relations between samples. Intra-category structure penalizes the structured relations in samples from the same category and inter-category structure focuses on cross-category relations at a category level. Transferring category structure from the teacher to the student supplements category-level structured relations for training a better student. Extensive experiments show that our method groups samples from the same category tighter in the embedding space and the superiority of our method in comparison with closely related works are validated in different datasets and models.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_13');
INSERT INTO `paper` VALUES (12122, 'Improving Monocular Depth Estimation by Leveraging Structural Awareness and Complementary Datasets', '', '', '', '', '', 'Monocular depth estimation plays a crucial role in 3D recognition and understanding. One key limitation of existing approaches lies in their lack of structural information exploitation, which leads to inaccurate spatial layout, discontinuous surface, and ambiguous boundaries. In this paper, we tackle this problem in three aspects. First, to exploit the spatial relationship of visual features, we propose a structure-aware neural network with spatial attention blocks. These blocks guide the network attention to global structures or local details across different feature layers. Second, we introduce a global focal relative loss for uniform point pairs to enhance spatial constraint in the prediction, and explicitly increase the penalty on errors in depth-wise discontinuous regions, which helps preserve the sharpness of estimation results. Finally, based on analysis of failure cases for prior methods, we collect a new Hard Case (HC) Depth dataset of challenging scenes, such as special lighting conditions, dynamic objects, and tilted camera angles. The new dataset is leveraged by an informed learning curriculum that mixes training examples incrementally to handle diverse data distributions. Experimental results show that our method outperforms state-of-the-art approaches by a large margin in terms of both prediction accuracy on NYUDv2 dataset and generalization performance on unseen datasets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_6');
INSERT INTO `paper` VALUES (12123, 'Improving Multispectral Pedestrian Detection by Addressing Modality Imbalance Problems', 'Multispectral pedestrian detection', 'Modality imbalance problems', 'Multimodal feature fusion', '', '', 'Multispectral pedestrian detection is capable of adapting to insufficient illumination conditions by leveraging color-thermal modalities. On the other hand, it is still lacking of in-depth insights on how to fuse the two modalities effectively. Compared with traditional pedestrian detection, we find multispectral pedestrian detection suffers from modality imbalance problems which will hinder the optimization process of dual-modality network and depress the performance of detector. Inspired by this observation, we propose Modality Balance Network (MBNet) which facilitates the optimization process in a much more flexible and balanced manner. Firstly, we design a novel Differential Modality Aware Fusion (DMAF) module to make the two modalities complement each other. Secondly, an illumination aware feature alignment module selects complementary features according to the illumination conditions and aligns the two modality features adaptively. Extensive experimental results demonstrate MBNet outperforms the state-of-the-arts on both the challenging KAIST and CVC-14 multispectral pedestrian datasets in terms of the accuracy and the computational efficiency. Code is available at https://github.com/CalayZhou/MBNet.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_46');
INSERT INTO `paper` VALUES (12124, 'Improving Object Detection with Selective Self-supervised Self-training', '', '', '', '', '', 'We study how to leverage Web images to augment human-curated object detection datasets. Our approach is two-pronged. On the one hand, we retrieve Web images by image-to-image search, which incurs less domain shift from the curated data than other search methods. The Web images are diverse, supplying a wide variety of object poses, appearances, their interactions with the context, etc. On the other hand, we propose a novel learning method motivated by two parallel lines of work that explore unlabeled data for image classification: self-training and self-supervised learning. They fail to improve object detectors in their vanilla forms due to the domain gap between the Web images and curated datasets. To tackle this challenge, we propose a selective net to rectify the supervision signals in Web images. It not only identifies positive bounding boxes but also creates a safe zone for mining hard negative boxes. We report state-of-the-art results on detecting backpacks and chairs from everyday scenes, along with other challenging object classes.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_35');
INSERT INTO `paper` VALUES (12125, 'Improving One-Stage Visual Grounding by Recursive Sub-query Construction', 'Visual grounding', 'Query modeling', 'Referring expressions', '', '', 'We improve one-stage visual grounding by addressing current limitations on grounding long and complex queries. Existing one-stage methods encode the entire language query as a single sentence embedding vector, e.g., taking the embedding from BERT or the hidden state from LSTM. This single vector representation is prone to overlooking the detailed descriptions in the query. To address this query modeling deficiency, we propose a recursive sub-query construction framework, which reasons between image and query for multiple rounds and reduces the referring ambiguity step by step. We show our new one-stage method obtains \\(5.0\\%, 4.5\\%, 7.5\\%, 12.8\\%\\) absolute improvements over the state-of-the-art one-stage approach on ReferItGame, RefCOCO, RefCOCO+, and RefCOCOg, respectively. In particular, superior performances on longer and more complex queries validates the effectiveness of our query modeling. Code is available at https://github.com/zyang-ur/ReSC.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_23');
INSERT INTO `paper` VALUES (12126, 'Improving Optical Flow on a Pyramid Level', '', '', '', '', '', 'In this work we review the coarse-to-fine spatial feature pyramid concept, which is used in state-of-the-art optical flow estimation networks to make exploration of the pixel flow search space computationally tractable and efficient. Within an individual pyramid level, we improve the cost volume construction process by departing from a warping- to a sampling-based strategy, which avoids ghosting and hence enables us to better preserve fine flow details. We further amplify the positive effects through a level-specific, loss max-pooling strategy that adaptively shifts the focus of the learning process on under-performing predictions. Our second contribution revises the gradient flow across pyramid levels. The typical operations performed at each pyramid level can lead to noisy, or even contradicting gradients across levels. We show and discuss how properly blocking some of these gradient components leads to improved convergence and ultimately better performance. Finally, we introduce a distillation concept to counteract the issue of catastrophic forgetting during finetuning and thus preserving knowledge over models sequentially trained on multiple datasets. Our findings are conceptually simple and easy to implement, yet result in compelling improvements on relevant error measures that we demonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and even show the portability of our findings to different optical flow and depth from stereo approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_46');
INSERT INTO `paper` VALUES (12127, 'Improving Pixel Embedding Learning Through Intermediate Distance Regression Supervision for Instance Segmentation', 'Instance segmentation', 'Pixel embedding', 'Distance regression', '', '', 'As a proposal-free approach, instance segmentation through pixel embedding learning and clustering is gaining more emphasis. Compared with bounding box refinement approaches, such as Mask R-CNN, it has potential advantages in handling complex shapes and dense objects. In this work, we propose a simple, yet highly effective, architecture for object-aware embedding learning. A distance regression module is incorporated into our architecture to generate seeds for fast clustering. At the same time, we show that the features learned by the distance regression module are able to promote the accuracy of learned object-aware embeddings significantly. By simply concatenating features of the distance regression module to the images as inputs of the embedding module, the mSBD scores on the CVPPP Leaf Segmentation Challenge can be further improved by more than 8% compared to the identical set-up without concatenation, yielding the best overall result amongst the leaderboard at CodaLab.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_16');
INSERT INTO `paper` VALUES (12128, 'Improving Query Efficiency of Black-Box Adversarial Attack', 'Black-box adversarial attack', 'Adversarial distribution', 'Query efficiency', 'Neural Process', '', 'Deep neural networks (DNNs) have demonstrated excellent performance on various tasks, however they are under the risk of adversarial examples that can be easily generated when the target model is accessible to an attacker (white-box setting). As plenty of machine learning models have been deployed via online services that only provide query outputs from inaccessible models (e.g., Google Cloud Vision API2), black-box adversarial attacks (inaccessible target model) are of critical security concerns in practice rather than white-box ones. However, existing query-based black-box adversarial attacks often require excessive model queries to maintain a high attack success rate. Therefore, in order to improve query efficiency, we explore the distribution of adversarial examples around benign inputs with the help of image structure information characterized by a Neural Process, and propose a Neural Process based black-box adversarial attack (NP-Attack) in this paper. Extensive experiments show that NP-Attack could greatly decrease the query counts under the black-box setting. Code is available at https://github.com/Sandy-Zeng/NPAttack.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_7');
INSERT INTO `paper` VALUES (12129, 'Improving Semantic Segmentation via Decoupled Body and Edge Supervision', 'Semantic segmentation', 'Edge supervision', 'Flow field', 'Multi-task learning', '', 'Existing semantic segmentation approaches either aim to improve the object’s inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, CamVid, KIITI and BDD show that our proposed approach establishes new state of the art while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models are made available to foster any further research (https://github.com/lxtGH/DecoupleSegNets).', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_26');
INSERT INTO `paper` VALUES (12130, 'Improving the Transferability of Adversarial Examples with Resized-Diverse-Inputs, Diversity-Ensemble and Region Fitting', 'Adversarial examples', 'The internal relationship', 'Region fitting', 'Resized-diverse-inputs', 'Diversity-ensemble', 'We introduce a three stage pipeline: resized-diverse-inputs (RDIM), diversity-ensemble (DEM) and region fitting, that work together to generate transferable adversarial examples. We first explore the internal relationship between existing attacks, and propose RDIM that is capable of exploiting this relationship. Then we propose DEM, the multi-scale version of RDIM, to generate multi-scale gradients. After the first two steps we transform value fitting into region fitting across iterations. RDIM and region fitting do not require extra running time and these three steps can be well integrated into other attacks. Our best attack fools six black-box defenses with a 93% success rate on average, which is higher than the state-of-the-art gradient-based attacks. Besides, we rethink existing attacks rather than simply stacking new methods on the old ones to get better performance. It is expected that our findings will serve as the beginning of exploring the internal relationship between attack methods.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_34');
INSERT INTO `paper` VALUES (12131, 'Improving Vision-and-Language Navigation with Image-Text Pairs from the Web', 'Vision-and-language navigation', 'Transfer learning', 'Embodied AI', '', '', 'Following a navigation instruction such as ‘Walk down the stairs and stop at the brown sofa’ requires embodied AI agents to ground referenced scene elements referenced (e.g. ‘stairs’) to visual content in the environment (pixels corresponding to ‘stairs’). We ask the following question – can we leverage abundant ‘disembodied’ web-scraped vision-and-language corpora (e.g. Conceptual Captions) to learn the visual groundings that improve performance on a relatively data-starved embodied perception task (Vision-and-Language Navigation)? Specifically, we develop VLN-BERT, a visiolinguistic transformer-based model for scoring the compatibility between an instruction (‘...stop at the brown sofa’) and a trajectory of panoramic RGB images captured by the agent. We demonstrate that pretraining VLN-BERT on image-text pairs from the web before fine-tuning on embodied path-instruction data significantly improves performance on VLN – outperforming prior state-of-the-art in the fully-observed setting by 4 absolute percentage points on success rate. Ablations of our pretraining curriculum show each stage to be impactful – with their combination resulting in further gains.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_16');
INSERT INTO `paper` VALUES (12132, 'In-Domain GAN Inversion for Real Image Editing', '', '', '', '', '', 'Recent work has shown that a variety of semantics emerge in the latent space of Generative Adversarial Networks (GANs) when being trained to synthesize images. However, it is difficult to use these learned semantics for real image editing. A common practice of feeding a real image to a trained GAN generator is to invert it back to a latent code. However, existing inversion methods typically focus on reconstructing the target image by pixel values yet fail to land the inverted code in the semantic domain of the original latent space. As a result, the reconstructed image cannot well support semantic editing through varying the inverted code. To solve this problem, we propose an in-domain GAN inversion approach, which not only faithfully reconstructs the input image but also ensures the inverted code to be semantically meaningful for editing. We first learn a novel domain-guided encoder to project a given image to the native latent space of GANs. We then propose domain-regularized optimization by involving the encoder as a regularizer to fine-tune the code produced by the encoder and better recover the target image. Extensive experiments suggest that our inversion method achieves satisfying real image reconstruction and more importantly facilitates various image editing tasks, significantly outperforming start-of-the-arts. (Code and models are available at https://genforce.github.io/idinvert/.)', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_35');
INSERT INTO `paper` VALUES (12133, 'In-Home Daily-Life Captioning Using Radio Signals', '', '', '', '', '', 'This paper aims to caption daily life – i.e., to create a textual description of people’s activities and interactions with objects in their homes. Addressing this problem requires novel methods beyond traditional video captioning, as most people would have privacy concerns about deploying cameras throughout their homes. We introduce RF-Diary, a new model for captioning daily life by analyzing the privacy-preserving radio signal in the home with the home’s floormap. RF-Diary can further observe and caption people’s life through walls and occlusions and in dark settings. In designing RF-Diary, we exploit the ability of radio signals to capture people’s 3D dynamics, and use the floormap to help the model learn people’s interactions with objects. We also use a multi-modal feature alignment training scheme that leverages existing video-based captioning datasets to improve the performance of our radio-based captioning model. Extensive experimental results demonstrate that RF-Diary generates accurate captions under visible conditions. It also sustains its good performance in dark or occluded settings, where video-based captioning approaches fail to generate meaningful captions.(For more information, please visit our project webpage: http://rf-diary.csail.mit.edu)', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_7');
INSERT INTO `paper` VALUES (12134, 'Inclusive GAN: Improving Data and Minority Coverage in Generative Models', 'GAN', 'Minority inclusion', 'Data coverage', '', '', 'Generative Adversarial Networks (GANs) have brought about rapid progress towards generating photorealistic images. Yet the equitable allocation of their modeling capacity among subgroups has received less attention, which could lead to potential biases against underrepresented minorities if left uncontrolled. In this work, we first formalize the problem of minority inclusion as one of data coverage, and then propose to improve data coverage by harmonizing adversarial training with reconstructive generation. The experiments show that our method outperforms the existing state-of-the-art methods in terms of data coverage on both seen and unseen data. We develop an extension that allows explicit control over the minority subgroups that the model should ensure to include, and validate its effectiveness at little compromise from the overall performance on the entire dataset. Code, models, and supplemental videos are available at GitHub.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_23');
INSERT INTO `paper` VALUES (12135, 'Incorporating Reinforced Adversarial Learning in Autoregressive Image Generation', 'Autoregressive models', 'Reinforcement learning', 'Vector quantized variational autoencoders', 'Generative adversarial networks', '', 'Autoregressive models recently achieved comparable results versus state-of-the-art Generative Adversarial Networks (GANs) with the help of Vector Quantized Variational AutoEncoders (VQ-VAE). However, autoregressive models have several limitations such as exposure bias and their training objective does not guarantee visual fidelity. To address these limitations, we propose to use Reinforced Adversarial Learning (RAL) based on policy gradient optimization for autoregressive models. By applying RAL, we enable a similar process for training and testing to address the exposure bias issue. In addition, visual fidelity has been further optimized with adversarial loss inspired by their strong counterparts: GANs. Due to the slow sampling speed of autoregressive models, we propose to use partial generation for faster training. RAL also empowers the collaboration between different modules of the VQ-VAE framework. To our best knowledge, the proposed method is first to enable adversarial learning in autoregressive models for image generation. Experiments on synthetic and real-world datasets show improvements over the MLE trained models. The proposed method improves both negative log-likelihood (NLL) and Fréchet Inception Distance (FID), which indicates improvements in terms of visual quality and diversity. The proposed method achieves state-of-the-art results on Celeba for 64\\(\\times \\)64 image resolution, showing promise for large scale image generation.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_2');
INSERT INTO `paper` VALUES (12136, 'Increasing the Robustness of Semantic Segmentation Models with Painting-by-Numbers', 'Semantic segmentation', 'Shape-bias', 'Corruption robustness', '', '', 'For safety-critical applications such as autonomous driving, CNNs have to be robust with respect to unavoidable image corruptions, such as image noise. While previous works addressed the task of robust prediction in the context of full-image classification, we consider it for dense semantic segmentation. We build upon an insight from image classification that output robustness can be improved by increasing the network-bias towards object shapes. We present a new training schema that increases this shape bias. Our basic idea is to alpha-blend a portion of the RGB training images with faked images, where each class-label is given a fixed, randomly chosen color that is not likely to appear in real imagery. This forces the network to rely more strongly on shape cues. We call this data augmentation technique “Painting-by-Numbers”. We demonstrate the effectiveness of our training schema for DeepLabv3\\(+\\) with various network backbones, MobileNet-V2, ResNets, and Xception, and evaluate it on the Cityscapes dataset. With respect to our 16 different types of image corruptions and 5 different network backbones, we are in 74% better than training with clean data. For cases where we are worse than a model trained without our training schema, it is mostly only marginally worse. However, for some image corruptions such as images with noise, we see a considerable performance gain of up to 25%.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_22');
INSERT INTO `paper` VALUES (12137, 'Incremental Few-Shot Meta-learning via Indirect Discriminant Alignment', '', '', '', '', '', 'We propose a method to train a model so it can learn new classification tasks while improving with each task solved. This amounts to combining meta-learning with incremental learning. Different tasks can have disjoint classes, so one cannot directly align different classifiers as done in model distillation. On the other hand, simply aligning features shared by all classes does not allow the base model sufficient flexibility to evolve to solve new tasks. We therefore indirectly align features relative to a minimal set of “anchor classes”. Such indirect discriminant alignment (IDA) adapts a new model to old classes without the need to re-process old data, while leaving maximum flexibility for the model to adapt to new tasks. This process enables incrementally improving the model by processing multiple learning episodes, each representing a different learning task, even with few training examples. Experiments on few-shot learning benchmarks show that this incremental approach performs favorably compared to training the model with the entire dataset at once.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_40');
INSERT INTO `paper` VALUES (12138, 'Indirect Local Attacks for Context-Aware Semantic Segmentation Networks', 'Adversarial attacks', 'Semantic segmentation', '', '', '', 'Recently, deep networks have achieved impressive semantic segmentation performance, in particular thanks to their use of larger contextual information. In this paper, we show that the resulting networks are sensitive not only to global adversarial attacks, where perturbations affect the entire input image, but also to indirect local attacks, where the perturbations are confined to a small image region that does not overlap with the area that the attacker aims to fool. To this end, we introduce an indirect attack strategy, namely adaptive local attacks, aiming to find the best image location to perturb, while preserving the labels at this location and producing a realistic-looking segmentation map. Furthermore, we propose attack detection techniques both at the global image level and to obtain a pixel-wise localization of the fooled regions. Our results are unsettling: Because they exploit a larger context, more accurate semantic segmentation networks are more sensitive to indirect local attacks. We believe that our comprehensive analysis will motivate the community to design architectures with contextual dependencies that do not trade off robustness for accuracy.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_36');
INSERT INTO `paper` VALUES (12139, 'Inducing Optimal Attribute Representations for Conditional GANs', 'Conditional GAN', 'Graph Convolutional Network', 'Multi-task learning', 'Face attributes', '', 'Conditional GANs (cGANs) are widely used in translating an image from one category to another. Meaningful conditions on GANs provide greater flexibility and control over the nature of the target domain synthetic data. Existing conditional GANs commonly encode target domain label information as hard-coded categorical vectors in the form of 0s and 1s. The major drawbacks of such representations are inability to encode the high-order semantic information of target categories and their relative dependencies. We propose a novel end-to-end learning framework based on Graph Convolutional Networks to learn the attribute representations to condition the generator. The GAN losses, the discriminator and attribute classification loss, are fed back to the graph resulting in the synthetic images that are more natural and clearer with respect to the attributes generation. Moreover, prior-arts are mostly given priorities to condition on the generator side, not on the discriminator side of GANs. We apply the conditions on the discriminator side as well via multi-task learning. We enhanced four state-of-the-art cGANs architectures: Stargan, Stargan-JNT, AttGAN and STGAN. Our extensive qualitative and quantitative evaluations on challenging face attributes manipulation data set, CelebA, LFWA, and RaFD, show that the cGANs enhanced by our methods outperform by a large margin, compared to their counter-parts and other conditioning methods, in terms of both target attributes recognition rates and quality measures such as PSNR and SSIM.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_5');
INSERT INTO `paper` VALUES (12140, 'Inequality-Constrained and Robust 3D Face Model Fitting', '3D model fitting', '3D face reconstruction', '3D shape', '', '', 'Fitting 3D morphable models (3DMMs) on faces is a well-studied problem, motivated by various industrial and research applications. 3DMMs express a 3D facial shape as a linear sum of basis functions. The resulting shape, however, is a plausible face only when the basis coefficients take values within limited intervals. Methods based on unconstrained optimization address this issue with a weighted \\(\\ell _2\\) penalty on coefficients; however, determining the weight of this penalty is difficult, and the existence of a single weight that works universally is questionable. We propose a new formulation that does not require the tuning of any weight parameter. Specifically, we formulate 3DMM fitting as an inequality-constrained optimization problem, where the primary constraint is that basis coefficients should not exceed the interval that is learned when the 3DMM is constructed. We employ additional constraints to exploit sparse landmark detectors, by forcing the facial shape to be within the error bounds of a reliable detector. To enable operation “in-the-wild”, we use a robust objective function, namely Gradient Correlation. Our approach performs comparably with deep learning (DL) methods on “in-the-wild” data that have inexact ground truth, and better than DL methods on more controlled data with exact ground truth. Since our formulation does not require any learning, it enjoys a versatility that allows it to operate with multiple frames of arbitrary sizes. This study’s results encourage further research on 3DMM fitting with inequality-constrained optimization methods, which have been unexplored compared to unconstrained methods.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_25');
INSERT INTO `paper` VALUES (12141, 'Inertial Safety from Structured Light', '', '', '', '', '', 'We present inertial safety maps (ISM), a novel scene representation designed for fast detection of obstacles in scenarios involving camera or scene motion, such as robot navigation and human-robot interaction. ISM is a motion-centric representation that encodes both scene geometry and motion; different camera motion results in different ISMs for the same scene. We show that ISM can be estimated with a two-camera stereo setup without explicitly recovering scene depths, by measuring differential changes in disparity over time. We develop an active, single-shot structured light-based approach for robustly measuring ISM in challenging scenarios with textureless objects and complex geometries. The proposed approach is computationally light-weight, and can detect intricate obstacles (e.g., thin wire fences) by processing high-resolution images at high-speeds with limited computational resources. ISM can be readily integrated with depth and range maps as a complementary scene representation, potentially enabling high-speed navigation and robotic manipulation in extreme environments, with minimal device complexity.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_44');
INSERT INTO `paper` VALUES (12142, 'Inference Graphs for CNN Interpretation', '', '', '', '', '', 'Convolutional neural networks (CNNs) have achieved superior accuracy in many visual related tasks. However, the inference process through intermediate layers is opaque, making it difficult to interpret such networks or develop trust in their operation. We propose to model the network hidden layers activity using probabilistic models. The activity patterns in layers of interest are modeled as Gaussian mixture models, and transition probabilities between clusters in consecutive modeled layers are estimated. Based on maximum-likelihood considerations, nodes and paths relevant for network prediction are chosen, connected, and visualized as an inference graph. We show that such graphs are useful for understanding the general inference process of a class, as well as explaining decisions the network makes regarding specific images.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_5');
INSERT INTO `paper` VALUES (12143, 'Info3D: Representation Learning on 3D Objects Using Mutual Information Maximization and Contrastive Learning', '3D shape analysis', 'Unsupervised learning', 'Rotation invariance', 'InfoMax', 'Contrastive learning', 'A major endeavor of computer vision is to represent, understand and extract structure from 3D data. Towards this goal, unsupervised learning is a powerful and necessary tool. Most current unsupervised methods for 3D shape analysis use datasets that are aligned, require objects to be reconstructed and suffer from deteriorated performance on downstream tasks. To solve these issues, we propose to extend the InfoMax and contrastive learning principles on 3D shapes. We show that we can maximize the mutual information between 3D objects and their “chunks” to improve the representations in aligned datasets. Furthermore, we can achieve rotation invariance in SO(3) group by maximizing the mutual information between the 3D objects and their geometric transformed versions. Finally, we conduct several experiments such as clustering, transfer learning, shape retrieval, and achieve state of art results.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_37');
INSERT INTO `paper` VALUES (12144, 'InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling', '3D object detection', 'Point cloud', '', '', '', 'Real-time 3D object detection is crucial for autonomous cars. Achieving promising performance with high efficiency, voxel-based approaches have received considerable attention. However, previous methods model the input space with features extracted from equally divided sub-regions without considering that point cloud is generally non-uniformly distributed over the space. To address this issue, we propose a novel 3D object detection framework with dynamic information modeling. The proposed framework is designed in a coarse-to-fine manner. Coarse predictions are generated in the first stage via a voxel-based region proposal network. We introduce InfoFocus, which improves the coarse detections by adaptively refining features guided by the information of point cloud density. Experiments are conducted on the large-scale nuScenes 3D detection benchmark. Results show that our framework achieves the state-of-the-art performance with 31 FPS and improves our baseline significantly by 9.0% mAP on the nuScenes test set.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_24');
INSERT INTO `paper` VALUES (12145, 'Informative Sample Mining Network for Multi-domain Image-to-Image Translation', 'Image-to-image translation', 'Multi-domain image generation', 'Generative adversarial networks', '', '', 'The performance of multi-domain image-to-image translation has been significantly improved by recent progress in deep generative models. Existing approaches can use a unified model to achieve translations between all the visual domains. However, their outcomes are far from satisfying when there are large domain variations. In this paper, we reveal that improving the sample selection strategy is an effective solution. To select informative samples, we dynamically estimate sample importance during the training of Generative Adversarial Networks, presenting Informative Sample Mining Network. We theoretically analyze the relationship between the sample importance and the prediction of the global optimal discriminator. Then a practical importance estimation function for general conditions is derived. Furthermore, we propose a novel multi-stage sample training scheme to reduce sample hardness while preserving sample informativeness. Extensive experiments on a wide range of specific image-to-image translation tasks are conducted, and the results demonstrate our superiority over current state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_24');
INSERT INTO `paper` VALUES (12146, 'Infrastructure-Based Multi-camera Calibration Using Radial Projections', '', '', '', '', '', 'Multi-camera systems are an important sensor platform for intelligent systems such as self-driving cars. Pattern-based calibration techniques can be used to calibrate the intrinsics of the cameras individually. However, extrinsic calibration of systems with little to no visual overlap between the cameras is a challenge. Given the camera intrinsics, infrastructure-based calibration techniques are able to estimate the extrinsics using 3D maps pre-built via SLAM or Structure-from-Motion. In this paper, we propose to fully calibrate a multi-camera system from scratch using an infrastructure-based approach. Assuming that the distortion is mainly radial, we introduce a two-stage approach. We first estimate the camera-rig extrinsics up to a single unknown translation component per camera. Next, we solve for both the intrinsic parameters and the missing translation components. Extensive experiments on multiple indoor and outdoor scenes with multiple multi-camera systems show that our calibration method achieves high accuracy and robustness. In particular, our approach is more robust than the naive approach of first estimating intrinsic parameters and pose per camera before refining the extrinsic parameters of the system. The implementation is available at https://github.com/youkely/InfrasCal.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_20');
INSERT INTO `paper` VALUES (12147, 'Inherent Adversarial Robustness of Deep Spiking Neural Networks: Effects of Discrete Input Encoding and Non-linear Activations', 'Spiking neural networks', 'Adversarial attack', 'Leaky-integrate-fire neuron', 'Input discretization', '', 'In the recent quest for trustworthy neural networks, we present Spiking Neural Network (SNN) as a potential candidate for inherent robustness against adversarial attacks. In this work, we demonstrate that adversarial accuracy of SNNs under gradient-based attacks is higher than their non-spiking counterparts for CIFAR datasets on deep VGG and ResNet architectures, particularly in blackbox attack scenario. We attribute this robustness to two fundamental characteristics of SNNs and analyze their effects. First, we exhibit that input discretization introduced by the Poisson encoder improves adversarial robustness with reduced number of timesteps. Second, we quantify the amount of adversarial accuracy with increased leak rate in Leaky-Integrate-Fire (LIF) neurons. Our results suggest that SNNs trained with LIF neurons and smaller number of timesteps are more robust than the ones with IF (Integrate-Fire) neurons and larger number of timesteps. Also we overcome the bottleneck of creating gradient-based adversarial inputs in temporal domain by proposing a technique for crafting attacks from SNN (https://github.com/ssharmin/spikingNN-adversarial-attack).', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_24');
INSERT INTO `paper` VALUES (12148, 'Injecting Prior Knowledge into Image Caption Generation', '', '', '', '', '', 'Automatically generating natural language descriptions from an image is a challenging problem in artificial intelligence that requires a good understanding of the visual and textual signals and the correlations between them. The state-of-the-art methods in image captioning struggles to approach human level performance, especially when data is limited. In this paper, we propose to improve the performance of the state-of-the-art image captioning models by incorporating two sources of prior knowledge: (i) a conditional latent topic attention, that uses a set of latent variables (topics) as an anchor to generate highly probable words and, (ii) a regularization technique that exploits the inductive biases in syntactic and semantic structure of captions and improves the generalization of image captioning models. Our experiments validate that our method produces more human interpretable captions and also leads to significant improvements on the MSCOCO dataset in both the full and low data regimes.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_26');
INSERT INTO `paper` VALUES (12149, 'Insights on Evaluation of Camera Re-localization Using Relative Pose Regression', 'Re-localization', 'Relative-pose-regression', 'Frustum-overlap', '', '', 'We consider the problem of relative pose regression in visual relocalization. Recently, several promising approaches have emerged in this area. We claim that even though they demonstrate on the same datasets using the same split to train and test, a faithful comparison between them was not available since on currently used evaluation metric, some approaches might perform favorably, while in reality performing worse. We reveal a tradeoff between accuracy and the 3D volume of the regressed subspace. We believe that unlike other relocalization approaches, in the case of relative pose regression, the regressed subspace 3D volume is less dependent on the scene and more affect by the method used to score the overlap, which determined how closely sampled viewpoints are. We propose three new metrics to remedy the issue mentioned above. The proposed metrics incorporate statistics about the regression subspace volume. We also propose a new pose regression network that serves as a new baseline for this task. We compare the performance of our trained model on Microsoft 7-Scenes and Cambridge Landmarks datasets both with the standard metrics and the newly proposed metrics and adjust the overlap score to reveal the tradeoff between the subspace and performance. The results show that the proposed metrics are more robust to different overlap threshold than the conventional approaches. Finally, we show that our network generalizes well, specifically, training on a single scene leads to little loss of performance on the other scenes.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_37');
INSERT INTO `paper` VALUES (12150, 'Instance Adaptive Self-training for Unsupervised Domain Adaptation', 'Domain adaptation', 'Semantic segmentation', 'Self-training', 'Regularization', '', 'The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on ‘GTA5 to Cityscapes’ and ‘SYNTHIA to Cityscapes’ demonstrate the superior performance of our approach compared with the state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_25');
INSERT INTO `paper` VALUES (12151, 'Instance-Aware Embedding for Point Cloud Instance Segmentation', '3D point cloud', 'Instance segmentation', 'Instance-aware', '', '', 'Although recent works have made significant progress in encoding meaningful context information for instance segmentation in 2D images, the works for 3D point cloud counterpart lag far behind. Conventional methods use radius search or other similar methods for aggregating local information. However, these methods are unaware of the instance context and fail to realize the boundary and geometric information of an instance, which are critical to separate adjacent objects. In this work, we study the influence of instance-aware knowledge by proposing an Instance-Aware Module (IAM). The proposed IAM learns discriminative instance embedding features in two-fold: (1) Instance contextual regions, covering the spatial extension of an instance, are implicitly learned and propagated in the decoding process. (2) Instance-dependent geometric knowledge is included in the embedding space, which is informative and critical to discriminate adjacent instances. Moreover, the proposed IAM is free from complicated and time-consuming operations, showing superiority in both accuracy and efficiency over the previous methods. To validate the effectiveness of our proposed method, comprehensive experiments have been conducted on three popular benchmarks for instance segmentation: ScannetV2, S3DIS, and PartNet and achieve state-of-the-art performance. The flexibility of our method allows it to handle both indoor scenes and CAD objects.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_16');
INSERT INTO `paper` VALUES (12152, 'Inter-Image Communication for Weakly Supervised Localization', '', '', '', '', '', 'Weakly supervised localization aims at finding target object regions using only image-level supervision. However, localization maps extracted from classification networks are often not accurate due to the lack of fine pixel-level supervision. In this paper, we propose to leverage pixel-level similarities across different objects for learning more accurate object locations in a complementary way. Particularly, two kinds of constraints are proposed to prompt the consistency of object features within the same categories. The first constraint is to learn the stochastic feature consistency among discriminative pixels that are randomly sampled from different images within a batch. The discriminative information embedded in one image can be leveraged to benefit its counterpart with inter-image communication. The second constraint is to learn the global consistency of object features throughout the entire dataset. We learn a feature center for each category and realize the global feature consistency by forcing the object features to approach class-specific centers. The global centers are actively updated with the training process. The two constraints can benefit each other to learn consistent pixel-level features within the same categories, and finally improve the quality of localization maps. We conduct extensive experiments on two popular benchmarks, i.e., ILSVRC and CUB-200-2011. Our method achieves the Top-1 localization error rate of \\(45.17\\%\\) on the ILSVRC validation set, surpassing the current state-of-the-art method by a large margin. The code is available at https://github.com/xiaomengyc/I2C.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_17');
INSERT INTO `paper` VALUES (12153, 'Interactive Annotation of 3D Object Geometry Using 2D Scribbles', '', '', '', '', '', 'Inferring detailed 3D geometry of the scene is crucial for robotics applications, simulation, and 3D content creation. However, such information is hard to obtain, and thus very few datasets support it. In this paper, we propose an interactive framework for annotating 3D object geometry from both point cloud data and RGB imagery. The key idea behind our approach is to exploit strong priors that humans have about the 3D world in order to interactively annotate complete 3D shapes. Our framework targets naive users without artistic or graphics expertise. We introduce two simple-to-use interaction modules. First, we make an automatic guess of the 3D shape and allow the user to provide feedback about large errors by drawing scribbles in desired 2D views. Next, we aim to correct minor errors, in which users drag and drop mesh vertices, assisted by a neural interactive module implemented as a Graph Convolutional Network. Experimentally, we show that only a few user interactions are needed to produce good quality 3D shapes on popular benchmarks such as ShapeNet, Pix3D and ScanNet. We implement our framework as a web service and conduct a user study, where we show that user annotated data using our method effectively facilitates real-world learning tasks. Web service: http://www.cs.toronto.edu/~shenti11/scribble3d.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_44');
INSERT INTO `paper` VALUES (12154, 'Interactive Multi-dimension Modulation with Dynamic Controllable Residual Learning for Image Restoration', '', '', '', '', '', 'Interactive image restoration aims to generate restored images by adjusting a controlling coefficient which determines the restoration level. Previous works are restricted in modulating image with a single coefficient. However, real images always contain multiple types of degradation, which cannot be well determined by one coefficient. To make a step forward, this paper presents a new problem setup, called multi-dimension (MD) modulation, which aims at modulating output effects across multiple degradation types and levels. Compared with the previous single-dimension (SD) modulation, the MD is setup to handle multiple degradations adaptively and relief unbalanced learning problem in different degradations. We also propose a deep architecture - CResMD with newly introduced controllable residual connections for multi-dimension modulation. Specifically, we add a controlling variable on the conventional residual connection to allow a weighted summation of input and residual. The values of these weights are generated by another condition network. We further propose a new data sampling strategy based on beta distribution to balance different degradation types and levels. With corrupted image and degradation information as inputs, the network can output the corresponding restored image. By tweaking the condition vector, users can control the output effects in MD space at test time. Extensive experiments demonstrate that the proposed CResMD achieve excellent performance on both SD and MD modulation tasks. Code is available at https://github.com/hejingwenhejingwen/CResMD.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_4');
INSERT INTO `paper` VALUES (12155, 'Interactive Video Object Segmentation Using Global and Local Transfer Modules', 'Video object segmentation', 'Interactive segmentation', 'Deep learning', '', '', 'An interactive video object segmentation algorithm, which takes scribble annotations on query objects as input, is proposed in this paper. We develop a deep neural network, which consists of the annotation network (A-Net) and the transfer network (T-Net). First, given user scribbles on a frame, A-Net yields a segmentation result based on the encoder-decoder architecture. Second, T-Net transfers the segmentation result bidirectionally to the other frames, by employing the global and local transfer modules. The global transfer module conveys the segmentation information in an annotated frame to a target frame, while the local transfer module propagates the segmentation information in a temporally adjacent frame to the target frame. By applying A-Net and T-Net alternately, a user can obtain desired segmentation results with minimal efforts. We train the entire network in two stages, by emulating user scribbles and employing an auxiliary loss. Experimental results demonstrate that the proposed interactive video object segmentation algorithm outperforms the state-of-the-art conventional algorithms. Codes and models are available at https://github.com/yuk6heo/IVOS-ATNet.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_18');
INSERT INTO `paper` VALUES (12156, 'InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image', '', '', '', '', '', 'Analysis of hand-hand interactions is a crucial step towards better understanding human behavior. However, most researches in 3D hand pose estimation have focused on the isolated single hand case. Therefore, we firstly propose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image. The proposed InterHand2.6M consists of 2.6 M labeled single and interacting hand frames under various poses from multiple subjects. Our InterNet simultaneously performs 3D single and interacting hand pose estimation. In our experiments, we demonstrate big gains in 3D interacting hand pose estimation accuracy when leveraging the interacting hand data in InterHand2.6M. We also report the accuracy of InterNet on InterHand2.6M, which serves as a strong baseline for this new dataset. Finally, we show 3D interacting hand pose estimation results from general images. Our code and dataset are available (https://mks0601.github.io/InterHand2.6M/).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_33');
INSERT INTO `paper` VALUES (12157, 'Interpretable and Generalizable Person Re-identification with Query-Adaptive Convolution and Temporal Lifting', '', '', '', '', '', 'For person re-identification, existing deep networks often focus on representation learning. However, without transfer learning, the learned model is fixed as is, which is not adaptable for handling various unseen scenarios. In this paper, beyond representation learning, we consider how to formulate person image matching directly in deep feature maps. We treat image matching as finding local correspondences in feature maps, and construct query-adaptive convolution kernels on the fly to achieve local matching. In this way, the matching process and results are interpretable, and this explicit matching is more generalizable than representation features to unseen scenarios, such as unknown misalignments, pose or viewpoint changes. To facilitate end-to-end training of this architecture, we further build a class memory module to cache feature maps of the most recent samples of each class, so as to compute image matching losses for metric learning. Through direct cross-dataset evaluation, the proposed Query-Adaptive Convolution (QAConv) method gains large improvements over popular learning methods (about 10%+ mAP), and achieves comparable results to many transfer learning methods. Besides, a model-free temporal cooccurrence based score weighting method called TLift is proposed, which improves the performance to a further extent, achieving state-of-the-art results in cross-dataset person re-identification. Code is available at https://github.com/ShengcaiLiao/QAConv.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_27');
INSERT INTO `paper` VALUES (12158, 'Interpretable Foreground Object Search as Knowledge Distillation', '', '', '', '', '', 'This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by \\(10.42\\%\\) in absolute difference and \\(24.06\\%\\) in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_12');
INSERT INTO `paper` VALUES (12159, 'Interpretable Neural Network Decoupling', 'Network interpretation', 'Architecture decoupling', '', '', '', 'The remarkable performance of convolutional neural networks (CNNs) is entangled with their huge number of uninterpretable parameters, which has become the bottleneck limiting the exploitation of their full potential. Towards network interpretation, previous endeavors mainly resort to the single filter analysis, which however ignores the relationship between filters. In this paper, we propose a novel architecture decoupling method to interpret the network from a perspective of investigating its calculation paths. More specifically, we introduce a novel architecture controlling module in each layer to encode the network architecture by a vector. By maximizing the mutual information between the vectors and input images, the module is trained to select specific filters to distill a unique calculation path for each input. Furthermore, to improve the interpretability and compactness of the decoupled network, the output of each layer is encoded to align the architecture encoding vector with the constraint of sparsity regularization. Unlike conventional pixel-level or filter-level network interpretation methods, we propose a path-level analysis to explore the relationship between the combination of filter and semantic concepts, which is more suitable to interpret the working rationale of the decoupled network. Extensive experiments show that the decoupled network achieves several applications, i.e., network interpretation, network acceleration, and adversarial samples detection.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_39');
INSERT INTO `paper` VALUES (12160, 'Interpretable Visual Reasoning via Probabilistic Formulation Under Natural Supervision', 'Visual Question Answering', 'Implicit reasoning', 'Temporal Reasoning Network', 'Explanable machine learning', '', 'Visual reasoning is crucial for visual question answering (VQA). However, without labelled programs, implicit reasoning under natural supervision is still quite challenging and previous models are hard to interpret. In this paper, we rethink implicit reasoning process in VQA, and propose a new formulation which maximizes the log-likelihood of joint distribution for the observed question and predicted answer. Accordingly, we derive a Temporal Reasoning Network (TRN) framework which models the implicit reasoning process as sequential planning in latent space. Our model is interpretable on both model design in probabilist and reasoning process via visualization. We experimentally demonstrate that TRN can support implicit reasoning across various datasets. The experimental results of our model are competitive to existing implicit reasoning models and surpass baseline by a large margin on complicated reasoning tasks without extra computation cost in forward stage.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_32');
INSERT INTO `paper` VALUES (12161, 'Intra-class Feature Variation Distillation for Semantic Segmentation', 'Semantic segmentation', 'Knowledge distillation', 'Intra-class feature variation', '', '', 'Current state-of-the-art semantic segmentation methods usually require high computational resources for accurate segmentation. One promising way to achieve a good trade-off between segmentation accuracy and efficiency is knowledge distillation. In this paper, different from previous methods performing knowledge distillation for densely pairwise relations, we propose a novel intra-class feature variation distillation (IFVD) to transfer the intra-class feature variation (IFV) of the cumbersome model (teacher) to the compact model (student). Concretely, we compute the feature center (regarded as the prototype) of each class and characterize the IFV with the set of similarity between the feature on each pixel and its corresponding class-wise prototype. The teacher model usually learns more robust intra-class feature representation than the student model, making them have different IFV. Transferring such IFV from teacher to student could make the student mimic the teacher better in terms of feature distribution, and thus improve the segmentation accuracy. We evaluate the proposed approach on three widely adopted benchmarks: Cityscapes, CamVid and Pascal VOC 2012, consistently improving state-of-the-art methods. The code is available at https://github.com/YukangWang/IFVD.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_21');
INSERT INTO `paper` VALUES (12162, 'Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation', '3D point clouds', '3D reconstruction', 'Deep learning', 'Applications', 'Methodology', 'We present a learning-based method for interpolating and manipulating 3D shapes represented as point clouds, that is explicitly designed to preserve intrinsic shape properties. Our approach is based on constructing a dual encoding space that enables shape synthesis and, at the same time, provides links to the intrinsic shape information, which is typically not available on point cloud data. Our method works in a single pass and avoids expensive optimization, employed by existing techniques. Furthermore, the strong regularization provided by our dual latent space approach also helps to improve shape recovery in challenging settings from noisy point clouds across different datasets. Extensive experiments show that our method results in more realistic and smoother interpolations compared to baselines. Both the code and our pre-trained network can be found online: https://github.com/mrakotosaon/intrinsic_interpolations.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_39');
INSERT INTO `paper` VALUES (12163, 'Invertible Image Rescaling', '', '', '', '', '', 'High-resolution digital images are usually downscaled to fit various display screens or save the cost of storage and bandwidth, meanwhile the post-upscaling is adopted to recover the original resolutions or the details in the zoom-in images. However, typical image downscaling is a non-injective mapping due to the loss of high-frequency information, which leads to the ill-posed problem of the inverse upscaling procedure and poses great challenges for recovering details from the downscaled low-resolution images. Simply upscaling with image super-resolution methods results in unsatisfactory recovering performance. In this work, we propose to solve this problem by modeling the downscaling and upscaling processes from a new perspective, i.e. an invertible bijective transformation, which can largely mitigate the ill-posed nature of image upscaling. We develop an Invertible Rescaling Net (IRN) with deliberately designed framework and objectives to produce visually-pleasing low-resolution images and meanwhile capture the distribution of the lost information using a latent variable following a specified distribution in the downscaling process. In this way, upscaling is made tractable by inversely passing a randomly-drawn latent variable with the low-resolution image through the network. Experimental results demonstrate the significant improvement of our model over existing methods in terms of both quantitative and qualitative evaluations of image upscaling reconstruction from downscaled images. Code is available at https://github.com/pkuxmq/Invertible-Image-Rescaling.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_8');
INSERT INTO `paper` VALUES (12164, 'Invertible Neural BRDF for Object Inverse Rendering', 'Reflectance', 'BRDF', 'Inverse rendering', 'Illumination estimation', '', 'We introduce a novel neural network-based BRDF model and a Bayesian framework for object inverse rendering, i.e., joint estimation of reflectance and natural illumination from a single image of an object of known geometry. The BRDF is expressed with an invertible neural network, namely, normalizing flow, which provides the expressive power of a high-dimensional representation, computational simplicity of a compact analytical model, and physical plausibility of a real-world BRDF. We extract the latent space of real-world reflectance by conditioning this model, which directly results in a strong reflectance prior. We refer to this model as the invertible neural BRDF model (iBRDF). We also devise a deep illumination prior by leveraging the structural bias of deep neural networks. By integrating this novel BRDF model and reflectance and illumination priors in a MAP estimation formulation, we show that this joint estimation can be computed efficiently with stochastic gradient descent. We experimentally validate the accuracy of the invertible neural BRDF model on a large number of measured data and demonstrate its use in object inverse rendering on a number of synthetic and real images. The results show new ways in which deep neural networks can help solve challenging radiometric inverse problems.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_45');
INSERT INTO `paper` VALUES (12165, 'Invertible Zero-Shot Recognition Flows', 'Zero-Shot Learning', 'Generative flows', 'Invertible networks', '', '', 'Deep generative models have been successfully applied to Zero-Shot Learning (ZSL) recently. However, the underlying drawbacks of GANs and VAEs (e.g., the hardness of training with ZSL-oriented regularizers and the limited generation quality) hinder the existing generative ZSL models from fully bypassing the seen-unseen bias. To tackle the above limitations, for the first time, this work incorporates a new family of generative models (i.e., flow-based models) into ZSL. The proposed Invertible Zero-shot Flow (IZF) learns factorized data embeddings (i.e., the semantic factors and the non-semantic ones) with the forward pass of an invertible flow network, while the reverse pass generates data samples. This procedure theoretically extends conventional generative flows to a factorized conditional scheme. To explicitly solve the bias problem, our model enlarges the seen-unseen distributional discrepancy based on a negative sample-based distance measurement. Notably, IZF works flexibly with either a naive Bayesian classifier or a held-out trainable one for zero-shot recognition. Experiments on widely-adopted ZSL benchmarks demonstrate the significant performance gain of IZF over existing methods, in both classic and generalized settings.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_36');
INSERT INTO `paper` VALUES (12166, 'Investigating Bias and Fairness in Facial Expression Recognition', 'Fairness', 'Bias mitigation', 'Facial expression recognition', '', '', 'Recognition of expressions of emotions and affect from facial images is a well-studied research problem in the fields of affective computing and computer vision with a large number of datasets available containing facial images and corresponding expression labels. However, virtually none of these datasets have been acquired with consideration of fair distribution across the human population. Therefore, in this work, we undertake a systematic investigation of bias and fairness in facial expression recognition by comparing three different approaches, namely a baseline, an attribute-aware and a disentangled approach, on two well-known datasets, RAF-DB and CelebA. Our results indicate that: (i) data augmentation improves the accuracy of the baseline model, but this alone is unable to mitigate the bias effect; (ii) both the attribute-aware and the disentangled approaches equipped with data augmentation perform better than the baseline approach in terms of accuracy and fairness; (iii) the disentangled approach is the best for mitigating demographic bias; and (iv) the bias mitigation strategies are more suitable in the existence of uneven attribute distribution or imbalanced number of subgroup data.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_35');
INSERT INTO `paper` VALUES (12167, 'Is Sharing of Egocentric Video Giving Away Your Biometric Signature?', '', '', '', '', '', 'Easy availability of wearable egocentric cameras, and the sense of privacy propagated by the fact that the wearer is never seen in the captured videos, has led to a tremendous rise in public sharing of such videos. Unlike hand-held cameras, egocentric cameras are harnessed on the wearer’s head, which makes it possible to track the wearer’s head motion by observing optical flow in the egocentric videos. In this work, we create a novel kind of privacy attack by extracting the wearer’s gait profile, a well known biometric signature, from such optical flow in the egocentric videos. We demonstrate strong wearer recognition capabilities based on extracted gait features, an unprecedented and critical weakness completely absent in hand-held videos. We demonstrate the following attack scenarios: (1) In a closed-set scenario, we show that it is possible to recognize the wearer of an egocentric video with an accuracy of more than 92.5% on the benchmark video dataset. (2) In an open-set setting, when the system has not seen the camera wearer even once during the training, we show that it is still possible to identify that the two egocentric videos have been captured by the same wearer with an Equal Error Rate (EER) of less than 14.35%. (3) We show that it is possible to extract gait signature even if only sparse optical flow and no other scene information from egocentric video is available. We demonstrate the accuracy of more than 84% for wearer recognition with only global optical flow. (4) While the first person to first person matching does not give us access to the wearer’s face, we show that it is possible to match the extracted gait features against the one obtained from a third person view such as a surveillance camera looking at the wearer in a completely different background at a different time. In essence, our work indicates that sharing one’s egocentric video should be treated as giving away one’s biometric identity and recommend much more oversight before sharing of egocentric videos. The code, trained models, and the datasets and their annotations are available at https://egocentricbiometric.github.io/.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_24');
INSERT INTO `paper` VALUES (12168, 'It Is Not the Journey But the Destination: Endpoint Conditioned Trajectory Prediction', 'Multimodal trajectory prediction', 'Social pooling', '', '', '', 'Human trajectory forecasting with multiple socially interacting agents is of critical importance for autonomous navigation in human environments, e.g., for self-driving cars and social robots. In this work, we present Predicted Endpoint Conditioned Network (PECNet) for flexible human trajectory prediction. PECNet infers distant trajectory endpoints to assist in long-range multi-modal trajectory prediction. A novel non-local social pooling layer enables PECNet to infer diverse yet socially compliant trajectories. Additionally, we present a simple “truncation-trick” for improving diversity and multi-modal trajectory prediction performance. We show that PECNet improves state-of-the-art performance on the Stanford Drone trajectory prediction benchmark by \\({\\sim }20.9\\%\\) and on the ETH/UCY benchmark by \\({\\sim }40.8\\%\\) (Code available at project homepage: https://karttikeya.github.io/publication/htf/).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_45');
INSERT INTO `paper` VALUES (12169, 'Iterative Distance-Aware Similarity Matrix Convolution with Mutual-Supervised Point Elimination for Efficient Point Cloud Registration', 'Point Cloud Registration', '', '', '', '', 'In this paper, we propose a novel learning-based pipeline for partially overlapping 3D point cloud registration. The proposed model includes an iterative distance-aware similarity matrix convolution module to incorporate information from both the feature and Euclidean space into the pairwise point matching process. These convolution layers learn to match points based on joint information of the entire geometric features and Euclidean offset for each point pair, overcoming the disadvantage of matching by simply taking the inner product of feature vectors. Furthermore, a two-stage learnable point elimination technique is presented to improve computational efficiency and reduce false positive correspondence pairs. A novel mutual-supervision loss is proposed to train the model without extra annotations of keypoints. The pipeline can be easily integrated with both traditional (e.g. FPFH) and learning-based features. Experiments on partially overlapping and noisy point cloud registration show that our method outperforms the current state-of-the-art, while being more computationally efficient.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_23');
INSERT INTO `paper` VALUES (12170, 'Iterative Feature Transformation for Fast and Versatile Universal Style Transfer', '', '', '', '', '', 'The general framework for fast universal style transfer consists of an autoencoder and a feature transformation at the bottleneck. We propose a new transformation that iteratively stylizes features with analytical gradient descent (Implementation is open-sourced at https://github.com/chiutaiyin/Iterative-feature-transformation-for-style-transfer). Experiments show this transformation is advantageous in part because it is fast. With control knobs to balance content preservation and style effect transferal, we also show this method can switch between artistic and photo-realistic style transfers and reduce distortion and artifacts. Finally, we show it can be used for applications requiring spatial control and multiple-style transfer.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_11');
INSERT INTO `paper` VALUES (12171, 'Jacks of All Trades, Masters of None: Addressing Distributional Shift and Obtrusiveness via Transparent Patch Attacks', '', '', '', '', '', 'We focus on the development of effective adversarial patch attacks and – for the first time – jointly address the antagonistic objectives of attack success and obtrusiveness via the design of novel semi-transparent patches. This work is motivated by our pursuit of a systematic performance analysis of patch attack robustness with regard to geometric transformations. Specifically, we first elucidate a) key factors underpinning patch attack success and b) the impact of distributional shift between training and testing/deployment when cast under the Expectation over Transformation (EoT) formalism. By focusing our analysis on three principal classes of transformations (rotation, scale, and location), our findings provide quantifiable insights into the design of effective patch attacks and demonstrate that scale, among all factors, significantly impacts patch attack success. Working from these findings, we then focus on addressing how to overcome the principal limitations of scale for the deployment of attacks in real physical settings: namely the obtrusiveness of large patches. Our strategy is to turn to the novel design of irregularly-shaped, semi-transparent partial patches which we construct via a new optimization process that jointly addresses the antagonistic goals of mitigating obtrusiveness and maximizing effectiveness. Our study – we hope – will help encourage more focus in the community on the issues of obtrusiveness, scale, and success in patch attacks.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_7');
INSERT INTO `paper` VALUES (12172, 'JGR-P2O: Joint Graph Reasoning Based Pixel-to-Offset Prediction Network for 3D Hand Pose Estimation from a Single Depth Image', '3D hand pose estimation', 'Depth image', 'Graph neural network', '', '', 'State-of-the-art single depth image-based 3D hand pose estimation methods are based on dense predictions, including voxel-to-voxel predictions, point-to-point regression, and pixel-wise estimations. Despite the good performance, those methods have a few issues in nature, such as the poor trade-off between accuracy and efficiency, and plain feature representation learning with local convolutions. In this paper, a novel pixel-wise prediction-based method is proposed to address the above issues. The key ideas are two-fold: (a) explicitly modeling the dependencies among joints and the relations between the pixels and the joints for better local feature representation learning; (b) unifying the dense pixel-wise offset predictions and direct joint regression for end-to-end training. Specifically, we first propose a graph convolutional network (GCN) based joint graph reasoning module to model the complex dependencies among joints and augment the representation capability of each pixel. Then we densely estimate all pixels’ offsets to joints in both image plane and depth space and calculate the joints’ positions by a weighted average over all pixels’ predictions, totally discarding the complex post-processing operations. The proposed model is implemented with an efficient 2D fully convolutional network (FCN) backbone and has only about 1.4M parameters. Extensive experiments on multiple 3D hand pose estimation benchmarks demonstrate that the proposed method achieves new state-of-the-art accuracy while running very efficiently with around a speed of 110 fps on a single NVIDIA 1080Ti GPU (This work was supported in part by the National Natural Science Foundation of China under Grants 61976095, in part by the Science and Technology Planning Project of Guangdong Province under Grant 2018B030323026. This work was also partially supported by the Academy of Finland.). The code is available at https://github.com/fanglinpu/JGR-P2O.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_8');
INSERT INTO `paper` VALUES (12173, 'JNR: Joint-Based Neural Rig Representation for Compact 3D Face Modeling', 'Face modeling', '3D face reconstruction', 'GANs', '', '', 'In this paper, we introduce a novel approach to learn a 3D face model using a joint-based face rig and a neural skinning network. Thanks to the joint-based representation, our model enjoys some significant advantages over prior blendshape-based models. First, it is very compact such that we are orders of magnitude smaller while still keeping strong modeling capacity. Second, because each joint has its semantic meaning, interactive facial geometry editing is made easier and more intuitive. Third, through skinning, our model supports adding mouth interior and eyes, as well as accessories (hair, eye glasses, etc.) in a simpler, more accurate and principled way. We argue that because the human face is highly structured and topologically consistent, it does not need to be learned entirely from data. Instead we can leverage prior knowledge in the form of a human-designed 3D face rig to reduce the data dependency, and learn a compact yet strong face model from only a small dataset (less than one hundred 3D scans). To further improve the modeling capacity, we train a skinning weight generator through adversarial learning. Experiments on fitting high-quality 3D scans (both neutral and expressive), noisy depth images, and RGB images demonstrate that its modeling capacity is on-par with state-of-the-art face models, such as FLAME and Facewarehouse, even though the model is 10 to 20 times smaller. This suggests broad value in both graphics and vision applications on mobile and edge devices.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_23');
INSERT INTO `paper` VALUES (12174, 'Joint 3D Layout and Depth Prediction from a Single Indoor Panorama Image', 'Indoor panorama image', 'Layout prediction', 'Depth estimation', 'Layout depth map', '', 'In this paper, we propose a method which jointly learns the layout prediction and depth estimation from a single indoor panorama image. Previous methods have considered layout prediction and depth estimation from a single panorama image separately. However, these two tasks are tightly intertwined. Leveraging the layout depth map as an intermediate representation, our proposed method outperforms existing methods for both panorama layout prediction and depth estimation. Experiments on the challenging real-world dataset of Stanford 2D–3D demonstrate that our approach obtains superior performance for both the layout prediction tasks (3D IoU: \\(85.81\\%\\) v.s. \\(79.79\\%\\)) and the depth estimation (Abs Rel: 0.068 v.s. 0.079).', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_39');
INSERT INTO `paper` VALUES (12175, 'Joint Bilateral Learning for Real-Time Universal Photorealistic Style Transfer', 'Style transfer', 'Bilateral learning', 'Local affine transform', '', '', 'Photorealistic style transfer is the task of transferring the artistic style of an image onto a content target, producing a result that is plausibly taken with a camera. Recent approaches, based on deep neural networks, produce impressive results but are either too slow to run at practical resolutions, or still contain objectionable artifacts. We propose a new end-to-end model for photorealistic style transfer that is both fast and inherently generates photorealistic results. The core of our approach is a feed-forward neural network that learns local edge-aware affine transforms that automatically obey the photorealism constraint. When trained on a diverse set of images and a variety of styles, our model can robustly apply style transfer to an arbitrary pair of input images. Compared to the state of the art, our method produces visually superior results and is three orders of magnitude faster, enabling real-time performance at 4K on a mobile phone. We validate our method with ablation and user studies.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_20');
INSERT INTO `paper` VALUES (12176, 'Joint Demosaicking and Denoising for CFA and MSFA Images Using a Mosaic-Adaptive Dense Residual Network', 'Image demosaicking and denoising', 'Color filter array', 'Multispectral filter array', 'Dense residual network', '', 'Color filter array (CFA) has been a basis for modern photography and recently multispectral filter array (MSFA) has gradually found its wide application. A deep learning network capable of joint demosaicking and denoising for both CFA and MSFA raw images is proposed in this paper. First, a novel dense residual network that includes multiple types of skip connections is introduced to learn features at different resolutions. Then, mosaic adaptive convolution and data augmentation based on mosaic shifting are put forward to fully make use of common characteristics of CFA and MSFA mosaic images. Moreover, an L1 loss function normalized by noise standard deviation is suggested to train the deep residual network so it does not rely on an explicit input of known or estimated noise standard deviation. Extensive experiments using simulated and real mosaic images from CFA cameras demonstrate that the proposed mosaic-adaptive dense residual network (MDRN) outperforms other state-of-the-art deep learning algorithms significantly. For simulated MSFA mosaics and real MSFA raw images, it also shows much improved results compared to other methods.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_39');
INSERT INTO `paper` VALUES (12177, 'Joint Disentangling and Adaptation for Cross-Domain Person Re-Identification', 'Person re-id', 'Feature disentangling', 'Domain adaptation', '', '', 'Although a significant progress has been witnessed in supervised person re-identification (re-id), it remains challenging to generalize re-id models to new domains due to the huge domain gaps. Recently, there has been a growing interest in using unsupervised domain adaptation to address this scalability issue. Existing methods typically conduct adaptation on the representation space that contains both id-related and id-unrelated factors, thus inevitably undermining the adaptation efficacy of id-related features. In this paper, we seek to improve adaptation by purifying the representation space to be adapted. To this end, we propose a joint learning framework that disentangles id-related/unrelated features and enforces adaptation to work on the id-related feature space exclusively. Our model involves a disentangling module that encodes cross-domain images into a shared appearance space and two separate structure spaces, and an adaptation module that performs adversarial alignment and self-training on the shared appearance space. The two modules are co-designed to be mutually beneficial. Extensive experiments demonstrate that the proposed joint learning framework outperforms the state-of-the-art methods by clear margins.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_6');
INSERT INTO `paper` VALUES (12178, 'Joint Learning of Social Groups, Individuals Action and Sub-group Activities in Videos', 'Collective behaviour recognition', 'Social grouping', 'Video understanding', '', '', 'The state-of-the art solutions for human activity understanding from a video stream formulate the task as a spatio-temporal problem which requires joint localization of all individuals in the scene and classification of their actions or group activity over time. Who is interacting with whom, e.g. not everyone in a queue is interacting with each other, is often not predicted. There are scenarios where people are best to be split into sub-groups, which we call social groups, and each social group may be engaged in a different social activity. In this paper, we solve the problem of simultaneously grouping people by their social interactions, predicting their individual actions and the social activity of each social group, which we call the social task. Our main contributions are: i) we propose an end-to-end trainable framework for the social task; ii) our proposed method also sets the state-of-the-art results on two widely adopted benchmarks for the traditional group activity recognition task (assuming individuals of the scene form a single group and predicting a single group activity label for the scene); iii) we introduce new annotations on an existing group activity dataset, re-purposing it for the social task. The data and code for our method is publicly available (https://github.com/mahsaep/Social-human-activity-understanding-and-grouping).', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_11');
INSERT INTO `paper` VALUES (12179, 'Joint Optimization for Multi-person Shape Models from Markerless 3D-Scans', 'Body shape', 'Skinning', 'Subdivision surfaces', 'Blendshapes', '', 'We propose a markerless end-to-end training framework for parametric 3D human shape models. The training of statistical 3D human shape models with minimal supervision is an important problem in computer vision. Contrary to prior work, the whole training process (i) uses a differentiable shape model surface and (ii) is trained end-to-end by jointly optimizing all parameters of a single, self-contained objective that can be solved with slightly modified off-the-shelf non-linear least squares solvers. The training process only requires a compact model definition and an off-the-shelf 2D RGB pose estimator. No pre-trained shape models are required. For training (iii) a medium-sized dataset of approximately 1000 low-resolution human body scans is sufficient to achieve competitive performance on the challenging FAUST surface correspondence benchmark. The training and evaluation code will be made available for research purposes to facilitate end-to-end shape model training on novel datasets with minimal setup cost.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_3');
INSERT INTO `paper` VALUES (12180, 'Joint Visual and Temporal Consistency for Unsupervised Domain Adaptive Person Re-identification', 'Domain adaption', 'Person re-identification', 'Convolution neural networks', '', '', 'Unsupervised domain adaptive person Re-IDentification (ReID) is challenging because of the large domain gap between source and target domains, as well as the lackage of labeled data on the target domain. This paper tackles this challenge through jointly enforcing visual and temporal consistency in the combination of a local one-hot classification and a global multi-class classification. The local one-hot classification assigns images in a training batch with different person IDs, then adopts a Self-Adaptive Classification (SAC) model to classify them. The global multi-class classification is achieved by predicting labels on the entire unlabeled training set with the Memory-based Temporal-guided Cluster (MTC). MTC predicts multi-class labels by considering both visual similarity and temporal consistency to ensure the quality of label prediction. The two classification models are combined in a unified framework, which effectively leverages the unlabeled data for discriminative feature learning. Experimental results on three large-scale ReID datasets demonstrate the superiority of proposed method in both unsupervised and unsupervised domain adaptive ReID tasks. For example, under unsupervised setting, our method outperforms recent unsupervised domain adaptive methods, which leverage more labels for training.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_29');
INSERT INTO `paper` VALUES (12181, 'Jointly De-Biasing Face Recognition and Demographic Attribute Estimation', 'Bias', 'Feature disentanglement', 'Face recognition', 'Fairness', '', 'We address the problem of bias in automated face recognition and demographic attribute estimation algorithms, where errors are lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network (DebFace) that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving state-of-the-art performance.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_20');
INSERT INTO `paper` VALUES (12182, 'Jointly Learning Visual Motion and Confidence from Local Patches in Event Cameras', '', '', '', '', '', 'We propose the first network to jointly learn visual motion and confidence from events in spatially local patches. Event-based sensors deliver high temporal resolution motion information in a sparse, non-redundant format. This creates the potential for low computation, low latency motion recognition. Neural networks which extract global motion information, however, are generally computationally expensive. Here, we introduce a novel shallow and compact neural architecture and learning approach to capture reliable visual motion information along with the corresponding confidence of inference. Our network makes a prediction of the visual motion at each spatial location using only local events. Our confidence network then identifies which of these predictions will be accurate. In the task of recovering pan-tilt ego velocities from events, we show that each individual confident local prediction of our network can be expected to be as accurate as state of the art optimization approaches which utilize the full image. Furthermore, on a publicly available dataset, we find our local predictions generalize to scenes with camera motions and the presence of independently moving objects. This makes the output of our network well suited for motion based tasks, such as the segmentation of independently moving objects. We demonstrate on a publicly available motion segmentation dataset that restricting predictions to confident regions is sufficient to achieve results that exceed state of the art methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_30');
INSERT INTO `paper` VALUES (12183, 'Journey Towards Tiny Perceptual Super-Resolution', '', '', '', '', '', 'Recent works in single-image perceptual super-resolution (SR) have demonstrated unprecedented performance in generating realistic textures by means of deep convolutional networks. However, these convolutional models are excessively large and expensive, hindering their effective deployment to end devices. In this work, we propose a neural architecture search (NAS) approach that integrates NAS and generative adversarial networks (GANs) with recent advances in perceptual SR and pushes the efficiency of small perceptual SR models to facilitate on-device execution. Specifically, we search over the architectures of both the generator and the discriminator sequentially, highlighting the unique challenges and key observations of searching for an SR-optimized discriminator and comparing them with existing discriminator architectures in the literature. Our tiny perceptual SR (TPSR) models outperform SRGAN and EnhanceNet on both full-reference perceptual metric (LPIPS) and distortion metric (PSNR) while being up to 26.4\\(\\times \\) more memory efficient and 33.6\\(\\times \\) more compute efficient respectively.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_6');
INSERT INTO `paper` VALUES (12184, 'JSENet: Joint Semantic Segmentation and Edge Detection Network for 3D Point Clouds', 'Semantic segmentation', 'Semantic edge detection', '3D point clouds', '3D scene understanding', '', 'Semantic segmentation and semantic edge detection can be seen as two dual problems with close relationships in computer vision. Despite the fast evolution of learning-based 3D semantic segmentation methods, little attention has been drawn to the learning of 3D semantic edge detectors, even less to a joint learning method for the two tasks. In this paper, we tackle the 3D semantic edge detection task for the first time and present a new two-stream fully-convolutional network that jointly performs the two tasks. In particular, we design a joint refinement module that explicitly wires region information and edge information to improve the performances of both tasks. Further, we propose a novel loss function that encourages the network to produce semantic segmentation results with better boundaries. Extensive evaluations on S3DIS and ScanNet datasets show that our method achieves on par or better performance than the state-of-the-art methods for semantic segmentation and outperforms the baseline methods for semantic edge detection. Code release: https://github.com/hzykent/JSENet .', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_14');
INSERT INTO `paper` VALUES (12185, 'JSSR: A Joint Synthesis, Segmentation, and Registration System for 3D Multi-modal Image Alignment of Large-Scale Pathological CT Scans', '', '', '', '', '', 'Multi-modal image registration is a challenging problem that is also an important clinical task for many real applications and scenarios. As a first step in analysis, deformable registration among different image modalities is often required in order to provide complementary visual information. During registration, semantic information is key to match homologous points and pixels. Nevertheless, many conventional registration methods are incapable in capturing high-level semantic anatomical dense correspondences. In this work, we propose a novel multi-task learning system, JSSR, based on an end-to-end 3D convolutional neural network that is composed of a generator, a registration and a segmentation component. The system is optimized to satisfy the implicit constraints between different tasks in an unsupervised manner. It first synthesizes the source domain images into the target domain, then an intra-modal registration is applied on the synthesized images and target images. The segmentation module are then applied on the synthesized and target images, providing additional cues based on semantic correspondences. The supervision from another fully-annotated dataset is used to regularize the segmentation. We extensively evaluate JSSR on a large-scale medical image dataset containing 1,485 patient CT imaging studies of four different contrast phases (i.e., 5,940 3D CT scans with pathological livers) on the registration, segmentation and synthesis tasks. The performance is improved after joint training on the registration and segmentation tasks by \\(0.9\\%\\) and \\(1.9\\%\\) respectively compared to a highly competitive and accurate deep learning baseline. The registration also consistently outperforms conventional state-of-the-art multi-modal registration methods.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_16');
INSERT INTO `paper` VALUES (12186, 'JSTASR: Joint Size and Transparency-Aware Snow Removal Algorithm Based on Modified Partial Convolution and Veiling Effect Removal', 'Transparency and size-aware snow removal', 'Snow detection', 'Differentiable dark channel prior', '', '', 'Snow removal usually affects the performance of computer vision. Comparing with other atmospheric phenomenon (e.g., haze and rain), snow is more complicated due to its transparency, various size, and accumulation of veiling effect, which make single image de-snowing more challenging. In this paper, first, we reformulate the snow model. Different from that in the previous works, in the proposed snow model, the veiling effect is included. Second, a novel joint size and transparency-aware snow removal algorithm called JSTASR is proposed. It can classify snow particles according to their sizes and conduct snow removal in different scales. Moreover, to remove the snow with different transparency, the transparency-aware snow removal is developed. It can address both transparent and non-transparent snow particles by applying the modified partial convolution. Experiments show that the proposed method achieves significant improvement on both synthetic and real-world datasets and is very helpful for object detection on snow images.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_45');
INSERT INTO `paper` VALUES (12187, 'Kernelized Memory Network for Video Object Segmentation', 'Video object segmentation', 'Memory network', 'Gaussian kernel', 'Hide-and-Seek', '', 'Semi-supervised video object segmentation (VOS) is a task that involves predicting a target object in a video when the ground truth segmentation mask of the target object is given in the first frame. Recently, space-time memory networks (STM) have received significant attention as a promising solution for semi-supervised VOS. However, an important point is overlooked when applying STM to VOS. The solution (STM) is non-local, but the problem (VOS) is predominantly local. To solve the mismatch between STM and VOS, we propose a kernelized memory network (KMN). Before being trained on real videos, our KMN is pre-trained on static images, as in previous works. Unlike in previous works, we use the Hide-and-Seek strategy in pre-training to obtain the best possible results in handling occlusions and segment boundary extraction. The proposed KMN surpasses the state-of-the-art on standard benchmarks by a significant margin (+5% on DAVIS 2017 test-dev set). In addition, the runtime of KMN is 0.12 s per frame on the DAVIS 2016 validation set, and the KMN rarely requires extra computation, when compared with STM.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_38');
INSERT INTO `paper` VALUES (12188, 'Key Frame Proposal Network for Efficient Pose Estimation in Videos', 'Fast human pose estimation in videos', 'Key frame proposal network (K-FPN)', 'Unsupervised learning', '', '', 'Human pose estimation in video relies on local information by either estimating each frame independently or tracking poses across frames. In this paper, we propose a novel method combining local approaches with global context. We introduce a light weighted, unsupervised, key frame proposal network (K-FPN) to select informative frames and a learned dictionary to recover the entire pose sequence from these frames. The K-FPN speeds up the pose estimation and provides robustness to bad frames with occlusion, motion blur, and illumination changes, while the learned dictionary provides global dynamic context. Experiments on Penn Action and sub-JHMDB datasets show that the proposed method achieves state-of-the-art accuracy, with substantial speed-up.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_36');
INSERT INTO `paper` VALUES (12189, 'Kinematic 3D Object Detection in Monocular Video', '3D object detection', 'Monocular', 'Video', 'Physics-based', '', 'Perceiving the physical world in 3D is fundamental for self-driving applications. Although temporal motion is an invaluable resource to human vision for detection, tracking, and depth perception, such features have not been thoroughly utilized in modern 3D object detectors. In this work, we propose a novel method for monocular video-based 3D object detection which leverages kinematic motion to extract scene dynamics and improve localization accuracy. We first propose a novel decomposition of object orientation and a self-balancing 3D confidence. We show that both components are critical to enable our kinematic model to work effectively. Collectively, using only a single model, we efficiently leverage 3D kinematics from monocular videos to improve the overall localization precision in 3D object detection while also producing useful by-products of scene dynamics (ego-motion and per-object velocity). We achieve state-of-the-art performance on monocular 3D object detection and the Bird’s Eye View tasks within the KITTI self-driving dataset.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_9');
INSERT INTO `paper` VALUES (12190, 'Kinship Identification Through Joint Learning Using Kinship Verification Ensembles', 'Kinship identification', 'Kinship verification ensemble', 'Joint learning', '', '', 'Kinship verification is a well-explored task: identifying whether or not two persons are kin. In contrast, kinship identification has been largely ignored so far. Kinship identification aims to further identify the particular type of kinship. An extension to kinship verification run short to properly obtain identification, because existing verification networks are individually trained on specific kinships and do not consider the context between different kinship types. Also, existing kinship verification datasets have biased positive-negative distributions which are different than real-world distributions.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_37');
INSERT INTO `paper` VALUES (12191, 'Know Your Surroundings: Exploiting Scene Information for Object Tracking', '', '', '', '', '', 'Current state-of-the-art trackers rely only on a target appearance model in order to localize the object in each frame. Such approaches are however prone to fail in case of e.g. fast appearance changes or presence of distractor objects, where a target appearance model alone is insufficient for robust tracking. Having the knowledge about the presence and locations of other objects in the surrounding scene can be highly beneficial in such cases. This scene information can be propagated through the sequence and used to, for instance, explicitly avoid distractor objects and eliminate target candidate regions.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_13');
INSERT INTO `paper` VALUES (12192, 'Knowledge Distillation for Multi-task Learning', '', '', '', '', '', 'Multi-task learning (MTL) is to learn one single model that performs multiple tasks for achieving good performance on all tasks and lower cost on computation. Learning such a model requires to jointly optimize losses of a set of tasks with different difficulty levels, magnitudes, and characteristics (e.g. cross-entropy, Euclidean loss), leading to the imbalance problem in multi-task learning. To address the imbalance problem, we propose a knowledge distillation based method in this work. We first learn a task-specific model for each task. We then learn the multi-task model for minimizing task-specific loss and for producing the same feature with task-specific models. As the task-specific network encodes different features, we introduce small task-specific adaptors to project multi-task features to the task-specific features. In this way, the adaptors align the task-specific feature and the multi-task feature, which enables a balanced parameter sharing across tasks. Extensive experimental results demonstrate that our method can optimize a multi-task learning model in a more balanced way and achieve better overall performance.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_13');
INSERT INTO `paper` VALUES (12193, 'Knowledge Distillation Meets Self-supervision', '', '', '', '', '', 'Knowledge distillation, which involves extracting the “dark knowledge” from a teacher network to guide the learning of a student network, has emerged as an important technique for model compression and transfer learning. Unlike previous works that exploit architecture-specific cues such as activation and attention for distillation, here we wish to explore a more general and model-agnostic approach for extracting “richer dark knowledge” from the pre-trained teacher model. We show that the seemingly different self-supervision task can serve as a simple yet powerful solution. For example, when performing contrastive learning between transformed entities, the noisy predictions of the teacher network reflect its intrinsic composition of semantic and pose information. By exploiting the similarity between those self-supervision signals as an auxiliary task, one can effectively transfer the hidden information from the teacher to the student. In this paper, we discuss practical ways to exploit those noisy self-supervision signals with selective transfer for distillation. We further show that self-supervision signals improve conventional distillation with substantial gains under few-shot and noisy-label scenarios. Given the richer knowledge mined from self-supervision, our knowledge distillation approach achieves state-of-the-art performance on standard benchmarks, i.e., CIFAR100 and ImageNet, under both similar-architecture and cross-architecture settings. The advantage is even more pronounced under the cross-architecture setting, where our method outperforms the state of the art by an average of 2.3% in accuracy rate on CIFAR100 across six different teacher-student pairs. The code and models are available at: https://github.com/xuguodong03/SSKD.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_34');
INSERT INTO `paper` VALUES (12194, 'Knowledge Transfer via Dense Cross-Layer Mutual-Distillation', 'Knowledge Distillation', 'Deep supervision', 'Convolutional Neural Network', 'Image classification', '', 'Knowledge Distillation (KD) based methods adopt the one-way Knowledge Transfer (KT) scheme in which training a lower-capacity student network is guided by a pre-trained high-capacity teacher network. Recently, Deep Mutual Learning (DML) presented a two-way KT strategy, showing that the student network can be also helpful to improve the teacher network. In this paper, we propose Dense Cross-layer Mutual-distillation (DCM), an improved two-way KT method in which the teacher and student networks are trained collaboratively from scratch. To augment knowledge representation learning, well-designed auxiliary classifiers are added to certain hidden layers of both teacher and student networks. To boost KT performance, we introduce dense bidirectional KD operations between the layers appended with classifiers. After training, all auxiliary classifiers are discarded, and thus there are no extra parameters introduced to final models. We test our method on a variety of KT tasks, showing its superiorities over related methods. Code is available at https://github.com/sundw2014/DCM.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_18');
INSERT INTO `paper` VALUES (12195, 'Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions', 'Video question answering', 'Video description', 'Knowledge bases', '', '', 'To understand movies, humans constantly reason over the dialogues and actions shown in specific scenes and relate them to the overall storyline already seen. Inspired by this behaviour, we design ROLL, a model for knowledge-based video story question answering that leverages three crucial aspects of movie understanding: dialog comprehension, scene reasoning, and storyline recalling. In ROLL, each of these tasks is in charge of extracting rich and diverse information by 1) processing scene dialogues, 2) generating unsupervised video scene descriptions, and 3) obtaining external knowledge in a weakly supervised fashion. To answer a given question correctly, the information generated by each inspired-cognitive task is encoded via Transformers and fused through a modality weighting mechanism, which balances the information from the different sources. Exhaustive evaluation demonstrates the effectiveness of our approach, which yields a new state-of-the-art on two challenging video question answering datasets: KnowIT VQA and TVQA+.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_34');
INSERT INTO `paper` VALUES (12196, 'L2-Constrained RemNet for Camera Model Identification and Image Manipulation Detection', 'Image forensics', 'Camera model identification', 'Image manipulation detection', 'Convolutional Neural Networks', '', 'Source camera model identification (CMI) and image manipulation detection are of paramount importance in image forensics. In this paper, we propose an L2-constrained Remnant Convolutional Neural Network (L2-constrained RemNet) for performing these two crucial tasks. The proposed network architecture consists of a dynamic preprocessor block and a classification block. An L2 loss is applied to the output of the preprocessor block, and categorical crossentropy loss is calculated based on the output of the classification block. The whole network is trained in an end-to-end manner by minimizing the total loss, which is a combination of the L2 loss and the categorical crossentropy loss. Aided by the L2 loss, the data-adaptive preprocessor learns to suppress the unnecessary image contents and assists the classification block in extracting robust image forensics features. We train and test the network on the Dresden database and achieve an overall accuracy of 98.15%, where all the test images are from devices and scenes not used during training to replicate practical applications. The network also outperforms other state-of-the-art CNNs even when the images are manipulated. Furthermore, we attain an overall accuracy of 99.68% in image manipulation detection, which implies that it can be used as a general-purpose network for image forensic tasks.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_16');
INSERT INTO `paper` VALUES (12197, 'Label Propagation with Augmented Anchors: A Simple Semi-supervised Learning Baseline for Unsupervised Domain Adaptation', 'Domain adaptation', 'Semi-supervised learning', 'Label propagation', '', '', 'Motivated by the problem relatedness between unsupervised domain adaptation (UDA) and semi-supervised learning (SSL), many state-of-the-art UDA methods adopt SSL principles (e.g., the cluster assumption) as their learning ingredients. However, they tend to overlook the very domain-shift nature of UDA. In this work, we take a step further to study the proper extensions of SSL techniques for UDA. Taking the algorithm of label propagation (LP) as an example, we analyze the challenges of adopting LP to UDA and theoretically analyze the conditions of affinity graph/matrix construction in order to achieve better propagation of true labels to unlabeled instances. Our analysis suggests a new algorithm of Label Propagation with Augmented Anchors (A\\(^2\\)LP), which could potentially improve LP via generation of unlabeled virtual instances (i.e., the augmented anchors) with high-confidence label predictions. To make the proposed A\\(^2\\)LP useful for UDA, we propose empirical schemes to generate such virtual instances. The proposed schemes also tackle the domain-shift challenge of UDA by alternating between pseudo labeling via A\\(^2\\)LP and domain-invariant feature learning. Experiments show that such a simple SSL extension improves over representative UDA methods of domain-invariant feature learning, and could empower two state-of-the-art methods on benchmark UDA datasets. Our results show the value of further investigation on SSL techniques for UDA problems.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_45');
INSERT INTO `paper` VALUES (12198, 'Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation', 'Image-to-image translation', 'Image reconstruction', 'Domain adaptation', 'Semantic segmentation', '', 'Unsupervised domain adaptation enables to alleviate the need for pixel-wise annotation in the semantic segmentation. One of the most common strategies is to translate images from the source domain to the target domain and then align their marginal distributions in the feature space using adversarial learning. However, source-to-target translation enlarges the bias in translated images and introduces extra computations, owing to the dominant data size of the source domain. Furthermore, consistency of the joint distribution in source and target domains cannot be guaranteed through global feature alignment. Here, we present an innovative framework, designed to mitigate the image translation bias and align cross-domain features with the same category. This is achieved by 1) performing the target-to-source translation and 2) reconstructing both source and target images from their predicted labels. Extensive experiments on adapting from synthetic to real urban scene understanding demonstrate that our framework competes favorably against existing state-of-the-art methods.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_29');
INSERT INTO `paper` VALUES (12199, 'Label-Efficient Learning on Point Clouds Using Approximate Convex Decompositions', '', '', '', '', '', 'The problems of shape classification and part segmentation from 3D point clouds have garnered increasing attention in the last few years. Both of these problems, however, suffer from relatively small training sets, creating the need for statistically efficient methods to learn 3D shape representations. In this paper, we investigate the use of Approximate Convex Decompositions (ACD) as a self-supervisory signal for label-efficient learning of point cloud representations. We show that using ACD to approximate ground truth segmentation provides excellent self-supervision for learning 3D point cloud representations that are highly effective on downstream tasks. We report improvements over the state-of-the-art for unsupervised representation learning on the ModelNet40 shape classification dataset and significant gains in few-shot part segmentation on the ShapeNetPart dataset. Our source code is publicly available (https://github.com/matheusgadelha/PointCloudLearningACD).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_28');
INSERT INTO `paper` VALUES (12200, 'Label-Similarity Curriculum Learning', 'Curriculum learning', 'Deep learning', 'Multi-modal learning', 'Classification', '', 'Curriculum learning can improve neural network training by guiding the optimization to desirable optima. We propose a novel curriculum learning approach for image classification that adapts the loss function by changing the label representation.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_11');
INSERT INTO `paper` VALUES (12201, 'LabelEnc: A New Intermediate Supervision Method for Object Detection', 'Object detection', 'Auxiliary supervision', 'AutoEncoder', '', '', 'In this paper we propose a new intermediate supervision method, named LabelEnc, to boost the training of object detection systems. The key idea is to introduce a novel label encoding function, mapping the ground-truth labels into latent embedding, acting as an auxiliary intermediate supervision to the detection backbone during training. Our approach mainly involves a two-step training procedure. First, we optimize the label encoding function via an AutoEncoder defined in the label space, approximating the “desired” intermediate representations for the target object detector. Second, taking advantage of the learned label encoding function, we introduce a new auxiliary loss attached to the detection backbones, thus benefiting the performance of the derived detector. Experiments show our method improves a variety of detection systems by around 2% on COCO dataset, no matter one-stage or two-stage frameworks. Moreover, the auxiliary structures only exist during training, i.e. it is completely cost-free in inference time.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_32');
INSERT INTO `paper` VALUES (12202, 'Ladybird: Quasi-Monte Carlo Sampling for Deep Implicit Field Based 3D Reconstruction with Symmetry', '3D reconstruction', 'Deep learning', 'Sampling', 'Symmetry', '', 'Deep implicit field regression methods are effective for 3D reconstruction from single-view images. However, the impact of different sampling patterns on the reconstruction quality is not well-understood. In this work, we first study the effect of point set discrepancy on the network training. Based on Farthest Point Sampling algorithm, we propose a sampling scheme that theoretically encourages better generalization performance, and results in fast convergence for SGD-based optimization algorithms. Secondly, based on the reflective symmetry of an object, we propose a feature fusion method that alleviates issues due to self-occlusions which makes it difficult to utilize local image features. Our proposed system Ladybird is able to create high quality 3D object reconstructions from a single input image. We evaluate Ladybird on a large scale 3D dataset (ShapeNet) demonstrating highly competitive results in terms of Chamfer distance, Earth Mover’s distance and Intersection Over Union (IoU).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_15');
INSERT INTO `paper` VALUES (12203, 'LandscapeAR: Large Scale Outdoor Augmented Reality by Matching Photographs with Terrain Models Using Learned Descriptors', '', '', '', '', '', 'We introduce a solution to large scale Augmented Reality for outdoor scenes by registering camera images to textured Digital Elevation Models (DEMs). To accommodate the inherent differences in appearance between real images and DEMs, we train a cross-domain feature descriptor using Structure From Motion (SFM) guided reconstructions to acquire training data. Our method runs efficiently on a mobile device and outperforms existing learned and hand-designed feature descriptors for this task.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_18');
INSERT INTO `paper` VALUES (12204, 'Large Batch Optimization for Object Detection: Training COCO in 12 minutes', 'Object detection', 'Large batch optimization', 'Periodical moments decay', '', '', 'Most of existing object detectors usually adopt a small training batch size (e.g. 16), which severely hinders the whole community from exploring large-scale datasets due to the extremely long training procedure. In this paper, we propose a versatile large batch optimization framework for object detection, named LargeDet, which successfully scales the batch size to larger than 1K for the first time. Specifically, we present a novel Periodical Moments Decay LAMB (PMD-LAMB) algorithm to effectively reduce the negative effects of the lagging historical gradients. Additionally, the Synchronized Batch Normalization (SyncBN) is utilized to help fast convergence. With LargeDet, we can not only prominently shorten the training period, but also significantly improve the detection accuracy of sparsely annotated large-scale datasets. For instance, we can finish the training of ResNet50 FPN detector on COCO within 12 min. Moreover, we achieve 12.2% mAP@0.5 absolute improvement for ResNet50 FPN on Open Images by training with batch size 640.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_29');
INSERT INTO `paper` VALUES (12205, 'Large Scale Holistic Video Understanding', '', '', '', '', '', 'Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_35');
INSERT INTO `paper` VALUES (12206, 'Large-Scale Few-Shot Learning via Multi-modal Knowledge Discovery', 'Large-scale few-shot learning', 'Multi-modal knowledge discovery', '', '', '', 'Large-scale few-shot learning aims at identifying hundreds of novel object categories where each category has only a few samples. It is a challenging problem since (1) the identifying process is susceptible to over-fitting with limited samples of an object, and (2) the sample imbalance between a base (known knowledge) category and a novel category is easy to bias the recognition results. To solve these problems, we propose a method based on multi-modal knowledge discovery. First, we use the visual knowledge to help the feature extractors focus on different visual parts. Second, we design a classifier to learn the distribution over all categories. In the second stage, we develop three schemes to minimize the prediction error and balance the training procedure: (1) Hard labels are used to provide precise supervision. (2) Semantic textual knowledge is utilized as weak supervision to find the potential relations between the novel and the base categories. (3) An imbalance control is presented from the data distribution to alleviate the recognition bias towards the base categories. We apply our method on three benchmark datasets, and it achieves state-of-the-art performances in all the experiments.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_42');
INSERT INTO `paper` VALUES (12207, 'Large-Scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline', 'Vision & Language', 'Visual dialog', '', '', '', 'Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed ViLBERT model for multi-turn visually-grounded conversations. Our model is pretrained on the Conceptual Captions and Visual Question Answering datasets, and finetuned on VisDial. Our best single model outperforms prior published work by \\(1\\%\\) absolute on NDCG and MRR.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_20');
INSERT INTO `paper` VALUES (12208, 'LarvaNet: Hierarchical Super-Resolution via Multi-exit Architecture', 'Efficient super-resolution', 'Deep convolutional neural network', 'Multi-exit architecture', '', '', 'In recent years, image super-resolution (SR) methods using convolutional neural networks (CNNs) have achieved successful results. Nevertheless, it is often difficult to apply them in resource-constrained environments due to the requirement of heavy computation and huge storage capacity. To address this issue, we propose an efficient network model for SR, called LarvaNet. First, we investigate a number of architectural factors for a baseline model and find optimal settings in terms of performance, number of parameters, and running time. Based on that, we design our model using a multi-exit architecture. Our experiments show that the proposed method achieves state-of-the-art SR performance with a reasonable number of parameters and running time. We also show that the multi-exit architecture of the proposed model allows us to control the trade-off between resource consumption and SR performance by selecting which exit point to be used.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_4');
INSERT INTO `paper` VALUES (12209, 'Late Temporal Modeling in 3D CNN Architectures with BERT for Action Recognition', 'Action recognition', 'Temporal attention', 'BERT', 'Late temporal modeling', '3D convolution', 'In this work, we combine 3D convolution with late temporal modeling for action recognition. For this aim, we replace the conventional Temporal Global Average Pooling (TGAP) layer at the end of 3D convolutional architecture with the Bidirectional Encoder Representations from Transformers (BERT) layer in order to better utilize the temporal information with BERT’s attention mechanism. We show that this replacement improves the performances of many popular 3D convolution architectures for action recognition, including ResNeXt, I3D, SlowFast and R(2+1)D. Moreover, we provide the-state-of-the-art results on both HMDB51 and UCF101 datasets with 85.10% and 98.69% top-1 accuracy, respectively. The code is publicly available github.com/artest08/LateTemporalModeling3DCNN.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_48');
INSERT INTO `paper` VALUES (12210, 'Latent Embedding Feedback and Discriminative Features for Zero-Shot Classification', 'Generalized zero-shot classification', 'Feature synthesis', '', '', '', 'Zero-shot learning strives to classify unseen categories for which no data is available during training. In the generalized variant, the test samples can further belong to seen or unseen categories. The state-of-the-art relies on Generative Adversarial Networks that synthesize unseen class features by leveraging class-specific semantic embeddings. During training, they generate semantically consistent features, but discard this constraint during feature synthesis and classification. We propose to enforce semantic consistency at all stages of (generalized) zero-shot learning: training, feature synthesis and classification. We first introduce a feedback loop, from a semantic embedding decoder, that iteratively refines the generated features during both the training and feature synthesis stages. The synthesized features together with their corresponding latent embeddings from the decoder are then transformed into discriminative features and utilized during classification to reduce ambiguities among categories. Experiments on (generalized) zero-shot object and action classification reveal the benefit of semantic consistency and iterative feedback, outperforming existing methods on six zero-shot learning benchmarks. Source code at https://github.com/akshitac8/tfvaegan.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_29');
INSERT INTO `paper` VALUES (12211, 'Latent Topic-Aware Multi-label Classification', 'Multi-label learning', 'Sample and feature extraction', 'Feature-label correlation', 'Topic', '', 'In real-world applications, data are often associated with different labels. Although most extant multi-label learning algorithms consider the label correlations, they rarely consider the topic information hidden in the labels, where each topic is a group of related labels and different topics have different groups of labels. In our study, we assume that there exists a common feature representation for labels in each topic. Then, feature-label correlation can be exploited in the latent topic space. This paper shows that the sample and feature exaction, which are two important procedures for removing noisy and redundant information encoded in training samples in both sample and feature perspectives, can be effectively and efficiently performed in the latent topic space by considering topic-based feature-label correlation. Empirical studies on several benchmarks demonstrate the effectiveness and efficiency of the proposed topic-aware framework.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_33');
INSERT INTO `paper` VALUES (12212, 'LatticeNet: Towards Lightweight Image Super-Resolution with Lattice Block', 'Super-resolution', 'Lattice block', 'LatticeNet', 'Lightweight', 'Attention', 'Deep neural networks with a massive number of layers have made a remarkable breakthrough on single image super-resolution (SR), but sacrifice computation complexity and memory storage. To address this problem, we focus on the lightweight models for fast and accurate image SR. Due to the frequent use of residual block (RB) in SR models, we pursue an economical structure to adaptively combine RBs. Drawing lessons from lattice filter bank, we design the lattice block (LB) in which two butterfly structures are applied to combine two RBs. LB has the potential of various linear combinations of two RBs. Each case of LB depends on the combination coefficients which are determined by the attention mechanism. LB favors the lightweight SR model with the reduction of about half amount of the parameters while keeping the similar SR performance. Moreover, we propose a lightweight SR model, LatticeNet, which uses series connection of LBs and the backward feature fusion. Extensive experiments demonstrate that our proposal can achieve superior accuracy on four available benchmark datasets against other state-of-the-art methods, while maintaining relatively low computation and memory requirements.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_17');
INSERT INTO `paper` VALUES (12213, 'Layer-Wise Conditioning Analysis in Exploring the Learning Dynamics of DNNs', 'Conditioning analysis', 'Normalization', 'Residual network', '', '', 'Conditioning analysis uncovers the landscape of an optimization objective by exploring the spectrum of its curvature matrix. This has been well explored theoretically for linear models. We extend this analysis to deep neural networks (DNNs) in order to investigate their learning dynamics. To this end, we propose layer-wise conditioning analysis, which explores the optimization landscape with respect to each layer independently. Such an analysis is theoretically supported under mild assumptions that approximately hold in practice. Based on our analysis, we show that batch normalization (BN) can stabilize the training, but sometimes result in the false impression of a local minimum, which has detrimental effects on the learning. Besides, we experimentally observe that BN can improve the layer-wise conditioning of the optimization problem. Finally, we find that the last linear layer of a very deep residual network displays ill-conditioned behavior. We solve this problem by only adding one BN layer before the last linear layer, which achieves improved performance over the original and pre-activation residual networks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_23');
INSERT INTO `paper` VALUES (12214, 'Layered Neighborhood Expansion for Incremental Multiple Graph Matching', 'Multi-graph matching', 'Clustering', 'Self-supervised learning', '', '', 'Graph matching has been a fundamental problem in computer vision and pattern recognition, for its practical flexibility as well as NP hardness challenge. Though the matching between two graphs and among multiple graphs have been intensively studied in literature, the online setting for incremental matching of a stream of graphs has been rarely considered. In this paper, we treat the graphs as graphs on a super-graph, and propose a novel breadth first search based method for expanding the neighborhood on the super-graph for a new coming graph, such that the matching with the new graph can be efficiently performed within the constructed neighborhood. Then depth first search is performed to update the overall pairwise matchings. Moreover, we show our approach can also be readily used in the batch mode setting, by adaptively determining the order of coming graph batch for matching, still under the neighborhood expansion based incremental matching framework. Experiments on both online and offline matching of graph collections show our approach’s state-of-the-art accuracy and efficiency.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_15');
INSERT INTO `paper` VALUES (12215, 'Laying the Foundations of Deep Long-Term Crowd Flow Prediction', 'Vision applications and systems', 'Datasets and evaluation', '', '', '', 'Predicting the crowd behavior in complex environments is a key requirement for crowd and disaster management, architectural design, and urban planning. Given a crowd’s immediate state, current approaches must be successively repeated over multiple time-steps for long-term predictions, leading to compute expensive and error-prone results. However, most applications require the ability to accurately predict hundreds of possible simulation outcomes (e.g., under different environment and crowd situations) at real-time rates, for which these approaches are prohibitively expensive. We propose the first deep framework to instantly predict the long-term flow of crowds in arbitrarily large, realistic environments. Central to our approach are a novel representation CAGE, which efficiently encodes crowd scenarios into compact, fixed-size representations that losslessly represent the environment, and a modified SegNet architecture for instant long-term crowd flow prediction. We conduct comprehensive experiments on novel synthetic and real datasets. Our results indicate that our approach is able to capture the essence of real crowd movement over very long time periods, while generalizing to never-before-seen environments and crowd contexts. The associated Supplementary Material, models, and datasets are available at github.com/SSSohn/LTCF.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_42');
INSERT INTO `paper` VALUES (12216, 'Leaping from 2D Detection to Efficient 6DoF Object Pose Estimation', 'Detection', 'Keypoint localization', '6DoF pose estimation', '', '', 'Estimating 6DoF object poses from single RGB images is very challenging due to severe occlusions and large search space of camera poses. Keypoint voting based methods have demonstrated its effectiveness and superiority on predicting object poses. However, those approaches are often affected by inaccurate semantic segmentation in computing the keypoint locations. To enable our model to focus on local regions without being distracted by backgrounds, we first localize object regions by a 2D object detector. In doing so, we not only reduce the search space of keypoints but also improve the robustness of the pose estimation. Moreover, since symmetric objects may suffer ambiguity along the symmetric dimension, we propose to select keypoints on the geometrically symmetric locations to resolve the ambiguity. The extensive experimental results on seven different datasets of the BOP challenge benchmark demonstrate that our method outperforms the state-of-the-art and achieves the 3-rd place in the BOP challenge.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_47');
INSERT INTO `paper` VALUES (12217, 'Learn Distributed GAN with Temporary Discriminators', '', '', '', '', '', 'In this work, we propose a method for training distributed GAN with sequential temporary discriminators. Our proposed method tackles the challenge of training GAN in the federated learning manner: How to update the generator with a flow of temporary discriminators? We apply our proposed method to learn a self-adaptive generator with a series of local discriminators from multiple data centers. We show our design of loss function indeed learns the correct distribution with provable guarantees. The empirical experiments show that our approach is capable of generating synthetic data which is practical for real-world applications such as training a segmentation model. Our TDGAN Code is available at: https://github.com/huiqu18/TDGAN-PyTorch.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_11');
INSERT INTO `paper` VALUES (12218, 'Learn to Propagate Reliably on Noisy Affinity Graphs', '', '', '', '', '', 'Recent works have shown that exploiting unlabeled data through label propagation can substantially reduce the labeling cost, which has been a critical issue in developing visual recognition models. Yet, how to propagate labels reliably, especially on a dataset with unknown outliers, remains an open question. Conventional methods such as linear diffusion lack the capability of handling complex graph structures and may perform poorly when the seeds are sparse. Latest methods based on graph neural networks would face difficulties on performance drop as they scale out to noisy graphs. To overcome these difficulties, we propose a new framework that allows labels to be propagated reliably on large-scale real-world data. This framework incorporates (1) a local graph neural network to predict accurately on varying local structures while maintaining high scalability, and (2) a confidence-based path scheduler that identifies outliers and moves forward the propagation frontier in a prudent way. Both components are learnable and closely coupled. Experiments on both ImageNet and Ms-Celeb-1M show that our confidence guided framework can significantly improve the overall accuracies of the propagated labels, especially when the graph is very noisy.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_27');
INSERT INTO `paper` VALUES (12219, 'Learn to Recover Visible Color for Video Surveillance in a Day', 'Video surveillance in a day', 'Color recovery', 'State synchronization network', '', '', 'In silicon sensors, the interference between visible and near-infrared (NIR) signals is a crucial problem. For all-day video surveillance, commercial camera systems usually adopt NIR cut filter, and auxiliary NIR LED illumination to selectively block or enhance NIR signal according to the surrounding light conditions. This switching between the daytime and the nighttime mode inevitably involves mechanical parts, and thus requires frequent maintenance. Furthermore, images captured at nighttime mode are in shortage of chrominance, which might hinder human interpretation and high-level computer vision algorithms in succession. In this paper, we present a deep learning based approach that directly generates human-friendly, visible color for video surveillance in a day. To enable training, we capture well-aligned video pairs through a customized optical device and contribute a large-scale dataset, video surveillance in a day (VSIAD). We propose a novel multi-task deep network with state synchronization modules to better utilize texture and chrominance information. Our trained model generates high-quality visible color images and achieves state-of-the-art performance on multiple metrics as well as subjective judgment.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_29');
INSERT INTO `paper` VALUES (12220, 'Learnable Cost Volume Using the Cayley Representation', 'Optical flow', 'Cost volume', 'Cayley representation', 'Inner product', '', 'Cost volume is an essential component of recent deep models for optical flow estimation and is usually constructed by calculating the inner product between two feature vectors. However, the standard inner product in the commonly-used cost volume may limit the representation capacity of flow models because it neglects the correlation among different channel dimensions and weighs each dimension equally. To address this issue, we propose a learnable cost volume (LCV) using an elliptical inner product, which generalizes the standard inner product by a positive definite kernel matrix. To guarantee its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The proposed LCV is a lightweight module and can be easily plugged into existing models to replace the vanilla cost volume. Experimental results show that the LCV module not only improves the accuracy of state-of-the-art models on standard benchmarks, but also promotes their robustness against illumination change, noises, and adversarial perturbations of the input signals.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_28');
INSERT INTO `paper` VALUES (12221, 'Learning 3D Part Assembly from a Single Image', 'Single-image 3D part assembly', 'Vision for robotic assembly', '', '', '', 'Autonomous assembly is a crucial capability for robots in many applications. For this task, several problems such as obstacle avoidance, motion planning, and actuator control have been extensively studied in robotics. However, when it comes to task specification, the space of possibilities remains underexplored. Towards this end, we introduce a novel problem, single-image-guided 3D part assembly, along with a learning-based solution. We study this problem in the setting of furniture assembly from a given complete set of parts and a single image depicting the entire assembled object. Multiple challenges exist in this setting, including handling ambiguity among parts (e.g., slats in a chair back and leg stretchers) and 3D pose prediction for parts and part subassemblies, whether visible or occluded. We address these issues by proposing a two-module pipeline that leverages strong 2D-3D correspondences and assembly-oriented graph message-passing to infer part relationships. In experiments with a PartNet-based synthetic benchmark, we demonstrate the effectiveness of our framework as compared with three baseline approaches (code and data available at https://github.com/AntheaLi/3DPartAssembly).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_40');
INSERT INTO `paper` VALUES (12222, 'Learning Actionness via Long-Range Temporal Order Verification', 'Temporal order', 'Action localization', 'Video recognition', '', '', 'Current methods for action recognition typically rely on supervision provided by manual labeling. Such methods, however, do not scale well given the high burden of manual video annotation and a very large number of possible actions. The annotation is particularly difficult for temporal action localization where large parts of the video present no action, or background. To address these challenges, we here propose a self-supervised and generic method to isolate actions from their background. We build on the observation that actions often follow a particular temporal order and, hence, can be predicted by other actions in the same video. As consecutive actions might be separated by minutes, differently to prior work on the arrow of time, we here exploit long-range temporal relations in 10–20 min long videos. To this end, we propose a new model that learns actionness via a self-supervised proxy task of order verification. The model assigns high actionness scores to clips which order is easy to predict from other clips in the video. To obtain a powerful and action-agnostic model, we train it on the large-scale unlabeled HowTo100M dataset with highly diverse actions from instructional videos. We validate our method on the task of action localization and demonstrate consistent improvements when combined with other recent weakly-supervised methods.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_28');
INSERT INTO `paper` VALUES (12223, 'Learning and Aggregating Deep Local Descriptors for Instance-Level Recognition', 'Deep local descriptors', 'Deep local features', 'Efficient match kernel', 'ASMK', 'Image retrieval', 'We propose an efficient method to learn deep local descriptors for instance-level recognition. The training only requires examples of positive and negative image pairs and is performed as metric learning of sum-pooled global image descriptors. At inference, the local descriptors are provided by the activations of internal components of the network. We demonstrate why such an approach learns local descriptors that work well for image similarity estimation with classical efficient match kernel methods. The experimental validation studies the trade-off between performance and memory requirements of the state-of-the-art image search approach based on match kernels. Compared to existing local descriptors, the proposed ones perform better in two instance-level recognition tasks and keep memory requirements lower. We experimentally show that global descriptors are not effective enough at large scale and that local descriptors are essential. We achieve state-of-the-art performance, in some cases even with a backbone network as small as ResNet18.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_27');
INSERT INTO `paper` VALUES (12224, 'Learning and Memorizing Representative Prototypes for 3D Point Cloud Semantic and Instance Segmentation', 'Point cloud', 'Instance segmentation', 'Memory network', '', '', '3D point cloud semantic and instance segmentation are crucial and fundamental for 3D scene understanding. Due to the complex structure, point sets are distributed off-balance and diversely, appearing as both category and pattern imbalance. It has been proved that deep networks can easily forget the non-dominant cases during training, which influences the model generalization and leads to unsatisfactory performance. Although re-weighting on instances may reduce the influence, it is hard to find a balance between the dominant and the non-dominant cases. To tackle the above issue, we propose a memory-augmented network that learns and memorizes the representative prototypes that encode both geometry and semantic information. The prototypes are shared by diverse 3D points and recorded in a universal memory module. During training, the memory slots are dynamically associated with both dominant and non-dominant cases, alleviating the forgetting issue. In testing, the distorted observations and rare cases can thus be augmented by retrieving the stored prototypes, leading to better generalization. Experiments on the benchmarks, i.e., S3DIS and ScanNetV2, show the superiority of our method on both effectiveness and efficiency, which substantially improves the accuracy not only on the entire dataset but also on non-dominant classes and samples.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_33');
INSERT INTO `paper` VALUES (12225, 'Learning Architectures for Binary Networks', 'Binary networks', 'Backbone architecture', 'Architecture search', '', '', 'Backbone architectures of most binary networks are well-known floating point (FP) architectures such as the ResNet family. Questioning that the architectures designed for FP networks might not be the best for binary networks, we propose to search architectures for binary networks (BNAS) by defining a new search space for binary architectures and a novel search objective. Specifically, based on the cell based search method, we define the new search space of binary layer types, design a new cell template, and rediscover the utility of and propose to use the Zeroise layer instead of using it as a placeholder. The novel search objective diversifies early search to learn better performing binary architectures. We show that our method searches architectures with stable training curves despite the quantization error inherent in binary networks. Quantitative analyses demonstrate that our searched architectures outperform the architectures used in state-of-the-art binary networks and outperform or perform on par with state-of-the-art binary networks that employ various techniques other than architectural changes.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_34');
INSERT INTO `paper` VALUES (12226, 'Learning Attentive and Hierarchical Representations for 3D Shape Recognition', '3D shape recognition', 'View-agnostic/specific attentions', 'Multi-granularity view aggregation', 'Hyperbolic neural networks', '', 'This paper proposes a novel method for 3D shape representation learning, namely Hyperbolic Embedded Attentive Representation (HEAR). Different from existing multi-view based methods, HEAR develops a unified framework to address both multi-view redundancy and single-view incompleteness. Specifically, HEAR firstly employs a hybrid attention (HA) module, which consists of a view-agnostic attention (VAA) block and a view-specific attention (VSA) block. These two blocks jointly explore distinct but complementary spatial saliency of local features for each single-view image. Subsequently, a multi-granular view pooling (MVP) module is introduced to aggregate the multi-view features with different granularities in a coarse-to-fine manner. The resulting feature set implicitly has hierarchical relations, which are therefore projected into a Hyperbolic space by adopting the Hyperbolic embedding. A hierarchical representation is learned by Hyperbolic multi-class logistic regression based on the Hyperbolic geometry. Experimental results clearly show that HEAR outperforms the state-of-the-art approaches on three 3D shape recognition tasks including generic 3D shape retrieval, 3D shape classification and sketch-based 3D shape retrieval.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_7');
INSERT INTO `paper` VALUES (12227, 'Learning Camera-Aware Noise Models', 'Noise model', 'Denoising', 'GANs', 'Sensor', '', 'Modeling imaging sensor noise is a fundamental problem for image processing and computer vision applications. While most previous works adopt statistical noise models, real-world noise is far more complicated and beyond what these models can describe. To tackle this issue, we propose a data-driven approach, where a generative noise model is learned from real-world noise. The proposed noise model is camera-aware, that is, different noise characteristics of different camera sensors can be learned simultaneously, and a single learned noise model can generate different noise for different camera sensors. Experimental results show that our method quantitatively and qualitatively outperforms existing statistical noise models and learning-based methods. The source code and more results are available at https://arcchang1236.github.io/CA-NoiseGAN/.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_21');
INSERT INTO `paper` VALUES (12228, 'Learning Canonical Representations for Scene Graph to Image Generation', 'Scene graphs', 'Canonical representations', 'Image generation', '', '', 'Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images. Previous approaches showed that scenes with few entities can be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases. In this work, we show that one limitation of current methods is their inability to capture semantic equivalence in graphs. We present a novel model that addresses these issues by learning canonical graph representations from the data, resulting in improved image generation for complex visual scenes (The project page is available at https://roeiherz.github.io/CanonicalSg2Im/). Our model demonstrates improved empirical performance on large scene graphs, robustness to noise in the input scene graph, and generalization on semantically equivalent graphs. Finally, we show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_13');
INSERT INTO `paper` VALUES (12229, 'Learning Connectivity of Neural Networks from a Topological Perspective', 'Learning connectivity', 'Topological perspective', '', '', '', 'Seeking effective neural networks is a critical and practical field in deep learning. Besides designing the depth, type of convolution, normalization, and nonlinearities, the topological connectivity of neural networks is also important. Previous principles of rule-based modular design simplify the difficulty of building an effective architecture, but constrain the possible topologies in limited spaces. In this paper, we attempt to optimize the connectivity in neural networks. We propose a topological perspective to represent a network into a complete graph for analysis, where nodes carry out aggregation and transformation of features, and edges determine the flow of information. By assigning learnable parameters to the edges which reflect the magnitude of connections, the learning process can be performed in a differentiable manner. We further attach auxiliary sparsity constraint to the distribution of connectedness, which promotes the learned topology focus on critical connections. This learning process is compatible with existing networks and owns adaptability to larger search spaces and different tasks. Quantitative results of experiments reflect the learned connectivity is superior to traditional rule-based ones, such as random, residual and complete. In addition, it obtains significant improvements in image classification and object detection without introducing excessive computation burden.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_44');
INSERT INTO `paper` VALUES (12230, 'Learning Data Augmentation Strategies for Object Detection', '', '', '', '', '', 'Much research on object detection focuses on building better model architectures and detection algorithms. Changing the model architecture, however, comes at the cost of adding more complexity to inference, making models slower. Data augmentation, on the other hand, doesn’t add any inference complexity, but is insufficiently studied in object detection for two reasons. First it is more difficult to design plausible augmentation strategies for object detection than for classification, because one must handle the complexity of bounding boxes if geometric transformations are applied. Secondly, data augmentation attracts less research attention perhaps because it is believed to add less value and to transfer poorly compared to advances in network architectures.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_34');
INSERT INTO `paper` VALUES (12231, 'Learning Delicate Local Representations for Multi-person Pose Estimation', 'Human pose estimation', 'COCO', 'MPII', 'Feature aggregation', 'Attention mechanism', 'In this paper, we propose a novel method called Residual Steps Network (RSN). RSN aggregates features with the same spatial size (Intra-level features) efficiently to obtain delicate local representations, which retain rich low-level spatial information and result in precise keypoint localization. Additionally, we observe the output features contribute differently to final performance. To tackle this problem, we propose an efficient attention mechanism - Pose Refine Machine (PRM) to make a trade-off between local and global representations in output features and further refine the keypoint locations. Our approach won the 1st place of COCO Keypoint Challenge 2019 and achieves state-of-the-art results on both COCO and MPII benchmarks, without using extra training data and pretrained model. Our single model achieves 78.6 on COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on COCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly available for further research at https://github.com/caiyuanhao1998/RSN/.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_27');
INSERT INTO `paper` VALUES (12232, 'Learning Discriminative Feature with CRF for Unsupervised Video Object Segmentation', 'Video object segmentation', 'Discriminative feature', 'CRF', '', '', 'In this paper, we introduce a novel network, called discriminative feature network (DFNet), to address the unsupervised video object segmentation task. To capture the inherent correlation among video frames, we learn discriminative features (D-features) from the input images that reveal feature distribution from a global perspective. The D-features are then used to establish correspondence with all features of test image under conditional random field (CRF) formulation, which is leveraged to enforce consistency between pixels. The experiments verify that DFNet outperforms state-of-the-art methods by a large margin with a mean IoU score of 83.4% and ranks first on the DAVIS-2016 leaderboard while using much fewer parameters and achieving much more efficient performance in the inference phase. We further evaluate DFNet on the FBMS dataset and the video saliency dataset ViSal, reaching a new state-of-the-art. To further demonstrate the generalizability of our framework, DFNet is also applied to the image object co-segmentation task. We perform experiments on a challenging dataset PASCAL-VOC and observe the superiority of DFNet. The thorough experiments verify that DFNet is able to capture and mine the underlying relations of images and discover the common foreground objects.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_27');
INSERT INTO `paper` VALUES (12233, 'Learning Disentangled Feature Representation for Hybrid-Distorted Image Restoration', 'Hybird-distorted image restoration', 'Feature disentanglement', 'Feature aggregation', '', '', 'Hybrid-distorted image restoration (HD-IR) is dedicated to restore real distorted image that is degraded by multiple distortions. Existing HD-IR approaches usually ignore the inherent interference among hybrid distortions which compromises the restoration performance. To decompose such interference, we introduce the concept of Disentangled Feature Learning to achieve the feature-level divide-and-conquer of hybrid distortions. Specifically, we propose the feature disentanglement module (FDM) to distribute feature representations of different distortions into different channels by revising gain-control-based normalization. We also propose a feature aggregation module (FAM) with channel-wise attention to adaptively filter out the distortion representations and aggregate useful content information from different channels for the construction of raw image. The effectiveness of the proposed scheme is verified by visualizing the correlation matrix of features and channel responses of different distortions. Extensive experimental results also prove superior performance of our approach compared with the latest HD-IR schemes.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_19');
INSERT INTO `paper` VALUES (12234, 'Learning Disentangled Representations via Mutual Information Estimation', 'Representation learning', 'Representation disentanglement', 'Mutual information maximization and minimization', '', '', 'In this paper, we investigate the problem of learning disentangled representations. Given a pair of images sharing some attributes, we aim to create a low-dimensional representation which is split into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image. To address this issue, we propose a model based on mutual information estimation without relying on image reconstruction or image generation. Mutual information maximization is performed to capture the attributes of data in the shared and exclusive representations while we minimize the mutual information between the shared and exclusive representation to enforce representation disentanglement. We show that these representations are useful to perform downstream tasks such as image classification and image retrieval based on the shared or exclusive component. Moreover, classification results show that our model outperforms the state-of-the-art models based on VAE/GAN approaches in representation disentanglement.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_13');
INSERT INTO `paper` VALUES (12235, 'Learning Disentangled Representations with Latent Variation Predictability', '', '', '', '', '', 'Latent traversal is a popular approach to visualize the disentangled latent representations. Given a bunch of variations in a single unit of the latent representation, it is expected that there is a change in a single factor of variation of the data while others are fixed. However, this impressive experimental observation is rarely explicitly encoded in the objective function of learning disentangled representations. This paper defines the variation predictability of latent disentangled representations. Given image pairs generated by latent codes varying in a single dimension, this varied dimension could be closely correlated with these image pairs if the representation is well disentangled. Within an adversarial generation process, we encourage variation predictability by maximizing the mutual information between latent variations and corresponding image pairs. We further develop an evaluation metric that does not rely on the ground-truth generative factors to measure the disentanglement of latent representations. The proposed variation predictability is a general constraint that is applicable to the VAE and GAN frameworks for boosting disentanglement of latent representations. Experiments show that the proposed variation predictability correlates well with existing ground-truth-required metrics and the proposed algorithm is effective for disentanglement learning.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_40');
INSERT INTO `paper` VALUES (12236, 'Learning Enriched Features for Real Image Restoration and Enhancement', 'Image denoising', 'Super-resolution', 'Image enhancement', '', '', 'With the goal of recovering high-quality image content from its degraded version, image restoration enjoys numerous applications, such as in surveillance, computational photography and medical imaging. Recently, convolutional neural networks (CNNs) have achieved dramatic improvements over conventional approaches for image restoration task. Existing CNN-based methods typically operate either on full-resolution or on progressively low-resolution representations. In the former case, spatially precise but contextually less robust results are achieved, while in the latter case, semantically reliable but spatially less accurate outputs are generated. In this paper, we present an architecture with the collective goals of maintaining spatially-precise high-resolution representations through the entire network and receiving strong contextual information from the low-resolution representations. The core of our approach is a multi-scale residual block containing several key elements: (a) parallel multi-resolution convolution streams for extracting multi-scale features, (b) information exchange across the multi-resolution streams, (c) spatial and channel attention mechanisms for capturing contextual information, and (d) attention based multi-scale feature aggregation. In a nutshell, our approach learns an enriched set of features that combines contextual information from multiple scales, while simultaneously preserving the high-resolution spatial details. Extensive experiments on five real image benchmark datasets demonstrate that our method, named as MIRNet, achieves state-of-the-art results for image denoising, super-resolution, and image enhancement. The source code and pre-trained models are available at https://github.com/swz30/MIRNet.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_30');
INSERT INTO `paper` VALUES (12237, 'Learning Event-Driven Video Deblurring and Interpolation', '', '', '', '', '', 'Event-based sensors, which have a response if the change of pixel intensity exceeds a triggering threshold, can capture high-speed motion with microsecond accuracy. Assisted by an event camera, we can generate high frame-rate sharp videos from low frame-rate blurry ones captured by an intensity camera. In this paper, we propose an effective event-driven video deblurring and interpolation algorithm based on deep convolutional neural networks (CNNs). Motivated by the physical model that the residuals between a blurry image and sharp frames are the integrals of events, the proposed network uses events to estimate the residuals for the sharp frame restoration. As the triggering threshold varies spatially, we develop an effective method to estimate dynamic filters to solve this problem. To utilize the temporal information, the sharp frames restored from the previous blurry frame are also considered. The proposed algorithm achieves superior performance against state-of-the-art methods on both synthetic and real datasets.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_41');
INSERT INTO `paper` VALUES (12238, 'Learning Feature Descriptors Using Camera Pose Supervision', 'Local features', 'Feature descriptors', 'Correspondence', 'Image matching', 'Camera pose', 'Recent research on learned visual descriptors has shown promising improvements in correspondence estimation, a key component of many 3D vision tasks. However, existing descriptor learning frameworks typically require ground-truth correspondences between feature points for training, which are challenging to acquire at scale. In this paper we propose a novel weakly-supervised framework that can learn feature descriptors solely from relative camera poses between images. To do so, we devise both a new loss function that exploits the epipolar constraint given by camera poses, and a new model architecture that makes the whole pipeline differentiable and efficient. Because we no longer need pixel-level ground-truth correspondences, our framework opens up the possibility of training on much larger and more diverse datasets for better and unbiased descriptors. We call the resulting descriptors CAmera Pose Supervised, or CAPS, descriptors. Though trained with weak supervision, CAPS descriptors outperform even prior fully-supervised descriptors and achieve state-of-the-art performance on a variety of geometric tasks. (Project page: https://qianqianwang68.github.io/CAPS/.)', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_44');
INSERT INTO `paper` VALUES (12239, 'Learning Feature Embeddings for Discriminant Model Based Tracking', '', '', '', '', '', 'After observing that the features used in most online discriminatively trained trackers are not optimal, in this paper, we propose a novel and effective architecture to learn optimal feature embeddings for online discriminative tracking. Our method, called DCFST, integrates the solver of a discriminant model that is differentiable and has a closed-form solution into convolutional neural networks. Then, the resulting network can be trained in an end-to-end way, obtaining optimal feature embeddings for the discriminant model-based tracker. As an instance, we apply the popular ridge regression model in this work to demonstrate the power of DCFST. Extensive experiments on six public benchmarks, OTB2015, NFS, GOT10k, TrackingNet, VOT2018, and VOT2019, show that our approach is efficient and generalizes well to class-agnostic target objects in online tracking, thus achieves state-of-the-art accuracy, while running beyond the real-time speed. Code will be made available.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_45');
INSERT INTO `paper` VALUES (12240, 'Learning Flow-Based Feature Warping for Face Frontalization with Illumination Inconsistent Supervision', 'Face frontalization', 'Illumination preserving', 'Optical flow', 'Guided filter', 'Attention mechanism', 'Despite recent advances in deep learning-based face frontalization methods, photo-realistic and illumination preserving frontal face synthesis is still challenging due to large pose and illumination discrepancy during training. We propose a novel Flow-based Feature Warping Model (FFWM) which can learn to synthesize photo-realistic and illumination preserving frontal images with illumination inconsistent supervision. Specifically, an Illumination Preserving Module (IPM) is proposed to learn illumination preserving image synthesis from illumination inconsistent image pairs. IPM includes two pathways which collaborate to ensure the synthesized frontal images are illumination preserving and with fine details. Moreover, a Warp Attention Module (WAM) is introduced to reduce the pose discrepancy in the feature level, and hence to synthesize frontal images more effectively and preserve more details of profile images. The attention mechanism in WAM helps reduce the artifacts caused by the displacements between the profile and the frontal images. Quantitative and qualitative experimental results show that our FFWM can synthesize photo-realistic and illumination preserving frontal images and performs favorably against the state-of-the-art results. Our code is available at https://github.com/csyxwei/FFWM.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_33');
INSERT INTO `paper` VALUES (12241, 'Learning from Extrinsic and Intrinsic Supervisions for Domain Generalization', 'Domain generalization', 'Unsupervised learning', 'Metric learning', 'Self-supervision', '', 'The generalization capability of neural networks across domains is crucial for real-world applications. We argue that a generalized object recognition system should well understand the relationships among different images and also the images themselves at the same time. To this end, we present a new domain generalization framework (called EISNet) that learns how to generalize across domains simultaneously from extrinsic relationship supervision and intrinsic self-supervision for images from multi-source domains. To be specific, we formulate our framework with feature embedding using a multi-task learning paradigm. Besides conducting the common supervised recognition task, we seamlessly integrate a momentum metric learning task and a self-supervised auxiliary task to collectively integrate the extrinsic and intrinsic supervisions. Also, we develop an effective momentum metric learning scheme with the K-hard negative mining to boost the network generalization ability. We demonstrate the effectiveness of our approach on two standard object recognition benchmarks VLCS and PACS, and show that our EISNet achieves state-of-the-art performance.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_10');
INSERT INTO `paper` VALUES (12242, 'Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-Tailed Classification', '', '', '', '', '', 'In real-world scenarios, data tends to exhibit a long-tailed distribution, which increases the difficulty of training deep networks. In this paper, we propose a novel self-paced knowledge distillation framework, termed Learning From Multiple Experts (LFME). Our method is inspired by the observation that networks trained on less imbalanced subsets of the distribution often yield better performances than their jointly-trained counterparts. We refer to these models as ‘Experts’, and the proposed LFME framework aggregates the knowledge from multiple ‘Experts’ to learn a unified student model. Specifically, the proposed framework involves two levels of adaptive learning schedules: Self-paced Expert Selection and Curriculum Instance Selection, so that the knowledge is adaptively transferred to the ‘Student’. We conduct extensive experiments and demonstrate that our method is able to achieve superior performances compared to state-of-the-art methods. We also show that our method can be easily plugged into state-of-the-art long-tailed classification algorithms for further improvements.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_15');
INSERT INTO `paper` VALUES (12243, 'Learning from Scale-Invariant Examples for Domain Adaptation in Semantic Segmentation', '', '', '', '', '', 'Self-supervised learning approaches for unsupervised domain adaptation (UDA) of semantic segmentation models suffer from challenges of predicting and selecting reasonable good quality pseudo labels. In this paper, we propose a novel approach of exploiting scale-invariance property of the semantic segmentation model for self-supervised domain adaptation. Our algorithm is based on a reasonable assumption that, in general, regardless of the size of the object and stuff (given context) the semantic labeling should be unchanged. We show that this constraint is violated over the images of the target domain, and hence could be used to transfer labels in-between differently scaled patches. Specifically, we show that semantic segmentation model produces output with high entropy when presented with scaled-up patches of target domain, in comparison to when presented original size images. These scale-invariant examples are extracted from the most confident images of the target domain. Dynamic class specific entropy thresholding mechanism is presented to filter out unreliable pseudo-labels. Furthermore, we also incorporate the focal loss to tackle the problem of class imbalance in self-supervised learning. Extensive experiments have been performed, and results indicate that exploiting the scale-invariant labeling, we outperform existing self-supervised based state-of-the-art domain adaptation methods. Specifically, we achieve 1.3% and 3.8% of lead for GTA5 to Cityscapes and SYNTHIA to Cityscapes with VGG16-FCN8 baseline network.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_18');
INSERT INTO `paper` VALUES (12244, 'Learning Gradient Fields for Shape Generation', '3D generation', 'Generative models', '', '', '', 'In this work, we propose a novel technique to generate shapes from point cloud data. A point cloud can be viewed as samples from a distribution of 3D points whose density is concentrated near the surface of the shape. Point cloud generation thus amounts to moving randomly sampled points to high-density areas. We generate point clouds by performing stochastic gradient ascent on an unnormalized probability density, thereby moving sampled points toward the high-likelihood regions. Our model directly predicts the gradient of the log density field and can be trained with a simple objective adapted from score-based generative models. We show that our method can reach state-of-the-art performance for point cloud auto-encoding and generation, while also allowing for extraction of a high-quality implicit surface. Code is available at https://github.com/RuojinCai/ShapeGF.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_22');
INSERT INTO `paper` VALUES (12245, 'Learning Graph-Convolutional Representations for Point Cloud Denoising', 'Point cloud', 'Denoising', 'Graph neural network', '', '', 'Point clouds are an increasingly relevant data type but they are often corrupted by noise. We propose a deep neural network based on graph-convolutional layers that can elegantly deal with the permutation-invariance problem encountered by learning-based point cloud processing methods. The network is fully-convolutional and can build complex hierarchies of features by dynamically constructing neighborhood graphs from similarity among the high-dimensional feature representations of the points. When coupled with a loss promoting proximity to the ideal surface, the proposed approach significantly outperforms state-of-the-art methods on a variety of metrics. In particular, it is able to improve in terms of Chamfer measure and of quality of the surface normals that can be estimated from the denoised data. We also show that it is especially robust both at high noise levels and in presence of structured noise such as the one encountered in real LiDAR scans.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_7');
INSERT INTO `paper` VALUES (12246, 'Learning Joint Spatial-Temporal Transformations for Video Inpainting', 'Video inpainting', 'Generative adversarial networks', '', '', '', 'High-quality video inpainting that completes missing regions in video frames is a promising yet challenging task. State-of-the-art approaches adopt attention models to complete a frame by searching missing contents from reference frames, and further complete whole videos frame by frame. However, these approaches can suffer from inconsistent attention results along spatial and temporal dimensions, which often leads to blurriness and temporal artifacts in videos. In this paper, we propose to learn a joint Spatial-Temporal Transformer Network (STTN) for video inpainting. Specifically, we simultaneously fill missing regions in all input frames by self-attention, and propose to optimize STTN by a spatial-temporal adversarial loss. To show the superiority of the proposed model, we conduct both quantitative and qualitative evaluations by using standard stationary masks and more realistic moving object masks. Demo videos are available at https://github.com/researchmm/STTN.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_31');
INSERT INTO `paper` VALUES (12247, 'Learning Joint Visual Semantic Matching Embeddings for Language-Guided Retrieval', '', '', '', '', '', 'Interactive image retrieval is an emerging research topic with the objective of integrating inputs from multiple modalities as query for retrieval, e.g., textual feedback from users to guide, modify or refine image retrieval. In this work, we study the problem of composing images and textual modifications for language-guided retrieval in the context of fashion applications. We propose a unified Joint Visual Semantic Matching (JVSM) model that learns image-text compositional embeddings by jointly associating visual and textual modalities in a shared discriminative embedding space via compositional losses. JVSM has been designed with versatility and flexibility in mind, being able to perform multiple image and text tasks in a single model, such as text-image matching and language-guided retrieval. We show the effectiveness of our approach in the fashion domain, where it is difficult to express keyword-based queries given the complex specificity of fashion terms. Our experiments on three datasets (Fashion-200k, UT-Zap50k, and Fashion-iq) show that JVSM achieves state-of-the-art results on language-guided retrieval and additionally we show its capabilities to perform image and text retrieval.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_9');
INSERT INTO `paper` VALUES (12248, 'Learning Lane Graph Representations for Motion Forecasting', 'HD map', 'Motion forecasting', 'Autonomous driving', '', '', 'We propose a motion forecasting model that exploits a novel structured map representation as well as actor-map interactions. Instead of encoding vectorized maps as raster images, we construct a lane graph from raw map data to explicitly preserve the map structure. To capture the complex topology and long range dependencies of the lane graph, we propose LaneGCN which extends graph convolutions with multiple adjacency matrices and along-lane dilation. To capture the complex interactions between actors and maps, we exploit a fusion network consisting of four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and actor-to-actor. Powered by LaneGCN and actor-map interactions, our model is able to predict accurate and realistic multi-modal trajectories. Our approach significantly outperforms the state-of-the-art on the large scale Argoverse motion forecasting benchmark.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_32');
INSERT INTO `paper` VALUES (12249, 'Learning Latent Representations Across Multiple Data Domains Using Lifelong VAEGAN', 'Lifelong learning', 'Representation learning', 'Generative modeling', 'VAEGAN model.', '', 'The problem of catastrophic forgetting occurs in deep learning models trained on multiple databases in a sequential manner. Recently, generative replay mechanisms (GRM) have been proposed to reproduce previously learned knowledge aiming to reduce the forgetting. However, such approaches lack an appropriate inference model and therefore can not provide latent representations of data. In this paper, we propose a novel lifelong learning approach, namely the Lifelong VAEGAN (L-VAEGAN), which not only induces a powerful generative replay network but also learns meaningful latent representations, benefiting representation learning. L-VAEGAN can allow to automatically embed the information associated with different domains into several clusters in the latent space, while also capturing semantically meaningful shared latent variables, across different data domains. The proposed model supports many downstream tasks that traditional generative replay methods can not, including interpolation and inference across different data domains.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_46');
INSERT INTO `paper` VALUES (12250, 'Learning Memory Augmented Cascading Network for Compressed Sensing of Images', 'Compressed sensing', 'Cascading network', 'Contextual memory', 'Progressive reconstruction', '', 'In this paper, we propose a cascading network for compressed sensing of images with progressive reconstruction. Specifically, we decompose the complex reconstruction mapping into the cascade of incremental detail reconstruction (IDR) modules and measurement residual updating (MRU) modules. The IDR module is designed to reconstruct the remaining details from the residual measurement vector, and MRU is employed to update the residual measurement vector and feed it into the next IDR module. The contextual memory module is introduced to augment the capacity of IDR modules, therefore facilitating the information interaction among all the IDR modules. The final reconstruction is calculated by accumulating the outputs of all the IDR modules. Extensive experiments on natural images and magnetic resonance images demonstrate the proposed method achieves better performance against the state-of-the-art methods.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_31');
INSERT INTO `paper` VALUES (12251, 'Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos', 'Temporal sentence localization', 'Event captioning in videos', 'Modality interaction', '', '', 'Automatically generating sentences to describe events and temporally localizing sentences in a video are two important tasks that bridge language and videos. Recent techniques leverage the multimodal nature of videos by using off-the-shelf features to represent videos, but interactions between modalities are rarely explored. Inspired by the fact that there exist cross-modal interactions in the human brain, we propose a novel method for learning pairwise modality interactions in order to better exploit complementary information for each pair of modalities in videos and thus improve performances on both tasks. We model modality interaction in both the sequence and channel levels in a pairwise fashion, and the pairwise interaction also provides some explainability for the predictions of target tasks. We demonstrate the effectiveness of our method and validate specific design choices through extensive ablation studies. Our method turns out to achieve state-of-the-art performances on four standard benchmark datasets: MSVD and MSR-VTT (event captioning task), and Charades-STA and ActivityNet Captions (temporal sentence localization task).', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_20');
INSERT INTO `paper` VALUES (12252, 'Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling', '', '', '', '', '', 'Monocular visual odometry (VO) suffers severely from error accumulation during frame-to-frame pose estimation. In this paper, we present a self-supervised learning method for VO with special consideration for consistency over longer sequences. To this end, we model the long-term dependency in pose prediction using a pose network that features a two-layer convolutional LSTM module. We train the networks with purely self-supervised losses, including a cycle consistency loss that mimics the loop closure module in geometric VO. Inspired by prior geometric systems, we allow the networks to see beyond a small temporal window during training, through a novel a loss that incorporates temporally distant (e.g., O(100)) frames. Given GPU memory constraints, we propose a stage-wise training mechanism, where the first stage operates in a local time window and the second stage refines the poses with a “global” loss given the first stage features. We demonstrate competitive results on several standard VO datasets, including KITTI and TUM RGB-D.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_42');
INSERT INTO `paper` VALUES (12253, 'Learning Multi-layer Latent Variable Model via Variational Optimization of Short Run MCMC for Approximate Inference', '', '', '', '', '', 'This paper studies the fundamental problem of learning deep generative models that consist of multiple layers of latent variables organized in top-down architectures. Such models have high expressivity and allow for learning hierarchical representations. Learning such a generative model requires inferring the latent variables for each training example based on the posterior distribution of these latent variables. The inference typically requires Markov chain Monte Caro (MCMC) that can be time consuming. In this paper, we propose to use noise initialized non-persistent short run MCMC, such as finite step Langevin dynamics initialized from the prior distribution of the latent variables, as an approximate inference engine, where the step size of the Langevin dynamics is variationally optimized by minimizing the Kullback-Leibler divergence between the distribution produced by the short run MCMC and the posterior distribution. Our experiments show that the proposed method outperforms variational auto-encoder (VAE) in terms of reconstruction error and synthesis quality. The advantage of the proposed method is that it is simple and automatic without the need to design an inference model.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_22');
INSERT INTO `paper` VALUES (12254, 'Learning Noise-Aware Encoder-Decoder from Noisy Labels by Alternating Back-Propagation for Saliency Detection', 'Noisy saliency', 'Latent variable model', 'Langevin dynamics', 'Alternating back-propagation', '', 'In this paper, we propose a noise-aware encoder-decoder framework to disentangle a clean saliency predictor from noisy training examples, where the noisy labels are generated by unsupervised handcrafted feature-based methods. The proposed model consists of two sub-models parameterized by neural networks: (1) a saliency predictor that maps input images to clean saliency maps, and (2) a noise generator, which is a latent variable model that produces noises from Gaussian latent vectors. The whole model that represents noisy labels is a sum of the two sub-models. The goal of training the model is to estimate the parameters of both sub-models, and simultaneously infer the corresponding latent vector of each noisy label. We propose to train the model by using an alternating back-propagation (ABP) algorithm, which alternates the following two steps: (1) learning back-propagation for estimating the parameters of two sub-models by gradient ascent, and (2) inferential back-propagation for inferring the latent vectors of training noisy examples by Langevin Dynamics. To prevent the network from converging to trivial solutions, we utilize an edge-aware smoothness loss to regularize hidden saliency maps to have similar structures as their corresponding images. Experimental results on several benchmark datasets indicate the effectiveness of the proposed model.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_21');
INSERT INTO `paper` VALUES (12255, 'Learning Object Depth from Camera Motion and Video Object Segmentation', 'Depth estimation', 'Video object segmentation', 'Robotics', '', '', 'Video object segmentation, i.e., the separation of a target object from background in video, has made significant progress on real and challenging videos in recent years. To leverage this progress in 3D applications, this paper addresses the problem of learning to estimate the depth of segmented objects given some measurement of camera motion (e.g., from robot kinematics or vehicle odometry). We achieve this by, first, introducing a diverse, extensible dataset and, second, designing a novel deep network that estimates the depth of objects using only segmentation masks and uncalibrated camera movement. Our data-generation framework creates artificial object segmentations that are scaled for changes in distance between the camera and object, and our network learns to estimate object depth even with segmentation errors. We demonstrate our approach across domains using a robot camera to locate objects from the YCB dataset and a vehicle camera to locate obstacles while driving.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_18');
INSERT INTO `paper` VALUES (12256, 'Learning Object Permanence from Video', 'Object Permanence', 'Reasoning', 'Video Analysis', '', '', 'Object Permanence allows people to reason about the location of non-visible objects, by understanding that they continue to exist even when not perceived directly. Object Permanence is critical for building a model of the world, since objects in natural visual scenes dynamically occlude and contain each-other. Intensive studies in developmental psychology suggest that object permanence is a challenging task that is learned through extensive experience.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_3');
INSERT INTO `paper` VALUES (12257, 'Learning Object Placement by Inpainting for Compositional Data Augmentation', 'Object placement', 'Inpainting', 'Data augmentation', '', '', 'We study the problem of common sense placement of visual objects in an image. This involves multiple aspects of visual recognition: the instance segmentation of the scene, 3D layout, and common knowledge of how objects are placed and where objects are moving in the 3D scene. This seemingly simple task is difficult for current learning-based approaches because of the lack of labeled training pair of foreground objects paired with cleaned background scenes. We propose a self-learning framework that automatically generates the necessary training data without any manual labeling by detecting, cutting, and inpainting objects from an image. We propose a PlaceNet that predicts a diverse distribution of common sense locations when given a foreground object and a background scene. We show one practical use of our object placement network for augmenting training datasets by recomposition of object-scene with a key property of contextual relationship preservation. We demonstrate improvement of object detection and instance segmentation performance on both Cityscape [4] and KITTI [9] datasets. We also show that the learned representation of our PlaceNet displays strong discriminative power in image retrieval and classification.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_34');
INSERT INTO `paper` VALUES (12258, 'Learning Object Relation Graph and Tentative Policy for Visual Navigation', 'Graph', 'Imitation learning', 'Tentative policy learning', 'Visual navigation', '', 'Target-driven visual navigation aims at navigating an agent towards a given target based on the observation of the agent. In this task, it is critical to learn informative visual representation and robust navigation policy. Aiming to improve these two components, this paper proposes three complementary techniques, object relation graph (ORG), trial-driven imitation learning (IL), and a memory-augmented tentative policy network (TPN). ORG improves visual representation learning by integrating object relationships, including category closeness and spatial correlations, e.g., a TV usually co-occurs with a remote spatially. Both Trial-driven IL and TPN underlie robust navigation policy, instructing the agent to escape from deadlock states, such as looping or being stuck. Specifically, trial-driven IL is a type of supervision used in policy network training, while TPN, mimicking the IL supervision in unseen environment, is applied in testing. Experiment in the artificial environment AI2-Thor validates that each of the techniques is effective. When combined, the techniques bring significantly improvement over baseline methods in navigation effectiveness and efficiency in unseen environments. We report 22.8% and 23.5% increase in success rate and Success weighted by Path Length (SPL), respectively. The code is available at https://github.com/xiaobaishu0097/ECCV-VN.git.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_2');
INSERT INTO `paper` VALUES (12259, 'Learning Open Set Network with Discriminative Reciprocal Points', '', '', '', '', '', 'Open set recognition is an emerging research area that aims to simultaneously classify samples from predefined classes and identify the rest as ‘unknown’. In this process, one of the key challenges is to reduce the risk of generalizing the inherent characteristics of numerous unknown samples learned from a small amount of known data. In this paper, we propose a new concept, Reciprocal Point, which is the potential representation of the extra-class space corresponding to each known category. The sample can be classified to known or unknown by the otherness with reciprocal points. To tackle the open set problem, we offer a novel open space risk regularization term. Based on the bounded space constructed by reciprocal points, the risk of unknown is reduced through multi-category interaction. The novel learning framework called Reciprocal Point Learning (RPL), which can indirectly introduce the unknown information into the learner with only known classes, so as to learn more compact and discriminative representations. Moreover, we further construct a new large-scale challenging aircraft dataset for open set recognition: Aircraft 300 (Air-300). Extensive experiments on multiple benchmark datasets indicate that our framework is significantly superior to other existing approaches and achieves state-of-the-art performance on standard open set benchmarks.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_30');
INSERT INTO `paper` VALUES (12260, 'Learning Pairwise Inter-plane Relations for Piecewise Planar Reconstruction', 'Piecewise planar', 'Reconstruction', 'Deep learning', 'Single-view', '', 'This paper proposes a novel single-image piecewise planar reconstruction technique that infers and enforces inter-plane relationships. Our approach takes a planar reconstruction result from an existing system, then utilizes convolutional neural network (CNN) to (1) classify if two planes are orthogonal or parallel; and 2) infer if two planes are touching and, if so, where in the image. We formulate an optimization problem to refine plane parameters and employ a message passing neural network to refine plane segmentation masks by enforcing the inter-plane relations. Our qualitative and quantitative evaluations demonstrate the effectiveness of the proposed approach in terms of plane parameters and segmentation accuracy.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_20');
INSERT INTO `paper` VALUES (12261, 'Learning Permutation Invariant Representations Using Memory Networks', 'Permutation invariant models', 'Multi Instance Learning', 'Whole slide image classification', 'Medical images', '', 'Many real-world tasks such as classification of digital histopathology images and 3D object detection involve learning from a set of instances. In these cases, only a group of instances or a set, collectively, contains meaningful information and therefore only the sets have labels, and not individual data instances. In this work, we present a permutation invariant neural network called Memory-based Exchangeable Model (MEM) for learning universal set functions. The MEM model consists of memory units that embed an input sequence to high-level features enabling it to learn inter-dependencies among instances through a self-attention mechanism. We evaluated the learning ability of MEM on various toy datasets, point cloud classification, and classification of whole slide images (WSIs) into two subtypes of the lung cancer—Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma. We systematically extracted patches from WSIs of the lung, downloaded from The Cancer Genome Atlas (TCGA) dataset, the largest public repository of WSIs, achieving a competitive accuracy of 84.84% for classification of two sub-types of lung cancer. The results on other datasets are promising as well, and demonstrate the efficacy of our model.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_40');
INSERT INTO `paper` VALUES (12262, 'Learning Predictive Models from Observation and Interaction', 'Video prediction', 'Visual planning', 'Action representations', 'Robotic manipulation', '', 'Learning predictive models from interaction with the world allows an agent, such as a robot, to learn about how the world works, and then use this learned model to plan coordinated sequences of actions to bring about desired outcomes. However, learning a model that captures the dynamics of complex skills represents a major challenge: if the agent needs a good model to perform these skills, it might never be able to collect the experience on its own that is required to learn these delicate and complex behaviors. Instead, we can imagine augmenting the training set with observational data of other agents, such as humans. Such data is likely more plentiful, but cannot always be combined with data from the original agent. For example, videos of humans might show a robot how to use a tool, but (i) are not annotated with suitable robot actions, and (ii) contain a systematic distributional shift due to the embodiment differences between humans and robots. We address the first challenge by formulating the corresponding graphical model and treating the action as an observed variable for the interaction data and an unobserved variable for the observation data, and the second challenge by using a domain-dependent prior. In addition to interaction data, our method is able to leverage videos of passive observations in a driving dataset and a dataset of robotic manipulation videos to improve video prediction performance. In a real-world tabletop robotic manipulation setting, our method is able to significantly improve control performance by learning a model from both robot data and observations of humans.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_42');
INSERT INTO `paper` VALUES (12263, 'Learning Progressive Joint Propagation for Human Motion Prediction', '3D motion prediction', 'Transformer network', 'Progressive decoding', 'Dictionary module', '', 'Despite the great progress in human motion prediction, it remains a challenging task due to the complicated structural dynamics of human behaviors. In this paper, we address this problem in three aspects. First, to capture the long-range spatial correlations and temporal dependencies, we apply a transformer-based architecture with the global attention mechanism. Specifically, we feed the network with the sequential joints encoded with the temporal information for spatial and temporal explorations. Second, to further exploit the inherent kinematic chains for better 3D structures, we apply a progressive-decoding strategy, which performs in a central-to-peripheral extension according to the structural connectivity. Last, in order to incorporate a general motion space for high-quality prediction, we build a memory-based dictionary, which aims to preserve the global motion patterns in training data to guide the predictions. We evaluate the proposed method on two challenging benchmark datasets (Human3.6M and CMU-Mocap). Experimental results show our superior performance compared with the state-of-the-art approaches.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_14');
INSERT INTO `paper` VALUES (12264, 'Learning Propagation Rules for Attribution Map Generation', 'Propagation rules', 'Attributions maps', 'Learnable module', '', '', 'Prior gradient-based attribution-map methods rely on hand-crafted propagation rules for the non-linear/activation layers during the backward pass, so as to produce gradients of the input and then the attribution map. Despite the promising results achieved, such methods are sensitive to the non-informative high-frequency components and lack adaptability for various models and samples. In this paper, we propose a dedicated method to generate attribution maps that allow us to learn the propagation rules automatically, overcoming the flaws of the hand-crafted ones. Specifically, we introduce a learnable plugin module, which enables adaptive propagation rules for each pixel, to the non-linear layers during the backward pass for mask generating. The masked input image is then fed into the model again to obtain new output that can be used as a guidance when combined with the original one. The introduced learnable module can be trained under any auto-grad framework with higher-order differential support. As demonstrated on five datasets and six network architectures, the proposed method yields state-of-the-art results and gives cleaner and more visually plausible attribution maps.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_40');
INSERT INTO `paper` VALUES (12265, 'Learning Semantic Neural Tree for Human Parsing', 'Human parsing', 'Semantic neural tree', 'Semantic segmentation', '', '', 'In this paper, we design a novel semantic neural tree for human parsing, which uses a tree architecture to encode physiological structure of human body, and design a coarse to fine process in a cascade manner to generate accurate results. Specifically, the semantic neural tree is designed to segment human regions into multiple semantic sub-regions (e.g., face, arms, and legs) in a hierarchical way using a new designed attention routing module. Meanwhile, we introduce the semantic aggregation module to combine multiple hierarchical features to exploit more context information for better performance. Our semantic neural tree can be trained in an end-to-end fashion by standard stochastic gradient descent (SGD) with back-propagation. Several experiments conducted on four challenging datasets for both single and multiple human parsing, i.e., LIP, PASCAL-Person-Part, CIHP and MHP-v2, demonstrate the effectiveness of the proposed method. Code can be found at https://isrc.iscas.ac.cn/gitlab/research/sematree.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_13');
INSERT INTO `paper` VALUES (12266, 'Learning Stereo from Single Images', 'Stereo matching', 'Correspondence training data', '', '', '', 'Supervised deep networks are among the best methods for finding correspondences in stereo image pairs. Like all supervised approaches, these networks require ground truth data during training. However, collecting large quantities of accurate dense correspondence data is very challenging. We propose that it is unnecessary to have such a high reliance on ground truth depths or even corresponding stereo pairs. Inspired by recent progress in monocular depth estimation, we generate plausible disparity maps from single images. In turn, we use those flawed disparity maps in a carefully designed pipeline to generate stereo training pairs. Training in this manner makes it possible to convert any collection of single RGB images into stereo training data. This results in a significant reduction in human effort, with no need to collect real depths or to hand-design synthetic data. We can consequently train a stereo matching network from scratch on datasets like COCO, which were previously hard to exploit for stereo. Through extensive experiments we show that our approach outperforms stereo networks trained with standard synthetic datasets, when evaluated on KITTI, ETH3D, and Middlebury. Code to reproduce our results is available at https://github.com/nianticlabs/stereo-from-mono/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_42');
INSERT INTO `paper` VALUES (12267, 'Learning Structural Similarity of User Interface Layouts Using Graph Networks', '', '', '', '', '', 'We propose a novel representation learning technique for measuring the similarity of user interface designs. A triplet network is used to learn a search embedding for layout similarity, with a hybrid encoder-decoder backbone comprising a graph convolutional network (GCN) and convolutional decoder (CNN). The properties of interface components and their spatial relationships are encoded via a graph which also models the containment (nesting) relationships of interface components. We supervise the training of a dual reconstruction and pair-wise loss using an auxiliary measure of layout similarity based on intersection over union (IoU) distance. The resulting embedding is shown to exceed state of the art performance for visual search of user interface layouts over the public Rico dataset, and an auto-annotated dataset of interface layouts collected from the web. We release the codes and dataset (https://github.com/dips4717/gcn-cnn.)', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_44');
INSERT INTO `paper` VALUES (12268, 'Learning Surrogates via Deep Embedding', '', '', '', '', '', 'This paper proposes a technique for training a neural network by minimizing a surrogate loss that approximates the target evaluation metric, which may be non-differentiable. The surrogate is learned via a deep embedding where the Euclidean distance between the prediction and the ground truth corresponds to the value of the evaluation metric. The effectiveness of the proposed technique is demonstrated in a post-tuning setup, where a trained model is tuned using the learned surrogate. Without a significant computational overhead and any bells and whistles, improvements are demonstrated on challenging and practical tasks of scene-text recognition and detection. In the recognition task, the model is tuned using a surrogate approximating the edit distance metric and achieves up to \\(39\\%\\) relative improvement in the total edit distance. In the detection task, the surrogate approximates the intersection over union metric for rotated bounding boxes and yields up to \\(4.25\\%\\) relative improvement in the \\(F_{1}\\) score.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_13');
INSERT INTO `paper` VALUES (12269, 'Learning Temporally Invariant and Localizable Features via Data Augmentation for Video Recognition', '', '', '', '', '', 'Deep-Learning-based video recognition has shown promising improvements along with the development of large-scale datasets and spatiotemporal network architectures. In image recognition, learning spatially invariant features is a key factor in improving recognition performance and robustness. Data augmentation based on visual inductive priors, such as cropping, flipping, rotating, or photometric jittering, is a representative approach to achieve these features. Recent state-of-the-art recognition solutions have relied on modern data augmentation strategies that exploit a mixture of augmentation operations. In this study, we extend these strategies to the temporal dimension for videos to learn temporally invariant or temporally localizable features to cover temporal perturbations or complex actions in videos. Based on our novel temporal data augmentation algorithms, video recognition performances are improved using only a limited amount of training data compared to the spatial-only data augmentation algorithms, including the 1st Visual Inductive Priors (VIPriors) for data-efficient action recognition challenge. Furthermore, learned features are temporally localizable that cannot be achieved using spatial augmentation algorithms. Our source code is available at https://github.com/taeoh-kim/temporal_data_augmentation.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_27');
INSERT INTO `paper` VALUES (12270, 'Learning to Balance Specificity and Invariance for In and Out of Domain Generalization', 'Distribution shift', 'Domain generalization', '', '', '', 'We introduce Domain-specific Masks for Generalization, a model for improving both in-domain and out-of-domain generalization performance. For domain generalization, the goal is to learn from a set of source domains to produce a single model that will best generalize to an unseen target domain. As such, many prior approaches focus on learning representations which persist across all source domains with the assumption that these domain agnostic representations will generalize well. However, often individual domains contain characteristics which are unique and when leveraged can significantly aid in-domain recognition performance. To produce a model which best generalizes to both seen and unseen domains, we propose learning domain specific masks. The masks are encouraged to learn a balance of domain-invariant and domain-specific features, thus enabling a model which can benefit from the predictive power of specialized features while retaining the universal applicability of domain-invariant features. We demonstrate competitive performance compared to naive baselines and state-of-the-art methods on both PACS and DomainNet (Our code is available at https://github.com/prithv1/DMG).', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_18');
INSERT INTO `paper` VALUES (12271, 'Learning to Cluster Under Domain Shift', 'Unsupervised learning', 'Domain adaptation', 'Deep clustering', '', '', 'While unsupervised domain adaptation methods based on deep architectures have achieved remarkable success in many computer vision tasks, they rely on a strong assumption, i.e. labeled source data must be available. In this work we overcome this assumption and we address the problem of transferring knowledge from a source to a target domain when both source and target data have no annotations. Inspired by recent works on deep clustering, our approach leverages information from data gathered from multiple source domains to build a domain-agnostic clustering model which is then refined at inference time when target data become available. Specifically, at training time we propose to optimize a novel information-theoretic loss which, coupled with domain-alignment layers, ensures that our model learns to correctly discover semantic labels while discarding domain-specific features. Importantly, our architecture design ensures that at inference time the resulting source model can be effectively adapted to the target domain without having access to source data, thanks to feature alignment and self-supervision. We evaluate the proposed approach in a variety of settings (Code available at https://github.com/willi-menapace/acids-clustering-domain-shift), considering several domain adaptation benchmarks and we show that our method is able to automatically discover relevant semantic information even in presence of few target samples and yields state-of-the-art results on multiple domain adaptation benchmarks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_44');
INSERT INTO `paper` VALUES (12272, 'Learning to Combine: Knowledge Aggregation for Multi-source Domain Adaptation', 'Multi-Source Domain Adaptation', 'Learning to Combine', 'Knowledge graph', 'Relation Alignment Loss', '', 'Transferring knowledges learned from multiple source domains to target domain is a more practical and challenging task than conventional single-source domain adaptation. Furthermore, the increase of modalities brings more difficulty in aligning feature distributions among multiple domains. To mitigate these problems, we propose a Learning to Combine for Multi-Source Domain Adaptation (LtC-MSDA) framework via exploring interactions among domains. In the nutshell, a knowledge graph is constructed on the prototypes of various domains to realize the information propagation among semantically adjacent representations. On such basis, a graph model is learned to predict query samples under the guidance of correlated prototypes. In addition, we design a Relation Alignment Loss (RAL) to facilitate the consistency of categories’ relational interdependency and the compactness of features, which boosts features’ intra-class invariance and inter-class separability. Comprehensive results on public benchmark datasets demonstrate that our approach outperforms existing methods with a remarkable margin. Our code is available at https://github.com/ChrisAllenMing/LtC-MSDA.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_43');
INSERT INTO `paper` VALUES (12273, 'Learning to Compose Hypercolumns for Visual Correspondence', 'Visual correspondence', 'Multi-layer features', 'Dynamic feature composition', '', '', 'Feature representation plays a crucial role in visual correspondence, and recent methods for image matching resort to deeply stacked convolutional layers. These models, however, are both monolithic and static in the sense that they typically use a specific level of features, e.g., the output of the last layer, and adhere to it regardless of the images to match. In this work, we introduce a novel approach to visual correspondence that dynamically composes effective features by leveraging relevant layers conditioned on the images to match. Inspired by both multi-layer feature composition in object detection and adaptive inference architectures in classification, the proposed method, dubbed Dynamic Hyperpixel Flow, learns to compose hypercolumn features on the fly by selecting a small number of relevant layers from a deep convolutional neural network. We demonstrate the effectiveness on the task of semantic correspondence, i.e., establishing correspondences between images depicting different instances of the same object or scene category. Experiments on standard benchmarks show that the proposed method greatly improves matching performance over the state of the art in an adaptive and efficient manner.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_21');
INSERT INTO `paper` VALUES (12274, 'Learning to Count in the Crowd from Limited Labeled Data', 'Crowd counting', 'Semi-supervised learning', 'Pseudo-labeling', 'Domain adaptation', 'Synthetic to real transfer', 'Recent crowd counting approaches have achieved excellent performance. However, they are essentially based on fully supervised paradigm and require large number of annotated samples. Obtaining annotations is an expensive and labour-intensive process. In this work, we focus on reducing the annotation efforts by learning to count in the crowd from limited number of labeled samples while leveraging a large pool of unlabeled data. Specifically, we propose a Gaussian Process-based iterative learning mechanism that involves estimation of pseudo-ground truth for the unlabeled data, which is then used as supervision for training the network. The proposed method is shown to be effective under the reduced data (semi-supervised) settings for several datasets like ShanghaiTech, UCF-QNRF, WorldExpo, UCSD, etc. Furthermore, we demonstrate that the proposed method can be leveraged to enable the network in learning to count from synthetic dataset while being able to generalize better to real-world datasets (synthetic-to-real transfer).', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_13');
INSERT INTO `paper` VALUES (12275, 'Learning to Detect Open Classes for Universal Domain Adaptation', 'Universal domain adaptation', 'Open class detection', '', '', '', 'Universal domain adaptation (UniDA) transfers knowledge between domains without any constraint on the label sets, extending the applicability of domain adaptation in the wild. In UniDA, both the source and target label sets may hold individual labels not shared by the other domain. A de facto challenge of UniDA is to classify the target examples in the shared classes against the domain shift. A more prominent challenge of UniDA is to mark the target examples in the target-individual label set (open classes) as “unknown”. These two entangled challenges make UniDA a highly under-explored problem. Previous work on UniDA focuses on the classification of data in the shared classes and uses per-class accuracy as the evaluation metric, which is badly biased to the accuracy of shared classes. However, accurately detecting open classes is the mission-critical task to enable real universal domain adaptation. It further turns UniDA problem into a well-established close-set domain adaptation problem. Towards accurate open class detection, we propose Calibrated Multiple Uncertainties (CMU) with a novel transferability measure estimated by a mixture of uncertainty quantities in complementation: entropy, confidence and consistency, defined on conditional probabilities calibrated by a multi-classifier ensemble model. The new transferability measure accurately quantifies the inclination of a target example to the open classes. We also propose a novel evaluation metric called H-score, which emphasizes the importance of both accuracies of the shared classes and the “unknown” class. Empirical results under the UniDA setting show that CMU outperforms the state-of-the-art domain adaptation methods on all the evaluation metrics, especially by a large margin on the H-score.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_34');
INSERT INTO `paper` VALUES (12276, 'Learning to Exploit Multiple Vision Modalities by Using Grafted Networks', 'Network Grafting Algorithm', 'Self-supervised learning', 'Thermal camera', 'Event-based vision', 'Object detection', 'Novel vision sensors such as thermal, hyperspectral, polarization, and event cameras provide information that is not available from conventional intensity cameras. An obstacle to using these sensors with current powerful deep neural networks is the lack of large labeled training datasets. This paper proposes a Network Grafting Algorithm (NGA), where a new front end network driven by unconventional visual inputs replaces the front end network of a pretrained deep network that processes intensity frames. The self-supervised training uses only synchronously-recorded intensity frames and novel sensor data to maximize feature similarity between the pretrained network and the grafted network. We show that the enhanced grafted network reaches competitive average precision (AP\\(_{50}\\)) scores to the pretrained network on an object detection task using thermal and event camera datasets, with no increase in inference costs. Particularly, the grafted network driven by thermal frames showed a relative improvement of 49.11% over the use of intensity frames. The grafted front end has only 5–8% of the total parameters and can be trained in a few hours on a single GPU equivalent to 5% of the time that would be needed to train the entire object detector from labeled data. NGA allows new vision sensors to capitalize on previously pretrained powerful deep models, saving on training cost and widening a range of applications for novel sensors.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_6');
INSERT INTO `paper` VALUES (12277, 'Learning to Factorize and Relight a City', '', '', '', '', '', 'We propose a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. Inspired by the classic intrinsic image decomposition, our learning signal builds upon two insights: 1) combining the disentangled factors should reconstruct the original image, and 2) the permanent factors should stay constant across multiple temporal samples of the same scene. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View, where the same locations are captured repeatedly through time. This data represents an unprecedented scale of spatio-temporal outdoor imagery. We show that our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry. Please visit http://factorize-a-city.github.io/ for animated results.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_32');
INSERT INTO `paper` VALUES (12278, 'Learning to Generate Customized Dynamic 3D Facial Expressions', 'Expression generation', 'Facial animation', '4D synthesis', '4DFAB', 'Graph neural networks', 'Recent advances in deep learning have significantly pushed the state-of-the-art in photorealistic video animation given a single image. In this paper, we extrapolate those advances to the 3D domain, by studying 3D image-to-video translation with a particular focus on 4D facial expressions. Although 3D facial generative models have been widely explored during the past years, 4D animation remains relatively unexplored. To this end, in this study we employ a deep mesh encoder-decoder like architecture to synthesize realistic high resolution facial expressions by using a single neutral frame along with an expression identification. In addition, processing 3D meshes remains a non-trivial task compared to data that live on grid-like structures, such as images. Given the recent progress in mesh processing with graph convolutions, we make use of a recently introduced learnable operator which acts directly on the mesh structure by taking advantage of local vertex orderings. In order to generalize to 4D facial expressions across subjects, we trained our model using a high resolution dataset with 4D scans of six facial expressions from 180 subjects. Experimental results demonstrate that our approach preserves the subject’s identity information even for unseen subjects and generates high quality expressions. To the best of our knowledge, this is the first study tackling the problem of 4D facial expression synthesis.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_17');
INSERT INTO `paper` VALUES (12279, 'Learning to Generate Grounded Visual Captions Without Localization Supervision', 'Image captioning', 'Video captioning', 'Self-supervised learning', 'Visual grounding', '', 'When automatically generating a sentence description for an image or video, it often remains unclear how well the generated caption is grounded, that is whether the model uses the correct image regions to output particular words, or if the model is hallucinating based on priors in the dataset and/or the language model. The most common way of relating image regions with words in caption models is through an attention mechanism over the regions that are used as input to predict the next word. The model must therefore learn to predict the attentional weights without knowing the word it should localize. This is difficult to train without grounding supervision since recurrent models can propagate past information and there is no explicit signal to force the captioning model to properly ground the individual decoded words. In this work, we help the model to achieve this via a novel cyclical training regimen that forces the model to localize each word in the image after the sentence decoder generates it, and then reconstruct the sentence from the localized image region(s) to match the ground-truth. Our proposed framework only requires learning one extra fully-connected layer (the localizer), a layer that can be removed at test time. We show that our model significantly improves grounding accuracy without relying on grounding supervision or introducing extra computation during inference, for both image and video captioning tasks. Code is available at https://github.com/chihyaoma/cyclical-visual-captioning.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_21');
INSERT INTO `paper` VALUES (12280, 'Learning to Generate Novel Domains for Domain Generalization', '', '', '', '', '', 'This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model’s ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport) outperforms current state-of-the-art DG methods on four benchmark datasets.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_33');
INSERT INTO `paper` VALUES (12281, 'Learning to Improve Image Compression Without Changing the Standard Decoder', 'DNN', 'Image compression', 'Decoder compatibility', '', '', 'In recent years we have witnessed an increasing interest in applying Deep Neural Networks (DNNs) to improve the rate-distortion performance in image compression. However, the existing approaches either train a post-processing DNN on the decoder side, or propose learning for image compression in an end-to-end manner. This way, the trained DNNs are required in the decoder, leading to the incompatibility to the standard image decoders (e.g., JPEG) in personal computers and mobiles. Therefore, we propose learning to improve the encoding performance with the standard decoder. In this paper, We work on JPEG as an example. Specifically, a frequency-domain pre-editing method is proposed to optimize the distribution of DCT coefficients, aiming at facilitating the JPEG compression. Moreover, we propose learning the JPEG quantization table jointly with the pre-editing network. Most importantly, we do not modify the JPEG decoder and therefore our approach is applicable when viewing images with the widely used standard JPEG decoder. The experiments validate that our approach successfully improves the rate-distortion performance of JPEG in terms of various quality metrics, such as PSNR, MS-SSIM and LPIPS. Visually, this translates to better overall color retention especially when strong compression is applied.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_12');
INSERT INTO `paper` VALUES (12282, 'Learning to Learn in a Semi-supervised Fashion', '', '', '', '', '', 'To address semi-supervised learning from both labeled and unlabeled data, we present a novel meta-learning scheme. We particularly consider that labeled and unlabeled data share disjoint ground truth label sets, which can be seen tasks like in person re-identification or image retrieval. Our learning scheme exploits the idea of leveraging information from labeled to unlabeled data. Instead of fitting the associated class-wise similarity scores as most meta-learning algorithms do, we propose to derive semantics-oriented similarity representations from labeled data, and transfer such representation to unlabeled ones. Thus, our strategy can be viewed as a self-supervised learning scheme, which can be applied to fully supervised learning tasks for improved performance. Our experiments on various tasks and settings confirm the effectiveness of our proposed approach and its superiority over the state-of-the-art methods.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_27');
INSERT INTO `paper` VALUES (12283, 'Learning to Learn Parameterized Classification Networks for Scalable Input Images', 'Efficient neural networks', 'Visual classification', 'Scale deviation', 'Meta learning', 'Knowledge Distillation', 'Convolutional Neural Networks (CNNs) do not have a predictable recognition behavior with respect to the input resolution change. This prevents the feasibility of deployment on different input image resolutions for a specific model. To achieve efficient and flexible image classification at runtime, we employ meta learners to generate convolutional weights of main networks for various input scales and maintain privatized Batch Normalization layers per scale. For improved training performance, we further utilize knowledge distillation on the fly over model predictions based on different input resolutions. The learned meta network could dynamically parameterize main networks to act on input images of arbitrary size with consistently better accuracy compared to individually trained models. Extensive experiments on the ImageNet demonstrate that our method achieves an improved accuracy-efficiency trade-off during the adaptive inference process. By switching executable input resolutions, our method could satisfy the requirement of fast adaption in different resource-constrained environments. Code and models are available at https://github.com/d-li14/SAN.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_2');
INSERT INTO `paper` VALUES (12284, 'Learning to Learn with Variational Information Bottleneck for Domain Generalization', 'Meta learning', 'Domain generalization', 'Variational inference', 'Information bottleneck', '', 'Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_12');
INSERT INTO `paper` VALUES (12285, 'Learning to Learn Words from Visual Scenes', '', '', '', '', '', 'Language acquisition is the process of learning words from the surrounding scene. We introduce a meta-learning framework that learns how to learn word representations from unconstrained scenes. We leverage the natural compositional structure of language to create training episodes that cause a meta-learner to learn strong policies for language acquisition. Experiments on two datasets show that our approach is able to more rapidly acquire novel words as well as more robustly generalize to unseen compositions, significantly outperforming established baselines. A key advantage of our approach is that it is data efficient, allowing representations to be learned from scratch without language pre-training. Visualizations and analysis suggest visual information helps our approach learn a rich cross-modal representation from minimal examples.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_26');
INSERT INTO `paper` VALUES (12286, 'Learning to Localize Actions from Moments', '', '', '', '', '', 'With the knowledge of action moments (i.e., trimmed video clips that each contains an action instance), humans could routinely localize an action temporally in an untrimmed video. Nevertheless, most practical methods still require all training videos to be labeled with temporal annotations (action category and temporal boundary) and develop the models in a fully-supervised manner, despite expensive labeling efforts and inapplicable to new categories. In this paper, we introduce a new design of transfer learning type to learn action localization for a large set of action categories, but only on action moments from the categories of interest and temporal annotations of untrimmed videos from a small set of action classes. Specifically, we present Action Herald Networks (AherNet) that integrate such design into an one-stage action localization framework. Technically, a weight transfer function is uniquely devised to build the transformation between classification of action moments or foreground video segments and action localization in synthetic contextual moments or untrimmed videos. The context of each moment is learnt through the adversarial mechanism to differentiate the generated features from those of background in untrimmed videos. Extensive experiments are conducted on the learning both across the splits of ActivityNet v1.3 and from THUMOS14 to ActivityNet v1.3. Our AherNet demonstrates the superiority even comparing to most fully-supervised action localization methods. More remarkably, we train AherNet to localize actions from 600 categories on the leverage of action moments in Kinetics-600 and temporal annotations from 200 classes in ActivityNet v1.3.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_9');
INSERT INTO `paper` VALUES (12287, 'Learning to Optimize Domain Specific Normalization for Domain Generalization', 'Domain generalization', '', '', '', '', 'We propose a simple but effective multi-source domain generalization technique based on deep neural networks by incorporating optimized normalization layers that are specific to individual domains. Our approach employs multiple normalization methods while learning separate affine parameters per domain. For each domain, the activations are normalized by a weighted average of multiple normalization statistics. The normalization statistics are kept track of separately for each normalization type if necessary. Specifically, we employ batch and instance normalizations in our implementation to identify the best combination of these two normalization methods in each domain. The optimized normalization layers are effective to enhance the generalizability of the learned model. We demonstrate the state-of-the-art accuracy of our algorithm in the standard domain generalization benchmarks, as well as viability to further tasks such as multi-source domain adaptation and domain generalization in the presence of label noise.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_5');
INSERT INTO `paper` VALUES (12288, 'Learning to Plan with Uncertain Topological Maps', 'Visual navigation', 'Topological maps', 'Graph neural networks', '', '', 'We train an agent to navigate in 3D environments using a hierarchical strategy including a high-level graph based planner and a local policy. Our main contribution is a data driven learning based approach for planning under uncertainty in topological maps, requiring an estimate of shortest paths in valued graphs with a probabilistic structure. Whereas classical symbolic algorithms achieve optimal results on noise-less topologies, or optimal results in a probabilistic sense on graphs with probabilistic structure, we aim to show that machine learning can overcome missing information in the graph by taking into account rich high-dimensional node features, for instance visual information available at each location of the map. Compared to purely learned neural white box algorithms, we structure our neural model with an inductive bias for dynamic programming based shortest path algorithms, and we show that a particular parameterization of our neural model corresponds to the Bellman-Ford algorithm. By performing an empirical analysis of our method in simulated photo-realistic 3D environments, we demonstrate that the inclusion of visual features in the learned neural planner outperforms classical symbolic solutions for graph based planning.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_28');
INSERT INTO `paper` VALUES (12289, 'Learning to Predict Context-Adaptive Convolution for Semantic Segmentation', 'Semantic segmentation', 'Dynamic filter', 'Feature weighting', '', '', 'Long-range contextual information is essential for achieving high-performance semantic segmentation. Previous feature re-weighting methods demonstrate that using global context for re-weighting feature channels can effectively improve the accuracy of semantic segmentation. However, the globally-sharing feature re-weighting vector might not be optimal for regions of different classes in the input image. In this paper, we propose a Context-adaptive Convolution Network (CaC-Net) to predict a spatially-varying feature weighting vector for each spatial location of the semantic feature maps. In CaC-Net, a set of context-adaptive convolution kernels are predicted from the global contextual information in a parameter-efficient manner. When used for convolution with the semantic feature maps, the predicted convolutional kernels can generate the spatially-varying feature weighting factors capturing both global and local contextual information. Comprehensive experimental results show that our CaC-Net achieves superior segmentation performance on three public datasets, PASCAL Context, PASCAL VOC 2012 and ADE20K.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_46');
INSERT INTO `paper` VALUES (12290, 'Learning to Predict Salient Faces: A Novel Visual-Audio Saliency Model', 'Visual-audio', 'Saliency prediction', 'Multiple-face video.', '', '', 'Recently, video streams have occupied a large proportion of Internet traffic, most of which contain human faces. Hence, it is necessary to predict saliency on multiple-face videos, which can provide attention cues for many content based applications. However, most of multiple-face saliency prediction works only consider visual information and ignore audio, which is not consistent with the naturalistic scenarios. Several behavioral studies have established that sound influences human attention, especially during the speech turn-taking in multiple-face videos. In this paper, we thoroughly investigate such influences by establishing a large-scale eye-tracking database of Multiple-face Video in Visual-Audio condition (MVVA). Inspired by the findings of our investigation, we propose a novel multi-modal video saliency model consisting of three branches: visual, audio and face. The visual branch takes the RGB frames as the input and encodes them into visual feature maps. The audio and face branches encode the audio signal and multiple cropped faces, respectively. A fusion module is introduced to integrate the information from three modalities, and to generate the final saliency map. Experimental results show that the proposed method outperforms 11 state-of-the-art saliency prediction works. It performs closer to human multi-modal attention.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_25');
INSERT INTO `paper` VALUES (12291, 'Learning to Restore ssTEM Images from Deformation and Corruption', 'Degradation modeling', 'Image restoration', 'Deep learning', '', '', 'Serial section transmission electron microscopy (ssTEM) plays an important role in biological research. Due to the imperfect sample preparation, however, ssTEM images suffer from inevitable artifacts that pose huge challenges for the subsequent analysis and visualization. In this paper, we propose a novel strategy for modeling the main type of degradation, i.e., Support Film Folds (SFF), by characterizing this degradation process as a combination of content deformation and corruption. Relying on that, we then synthesize a sufficient amount of paired samples (degraded/groundtruth), which enables the training of a tailored deep restoration network. To the best of our knowledge, this is the first learning-based framework for ssTEM image restoration. Experiments on both synthetic and real test data demonstrate the superior performance of our proposed method over existing solutions, in terms of both image restoration quality and neuron segmentation accuracy.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_26');
INSERT INTO `paper` VALUES (12292, 'Learning to Scale Multilingual Representations for Vision-Language Tasks', 'Scalable vision-language models', 'Multilingual word embeddings', 'Image-sentence retrieval', '', '', 'Current multilingual vision-language models either require a large number of additional parameters for each supported language, or suffer performance degradation as languages are added. In this paper, we-9*6 propose a Scalable Multilingual Aligned Language Representation (SMALR) that supports many languages with few model parameters without sacrificing downstream task performance. SMALR learns a fixed size language-agnostic representation for most words in a multilingual vocabulary, keeping language-specific features for just a few. We use a masked cross-language modeling loss to align features with context from other languages. Additionally, we propose a cross-lingual consistency module that ensures predictions made for a query and its machine translation are comparable. The effectiveness of SMALR is demonstrated with ten diverse languages, over twice the number supported in vision-language tasks to date. We evaluate on multilingual image-sentence retrieval and outperform prior work by 3–4% with less than 1/5th the training parameters compared to other word embedding methods.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_12');
INSERT INTO `paper` VALUES (12293, 'Learning to See in the Dark with Events', 'Domain adaptation', 'Event camera', 'Image reconstruction', 'Low light imaging', '', 'Imaging in the dark environment is important for many real-world applications like video surveillance. Recently, the development of Event Cameras raises promising directions in solving this task thanks to its High Dynamic Range (HDR) and low requirement of computational sources. However, such cameras record sparse, asynchronous intensity changes of the scene (called events), instead of canonical images. In this paper, we propose learning to see in the dark by translating HDR events in low light to canonical sharp images as if captured in day light. Since it is extremely challenging to collect paired event-image training data, a novel unsupervised domain adaptation network is proposed that explicitly separates domain-invariant features (e.g. scene structures) from the domain-specific ones (e.g. detailed textures) to ease representation learning. A detail enhancing branch is proposed to reconstruct day light-specific features from the domain-invariant representations in a residual manner, regularized by a ranking loss. To evaluate the proposed approach, a novel large-scale dataset is captured with a DAVIS240C camera with both day/low light events and intensity images. Experiments on this dataset show that the proposed domain adaptation approach achieves superior performance than various state-of-the-art architectures.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_39');
INSERT INTO `paper` VALUES (12294, 'Learning to Segment Microscopy Images with Lazy Labels', 'Microscopy images', 'Multi-task learning', 'Convolutional neural networks', 'Image segmentation', '', 'The need for labour intensive pixel-wise annotation is a major limitation of many fully supervised learning methods for segmenting bioimages that can contain numerous object instances with thin separations. In this paper, we introduce a deep convolutional neural network for microscopy image segmentation. Annotation issues are circumvented by letting the network being trainable on coarse labels combined with only a very small number of images with pixel-wise annotations. We call this new labelling strategy ‘lazy’ labels. Image segmentation is stratified into three connected tasks: rough inner region detection, object separation and pixel-wise segmentation. These tasks are learned in an end-to-end multi-task learning framework. The method is demonstrated on two microscopy datasets, where we show that the model gives accurate segmentation results even if exact boundary labels are missing for a majority of annotated data. It brings more flexibility and efficiency for training deep neural networks that are data hungry and is applicable to biomedical images with poor contrast at the object boundaries or with diverse textures and repeated patterns.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_27');
INSERT INTO `paper` VALUES (12295, 'Learning to Separate: Detecting Heavily-Occluded Objects in Urban Scenes', '', '', '', '', '', 'While visual object detection with deep learning has received much attention in the past decade, cases when heavy intra-class occlusions occur have not been studied thoroughly. In this work, we propose a Non-Maximum-Suppression (NMS) algorithm that dramatically improves the detection recall while maintaining high precision in scenes with heavy occlusions. Our NMS algorithm is derived from a novel embedding mechanism, in which the semantic and geometric features of the detected boxes are jointly exploited. The embedding makes it possible to determine whether two heavily-overlapping boxes belong to the same object in the physical world. Our approach is particularly useful for car detection and pedestrian detection in urban scenes where occlusions often happen. We show the effectiveness of our approach by creating a model called SG-Det (short for Semantics and Geometry Detection) and testing SG-Det on two widely-adopted datasets, KITTI and CityPersons for which it achieves state-of-the-art performance. Our code is available at https://github.com/ChenhongyiYang/SG-NMS.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_31');
INSERT INTO `paper` VALUES (12296, 'Learning to Transfer Learn: Reinforcement Learning-Based Selection for Adaptive Transfer Learning', 'Transfer learning', 'Visual understanding', 'Reinforcement learning', '', '', 'We propose a novel adaptive transfer learning framework, learning to transfer learn (L2TL), to improve performance on a target dataset by careful extraction of the related information from a source dataset. Our framework considers cooperative optimization of shared weights between models for source and target tasks, and adjusts the constituent loss weights adaptively. The adaptation of the weights is based on a reinforcement learning (RL) selection policy, guided with a performance metric on the target validation set. We demonstrate that L2TL outperforms fine-tuning baselines and other adaptive transfer learning methods on eight datasets. In the regimes of small-scale target datasets and significant label mismatch between source and target datasets, L2TL shows particularly large benefits.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_21');
INSERT INTO `paper` VALUES (12297, 'Learning Trailer Moments in Full-Length Movies with Co-Contrastive Attention', 'Trailer moment detection', 'Video highlight detection', 'Co-contrastive attention', 'Weak supervision', 'Video feature augmentation', 'A movie’s key moments stand out of the screenplay to grab an audience’s attention and make movie browsing efficient. But a lack of annotations makes the existing approaches not applicable to movie key moment detection. To get rid of human annotations, we leverage the officially-released trailers as the weak supervision to learn a model that can detect the key moments from full-length movies. We introduce a novel ranking network that utilizes the Co-Attention between movies and trailers as guidance to generate the training pairs, where the moments highly corrected with trailers are expected to be scored higher than the uncorrelated moments. Additionally, we propose a Contrastive Attention module to enhance the feature representations such that the comparative contrast between features of the key and non-key moments are maximized. We construct the first movie-trailer dataset, and the proposed Co-Attention assisted ranking network shows superior performance even over the supervised(The term “supervised” refers to the approach with access to the manual ground-truth annotations for training.) approach. The effectiveness of our Contrastive Attention module is also demonstrated by the performance improvement over the state-of-the-art on the public benchmarks.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_18');
INSERT INTO `paper` VALUES (12298, 'Learning Visual Commonsense for Robust Scene Graph Generation', '', '', '', '', '', 'Scene graph generation models understand the scene through object and predicate recognition, but are prone to mistakes due to the challenges of perception in the wild. Perception errors often lead to nonsensical compositions in the output scene graph, which do not follow real-world rules and patterns, and can be corrected using commonsense knowledge. We propose the first method to acquire visual commonsense such as affordance and intuitive physics automatically from data, and use that to improve the robustness of scene understanding. To this end, we extend Transformer models to incorporate the structure of scene graphs, and train our Global-Local Attention Transformer on a scene graph corpus. Once trained, our model can be applied on any scene graph generation model and correct its obvious mistakes, resulting in more semantically plausible scene graphs. Through extensive experiments, we show our model learns commonsense better than any alternative, and improves the accuracy of state-of-the-art scene graph generation methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_38');
INSERT INTO `paper` VALUES (12299, 'Learning Visual Context by Comparison', 'Context modeling', 'Attention mechanism', 'Chest X-ray', '', '', 'Finding diseases from an X-ray image is an important yet highly challenging task. Current methods for solving this task exploit various characteristics of the chest X-ray image, but one of the most important characteristics is still missing: the necessity of comparison between related regions in an image. In this paper, we present Attend-and-Compare Module (ACM) for capturing the difference between an object of interest and its corresponding context. We show that explicit difference modeling can be very helpful in tasks that require direct comparison between locations from afar. This module can be plugged into existing deep learning models. For evaluation, we apply our module to three chest X-ray recognition tasks and COCO object detection & segmentation tasks and observe consistent improvements across tasks. The code is available at https://github.com/mk-minchul/attend-and-compare.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_34');
INSERT INTO `paper` VALUES (12300, 'Learning Visual Representations with Caption Annotations', '', '', '', '', '', 'Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce image-conditioned masked language modeling (ICMLM) – a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: https://europe.naverlabs.com/ICMLM.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_10');
INSERT INTO `paper` VALUES (12301, 'Learning What Makes a Difference from Counterfactual Examples and Gradient Supervision', '', '', '', '', '', 'One of the primary challenges limiting the applicability of deep learning is its susceptibility to learning spurious correlations rather than the underlying mechanisms of the task of interest. The resulting failure to generalise cannot be addressed by simply using more data from the same distribution. We propose an auxiliary training objective that improves the generalization capabilities of neural networks by leveraging an overlooked supervisory signal found in existing datasets. We use pairs of minimally-different examples with different labels, a.k.a counterfactual or contrasting examples, which provide a signal indicative of the underlying causal structure of the task. We show that such pairs can be identified in a number of existing datasets in computer vision (visual question answering, multi-label image classification) and natural language processing (sentiment analysis, natural language inference). The new training objective orients the gradient of a model’s decision function with pairs of counterfactual examples. Models trained with this technique demonstrate improved performance on out-of-distribution test sets.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_34');
INSERT INTO `paper` VALUES (12302, 'Learning What to Learn for Video Object Segmentation', '', '', '', '', '', 'Video object segmentation (VOS) is a highly challenging problem, since the target object is only defined by a first-frame reference mask during inference. The problem of how to capture and utilize this limited information to accurately segment the target remains a fundamental research question. We address this by introducing an end-to-end trainable VOS architecture that integrates a differentiable few-shot learner. Our learner is designed to predict a powerful parametric model of the target by minimizing a segmentation error in the first frame. We further go beyond the standard few-shot learning paradigm by learning what our target model should learn in order to maximize segmentation accuracy. We perform extensive experiments on standard benchmarks. Our approach sets a new state-of-the-art on the large-scale YouTube-VOS 2018 dataset by achieving an overall score of 81.5, corresponding to a \\(2.6\\%\\) relative improvement over the previous best result. The code and models are available at https://github.com/visionml/pytracking.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_46');
INSERT INTO `paper` VALUES (12303, 'Learning Where to Focus for Efficient Video Object Detection', 'Flow-warping', 'Learnable Spatio-Temporal Sampling', 'Spatial correspondences', 'Temporal relations', '', 'Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at LSTS.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_2');
INSERT INTO `paper` VALUES (12304, 'Learning with Noisy Class Labels for Instance Segmentation', 'Noisy class labels', 'Instance segmentation', 'Foreground-instance sub-task', 'Foreground-background sub-task', '', 'Instance segmentation has achieved siginificant progress in the presence of correctly annotated datasets. Yet, object classes in large-scale datasets are sometimes ambiguous, which easily causes confusion. In addition, limited experience and knowledge of annotators can also lead to mislabeled object classes. To solve this issue, a novel method is proposed in this paper, which uses different losses describing different roles of noisy class labels to enhance the learning. Specifically, in instance segmentation, noisy class labels play different roles in the foreground-background sub-task and the foreground-instance sub-task. Hence, on the one hand, the noise-robust loss (e.g., symmetric loss) is used to prevent incorrect gradient guidance for the foreground-instance sub-task. On the other hand, standard cross entropy loss is used to fully exploit correct gradient guidance for the foreground-background sub-task. Extensive experiments conducted with three popular datasets (i.e., Pascal VOC, Cityscapes and COCO) have demonstrated the effectiveness of our method in a wide range of noisy class labels scenarios. Code will be available at: github.com/longrongyang/LNCIS.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_3');
INSERT INTO `paper` VALUES (12305, 'Learning with Privileged Information for Efficient Image Super-Resolution', 'Privileged information', 'Super-resolution', 'Distillation', '', '', 'Convolutional neural networks (CNNs) have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Most SR methods based on CNNs have focused on achieving performance gains in terms of quality metrics, such as PSNR and SSIM, over classical approaches. They typically require a large amount of memory and computational units. FSRCNN, consisting of few numbers of convolutional layers, has shown promising results, while using an extremely small number of network parameters. We introduce in this paper a novel distillation framework, consisting of teacher and student networks, that allows to boost the performance of FSRCNN drastically. To this end, we propose to use ground-truth high-resolution (HR) images as privileged information. The encoder in the teacher learns the degradation process, subsampling of HR images, using an imitation loss. The student and the decoder in the teacher, having the same network architecture as FSRCNN, try to reconstruct HR images. Intermediate features in the decoder, affordable for the student to learn, are transferred to the student through feature distillation. Experimental results on standard benchmarks demonstrate the effectiveness and the generalization ability of our framework, which significantly boosts the performance of FSRCNN as well as other SR methods. Our code and model are available online: https://cvlab.yonsei.ac.kr/projects/PISR.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_28');
INSERT INTO `paper` VALUES (12306, 'Least Squares Surface Reconstruction on Arbitrary Domains', 'Height-from-gradient', 'Surface integration', 'Savitzky-Golay filter', 'Surface reconstruction', 'Least squares', 'Almost universally in computer vision, when surface derivatives are required, they are computed using only first order accurate finite difference approximations. We propose a new method for computing numerical derivatives based on 2D Savitzky-Golay filters and K-nearest neighbour kernels. The resulting derivative matrices can be used for least squares surface reconstruction over arbitrary (even disconnected) domains in the presence of large noise and allowing for higher order polynomial local surface approximations. They are useful for a range of tasks including normal-from-depth (i.e. surface differentiation), height-from-normals (i.e. surface integration) and shape-from-x. We show how to write both orthographic or perspective height-from-normals as a linear least squares problem using the same formulation and avoiding a nonlinear change of variables in the perspective case. We demonstrate improved performance relative to state-of-the-art across these tasks on both synthetic and real data and make available an open source implementation of our method.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_32');
INSERT INTO `paper` VALUES (12307, 'LEED: Label-Free Expression Editing via Disentanglement', 'Facial expression editing', 'Image synthesis', 'Disentangled representation learning', '', '', 'Recent studies on facial expression editing have obtained very promising progress. On the other hand, existing methods face the constraint of requiring a large amount of expression labels which are often expensive and time-consuming to collect. This paper presents an innovative label-free expression editing via disentanglement (LEED) framework that is capable of editing the expression of both frontal and profile facial images without requiring any expression label. The idea is to disentangle the identity and expression of a facial image in the expression manifold, where the neutral face captures the identity attribute and the displacement between the neutral image and the expressive image captures the expression attribute. Two novel losses are designed for optimal expression disentanglement and consistent synthesis, including a mutual expression information loss that aims to extract pure expression-related features and a siamese loss that aims to enhance the expression similarity between the synthesized image and the reference image. Extensive experiments over two public facial expression datasets show that LEED achieves superior facial expression editing qualitatively and quantitatively.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_46');
INSERT INTO `paper` VALUES (12308, 'LEMMA: A Multi-view Dataset for LEarning Multi-agent Multi-task Activities', 'Dataset', 'Multi-agent multi-task activities', 'Compositional action recognition', 'Action and task anticipations', 'Multiview', 'Understanding and interpreting human actions is a long-standing challenge and a critical indicator of perception in artificial intelligence. However, a few imperative components of daily human activities are largely missed in prior literature, including the goal-directed actions, concurrent multi-tasks, and collaborations among multi-agents. We introduce the LEMMA dataset to provide a single home to address these missing dimensions with meticulously designed settings, wherein the number of tasks and agents varies to highlight different learning objectives. We densely annotate the atomic-actions with human-object interactions to provide ground-truths of the compositionality, scheduling, and assignment of daily activities. We further devise challenging compositional action recognition and action/task anticipation benchmarks with baseline models to measure the capability of compositional action understanding and temporal reasoning. We hope this effort would drive the machine vision community to examine goal-directed human activities and further study the task scheduling and assignment in the real world.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_46');
INSERT INTO `paper` VALUES (12309, 'Length-Controllable Image Captioning', 'Controllable image captioning', 'Non-autoregressive model', '', '', '', 'The last decade has witnessed remarkable progress in the image captioning task; however, most existing methods cannot control their captions, e.g., choosing to describe the image either roughly or in detail. In this paper, we propose to use a simple length level embedding to endow them with this ability. Moreover, due to their autoregressive nature, the computational complexity of existing models increases linearly as the length of the generated captions grows. Thus, we further devise a non-autoregressive image captioning approach that can generate captions in a length-irrelevant complexity. We verify the merit of the proposed length level embedding on three models: two state-of-the-art (SOTA) autoregressive models with different types of decoder, as well as our proposed non-autoregressive model, to show its generalization ability. In the experiments, our length-controllable image captioning models not only achieve SOTA performance on the challenging MS COCO dataset but also generate length-controllable and diverse image captions. Specifically, our non-autoregressive model outperforms the autoregressive baselines in terms of controllability and diversity, and also significantly improves the decoding efficiency for long captions. Our code and models are released at https://github.com/bearcatt/LaBERT.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_42');
INSERT INTO `paper` VALUES (12310, 'Lensless Imaging with Focusing Sparse URA Masks in Long-Wave Infrared and Its Application for Human Detection', 'Lensless imaging', 'Long-wave infrared (LWIR) imaging', 'Diffractive optics', 'Image reconstruction', 'Diffraction simulation', 'We introduce a lensless imaging framework for contemporary computer vision applications in long-wavelength infrared (LWIR). The framework consists of two parts: a novel lensless imaging method that utilizes the idea of local directional focusing for optimal binary sparse coding, and lensless imaging simulator based on Fresnel-Kirchhoff diffraction approximation. Our lensless imaging approach, besides being computationally efficient, is calibration-free and allows for wide FOV imaging. We employ our lensless imaging simulation software for optimizing reconstruction parameters and for synthetic image generation for CNN training. We demonstrate the advantages of our framework on a dual-camera system (RGB-LWIR lensless), where we perform CNN-based human detection using the fused RGB-LWIR data.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_15');
INSERT INTO `paper` VALUES (12311, 'LevelSet R-CNN: A Deep Variational Method for Instance Segmentation', '', '', '', '', '', 'Obtaining precise instance segmentation masks is of high importance in many modern applications such as robotic manipulation and autonomous driving. Currently, many state of the art models are based on the Mask R-CNN framework which, while very powerful, outputs masks at low resolutions which could result in imprecise boundaries. On the other hand, classic variational methods for segmentation impose desirable global and local data and geometry constraints on the masks by optimizing an energy functional. While mathematically elegant, their direct dependence on good initialization, non-robust image cues and manual setting of hyperparameters renders them unsuitable for modern applications. We propose LevelSet R-CNN, which combines the best of both worlds by obtaining powerful feature representations that are combined in an end-to-end manner with a variational segmentation framework. We demonstrate the effectiveness of our approach on COCO and Cityscapes datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_33');
INSERT INTO `paper` VALUES (12312, 'Leveraging Acoustic Images for Effective Self-supervised Audio Representation Learning', 'Audio-visual representations', 'Acoustic images', 'Audio- and video-based classification', 'Cross-modal retrieval', 'Self-supervised learning', 'In this paper, we propose the use of a new modality characterized by a richer information content, namely acoustic images, for the sake of audio-visual scene understanding. Each pixel in such images is characterized by a spectral signature, associated to a specific direction in space and obtained by processing the audio signals coming from an array of microphones. By coupling such array with a video camera, we obtain spatio-temporal alignment of acoustic images and video frames. This constitutes a powerful source of self-supervision, which can be exploited in the learning pipeline we are proposing, without resorting to expensive data annotations. However, since 2D planar arrays are cumbersome and not as widespread as ordinary microphones, we propose that the richer information content of acoustic images can be distilled, through a self-supervised learning scheme, into more powerful audio and visual feature representations. The learnt feature representations can then be employed for downstream tasks such as classification and cross-modal retrieval, without the need of a microphone array. To prove that, we introduce a novel multimodal dataset consisting in RGB videos, raw audio signals and acoustic images, aligned in space and synchronized in time. Experimental results demonstrate the validity of our hypothesis and the effectiveness of the proposed pipeline, also when tested for tasks and datasets different from those used for training.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_8');
INSERT INTO `paper` VALUES (12313, 'Leveraging Seen and Unseen Semantic Relationships for Generative Zero-Shot Learning', 'Generalized zero-shot learning', 'Generative Modeling (GANs)', 'Seen and unseen relationship', '', '', 'Zero-shot learning (ZSL) addresses the unseen class recognition problem by leveraging semantic information to transfer knowledge from seen classes to unseen classes. Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. These generative models are trained using the seen classes and are expected to implicitly transfer the knowledge from seen to unseen classes. However, their performance is stymied by overfitting, which leads to substandard performance on Generalized Zero-Shot learning (GZSL). To address this concern, we propose the novel LsrGAN, a generative model that Leverages the Semantic Relationship between seen and unseen categories and explicitly performs knowledge transfer by incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss guides the LsrGAN to generate visual features that mirror the semantic relationships between seen and unseen classes. Experiments on seven benchmark datasets, including the challenging Wikipedia text-based CUB and NABirds splits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of the LsrGAN compared to previous state-of-the-art approaches under both ZSL and GZSL. Code is available at https://github.com/Maunil/LsrGAN.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_5');
INSERT INTO `paper` VALUES (12314, 'Lifespan Age Transformation Synthesis', '', '', '', '', '', 'We address the problem of single photo age progression and regression—the prediction of how a person might look in the future, or how they looked in the past. Most existing aging methods are limited to changing the texture, overlooking transformations in head shape that occur during the human aging and growth process. This limits the applicability of previous methods to aging of adults to slightly older adults, and application of those methods to photos of children does not produce quality results. We propose a novel multi-domain image-to-image generative adversarial network architecture, whose learned latent space models a continuous bi-directional aging process. The network is trained on the FFHQ dataset, which we labeled for ages, gender, and semantic segmentation. Fixed age classes are used as anchors to approximate continuous age transformation. Our framework can predict a full head portrait for ages 0–70 from a single photo, modifying both texture and shape of the head. We demonstrate results on a wide variety of photos and datasets, and show significant improvement over the state of the art.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_44');
INSERT INTO `paper` VALUES (12315, 'Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D', '', '', '', '', '', 'The goal of perception for autonomous vehicles is to extract semantic representations from multiple sensors and fuse these representations into a single “bird’s-eye-view” coordinate frame for consumption by motion planning. We propose a new end-to-end architecture that directly extracts a bird’s-eye-view representation of a scene given image data from an arbitrary number of cameras. The core idea behind our approach is to “lift” each image individually into a frustum of features for each camera, then “splat” all frustums into a rasterized bird’s-eye-view grid. By training on the entire camera rig, we provide evidence that our model is able to learn not only how to represent images but how to fuse predictions from all cameras into a single cohesive representation of the scene while being robust to calibration error. On standard bird’s-eye-view tasks such as object segmentation and map segmentation, our model outperforms all baselines and prior work. In pursuit of the goal of learning dense representations for motion planning, we show that the representations inferred by our model enable interpretable end-to-end motion planning by “shooting” template trajectories into a bird’s-eye-view cost map output by our network. We benchmark our approach against models that use oracle depth from lidar. Project page with code: https://nv-tlabs.github.io/lift-splat-shoot.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_12');
INSERT INTO `paper` VALUES (12316, 'LightNet: Deep Learning Based Illumination Estimation from Virtual Images', 'Augmented reality', 'Illumination estimation', 'Lighting direction', 'Virtual images', 'Densenet', 'In the era of virtual reality (VR), estimating illumination with lighting direction and lighting virtual objects has been a challenging problem. In VR, poor estimation of illumination and lighting direction makes any virtual objects into unrealistic. The inaccurate estimation of lighting can also cause strong artifacts in relighting of the virtual images. Inspired by these issues, the main objective of this paper is to enrich visual rationality of single image by providing accurate assessments of real illumination and lighting direction. We proposed a LightNet architecture by modelling Denseset121 network to estimate the light direction and color temperature level in any virtual reality images. We present quantitative results on VIDIT dataset to evaluate the performance and achieved good results in all the performance metrics. The experimental results proved that the proposed model is robust and provides a good level of accuracy in estimating illumination and lighting direction.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_34');
INSERT INTO `paper` VALUES (12317, 'Lightweight Action Recognition in Compressed Videos', 'Lightweight action recognition', 'Compressed videos', 'Temporal trilinear pooling', 'Knowledge distillation', '', 'Most existing action recognition models are large convolutional neural networks that work only with raw RGB frames as input. However, practical applications require lightweight models that directly process compressed videos. In this work, for the first time, such a model is developed, which is lightweight enough to run in real-time on embedded AI devices without sacrifices in recognition accuracy. A new Aligned Temporal Trilinear Pooling (ATTP) module is formulated to fuse three modalities in a compressed video. To remedy the weaker motion vectors (compared to optical flow computed from raw RGB streams) for representing dynamic content, we introduce a temporal fusion method to explicitly induce the temporal context, as well as knowledge distillation from a model trained with optical flows via feature alignment. Compared to existing compressed video action recognition models, it is much more compact and faster thanks to adopting a lightweight CNN backbone.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_24');
INSERT INTO `paper` VALUES (12318, 'Likelihood Landscapes: A Unifying Principle Behind Many Adversarial Defenses', 'Adversarial robustness', 'Understanding robustness', 'Deep learning', '', '', 'Convolutional Neural Networks have been shown to be vulnerable to adversarial examples, which are known to locate in subspaces close to where normal data lies but are not naturally occurring and of low probability. In this work, we investigate the potential effect defense techniques have on the geometry of the likelihood landscape - likelihood of the input images under the trained model. We first propose a way to visualize the likelihood landscape leveraging an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense techniques results in a similar effect of flattening the likelihood landscape. We further explore directly regularizing towards a flat landscape for adversarial robustness.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_3');
INSERT INTO `paper` VALUES (12319, 'LIMP: Learning Latent Shape Representations with Metric Preservation Priors', 'Learning shapes', 'Generative model', 'Metric distortion', '', '', 'In this paper, we advocate the adoption of metric preservation as a powerful prior for learning latent representations of deformable 3D shapes. Key to our construction is the introduction of a geometric distortion criterion, defined directly on the decoded shapes, translating the preservation of the metric on the decoding to the formation of linear paths in the underlying latent space. Our rationale lies in the observation that training samples alone are often insufficient to endow generative models with high fidelity, motivating the need for large training datasets. In contrast, metric preservation provides a rigorous way to control the amount of geometric distortion incurring in the construction of the latent space, leading in turn to synthetic samples of higher quality. We further demonstrate, for the first time, the adoption of differentiable intrinsic distances in the backpropagation of a geodesic loss. Our geometric priors are particularly relevant in the presence of scarce training data, where learning any meaningful latent structure can be especially challenging. The effectiveness and potential of our generative model is showcased in applications of style transfer, content generation, and shape completion.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_2');
INSERT INTO `paper` VALUES (12320, 'Linguistic Structure Guided Context Modeling for Referring Image Segmentation', 'Referring segmentation', 'Multimodal context', 'Linguistic structure', 'Graph propagation', 'Dependency Parsing Tree', 'Referring image segmentation aims to predict the foreground mask of the object referred by a natural language sentence. Multimodal context of the sentence is crucial to distinguish the referent from the background. Existing methods either insufficiently or redundantly model the multimodal context. To tackle this problem, we propose a “gather-propagate-distribute” scheme to model multimodal context by cross-modal interaction and implement this scheme as a novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which guides all the words to include valid multimodal context of the sentence while excluding disturbing ones through three steps over the multimodal feature, i.e., gathering, constrained propagation and distributing. Extensive experiments on four benchmarks demonstrate that our method outperforms all the previous state-of-the-arts.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_4');
INSERT INTO `paper` VALUES (12321, 'LIRA: Lifelong Image Restoration from Unknown Blended Distortions', 'Image restoration', 'Blended distortions', 'Lifelong learning', '', '', 'Most existing image restoration networks are designed in a disposable way and catastrophically forget previously learned distortions when trained on a new distortion removal task. To alleviate this problem, we raise the novel lifelong image restoration problem for blended distortions. We first design a base fork-join model in which multiple pre-trained expert models specializing in individual distortion removal task work cooperatively and adaptively to handle blended distortions. When the input is degraded by a new distortion, inspired by adult neurogenesis in human memory system, we develop a neural growing strategy where the previously trained model can incorporate a new expert branch and continually accumulate new knowledge without interfering with learned knowledge. Experimental results show that the proposed approach can not only achieve state-of-the-art performance on blended distortions removal tasks in both PSNR/SSIM metrics, but also maintain old expertise while learning new restoration tasks.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_36');
INSERT INTO `paper` VALUES (12322, 'LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation', '', '', '', '', '', 'Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_11');
INSERT INTO `paper` VALUES (12323, 'Local Correlation Consistency for Knowledge Distillation', 'Knowledge distillation', 'Local correlation consistency', 'Class-aware attention', '', '', 'Sufficient knowledge extraction from the teacher network plays a critical role in the knowledge distillation task to improve the performance of the student network. Existing methods mainly focus on the consistency of instance-level features and their relationships, but neglect the local features and their correlation, which also contain many details and discriminative patterns. In this paper, we propose the local correlation exploration framework for knowledge distillation. It models three kinds of local knowledge, including intra-instance local relationship, inter-instance relationship on the same local position, and the inter-instance relationship across different local positions. Moreover, to make the student focus on those informative local regions of the teacher’s feature maps, we propose a novel class-aware attention module to highlight the class-relevant regions and remove the confusing class-irrelevant regions, which makes the local correlation knowledge more accurate and valuable. We conduct extensive experiments and ablation studies on challenging datasets, including CIFAR100 and ImageNet, to show our superiority over the state-of-the-art methods.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_2');
INSERT INTO `paper` VALUES (12324, 'Localizing the Common Action Among a Few Videos', 'Common action localization', 'Few-shot learning', '', '', '', 'This paper strives to localize the temporal extent of an action in a long untrimmed video. Where existing work leverages many examples with their start, their ending, and/or the class of the action during training time, we propose few-shot common action localization. The start and end of an action in a long untrimmed video is determined based on just a hand-full of trimmed video examples containing the same action, without knowing their common class label. To address this task, we introduce a new 3D convolutional network architecture able to align representations from the support videos with the relevant query video segments. The network contains: (i) a mutual enhancement module to simultaneously complement the representation of the few trimmed support videos and the untrimmed query video; (ii) a progressive alignment module that iteratively fuses the support videos into the query branch; and (iii) a pairwise matching module to weigh the importance of different support videos. Evaluation of few-shot common action localization in untrimmed videos containing a single or multiple action instances demonstrates the effectiveness and general applicability of our proposal.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_30');
INSERT INTO `paper` VALUES (12325, 'Location Sensitive Image Retrieval and Tagging', '', '', '', '', '', 'People from different parts of the globe describe objects and concepts in distinct manners. Visual appearance can thus vary across different geographic locations, which makes location a relevant contextual information when analysing visual data. In this work, we address the task of image retrieval related to a given tag conditioned on a certain location on Earth. We present LocSens, a model that learns to rank triplets of images, tags and coordinates by plausibility, and two training strategies to balance the location influence in the final ranking. LocSens learns to fuse textual and location information of multimodal queries to retrieve related images at different levels of location granularity, and successfully utilizes location information to improve image tagging.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_38');
INSERT INTO `paper` VALUES (12326, 'Long-Term Human Motion Prediction with Scene Context', '', '', '', '', '', 'Human movement is goal-directed and influenced by the spatial layout of the objects in the scene. To plan future human motion, it is crucial to perceive the environment – imagine how hard it is to navigate a new room with lights off. Existing works on predicting human motion do not pay attention to the scene context and thus struggle in long-term prediction. In this work, we propose a novel three-stage framework that exploits scene context to tackle this task. Given a single scene image and 2D pose histories, our method first samples multiple human motion goals, then plans 3D human paths towards each goal, and finally predicts 3D human pose sequences following each path. For stable training and rigorous evaluation, we contribute a synthetic dataset with clean annotations. In both synthetic and real datasets, our method shows consistent quantitative and qualitative improvements over existing methods. Project page: https://people.eecs.berkeley.edu/~zhecao/hmp/index.html (Please refer to our arXiv for a longer version of the paper with more visualizations.)', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_23');
INSERT INTO `paper` VALUES (12327, 'Long-Term Human Video Generation of Multiple Futures Using Poses', 'Future video prediction', 'Long-term video generation', 'Human pose prediction', 'Generative adversarial network', '', 'Generating future video from an input video is a useful task for applications such as content creation and autonomous agents. Especially, prediction of human video is highly important. While most previous works predict a single future, multiple futures with different behavior can potentially occur. Moreover, if the predicted future is too short (e.g., less than one second), it may not be fully usable by a human or other systems. In this paper, we propose a novel method for future human pose prediction capable of predicting multiple long-term futures. This makes the predictions more suitable for real applications. After predicting future human motion, we generate future videos based on predicted poses. First, from an input human video, we generate sequences of future human poses (i.e., the image coordinates of their body-joints) via adversarial learning. Adversarial learning suffers from mode collapse, which makes it difficult to generate a variety of multiple poses. We solve this problem by utilizing two additional inputs to the generator to make the outputs diverse, namely, a latent code (to reflect various behaviors) and an attraction point (to reflect various trajectories). In addition, we generate long-term future human poses using a novel approach based on unidimensional convolutional neural networks. Last, we generate an output video based on the generated poses for visualization. We evaluate the generated future poses and videos using three criteria (i.e., realism, diversity and accuracy), and show that our proposed method outperforms other state-of-the-art works.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_36');
INSERT INTO `paper` VALUES (12328, 'Look Here! A Parametric Learning Based Approach to Redirect Visual Attention', 'Automatic image editing', 'Visual attention', 'Adversarial networks', '', '', 'Across photography, marketing, and website design, being able to direct the viewer’s attention is a powerful tool. Motivated by professional workflows, we introduce an automatic method to make an image region more attention-capturing via subtle image edits that maintain realism and fidelity to the original. From an input image and a user-provided mask, our GazeShiftNet model predicts a distinct set of global parametric transformations to be applied to the foreground and background image regions separately. We present the results of quantitative and qualitative experiments that demonstrate improvements over prior state-of-the-art. In contrast to existing attention shifting algorithms, our global parametric approach better preserves image semantics and avoids typical generative artifacts. Our edits enable inference at interactive rates on any image size, and easily generalize to videos. Extensions of our model allow for multi-style edits and the ability to both increase and attenuate attention in an image region. Furthermore, users can customize the edited images by dialing the edits up or down via interpolations in parameter space. This paper presents a practical tool that can simplify future image editing pipelines.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_21');
INSERT INTO `paper` VALUES (12329, 'Low Light Video Enhancement Using Synthetic Data Produced with an Intermediate Domain Mapping', '', '', '', '', '', 'Advances in low-light video RAW-to-RGB translation are opening up the possibility of fast low-light imaging on commodity devices (e.g. smartphone cameras) without the need for a tripod. However, it is challenging to collect the required paired short-long exposure frames to learn a supervised mapping. Current approaches require a specialised rig or the use of static videos with no subject or object motion, resulting in datasets that are limited in size, diversity, and motion. We address the data collection bottleneck for low-light video RAW-to-RGB by proposing a data synthesis mechanism, dubbed SIDGAN, that can generate abundant dynamic video training pairs. SIDGAN maps videos found ‘in the wild’ (e.g. internet videos) into a low-light (short, long exposure) domain. By generating dynamic video data synthetically, we enable a recently proposed state-of-the-art RAW-to-RGB model to attain higher image quality (improved colour, reduced artifacts) and improved temporal consistency, compared to the same model trained with only static real video data.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_7');
INSERT INTO `paper` VALUES (12330, 'LST-Net: Learning a Convolutional Neural Network with a Learnable Sparse Transform', 'CNN', 'Network architecture', 'Learnable sparse transform', '', '', 'The 2D convolutional (Conv2d) layer is the fundamental element to a deep convolutional neural network (CNN). Despite the great success of CNN, the conventional Conv2d is still limited in effectively reducing the spatial and channel-wise redundancy of features. In this paper, we propose to mitigate this issue by learning a CNN with a learnable sparse transform (LST), which converts the input features into a more compact and sparser domain so that the spatial and channel-wise redundancy can be more effectively reduced. The proposed LST can be efficiently implemented with existing CNN modules, such as point-wise and depth-wise separable convolutions, and it is portable to existing CNN architectures for seamless training and inference. We further present a hybrid soft thresholding and ReLU (ST-ReLU) activation scheme, making the trained network, namely LST-Net, more robust to image corruptions at the inference stage. Extensive experiments on CIFAR-10/100, ImageNet, ImageNet-C and Places365-Standard datasets validated that the proposed LST-Net can obtain even higher accuracy than its counterpart networks with fewer parameters and less overhead.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_33');
INSERT INTO `paper` VALUES (12331, 'MABNet: A Lightweight Stereo Network Based on Multibranch Adjustable Bottleneck Module', 'Stereo matching', 'Disparity estimation', 'Multibranch adjustable bottleneck module', 'Compact networks', '', 'Recently, end-to-end CNNs have presented remarkable performance for disparity estimation. But most of them are too heavy to resource-constrained devices, because of enormous parameters necessary for satisfactory results. To address the issue, we propose two compact stereo networks, MABNet and its light version MABNet_tiny. MABNet is based on a novel Multibranch Adjustable Bottleneck (MAB) module, which is less demanding on parameters and computation. In a MAB module, feature map is split into various parallel branches, where the depthwise separable convolutions with different dilation rates extract features with multiple receptive fields however at an affordable computational budget. Besides, the number of channels in each branch is adjustable independently to tradeoff computation and accuracy. On SceneFlow and KITTI datasets, our MABNet achieves competitive accuracy with fewer parameters of 1.65M. Especially, MABNet_tiny reduces the parameters 47K by cutting down the channels and layers in MABNet.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_21');
INSERT INTO `paper` VALUES (12332, 'Making Affine Correspondences Work in Camera Geometry Computation', '', '', '', '', '', 'Local features e.g. SIFT and its affine and learned variants provide region-to-region rather than point-to-point correspondences. This has recently been exploited to create new minimal solvers for classical problems such as homography, essential and fundamental matrix estimation. The main advantage of such solvers is that their sample size is smaller, e.g., only two instead of four matches are required to estimate a homography. Works proposing such solvers often claim a significant improvement in run-time thanks to fewer RANSAC iterations. We show that this argument is not valid in practice if the solvers are used naively. To overcome this, we propose guidelines for effective use of region-to-region matches in the course of a full model estimation pipeline. We propose a method for refining the local feature geometries by symmetric intensity-based matching, combine uncertainty propagation inside RANSAC with preemptive model verification, show a general scheme for computing uncertainty of minimal solvers results, and adapt the sample cheirality check for homography estimation. Our experiments show that affine solvers can achieve accuracy comparable to point-based solvers at faster run-times when following our guidelines. We make code available at https://github.com/danini/affine-correspondences-for-camera-geometry.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_42');
INSERT INTO `paper` VALUES (12333, 'Making an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors', '', '', '', '', '', 'We present a systematic study of the transferability of adversarial attacks on state-of-the-art object detection frameworks. Using standard detection datasets, we train patterns that suppress the objectness scores produced by a range of commonly used detectors, and ensembles of detectors. Through extensive experiments, we benchmark the effectiveness of adversarially trained patches under both white-box and black-box settings, and quantify transferability of attacks between datasets, object classes, and detector models. Finally, we present a detailed study of physical world attacks using printed posters and wearable clothes, and rigorously quantify the performance of such attacks with different metrics.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_1');
INSERT INTO `paper` VALUES (12334, 'Making Sense of CNNs: Interpreting Deep Representations and Their Invariances with INNs', '', '', '', '', '', 'To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_38');
INSERT INTO `paper` VALUES (12335, 'Malleable 2.5D Convolution: Learning Receptive Fields Along the Depth-Axis for RGB-D Scene Parsing', 'RGB-D scene parsing', 'Geometry in CNN', 'Malleable 2.5D convolution', '', '', 'Depth data provide geometric information that can bring progress in RGB-D scene parsing tasks. Several recent works propose RGB-D convolution operators that construct receptive fields along the depth-axis to handle 3D neighborhood relations between pixels. However, these methods pre-define depth receptive fields by hyperparameters, making them rely on parameter selection. In this paper, we propose a novel operator called malleable 2.5D convolution to learn the receptive field along the depth-axis. A malleable 2.5D convolution has one or more 2D convolution kernels. Our method assigns each pixel to one of the kernels or none of them according to their relative depth differences, and the assigning process is formulated as a differentiable form so that it can be learnt by gradient descent. The proposed operator runs on standard 2D feature maps and can be seamlessly incorporated into pre-trained CNNs. We conduct extensive experiments on two challenging RGB-D semantic segmentation dataset NYUDv2 and Cityscapes to validate the effectiveness and the generalization ability of our method.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_33');
INSERT INTO `paper` VALUES (12336, 'Manifold Projection for Adversarial Defense on Face Recognition', 'Face recognition', 'Adversarial defense', '', '', '', 'Although deep convolutional neural network based face recognition system has achieved remarkable success, it is susceptible to adversarial images: carefully constructed imperceptible perturbations can easily mislead deep neural networks. A recent study has shown that in addition to regular off-manifold adversarial images, there are also adversarial images on the manifold. In this paper, we propose Adversarial Variational AutoEncoder (A-VAE), a novel framework to tackle both types of attacks. We hypothesize that both off-manifold and on-manifold attacks move the image away from the high probability region of image manifold. We utilize variational autoencoder (VAE) to estimate the lower bound of the log-likelihood of image and explore to project the input images back into the high probability regions of image manifold again. At inference time, our model synthesizes multiple similar realizations of a given image by random sampling, then the nearest neighbor of the given image is selected as the final input of the face recognition model. As a preprocessing operation, our method is attack-agnostic and can adapt to a wide range of resolutions. The experimental results on LFW demonstrate that our method achieves state-of-the-art defense success rate against conventional off-manifold attacks such as FGSM, PGD, and C&W under both grey-box and white-box settings, and even on-manifold attack.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_18');
INSERT INTO `paper` VALUES (12337, 'Many-Shot from Low-Shot: Learning to Annotate Using Mixed Supervision for Object Detection', '', '', '', '', '', 'Object detection has witnessed significant progress by relying on large, manually annotated datasets. Annotating such datasets is highly time consuming and expensive, which motivates the development of weakly supervised and few-shot object detection methods. However, these methods largely underperform with respect to their strongly supervised counterpart, as weak training signals often result in partial or oversized detections. Towards solving this problem we introduce, for the first time, an online annotation module (OAM) that learns to generate a many-shot set of reliable annotations from a larger volume of weakly labelled images. Our OAM can be jointly trained with any fully supervised two-stage object detection method, providing additional training annotations on the fly. This results in a fully end-to-end strategy that only requires a low-shot set of fully annotated images. The integration of the OAM with Fast(er) R-CNN improves their performance by \\(17\\%\\) mAP, \\(9\\%\\) AP50 on PASCAL VOC 2007 and MS-COCO benchmarks, and significantly outperforms competing methods using mixed supervision.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_3');
INSERT INTO `paper` VALUES (12338, 'Mapillary Planet-Scale Depth Dataset', '', '', '', '', '', 'Learning-based methods produce remarkable results on single image depth tasks when trained on well-established benchmarks, however, there is a large gap from these benchmarks to real-world performance that is usually obscured by the common practice of fine-tuning on the target dataset. We introduce a new depth dataset that is an order of magnitude larger than previous datasets, but more importantly, contains an unprecedented gamut of locations, camera models and scene types while offering metric depth (not just up-to-scale). Additionally, we investigate the problem of training single image depth networks using images captured with many different cameras, validating an existing approach and proposing a simpler alternative. With our contributions we achieve excellent results on challenging benchmarks before fine-tuning, and set the state of the art on the popular KITTI dataset after fine-tuning.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_35');
INSERT INTO `paper` VALUES (12339, 'Mapping in a Cycle: Sinkhorn Regularized Unsupervised Learning for Point Cloud Shapes', 'Point cloud', 'Unsupervised learning', 'Dense correspondence', 'Cycle-consistency', '', 'We propose an unsupervised learning framework with the pretext task of finding dense correspondences between point cloud shapes from the same category based on the cycle-consistency formulation. In order to learn discriminative pointwise features from point cloud data, we incorporate in the formulation a regularization term based on Sinkhorn normalization to enhance the learned pointwise mappings to be as bijective as possible. Besides, a random rigid transform of the source shape is introduced to form a triplet cycle to improve the model’s robustness against perturbations. Comprehensive experiments demonstrate that the learned pointwise features through our framework benefits various point cloud analysis tasks, e.g. partial shape registration and keypoint transfer. We also show that the learned pointwise features can be leveraged by supervised methods to improve the part segmentation performance with either the full training dataset or just a small portion of it.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_27');
INSERT INTO `paper` VALUES (12340, 'Margin-Mix: Semi-Supervised Learning for Face Expression Recognition', 'Margin loss', 'Semi-supervised learning', 'Data mixup', 'Face expression recognition', '', 'In this paper, as we aim to construct a semi-supervised learning algorithm, we exploit the characteristics of the Deep Convolutional Networks to provide, for an input image, both an embedding descriptor and a prediction. The unlabeled data is combined with the labeled one in order to provide synthetic data, which describes better the input space. The network is asked to provide a large margin between clusters, while new data is self-labeled by the distance to class centroids, in the embedding space. The method is tested on standard benchmarks for semi-supervised learning, where it matches state of the art performance and on the problem of face expression recognition where it increases the accuracy by a noticeable margin .', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_1');
INSERT INTO `paper` VALUES (12341, 'Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting', 'Scene text', 'Detection', 'Recognition', '', '', 'Recent end-to-end trainable methods for scene text spotting, integrating detection and recognition, showed much progress. However, most of the current arbitrary-shape scene text spotters use region proposal networks (RPN) to produce proposals. RPN relies heavily on manually designed anchors and its proposals are represented with axis-aligned rectangles. The former presents difficulties in handling text instances of extreme aspect ratios or irregular shapes, and the latter often includes multiple neighboring instances into a single proposal, in cases of densely oriented text. To tackle these problems, we propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that adopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is anchor-free and gives accurate representations of arbitrary-shape proposals. It is therefore superior to RPN in detecting text instances of extreme aspect ratios or irregular shapes. Furthermore, the accurate proposals produced by SPN allow masked RoI features to be used for decoupling neighboring text instances. As a result, our Mask TextSpotter v3 can handle text instances of extreme aspect ratios or irregular shapes, and its recognition accuracy won’t be affected by nearby text or background noise. Specifically, we outperform state-of-the-art methods by 21.9% on the Rotated ICDAR 2013 dataset (rotation robustness), 5.9% on the Total-Text dataset (shape robustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset (aspect ratio robustness). Code is available at: https://github.com/MhLiao/MaskTextSpotterV3', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_41');
INSERT INTO `paper` VALUES (12342, 'Mask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve', '', '', '', '', '', 'Object recognition has seen significant progress in the image domain, with focus primarily on 2D perception. We propose to leverage existing large-scale datasets of 3D models to understand the underlying 3D structure of objects seen in an image by constructing a CAD-based representation of the objects and their poses. We present Mask2CAD, which jointly detects objects in real-world images and for each detected object, optimizes for the most similar CAD model and its pose. We construct a joint embedding space between the detected regions of an image corresponding to an object and 3D CAD models, enabling retrieval of CAD models for an input RGB image. This produces a clean, lightweight representation of the objects in an image; this CAD-based representation ensures a valid, efficient shape representation for applications such as content creation or interactive scenarios, and makes a step towards understanding the transformation of real-world imagery to a synthetic domain. Experiments on real-world images from Pix3D demonstrate the advantage of our approach in comparison to state of the art. To facilitate future research, we additionally propose a new image-to-3D baseline on ScanNet which features larger shape diversity, real-world occlusions, and challenging image views.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_16');
INSERT INTO `paper` VALUES (12343, 'Matching Guided Distillation', '', '', '', '', '', 'Feature distillation is an effective way to improve the performance for a smaller student model, which has fewer parameters and lower computation cost compared to the larger teacher model. Unfortunately, there is a common obstacle—the gap in semantic feature structure between the intermediate features of teacher and student. The classic scheme prefers to transform intermediate features by adding the adaptation module, such as naive convolutional, attention-based or more complicated one. However, this introduces two problems: a) The adaptation module brings more parameters into training. b) The adaptation module with random initialization or special transformation isn’t friendly for distilling a pre-trained student. In this paper, we present Matching Guided Distillation (MGD) as an efficient and parameter-free manner to solve these problems. The key idea of MGD is to pose matching the teacher channels with students’ as an assignment problem. We compare three solutions of the assignment problem to reduce channels from teacher features with partial distillation loss. The overall training takes a coordinate-descent approach between two optimization objects—assignments update and parameters update. Since MGD only contains normalization or pooling operations with negligible computation cost, it is flexible to plug into network with other distillation methods. The project site is http://kaiyuyue.com/mgd.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_19');
INSERT INTO `paper` VALUES (12344, 'MatryODShka: Real-time 6DoF Video View Synthesis Using Multi-sphere Images', '', '', '', '', '', 'We introduce a method to convert stereo 360\\(^\\circ \\) (omnidirectional stereo) imagery into a layered, multi-sphere image representation for six degree-of-freedom (6DoF) rendering. Stereo 360\\(^\\circ \\) imagery can be captured from multi-camera systems for virtual reality (VR), but lacks motion parallax and correct-in-all-directions disparity cues. Together, these can quickly lead to VR sickness when viewing content. One solution is to try and generate a format suitable for 6DoF rendering, such as by estimating depth. However, this raises questions as to how to handle disoccluded regions in dynamic scenes. Our approach is to simultaneously learn depth and disocclusions via a multi-sphere image representation, which can be rendered with correct 6DoF disparity and motion parallax in VR. This significantly improves comfort for the viewer, and can be inferred and rendered in real time on modern GPU hardware. Together, these move towards making VR video a more comfortable immersive medium.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_26');
INSERT INTO `paper` VALUES (12345, 'MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation', 'Video generation', 'Generative adversarial networks', 'Representation disentanglement', '', '', 'The synthesis of natural emotional reactions is an essential criterion in vivid talking-face video generation. This criterion is nevertheless seldom taken into consideration in previous works due to the absence of a large-scale, high-quality emotional audio-visual dataset. To address this issue, we build the Multi-view Emotional Audio-visual Dataset (MEAD), a talking-face video corpus featuring 60 actors and actresses talking with eight different emotions at three different intensity levels. High-quality audio-visual clips are captured at seven different view angles in a strictly-controlled environment. Together with the dataset, we release an emotional talking-face generation baseline that enables the manipulation of both emotion and its intensity. Our dataset could benefit a number of different research fields including conditional generation, cross-modal understanding and expression recognition. Code, model and data are publicly available on our project page \\(^{\\ddagger }\\) \\(^{\\ddagger }\\)https://wywu.github.io/projects/MEAD/MEAD.html.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_42');
INSERT INTO `paper` VALUES (12346, 'Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and Objects for 3D Hand Pose Estimation Under Hand-Object Interaction', '', '', '', '', '', 'We study how well different types of approaches generalise in the task of 3D hand pose estimation under single hand scenarios and hand-object interaction. We show that the accuracy of state-of-the-art methods can drop, and that they fail mostly on poses absent from the training set. Unfortunately, since the space of hand poses is highly dimensional, it is inherently not feasible to cover the whole space densely, despite recent efforts in collecting large-scale training datasets. This sampling problem is even more severe when hands are interacting with objects and/or inputs are RGB rather than depth images, as RGB images also vary with lighting conditions and colors. To address these issues, we designed a public challenge (HANDS’19) to evaluate the abilities of current 3D hand pose estimators (HPEs) to interpolate and extrapolate the poses of a training set. More exactly, HANDS’19 is designed (a) to evaluate the influence of both depth and color modalities on 3D hand pose estimation, under the presence or absence of objects; (b) to assess the generalisation abilities w.r.t. four main axes: shapes, articulations, viewpoints, and objects; (c) to explore the use of a synthetic hand models to fill the gaps of current datasets. Through the challenge, the overall accuracy has dramatically improved over the baseline, especially on extrapolation tasks, from 27 mm to 13 mm mean joint error. Our analyses highlight the impacts of: Data pre-processing, ensemble approaches, the use of a parametric 3D hand model (MANO), and different HPE methods/backbones.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_6');
INSERT INTO `paper` VALUES (12347, 'Measuring the Importance of Temporal Features in Video Saliency', 'Gaze prediction', 'Saliency', 'Video', 'Temporal modelling', 'Model evaluation', 'Where people look when watching videos is believed to be heavily influenced by temporal patterns. In this work, we test this assumption by quantifying to which extent gaze on recent video saliency benchmarks can be predicted by a static baseline model. On the recent LEDOV dataset, we find that at least 75% of the explainable information as defined by a gold standard model can be explained using static features. Our baseline model “DeepGaze MR” even outperforms state-of-the-art video saliency models, despite deliberately ignoring all temporal patterns. Visual inspection of our static baseline’s failure cases shows that clear temporal effects on human gaze placement exist, but are both rare in the dataset and not captured by any of the recent video saliency models. To focus the development of video saliency models on better capturing temporal effects we construct a meta-dataset consisting of those examples requiring temporal information.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_40');
INSERT INTO `paper` VALUES (12348, 'Memory Selection Network for Video Propagation', '', '', '', '', '', 'Video propagation is a fundamental problem in video processing where guidance frame predictions are propagated to guide predictions of the target frame. Previous research mainly treats the previous adjacent frame as guidance, which, however, could make the propagation vulnerable to occlusion, large motion and inaccurate information in the previous adjacent frame. To tackle this challenge, we propose a memory selection network, which learns to select suitable guidance from all previous frames for effective and robust propagation. Experimental results on video object segmentation and video colorization tasks show that our method consistently improves performance and can robustly handle challenging scenarios in video propagation.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_11');
INSERT INTO `paper` VALUES (12349, 'Memory-Augmented Dense Predictive Coding for Video Representation Learning', '', '', '', '', '', 'The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condensed representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of the learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_19');
INSERT INTO `paper` VALUES (12350, 'Memory-Efficient Incremental Learning Through Feature Adaptation', '', '', '', '', '', 'We introduce an approach for incremental learning that preserves feature descriptors of training images from previously learned classes, instead of the images themselves, unlike most existing work. Keeping the much lower-dimensional feature embeddings of images reduces the memory footprint significantly. We assume that the model is updated incrementally for new classes as new data becomes available sequentially. This requires adapting the previously stored feature vectors to the updated feature space without having access to the corresponding original training images. Feature adaptation is learned with a multi-layer perceptron, which is trained on feature pairs corresponding to the outputs of the original and updated network on a training image. We validate experimentally that such a transformation generalizes well to the features of the previous set of classes, and maps features to a discriminative subspace in the feature space. As a result, the classifier is optimized jointly over new and old classes without requiring old class images. Experimental results show that our method achieves state-of-the-art classification accuracy in incremental learning benchmarks, while having at least an order of magnitude lower memory footprint compared to image-preserving strategies.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_41');
INSERT INTO `paper` VALUES (12351, 'Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance', 'Mesh reconstruction', 'Point cloud', '', '', '', 'We are interested in reconstructing the mesh representation of object surfaces from point clouds. Surface reconstruction is a prerequisite for downstream applications such as rendering, collision avoidance for planning, animation, etc. However, the task is challenging if the input point cloud has a low resolution, which is common in real-world scenarios (e.g., from LiDAR or Kinect sensors). Existing learning-based mesh generative methods mostly predict the surface by first building a shape embedding that is at the whole object level, a design that causes issues in generating fine-grained details and generalizing to unseen categories. Instead, we propose to leverage the input point cloud as much as possible, by only adding connectivity information to existing points. Particularly, we predict which triplets of points should form faces. Our key innovation is a surrogate of local connectivity, calculated by comparing the intrinsic/extrinsic metrics. We learn to predict this surrogate using a deep point cloud network and then feed it to an efficient post-processing module for high-quality mesh generation. We demonstrate that our method can not only preserve details, handle ambiguous structures, but also possess strong generalizability to unseen categories by experiments on synthetic and real data.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_5');
INSERT INTO `paper` VALUES (12352, 'MessyTable: Instance Association in Multiple Camera Views', '', '', '', '', '', 'We present an interesting and challenging dataset that features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics that we assume good performance in object association. The dataset challenges existing methods in mining subtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association. We report interesting findings with some popular baselines, and discuss how this dataset could help inspire new problems and catalyse more robust formulations to tackle real-world instance association problems. (Project page: https://caizhongang.github.io/projects/MessyTable/.)', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_1');
INSERT INTO `paper` VALUES (12353, 'Meta-learning with Network Pruning', 'Meta-learning', 'Few-shot learning', 'Network pruning', 'Sparsity', 'Generalization analysis', 'Meta-learning is a powerful paradigm for few-shot learning. Although with remarkable success witnessed in many applications, the existing optimization based meta-learning models with over-parameterized neural networks have been evidenced to ovetfit on training tasks. To remedy this deficiency, we propose a network pruning based meta-learning approach for overfitting reduction via explicitly controlling the capacity of network. A uniform concentration analysis reveals the benefit of network capacity constraint for reducing generalization gap of the proposed meta-learner. We have implemented our approach on top of Reptile assembled with two network pruning routines: Dense-Sparse-Dense (DSD) and Iterative Hard Thresholding (IHT). Extensive experimental results on benchmark datasets with different over-parameterized deep networks demonstrate that our method not only effectively alleviates meta-overfitting but also in many cases improves the overall generalization performance when applied to few-shot classification tasks.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_40');
INSERT INTO `paper` VALUES (12354, 'Meta-rPPG: Remote Heart Rate Estimation Using a Transductive Meta-learner', 'Remote heart rate estimation', 'rPPG', 'Meta-learning', 'Transductive inference', '', 'Remote heart rate estimation is the measurement of heart rate without any physical contact with the subject and is accomplished using remote photoplethysmography (rPPG) in this work. rPPG signals are usually collected using a video camera with a limitation of being sensitive to multiple contributing factors, e.g. variation in skin tone, lighting condition and facial structure. End-to-end supervised learning approach performs well when training data is abundant, covering a distribution that doesn’t deviate too much from the distribution of testing data or during deployment. To cope with the unforeseeable distributional changes during deployment, we propose a transductive meta-learner that takes unlabeled samples during testing (deployment) for a self-supervised weight adjustment (also known as transductive inference), providing fast adaptation to the distributional changes. Using this approach, we achieve state-of-the-art performance on MAHNOB-HCI and UBFC-rPPG.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_24');
INSERT INTO `paper` VALUES (12355, 'Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data Generation', '', '', '', '', '', 'Procedural models are being widely used to synthesize scenes for graphics, gaming, and to create (labeled) synthetic datasets for ML. In order to produce realistic and diverse scenes, a number of parameters governing the procedural models have to be carefully tuned by experts. These parameters control both the structure of scenes being generated (e.g. how many cars in the scene), as well as parameters which place objects in valid configurations. Meta-Sim aimed at automatically tuning parameters given a target collection of real images in an unsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition to parameters, which is a challenging problem due to its discrete nature. Meta-Sim2 proceeds by learning to sequentially sample rule expansions from a given probabilistic scene grammar. Due to the discrete nature of the problem, we use Reinforcement Learning to train our model, and design a feature space divergence between our synthesized and target images that is key to successful training. Experiments on a real driving dataset show that, without any supervision, we can successfully learn to generate data that captures discrete structural statistics of objects, such as their frequency, in real images. We also show that this leads to downstream improvement in the performance of an object detector trained on our generated dataset as opposed to other baseline simulation methods. Project page: https://nv-tlabs.github.io/meta-sim-structure/.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_42');
INSERT INTO `paper` VALUES (12356, 'MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation', 'Knowledge distillation', 'Meta learning', '', '', '', 'Knowledge Distillation (KD) has been one of the most popular methods to learn a compact model. However, it still suffers from high demand in time and computational resources caused by sequential training pipeline. Furthermore, the soft targets from deeper models do not often serve as good cues for the shallower models due to the gap of compatibility. In this work, we consider these two problems at the same time. Specifically, we propose that better soft targets with higher compatibility can be generated by using a label generator to fuse the feature maps from deeper stages in a top-down manner, and we can employ the meta-learning technique to optimize this label generator. Utilizing the soft targets learned from the intermediate feature maps of the model, we can achieve better self-boosting of the network in comparison with the state-of-the-art. The experiments are conducted on two standard classification benchmarks, namely CIFAR-100 and ILSVRC2012. We test various network architectures to show the generalizability of our MetaDistiller. The experiments results on two datasets strongly demonstrate the effectiveness of our method .', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_41');
INSERT INTO `paper` VALUES (12357, 'Microscopy Image Restoration with Deep Wiener-Kolmogorov Filters', 'Deblurring', 'Denoising', 'Learnable regularizers', 'Microscopy deblurring', 'Wiener filter', 'Microscopy is a powerful visualization tool in biology, enabling the study of cells, tissues, and the fundamental biological processes; yet, the observed images typically suffer from blur and background noise. In this work, we propose a unifying framework of algorithms for Gaussian image deblurring and denoising. These algorithms are based on deep learning techniques for the design of learnable regularizers integrated into the Wiener-Kolmogorov filter. Our extensive experimentation line showcases that the proposed approach achieves a superior quality of image reconstruction and surpasses the solutions that rely either on deep learning or on optimization schemes alone. Augmented with the variance stabilizing transformation, the proposed reconstruction pipeline can also be successfully applied to the problem of Poisson image deblurring, surpassing the state-of-the-art methods. Moreover, several variants of the proposed framework demonstrate competitive performance at low computational complexity, which is of high importance for real-time imaging applications .', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_12');
INSERT INTO `paper` VALUES (12358, 'MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection', 'Object detection', 'Knowledge distillation', '', '', '', 'Modern object detection methods can be divided into one-stage approaches and two-stage ones. One-stage detectors are more efficient owing to straightforward architectures, but the two-stage detectors still take the lead in accuracy. Although recent work try to improve the one-stage detectors by imitating the structural design of the two-stage ones, the accuracy gap is still significant. In this paper, we propose MimicDet, a novel and efficient framework to train a one-stage detector by directly mimic the two-stage features, aiming to bridge the accuracy gap between one-stage and two-stage detectors. Unlike conventional mimic methods, MimicDet has a shared backbone for one-stage and two-stage detectors, then it branches into two heads which are well designed to have compatible features for mimicking. Thus MimicDet can be end-to-end trained without the pre-train of the teacher network. And the cost does not increase much, which makes it practical to adopt large networks as backbones. We also make several specialized designs such as dual-path mimicking and staggered feature pyramid to facilitate the mimicking process. Experiments on the challenging COCO detection benchmark demonstrate the effectiveness of MimicDet. It achieves 46.1 mAP with ResNeXt-101 backbone on the COCO test-dev set, which significantly surpasses current state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_32');
INSERT INTO `paper` VALUES (12359, 'Mind the Discriminability: Asymmetric Adversarial Domain Adaptation', 'Adversarial domain adaptation', 'Asymmetric training', '', '', '', 'Adversarial domain adaptation has made tremendous success by learning domain-invariant feature representations. However, conventional adversarial training pushes two domains together and brings uncertainty to feature learning, which deteriorates the discriminability in the target domain. In this paper, we tackle this problem by designing a simple yet effective scheme, namely Asymmetric Adversarial Domain Adaptation (AADA). We notice that source features preserve great feature discriminability due to full supervision, and therefore a novel asymmetric training scheme is designed to keep the source features fixed and encourage the target features approaching to the source features, which best preserves the feature discriminability learned from source labeled data. This is achieved by an autoencoder-based domain discriminator that only embeds the source domain, while the feature extractor learns to deceive the autoencoder by embedding the target domain. Theoretical justifications corroborate that our method minimizes the domain discrepancy and spectral analysis is employed to quantize the improved feature discriminability. Extensive experiments on several benchmarks validate that our method outperforms existing adversarial domain adaptation methods significantly and demonstrates robustness with respect to hyper-parameter sensitivity.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_35');
INSERT INTO `paper` VALUES (12360, 'MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection', '', '', '', '', '', 'We address the weakly supervised video highlight detection problem for learning to detect segments that are more attractive in training videos given their video event label but without expensive supervision of manually annotating highlight segments. While manually averting localizing highlight segments, weakly supervised modeling is challenging, as a video in our daily life could contain highlight segments with multiple event types, e.g., skiing and surfing. In this work, we propose casting weakly supervised video highlight detection modeling for a given specific event as a multiple instance ranking network (MINI-Net) learning. We consider each video as a bag of segments, and therefore, the proposed MINI-Net learns to enforce a higher highlight score for a positive bag that contains highlight segments of a specific event than those for negative bags that are irrelevant. In particular, we form a max-max ranking loss to acquire a reliable relative comparison between the most likely positive segment instance and the hardest negative segment instance. With this max-max ranking loss, our MINI-Net effectively leverages all segment information to acquire a more distinct video feature representation for localizing the highlight segments of a specific event in a video. The extensive experimental results on three challenging public benchmarks clearly validate the efficacy of our multiple instance ranking approach for solving the problem.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_21');
INSERT INTO `paper` VALUES (12361, 'Minimal Rolling Shutter Absolute Pose with Unknown Focal Length and Radial Distortion', '', '', '', '', '', 'The internal geometry of most modern consumer cameras is not adequately described by the perspective projection. Almost all cameras exhibit some radial lens distortion and are equipped with electronic rolling shutter that induces distortions when the camera moves during the image capture. When focal length has not been calibrated offline, the parameters that describe the radial and rolling shutter distortions are usually unknown. While, for global shutter cameras, minimal solvers for the absolute camera pose and unknown focal length and radial distortion are available, solvers for the rolling shutter were missing. We present the first minimal solutions for the absolute pose of a rolling shutter camera with unknown rolling shutter parameters, focal length, and radial distortion. Our new minimal solvers combine iterative schemes designed for calibrated rolling shutter cameras with fast generalized eigenvalue and Gröbner basis solvers. In a series of experiments, with both synthetic and real data, we show that our new solvers provide accurate estimates of the camera pose, rolling shutter parameters, focal length, and radial distortion parameters. The implementation of our solvers is available at github.com/CenekAlbl/RnP.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_41');
INSERT INTO `paper` VALUES (12362, 'Minimum Class Confusion for Versatile Domain Adaptation', 'Versatile domain adaptation', 'Minimum class confusion', '', '', '', 'There are a variety of Domain Adaptation (DA) scenarios subject to label sets and domain configurations, including closed-set and partial-set DA, as well as multi-source and multi-target DA. It is notable that existing DA methods are generally designed only for a specific scenario, and may underperform for scenarios they are not tailored to. To this end, this paper studies Versatile Domain Adaptation (VDA), where one method can handle several different DA scenarios without any modification. Towards this goal, a more general inductive bias other than the domain alignment should be explored. We delve into a missing piece of existing methods: class confusion, the tendency that a classifier confuses the predictions between the correct and ambiguous classes for target examples, which is common in different DA scenarios. We uncover that reducing such pairwise class confusion leads to significant transfer gains. With this insight, we propose a general loss function: Minimum Class Confusion (MCC). It can be characterized as (1) a non-adversarial DA method without explicitly deploying domain alignment, enjoying faster convergence speed; (2) a versatile approach that can handle four existing scenarios: Closed-Set, Partial-Set, Multi-Source, and Multi-Target DA, outperforming the state-of-the-art methods in these scenarios, especially on one of the largest and hardest datasets to date (\\(7.3\\%\\) on DomainNet). Its versatility is further justified by two scenarios proposed in this paper: Multi-Source Partial DA and Multi-Target Partial DA. In addition, it can also be used as a general regularizer that is orthogonal and complementary to a variety of existing DA methods, accelerating convergence and pushing these readily competitive methods to stronger ones. Code is available at https://github.com/thuml/Versatile-Domain-Adaptation.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_28');
INSERT INTO `paper` VALUES (12363, 'Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation', 'Semantic segmentation', 'Weakly supervised learning', '', '', '', 'This paper studies the problem of learning semantic segmentation from image-level supervision only. Current popular solutions leverage object localization maps from classifiers as supervision signals, and struggle to make the localization maps capture more complete object content. Rather than previous efforts that primarily focus on intra-image information, we address the value of cross-image semantic relations for comprehensive object pattern mining. To achieve this, two neural co-attentions are incorporated into the classifier to complimentarily capture cross-image semantic similarities and differences. In particular, given a pair of training images, one co-attention enforces the classifier to recognize the common semantics from co-attentive objects, while the other one, called contrastive co-attention, drives the classifier to identify the unshared semantics from the rest, uncommon objects. This helps the classifier discover more object patterns and better ground semantics in image regions. In addition to boosting object pattern learning, the co-attention can leverage context from other related images to improve localization map inference, hence eventually benefiting semantic segmentation learning. More essentially, our algorithm provides a unified framework that handles well different WSSS settings, i.e., learning WSSS with (1) precise image-level supervision only, (2) extra simple single-label data, and (3) extra noisy web data. It sets new state-of-the-arts on all these settings, demonstrating well its efficacy and generalizability.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_21');
INSERT INTO `paper` VALUES (12364, 'Mining Inter-Video Proposal Relations for Video Object Detection', 'Video object detection', 'Inter-Video Proposal Relation', 'Multi-level triplet selection', 'Hierachical Video Relation Network', '', 'Recent studies have shown that, context aggregating information from proposals in different frames can clearly enhance the performance of video object detection. However, these approaches mainly exploit the intra-proposal relation within single video, while ignoring the intra-proposal relation among different videos, which can provide important discriminative cues for recognizing confusing objects. To address the limitation, we propose a novel Inter-Video Proposal Relation module. Based on a concise multi-level triplet selection scheme, this module can learn effective object representations via modeling relations of hard proposals among different videos. Moreover, we design a Hierarchical Video Relation Network (HVR-Net), by integrating intra-video and inter-video proposal relations in a hierarchical fashion. This design can progressively exploit both intra and inter contexts to boost video object detection. We examine our method on the large-scale video object detection benchmark, i.e., ImageNet VID, where HVR-Net achieves the SOTA results. Codes and models are available at https://github.com/youthHan/HVRNet.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_26');
INSERT INTO `paper` VALUES (12365, 'Mining Self-similarity: Label Super-Resolution with Epitomic Representations', 'Label super-resolution', 'Semantic segmentation', 'Self-similarity', '', '', 'We show that simple patch-based models, such as epitomes (Jojic et al., 2003), can have superior performance to the current state of the art in semantic segmentation and label super-resolution, which uses deep convolutional neural networks. We derive a new training algorithm for epitomes which allows, for the first time, learning from very large data sets and derive a label super-resolution algorithm as a statistical inference over epitomic representations. We illustrate our methods on land cover mapping and medical image analysis tasks.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_32');
INSERT INTO `paper` VALUES (12366, 'Mitigating Dataset Imbalance via Joint Generation and Classification', '', '', '', '', '', 'Supervised deep learning methods are enjoying enormous success in many practical applications of computer vision and have the potential to revolutionize robotics. However, the marked performance degradation to biases and imbalanced data questions the reliability of these methods. In this work we address these questions from the perspective of dataset imbalance resulting out of severe under-representation of annotated training data for certain classes and its effect on both deep classification and generation methods. We introduce a joint dataset repairment strategy by combining a neural network classifier with Generative Adversarial Networks (GAN) that makes up for the deficit of training examples from the under-representated class by producing additional training examples. We show that the combined training helps to improve the robustness of both the classifier and the GAN against severe class imbalance. We show the effectiveness of our proposed approach on three very different datasets with different degrees of imbalance in them. The code is available at https://github.com/AadSah/ImbalanceCycleGAN.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_14');
INSERT INTO `paper` VALUES (12367, 'Mitigating Embedding and Class Assignment Mismatch in Unsupervised Image Classification', '', '', '', '', '', 'Unsupervised image classification is a challenging computer vision task. Deep learning-based algorithms have achieved superb results, where the latest approach adopts unified losses from embedding and class assignment processes. Since these processes inherently have different goals, jointly optimizing them may lead to a suboptimal solution. To address this limitation, we propose a novel two-stage algorithm in which an embedding module for pretraining precedes a refining module that concurrently performs embedding and class assignment. Our model outperforms SOTA when tested with multiple datasets, by substantially high accuracy of 81.0% for the CIFAR-10 dataset (i.e., increased by 19.3 percent points), 35.3% accuracy for CIFAR-100-20 (9.6 pp) and 66.5% accuracy for STL-10 (6.9 pp) in unsupervised tasks.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_45');
INSERT INTO `paper` VALUES (12368, 'Model-Agnostic Boundary-Adversarial Sampling for Test-Time Generalization in Few-Shot Learning', 'Few-shot learning', 'Meta-learning', 'Adversarial learning', '', '', 'Few-shot learning is an important research problem that tackles one of the greatest challenges of machine learning: learning a new task from a limited amount of labeled data. We propose a model-agnostic method that improves the test-time performance of any few-shot learning models with no additional training, and thus is free from the training-test domain gap. Based on only the few support samples in a meta-test task, our method generates the samples adversarial to the base few-shot classifier’s boundaries and fine-tunes its embedding function in the direction that increases the classification margins of the adversarial samples. Consequently, the embedding space becomes denser around the labeled samples which makes the classifier robust to query samples. Experimenting on miniImageNet, CIFAR-FS, and FC100, we demonstrate that our method brings significant performance improvement to three different base methods with various properties, and achieves the state-of-the-art performance in a number of few-shot learning tasks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_35');
INSERT INTO `paper` VALUES (12369, 'Model-Based Occlusion Disentanglement for Image-to-Image Translation', 'GAN', 'Image-to-image translation', 'Occlusions', 'Raindrop', 'Soil', 'Image-to-image translation is affected by entanglement phenomena, which may occur in case of target data encompassing occlusions such as raindrops, dirt, etc. Our unsupervised model-based learning disentangles scene and occlusions, while benefiting from an adversarial pipeline to regress physical parameters of the occlusion model. The experiments demonstrate our method is able to handle varying types of occlusions and generate highly realistic translations, qualitatively and quantitatively outperforming the state-of-the-art on multiple datasets.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_27');
INSERT INTO `paper` VALUES (12370, 'Modeling 3D Shapes by Reinforcement Learning', '', '', '', '', '', 'We explore how to enable machines to model 3D shapes like human modelers using deep reinforcement learning (RL). In 3D modeling software like Maya, a modeler usually creates a mesh model in two steps: (1) approximating the shape using a set of primitives; (2) editing the meshes of the primitives to create detailed geometry. Inspired by such artist-based modeling, we propose a two-step neural framework based on RL to learn 3D modeling policies. By taking actions and collecting rewards in an interactive environment, the agents first learn to parse a target shape into primitives and then to edit the geometry. To effectively train the modeling agents, we introduce a novel training algorithm that combines heuristic policy, imitation learning and reinforcement learning. Our experiments show that the agents can learn good policies to produce regular and structure-aware mesh models, which demonstrates the feasibility and effectiveness of the proposed RL framework .', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_32');
INSERT INTO `paper` VALUES (12371, 'Modeling Artistic Workflows for Image Generation and Editing', '', '', '', '', '', 'People often create art by following an artistic workflow involving multiple stages that inform the overall design. If an artist wishes to modify an earlier decision, significant work may be required to propagate this new decision forward to the final artwork. Motivated by the above observations, we propose a generative model that follows a given artistic workflow, enabling both multi-stage image generation as well as multi-stage image editing of an existing piece of art. Furthermore, for the editing scenario, we introduce an optimization process along with learning-based regularization to ensure the edited image produced by the model closely aligns with the originally provided image. Qualitative and quantitative results on three different artistic datasets demonstrate the effectiveness of the proposed framework on both image generation and editing tasks.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_10');
INSERT INTO `paper` VALUES (12372, 'Modeling the Effects of Windshield Refraction for Camera Calibration', '', '', '', '', '', 'In this paper, we study the effects of windshield refraction for autonomous driving applications. These distortion effects are surprisingly large and can not be explained by traditional camera models. Instead of using a generalized camera approach, we propose a novel approach to jointly optimize a traditional camera model, and a mathematical representation of the windshield’s surface. First, using the laws of geometric optics, the refraction is modeled using a local spherical approximation to the windshield’s geometry. Next, a spline-based model is proposed as a refinement to better adapt to deviations from the ideal shape and manufacturing variations. By jointly optimizing refraction and camera parameters, the projection error can be significantly reduced. The proposed models are validated on real windshield observations and custom setups to compare recordings with and without windshield, with accurate laser scan measurements as 3D ground truth.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_24');
INSERT INTO `paper` VALUES (12373, 'Modeling the Space of Point Landmark Constrained Diffeomorphisms', 'Teichmüller Map', 'Conformal geometry', 'Point landmark constrained diffeomorphism', '', '', 'Surface registration plays a fundamental role in shape analysis and geometric processing. Generally, there are three criteria in evaluating a surface mapping result: diffeomorphism, small distortion, and feature alignment. To fulfill these requirements, this work proposes a novel model of the space of point landmark constrained diffeomorphisms. Based on Teichmüller theory, this mapping space is generated by the Beltrami coefficients, which are infinitesimally Teichmüller equivalent to 0. These Beltrami coefficients are the solutions to a linear equation group. By using this theoretic model, optimal registrations can be achieved by iterative optimization with linear constraints in the diffeomorphism space, such as harmonic maps and Teichmüller maps, which minimize different types of distortion. The theoretical model is rigorous and has practical value. Our experimental results demonstrate the efficiency and efficacy of the proposed method.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_22');
INSERT INTO `paper` VALUES (12374, 'Momentum Batch Normalization for Deep Learning with Small Batch Size', 'Batch normalization', 'Small batch size', 'Noise', 'Momentum', '', 'Normalization layers play an important role in deep network training. As one of the most popular normalization techniques, batch normalization (BN) has shown its effectiveness in accelerating the model training speed and improving model generalization capability. The success of BN has been explained from different views, such as reducing internal covariate shift, allowing the use of large learning rate, smoothing optimization landscape, etc. To make a deeper understanding of BN, in this work we prove that BN actually introduces a certain level of noise into the sample mean and variance during the training process, while the noise level depends only on the batch size. Such a noise generation mechanism of BN regularizes the training process, and we present an explicit regularizer formulation of BN. Since the regularization strength of BN is determined by the batch size, a small batch size may cause the under-fitting problem, resulting in a less effective model. To reduce the dependency of BN on batch size, we propose a momentum BN (MBN) scheme by averaging the mean and variance of current mini-batch with the historical means and variances. With a dynamic momentum parameter, we can automatically control the noise level in the training process. As a result, MBN works very well even when the batch size is very small (e.g., 2), which is hard to achieve by traditional BN.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_14');
INSERT INTO `paper` VALUES (12375, 'Monocular 3D Object Detection via Feature Domain Adaptation', 'Monocular', '3D Object detection', 'Domain adaptation', 'Pseudo-Lidar', '', 'Monocular 3D object detection is a challenging task due to unreliable depth, resulting in a distinct performance gap between monocular and LiDAR-based approaches. In this paper, we propose a novel domain adaptation based monocular 3D object detection framework named DA-3Ddet, which adapts the feature from unsound image-based pseudo-LiDAR domain to the accurate real LiDAR domain for performance boosting. In order to solve the overlooked problem of inconsistency between the foreground mask of pseudo and real LiDAR caused by inaccurately estimated depth, we also introduce a context-aware foreground segmentation module which helps to involve relevant points for foreground masking. Extensive experiments on KITTI dataset demonstrate that our simple yet effective framework outperforms other state-of-the-arts by a large margin.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_2');
INSERT INTO `paper` VALUES (12376, 'Monocular Differentiable Rendering for Self-supervised 3D Object Detection', '', '', '', '', '', '3D object detection from monocular images is an ill-posed problem due to the projective entanglement of depth and scale. To overcome this ambiguity, we present a novel self-supervised method for textured 3D shape reconstruction and pose estimation of rigid objects with the help of strong shape priors and 2D instance masks. Our method predicts the 3D location and meshes of each object in an image using differentiable rendering and a self-supervised objective derived from a pretrained monocular depth estimation network. We use the KITTI 3D object detection dataset to evaluate the accuracy of the method. Experiments demonstrate that we can effectively use noisy monocular depth and differentiable rendering as an alternative to expensive 3D ground-truth labels or LiDAR information.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_31');
INSERT INTO `paper` VALUES (12377, 'Monocular Expressive Body Regression Through Body-Driven Attention', '', '', '', '', '', 'To understand how people look, interact, or perform tasks, we need to quickly and accurately capture their 3D body, face, and hands together from an RGB image. Most existing methods focus only on parts of the body. A few recent approaches reconstruct full expressive 3D humans from images using 3D body models that include the face and hands. These methods are optimization-based and thus slow, prone to local optima, and require 2D keypoints as input. We address these limitations by introducing ExPose (EXpressive POse and Shape rEgression), which directly regresses the body, face, and hands, in SMPL-X format, from an RGB image. This is a hard problem due to the high dimensionality of the body and the lack of expressive training data. Additionally, hands and faces are much smaller than the body, occupying very few image pixels. This makes hand and face estimation hard when body images are downscaled for neural networks. We make three main contributions. First, we account for the lack of training data by curating a dataset of SMPL-X fits on in-the-wild images. Second, we observe that body estimation localizes the face and hands reasonably well. We introduce body-driven attention for face and hand regions in the original image to extract higher-resolution crops that are fed to dedicated refinement modules. Third, these modules exploit part-specific knowledge from existing face- and hand-only datasets. ExPose estimates expressive 3D humans more accurately than existing optimization methods at a small fraction of the computational cost. Our data, model and code are available for research at https://expose.is.tue.mpg.de.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_2');
INSERT INTO `paper` VALUES (12378, 'Monocular Real-Time Volumetric Performance Capture', '', '', '', '', '', 'We present the first approach to volumetric performance capture and novel-view rendering at real-time speed from monocular video, eliminating the need for expensive multi-view systems or cumbersome pre-acquisition of a personalized template model. Our system reconstructs a fully textured 3D human from each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While PIFu achieves high-resolution reconstruction in a memory-efficient manner, its computationally expensive inference prevents us from deploying such a system for real-time applications. To this end, we propose a novel hierarchical surface localization algorithm and a direct rendering method without explicitly extracting surface meshes. By culling unnecessary regions for evaluation in a coarse-to-fine manner, we successfully accelerate the reconstruction by two orders of magnitude from the baseline without compromising the quality. Furthermore, we introduce an Online Hard Example Mining (OHEM) technique that effectively suppresses failure modes due to the rare occurrence of challenging examples. We adaptively update the sampling probability of the training data based on the current reconstruction accuracy, which effectively alleviates reconstruction artifacts. Our experiments and evaluations demonstrate the robustness of our system to various challenging angles, illuminations, poses, and clothing styles. We also show that our approach compares favorably with the state-of-the-art monocular performance capture. Our proposed approach removes the need for multi-view studio settings and enables a consumer-accessible solution for volumetric capture.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_4');
INSERT INTO `paper` VALUES (12379, 'Monotonicity Prior for Cloud Tomography', 'Scattering', 'Regularization', 'Physics-based vision', '', '', 'We introduce a differentiable monotonicity prior, useful to express signals of monotonic tendency. An important natural signal of this tendency is the optical extinction coefficient, as a function of altitude in a cloud. Cloud droplets become larger as vapor condenses on them in an updraft. Reconstruction of the volumetric structure of clouds is important for climate research. Data for such reconstruction is multi-view images of each cloud taken simultaneously. This acquisition mode is expected by upcoming future spaceborne imagers. We achieve three-dimensional volumetric reconstruction through stochastic scattering tomography, which is based on optimization of a cost function. Part of the cost is the monotonicity prior, which helps to improve the reconstruction quality. The stochastic tomography is based on Monte-Carlo (MC) radiative transfer. It is formulated and implemented in a coarse-to-fine form, making it scalable to large fields.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_17');
INSERT INTO `paper` VALUES (12380, 'More Classifiers, Less Forgetting: A Generic Multi-classifier Paradigm for Incremental Learning', 'Incremental learning', 'Regularization', 'Classifier ensemble', '', '', 'Overcoming catastrophic forgetting in neural networks is a long-standing and core research objective for incremental learning. Notable studies have shown regularization strategies enable the network to remember previously acquired knowledge devoid of heavy forgetting. Since those regularization strategies are mostly associated with classifier outputs, we propose a MUlti-Classifier (MUC) incremental learning paradigm that integrates an ensemble of auxiliary classifiers to estimate more effective regularization constraints. Additionally, we extend two common methods, focusing on parameter and activation regularization, from the conventional single-classifier paradigm to MUC. Our classifier ensemble promotes regularizing network parameters or activations when moving to learn the next task. Under the setting of task-agnostic evaluation, our experimental results on CIFAR-100 and Tiny ImageNet incremental benchmarks show that our method outperforms other baselines. Specifically, MUC obtains 3%–5% accuracy boost and 4%–5% decline of forgetting ratio, compared with MAS and LwF. Our code is available at https://github.com/Liuy8/MUC.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_42');
INSERT INTO `paper` VALUES (12381, 'Motion Capture from Internet Videos', 'Motion capture', 'Human pose estimation', '', '', '', 'Recent advances in image-based human pose estimation make it possible to capture 3D human motion from a single RGB video. However, the inherent depth ambiguity and self-occlusion in a single view prohibit the recovery of as high-quality motion as multi-view reconstruction. While multi-view videos are not common, the videos of a celebrity performing a specific action are usually abundant on the Internet. Even if these videos were recorded at different time instances, they would encode the same motion characteristics of the person. Therefore, we propose to capture human motion by jointly analyzing these Internet videos instead of using single videos separately. However, this new task poses many new challenges that cannot be addressed by existing methods, as the videos are unsynchronized, the camera viewpoints are unknown, the background scenes are different, and the human motions are not exactly the same among videos. To address these challenges, we propose a novel optimization-based framework and experimentally demonstrate its ability to recover much more precise and detailed motion from multiple videos, compared against monocular motion capture methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_13');
INSERT INTO `paper` VALUES (12382, 'Motion Guided 3D Pose Estimation from Videos', '3D pose estimation', 'Motion loss', 'Graph convolution', '', '', 'We propose a new loss function, called motion loss, for supervising models for monocular 3D Human pose estimation from videos. It works by comparing the motion pattern of the prediction against ground truth key point trajectories. In computing motion loss, we introduce pairwise motion encoding, a simple yet effective representation for keypoint motion. We design a new graph convolutional network architecture, U-shaped GCN (UGCN). It captures both short-term and long-term motion information to fully leverage the supervision from the motion loss (Codes and models at http://wangjingbo.top/papers/ECCV2020-Video-Pose/MotionLossPage.html). We experiment training UGCN with the motion loss on two large scale benchmarks: Human3.6M and MPI-INF-3DHP. Our models surpass other state-of-the-art models by a large margin. It also demonstrates strong capacity in producing smooth 3D sequences and recovering keypoint motion.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_45');
INSERT INTO `paper` VALUES (12383, 'Motion Prediction for First-Person Vision Multi-object Tracking', 'Motion model', 'Prediction', 'Mobile cameras', 'Mobile agents', 'Tracking', 'Tracking multiple independently moving objects with cameras mounted on moving robots is becoming increasingly common. However, most causal trackers rely on linear motion models that may be inaccurate in these scenarios. To overcome this problem, we present a real-time multi-object tracker based on the Early Association Probability Hypothesis Density Particle Filter with a prediction model that disentangles the motion of objects from that of the camera. Moreover, the prediction model allows us to intentionally reduce the video frame rate at which the tracker operates, with only a minor reduction in accuracy. Specifically, the model allows us to halve the processed frames while still outperforming alternative prediction models, including traditional linear motion predictors in moving-camera sequences. Experimental results show that the proposed model improves both accuracy and precision of the tracker.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_29');
INSERT INTO `paper` VALUES (12384, 'Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior', 'Video adversarial attack', 'Video motion', 'Noise sampler', '', '', 'Deep neural networks are known to be susceptible to adversarial noise, which is tiny and imperceptible perturbation. Most of previous works on adversarial attack mainly focus on image models, while the vulnerability of video models is less explored. In this paper, we aim to attack video models by utilizing intrinsic movement pattern and regional relative motion among video frames. We propose an effective motion-excited sampler to obtain motion-aware noise prior, which we term as sparked prior. Our sparked prior underlines frame correlations and utilizes video dynamics via relative motion. By using the sparked prior in gradient estimation, we can successfully attack a variety of video classification models with fewer number of queries. Extensive experimental results on four benchmark datasets validate the efficacy of our proposed method.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_15');
INSERT INTO `paper` VALUES (12385, 'MotionSqueeze: Neural Motion Feature Learning for Video Understanding', 'Video understanding', 'Action recognition', 'Motion feature learning', 'Efficient video processing', '', 'Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1 & V2 datasets.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_21');
INSERT INTO `paper` VALUES (12386, 'MovieNet: A Holistic Dataset for Movie Understanding', '', '', '', '', '', 'Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet – a holistic dataset for movie understanding. MovieNet contains 1, 100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc.. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1 M characters with bounding boxes and identities, 42 K scene boundaries, 2.5 K aligned description sentences, 65 K tags of place and action, and 92 K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at https://movienet.github.io.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_41');
INSERT INTO `paper` VALUES (12387, 'MPCC: Matching Priors and Conditionals for Clustering', '', '', '', '', '', 'Clustering is a fundamental task in unsupervised learning that depends heavily on the data representation that is used. Deep generative models have appeared as a promising tool to learn informative low-dimensional data representations. We propose Matching Priors and Conditionals for Clustering (MPCC), a GAN-based model with an encoder to infer latent variables and cluster categories from data, and a flexible decoder to generate samples from a conditional latent space. With MPCC we demonstrate that a deep generative model can be competitive/superior against discriminative methods in clustering tasks surpassing the state of the art over a diverse set of benchmark datasets. Our experiments show that adding a learnable prior and augmenting the number of encoder updates improve the quality of the generated samples, obtaining an inception score of \\(9.49 \\pm 0.15\\) and improving the Fréchet inception distance over the state of the art by a \\(46.9\\%\\) in CIFAR10.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_39');
INSERT INTO `paper` VALUES (12388, 'MTI-Net: Multi-scale Task Interaction Networks for Multi-task Learning', 'Multi-task learning', 'Scene understanding', '', '', '', 'In this paper, we argue about the importance of considering task interactions at multiple scales when distilling task information in a multi-task learning setup. In contrast to common belief, we show that tasks with high affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa. We propose a novel architecture, namely MTI-Net, that builds upon this finding in three ways. First, it explicitly models task interactions at every scale via a multi-scale multi-modal distillation unit. Second, it propagates distilled task information from lower to higher scales via a feature propagation module. Third, it aggregates the refined task features from all scales via a feature aggregation unit to produce the final per-task predictions.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_31');
INSERT INTO `paper` VALUES (12389, 'MuCAN: Multi-correspondence Aggregation Network for Video Super-Resolution', 'Video super-resolution', 'Correspondence aggregation', '', '', '', 'Video super-resolution (VSR) aims to utilize multiple low-resolution frames to generate a high-resolution prediction for each frame. In this process, inter- and intra-frames are the key sources for exploiting temporal and spatial information. However, there are a couple of limitations for existing VSR methods. First, optical flow is often used to establish one-on-one temporal correspondences. But flow estimation itself is error-prone and hence largely affects the ultimate recovery result. Second, similar patterns existing in natural images are rarely exploited for the VSR task. Motivated by these findings, we propose a temporal multi-correspondence aggregation strategy to leverage most similar patches across frames, and also a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Based on these two novel modules, we build an effective multi-correspondence aggregation network (MuCAN) for VSR. Our method achieves state-of-the-art results on multiple benchmark datasets. Extensive experiments justify the effectiveness of our method.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_20');
INSERT INTO `paper` VALUES (12390, 'Multi-agent Embodied Question Answering in Interactive Environments', '3D reconstruction', 'Embodied vision', 'Question answering', '', '', 'We investigate a new AI task—Multi-Agent Interactive Question Answering—where several agents explore the scene jointly in interactive environments to answer a question. To cooperate efficiently and answer accurately, agents must be well-organized to have balanced work division and share knowledge about the objects involved. We address this new problem in two stages: Multi-Agent 3D Reconstruction in Interactive Environments and Question Answering. Our proposed framework features multi-layer structural and semantic memories shared by all agents, as well as a question answering model built upon a 3D-CNN network to encode the scene memories. During the reconstruction, agents simultaneously explore and scan the scene with a clear division of work, organized by next viewpoints planning. We evaluate our framework on the IQuADv1 dataset and outperform the IQA baseline in a single-agent scenario. In multi-agent scenarios, our framework shows favorable speedups while remaining high accuracy.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_39');
INSERT INTO `paper` VALUES (12391, 'Multi-attention Based Ultra Lightweight Image Super-Resolution', 'Super-Resolution', 'Feature extraction', 'Multi-attention', 'Low-computing resources', 'Lightweight convolutional neural networks', 'Lightweight image super-resolution (SR) networks have the utmost significance for real-world applications. There are several deep learning based SR methods with remarkable performance, but their memory and computational cost are hindrances in practical usage. To tackle this problem, we propose a Multi-Attentive Feature Fusion Super-Resolution Network (MAFFSRN). MAFFSRN consists of proposed feature fusion groups (FFGs) that serve as a feature extraction block. Each FFG contains a stack of proposed multi-attention blocks (MAB) that are combined in a novel feature fusion structure. Further, the MAB with a cost-efficient attention mechanism (CEA) helps us to refine and extract the features using multiple attention mechanisms. The comprehensive experiments show the superiority of our model over the existing state-of-the-art. We participated in AIM 2020 efficient SR challenge with our MAFFSRN model and won 1st, 3rd, and 4th places in memory usage, floating-point operations (FLOPs) and number of parameters, respectively.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_6');
INSERT INTO `paper` VALUES (12392, 'Multi-channel Transformers for Multi-articulatory Sign Language Translation', 'Sign language translation', 'Multi-channel', 'Sequence-to-sequence', '', '', 'Sign languages use multiple asynchronous information channels (articulators), not just the hands but also the face and body, which computational approaches often ignore. In this paper we tackle the multi-articulatory sign language translation task and propose a novel multi-channel transformer architecture. The proposed architecture allows both the inter and intra contextual relationships between different sign articulators to be modelled within the transformer network itself, while also maintaining channel specific information. We evaluate our approach on the RWTH-PHOENIX-Weather-2014T dataset and report competitive translation performance. Importantly, we overcome the reliance on gloss annotations which underpin other state-of-the-art approaches, thereby removing the need for expensive curated datasets.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_18');
INSERT INTO `paper` VALUES (12393, 'Multi-CryoGAN: Reconstruction of Continuous Conformations in Cryo-EM Using Generative Adversarial Networks', 'Cryo-EM', 'Inverse problem', 'Image reconstruction', 'Generative adversarial networks', 'Continuous protein conformations', 'We propose a deep-learning-based reconstruction method for cryo-electron microscopy (Cryo-EM) that can model multiple conformations of a nonrigid biomolecule in a standalone manner. Cryo-EM produces many noisy projections from separate instances of the same but randomly oriented biomolecule. Current methods rely on pose and conformation estimation which are inefficient for the reconstruction of continuous conformations that carry valuable information. We introduce Multi-CryoGAN, which sidesteps the additional processing by casting the volume reconstruction into the distribution matching problem. By introducing a manifold mapping module, Multi-CryoGAN can learn continuous structural heterogeneity without pose estimation nor clustering. We also give a theoretical guarantee of recovery of the true conformations. Our method can successfully reconstruct 3D protein complexes on synthetic 2D Cryo-EM datasets for both continuous and discrete structural variability scenarios. Multi-CryoGAN is the first model that can reconstruct continuous conformations of a biomolecule from Cryo-EM images in a fully unsupervised and end-to-end manner.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_28');
INSERT INTO `paper` VALUES (12394, 'Multi-level Wavelet-Based Generative Adversarial Network for Perceptual Quality Enhancement of Compressed Video', 'Video perceptual quality enhancement', 'Wavelet packet transform', 'GAN', '', '', 'The past few years have witnessed fast development in video quality enhancement via deep learning. Existing methods mainly focus on enhancing the objective quality of compressed video while ignoring its perceptual quality. In this paper, we focus on enhancing the perceptual quality of compressed video. Our main observation is that enhancing the perceptual quality mostly relies on recovering high-frequency sub-bands in wavelet domain. Accordingly, we propose a novel generative adversarial network (GAN) based on multi-level wavelet packet transform (WPT) to enhance the perceptual quality of compressed video, which is called multi-level wavelet-based GAN (MW-GAN). In MW-GAN, we first apply motion compensation with a pyramid architecture to obtain temporal information. Then, we propose a wavelet reconstruction network with wavelet-dense residual blocks (WDRB) to recover the high-frequency details. In addition, the adversarial loss of MW-GAN is added via WPT to further encourage high-frequency details recovery for video frames. Experimental results demonstrate the superiority of our method.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_24');
INSERT INTO `paper` VALUES (12395, 'Multi-loss Rebalancing Algorithm for Monocular Depth Estimation', 'Monocular depth estimation', 'Multi-loss rebalancing', '', '', '', 'An algorithm to combine multiple loss terms adaptively for training a monocular depth estimator is proposed in this work. We construct a loss function space containing tens of losses. Using more losses can improve inference capability without any additional complexity in the test phase. However, when many losses are used, some of them may be neglected during training. Also, since each loss decreases at a different speed, adaptive weighting is required to balance the contributions of the losses. To address these issues, we propose the loss rebalancing algorithm that initializes and rebalances the weight for each loss function adaptively in the course of training. Experimental results show that the proposed algorithm provides state-of-the-art depth estimation results on various datasets. Codes are available at https://github.com/jaehanlee-mcl/multi-loss-rebalancing-depth.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_46');
INSERT INTO `paper` VALUES (12396, 'Multi-modal Transformer for Video Retrieval', 'Video', 'Language', 'Retrieval', 'Multi-modal', 'Cross-modal', 'The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_13');
INSERT INTO `paper` VALUES (12397, 'Multi-objective Reinforced Evolution in Mobile Neural Architecture Search', 'Image super-resolution', 'Neural Architecture Search', '', '', '', 'Fabricating neural models for a wide range of mobile devices is a challenging task due to highly constrained resources. Recent trends favor neural architecture search involving evolutionary algorithms (EA) and reinforcement learning (RL), however, they are separately used. In this paper, we present a novel multi-objective algorithm called MoreMNAS (Multi-Objective Reinforced Evolution in Mobile Neural Architecture Search) by leveraging good virtues from both sides. Particularly, we devise a variant of multi-objective genetic algorithm NSGA-II, where mutations are performed either by reinforcement learning or a natural mutating process. It maintains a delicate balance between exploration and exploitation. Not only does it prevent the search degradation, but it also makes use of the learned knowledge. Our experiments conducted in Super-resolution domain (SR) deliver rivaling models compared with some state-of-the-art methods at fewer FLOPS (Evaluation code can be found at https://github.com/xiaomi-automl/MoreMNAS).', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_6');
INSERT INTO `paper` VALUES (12398, 'Multi-person 3D Pose Estimation in Crowded Scenes Based on Multi-view Geometry', '3D pose estimation', 'Occlusion', 'Correspondence problem', '', '', 'Epipolar constraints are at the core of feature matching and depth estimation in current multi-person multi-camera 3D human pose estimation methods. Despite the satisfactory performance of this formulation in sparser crowd scenes, its effectiveness is frequently challenged under denser crowd circumstances mainly due to two sources of ambiguity. The first is the mismatch of human joints resulting from the simple cues provided by the Euclidean distances between joints and epipolar lines. The second is the lack of robustness from the naive formulation of the problem as a least squares minimization. In this paper, we depart from the multi-person 3D pose estimation formulation, and instead reformulate it as crowd pose estimation. Our method consists of two key components: a graph model for fast cross-view matching, and a maximum a posteriori (MAP) estimator for the reconstruction of the 3D human poses. We demonstrate the effectiveness and superiority of our proposed method on four benchmark datasets. Our code is available at: https://github.com/HeCraneChen/3D-Crowd-Pose-Estimation-Based-on-MVG.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_32');
INSERT INTO `paper` VALUES (12399, 'Multi-scale Positive Sample Refinement for Few-Shot Object Detection', 'Few-shot object detection', 'Multi-scale refinement', '', '', '', 'Few-shot object detection (FSOD) helps detectors adapt to unseen classes with few training instances, and is useful when manual annotation is time-consuming or data acquisition is limited. Unlike previous attempts that exploit few-shot classification techniques to facilitate FSOD, this work highlights the necessity of handling the problem of scale variations, which is challenging due to the unique sample distribution. To this end, we propose a Multi-scale Positive Sample Refinement (MPSR) approach to enrich object scales in FSOD. It generates multi-scale positive samples as object pyramids and refines the prediction at various scales. We demonstrate its advantage by integrating it as an auxiliary branch to the popular architecture of Faster R-CNN with FPN, delivering a strong FSOD solution. Several experiments are conducted on PASCAL VOC and MS COCO, and the proposed approach achieves state of the art results and significantly outperforms other counterparts, which shows its effectiveness. Code is available at https://github.com/jiaxi-wu/MPSR.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_27');
INSERT INTO `paper` VALUES (12400, 'Multi-source Open-Set Deep Adversarial Domain Adaptation', 'Domain adaptation', 'Multi-source', 'Open-set', '', '', 'We introduce a novel learning paradigm of multi-source open-set unsupervised domain adaptation (MS-OSDA). Recently, the notion of single-source open-set domain adaptation (SS-OSDA) which considers the presence of previously unseen open-set (unknown) classes in the target-domain in addition to the source-domain closed-set (known) classes has drawn attention. In the SS-OSDA setting, the labeled samples are assumed to be drawn from the same source. Yet, it is more plausible to assume that the labeled samples are distributed over multiple source-domains, but the existing SS-OSDA techniques cannot directly handle this more realistic scenario considering the diversities among multiple source-domains. As a remedy, we propose a novel adversarial learning-driven approach to deal with MS-OSDA. Precisely, we model a shared feature space for all the domains which explicitly mitigates the domain-gap among the source-domains. The adversarial learning strategy is introduced to align the known-class samples from the target-domain with the source data while making the unknown-classes more separable. We validate our method on the Office-31, Office-Home, Office-CalTech, and Digits datasets and find that the proposed model consistently outperforms the baseline and benchmark SS-OSDA approaches.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_44');
INSERT INTO `paper` VALUES (12401, 'Multi-task Curriculum Framework for Open-Set Semi-supervised Learning', 'Semi-supervised learning', 'Out-of-distribution detection', 'Multi-task learning', '', '', 'Semi-supervised learning (SSL) has been proposed to leverage unlabeled data for training powerful models when only limited labeled data is available. While existing SSL methods assume that samples in the labeled and unlabeled data share the classes of their samples, we address a more complex novel scenario named open-set SSL, where out-of-distribution (OOD) samples are contained in unlabeled data. Instead of training an OOD detector and SSL separately, we propose a multi-task curriculum learning framework. First, to detect the OOD samples in unlabeled data, we estimate the probability of the sample belonging to OOD. We use a joint optimization framework, which updates the network parameters and the OOD score alternately. Simultaneously, to achieve high performance on the classification of in-distribution (ID) data, we select ID samples in unlabeled data having small OOD scores, and use these data with labeled data for training the deep neural networks to classify ID samples in a semi-supervised manner. We conduct several experiments, and our method achieves state-of-the-art results by successfully eliminating the effect of OOD samples.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_26');
INSERT INTO `paper` VALUES (12402, 'Multi-Temporal Recurrent Neural Networks for Progressive Non-uniform Single Image Deblurring with Incremental Temporal Training', '', '', '', '', '', 'Blind non-uniform image deblurring for severe blurs induced by large motions is still challenging. Multi-scale (MS) approach has been widely used for deblurring that sequentially recovers the downsampled original image in low spatial scale first and then further restores in high spatial scale using the result(s) from lower spatial scale(s). Here, we investigate a novel alternative approach to MS, called multi-temporal (MT), for non-uniform single image deblurring by exploiting time-resolved deblurring dataset from high-speed cameras. MT approach models severe blurs as a series of small blurs so that it deblurs small amount of blurs in the original spatial scale progressively instead of restoring the images in different spatial scales. To realize MT approach, we propose progressive deblurring over iterations and incremental temporal training with temporally augmented training data. Our MT approach, that can be seen as a form of curriculum learning in a wide sense, allows a number of state-of-the-art MS based deblurring methods to yield improved performances without using MS approach. We also proposed a MT recurrent neural network with recurrent feature maps that outperformed state-of-the-art deblurring methods with the smallest number of parameters.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_20');
INSERT INTO `paper` VALUES (12403, 'Multi-view Action Recognition Using Cross-View Video Prediction', '', '', '', '', '', 'In this work, we address the problem of action recognition in a multi-view environment. Most of the existing approaches utilize pose information for multi-view action recognition. We focus on RGB modality instead and propose an unsupervised representation learning framework, which encodes the scene dynamics in videos captured from multiple viewpoints via predicting actions from unseen views. The framework takes multiple short video clips from different viewpoints and time as input and learns an holistic internal representation which is used to predict a video clip from an unseen viewpoint and time. The ability of the proposed network to render unseen video frames enables it to learn a meaningful and robust representation of the scene dynamics. We evaluate the effectiveness of the learned representation for multi-view video action recognition in a supervised approach. We observe a significant improvement in the performance with RGB modality on NTU-RGB+D dataset, which is the largest dataset for multi-view action recognition. The proposed framework also achieves state-of-the-art results with depth modality, which validates the generalization capability of the approach to other data modalities. The code is publicly available at https://github.com/svyas23/cross-view-action.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_26');
INSERT INTO `paper` VALUES (12404, 'Multi-view Adaptive Graph Convolutions for Graph Classification', 'Distance metric learning', 'Graph neural networks', 'Graph classification', 'Multi-view', 'View pooling', 'In this paper, a novel multi-view methodology for graph-based neural networks is proposed. A systematic and methodological adaptation of the key concepts of classical deep learning methods such as convolution, pooling and multi-view architectures is developed for the context of non-Euclidean manifolds. The aim of the proposed work is to present a novel multi-view graph convolution layer, as well as a new view pooling layer making use of: a) a new hybrid Laplacian that is adjusted based on feature distance metric learning, b) multiple trainable representations of a feature matrix of a graph, using trainable distance matrices, adapting the notion of views to graphs and c) a multi-view graph aggregation scheme called graph view pooling, in order to synthesise information from the multiple generated “views”. The aforementioned layers are used in an end-to-end graph neural network architecture for graph classification and show competitive results to other state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_24');
INSERT INTO `paper` VALUES (12405, 'Multi-view Convolutional Network for Crowd Counting in Drone-Captured Images', 'Unmanned aerial vehicles', 'Crowd counting', 'Computer vision', 'Convolutional neural networks', 'Multi-view learning', 'This paper proposes a novel lightweight and fast convolutional neural network to learn a regression model for crowd counting in images captured from drones. The learning system is initially based on a multi-input model trained on two different views of the same input for the task at hand: (i) real-world images; and (ii) corresponding synthetically created “crowd heatmaps”. The synthetic input is intended to help the network focus on the most important parts of the images. The network is trained from scratch on a subset of the VisDrone dataset. During inference, the synthetic path of the network is disregarded resulting in a traditional single-view model optimized for resource-constrained devices. The derived model achieves promising results on the test images, outperforming models developed by state-of-the-art lightweight architectures that can be used for crowd counting and detection.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_35');
INSERT INTO `paper` VALUES (12406, 'Multi-view Optimization of Local Feature Geometry', '3D reconstruction', 'Local features', '', '', '', 'In this work, we address the problem of refining the geometry of local image features from multiple views without known scene or camera geometry. Current approaches to local feature detection are inherently limited in their keypoint localization accuracy because they only operate on a single view. This limitation has a negative impact on downstream tasks such as Structure-from-Motion, where inaccurate keypoints lead to large errors in triangulation and camera localization. Our proposed method naturally complements the traditional feature extraction and matching paradigm. We first estimate local geometric transformations between tentative matches and then optimize the keypoint locations over multiple views jointly according to a non-linear least squares formulation. Throughout a variety of experiments, we show that our method consistently improves the triangulation and camera localization performance for both hand-crafted and learned local features.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_39');
INSERT INTO `paper` VALUES (12407, 'Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability', 'Memorability estimation', 'Memorability decay', 'Multimodal video understanding', '', '', 'A key capability of an intelligent system is deciding when events from past experience must be remembered and when they can be forgotten. Towards this goal, we develop a predictive model of human visual event memory and how those memories decay over time. We introduce Memento10k, a new, dynamic video memorability dataset containing human annotations at different viewing delays. Based on our findings we propose a new mathematical formulation of memorability decay, resulting in a model that is able to produce the first quantitative estimation of how a video decays in memory over time. In contrast with previous work, our model can predict the probability that a video will be remembered at an arbitrary delay. Importantly, our approach combines visual and semantic information (in the form of textual captions) to fully represent the meaning of events. Our experiments on two video memorability benchmarks, including Memento10k, show that our model significantly improves upon the best prior approach (by 12% on average).', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_14');
INSERT INTO `paper` VALUES (12408, 'Multimodal Shape Completion via Conditional Generative Adversarial Networks', 'Shape completion', 'Multimodal mapping', 'Conditional generative adversarial network', '', '', 'Several deep learning methods have been proposed for completing partial data from shape acquisition setups, i.e., filling the regions that were missing in the shape. These methods, however, only complete the partial shape with a single output, ignoring the ambiguity when reasoning the missing geometry. Hence, we pose a multi-modal shape completion problem, in which we seek to complete the partial shape with multiple outputs by learning a one-to-many mapping. We develop the first multimodal shape completion method that completes the partial shape via conditional generative modeling, without requiring paired training data. Our approach distills the ambiguity by conditioning the completion on a learned multimodal distribution of possible results. We extensively evaluate the approach on several datasets that contain varying forms of shape incompleteness, and compare among several baseline methods and variants of our methods qualitatively and quantitatively, demonstrating the merit of our method in completing partial shapes with both diversity and quality.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_17');
INSERT INTO `paper` VALUES (12409, 'Multiple Class Novelty Detection Under Data Distribution Shift', 'Dataset distribution shift', 'Multiple class novelty detection', '', '', '', 'The novelty detection models learn a decision boundary around multiple categories of a given dataset. This helps such models in detecting any novel classes encountered during testing. However, in many cases, the test data distribution can be different from that of the training data. For such cases, the novelty detection models risk detecting a known class as novel due to the dataset distribution shift. This scenario is often ignored while working with novelty detection. To this end, we consider the problem of multiple class novelty detection under dataset distribution shift to improve the novelty detection performance. Firstly, we discuss the problem setting in detail and show how it affects the performance of current novelty detection methods. Secondly, we show that one could improve those novelty detection methods with a simple integration of domain adversarial loss. Finally, we propose a method which brings together the techniques from novelty detection and domain adaptation to improve generalization of multiple class novelty detection on different domains. We evaluate the proposed method on digits and object recognition datasets and show that it provides improvements over the baseline methods.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_26');
INSERT INTO `paper` VALUES (12410, 'Multiple Expert Brainstorming for Domain Adaptive Person Re-Identification', 'Domain adaptation', 'Person re-ID', 'Ensemble learning', '', '', 'Often the best performing deep neural models are ensembles of multiple base-level networks, nevertheless, ensemble learning with respect to domain adaptive person re-ID remains unexplored. In this paper, we propose a multiple expert brainstorming network (MEB-Net) for domain adaptive person re-ID, opening up a promising direction about model ensemble problem under unsupervised conditions. MEB-Net adopts a mutual learning strategy, where multiple networks with different architectures are pre-trained within a source domain as expert models equipped with specific features and knowledge, while the adaptation is then accomplished through brainstorming (mutual learning) among expert models. MEB-Net accommodates the heterogeneity of experts learned with different architectures and enhances discrimination capability of the adapted re-ID model, by introducing a regularization scheme about authority of experts. Extensive experiments on large-scale datasets (Market-1501 and DukeMTMC-reID) demonstrate the superior performance of MEB-Net over the state-of-the-arts. Code is available at https://github.com/YunpengZhai/MEB-Net.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_35');
INSERT INTO `paper` VALUES (12411, 'Multiple Interaction Learning with Question-Type Prior Knowledge for Constraining Answer Search Space in Visual Question Answering', 'Visual Question Answering', 'Multiple interaction learning', '', '', '', 'Different approaches have been proposed to Visual Question Answering (VQA). However, few works are aware of the behaviors of varying joint modality methods over question type prior knowledge extracted from data in constraining answer search space, of which information gives a reliable cue to reason about answers for questions asked in input images. In this paper, we propose a novel VQA model that utilizes the question-type prior information to improve VQA by leveraging the multiple interactions between different joint modality methods based on their behaviors in answering questions from different types. The solid experiments on two benchmark datasets, i.e., VQA 2.0 and TDIUC, indicate that the proposed method yields the best performance with the most competitive approaches.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_34');
INSERT INTO `paper` VALUES (12412, 'Multiple Sound Sources Localization from Coarse to Fine', 'Sound localization', 'Audiovisual alignment', 'Complex scene', '', '', 'How to visually localize multiple sound sources in unconstrained videos is a formidable problem, especially when lack of the pairwise sound-object annotations. To solve this problem, we develop a two-stage audiovisual learning framework that disentangles audio and visual representations of different categories from complex scenes, then performs cross-modal feature alignment in a coarse-to-fine manner. Our model achieves state-of-the-art results on public dataset of localization, as well as considerable performance on multi-source sound localization in complex scenes. We then employ the localization results for sound separation and obtain comparable performance to existing methods. These outcomes demonstrate our model’s ability in effectively aligning sounds with specific visual sources. Code is available at https://github.com/shvdiwnkozbw/Multi-Source-Sound-Localization.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_18');
INSERT INTO `paper` VALUES (12413, 'Multitask Learning Strengthens Adversarial Robustness', 'Multitask learning', 'Adversarial robustness', '', '', '', 'Although deep networks achieve strong accuracy on a range of computer vision benchmarks, they remain vulnerable to adversarial attacks, where imperceptible input perturbations fool the network. We present both theoretical and empirical analyses that connect the adversarial robustness of a model to the number of tasks that it is trained on. Experiments on two datasets show that attack difficulty increases as the number of target tasks increase. Moreover, our results suggest that when models are trained on multiple tasks at once, they become more robust to adversarial attacks on individual tasks. While adversarial defense remains an open challenge, our results suggest that deep networks are vulnerable partly because they are trained on too few tasks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_10');
INSERT INTO `paper` VALUES (12414, 'Multiview Detection with Feature Perspective Transformation', 'Multiview detection', 'Anchor-free', 'Perspective transformation', 'Fully convolutional', 'Synthetic data', 'Incorporating multiple camera views for detection alleviates the impact of occlusions in crowded scenes. In a multiview detection system, we need to answer two important questions. First, how should we aggregate cues from multiple views? Second, how should we aggregate information from spatially neighboring locations? To address these questions, we introduce a novel multiview detector, MVDet. During multiview aggregation, for each location on the ground, existing methods use multiview anchor box features as representation, which potentially limits performance as pre-defined anchor boxes can be inaccurate. In contrast, via feature map perspective transformation, MVDet employs anchor-free representations with feature vectors directly sampled from corresponding pixels in multiple views. For spatial aggregation, different from previous methods that require design and operations outside of neural networks, MVDet takes a fully convolutional approach with large convolutional kernels on the multiview aggregated feature map. The proposed model is end-to-end learnable and achieves 88.2% MODA on Wildtrack dataset, outperforming the state-of-the-art by 14.1%. We also provide detailed analysis of MVDet on a newly introduced synthetic dataset, MultiviewX, which allows us to control the level of occlusion. Code and MultiviewX dataset are available at https://github.com/hou-yz/MVDet.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_1');
INSERT INTO `paper` VALUES (12415, 'MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution', '', '', '', '', '', 'We propose the width-resolution mutual learning method (MutualNet) to train a network that is executable at dynamic resource constraints to achieve adaptive accuracy-efficiency trade-offs at runtime. Our method trains a cohort of sub-networks with different widths (i.e., number of channels in a layer) using different input resolutions to mutually learn multi-scale representations for each sub-network. It achieves consistently better ImageNet top-1 accuracy over the state-of-the-art adaptive network US-Net under different computation constraints, and outperforms the best compound scaled MobileNet in EfficientNet by 1.5%. The superiority of our method is also validated on COCO object detection and instance segmentation as well as transfer learning. Surprisingly, the training strategy of MutualNet can also boost the performance of a single network, which substantially outperforms the powerful AutoAugmentation in both efficiency (GPU search hours: 15000 vs. 0) and accuracy (ImageNet: 77.6% vs. 78.6%). Code is available at https://github.com/taoyang1122/MutualNet.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_18');
INSERT INTO `paper` VALUES (12416, 'n-Reference Transfer Learning for Saliency Prediction', 'Deep learning', 'Saliency prediction', 'n-shot transfer learning', '', '', 'Benefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models. To solve this problem, we propose a few-shot transfer learning paradigm for saliency prediction, which enables efficient transfer of knowledge learned from the existing large-scale saliency datasets to a target domain with limited labeled samples. Specifically, few target domain samples are used as the reference to train a model with a source domain dataset such that the training process can converge to a local minimum in favor of the target domain. Then, the learned model is further fine-tuned with the reference. The proposed framework is gradient-based and model-agnostic. We conduct comprehensive experiments and ablation study on various source domain and target domain pairs. The results show that the proposed framework achieves a significant performance improvement. The code is publicly available at https://github.com/luoyan407/n-reference.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_30');
INSERT INTO `paper` VALUES (12417, 'Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation', 'Semi-supervised learning', 'Pseudo label', 'Semantic segmentation', 'Instance segmentation', 'Panoptic segmentation', 'Supervised learning in large discriminative models is a mainstay for modern computer vision. Such an approach necessitates investing in large-scale human-annotated datasets for achieving state-of-the-art results. In turn, the efficacy of supervised learning may be limited by the size of the human annotated dataset. This limitation is particularly notable for image segmentation tasks, where the expense of human annotation is especially large, yet large amounts of unlabeled data may exist. In this work, we ask if we may leverage semi-supervised learning in unlabeled video sequences and extra images to improve the performance on urban scene segmentation, simultaneously tackling semantic, instance, and panoptic segmentation. The goal of this work is to avoid the construction of sophisticated, learned architectures specific to label propagation (e.g., patch matching and optical flow). Instead, we simply predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. The procedure is iterated for several times. As a result, our Naive-Student model, trained with such simple yet effective iterative semi-supervised learning, attains state-of-the-art results at all three Cityscapes benchmarks, reaching the performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable step towards building a simple procedure to harness unlabeled video sequences and extra images to surpass state-of-the-art performance on core computer vision tasks.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_40');
INSERT INTO `paper` VALUES (12418, 'NAS-Count: Counting-by-Density with Neural Architecture Search', 'Crowd counting', 'Neural Architecture Search', 'Multi-scale', '', '', 'Most of the recent advances in crowd counting have evolved from hand-designed density estimation networks, where multi-scale features are leveraged to address the scale variation problem, but at the expense of demanding design efforts. In this work, we automate the design of counting models with Neural Architecture Search (NAS) and introduce an end-to-end searched encoder-decoder architecture, Automatic Multi-Scale Network (AMSNet). Specifically, we utilize a counting-specific two-level search space. The encoder and decoder in AMSNet are composed of different cells discovered from micro-level search, while the multi-path architecture is explored through macro-level search. To solve the pixel-level isolation issue in MSE loss, AMSNet is optimized with an auto-searched Scale Pyramid Pooling Loss (SPPLoss) that supervises the multi-scale structural information. Extensive experiments on four datasets show AMSNet produces state-of-the-art results that outperform hand-designed models, fully demonstrating the efficacy of NAS-Count.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_45');
INSERT INTO `paper` VALUES (12419, 'NAS-DIP: Learning Deep Image Prior with Neural Architecture Search', '', '', '', '', '', 'Recent work has shown that the structure of deep convolutional neural networks can be used as a structured image prior for solving various inverse image restoration tasks. Instead of using hand-designed architectures, we propose to search for neural architectures that capture stronger image priors. Building upon a generic U-Net architecture, our core contribution lies in designing new search spaces for (1) an upsampling cell and (2) a pattern of cross-scale residual connections. We search for an improved network by leveraging an existing neural architecture search algorithm (using reinforcement learning with a recurrent neural network controller). We validate the effectiveness of our method via a wide variety of applications, including image restoration, dehazing, image-to-image translation, and matrix factorization. Extensive experimental results show that our algorithm performs favorably against state-of-the-art learning-free approaches and reaches competitive performance with existing learning-based methods in some cases.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_26');
INSERT INTO `paper` VALUES (12420, 'NASA Neural Articulated Shape Approximation', '3D deep learning', 'Neural object representation', 'Articulated objects', 'Deformation', 'Skinning', 'Efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics. To efficiently simulate deformation, existing approaches represent 3D objects using polygonal meshes and deform them using skinning techniques. This paper introduces neural articulated shape approximation (NASA), an alternative framework that enables representation of articulated deformable objects using neural indicator functions that are conditioned on pose. Occupancy testing using NASA is straightforward, circumventing the complexity of meshes and the issue of water-tightness. We demonstrate the effectiveness of NASA for 3D tracking applications, and discuss other potential extensions.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_36');
INSERT INTO `paper` VALUES (12421, 'Negative Margin Matters: Understanding Margin in Few-Shot Classification', 'Few-shot classification', 'Metric learning', 'Large margin loss', '', '', 'This paper introduces a negative margin loss to metric learning based few-shot learning methods. The negative margin loss significantly outperforms regular softmax loss, and achieves state-of-the-art accuracy on three standard few-shot classification benchmarks with few bells and whistles. These results are contrary to the common practice in the metric learning field, that the margin is zero or positive. To understand why the negative margin loss performs well for the few-shot classification, we analyze the discriminability of learned features w.r.t different margins for training and novel classes, both empirically and theoretically. We find that although negative margin reduces the feature discriminability for training classes, it may also avoid falsely mapping samples of the same novel class to multiple peaks or clusters, and thus benefit the discrimination of novel classes. Code is available at https://github.com/bl0/negative-margin.few-shot.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_26');
INSERT INTO `paper` VALUES (12422, 'Negative Pseudo Labeling Using Class Proportion for Semantic Segmentation in Pathology', 'Pathological image', 'Semantic segmentation', 'Negative learning', 'Semi-supervised learning', 'Learning from label proportion', 'In pathological diagnosis, since the proportion of the adenocarcinoma subtypes is related to the recurrence rate and the survival time after surgery, the proportion of cancer subtypes for pathological images has been recorded as diagnostic information in some hospitals. In this paper, we propose a subtype segmentation method that uses such proportional labels as weakly supervised labels. If the estimated class rate is higher than that of the annotated class rate, we generate negative pseudo labels, which indicate, “input image does not belong to this negative label,” in addition to standard pseudo labels. It can force out the low confidence samples and mitigate the problem of positive pseudo label learning which cannot label low confident unlabeled samples. Our method outperformed the state-of-the-art semi-supervised learning (SSL) methods.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_26');
INSERT INTO `paper` VALUES (12423, 'NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis', 'Scene representation', 'View synthesis', 'Image-based rendering', 'Volume rendering', '3D deep learning', 'We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction \\((\\theta ,\\phi )\\)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_24');
INSERT INTO `paper` VALUES (12424, 'Neural Batch Sampling with Reinforcement Learning for Semi-supervised Anomaly Detection', 'Anomaly detection', 'Semi-supervised learning', '', '', '', 'We are interested in the detection and segmentation of anomalies in images where the anomalies are typically small (i.e., a small tear in woven fabric, broken pin of an IC chip). From a statistical learning point of view, anomalies have low occurrence probability and are not from the main modes of a data distribution. Learning a generative model of anomalous data from a natural distribution of data can be difficult because the data distribution is heavily skewed towards a large amount of non-anomalous data. When training a generative model on such imbalanced data using an iterative learning algorithm like stochastic gradient descent (SGD), we observe an expected yet interesting trend in the loss values (a measure of the learned models performance) after each gradient update across data samples. Naturally, as the model sees more non-anomalous data during training, the loss values over a non-anomalous data sample decreases, while the loss values on an anomalous data sample fluctuates. In this work, our key hypothesis is that this change in loss values during training can be used as a feature to identify anomalous data. In particular, we propose a novel semi-supervised learning algorithm for anomaly detection and segmentation using an anomaly classifier that uses as input the loss profile of a data sample processed through an autoencoder. The loss profile is defined as a sequence of reconstruction loss values produced during iterative training. To amplify the difference in loss profiles between anomalous and non-anomalous data, we also introduce a Reinforcement Learning based meta-algorithm, which we call the neural batch sampler, to strategically sample training batches during autoencoder training. Experimental results on multiple datasets with a high diversity of textures and objects, often with multiple modes of defects within them, demonstrate the capabilities and effectiveness of our method when compared with existing state-of-the-art baselines.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_45');
INSERT INTO `paper` VALUES (12425, 'Neural Dense Non-Rigid Structure from Motion with Latent Space Constraints', 'Neural Non-Rigid Structure from Motion', 'Sequence period detection', 'Latent space constraints', 'Deformation auto-decoder', '', 'We introduce the first dense neural non-rigid structure from motion (N-NRSfM) approach, which can be trained end-to-end in an unsupervised manner from 2D point tracks. Compared to the competing methods, our combination of loss functions is fully-differentiable and can be readily integrated into deep-learning systems. We formulate the deformation model by an auto-decoder and impose subspace constraints on the recovered latent space function in a frequency domain. Thanks to the state recurrence cue, we classify the reconstructed non-rigid surfaces based on their similarity and recover the period of the input sequence. Our N-NRSfM approach achieves competitive accuracy on widely-used benchmark sequences and high visual quality on various real videos. Apart from being a standalone technique, our method enables multiple applications including shape compression, completion and interpolation, among others. Combined with an encoder trained directly on 2D images, we perform scenario-specific monocular 3D shape reconstruction at interactive frame rates. To facilitate the reproducibility of the results and boost the new research direction, we open-source our code and provide trained models for research purposes (http://gvv.mpi-inf.mpg.de/projects/Neural_NRSfM/).', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_13');
INSERT INTO `paper` VALUES (12426, 'Neural Design Network: Graphic Layout Generation with Constraints', '', '', '', '', '', 'Graphic design is essential for visual communication with layouts being fundamental to composing attractive designs. Layout generation differs from pixel-level image synthesis and is unique in terms of the requirement of mutual relations among the desired components. We propose a method for design layout generation that can satisfy user-specified constraints. The proposed neural design network (NDN) consists of three modules. The first module predicts a graph with complete relations from a graph with user-specified relations. The second module generates a layout from the predicted graph. Finally, the third module fine-tunes the predicted layout. Quantitative and qualitative experiments demonstrate that the generated layouts are visually similar to real design layouts. We also construct real designs based on predicted layouts for a better understanding of the visual quality. Finally, we demonstrate a practical application on layout recommendation.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_29');
INSERT INTO `paper` VALUES (12427, 'Neural Geometric Parser for Single Image Camera Calibration', 'Single image camera calibration', 'Neural geometric parser', 'Horizon line', 'Focal length', 'Vanishing Points', 'We propose a neural geometric parser learning single image camera calibration for man-made scenes. Unlike previous neural approaches that rely only on semantic cues obtained from neural networks, our approach considers both semantic and geometric cues, resulting in significant accuracy improvement. The proposed framework consists of two networks. Using line segments of an image as geometric cues, the first network estimates the zenith vanishing point and generates several candidates consisting of the camera rotation and focal length. The second network evaluates each candidate based on the given image and the geometric cues, where prior knowledge of man-made scenes is used for the evaluation. With the supervision of datasets consisting of the horizontal line and focal length of the images, our networks can be trained to estimate the same camera parameters. Based on the Manhattan world assumption, we can further estimate the camera rotation and focal length in a weakly supervised manner. The experimental results reveal that the performance of our neural approach is significantly higher than that of existing state-of-the-art camera calibration techniques for single images of indoor and outdoor scenes.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_32');
INSERT INTO `paper` VALUES (12428, 'Neural Hair Rendering', 'Neural rendering', 'Unsupervised image translation', '', '', '', 'In this paper, we propose a generic neural-based hair rendering pipeline that can synthesize photo-realistic images from virtual 3D hair models. Unlike existing supervised translation methods that require model-level similarity to preserve consistent structure representation for both real images and fake renderings, our method adopts an unsupervised solution to work on arbitrary hair models. The key component of our method is a shared latent space to encode appearance-invariant structure information of both domains, which generates realistic renderings conditioned by extra appearance inputs. This is achieved by domain-specific pre-disentangled structure representation, partially shared domain encoder layers and a structure discriminator. We also propose a simple yet effective temporal conditioning method to enforce consistency for video sequence generation. We demonstrate the superiority of our method by testing it on a large number of portraits and comparing it with alternative baselines and state-of-the-art unsupervised image translation methods.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_22');
INSERT INTO `paper` VALUES (12429, 'Neural Object Learning for 6D Pose Estimation Using a Few Cluttered Images', '6D pose estimation', 'Object learning', 'Object model', 'Object modeling', 'Differentiable rendering', 'Recent methods for 6D pose estimation of objects assume either textured 3D models or real images that cover the entire range of target poses. However, it is difficult to obtain textured 3D models and annotate the poses of objects in real scenarios. This paper proposes a method, Neural Object Learning (NOL), that creates synthetic images of objects in arbitrary poses by combining only a few observations from cluttered images. A novel refinement step is proposed to align inaccurate poses of objects in source images, which results in better quality images. Evaluations performed on two public datasets show that the rendered images created by NOL lead to state-of-the-art performance in comparison to methods that use 13 times the number of real images. Evaluations on our new dataset show multiple objects can be trained and recognized simultaneously using a sequence of a fixed scene.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_38');
INSERT INTO `paper` VALUES (12430, 'Neural Point-Based Graphics', 'Image-based rendering', 'Scene modeling', 'Neural rendering', 'Convolutional networks', '', 'We present a new point-based approach for modeling the appearance of real scenes. The approach uses a raw point cloud as the geometric representation of a scene, and augments each point with a learnable neural descriptor that encodes local geometry and appearance. A deep rendering network is learned in parallel with the descriptors, so that new views of the scene can be obtained by passing the rasterizations of a point cloud from new viewpoints through this network. The input rasterizations use the learned descriptors as point pseudo-colors. We show that the proposed approach can be used for modeling complex scenes and obtaining their photorealistic views, while avoiding explicit surface estimation and meshing. In particular, compelling results are obtained for scenes scanned using hand-held commodity RGB-D sensors as well as standard RGB cameras even in the presence of objects that are challenging for standard mesh-based modeling.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_42');
INSERT INTO `paper` VALUES (12431, 'Neural Predictor for Neural Architecture Search', 'Neural architecture search', 'Automated machine learning', 'Graph neural networks', 'NASBench-101', 'Mobile models', 'Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracies for architectures. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than \\(20 \\times \\) as sample efficient as Regularized Evolution on the NASBench-101 benchmark. On ImageNet, it approaches the efficiency of more complex and restrictive approaches based on weight sharing such as ProxylessNAS while being fully (embarrassingly) parallelizable and friendly to hyper-parameter tuning.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_39');
INSERT INTO `paper` VALUES (12432, 'Neural Re-rendering of Humans from a Single Image', 'Neural rendering', 'Pose transfer', 'Novel view synthesis', '', '', 'Human re-rendering from a single image is a starkly underconstrained problem, and state-of-the-art algorithms often exhibit undesired artefacts, such as over-smoothing, unrealistic distortions of the body parts and garments, or implausible changes of the texture. To address these challenges, we propose a new method for neural re-rendering of a human under a novel user-defined pose and viewpoint, given one input image. Our algorithm represents body pose and shape as a parametric mesh which can be reconstructed from a single image and easily reposed. Instead of a colour-based UV texture map, our approach further employs a learned high-dimensional UV feature map to encode appearance. This rich implicit representation captures detailed appearance variation across poses, viewpoints, person identities and clothing styles better than learned colour texture maps. The body model with the rendered feature maps is fed through a neural image-translation network that creates the final rendered colour image. The above components are combined in an end-to-end-trained neural network architecture that takes as input a source person image, and images of the parametric body model in the source pose and desired target pose. Experimental evaluation demonstrates that our approach produces higher quality single-image re-rendering results than existing methods.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_35');
INSERT INTO `paper` VALUES (12433, 'Neural Voice Puppetry: Audio-Driven Facial Reenactment', '', '', '', '', '', 'We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis (Video, Code and Demo: https://justusthies.github.io/posts/neural-voice-puppetry/). Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_42');
INSERT INTO `paper` VALUES (12434, 'Neural Wireframe Renderer: Learning Wireframe to Image Translations', '', '', '', '', '', 'In architecture and computer-aided design, wireframes (i.e., line-based models) are widely used as basic 3D models for design evaluation and fast design iterations. However, unlike a full design file, a wireframe model lacks critical information, such as detailed shape, texture, and materials, needed by a conventional renderer to produce 2D renderings of the objects or scenes. In this paper, we bridge the information gap by generating photo-realistic rendering of indoor scenes from wireframe models in an image translation framework. While existing image synthesis methods can generate visually pleasing images for common objects such as faces and birds, these methods do not explicitly model and preserve essential structural constraints in a wireframe model, such as junctions, parallel lines, and planar surfaces. To this end, we propose a novel model based on a structure-appearance joint representation learned from both images and wireframes. In our model, structural constraints are explicitly enforced by learning a joint representation in a shared encoder network that must support the generation of both images and wireframes. Experiments on a wireframe-scene dataset show that our wireframe-to-image translation model significantly outperforms the state-of-the-art methods in both visual quality and structural integrity of generated images.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_17');
INSERT INTO `paper` VALUES (12435, 'NeuRoRA: Neural Robust Rotation Averaging', 'Robust rotation averaging', 'Message passing neural networks', '', '', '', 'Multiple rotation averaging is an essential task for structure from motion, mapping, and robot navigation. The conventional methods for this task seek parameters of the absolute orientations that agree best with the observed noisy measurements according to a robust cost function. These robust cost functions are highly nonlinear and are designed based on certain assumptions about the noise and outlier distributions. In this work, we aim to build a neural network that learns the noise patterns from the data and predict/regress the model parameters from the noisy relative orientations. The proposed network is a combination of two networks: (1) a view-graph cleaning network, which detects outlier edges in the view-graph and rectifies noisy measurements; and (2) a fine-tuning network, which fine-tunes an initialization of absolute orientations bootstrapped from the cleaned graph, in a single step. The proposed combined network is very fast, moreover, being trained on a large number of synthetic graphs, it is more accurate than the conventional iterative optimization methods.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_9');
INSERT INTO `paper` VALUES (12436, 'New Threats Against Object Detector with Non-local Block', 'Non-local block', 'Adversarial examples', 'Object detection', '', '', 'The introduction of non-local blocks to the traditional CNN architecture enhances its performance for various computer vision tasks by improving its capabilities of capturing long-range dependencies. However, the usage of non-local blocks may also introduce new threats to computer vision systems. Therefore, it is important to study the threats caused by non-local blocks before directly applying them on commercial systems. In this paper, two new threats named disappearing attack and appearing attack against object detectors with a non-local block are investigated. The former aims at misleading an object detector with a non-local block such that it is unable to detect a target object category while the latter aims at misleading the object detector such that it detects a predefined object category, which is not present in images. Different from the existing attacks against object detectors, these threats are able to be performed in long range cases. This means that the target object and the universal adversarial patches learned from the proposed algorithms can have long distance between them. To examine the threats, digital and physical experiments are conducted on Faster R-CNN with a non-local block and 6331 images from 56 videos. The experiments show that the universal patches are able to mislead the detector with greater probabilities. To explain the threats from non-local blocks, the reception fields of CNN models with and without non-local blocks are studied empirically and theoretically.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_29');
INSERT INTO `paper` VALUES (12437, 'Next-Best View Policy for 3D Reconstruction', '3d reconstruction', 'View planning', 'Reinforcement learning', '3d model dataset', '', 'Manually selecting viewpoints or using commonly available flight planners like circular path for large-scale 3D reconstruction using drones often results in incomplete 3D models. Recent works have relied on hand-engineered heuristics such as information gain to select the Next-Best Views. In this work, we present a learning-based algorithm called Scan-RL to learn a Next-Best View (NBV) Policy. To train and evaluate the agent, we created Houses3K, a dataset of 3D house models. Our experiments show that using Scan-RL, the agent can scan houses with fewer number of steps and a shorter distance compared to our baseline circular path. Experimental results also demonstrate that a single NBV policy can be used to scan multiple houses including those that were not seen during training. The link to Scan-RL’s code is available at https://github.com/darylperalta/ScanRL and Houses3K dataset can be found at https://github.com/darylperalta/Houses3K.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_33');
INSERT INTO `paper` VALUES (12438, 'Nighttime Defogging Using High-Low Frequency Decomposition and Grayscale-Color Networks', '', '', '', '', '', 'We address the problem of nighttime defogging from a single image by introducing a framework consisting of two modules: grayscale and color modules. Given an RGB foggy nighttime image, our grayscale module takes the grayscale version of the image as input, and decomposes it into high and low frequency layers. The high frequency layers contain the scene texture information, which is less affected by fog. While the low frequency layers contain the scene layout/structure information including fog and glow. Our grayscale module then enhances the visibility of the textures in the high frequency layers, and removes the presence of glow and fog in the low frequency layers. Having processed the high/low frequency information, it fuses the two layers to obtain a grayscale defogged image. Our second module, the color module, takes the original RGB image, and process it similarly to what the grayscale module does. However, to obtain fog-free high and low frequency information, the module is guided by the grayscale module. The reason of doing this is because grayscale images are less affected by multiple colors of atmospheric light, which are commonly present in nighttime scenes. Moreover, having the grayscale module allows us to have consistency losses between the outputs of the two modules, which is critical to our framework, since we do not have paired ground-truths for our real data. Our extensive experiments on real foggy nighttime images show the effectiveness of our method.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_28');
INSERT INTO `paper` VALUES (12439, 'NODIS: Neural Ordinary Differential Scene Understanding', 'Semantic image understanding', 'Scene graph', 'Visual relationship detection', '', '', 'Semantic image understanding is a challenging topic in computer vision. It requires to detect all objects in an image, but also to identify all the relations between them. Detected objects, their labels and the discovered relations can be used to construct a scene graph which provides an abstract semantic interpretation of an image. In previous works, relations were identified by solving an assignment problem formulated as (Mixed-)Integer Linear Programs. In this work, we interpret that formulation as Ordinary Differential Equation (ODE). The proposed architecture performs scene graph inference by solving a neural variant of an ODE by end-to-end learning. The connection between (Mixed-)Integer Linear Program and ODEs in combination with the end-to-end training amounts to learning how to solve assignment problems with image-specific objective functions. Intuitive, visual explanations are provided for the role of the single free variable of the ODE modules which are associated with time in many natural processes. The proposed model achieves results equal to or above state-of-the-art on all three benchmark tasks: scene graph generation (SGGEN), classification (SGCLS) and visual relationship detection (PREDCLS) on Visual Genome benchmark. The strong results on scene graph classification support the claim that assignment problems can indeed be solved by neural ODEs.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_38');
INSERT INTO `paper` VALUES (12440, 'Noise-Aware Merging of High Dynamic Range Image Stacks Without Camera Calibration', 'High dynamic range reconstruction', 'Exposure stacks', 'Camera noise', 'Computational photography', '', 'A near-optimal reconstruction of the radiance of a High Dynamic Range scene from an exposure stack can be obtained by modeling the camera noise distribution. The latent radiance is then estimated using Maximum Likelihood Estimation. But this requires a well-calibrated noise model of the camera, which is difficult to obtain in practice. We show that an unbiased estimation of comparable variance can be obtained with a simpler Poisson noise estimator, which does not require the knowledge of camera-specific noise parameters. We demonstrate this empirically for four different cameras, ranging from a smartphone camera to a full-frame mirrorless camera. Our experimental results are consistent for simulated as well as real images, and across different camera settings.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_23');
INSERT INTO `paper` VALUES (12441, 'NoiseRank: Unsupervised Label Noise Reduction with Dependence Models', 'Label noise', 'Unsupervised learning', 'Classification', '', '', 'Label noise is increasingly prevalent in datasets acquired from noisy channels. Existing approaches that detect and remove label noise generally rely on some form of supervision, which is not scalable and error-prone. In this paper, we propose NoiseRank, for unsupervised label noise reduction using Markov Random Fields (MRF). We construct a dependence model to estimate the posterior probability of an instance being incorrectly labeled given the dataset, and rank instances based on their estimated probabilities. Our method i) does not require supervision from ground-truth labels or priors on label or noise distribution, ii) is interpretable by design, enabling transparency in label noise removal, iii) is agnostic to classifier architecture/optimization framework and content modality. These advantages enable wide applicability in real noise settings, unlike prior works constrained by one or more conditions. NoiseRank improves state-of-the-art classification on Food101-N (\\(\\sim \\)20% noise), and is effective on high noise Clothing-1M (\\(\\sim \\)40% noise).', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_44');
INSERT INTO `paper` VALUES (12442, 'Noisy Student Training Using Body Language Dataset Improves Facial Expression Recognition', 'Facial expression recognition', 'Student-teacher network', 'Semi-supervised learning', 'Multi-level attention', '', 'Facial expression recognition from videos in the wild is a challenging task due to the lack of abundant labelled training data. Large DNN (deep neural network) architectures and ensemble methods have resulted in better performance, but soon reach saturation at some point due to data inadequacy. In this paper, we use a self-training method that utilizes a combination of a labelled dataset and an unlabelled dataset (Body Language Dataset - BoLD). Experimental analysis shows that training a noisy student network iteratively helps in achieving significantly better results. Additionally, our model isolates different regions of the face and processes them independently using a multi-level attention mechanism which further boosts the performance. Our results show that the proposed method achieves state-of-the-art performance on benchmark datasets CK+ and AFEW 8.0 when compared to single models.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_53');
INSERT INTO `paper` VALUES (12443, 'Non-local Spatial Propagation Network for Depth Completion', 'Depth completion', 'Non-local', 'Spatial propagation network', '', '', 'In this paper, we propose a robust and efficient end-to-end non-local spatial propagation network for depth completion. The proposed network takes RGB and sparse depth images as inputs and estimates non-local neighbors and their affinities of each pixel, as well as an initial depth map with pixel-wise confidences. The initial depth prediction is then iteratively refined by its confidence and non-local spatial propagation procedure based on the predicted non-local neighbors and corresponding affinities. Unlike previous algorithms that utilize fixed-local neighbors, the proposed algorithm effectively avoids irrelevant local neighbors and concentrates on relevant non-local neighbors during propagation. In addition, we introduce a learnable affinity normalization to better learn the affinity combinations compared to conventional methods. The proposed algorithm is inherently robust to the mixed-depth problem on depth boundaries, which is one of the major issues for existing depth estimation/completion algorithms. Experimental results on indoor and outdoor datasets demonstrate that the proposed algorithm is superior to conventional algorithms in terms of depth completion accuracy and robustness to the mixed-depth problem. Our implementation is publicly available on the project page (https://github.com/zzangjinsun/NLSPN_ECCV20).', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_8');
INSERT INTO `paper` VALUES (12444, 'NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image', '3D human reconstruction', 'Single-view 3D reconstruction', 'Single-image 3D reconstruction', 'Generation and adversarial networks.', '', 'We propose NormalGAN, a fast adversarial learning-based method to reconstruct the complete and detailed 3D human from a single RGB-D image. Given a single front-view RGB-D image, NormalGAN performs two steps: front-view RGB-D rectification and back-view RGB-D inference. The final model was then generated by simply combining the front-view and back-view RGB-D information. However, inferring back-view RGB-D image with high-quality geometric details and plausible texture is not trivial. Our key observation is: Normal maps generally encode much more information of 3D surface details than RGB and depth images. Therefore, learning geometric details from normal maps is superior than other representations. In NormalGAN, an adversarial learning framework conditioned by normal maps is introduced, which is used to not only improve the front-view depth denoising performance, but also infer the back-view depth image with surprisingly geometric details. Moreover, for texture recovery, we remove shading information from the front-view RGB image based on the refined normal map, which further improves the quality of the back-view color inference. Results and experiments on both testing data set and real captured data demonstrate the superior performance of our approach. Given a consumer RGB-D sensor, NormalGAN can generate the complete and detailed 3D human reconstruction results in 20 fps, which further enables convenient interactive experiences in telepresence, AR/VR and gaming scenarios.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_26');
INSERT INTO `paper` VALUES (12445, 'Not only Look, But Also Listen: Learning Multimodal Violence Detection Under Weak Supervision', 'Violence detection', 'Multimodality', 'Weak supervision', 'Relation networks', '', 'Violence detection has been studied in computer vision for years. However, previous work are either superficial, e.g., classification of short-clips, and the single scenario, or undersupplied, e.g., the single modality, and hand-crafted features based multimodality. To address this problem, in this work we first release a large-scale and multi-scene dataset named XD-Violence with a total duration of 217 h, containing 4754 untrimmed videos with audio signals and weak labels. Then we propose a neural network containing three parallel branches to capture different relations among video snippets and integrate features, where holistic branch captures long-range dependencies using similarity prior, localized branch captures local positional relation using proximity prior, and score branch dynamically captures the closeness of predicted score. Besides, our method also includes an approximator to meet the needs of online detection. Our method outperforms other state-of-the-art methods on our released dataset and other existing benchmark. Moreover, extensive experimental results also show the positive effect of multimodal (audio-visual) input and modeling relationships. The code and dataset will be released in https://roc-ng.github.io/XD-Violence/.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_20');
INSERT INTO `paper` VALUES (12446, 'Novel View Synthesis on Unpaired Data by Conditional Deformable Variational Auto-Encoder', 'View synthesis', 'cVAE', 'GAN', '', '', 'Novel view synthesis often needs the paired data from both the source and target views. This paper proposes a view translation model under cVAE-GAN framework without requiring the paired data. We design a conditional deformable module (CDM) which uses the view condition vectors as the filters to convolve the feature maps of the main branch in VAE. It generates several pairs of displacement maps to deform the features, like the 2D optical flows. The results are fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature, given its deformed one as the input from the side branch. Taking the advantage of the CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder takes the code drawn from it to synthesize the reconstructed and the view-translated images. To further ensure the disentanglement between the views and other factors, we add adversarial training on the code. The results and ablation studies on MultiPIE and 3D chair datasets validate the effectiveness of the framework in cVAE and the designed module.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_6');
INSERT INTO `paper` VALUES (12447, 'NSGANetV2: Evolutionary Multi-objective Surrogate-Assisted Neural Architecture Search', 'NAS', 'Evolutionary algorithms', 'Surrogate-assisted search', '', '', 'In this paper, we propose an efficient NAS algorithm for generating task-specific models that are competitive under multiple competing objectives. It comprises of two surrogates, one at the architecture level to improve sample efficiency and one at the weights level, through a supernet, to improve gradient descent training efficiency. On standard benchmark datasets (C10, C100, ImageNet), the resulting models, dubbed NSGANetV2, either match or outperform models from existing approaches with the search being orders of magnitude more sample efficient. Furthermore, we demonstrate the effectiveness and versatility of the proposed method on six diverse non-standard datasets, e.g. STL-10, Flowers102, Oxford Pets, FGVC Aircrafts etc. In all cases, NSGANetV2s improve the state-of-the-art (under mobile setting), suggesting that NAS can be a viable alternative to conventional transfer learning approaches in handling diverse scenarios such as small-scale or fine-grained datasets. Code is available at https://github.com/mikelzc1990/nsganetv2.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_3');
INSERT INTO `paper` VALUES (12448, 'Null-Sampling for Interpretable and Fair Representations', 'Fairness', 'Interpretability', 'Adversarial learning', 'Normalising flows', 'Invertible neural networks', 'We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. Invariance implies a selectivity for high level, relevant correlations w.r.t. class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished. To address this problem, we introduce an adversarially trained model with a null-sampling procedure to produce invariant representations in the data domain. To enable disentanglement, a partially-labelled representative set is used. By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors. We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset. (The code can be found at https://github.com/predictive-analytics-lab/nifr).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_34');
INSERT INTO `paper` VALUES (12449, 'Object as Hotspots: An Anchor-Free 3D Object Detection Approach via Firing of Hotspots', 'Point clouds', '3D detection', 'Inter-object point-sparsity imbalance', '', '', 'Accurate 3D object detection in LiDAR based point clouds suffers from the challenges of data sparsity and irregularities. Existing methods strive to organize the points regularly, e.g. voxelize, pass them through a designed 2D/3D neural network, and then define object-level anchors that predict offsets of 3D bounding boxes using collective evidences from all the points on the objects of interest. Contrary to the state-of-the-art anchor-based methods, based on the very nature of data sparsity, we observe that even points on an individual object part are informative about semantic information of the object. We thus argue in this paper for an approach opposite to existing methods using object-level anchors. Inspired by compositional models, which represent an object as parts and their spatial relations, we propose to represent an object as composition of its interior non-empty voxels, termed hotspots, and the spatial relations of hotspots. This gives rise to the representation of Object as Hotspots (OHS). Based on OHS, we further propose an anchor-free detection head with a novel ground truth assignment strategy that deals with inter-object point-sparsity imbalance to prevent the network from biasing towards objects with more points. Experimental results show that our proposed method works remarkably well on objects with a small number of points. Notably, our approach ranked \\(1^{st}\\) on KITTI 3D Detection Benchmark for cyclist and pedestrian detection, and achieved state-of-the-art performance on NuScenes 3D Detection Benchmark.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_5');
INSERT INTO `paper` VALUES (12450, 'Object Detection Using Clustering Algorithm Adaptive Searching Regions in Aerial Images', '', '', '', '', '', 'Aerial images are increasingly used for critical tasks, such as traffic monitoring, pedestrian tracking, and infrastructure inspection. However, aerial images have the following main challenges: 1) small objects with non-uniform distribution; 2) the large difference in object size. In this paper, we propose a new network architecture, Cluster Region Estimation Network (CRENet), to solve these challenges. CRENet uses a clustering algorithm to search cluster regions containing dense objects, which makes the detector focus on these regions to reduce background interference and improve detection efficiency. However, not every cluster region can bring precision gain, so each cluster region difficulty score is calculated to mine the difficult region and eliminate the simple cluster region, which can speed up the detection. Then, a Gaussian scaling function(GSF) is used to scale the difficult cluster region to reduce the difference of object size. Our experiments show that CRENet achieves better performance than previous approaches on the VisDrone dataset. Our best model achieved 4.3\\(\\%\\) improvement on the VisDrone dataset.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_39');
INSERT INTO `paper` VALUES (12451, 'Object Detection with a Unified Label Space from Multiple Datasets', '', '', '', '', '', 'Given multiple datasets with different label spaces, the goal of this work is to train a single object detector predicting over the union of all the label spaces. The practical benefits of such an object detector are obvious and significant—application-relevant categories can be picked and merged form arbitrary existing datasets. However, naïve merging of datasets is not possible in this case, due to inconsistent object annotations. Consider an object category like faces that is annotated in one dataset, but is not annotated in another dataset, although the object itself appears in the latter’s images. Some categories, like face here, would thus be considered foreground in one dataset, but background in another. To address this challenge, we design a framework which works with such partial annotations, and we exploit a pseudo labeling approach that we adapt for our specific case. We propose loss functions that carefully integrate partial but correct annotations with complementary but noisy pseudo labels. Evaluation in the proposed novel setting requires full annotation on the test set. We collect the required annotations (Project page: http://www.nec-labs.com/~mas/UniDet This work was part of Xiangyun Zhao’s internship at NEC Labs America.) and define a new challenging experimental setup for this task based on existing public datasets. We show improved performances compared to competitive baselines and appropriate adaptations of existing work.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_11');
INSERT INTO `paper` VALUES (12452, 'Object Retrieval and Localization in Large Art Collections Using Deep Multi-style Feature Fusion and Iterative Voting', 'Visual retrieval', 'Searching art collections', 'Feature fusion', '', '', 'The search for specific objects or motifs is essential to art history as both assist in decoding the meaning of artworks. Digitization has produced large art collections, but manual methods prove to be insufficient to analyze them. In the following, we introduce an algorithm that allows users to search for image regions containing specific motifs or objects and find similar regions in an extensive dataset, helping art historians to analyze large digitized art collections. Computer vision has presented efficient methods for visual instance retrieval across photographs. However, applied to art collections, they reveal severe deficiencies because of diverse motifs and massive domain shifts induced by differences in techniques, materials, and styles. In this paper, we present a multi-style feature fusion approach that successfully reduces the domain gap and improves retrieval results without labelled data or curated image collections. Our region-based voting with GPU-accelerated approximate nearest-neighbour search [29] allows us to find and localize even small motifs within an extensive dataset in a few seconds. We obtain state-of-the-art results on the Brueghel dataset [2, 52] and demonstrate its generalization to inhomogeneous collections with a large number of distractors.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_12');
INSERT INTO `paper` VALUES (12453, 'Object Tracking Using Spatio-Temporal Networks for Future Prediction Location', 'Object tracking', 'Trajectory prediction', 'Background motion', '', '', 'We introduce an object tracking algorithm that predicts the future locations of the target object and assists the tracker to handle object occlusion. Given a few frames of an object that are extracted from a complete input sequence, we aim to predict the object’s location in the future frames. To facilitate the future prediction ability, we follow three key observations: 1) object motion trajectory is affected significantly by camera motion; 2) the past trajectory of an object can act as a salient cue to estimate the object motion in the spatial domain; 3) previous frames contain the surroundings and appearance of the target object, which is useful for predicting the target object’s future locations. We incorporate these three observations into our method that employs a multi-stream convolutional-LSTM network. By combining the heatmap scores from our tracker (that utilises appearance inference) and the locations of the target object from our trajectory inference, we predict the final target’s location in each frame. Comprehensive evaluations show that our method sets new state-of-the-art performance on a few commonly used tracking benchmarks.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_1');
INSERT INTO `paper` VALUES (12454, 'Object-and-Action Aware Model for Visual Language Navigation', 'Vision-and-Language Navigation', 'Modular network', 'Reward shaping', '', '', 'Vision-and-Language Navigation (VLN) is unique in that it requires turning relatively general natural-language instructions into robot agent actions, on the basis of visible environments. This requires to extract value from two very different types of natural-language information. The first is object description (e.g., ‘table’, ‘door’), each presenting as a tip for the agent to determine the next action by finding the item visible in the environment, and the second is action specification (e.g., ‘go straight’, ‘turn left’) which allows the robot to directly predict the next movements without relying on visual perceptions. However, most existing methods pay few attention to distinguish these information from each other during instruction encoding and mix together the matching between textual object/action encoding and visual perception/orientation features of candidate viewpoints. In this paper, we propose an Object-and-Action Aware Model (OAAM) that processes these two different forms of natural language based instruction separately. This enables each process to match object-centered/action-centered instruction to their own counterpart visual perception/action orientation flexibly. However, one side-issue caused by above solution is that an object mentioned in instructions may be observed in the direction of two or more candidate viewpoints, thus the OAAM may not predict the viewpoint on the shortest path as the next action. To handle this problem, we design a simple but effective path loss to penalize trajectories deviating from the ground truth path. Experimental results demonstrate the effectiveness of the proposed model and path loss, and the superiority of their combination with a \\(50\\%\\) SPL score on the R2R dataset and a \\(40\\%\\) CLS score on the R4R dataset in unseen environments, outperforming the previous state-of-the-art.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_18');
INSERT INTO `paper` VALUES (12455, 'Object-Based Illumination Estimation with Rendering-Aware Neural Networks', '', '', '', '', '', 'We present a scheme for fast environment light estimation from the RGBD appearance of individual objects and their local image areas. Conventional inverse rendering is too computationally demanding for real-time applications, and the performance of purely learning-based techniques may be limited by the meager input data available from individual objects. To address these issues, we propose an approach that takes advantage of physical principles from inverse rendering to constrain the solution, while also utilizing neural networks to expedite the more computationally expensive portions of its processing, to increase robustness to noisy input data as well as to improve temporal and spatial stability. This results in a rendering-aware system that estimates the local illumination distribution at an object with high accuracy and in real time. With the estimated lighting, virtual objects can be rendered in AR scenarios with shading that is consistent to the real scene, leading to improved realism.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_23');
INSERT INTO `paper` VALUES (12456, 'Object-Contextual Representations for Semantic Segmentation', 'Semantic segmentation', 'Context aggregation', '', '', '', 'In this paper, we study the context aggregation problem in semantic segmentation. Motivated by that the label of a pixel is the category of the object that the pixel belongs to, we present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of the ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, we compute the relation between each pixel and each object region, and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations. We empirically demonstrate our method achieves competitive performance on various benchmarks: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. Our submission “HRNet + OCR + SegFix” achieves the \\({1}^{\\mathrm {st}}\\) place on the Cityscapes leaderboard by the ECCV 2020 submission deadline. Code is available at: https://git.io/openseg and https://git.io/HRNet.OCR.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_11');
INSERT INTO `paper` VALUES (12457, 'Occlusion-Aware Depth Estimation with Adaptive Normal Constraints', 'Multi-view depth estimation', 'Normal constraint', 'Occlusion-aware strategy', 'Deep learning', '', 'We present a new learning-based method for multi-frame depth estimation from a color video, which is a fundamental problem in scene understanding, robot navigation or handheld 3D reconstruction. While recent learning-based methods estimate depth at high accuracy, 3D point clouds exported from their depth maps often fail to preserve important geometric feature (e.g., corners, edges, planes) of man-made scenes. Widely-used pixel-wise depth errors do not specifically penalize inconsistency on these features. These inaccuracies are particularly severe when subsequent depth reconstructions are accumulated in an attempt to scan a full environment with man-made objects with this kind of features. Our depth estimation algorithm therefore introduces a Combined Normal Map (CNM) constraint, which is designed to better preserve high-curvature features and global planar regions. In order to further improve the depth estimation accuracy, we introduce a new occlusion-aware strategy that aggregates initial depth predictions from multiple adjacent views into one final depth map and one occlusion probability map for the current reference view. Our method outperforms the state-of-the-art in terms of depth estimation accuracy, and preserves essential geometric features of man-made indoor scenes much better than other algorithms.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_37');
INSERT INTO `paper` VALUES (12458, 'Occlusion-Aware Siamese Network for Human Pose Estimation', 'Siamese network', 'Occlusion', 'Human pose estimation', '', '', 'Pose estimation usually suffers from varying degrees of performance degeneration owing to occlusion. To conquer this dilemma, we propose an occlusion-aware siamese network to improve the performance. Specifically, we introduce scheme of feature erasing and reconstruction. Firstly, we utilize attention mechanism to predict the occlusion-aware attention map which is explicitly supervised and clean the feature map which is contaminated by different types of occlusions. Nevertheless, the cleaning procedure not only removes the useless information but also erases some valuable details. To overcome the defects caused by the erasing operation, we perform feature reconstruction to recover the information destroyed by occlusion and details lost in cleaning procedure. To make reconstructed features more precise and informative, we adopt siamese network equipped with OT divergence to guide the features of occluded images towards those of the un-occluded images. Algorithm is validated on MPII, LSP and COCO benchmarks and we achieve promising results.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_24');
INSERT INTO `paper` VALUES (12459, 'Occupancy Anticipation for Efficient Exploration and Navigation', '', '', '', '', '', 'State-of-the-art navigation methods leverage a spatial memory to generalize to new environments, but their occupancy maps are limited to capturing the geometric structures directly observed by the agent. We propose occupancy anticipation, where the agent uses its egocentric RGB-D observations to infer the occupancy state beyond the visible regions. In doing so, the agent builds its spatial awareness more rapidly, which facilitates efficient exploration and navigation in 3D environments. By exploiting context in both the egocentric views and top-down maps our model successfully anticipates a broader map of the environment, with performance significantly better than strong baselines. Furthermore, when deployed for the sequential decision-making tasks of exploration and navigation, our model outperforms state-of-the-art methods on the Gibson and Matterport3D datasets. Our approach is the winning entry in the 2020 Habitat PointNav Challenge. Project page: http://vision.cs.utexas.edu/projects/occupancy_anticipation/.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_24');
INSERT INTO `paper` VALUES (12460, 'Ocean: Object-Aware Anchor-Free Tracking', 'Visual tracking', 'Anchor-free', 'Object-aware', '', '', 'Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., \\(IoU\\ge 0.6\\)). This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classification of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on five benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at https://github.com/researchmm/TracKit.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_46');
INSERT INTO `paper` VALUES (12461, 'ODIN: An Object Detection and Instance Segmentation Diagnosis Framework', 'Object detection', 'Instance segmentation', 'Metrics', 'Evaluation', '', 'Object detection and instance segmentation are major tasks in Computer Vision and have substantially progressed after the introduction of Deep Convolutional Neural Network (DCNN). Analyzing the performance of DCNNs is an open research issue, addressed with attention techniques that inspect the response of inner network layers to input stimuli. A complementary approach relies on the black-box diagnosis of errors, which exploits ad hoc metadata on the input data set and factors the performance into indicators sensible or impacted by specific facets of the input (e.g., object size, presence of occlusions, image acquisition conditions, etc.). In this paper we present an open source error diagnosis framework for object detection and instance segmentation that helps model developers to add meta-annotations to their data sets, to compute performance metrics split by meta-annotation values, and to visualize diagnosis reports. The framework accepts the popular PASCAL VOC and MS COCO input formats, is agnostic to the training platform, and can be extended with application- and domain-specific meta-annotations and metrics with almost no coding.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_3');
INSERT INTO `paper` VALUES (12462, 'Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search', 'Neural architecture search', 'Generative adversarial networks', 'Reinforcement learning', 'Markov decision process', 'Off-policy', 'In this paper, we introduce a new reinforcement learning (RL) based neural architecture search (NAS) methodology for effective and efficient generative adversarial network (GAN) architecture search. The key idea is to formulate the GAN architecture search problem as a Markov decision process (MDP) for smoother architecture sampling, which enables a more effective RL-based search algorithm by targeting the potential global optimal architecture. To improve efficiency, we exploit an off-policy GAN architecture search algorithm that makes efficient use of the samples generated by previous policies. Evaluation on two standard benchmark datasets (i.e., CIFAR-10 and STL-10) demonstrates that the proposed method is able to discover highly competitive architectures for generally better image generation results with a considerably reduced computational burden: 7 GPU hours. Our code is available at https://github.com/Yuantian013/E2GAN.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_11');
INSERT INTO `paper` VALUES (12463, 'OID: Outlier Identifying and Discarding in Blind Image Deblurring', 'Blind deblurring', 'Outliers', 'Identifying and discarding', '', '', 'Blind deblurring methods are sensitive to outliers, such as saturated pixels and non-Gaussian noise. Even a small amount of outliers can dramatically degrade the quality of the estimated blur kernel, because the outliers are not conforming to the linear formation of the blurring process. Prior arts develop sophisticated edge-selecting steps or noise filtering pre-processing steps to deal with outliers (i.e. indirect approaches). However, these indirect approaches may fail when massive outliers are presented, since informative details may be polluted by outliers or erased during the pre-processing steps. To address these problems, this paper develops a simple yet effective Outlier Identifying and Discarding (OID) method, which alleviates limitations in existing Maximum A Posteriori (MAP)-based deblurring models when significant outliers are presented. Unlike previous indirect outlier processing methods, OID tackles outliers directly by explicitly identifying and discarding them, when updating both the latent image and the blur kernel during the deblurring process, where the outliers are detected by using the sparse and entropy-based modules. OID is easy to implement and extendable for non-blind restoration. Extensive experiments demonstrate the superiority of OID against recent works both quantitatively and qualitatively.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_36');
INSERT INTO `paper` VALUES (12464, 'Omni-Sourced Webly-Supervised Learning for Video Recognition', '', '', '', '', '', 'We introduce OmniSource, a novel framework for leveraging web data to train video recognition models. OmniSource overcomes the barriers between data formats, such as images, short videos, and long untrimmed videos for webly-supervised learning. First, data samples with multiple formats, curated by task-specific data collection and automatically filtered by a teacher model, are transformed into a unified form. Then a joint-training strategy is proposed to deal with the domain gaps between multiple data sources and formats in webly-supervised learning. Several good practices, including data balancing, resampling, and cross-dataset mixup are adopted in joint training. Experiments show that by utilizing data from multiple sources and formats, OmniSource is more data-efficient in training. With only 3.5M images and 800K min videos crawled from the internet without human labeling (less than \\(2\\%\\) of prior works), our models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet baseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark. With OmniSource, we establish new records with different pretraining strategies for video recognition. Our best models achieve 80.4%, 80.5%, and 83.6% Top-1 accuracies on the Kinetics-400 benchmark respectively for training-from-scratch, ImageNet pre-training and IG-65M pre-training.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_40');
INSERT INTO `paper` VALUES (12465, 'On Disentangling Spoof Trace for Generic Face Anti-spoofing', '', '', '', '', '', 'Prior studies show that the key to face anti-spoofing lies in the subtle image pattern, termed “spoof trace\", e.g., color distortion, 3D mask edge, Moiré pattern, and many others. Designing a generic anti-spoofing model to estimate those spoof traces can improve both generalization and interpretability. Yet, this is a challenging task due to the diversity of spoof types and the lack of ground truth. This work designs a novel adversarial learning framework to disentangle the spoof traces from input faces as a hierarchical combination of patterns. With the disentangled spoof traces, we unveil the live counterpart from spoof face, and synthesize realistic new spoof faces after a proper geometric correction. Our method demonstrates superior spoof detection performance on both seen and unseen spoof scenarios while providing visually-convincing estimation of spoof traces. Code is available at https://github.com/yaojieliu/ECCV20-STDN.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_24');
INSERT INTO `paper` VALUES (12466, 'On Diverse Asynchronous Activity Anticipation', '', '', '', '', '', 'We investigate the joint anticipation of long-term activity labels and their corresponding times with the aim of improving both the naturalness and diversity of predictions. We address these matters using Conditional Adversarial Generative Networks for Discrete Sequences. Central to our approach is a reexamination of the unavoidable sample quality vs. diversity tradeoff of the recently emerged Gumbel-Softmax relaxation based GAN on discrete data. In particular, we ameliorate this trade-off with a simple but effective sample distance regularizer. Moreover, we provide a unified approach to inference of activity labels and their times so that a single integrated optimization succeeds for both. With this novel approach in hand, we demonstrate the effectiveness of the resulting discrete sequential GAN on multimodal activity anticipation. We evaluate the approach on three standard datasets and show that it outperforms previous approaches in terms of both accuracy and diversity, thereby yielding a new state-of-the-art in activity anticipation.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_46');
INSERT INTO `paper` VALUES (12467, 'On Dropping Clusters to Regularize Graph Convolutional Neural Networks', 'Regularization', 'Dropout', 'Graph convolutional networks', '', '', 'Dropout has been widely adopted to regularize graph convolutional networks (GCNs) by randomly zeroing entries of the node feature vectors and obtains promising performance on various tasks. However, the information of individually zeroed entries could still present in other correlated entries by propagating (1) spatially between entries of different node feature vectors and (2) depth-wisely between different entries of each node feature vector, which essentially weakens the effectiveness of dropout. This is mainly because in a GCN, neighboring node feature vectors after linear transformations are aggregated to produce new node feature vectors in the subsequent layer. To effectively regularize GCNs, we devise DropCluster which first randomly zeros some seed entries and then zeros entries that are spatially or depth-wisely correlated to those seed entries. In this way, the information of the seed entries is thoroughly removed and cannot flow to subsequent layers via the correlated entries. We validate the effectiveness of the proposed DropCluster by comprehensively comparing it with dropout and its representative variants, such as SpatialDropout, Gaussian dropout and DropEdge, on skeleton-based action recognition.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_15');
INSERT INTO `paper` VALUES (12468, 'On Modulating the Gradient for Meta-learning', 'Meta learning', 'Few shot learning', 'Adaptive gradients', '', '', 'Inspired by optimization techniques, we propose a novel meta-learning algorithm with gradient modulation to encourage fast-adaptation of neural networks in the absence of abundant data. Our method, termed ModGrad, is designed to circumvent the noisy nature of the gradients which is prevalent in low-data regimes. Furthermore and having the scalability concern in mind, we formulate ModGrad via low-rank approximations, which in turn enables us to employ ModGrad to adapt hefty neural networks. We thoroughly assess and contrast ModGrad against a large family of meta-learning techniques and observe that the proposed algorithm outperforms baselines comfortably while enjoying faster convergence.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_33');
INSERT INTO `paper` VALUES (12469, 'On Sparse Connectivity, Adversarial Robustness, and a Novel Model of the Artificial Neuron', 'Sparse neural networks', 'Unsupervised training', 'Adversarial robustness', '', '', 'In this paper, we propose two closely connected methods to improve computational efficiency and stability against adversarial perturbations on contour recognition tasks: (a) a novel model of an artificial neuron, a “strong neuron,” with inherent robustness against adversarial perturbations and (b) a novel constructive training algorithm that generates sparse networks with O(1) connections per neuron.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_25');
INSERT INTO `paper` VALUES (12470, 'On the Effectiveness of Image Rotation for Open Set Domain Adaptation', 'Open Set Domain Adaptation', 'Self-supervised learning', '', '', '', 'Open Set Domain Adaptation (OSDA) bridges the domain gap between a labeled source domain and an unlabeled target domain, while also rejecting target classes that are not present in the source. To avoid negative transfer, OSDA can be tackled by first separating the known/unknown target samples and then aligning known target samples with the source data. We propose a novel method to addresses both these problems using the self-supervised task of rotation recognition. Moreover, we assess the performance with a new open set metric that properly balances the contribution of recognizing the known classes and rejecting the unknown samples. Comparative experiments with existing OSDA methods on the standard Office-31 and Office-Home benchmarks show that: (i) our method outperforms its competitors, (ii) reproducibility for this field is a crucial issue to tackle, (iii) our metric provides a reliable tool to allow fair open set evaluation.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_25');
INSERT INTO `paper` VALUES (12471, 'On the Usage of the Trifocal Tensor in Motion Segmentation', 'Motion segmentation', 'Structure from motion', 'Multi-model fitting', 'Trifocal tensor', '', 'Motion segmentation, i.e., the problem of clustering data in multiple images based on different 3D motions, is an important task for reconstructing and understanding dynamic scenes. In this paper we address motion segmentation in multiple images by combining partial results coming from triplets of images, which are obtained by fitting a number of trifocal tensors to correspondences. We exploit the fact that the trifocal tensor is a stronger model than the fundamental matrix, as it provides fewer but more reliable matches over three images than fundamental matrices provide over the two. We also consider an alternative solution which merges partial results coming from both triplets and pairs of images, showing the strength of three-frame segmentation in a combination with two-frame segmentation. Our real experiments on standard as well as new datasets demonstrate the superior accuracy of the proposed approaches when compared to previous techniques .', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_31');
INSERT INTO `paper` VALUES (12472, 'On Transferability of Histological Tissue Labels in Computational Pathology', 'Cancer detection', 'Cancer grade classification', 'Deep learning', 'Domain adaptation', 'Zero-shot transfer', 'Deep learning tools in computational pathology, unlike natural vision tasks, face with limited histological tissue labels for classification. This is due to expensive procedure of annotation done by expert pathologist. As a result, the current models are limited to particular diagnostic task in mind where the training workflow is repeated for different organ sites and diseases. In this paper, we explore the possibility of transferring diagnostically-relevant histology labels from a source-domain into multiple target-domains to classify similar tissue structures and cancer grades. We achieve this by training a Convolutional Neural Network (CNN) model on a source-domain of diverse histological tissue labels for classification and then transfer them to different target domains for diagnosis without re-training/fine-tuning (zero-shot). We expedite this by an efficient color augmentation to account for color disparity across different tissue scans and conduct thorough experiments for evaluation.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_27');
INSERT INTO `paper` VALUES (12473, 'One Weight Bitwidth to Rule Them All', 'Model compression', 'Deep learning architectures', 'Quantization', 'ConvNets', 'Image classification', 'Weight quantization for deep ConvNets has shown promising results for applications such as image classification and semantic segmentation and is especially important for applications where memory storage is limited. However, when aiming for quantization without accuracy degradation, different tasks may end up with different bitwidths. This creates complexity for software and hardware support and the complexity accumulates when one considers mixed-precision quantization, in which case each layer’s weights use a different bitwidth. Our key insight is that optimizing for the least bitwidth subject to no accuracy degradation is not necessarily an optimal strategy. This is because one cannot decide optimality between two bitwidths if one has smaller model size while the other has better accuracy. In this work, we take the first step to understand if some weight bitwidth is better than others by aligning all to the same model size using a width-multiplier. Under this setting, somewhat surprisingly, we show that using a single bitwidth for the whole network can achieve better accuracy compared to mixed-precision quantization targeting zero accuracy degradation when both have the same model size. In particular, our results suggest that when the number of channels becomes a target hyperparameter, a single weight bitwidth throughout the network shows superior results for model compression.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_7');
INSERT INTO `paper` VALUES (12474, 'One-Pixel Signature: Characterizing CNN Models for Backdoor Detection', 'Backdoor detection', 'Convolutional neural networks', 'Trojan attack', 'Backdoor trigger', 'Adversarial learning', 'We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is to detect/classify if a CNN model has been maliciously inserted with an unknown Trojan trigger or not. We design the one-pixel signature representation to reveal the characteristics of both clean and backdoored CNN models. Here, each CNN model is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choice of CNN architectures, and how they were trained. It can be computed efficiently for a black-box CNN model without accessing the network parameters. Our proposed one-pixel signature demonstrates a substantial improvement (by around 30% in the absolute detection accuracy) over the existing competing methods for backdoored CNN detection/classification. One-pixel signature is a general representation that can be used to characterize CNN models beyond backdoor detection.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_20');
INSERT INTO `paper` VALUES (12475, 'One-Shot Unsupervised Cross-Domain Detection', 'Object detection', 'Cross-domain analysis', 'Self-supervision', '', '', 'Despite impressive progress in object detection over the last years, it is still an open challenge to reliably detect objects across visual domains. All current approaches access a sizable amount of target data at training time. This is a heavy assumption, as often it is not possible to anticipate the domain where a detector will be used, nor to access it in advance for data acquisition. Consider for instance the task of monitoring image feeds from social media: as every image is uploaded by a different user it belongs to a different target domain that is impossible to foresee during training. Our work addresses this setting, presenting an object detection algorithm able to perform unsupervised adaptation across domains by using only one target sample, seen at test time. We introduce a multi-task architecture that one-shot adapts to any incoming sample by iteratively solving a self-supervised task on it. We further enhance this auxiliary adaptation with cross-task pseudo-labeling. A thorough benchmark analysis against the most recent cross-domain detection methods and a detailed ablation study show the advantage of our approach.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_43');
INSERT INTO `paper` VALUES (12476, 'OneGAN: Simultaneous Unsupervised Learning of Conditional Image Generation, Foreground Segmentation, and Fine-Grained Clustering', '', '', '', '', '', 'We present a method for simultaneously learning, in an unsupervised manner, (i) a conditional image generator, (ii) foreground extraction and segmentation, (iii) clustering into a two-level class hierarchy, and (iv) object removal and background completion, all done without any use of annotation. The method combines a Generative Adversarial Network and a Variational Auto-Encoder, with multiple encoders, generators and discriminators, and benefits from solving all tasks at once. The input to the training scheme is a varied collection of unlabeled images from the same domain, as well as a set of background images without a foreground object. In addition, the image generator can mix the background from one image, with a foreground that is conditioned either on that of a second image or on the index of a desired cluster. The method obtains state of the art results in comparison to the literature methods, when compared to the current state of the art in each of the tasks.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_31');
INSERT INTO `paper` VALUES (12477, 'Online Continual Learning Under Extreme Memory Constraints', 'Continual Learning', 'Online learning', 'Memory efficient', '', '', 'Continual Learning (CL) aims to develop agents emulating the human ability to sequentially learn new tasks while being able to retain knowledge obtained from past experiences. In this paper, we introduce the novel problem of Memory-Constrained Online Continual Learning (MC-OCL) which imposes strict constraints on the memory overhead that a possible algorithm can use to avoid catastrophic forgetting. As most, if not all, previous CL methods violate these constraints, we propose an algorithmic solution to MC-OCL: Batch-level Distillation (BLD), a regularization-based CL approach, which effectively balances stability and plasticity in order to learn from data streams, while preserving the ability to solve old tasks through distillation. Our extensive experimental evaluation, conducted on three publicly available benchmarks, empirically demonstrates that our approach successfully addresses the MC-OCL problem and achieves comparable accuracy to prior distillation methods requiring higher memory overhead (Code available at https://github.com/DonkeyShot21/batch-level-distillation).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_43');
INSERT INTO `paper` VALUES (12478, 'Online Ensemble Model Compression Using Knowledge Distillation', 'Deep model compression', 'Image classification', 'Knowledge distillation', 'Ensemble deep model training', '', 'This paper presents a novel knowledge distillation based model compression framework consisting of a student ensemble. It enables distillation of simultaneously learnt ensemble knowledge onto each of the compressed student models. Each model learns unique representations from the data distribution due to its distinct architecture. This helps the ensemble generalize better by combining every model’s knowledge. The distilled students and ensemble teacher are trained simultaneously without requiring any pretrained weights. Moreover, our proposed method can deliver multi-compressed students with single training, which is efficient and flexible for different scenarios. We provide comprehensive experiments using state-of-the-art classification models to validate our framework’s effectiveness. Notably, using our framework a 97% compressed ResNet110 student model managed to produce a 10.64% relative accuracy gain over its individual baseline training on CIFAR100 dataset. Similarly a 95% compressed DenseNet-BC (k = 12) model managed a 8.17% relative accuracy gain.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_2');
INSERT INTO `paper` VALUES (12479, 'Online Invariance Selection for Local Feature Descriptors', 'Local descriptors', 'Invariance', 'Visual localization', '', '', 'To be invariant, or not to be invariant: that is the question formulated in this work about local descriptors. A limitation of current feature descriptors is the trade-off between generalization and discriminative power: more invariance means less informative descriptors. We propose to overcome this limitation with a disentanglement of invariance in local descriptors and with an online selection of the most appropriate invariance given the context. Our framework (https://github.com/rpautrat/LISRD) consists in a joint learning of multiple local descriptors with different levels of invariance and of meta descriptors encoding the regional variations of an image. The similarity of these meta descriptors across images is used to select the right invariance when matching the local descriptors. Our approach, named Local Invariance Selection at Runtime for Descriptors (LISRD), enables descriptors to adapt to adverse changes in images, while remaining discriminative when invariance is not required. We demonstrate that our method can boost the performance of current descriptors and outperforms state-of-the-art descriptors in several matching tasks, when evaluated on challenging datasets with day-night illumination as well as viewpoint changes.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_42');
INSERT INTO `paper` VALUES (12480, 'Online Meta-learning for Multi-source and Semi-supervised Domain Adaptation', 'Meta-learning', 'Domain adaptation', '', '', '', 'Domain adaptation (DA) is the topical problem of adapting models from labelled source datasets so that they perform well on target datasets where only unlabelled or partially labelled data is available. Many methods have been proposed to address this problem through different ways to minimise the domain shift between source and target datasets. In this paper we take an orthogonal perspective and propose a framework to further enhance performance by meta-learning the initial conditions of existing DA algorithms. This is challenging compared to the more widely considered setting of few-shot meta-learning, due to the length of the computation graph involved. Therefore we propose an online shortest-path meta-learning framework that is both computationally tractable and practically effective for improving DA performance. We present variants for both multi-source unsupervised domain adaptation (MSDA), and semi-supervised domain adaptation (SSDA). Importantly, our approach is agnostic to the base adaptation algorithm, and can be applied to improve many techniques. Experimentally, we demonstrate improvements on classic (DANN) and recent (MCD and MME) techniques for MSDA and SSDA, and ultimately achieve state of the art results on several DA benchmarks including the largest scale DomainNet.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_23');
INSERT INTO `paper` VALUES (12481, 'Online Multi-modal Person Search in Videos', 'Online person search', 'Multi-modality', 'Dynamic memory bank', 'Uncertain instance cache', 'Reinforcement learning', 'The task of searching certain people in videos has seen increasing potential in real-world applications, such as video organization and editing. Most existing approaches are devised to work in an offline manner, where identities can only be inferred after an entire video is examined. This working manner precludes such methods from being applied to online services or those applications that require real-time responses. In this paper, we propose an online person search framework, which can recognize people in a video on the fly. This framework maintains a multi-modal memory bank at its heart as the basis for person recognition, and updates it dynamically with a policy obtained by reinforcement learning. Our experiments on a large movie dataset show that the proposed method is effective, not only achieving remarkable improvements over online schemes but also outperforming offline methods.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_11');
INSERT INTO `paper` VALUES (12482, 'OnlineAugment: Online Data Augmentation with Less Domain Knowledge', '', '', '', '', '', 'Data augmentation is one of the most important tools in training modern deep neural networks. Recently, great advances have been made in searching for optimal augmentation policies in the image classification domain. However, two key points related to data augmentation remain uncovered by the current methods. First is that most if not all modern augmentation search methods are offline and learning policies are isolated from their usage. The learned policies are mostly constant throughout the training process and are not adapted to the current training model state. Second, the policies rely on class-preserving image processing functions. Hence applying current offline methods to new tasks may require domain knowledge to specify such kind of operations. In this work, we offer an orthogonal online data augmentation scheme together with three new augmentation networks, co-trained with the target learning task. It is both more efficient, in the sense that it does not require expensive offline training when entering a new domain, and more adaptive as it adapts to the learner state. Our augmentation networks require less domain knowledge and are easily applicable to new tasks. Extensive experiments demonstrate that the proposed scheme alone performs on par with the state-of-the-art offline data augmentation methods, as well as improving upon the state-of-the-art in combination with those methods.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_19');
INSERT INTO `paper` VALUES (12483, 'Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions', '', '', '', '', '', 'We propose a novel algorithm, named Open-Edit, which is the first attempt on open-domain image manipulation with open-vocabulary instructions. It is a challenging task considering the large variation of image domains and the lack of training supervision. Our approach takes advantage of the unified visual-semantic embedding space pretrained on a general image-caption dataset, and manipulates the embedded visual features by applying text-guided vector arithmetic on the image feature maps. A structure-preserving image decoder then generates the manipulated images from the manipulated feature maps. We further propose an on-the-fly sample-specific optimization approach with cycle-consistency constraints to regularize the manipulated images and force them to preserve details of the source images. Our approach shows promising results in manipulating open-vocabulary color, texture, and high-level attributes for various scenarios of open-domain images (Code is released at https://github.com/xh-liu/Open-Edit).', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_6');
INSERT INTO `paper` VALUES (12484, 'Open-Set Adversarial Defense', 'Adversarial defense', 'Open-set recognition', '', '', '', 'Open-set recognition and adversarial defense study two key aspects of deep learning that are vital for real-world deployment. The objective of open-set recognition is to identify samples from open-set classes during testing, while adversarial defense aims to defend the network against images with imperceptible adversarial perturbations. In this paper, we show that open-set recognition systems are vulnerable to adversarial attacks. Furthermore, we show that adversarial defense mechanisms trained on known classes do not generalize well to open-set samples. Motivated by this observation, we emphasize the need of an Open-Set Adversarial Defense (OSAD) mechanism. This paper proposes an Open-Set Defense Network (OSDN) as a solution to the OSAD problem. The proposed network uses an encoder with feature-denoising layers coupled with a classifier to learn a noise-free latent feature representation. Two techniques are employed to obtain an informative latent feature space with the objective of improving open-set performance. First, a decoder is used to ensure that clean images can be reconstructed from the obtained latent features. Then, self-supervision is used to ensure that the latent features are informative enough to carry out an auxiliary task. We introduce a testing protocol to evaluate OSAD performance and show the effectiveness of the proposed method in multiple object classification datasets. The implementation code of the proposed method is available at: https://github.com/rshaojimmy/ECCV2020-OSAD.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_40');
INSERT INTO `paper` VALUES (12485, 'Optical Flow Distillation: Towards Efficient and Stable Video Style Transfer', 'Knowledge distillation', 'Optical flow', 'Video style transfer', '', '', 'Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g. occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_37');
INSERT INTO `paper` VALUES (12486, 'Orderly Disorder in Point Cloud Domain', 'Point cloud', 'Deep neural network', 'Orderly disorder', 'Segmentation', 'Classification', 'In the real world, out-of-distribution samples, noise and distortions exist in test data. Existing deep networks developed for point cloud data analysis are prone to overfitting and a partial change in test data leads to unpredictable behaviour of the networks. In this paper, we propose a smart yet simple deep network for analysis of 3D models using ‘orderly disorder’ theory. Orderly disorder is a way of describing the complex structure of disorders within complex systems. Our method extracts the deep patterns inside a 3D object via creating a dynamic link to seek the most stable patterns and at once, throws away the unstable ones. Patterns are more robust to changes in data distribution, especially those that appear in the top layers. Features are extracted via an innovative cloning decomposition technique and then linked to each other to form stable complex patterns. Our model alleviates the vanishing-gradient problem, strengthens dynamic link propagation and substantially reduces the number of parameters. Extensive experiments on challenging benchmark datasets verify the superiority of our light network on the segmentation and classification tasks, especially in the presence of noise wherein our network’s performance drops less than 10% while the state-of-the-art networks fail to work.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_30');
INSERT INTO `paper` VALUES (12487, 'Orientation-Aware Vehicle Re-Identification with Semantics-Guided Part Attention Network', 'Vehicle re-identification', 'Spatial attention', 'Semantics-guided learning', 'Visibility-aware features', '', 'Vehicle re-identification (re-ID) focuses on matching images of the same vehicle across different cameras. It is fundamentally challenging because differences between vehicles are sometimes subtle. While several studies incorporate spatial-attention mechanisms to help vehicle re-ID, they often require expensive keypoint labels or suffer from noisy attention mask if not trained with expensive labels. In this work, we propose a dedicated Semantics-guided Part Attention Network (SPAN) to robustly predict part attention masks for different views of vehicles given only image-level semantic labels during training. With the help of part attention masks, we can extract discriminative features in each part separately. Then we introduce Co-occurrence Part-attentive Distance Metric (CPDM) which places greater emphasis on co-occurrence vehicle parts when evaluating the feature distance of two images. Extensive experiments validate the effectiveness of the proposed method and show that our framework outperforms the state-of-the-art approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_20');
INSERT INTO `paper` VALUES (12488, 'OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features', 'One-shot detection', 'Object detection', 'Few-shot learning', '', '', 'In this paper, we consider the task of one-shot object detection, which consists in detecting objects defined by a single demonstration. Differently from the standard object detection, the classes of objects used for training and testing do not overlap. We build the one-stage system that performs localization and recognition jointly. We use dense correlation matching of learned local features to find correspondences, a feed-forward geometric transformation model to align features and bilinear resampling of the correlation tensor to compute the detection score of the aligned features. All the components are differentiable, which allows end-to-end training. Experimental evaluation on several challenging domains (retail products, 3D objects, buildings and logos) shows that our method can detect unseen classes (e.g., toothpaste when trained on groceries) and outperforms several baselines by a significant margin. Our code is available online: https://github.com/aosokin/os2d.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_38');
INSERT INTO `paper` VALUES (12489, 'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks', 'Object semantics', 'Vision-and-language', 'Pre-training', '', '', 'Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks (The code and pre-trained models are released: https://github.com/microsoft/Oscar).', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_8');
INSERT INTO `paper` VALUES (12490, 'P\\(^{2}\\)Net: Patch-Match and Plane-Regularization for Unsupervised Indoor Depth Estimation', 'Unsupervised depth estimation', 'Patch-based representation', 'Multiview photometric consistency', 'Piece-wise planar loss', '', 'This paper tackles the unsupervised depth estimation task in indoor environments. The task is extremely challenging because of the vast areas of non-texture regions in these scenes. These areas could overwhelm the optimization process in the commonly used unsupervised depth estimation framework proposed for outdoor environments. However, even when those regions are masked out, the performance is still unsatisfactory. In this paper, we argue that the poor performance suffers from the non-discriminative point-based matching. To this end, we propose P\\(^2\\)Net. We first extract points with large local gradients and adopt patches centered at each point as its representation. Multiview consistency loss is then defined over patches. This operation significantly improves the robustness of the network training. Furthermore, because those textureless regions in indoor scenes (e.g., wall, floor, roof, etc.) usually correspond to planar regions, we propose to leverage superpixels as a plane prior. We enforce the predicted depth to be well fitted by a plane within each superpixel. Extensive experiments on NYUv2 and ScanNet show that our P\\(^2\\)Net outperforms existing approaches by a large margin.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_13');
INSERT INTO `paper` VALUES (12491, 'PackDet: Packed Long-Head Object Detector', 'Object detection', 'Anchor-free', 'Packing features', 'Long head', '', 'State-of-the-art object detectors exploit multi-branch structure and predict objects at several different scales, although substantially boosted accuracy is acquired, low efficiency is inevitable as fragmented structure is hardware unfriendly. To solve this issue, we propose a packing operator (PackOp) to combine all head branches together at spatial. Packed features are computationally more efficient and allow to use cross-head group normalization (GN) at handy, leading to notable accuracy improvement against the common head-separate GN. All of these are only at the cost of less than 5.7% relative increase on runtime memory and introduction of a few noisy training samples, however, whose side-effects could be diminished by good packing patterns design. With PackOp, we propose a new anchor-free one-stage detector, PackDet, which features a single deeper/longer but narrower head compared to the existing methods: multiple shallow but wide heads. Our best models on COCO test-dev achieve better speed-accuracy balance: 35.1%, 42.3%, 44.0%, 47.4% AP with 22.6, 16.9, 12.4, 4.7 FPS using MobileNet-v2, ResNet-50, ResNet-101, and ResNeXt-101-DCN backbone, respectively. Codes will be released.(https://github.com/kding1225/PackDet)', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_11');
INSERT INTO `paper` VALUES (12492, 'Pairwise Similarity Knowledge Transfer for Weakly Supervised Object Localization', 'Weakly supervised object localization', 'Transfer learning', 'Multiple instance learning', 'Object detection', '', 'Weakly Supervised Object Localization (WSOL) methods only require image level labels as opposed to expensive bounding box annotations required by fully supervised algorithms. We study the problem of learning localization model on target classes with weakly supervised image labels, helped by a fully annotated source dataset. Typically, a WSOL model is first trained to predict class generic objectness scores on an off-the-shelf fully supervised source dataset and then it is progressively adapted to learn the objects in the weakly supervised target dataset. In this work, we argue that learning only an objectness function is a weak form of knowledge transfer and propose to learn a classwise pairwise similarity function that directly compares two input proposals as well. The combined localization model and the estimated object annotations are jointly learned in an alternating optimization paradigm as is typically done in standard WSOL methods. In contrast to the existing work that learns pairwise similarities, our approach optimizes a unified objective with convergence guarantee and it is computationally efficient for large-scale applications. Experiments on the COCO and ILSVRC 2013 detection datasets show that the performance of the localization model improves significantly with the inclusion of pairwise similarity function. For instance, in the ILSVRC dataset, the Correct Localization (CorLoc) performance improves from \\(72.8\\%\\) to \\(78.2\\%\\) which is a new state-of-the-art for WSOL task in the context of knowledge transfer.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_24');
INSERT INTO `paper` VALUES (12493, 'PAMS: Quantized Super-Resolution via Parameterized Max Scale', 'Super resolution', 'Network quantization', '', '', '', 'Deep convolutional neural networks (DCNNs) have shown dominant performance in the task of super-resolution (SR). However, their heavy memory cost and computation overhead significantly restrict their practical deployments on resource-limited devices, which mainly arise from the floating-point storage and operations between weights and activations. Although previous endeavors mainly resort to fixed-point operations, quantizing both weights and activations with fixed coding lengths may cause significant performance drop, especially on low bits. Specifically, most state-of-the-art SR models without batch normalization have a large dynamic quantization range, which also serves as another cause of performance drop. To address these two issues, we propose a new quantization scheme termed PArameterized Max Scale (PAMS), which applies the trainable truncated parameter to explore the upper bound of the quantization range adaptively. Finally, a structured knowledge transfer (SKT) loss is introduced to fine-tune the quantized network. Extensive experiments demonstrate that the proposed PAMS scheme can well compress and accelerate the existing SR models such as EDSR and RDN. Notably, 8-bit PAMS-EDSR improves PSNR on Set5 benchmark from 32.095 dB to 32.124 dB with 2.42\\(\\times \\) compression ratio, which achieves a new state-of-the-art.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_34');
INSERT INTO `paper` VALUES (12494, 'Panel: Bodily Expressed Emotion Understanding Research: A Multidisciplinary Perspective', '', '', '', '', '', 'Developing computational methods for bodily expressed emotion understanding can benefit from knowledge and approaches of multiple fields, including computer vision, robotics, psychology/psychiatry, graphics, data mining, machine learning, and movement analysis. The panel, consisting of active researchers in some closely-related fields, attempts to open a discussion on the future of this new and exciting research area. This paper documents the opinions expressed by the individual panelists.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_51');
INSERT INTO `paper` VALUES (12495, 'ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds', '', '', '', '', '', 'We propose a novel, end-to-end trainable, deep network called ParSeNet that decomposes a 3D point cloud into parametric surface patches, including B-spline patches as well as basic geometric primitives. ParSeNet is trained on a large-scale dataset of man-made 3D shapes and captures high-level semantic priors for shape decomposition. It handles a much richer class of primitives than prior work, and allows us to represent surfaces with higher fidelity. It also produces repeatable and robust parametrizations of a surface compared to purely geometric approaches. We present extensive experiments to validate our approach against analytical and learning-based alternatives. Our source code is publicly available at: https://hippogriff.github.io/parsenet.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_16');
INSERT INTO `paper` VALUES (12496, 'Part-Aware Prototype Network for Few-Shot Semantic Segmentation', '', '', '', '', '', 'Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin (Code is available at: https://github.com/Xiangyi1996/PPNet-PyTorch).', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_9');
INSERT INTO `paper` VALUES (12497, 'Partially-Shared Variational Auto-encoders for Unsupervised Domain Adaptation with Target Shift', '', '', '', '', '', 'Target shift, the different label distributions of source and target domains, is an important problem for practical use of unsupervised domain adaptation (UDA); as we do not know labels in target domain datasets, we cannot ensure an identical label distribution between the two domains. Despite this inaccessibility, modern UDA methods commonly try to match the shape of the feature distributions over the domains while projecting the features to labels by a common classifier. This implicitly assumes the identical label distribution. To overcome this problem, we propose a method that generates a pseudo pair by domain conversion where the label is preserved identically even trained with target-shifted datasets. A pair-wise metric learning enables to align feature over the domains without matching the shape of distributions. We conducted two experiments: one is a regression of pose-estimation, where label distribution is continuous and the target shift problem can seriously degrade the quality of UDA. The other is digit classification task where we can systematically control the distribution difference. The code and dataset are available at https://github.com/iiyama-lab/PS-VAEs.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_1');
INSERT INTO `paper` VALUES (12498, 'Particularity Beyond Commonality: Unpaired Identity Transfer with Multiple References', '', '', '', '', '', 'Unpaired image-to-image translation aims to translate images from the source class to target one by providing sufficient data for these classes. Current few-shot translation methods use multiple reference images to describe the target domain through extracting common features. In this paper, we focus on a more specific identity transfer problem and advocate that particular property in each individual image can also benefit generation. We accordingly propose a new multi-reference identity transfer framework by simultaneously making use of particularity and commonality of reference. It is achieved via a semantic pyramid alignment module to make proper use of geometric information for individual images, as well as an attention module to aggregate for the final transformation. Extensive experiments demonstrate the effectiveness of our framework given the promising results in a number of identity transfer applications.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_27');
INSERT INTO `paper` VALUES (12499, 'PAS Tracker: Position-, Appearance- and Size-Aware Multi-object Tracking in Drone Videos', 'Multi-object tracking', 'Object detection', 'Drone imagery', '', '', 'While most multi-object tracking methods based on tracking-by-detection use either spatial or appearance cues for associating detections or apply one cue after another, our proposed PAS tracker employs a novel similarity measure that combines position, appearance and size information jointly to make full use of object representations. We further extend the PAS tracker by introducing a filtering technique to remove false positive detections, particularly in crowded scenarios, and apply a camera motion compensation model to align track positions across frames. To provide high quality detections as input for the proposed tracker, the performance of eight state-of-the-art object detectors is compared on the VisDrone MOT dataset, on which the PAS tracker achieves state-of-the-art performance and ranks 3rd in the VisDrone2020 MOT challenge. In an ablation study, we demonstrate the effectiveness of the introduced tracking components and the impact of the employed detections on the tracking performance is analyzed in detail.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_36');
INSERT INTO `paper` VALUES (12500, 'Password-Conditioned Anonymization and Deanonymization with Face Identity Transformers', '', '', '', '', '', 'Cameras are prevalent in our daily lives, and enable many useful systems built upon computer vision technologies such as smart cameras and home robots for service applications. However, there is also an increasing societal concern as the captured images/videos may contain privacy-sensitive information (e.g., face identity). We propose a novel face identity transformer which enables automated photo-realistic password-based anonymization and deanonymization of human faces appearing in visual data. Our face identity transformer is trained to (1) remove face identity information after anonymization, (2) recover the original face when given the correct password, and (3) return a wrong—but photo-realistic—face given a wrong password. With our carefully designed password scheme and multi-task learning objective, we achieve both anonymization and deanonymization using the same single network. Extensive experiments show that our method enables multimodal password conditioned anonymizations and deanonymizations, without sacrificing privacy compared to existing anonymization methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_43');
INSERT INTO `paper` VALUES (12501, 'Patch-Based CNN Evaluation for Bark Classification', 'Bark classification', 'Convolutional neural networks', 'Transfer learning', 'Patch-based CNNs', 'Image re-scaling', 'The identification of tree species from bark images is a challenging computer vision problem. However, even in the era of deep learning today, bark recognition continues to be explored by traditional methods using time-consuming handcrafted features, mainly due to the problem of limited data. In this work, we implement a patch-based convolutional neural network alternative for analyzing a challenging bark dataset Bark-101, comprising of 2587 images from 101 classes. We propose to apply image re-scaling during the patch extraction process to compensate for the lack of sufficient data. Individual patch-level predictions from fine-tuned CNNs are then combined by classical majority voting to obtain image-level decisions. Since ties can often occur in the voting process, we investigate various tie-breaking strategies from ensemble-based classifiers. Our study outperforms the classification accuracy achieved by traditional methods applied to Bark-101, thus demonstrating the feasibility of applying patch-based CNNs to such challenging datasets.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_15');
INSERT INTO `paper` VALUES (12502, 'Patch-Wise Attack for Fooling Deep Neural Network', 'Adversarial examples', 'Patch-wise', 'Black-box attack', 'Transferability', '', 'By adding human-imperceptible noise to clean images, the resultant adversarial examples can fool other unknown models. Features of a pixel extracted by deep neural networks (DNNs) are influenced by its surrounding regions, and different DNNs generally focus on different discriminative regions in recognition. Motivated by this, we propose a patch-wise iterative algorithm – a black-box attack towards mainstream normally trained and defense models, which differs from the existing attack methods manipulating pixel-wise noise. In this way, without sacrificing the performance of white-box attack, our adversarial examples can have strong transferability. Specifically, we introduce an amplification factor to the step size in each iteration, and one pixel’s overall gradient overflowing the \\(\\epsilon \\)-constraint is properly assigned to its surrounding regions by a project kernel. Our method can be generally integrated to any gradient-based attack methods. Compared with the current state-of-the-art attacks, we significantly improve the success rate by 9.2% for defense models and 3.7% for normally trained models on average. Our code is available at https://github.com/qilong-zhang/Patch-wise-iterative-attack', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_19');
INSERT INTO `paper` VALUES (12503, 'PatchAttack: A Black-Box Texture-Based Attack with Reinforcement Learning', 'Adversarial machine learning', 'Black-box attack', '', '', '', 'Patch-based attacks introduce a perceptible but localized change to the input that induces misclassification. A limitation of current patch-based black-box attacks is that they perform poorly for targeted attacks, and even for the less challenging non-targeted scenarios, they require a large number of queries. Our proposed PatchAttack is query efficient and can break models for both targeted and non-targeted attacks. PatchAttack induces misclassifications by superimposing small textured patches on the input image. We parametrize the appearance of these patches by a dictionary of class-specific textures. This texture dictionary is learned by clustering Gram matrices of feature activations from a VGG backbone. PatchAttack optimizes the position and texture parameters of each patch using reinforcement learning. Our experiments show that PatchAttack achieves \\({>}99\\%\\) success rate on ImageNet for a wide range of architectures, while only manipulating \\(3\\%\\) of the image for non-targeted attacks and \\(10\\%\\) on average for targeted attacks. Furthermore, we show that PatchAttack circumvents state-of-the-art adversarial defense methods successfully. The code is publicly available here.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_41');
INSERT INTO `paper` VALUES (12504, 'PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape Representations', 'Implicit functions', 'Patch-based surface representation', 'Intra-object class generalizability', '', '', 'Implicit surface representations, such as signed-distance functions, combined with deep learning have led to impressive models which can represent detailed shapes of objects with arbitrary topology. Since a continuous function is learned, the reconstructions can also be extracted at any arbitrary resolution. However, large datasets such as ShapeNet are required to train such models.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_18');
INSERT INTO `paper` VALUES (12505, 'PatchPerPix for Instance Segmentation', '', '', '', '', '', 'We present a novel method for proposal free instance segmentation that can handle sophisticated object shapes which span large parts of an image and form dense object clusters with crossovers. Our method is based on predicting dense local shape descriptors, which we assemble to form instances. All instances are assembled simultaneously in one go. To our knowledge, our method is the first non-iterative method that yields instances that are composed of learnt shape patches. We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy data of cell nuclei. We show furthermore that our method also applies to 3d light microscopy data of Drosophila neurons, which exhibit extreme cases of complex shape clusters.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_18');
INSERT INTO `paper` VALUES (12506, 'Peeking into Occluded Joints: A Novel Framework for Crowd Pose Estimation', 'Pose estimation', 'Occlusion', 'Progressive GCN', '', '', 'Although occlusion widely exists in nature and remains a fundamental challenge for pose estimation, existing heatmap-based approaches suffer serious degradation on occlusions. Their intrinsic problem is that they directly localize the joints based on visual information; however, the invisible joints are lack of that. In contrast to localization, our framework estimates the invisible joints from an inference perspective by proposing an Image-Guided Progressive GCN module which provides a comprehensive understanding of both image context and pose structure. Moreover, existing benchmarks contain limited occlusions for evaluation. Therefore, we thoroughly pursue this problem and propose a novel OPEC-Net framework together with a new Occluded Pose (OCPose) dataset with 9k annotated images. Extensive quantitative and qualitative evaluations on benchmarks demonstrate that OPEC-Net achieves significant improvements over recent leading works. Notably, our OCPose is the most complex occlusion dataset with respect to average IoU between adjacent instances. Source code and OCPose will be publicly available.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_29');
INSERT INTO `paper` VALUES (12507, 'People as Scene Probes', '', '', '', '', '', 'By analyzing the motion of people and other objects in a scene, we demonstrate how to infer depth, occlusion, lighting, and shadow information from video taken from a single camera viewpoint. This information is then used to composite new objects into the same scene with a high degree of automation and realism. In particular, when a user places a new object (2D cut-out) in the image, it is automatically rescaled, relit, occluded properly, and casts realistic shadows in the correct direction relative to the sun, and which conform properly to scene geometry. We demonstrate results (best viewed in supplementary video) on a range of scenes and compare to alternative methods for depth estimation and shadow compositing.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_26');
INSERT INTO `paper` VALUES (12508, 'Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations', '', '', '', '', '', 'In this paper we propose a novel end-to-end learnable network that performs joint perception, prediction and motion planning for self-driving vehicles and produces interpretable intermediate representations. Unlike existing neural motion planners, our motion planning costs are consistent with our perception and prediction estimates. This is achieved by a novel differentiable semantic occupancy representation that is explicitly used as cost by the motion planning process. Our network is learned end-to-end from human demonstrations. The experiments in a large-scale manual-driving dataset and closed-loop simulation show that the proposed model significantly outperforms state-of-the-art planners in imitating the human behaviors while producing much safer trajectories.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_25');
INSERT INTO `paper` VALUES (12509, 'Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild', '', '', '', '', '', 'We present a method that infers spatial arrangements and shapes of humans and objects in a globally consistent 3D scene, all from a single image in-the-wild captured in an uncontrolled environment. Notably, our method runs on datasets without any scene- or object-level 3D supervision. Our key insight is that considering humans and objects jointly gives rise to “3D common sense” constraints that can be used to resolve ambiguity. In particular, we introduce a scale loss that learns the distribution of object size from data; an occlusion-aware silhouette re-projection loss to optimize object pose; and a human-object interaction loss to capture the spatial layout of objects with which humans interact. We empirically validate that our constraints dramatically reduce the space of likely 3D spatial configurations. We demonstrate our approach on challenging, in-the-wild images of humans interacting with large objects (such as bicycles, motorcycles, and surfboards) and handheld objects (such as laptops, tennis rackets, and skateboards). We quantify the ability of our approach to recover human-object arrangements and outline remaining challenges in this relatively unexplored domain. The project webpage can be found at https://jasonyzhang.com/phosa.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_3');
INSERT INTO `paper` VALUES (12510, 'Personalized Face Modeling for Improved Face Reconstruction and Motion Retargeting', '3D face reconstruction', 'Face modeling', 'Face tracking', 'Facial motion retargeting', '', 'Traditional methods for image-based 3D face reconstruction and facial motion retargeting fit a 3D morphable model (3DMM) to the face, which has limited modeling capacity and fail to generalize well to in-the-wild data. Use of deformation transfer or multilinear tensor as a personalized 3DMM for blendshape interpolation does not address the fact that facial expressions result in different local and global skin deformations in different persons. Moreover, existing methods learn a single albedo per user which is not enough to capture the expression-specific skin reflectance variations. We propose an end-to-end framework that jointly learns a personalized face model per user and per-frame facial motion parameters from a large corpus of in-the-wild videos of user expressions. Specifically, we learn user-specific expression blendshapes and dynamic (expression-specific) albedo maps by predicting personalized corrections on top of a 3DMM prior. We introduce novel training constraints to ensure that the corrected blendshapes retain their semantic meanings and the reconstructed geometry is disentangled from the albedo. Experimental results show that our personalization accurately captures fine-grained facial dynamics in a wide range of conditions and efficiently decouples the learned face model from facial motion, resulting in more accurate face reconstruction and facial motion retargeting compared to state-of-the-art methods.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_9');
INSERT INTO `paper` VALUES (12511, 'PG-Net: Pixel to Global Matching Network for Visual Tracking', '', '', '', '', '', 'Siamese neural network has been well investigated by tracking frameworks due to its fast speed and high accuracy. However, very few efforts were spent on background-extraction by those approaches. In this paper, a Pixel to Global Matching Network (PG-Net) is proposed to suppress t+he influence of background in search image while achieving state-of-the-art tracking performance. To achieve this purpose, each pixel on search feature is utilized to calculate the similarity with global template feature. This calculation method can appropriately reduce the matching area, thus introducing less background interference. In addition, we propose a new tracking framework to perform correlation-shared tracking and multiple losses for training, which not only reduce the computational burden but also improve the performance. We conduct comparison experiments on various public tracking datasets, which obtains state-of-the-art performance while running with fast speed.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_26');
INSERT INTO `paper` VALUES (12512, 'Phenotyping Problems of Parts-per-Object Count', 'Deep neural networks', 'Object detection', 'Part counting', 'Yield estimation', '', 'The need to count the number of parts per object arises in many yield estimation problems, like counting the number of bananas in a bunch, or the number of spikelets in a wheat spike. We propose a two-stage detection and counting approach for such tasks, operating in field conditions with multiple objects per image. The approach is implemented as a single network, tested on the two mentioned problems. Experiments were conducted to find the optimal counting architecture and the most suitable training configuration. In both problems, the approach showed promising results, achieving a mean relative deviation in range of \\(11\\%\\)–\\(12\\%\\) of the total visible count. For wheat, the method was tested in estimating the average count in an image, and was shown to be preferable to a simpler alternative. For bananas, estimation of the actual physical bunch count was tested, yielding mean relative deviation of \\(12.4\\%\\).', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_19');
INSERT INTO `paper` VALUES (12513, 'Phonologically-Meaningful Subunits for Deep Learning-Based Sign Language Recognition', 'Sign language recognition', 'Gesture recognition', 'Deep learning', 'Assistive technology', 'Human computer interaction', 'The large majority of sign language recognition systems based on deep learning adopt a word model approach. Here we present a system that works with subunits, rather than word models. We propose a pipelined approach to deep learning that uses a factorisation algorithm to derive hand motion features, embedded within a low-rank trajectory space. Recurrent neural networks are then trained on these embedded features for subunit recognition, followed by a second-stage neural network for sign recognition. Our evaluation shows that our proposed solution compares well in accuracy against the state of the art, providing added benefits of better interpretability and phonologically-meaningful subunits that can operate across different signers and sign languages.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_15');
INSERT INTO `paper` VALUES (12514, 'Photon-Efficient 3D Imaging with A Non-local Neural Network', 'Photon-efficient imaging', 'Long-range correlation', 'Non-local network', 'Depth reconstruction', '', 'Photon-efficient imaging has enabled a number of applications relying on single-photon sensors that can capture a 3D image with as few as one photon per pixel. In practice, however, measurements of low photon counts are often mixed with heavy background noise, which poses a great challenge for existing computational reconstruction algorithms. In this paper, we first analyze the long-range correlations in both spatial and temporal dimensions of the measurements. Then we propose a non-local neural network for depth reconstruction by exploiting the long-range correlations. The proposed network achieves decent reconstruction fidelity even under photon counts (and signal-to-background ratio, SBR) as low as 1 photon/pixel (and 0.01 SBR), which significantly surpasses the state-of-the-art. Moreover, our non-local network trained on simulated data can be well generalized to different real-world imaging systems, which could extend the application scope of photon-efficient imaging in challenging scenarios with a strict limit on optical flux. Code is available at https://github.com/JiayongO-O/PENonLocal.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_14');
INSERT INTO `paper` VALUES (12515, 'PhraseClick: Toward Achieving Flexible Interactive Segmentation by Phrase and Click', 'Interactive segmentation', 'Click', 'Phrase', 'Flexible', 'Attribute', 'Existing interactive object segmentation methods mainly take spatial interactions such as bounding boxes or clicks as input. However, these interactions do not contain information about explicit attributes of the target-of-interest and thus cannot quickly specify what the selected object exactly is, especially when there are diverse scales of candidate objects or the target-of-interest contains multiple objects. Therefore, excessive user interactions are often required to reach desirable results. On the other hand, in existing approaches attribute information of objects is often not well utilized in interactive segmentation. We propose to employ phrase expressions as another interaction input to infer the attributes of target object. In this way, we can 1) leverage spatial clicks to locate the target object and 2) utilize semantic phrases to qualify the attributes of the target object. Specifically, the phrase expressions focus on “what” the target object is and the spatial clicks are in charge of “where” the target object is, which together help to accurately segment the target-of-interest with smaller number of interactions. Moreover, the proposed approach is flexible in terms of interaction modes and can efficiently handle complex scenarios by leveraging the strengths of each type of input. Our multi-modal phrase+click approach achieves new state-of-the-art performance on interactive segmentation. To the best of our knowledge, this is the first work to leverage both clicks and phrases for interactive segmentation.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_25');
INSERT INTO `paper` VALUES (12516, 'Physical Plausibility of 6D Pose Estimates in Scenes of Static Rigid Objects', '6D object pose estimation', 'Evaluation', 'Physical plausibility', 'Static equilibrium', '', 'To enable robots to reason about manipulation of objects and AR applications to present augmented scenes to human users, accurate scene explanations based on objects and their 6D pose are required. With the pose-error functions commonly used to evaluate 6D object pose estimation approaches, the accuracy of estimates is measured by surface alignment of a target object under the estimated and true pose. However, an object floating above the ground may yield the same error as an object translated on the ground by the same magnitude. We argue that, to be intelligible for human observers, pose estimates additionally need to adhere to physical principles. To this end, we provide a definition of physical plausibility in scenes of static rigid objects, derive novel pose-error functions and compare them to existing evaluation approaches in 6D object pose estimation. Code to compute the presented pose-error functions is publicly available at github.com/dornik/plausible-poses.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_43');
INSERT INTO `paper` VALUES (12517, 'Physics-Based Feature Dehazing Networks', 'Image dehazing', 'Physics model', 'Feature dehazing unit', 'Deep convolutional neural networks', '', 'We propose a physics-based feature dehazing network for image dehazing. In contrast to most existing end-to-end trainable network-based dehazing methods, we explicitly consider the physics model of the haze process in the network design and remove haze in a deep feature space. We propose an effective feature dehazing unit (FDU), which is applied to the deep feature space to explore useful features for image dehazing based on the physics model. The FDU is embedded into an encoder and decoder architecture with residual learning, so that the proposed network can be trained in an end-to-end fashion and effectively help haze removal. The encoder and decoder modules are adopted for feature extraction and clear image reconstruction, respectively. The residual learning is applied to increase the accuracy and ease the training of deep neural networks. We analyze the effectiveness of the proposed network and demonstrate that it can effectively dehaze images with favorable performance against state-of-the-art methods.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_12');
INSERT INTO `paper` VALUES (12518, 'PieNet: Personalized Image Enhancement Network', 'Image enhancement', 'Personalization', 'Metric learning', '', '', 'Image enhancement is an inherently subjective process since people have diverse preferences for image aesthetics. However, most enhancement techniques pay less attention to the personalization issue despite its importance. In this paper, we propose the first deep learning approach to personalized image enhancement, which can enhance new images for a new user, by asking him or her to select about 10–20 preferred images from a random set of images. First, we represent various users’ preferences for enhancement as feature vectors in an embedding space, called preference vectors. We construct the embedding space based on metric learning. Then, we develop the personalized image enhancement network (PieNet) to enhance images adaptively using each user’s preference vector. Experimental results demonstrate that the proposed algorithm is capable of achieving personalization successfully, as well as outperforming conventional general image enhancement algorithms significantly. The source codes and trained models are available at https://github.com/hukim1124/PieNet.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_23');
INSERT INTO `paper` VALUES (12519, 'Piggyback GAN: Efficient Lifelong Learning for Image Conditioned Generation', 'Lifelong learning', 'Generative adversarial networks', '', '', '', 'Humans accumulate knowledge in a lifelong fashion. Modern deep neural networks, on the other hand, are susceptible to catastrophic forgetting: when adapted to perform new tasks, they often fail to preserve their performance on previously learned tasks. Given a sequence of tasks, a naive approach addressing catastrophic forgetting is to train a separate standalone model for each task, which scales the total number of parameters drastically without efficiently utilizing previous models. In contrast, we propose a parameter efficient framework, Piggyback GAN, which learns the current task by building a set of convolutional and deconvolutional filters that are factorized into filters of the models trained on previous tasks. For the current task, our model achieves high generation quality on par with a standalone model at a lower number of parameters. For previous tasks, our model can also preserve generation quality since the filters for previous tasks are not altered. We validate Piggyback GAN on various image-conditioned generation tasks across different domains, and provide qualitative and quantitative results to show that the proposed approach can address catastrophic forgetting effectively and efficiently.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_24');
INSERT INTO `paper` VALUES (12520, 'Pillar-Based Object Detection for Autonomous Driving', '', '', '', '', '', 'We present a simple and flexible object detection framework optimized for autonomous driving. Building on the observation that point clouds in this application are extremely sparse, we propose a practical pillar-based approach to fix the imbalance issue caused by anchors. In particular, our algorithm incorporates a cylindrical projection into multi-view feature learning, predicts bounding box parameters per pillar rather than per point or per anchor, and includes an aligned pillar-to-point projection module to improve the final prediction. Our anchor-free approach avoids hyperparameter search associated with past methods, simplifying 3D object detection while significantly improving upon state-of-the-art.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_2');
INSERT INTO `paper` VALUES (12521, 'PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments', 'Orientated object detection', 'IoU loss', '', '', '', 'Object detection using an oriented bounding box (OBB) can better target rotated objects by reducing the overlap with background areas. Existing OBB approaches are mostly built on horizontal bounding box detectors by introducing an additional angle dimension optimized by a distance loss. However, as the distance loss only minimizes the angle error of the OBB and that it loosely correlates to the IoU, it is insensitive to objects with high aspect ratios. Therefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both the angle and IoU for accurate OBB regression. The PIoU loss is derived from IoU metric with a pixel-wise form, which is simple and suitable for both horizontal and oriented bounding box. To demonstrate its effectiveness, we evaluate the PIoU loss on both anchor-based and anchor-free frameworks. The experimental results show that PIoU loss can dramatically improve the performance of OBB detectors, particularly on objects with high aspect ratios and complex backgrounds. Besides, previous evaluation datasets did not include scenarios where the objects have high aspect ratios, hence a new dataset, Retail50K, is introduced to encourage the community to adapt OBB detectors for more complex environments.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_12');
INSERT INTO `paper` VALUES (12522, 'PiP: Planning-Informed Trajectory Prediction for Autonomous Driving', '', '', '', '', '', 'It is critical to predict the motion of surrounding vehicles for self-driving planning, especially in a socially compliant and flexible way. However, future prediction is challenging due to the interaction and uncertainty in driving behaviors. We propose planning-informed trajectory prediction (PiP) to tackle the prediction problem in the multi-agent setting. Our approach is differentiated from the traditional manner of prediction, which is only based on historical information and decoupled with planning. By informing the prediction process with the planning of the ego vehicle, our method achieves the state-of-the-art performance of multi-agent forecasting on highway datasets. Moreover, our approach enables a novel pipeline which couples the prediction and planning, by conditioning PiP on multiple candidate trajectories of the ego vehicle, which is highly beneficial for autonomous driving in interactive scenarios.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_36');
INSERT INTO `paper` VALUES (12523, 'PIPAL: A Large-Scale Image Quality Assessment Dataset for Perceptual Image Restoration', 'Perceptual image restoration', 'Image quality assessment', 'Generative adversarial network', 'Perceptual super-resolution', '', 'Image quality assessment (IQA) is the key factor for the fast development of image restoration (IR) algorithms. The most recent IR methods based on Generative Adversarial Networks (GANs) have achieved significant improvement in visual performance, but also presented great challenges for quantitative evaluation. Notably, we observe an increasing inconsistency between perceptual quality and the evaluation results. Then we raise two questions: (1) Can existing IQA methods objectively evaluate recent IR algorithms? (2) When focus on beating current benchmarks, are we getting better IR algorithms? To answer these questions and promote the development of IQA methods, we contribute a large-scale IQA dataset, called Perceptual Image Processing Algorithms (PIPAL) dataset. Especially, this dataset includes the results of GAN-based methods, which are missing in previous datasets. We collect more than 1.13 million human judgments to assign subjective scores for PIPAL images using the more reliable “Elo system”. Based on PIPAL, we present new benchmarks for both IQA and super-resolution methods. Our results indicate that existing IQA methods cannot fairly evaluate GAN-based IR algorithms. While using appropriate evaluation methods is important, IQA methods should also be updated along with the development of IR algorithms. At last, we improve the performance of IQA networks on GAN-based distortions by introducing anti-aliasing pooling. Experiments show the effectiveness of the proposed method.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_37');
INSERT INTO `paper` VALUES (12524, 'Pix2Surf: Learning Parametric 3D Surface Models of Objects from Images', '3D reconstruction', 'Multi-view', 'Single-view', 'Parametrization', '', 'We investigate the problem of learning to generate 3D parametric surface representations for novel object instances, as seen from one or more views. Previous work on learning shape reconstruction from multiple views uses discrete representations such as point clouds or voxels, while continuous surface generation approaches lack multi-view consistency. We address these issues by designing neural networks capable of generating high-quality parametric 3D surfaces which are also consistent between views. Furthermore, the generated 3D surfaces preserve accurate image pixel to 3D surface point correspondences, allowing us to lift texture information to reconstruct shapes with rich geometry and appearance. Our method is supervised and trained on a public dataset of shapes from common object categories. Quantitative results indicate that our method significantly outperforms previous work, while qualitative results demonstrate the high quality of our reconstructions.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_8');
INSERT INTO `paper` VALUES (12525, 'Pixel-Pair Occlusion Relationship Map (P2ORM): Formulation, Inference and Application', 'Occlusion relation', 'Occlusion boundary', 'Depth refinement', '', '', 'We formalize concepts around geometric occlusion in 2D images (i.e., ignoring semantics), and propose a novel unified formulation of both occlusion boundaries and occlusion orientations via a pixel-pair occlusion relation. The former provides a way to generate large-scale accurate occlusion datasets while, based on the latter, we propose a novel method for task-independent pixel-level occlusion relationship estimation from single images. Experiments on a variety of datasets demonstrate that our method outperforms existing ones on this task. To further illustrate the value of our formulation, we also propose a new depth map refinement method that consistently improve the performance of state-of-the-art monocular depth estimation methods.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_40');
INSERT INTO `paper` VALUES (12526, 'PL\\(_1\\)P - Point-Line Minimal Problems Under Partial Visibility in Three Views', 'Minimal problems', 'Calibrated cameras', '3D reconstruction', '', '', 'We present a complete classification of minimal problems for generic arrangements of points and lines in space observed partially by three calibrated perspective cameras when each line is incident to at most one point. This is a large class of interesting minimal problems that allows missing observations in images due to occlusions and missed detections. There is an infinite number of such minimal problems; however, we show that they can be reduced to 140616 equivalence classes by removing superfluous features and relabeling the cameras. We also introduce camera-minimal problems, which are practical for designing minimal solvers, and show how to pick a simplest camera-minimal problem for each minimal problem. This simplification results in 74575 equivalence classes. Only 76 of these were known; the rest are new. To identify problems having potential for practical solving of image matching and 3D reconstruction, we present several natural subfamilies of camera-minimal problems as well as compute solution counts for all camera-minimal problems which have less than 300 solutions for generic data.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_11');
INSERT INTO `paper` VALUES (12527, 'Placepedia: Comprehensive Place Understanding with Multi-faceted Annotations', '', '', '', '', '', 'Place is an important element in visual understanding. Given a photo of a building, people can often tell its functionality, e.g. a restaurant or a shop, its cultural style, e.g. Asian or European, as well as its economic type, e.g. industry oriented or tourism oriented. While place recognition has been widely studied in previous work, there remains a long way towards comprehensive place understanding, which is far beyond categorizing a place with an image and requires information of multiple aspects. In this work, we contribute Placepedia\\(^{1}\\), a large-scale place dataset with more than 35M photos from 240K unique places. Besides the photos, each place also comes with massive multi-faceted information, e.g. GDP, population, etc., and labels at multiple levels, including function, city, country, etc. This dataset, with its large amount of data and rich annotations, allows various studies to be conducted. Particularly, in our studies, we develop 1) PlaceNet, a unified framework for multi-level place recognition, and 2) a method for city embedding, which can produce a vector representation for a city that captures both visual and multi-faceted side information. Such studies not only reveal key challenges in place understanding, but also establish connections between visual observations and underlying socioeconomic/cultural implications. (\\(^{1}\\)The dataset is available at: https://hahehi.github.io/placepedia.html).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_6');
INSERT INTO `paper` VALUES (12528, 'PlugNet: Degradation Aware Scene Text Recognition Supervised by a Pluggable Super-Resolution Unit', 'Scene text recognition', 'Neural network', 'Feature learning', '', '', 'In this paper, we address the problem of recognizing degradation images that are suffering from high blur or low-resolution. We propose a novel degradation aware scene text recognizer with a pluggable super-resolution unit (PlugNet) to recognize low-quality scene text to solve this task from the feature-level. The whole networks can be trained end-to-end with a pluggable super-resolution unit (PSU) and the PSU will be removed after training so that it brings no extra computation. The PSU aims to obtain a more robust feature representation for recognizing low-quality text images. Moreover, to further improve the feature quality, we introduce two types of feature enhancement strategies: Feature Squeeze Module (FSM) which aims to reduce the loss of spatial acuity and Feature Enhance Module (FEM) which combines the feature maps from low to high to provide diversity semantics. As a consequence, the PlugNet achieves state-of-the-art performance on various widely used text recognition benchmarks like IIIT5K, SVT, SVTP, ICDAR15 and etc.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_10');
INSERT INTO `paper` VALUES (12529, 'PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning', 'Incremental-learning', 'Representation-learning pooling', '', '', '', 'Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks – a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_6');
INSERT INTO `paper` VALUES (12530, 'Point-Set Anchors for Object Detection, Instance Segmentation and Pose Estimation', 'Object detection', 'Instance segmentation', 'Human pose estimation', 'Anchor box', 'Point-based representation', 'A recent approach for object detection and human pose estimation is to regress bounding boxes or human keypoints from a central point on the object or person. While this center-point regression is simple and efficient, we argue that the image features extracted at a central point contain limited information for predicting distant keypoints or bounding box boundaries, due to object deformation and scale/orientation variation. To facilitate inference, we propose to instead perform regression from a set of points placed at more advantageous positions. This point set is arranged to reflect a good initialization for the given task, such as modes in the training data for pose estimation, which lie closer to the ground truth than the central point and provide more informative features for regression. As the utility of a point set depends on how well its scale, aspect ratio and rotation matches the target, we adopt the anchor box technique of sampling these transformations to generate additional point-set candidates. We apply this proposed framework, called Point-Set Anchors, to object detection, instance segmentation, and human pose estimation. Our results show that this general-purpose approach can achieve performance competitive with state-of-the-art methods for each of these tasks .', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_31');
INSERT INTO `paper` VALUES (12531, 'PointAR: Efficient Lighting Estimation for Mobile Augmented Reality', 'Lighting estimation', 'Deep learning', 'Mobile AR', '', '', 'We propose an efficient lighting estimation pipeline that is suitable to run on modern mobile devices, with comparable resource complexities to state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a single RGB-D image captured from the mobile camera and a 2D location in that image, and estimates 2nd degree spherical harmonics coefficients. This estimated spherical harmonics coefficients can be directly utilized by rendering engines for supporting spatially variant indoor lighting, in the context of augmented reality. Our key insight is to formulate the lighting estimation as a point cloud-based learning problem directly from point clouds, which is in part inspired by the Monte Carlo integration leveraged by real-time spherical harmonics lighting. While existing approaches estimate lighting information with complex deep learning pipelines, our method focuses on reducing the computational complexity. Through both quantitative and qualitative experiments, we demonstrate that PointAR achieves lower lighting estimation errors compared to state-of-the-art methods. Further, our method requires an order of magnitude lower resource, comparable to that of mobile-specific DNNs.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_40');
INSERT INTO `paper` VALUES (12532, 'PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding', 'Unsupervised learning', 'Point cloud recognition', 'Representation learning', '3D scene understanding', '', 'Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (e.g., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suit of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets – demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_34');
INSERT INTO `paper` VALUES (12533, 'PointMixup: Augmentation for Point Clouds', 'Interpolation', 'Point cloud classification', 'Data augmentation', '', '', 'This paper introduces data augmentation for point clouds by interpolation between examples. Data augmentation by interpolation has shown to be a simple and effective approach in the image domain. Such a mixup is however not directly transferable to point clouds, as we do not have a one-to-one correspondence between the points of two different objects. In this paper, we define data augmentation between point clouds as a shortest path linear interpolation. To that end, we introduce PointMixup, an interpolation method that generates new examples through an optimal assignment of the path function between two point clouds. We prove that our PointMixup finds the shortest path between two point clouds and that the interpolation is assignment invariant and linear. With the definition of interpolation, PointMixup allows to introduce strong interpolation-based regularizers such as mixup and manifold mixup to the point cloud domain. Experimentally, we show the potential of PointMixup for point cloud classification, especially when examples are scarce, as well as increased robustness to noise and geometric transformations to points. The code for PointMixup and the experimental details are publicly available (Code is available at: https://github.com/yunlu-chen/PointMixup/).', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_20');
INSERT INTO `paper` VALUES (12534, 'PointPWC-Net: Cost Volume on Point Clouds for (Self-)Supervised Scene Flow Estimation', 'Cost volume', 'Self-supervision', 'Coarse-to-fine', 'Scene flow', '', 'We propose a novel end-to-end deep scene flow model, called PointPWC-Net, that directly processes 3D point cloud scenes with large motions in a coarse-to-fine fashion. Flow computed at the coarse level is upsampled and warped to a finer level, enabling the algorithm to accommodate for large motion without a prohibitive search space. We introduce novel cost volume, upsampling, and warping layers to efficiently handle 3D point cloud data. Unlike traditional cost volumes that require exhaustively computing all the cost values on a high-dimensional grid, our point-based formulation discretizes the cost volume onto input 3D points, and a PointConv operation efficiently computes convolutions on the cost volume. Experiment results on FlyingThings3D and KITTI outperform the state-of-the-art by a large margin. We further explore novel self-supervised losses to train our model and achieve comparable results to state-of-the-art trained with supervised loss. Without any fine-tuning, our method also shows great generalization ability on the KITTI Scene Flow 2015 dataset, outperforming all previous methods. The code is released at https://github.com/DylanWusee/PointPWC.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_6');
INSERT INTO `paper` VALUES (12535, 'Points2Surf Learning Implicit Surfaces from Point Clouds', 'Surface reconstruction', 'Implicit surfaces', 'Point clouds', 'Patch-based', 'Local and global', 'A key step in any scanning-based asset creation workflow is to convert unordered point clouds to a surface. Classical methods (e.g., Poisson reconstruction) start to degrade in the presence of noisy and partial scans. Hence, deep learning based methods have recently been proposed to produce complete surfaces, even from partial scans. However, such data-driven methods struggle to generalize to new shapes with large geometric and topological variations. We present Points2Surf, a novel patch-based learning framework that produces accurate surfaces directly from raw scans without normals. Learning a prior over a combination of detailed local patches and coarse global information improves generalization performance and reconstruction accuracy. O5ur extensive comparison on both synthetic and real data demonstrates a clear advantage of our method over state-of-the-art alternatives on previously unseen classes (on average, Points2Surf brings down reconstruction error by 30% over SPR and by 270%+ over deep learning based SotA methods) at the cost of longer computation times and a slight increase in small-scale topological noise in some cases. Our source code, pre-trained model, and dataset are available at: https://github.com/ErlerPhilipp/points2surf.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_7');
INSERT INTO `paper` VALUES (12536, 'PointTriNet: Learned Triangulation of 3D Point Sets', 'Geometric learning', 'Triangulation', 'Geometry processing', '', '', 'This work considers a new task in geometric deep learning: generating a triangulation among a set of points in 3D space. We present PointTriNet, a differentiable and scalable approach enabling point set triangulation as a layer in 3D learning pipelines. The method iteratively applies two neural networks: a classification network predicts whether a candidate triangle should appear in the triangulation, while a proposal network suggests additional candidates. Both networks are structured as PointNets over nearby points and triangles, using a novel triangle-relative input encoding. Since these learning problems operate on local geometric data, our method is efficient and scalable, and generalizes to unseen shape categories. Our networks are trained in an unsupervised manner from a collection of shapes represented as point clouds. We demonstrate the effectiveness of this approach for classical meshing tasks, robustness to outliers, and as a component in end-to-end learning systems.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_45');
INSERT INTO `paper` VALUES (12537, 'Polarimetric Multi-view Inverse Rendering', 'Multi-view reconstruction', 'Inverse rendering', 'Polarization', '', '', 'A polarization camera has great potential for 3D reconstruction since the angle of polarization (AoP) of reflected light is related to an object’s surface normal. In this paper, we propose a novel 3D reconstruction method called Polarimetric Multi-View Inverse Rendering (Polarimetric MVIR) that effectively exploits geometric, photometric, and polarimetric cues extracted from input multi-view color polarization images. We first estimate camera poses and an initial 3D model by geometric reconstruction with a standard structure-from-motion and multi-view stereo pipeline. We then refine the initial model by optimizing photometric rendering errors and polarimetric errors using multi-view RGB and AoP images, where we propose a novel polarimetric cost function that enables us to effectively constrain each estimated surface vertex’s normal while considering four possible ambiguous azimuth angles revealed from the AoP measurement. Experimental results using both synthetic and real data demonstrate that our Polarimetric MVIR can reconstruct a detailed 3D shape without assuming a specific polarized reflection depending on the material.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_6');
INSERT INTO `paper` VALUES (12538, 'Polarized Optical-Flow Gyroscope', 'Low level vision', 'Self-calibration', 'Bio-inspired', '', '', 'We merge by generalization two principles of passive optical sensing of motion. One is common spatially resolved imaging, where motion induces temporal readout changes at high-contrast spatial features, as used in traditional optical-flow. The other is the polarization compass, where axial rotation induces temporal readout changes due to the change of incoming polarization angle, relative to the camera frame. The latter has traditionally been modeled for uniform objects. This merger generalizes the brightness constancy assumption and optical-flow, to handle polarization. It also generalizes the polarization compass concept to handle arbitrarily textured objects. This way, scene regions having partial polarization contribute to motion estimation, irrespective of their texture and non-uniformity. As an application, we derive and demonstrate passive sensing of differential ego-rotation around the camera optical axis.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_22');
INSERT INTO `paper` VALUES (12539, 'Polynomial Regression Network for Variable-Number Lane Detection', 'Lane detection', 'Polynomial curve', 'Deep neural network', 'Polynomial regression', '', 'Lane detection is a fundamental yet challenging task in autonomous driving and intelligent traffic systems due to perspective projection and occlusion. Most of previous methods utilize semantic segmentation to identify the regions of traffic lanes in an image, and then adopt some curve-fitting method to reconstruct the lanes. In this work, we propose to use polynomial curves to represent traffic lanes and then propose a novel polynomial regression network (PRNet) to directly predict them, where semantic segmentation is not involved. Specifically, PRNet consists of one major branch and two auxiliary branches: (1) polynomial regression to estimate the polynomial coefficients of lanes, (2) initialization classification to detect the initial retrieval point of each lane, and (3) height regression to determine the ending point of each lane. Through the cooperation of three branches, PRNet can detect variable-number of lanes and is highly effective and efficient. We experimentally evaluate the proposed PRNet on two popular benchmark datasets: TuSimple and CULane. The results show that our method significantly outperforms the previous state-of-the-art methods in terms of both accuracy and speed.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_42');
INSERT INTO `paper` VALUES (12540, 'Polysemy Deciphering Network for Human-Object Interaction Detection', 'Human-object interaction', 'Verb polysemy', 'Attention model', '', '', 'Human-Object Interaction (HOI) detection is important in human-centric scene understanding. Existing works typically assume that the same verb in different HOI categories has similar visual characteristics, while ignoring the diverse semantic meanings of the verb. To address this issue, in this paper, we propose a novel Polysemy Deciphering Network (PD-Net), which decodes the visual polysemy of verbs for HOI detection in three ways. First, PD-Net augments human pose and spatial features for HOI detection using language priors, enabling the verb classifiers to receive language hints that reduce the intra-class variation of the same verb. Second, we introduce a novel Polysemy Attention Module (PAM) that guides PD-Net to make decisions based on more important feature types according to the language priors. Finally, the above two strategies are applied to two types of classifiers for verb recognition, i.e., object-shared and object-specific verb classifiers, whose combination further relieves the verb polysemy problem. By deciphering the visual polysemy of verbs, we achieve the best performance on both HICO-DET and V-COCO datasets. In particular, PD-Net outperforms state-of-the-art approaches by 3.81% mAP in the Known-Object evaluation mode of HICO-DET. Code of PD-Net is available at https://github.com/MuchHair/PD-Net.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_5');
INSERT INTO `paper` VALUES (12541, 'Pose Augmentation: Class-Agnostic Object Pose Transformation for Object Recognition', 'Pose transform', 'Data augmentation', 'Disentangled representation learning', 'Object recognition', 'GANs', 'Object pose increases intraclass object variance which makes object recognition from 2D images harder. To render a classifier robust to pose variations, most deep neural networks try to eliminate the influence of pose by using large datasets with many poses for each class. Here, we propose a different approach: a class-agnostic object pose transformation network (OPT-Net) can transform an image along 3D yaw and pitch axes to synthesize additional poses continuously. Synthesized images lead to better training of an object classifier. We design a novel eliminate-add structure to explicitly disentangle pose from object identity: first ‘eliminate’ pose information of the input image and then ‘add’ target pose information (regularized as continuous variables) to synthesize any target pose. We trained OPT-Net on images of toy vehicles shot on a turntable from the iLab-20M dataset. After training on unbalanced discrete poses (5 classes with 6 poses per object instance, plus 5 classes with only 2 poses), we show that OPT-Net can synthesize balanced continuous new poses along yaw and pitch axes with high quality. Training a ResNet-18 classifier with original plus synthesized poses improves mAP accuracy by 9% over training on original poses only. Further, the pre-trained OPT-Net can generalize to new object classes, which we demonstrate on both iLab-20M and RGB-D. We also show that the learned features can generalize to ImageNet. (The code is released at this github url).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_9');
INSERT INTO `paper` VALUES (12542, 'Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose', '', '', '', '', '', 'Most of the recent deep learning-based 3D human pose and mesh estimation methods regress the pose and shape parameters of human mesh models, such as SMPL and MANO, from an input image. The first weakness of these methods is the overfitting to image appearance, due to the domain gap between the training data captured from controlled settings such as a lab, and in-the-wild data in inference time. The second weakness is that the estimation of the pose parameters is quite challenging due to the representation issues of 3D rotations. To overcome the above weaknesses, we propose Pose2Mesh, a novel graph convolutional neural network (GraphCNN)-based system that estimates the 3D coordinates of human mesh vertices directly from the 2D human pose. The 2D human pose as input provides essential human body articulation information without image appearance. Also, the proposed system avoids the representation issues, while fully exploiting the mesh topology using GraphCNN in a coarse-to-fine manner. We show that our Pose2Mesh significantly outperforms the previous 3D human pose and mesh estimation methods on various benchmark datasets. The codes are publicly available(https://github.com/hongsukchoi/Pose2Mesh_RELEASE).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_45');
INSERT INTO `paper` VALUES (12543, 'Post Training Mixed-Precision Quantization Based on Key Layers Selection', 'CNN', 'Quantization', 'Mixed-precision', 'Key layers selection', '', 'Model quantization has been extensively used to compress and accelerate deep neural network inference. Because post-training quantization methods are simple to use, they have gained considerable attention. However, when the model is quantized below 8-bits, significant accuracy degradation will be involved. This paper seeks to address this problem by building mixed-precision inference networks based on key activation layers selection. In post training quantization process, key activation layers are quantized by 8-bit precision, and non-key activation layers are quantized by 4-bit precision. The experimental results indicate an impressive promotion with our method. Relative to ResNet-50(W8A8) and VGG-16(W8A8), our proposed method can accelerate inference with lower power consumption and a little accuracy loss.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_9');
INSERT INTO `paper` VALUES (12544, 'Post-training Piecewise Linear Quantization for Deep Neural Networks', 'Deep neural networks', 'Post-training quantization', 'Piecewise linear quantization', '', '', 'Quantization plays an important role in the energy-efficient deployment of deep neural networks on resource-limited devices. Post-training quantization is highly desirable since it does not require retraining or access to the full training dataset. The well-established uniform scheme for post-training quantization achieves satisfactory results by converting neural networks from full-precision to 8-bit fixed-point integers. However, it suffers from significant performance degradation when quantizing to lower bit-widths. In this paper, we propose a piecewise linear quantization (PWLQ) scheme (Code will be made available at https://github.com/jun-fang/PWLQ) to enable accurate approximation for tensor values that have bell-shaped distributions with long tails. Our approach breaks the entire quantization range into non-overlapping regions for each tensor, with each region being assigned an equal number of quantization levels. Optimal breakpoints that divide the entire range are found by minimizing the quantization error. Compared to state-of-the-art post-training quantization methods, experimental results show that our proposed method achieves superior performance on image classification, semantic segmentation, and object detection with minor overhead.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_5');
INSERT INTO `paper` VALUES (12545, 'Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy', 'Stablized one-shot NAS', 'Network topology', '', '', '', 'One-shot NAS method has attracted much interest from the research community due to its remarkable training efficiency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for flexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e, over \\(3.4 \\times 10^{10}\\) different topological structures). Specifically, the difficulties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves \\(76.4\\%\\) top-1 accuracy with only 326M MAdds. Our moderate model ST-NAS-B achieves \\(77.9\\%\\) top-1 accuracy just required 503M MAdds. Both of our models offer superior performances in comparison to other concurrent works on one-shot NAS.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_37');
INSERT INTO `paper` VALUES (12546, 'Practical Deep Raw Image Denoising on Mobile Devices', '', '', '', '', '', 'Deep learning-based image denoising approaches have been extensively studied in recent years, prevailing in many public benchmark datasets. However, the stat-of-the-art networks are computationally too expensive to be directly applied on mobile devices. In this work, we propose a light-weight, efficient neural network-based raw image denoiser that runs smoothly on mainstream mobile devices, and produces high quality denoising results. Our key insights are twofold: (1) by measuring and estimating sensor noise level, a smaller network trained on synthetic sensor-specific data can out-perform larger ones trained on general data; (2) the large noise level variation under different ISO settings can be removed by a novel k-Sigma Transform, allowing a small network to efficiently handle a wide range of noise levels. We conduct extensive experiments to demonstrate the efficiency and accuracy of our approach. Our proposed mobile-friendly denoising model runs at \\(\\sim \\)70 ms per megapixel on Qualcomm Snapdragon 855 chipset, and it is the basis of the night shot feature of several flagship smartphones released in 2019.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_1');
INSERT INTO `paper` VALUES (12547, 'Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases', 'Trojan attack', 'Adversarial perturbation', 'Interpretability', 'Neuron activation', '', 'When the training data are maliciously tampered, the predictions of the acquired deep neural network (DNN) can be manipulated by an adversary known as the Trojan attack (or poisoning backdoor attack). The lack of robustness of DNNs against Trojan attacks could significantly harm real-life machine learning (ML) systems in downstream applications, therefore posing widespread concern to their trustworthiness. In this paper, we study the problem of the Trojan network (TrojanNet) detection in the data-scarce regime, where only the weights of a trained DNN are accessed by the detector. We first propose a data-limited TrojanNet detector (TND), when only a few data samples are available for TrojanNet detection. We show that an effective data-limited TND can be established by exploring connections between Trojan attack and prediction-evasion adversarial attacks including per-sample attack as well as all-sample universal attack. In addition, we propose a data-free TND, which can detect a TrojanNet without accessing any data samples. We show that such a TND can be built by leveraging the internal response of hidden neurons, which exhibits the Trojan behavior even at random noise inputs. The effectiveness of our proposals is evaluated by extensive experiments under different model architectures and datasets including CIFAR-10, GTSRB, and ImageNet.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_14');
INSERT INTO `paper` VALUES (12548, 'Practical Poisoning Attacks on Neural Networks', 'Data poisoning', 'Neural networks', '', '', '', 'Data poisoning attacks on machine learning models have attracted much recent attention, wherein poisoning samples are injected at the training phase to achieve adversarial goals at test time. Although existing poisoning techniques prove to be effective in various scenarios, they rely on certain assumptions on the adversary knowledge and capability to ensure efficacy, which may be unrealistic in practice. This paper presents a new, practical targeted poisoning attack method on neural networks in vision domain, namely BlackCard. BlackCard possesses a set of critical properties for ensuring attacking efficacy in practice, which has never been simultaneously achieved by any existing work, including knowledge-oblivious, clean-label, and clean-test. Importantly, we show that the effectiveness of BlackCard can be intuitively guaranteed by a set of analytical reasoning and observations, through exploiting an essential characteristic of gradient-descent optimization which is pervasively adopted in DNN models. We evaluate the efficacy of BlackCard for generating targeted poisoning attacks via extensive experiments using various datasets and DNN models. Results show that BlackCard is effective with a rather high success rate while preserving all the claimed properties.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_9');
INSERT INTO `paper` VALUES (12549, 'Predicting Camera Viewpoint Improves Cross-Dataset Generalization for 3D Human Pose Estimation', 'Monocular 3d human pose estimation', 'Cross dataset evaluation', 'Dataset bias', '', '', 'Monocular estimation of 3d human pose has attracted increased attention with the availability of large ground-truth motion capture datasets. However, the diversity of training data available is limited and it is not clear to what extent methods generalize outside the specific datasets they are trained on. In this work we carry out a systematic study of the diversity and biases present in specific datasets and its effect on cross-dataset generalization across a compendium of 5 pose datasets. We specifically focus on systematic differences in the distribution of camera viewpoints relative to a body-centered coordinate frame. Based on this observation, we propose an auxiliary task of predicting the camera viewpoint in addition to pose. We find that models trained to jointly predict viewpoint and pose systematically show significantly improved cross-dataset generalization.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_36');
INSERT INTO `paper` VALUES (12550, 'Predicting Visual Overlap of Images Through Interpretable Non-metric Box Embeddings', 'Image embedding', 'Representation learning', 'Image localization', 'Interpretable representation', '', 'To what extent are two images picturing the same 3D surfaces? Even when this is a known scene, the answer typically requires an expensive search across scale space, with matching and geometric verification of large sets of local features. This expense is further multiplied when a query image is evaluated against a gallery, e.g. in visual relocalization. While we don’t obviate the need for geometric verification, we propose an interpretable image-embedding that cuts the search in scale space to essentially a lookup.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_37');
INSERT INTO `paper` VALUES (12551, 'Prediction and Recovery for Adaptive Low-Resolution Person Re-Identification', 'Low-resolution person re-identification', 'Adaptive scale factor prediction', 'Dynamic soft label', '', '', 'Low-resolution person re-identification (LR re-id) is a challenging task with low-resolution probes and high-resolution gallery images. To address the resolution mismatch, existing methods typically recover missing details for low-resolution probes by super-resolution. However, they usually pre-specify fixed scale factors for all images, and ignore the fact that choosing a preferable scale factor for certain image content probably greatly benefits the identification. In this paper, we propose a novel Prediction, Recovery and Identification (PRI) model for LR re-id, which adaptively recovers missing details by predicting a preferable scale factor based on the image content. To deal with the lack of ground-truth optimal scale factors, our model contains a self-supervised scale factor metric that automatically generates dynamic soft labels. The generated labels indicate probabilities that each scale factor is optimal, which are used as guidance to enhance the content-aware scale factor prediction. Consequently, our model can more accurately predict and recover the content-aware details, and achieve state-of-the-art performances on four LR re-id datasets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_12');
INSERT INTO `paper` VALUES (12552, 'Preserving Semantic Neighborhoods for Robust Cross-Modal Retrieval', '', '', '', '', '', 'The abundance of multimodal data (e.g. social media posts) has inspired interest in cross-modal retrieval methods. Popular approaches rely on a variety of metric learning losses, which prescribe what the proximity of image and text should be, in the learned space. However, most prior methods have focused on the case where image and text convey redundant information; in contrast, real-world image-text pairs convey complementary information with little overlap. Further, images in news articles and media portray topics in a visually diverse fashion; thus, we need to take special care to ensure a meaningful image representation. We propose novel within-modality losses which encourage semantic coherency in both the text and image subspaces, which does not necessarily align with visual coherency. Our method ensures that not only are paired images and texts close, but the expected image-image and text-text relationships are also observed. Our approach improves the results of cross-modal retrieval on four datasets compared to five baselines.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_19');
INSERT INTO `paper` VALUES (12553, 'Prime-Aware Adaptive Distillation', 'Knowledge distillation', 'Adaptive weighting', 'Uncertainty learning', '', '', 'Knowledge distillation (KD) aims to improve the performance of a student network by mimicing the knowledge from a powerful teacher network. Existing methods focus on studying what knowledge should be transferred and treat all samples equally during training. This paper introduces the adaptive sample weighting to KD. We discover that previous effective hard mining methods are not appropriate for distillation. Furthermore, we propose Prime-Aware Adaptive Distillation (PAD) by the incorporation of uncertainty learning. PAD perceives the prime samples in distillation and then emphasizes their effect adaptively. PAD is fundamentally different from and would refine existing methods with the innovative view of unequal training. For this reason, PAD is versatile and has been applied in various tasks including classification, metric learning, and object detection. With ten teacher-student combinations on six datasets, PAD promotes the performance of existing distillation methods and outperforms recent state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_39');
INSERT INTO `paper` VALUES (12554, 'Principal Feature Visualisation in Convolutional Neural Networks', 'Visual explanations', 'Deep neural networks', 'Interpretability', 'Principal component analysis', 'Explainable AI', 'We introduce a new visualisation technique for CNNs called Principal Feature Visualisation (PFV). It uses a single forward pass of the original network to map principal features from the final convolutional layer to the original image space as RGB channels. By working on a batch of images we can extract contrasting features, not just the most dominant ones with respect to the classification. This allows us to differentiate between several features in one image in an unsupervised manner. This enables us to assess the feasibility of transfer learning and to debug a pre-trained classifier by localising misleading or missing features.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_2');
INSERT INTO `paper` VALUES (12555, 'Prior-Based Domain Adaptive Object Detection for Hazy and Rainy Conditions', 'Detection', 'Unsupervised domain adaptation', 'Adverse weather', 'Rain', 'Haze', 'Adverse weather conditions such as haze and rain corrupt the quality of captured images, which cause detection networks trained on clean images to perform poorly on these corrupted images. To address this issue, we propose an unsupervised prior-based domain adversarial object detection framework for adapting the detectors to hazy and rainy conditions. In particular, we use weather-specific prior knowledge obtained using the principles of image formation to define a novel prior-adversarial loss. The prior-adversarial loss, which we use to supervise the adaptation process, aims to reduce the weather-specific information in the features, thereby mitigating the effects of weather on the detection performance. Additionally, we introduce a set of residual feature recovery blocks in the object detection pipeline to de-distort the feature space, resulting in further improvements. Evaluations performed on various datasets (Foggy-Cityscapes, Rainy-Cityscapes, RTTS and UFDD) for rainy and hazy conditions demonstrates the effectiveness of the proposed approach.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_45');
INSERT INTO `paper` VALUES (12556, 'Privacy Preserving Structure-from-Motion', '', '', '', '', '', 'Over the last years, visual localization and mapping solutions have been adopted by an increasing number of mixed reality and robotics systems. The recent trend towards cloud-based localization and mapping systems has raised significant privacy concerns. These are mainly grounded by the fact that these services require users to upload visual data to their servers, which can reveal potentially confidential information, even if only derived image features are uploaded. Recent research addresses some of these concerns for the task of image-based localization by concealing the geometry of the query images and database maps. The core idea of the approach is to lift 2D/3D feature points to random lines, while still providing sufficient constraints for camera pose estimation. In this paper, we further build upon this idea and propose solutions to the different core algorithms of an incremental Structure-from-Motion pipeline based on random line features. With this work, we make another fundamental step towards enabling privacy preserving cloud-based mapping solutions. Various experiments on challenging real-world datasets demonstrate the practicality of our approach achieving comparable results to standard Structure-from-Motion systems.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_20');
INSERT INTO `paper` VALUES (12557, 'Privacy Preserving Visual SLAM', 'Visual SLAM', 'Privacy', 'Line cloud', 'Point cloud', '', 'This study proposes a privacy-preserving Visual SLAM framework for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Previous studies have proposed localization methods to estimate a camera pose using a line-cloud map for a single image or a reconstructed point cloud. These methods offer a scene privacy protection against the inversion attacks by converting a point cloud to a line cloud, which reconstruct the scene images from the point cloud. However, they are not directly applicable to a video sequence because they do not address computational efficiency. This is a critical issue to solve for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Moreover, there has been no study on a method to optimize a line-cloud map of a server with a point cloud reconstructed from a client video because any observation points on the image coordinates are not available to prevent the inversion attacks, namely the reversibility of the 3D lines. The experimental results with synthetic and real data show that our Visual SLAM framework achieves the intended privacy-preserving formation and real-time performance using a line-cloud map.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_7');
INSERT INTO `paper` VALUES (12558, 'Privacy-Aware Face Recognition with Lensless Multi-pinhole Camera', 'Coded aperture', 'Lensless multi-pinhole camera', 'Face recognition', 'Image debluring', '', 'Face recognition and privacy protection are closely related. A high-quality facial image is required to achieve a high accuracy in face recognition; however, this undermines the privacy of the person being photographed. From the perspective of confidentiality, storing facial images as raw data is a problem. If a low-quality facial image is used, to protect user privacy, the accuracy of recognition decreases. In this paper, we propose a method for face recognition that solves these problems. We train a neural network with an unblurred image at first, and then train the neural network with a blurred image, using the features of the neural network trained with the unblurred image, as an initial value. This makes it possible to train features that are similar to the features trained with the neural network using a high-quality image. This enables us to perform face recognition without compromising user privacy. Our method consists of a neural network for face feature extraction, which extracts suitable features for face recognition from a blurred facial image, and a face recognition neural network. After pretraining both networks, we fine-tune them in an end-to-end manner. In experiments, the proposed method achieved accuracy comparable to that of conventional face recognition methods, which take as input unblurred face images from simulations and from images captured by our camera system.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_35');
INSERT INTO `paper` VALUES (12559, 'Probabilistic Anchor Assignment with IoU Prediction for Object Detection', '', '', '', '', '', 'In object detection, determining which anchors to assign as positive or negative samples, known as anchor assignment, has been revealed as a core procedure that can significantly affect a model’s performance. In this paper we propose a novel anchor assignment strategy that adaptively separates anchors into positive and negative samples for a ground truth bounding box according to the model’s learning status such that it is able to reason about the separation in a probabilistic manner. To do so we first calculate the scores of anchors conditioned on the model and fit a probability distribution to these scores. The model is then trained with anchors separated into positive and negative samples according to their probabilities. Moreover, we investigate the gap between the training and testing objectives and propose to predict the Intersection-over-Unions of detected boxes as a measure of localization quality to reduce the discrepancy. The combined score of classification and localization qualities serving as a box selection metric in non-maximum suppression well aligns with the proposed anchor assignment strategy and leads significant performance improvements. The proposed methods only add a single convolutional layer to RetinaNet baseline and does not require multiple anchors per location, so are efficient. Experimental results verify the effectiveness of the proposed methods. Especially, our models set new records for single-stage detectors on MS COCO test-dev dataset with various backbones. Code is available at https://github.com/kkhoot/PAA.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_22');
INSERT INTO `paper` VALUES (12560, 'Probabilistic Deep Learning for Instance Segmentation', 'Instance segmentation', 'Probabilistic deep learning', 'Bayesian inference', 'Digital microscopy', '', 'Probabilistic convolutional neural networks, which predict distributions of predictions instead of point estimates, led to recent advances in many areas of computer vision, from image reconstruction to semantic segmentation. Besides state of the art benchmark results, these networks made it possible to quantify local uncertainties in the predictions. These were used in active learning frameworks to target the labeling efforts of specialist annotators or to assess the quality of a prediction in a safety-critical environment. However, for instance segmentation problems these methods are not frequently used so far. We seek to close this gap by proposing a generic method to obtain model-inherent uncertainty estimates within proposal-free instance segmentation models. Furthermore, we analyze the quality of the uncertainty estimates with a metric adapted from semantic segmentation. We evaluate our method on the BBBC010 C. elegans dataset, where it yields competitive performance while also predicting uncertainty estimates that carry information about object-level inaccuracies like false splits and false merges. We perform a simulation to show the potential use of such uncertainty estimates in guided proofreading.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_29');
INSERT INTO `paper` VALUES (12561, 'Probabilistic Future Prediction for Video Scene Understanding', '', '', '', '', '', 'We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_45');
INSERT INTO `paper` VALUES (12562, 'Probabilistic Object Detection via Deep Ensembles', 'Object detection', 'Uncertainty estimation', 'Deep ensembles', '', '', 'Probabilistic object detection is the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detection. Measuring uncertainty is important in robotic applications where actions based on erroneous, but high confidence visual detections, can lead to catastrophic consequences. We introduce an approach that employs deep ensembles for estimating predictive uncertainty. The proposed framework achieved 4th place in the ECCV 2020 ACRV Robotic Vision Challenge on Probabilistic Object Detection.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_7');
INSERT INTO `paper` VALUES (12563, 'Probabilistic Object Detection with an Ensemble of Experts', '', '', '', '', '', 'Probabilistic object detection requires detectors to localise and classify objects in an image, while also providing accurate spatial and semantic uncertainty. In this work, we present an ‘Ensemble of Experts’ as a method to solve this challenging problem. This technique utilises a ranked ensembling association process and leverages the individual strengths of each expert detector to create a final set of detections with a meaningful spatial and semantic uncertainty. Our approach placed first place in the 3rd Probabilistic Object Detection Challenge with a PDQ of 22.848.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_5');
INSERT INTO `paper` VALUES (12564, 'Procedure Planning in Instructional Videos', 'Latent space planning', 'Task planning', 'Video understanding', 'Representation for action and skill', '', 'In this paper, we study the problem of procedure planning in instructional videos, which can be seen as a step towards enabling autonomous agents to plan for complex tasks in everyday settings such as cooking. Given the current visual observation of the world and a visual goal, we ask the question “What actions need to be taken in order to achieve the goal?”. The key technical challenge is to learn structured and plannable state and action spaces directly from unstructured videos. We address this challenge by proposing Dual Dynamics Networks (DDN), a framework that explicitly leverages the structured priors imposed by the conjugate relationships between states and actions in a learned plannable latent space. We evaluate our method on real-world instructional videos. Our experiments show that DDN learns plannable representations that lead to better planning performance compared to existing planning approaches and neural network policies.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_20');
INSERT INTO `paper` VALUES (12565, 'Procrustean Regression Networks: Learning 3D Structure of Non-rigid Objects from 2D Annotations', '', '', '', '', '', 'We propose a novel framework for training neural networks which is capable of learning 3D information of non-rigid objects when only 2D annotations are available as ground truths. Recently, there have been some approaches that incorporate the problem setting of non-rigid structure-from-motion (NRSfM) into deep learning to learn 3D structure reconstruction. The most important difficulty of NRSfM is to estimate both the rotation and deformation at the same time, and previous works handle this by regressing both of them. In this paper, we resolve this difficulty by proposing a loss function wherein the suitable rotation is automatically determined. Trained with the cost function consisting of the reprojection error and the low-rank term of aligned shapes, the network learns the 3D structures of such objects as human skeletons and faces during the training, whereas the testing is done in a single-frame basis. The proposed method can handle inputs with missing entries and experimental results validate that the proposed framework shows superior reconstruction performance to the state-of-the-art method on the Human 3.6M, 300-VW, and SURREAL datasets, even though the underlying network structure is very simple.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_1');
INSERT INTO `paper` VALUES (12566, 'PROFIT: A Novel Training Method for sub-4-bit MobileNet Models', 'Mobile network', 'Quantization', 'Activation distribution', 'h-swish activation', '', '4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48% top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86% of top-1 accuracy. The quantized model and source code is available at https://github.com/EunhyeokPark/PROFIT.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_26');
INSERT INTO `paper` VALUES (12567, 'ProgressFace: Scale-Aware Progressive Learning for Face Detection', 'Face detection', 'Progressive learning', 'Anchor-free methods', '', '', 'Scale variation stands out as one of key challenges in face detection. Recent attempts have been made to cope with this issue by incorporating image/feature pyramids or adjusting anchor sampling/matching strategies. In this work, we propose a novel scale-aware progressive training mechanism to address large scale variations across faces. Inspired by curriculum learning, our method gradually learns large-to-small face instances. The preceding models learned with easier samples (i.e., large faces) can provide good initialization for succeeding learning with harder samples (i.e., small faces), ultimately deriving a better optimum of face detectors. Moreover, we propose an auxiliary anchor-free enhancement module to facilitate the learning of small faces by supplying positive anchors that may be not covered according to the criterion of IoU overlap. Such anchor-free module will be removed during inference and hence no extra computation cost is introduced. Extensive experimental results demonstrate the superiority of our method compared to the state-of-the-arts on the standard FDDB and WIDER FACE benchmarks. Especially, our ProgressFace-Light with MobileNet-0.25 backbone achieves 87.9% AP on the hard set of WIDER FACE, surpassing largely RetinaFace with the same backbone by 9.7%. Code and our trained face detection models are available at https://github.com/jiashu-zhu/ProgressFace.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_21');
INSERT INTO `paper` VALUES (12568, 'Progressive Point Cloud Deconvolution Generation Network', 'Point cloud generation', 'GAN', 'Deconvolution network', 'Bilateral interpolation', '', 'In this paper, we propose an effective point cloud generation method, which can generate multi-resolution point clouds of the same shape from a latent vector. Specifically, we develop a novel progressive deconvolution network with the learning-based bilateral interpolation. The learning-based bilateral interpolation is performed in the spatial and feature spaces of point clouds so that local geometric structure information of point clouds can be exploited. Starting from the low-resolution point clouds, with the bilateral interpolation and max-pooling operations, the deconvolution network can progressively output high-resolution local and global feature maps. By concatenating different resolutions of local and global feature maps, we employ the multi-layer perceptron as the generation network to generate multi-resolution point clouds. In order to keep the shapes of different resolutions of point clouds consistent, we propose a shape-preserving adversarial loss to train the point cloud deconvolution generation network. Experimental results on ShpaeNet and ModelNet datasets demonstrate that our proposed method can yield good performance. Our code is available at https://github.com/fpthink/PDGN.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_24');
INSERT INTO `paper` VALUES (12569, 'Progressive Refinement Network for Occluded Pedestrian Detection', 'Occluded pedestrian detection', 'Progressive Refinement Network', 'Anchor calibration', 'Occlusion loss', 'Receptive Field Backfeed', 'We present Progressive Refinement Network (PRNet), a novel single-stage detector that tackles occluded pedestrian detection. Motivated by human’s progressive process on annotating occluded pedestrians, PRNet achieves sequential refinement by three phases: Finding high-confident anchors of visible parts, calibrating such anchors to a full-body template derived from occlusion statistics, and then adjusting the calibrated anchors to final full-body regions. Unlike conventional methods that exploit predefined anchors, the confidence-aware calibration offers adaptive anchor initialization for detection with occlusions, and helps reduce the gap between visible-part and full-body detection. In addition, we introduce an occlusion loss to up-weigh hard examples, and a Receptive Field Backfeed (RFB) module to diversify receptive fields in early layers that commonly fire only on visible parts or small-size full-body regions. Experiments were performed within and across CityPersons, ETH, and Caltech datasets. Results show that PRNet can match the speed of existing single-stage detectors, consistently outperforms alternatives in terms of overall miss rate, and offers significantly better cross-dataset generalization. Code is available (https://github.com/sxlpris).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_3');
INSERT INTO `paper` VALUES (12570, 'Progressive Transformers for End-to-End Sign Language Production', 'Sign language production', 'Continuous sequence synthesis', 'Transformers', 'Sequence-to-sequence', 'Human pose generation', 'The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_40');
INSERT INTO `paper` VALUES (12571, 'Progressively Guided Alternate Refinement Network for RGB-D Salient Object Detection', 'RGB-D salient object detection', 'Lightweight depth stream', 'Alternate refinement', 'Progressive guidance', '', 'In this paper, we aim to develop an efficient and compact deep network for RGB-D salient object detection, where the depth image provides complementary information to boost performance in complex scenarios. Starting from a coarse initial prediction by a multi-scale residual block, we propose a progressively guided alternate refinement network to refine it. Instead of using ImageNet pre-trained backbone network, we first construct a lightweight depth stream by learning from scratch, which can extract complementary features more efficiently with less redundancy. Then, different from the existing fusion based methods, RGB and depth features are fed into proposed guided residual (GR) blocks alternately to reduce their mutual degradation. By assigning progressive guidance in the stacked GR blocks within each side-output, the false detection and missing parts can be well remedied. Extensive experiments on seven benchmark datasets demonstrate that our model outperforms existing state-of-the-art approaches by a large margin, and also shows superiority in efficiency (71 FPS) and model size (64.9 MB).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_31');
INSERT INTO `paper` VALUES (12572, 'Propagating Over Phrase Relations for One-Stage Visual Grounding', 'One-stage phrase grounding', 'Linguistic graph', 'Relational propagation', 'Visual grounding', '', 'Phrase level visual grounding aims to locate in an image the corresponding visual regions referred to by multiple noun phrases in a given sentence. Its challenge comes not only from large variations in visual contents and unrestricted phrase descriptions but also from unambiguous referrals derived from phrase relational reasoning. In this paper, we propose a linguistic structure guided propagation network for one-stage phrase grounding. It explicitly explores the linguistic structure of the sentence and performs relational propagation among noun phrases under the guidance of the linguistic relations between them. Specifically, we first construct a linguistic graph parsed from the sentence and then capture multimodal feature maps for all the phrasal nodes independently. The node features are then propagated over the edges with a tailor-designed relational propagation module and ultimately integrated for final prediction. Experiments on Flickr30K Entities dataset show that our model outperforms state-of-the-art methods and demonstrate the effectiveness of propagating among phrases with linguistic relations (Source code will be available at https://github.com/sibeiyang/lspn.).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_35');
INSERT INTO `paper` VALUES (12573, 'Proposal-Based Video Completion', '', '', '', '', '', 'Video inpainting is an important technique for a wide variety of applications from video content editing to video restoration. Early approaches follow image inpainting paradigms, but are challenged by complex camera motion and non-rigid deformations. To address these challenges flow-guided propagation techniques have been proposed. However, computation of flow is non-trivial for unobserved regions and propagation across a whole video sequence is computationally demanding. In contrast, in this paper, we propose a video inpainting algorithm based on proposals: we use 3D convolutions to obtain an initial inpainting estimate which is subsequently refined by fusing a generated set of proposals. Different from existing approaches for video inpainting, and inspired by well-explored mechanisms for object detection, we argue that proposals provide a rich source of information that permits combining similarly looking patches that may be spatially and temporally far from the region to be inpainted. We validate the effectiveness of our method on the challenging YouTube VOS and DAVIS datasets using different settings and demonstrate results outperforming state-of-the-art on standard metrics.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_3');
INSERT INTO `paper` VALUES (12574, 'Prototype Mixture Models for Few-Shot Semantic Segmentation', 'Semantic segmentation', 'Few-shot segmentation', 'Few-shot learning', 'Mixture models', '', 'Few-shot segmentation is challenging because objects within the support and query images could significantly differ in appearance and pose. Using a single prototype acquired directly from the support image to segment the query image causes semantic ambiguity. In this paper, we propose prototype mixture models (PMMs), which correlate diverse image regions with multiple prototypes to enforce the prototype-based semantic representation. Estimated by an Expectation-Maximization algorithm, PMMs incorporate rich channel-wised and spatial semantics from limited support images. Utilized as representations as well as classifiers, PMMs fully leverage the semantics to activate objects in the query image while depressing background regions in a duplex manner. Extensive experiments on Pascal VOC and MS-COCO datasets show that PMMs significantly improve upon state-of-the-arts. Particularly, PMMs improve 5-shot segmentation performance on MS-COCO by up to 5.82% with only a moderate cost for model size and inference speed (Code is available at github.com/Yang-Bob/PMMs.).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_45');
INSERT INTO `paper` VALUES (12575, 'Prototype Rectification for Few-Shot Learning', 'Few-shot learning', 'Prototype rectification', 'Intra-class bias', 'Cross-class bias', '', 'Few-shot learning requires to recognize novel classes with scarce labeled data. Prototypical network is useful in existing researches, however, training on narrow-size distribution of scarce data usually tends to get biased prototypes. In this paper, we figure out two key influencing factors of the process: the intra-class bias and the cross-class bias. We then propose a simple yet effective approach for prototype rectification in transductive setting. The approach utilizes label propagation to diminish the intra-class bias and feature shifting to diminish the cross-class bias. We also conduct theoretical analysis to derive its rationality as well as the lower bound of the performance. Effectiveness is shown on three few-shot benchmarks. Notably, our approach achieves state-of-the-art performance on both miniImageNet (70.31% on 1-shot and 81.89% on 5-shot) and tieredImageNet (78.74% on 1-shot and 86.92% on 5-shot).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_43');
INSERT INTO `paper` VALUES (12576, 'ProxyBNN: Learning Binarized Neural Networks via Proxy Matrices', 'Binarized Neural Networks', 'Proxy matrix', '', '', '', 'Training Binarized Neural Networks (BNNs) is challenging due to the discreteness. In order to efficiently optimize BNNs through backward propagations, real-valued auxiliary variables are commonly used to accumulate gradient updates. Those auxiliary variables are then directly quantized to binary weights in the forward pass, which brings about large quantization errors. In this paper, by introducing an appropriate proxy matrix, we reduce the weights quantization error while circumventing explicit binary regularizations on the full-precision auxiliary variables. Specifically, we regard pre-binarization weights as a linear combination of the basis vectors. The matrix composed of basis vectors is referred to as the proxy matrix, and auxiliary variables serve as the coefficients of this linear combination. We are the first to empirically identify and study the effectiveness of learning both basis and coefficients to construct the pre-binarization weights. This new proxy learning contributes to new leading performances on benchmark datasets.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_14');
INSERT INTO `paper` VALUES (12577, 'ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component Analysis', 'Metric learning', 'Zero-shot learning', 'Image retrieval', '', '', 'We consider the problem of distance metric learning (DML), where the task is to learn an effective similarity measure between images. We revisit ProxyNCA and incorporate several enhancements. We find that low temperature scaling is a performance-critical component and explain why it works. Besides, we also discover that Global Max Pooling works better in general when compared to Global Average Pooling. Additionally, our proposed fast moving proxies also addresses small gradient issue of proxies, and this component synergizes well with low temperature scaling and Global Max Pooling. Our enhanced model, called ProxyNCA++, achieves a 22.9% point average improvement of Recall@1 across four different zero-shot retrieval datasets compared to the original ProxyNCA algorithm. Furthermore, we achieve state-of-the-art results on the CUB200, Cars196, Sop, and InShop datasets, achieving Recall@1 scores of 72.2, 90.1, 81.4, and 90.9, respectively.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_27');
INSERT INTO `paper` VALUES (12578, 'PSConv: Squeezing Feature Pyramid into One Compact Poly-Scale Convolutional Layer', 'Convolutional kernel', 'Multi-scale feature fusion', 'Dilated convolution', 'Categorization and detection', '', 'Despite their strong modeling capacities, Convolutional Neural Networks (CNNs) are often scale-sensitive. For enhancing the robustness of CNNs to scale variance, multi-scale feature fusion from different layers or filters attracts great attention among existing solutions, while the more granular kernel space is overlooked. We bridge this regret by exploiting multi-scale features in a finer granularity. The proposed convolution operation, named Poly-Scale Convolution (PSConv), mixes up a spectrum of dilation rates and tactfully allocates them in the individual convolutional kernels of each filter regarding a single convolutional layer. Specifically, dilation rates vary cyclically along the axes of input and output channels of the filters, aggregating features over a wide range of scales in a neat style. PSConv could be a drop-in replacement of the vanilla convolution in many prevailing CNN backbones, allowing better representation learning without introducing additional parameters and computational complexities. Comprehensive experiments on the ImageNet and MS COCO benchmarks validate the superior performance of PSConv. Code and models are available at https://github.com/d-li14/PSConv.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_37');
INSERT INTO `paper` VALUES (12579, 'Pseudo RGB-D for Self-improving Monocular SLAM and Depth Prediction', 'Self-supervised learning', 'Self-improving', 'Monocular depth prediction', 'Monocular SLAM', '', 'Classical monocular Simultaneous Localization And Mapping (SLAM) and the recently emerging convolutional neural networks (CNNs) for monocular depth prediction represent two largely disjoint approaches towards building a 3D map of the surrounding environment. In this paper, we demonstrate that the coupling of these two by leveraging the strengths of each mitigates the other’s shortcomings. Specifically, we propose a joint narrow and wide baseline based self-improving framework, where on the one hand the CNN-predicted depth is leveraged to perform pseudo RGB-D feature-based SLAM, leading to better accuracy and robustness than the monocular RGB SLAM baseline. On the other hand, the bundle-adjusted 3D scene structures and camera poses from the more principled geometric SLAM are injected back into the depth network through novel wide baseline losses proposed for improving the depth prediction network, which then continues to contribute towards better pose and 3D structure estimation in the next iteration. We emphasize that our framework only requires unlabeled monocular videos in both training and inference stages, and yet is able to outperform state-of-the-art self-supervised monocular and stereo depth prediction networks (e.g., Monodepth2) and feature-based monocular SLAM system (i.e., ORB-SLAM). Extensive experiments on KITTI and TUM RGB-D datasets verify the superiority of our self-improving geometry-CNN framework.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_26');
INSERT INTO `paper` VALUES (12580, 'PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions', 'Part-tree to point-cloud', 'Conditional generative adversarial network', 'Part-based and structure-aware point cloud generation', '', '', 'Generative 3D shape modeling is a fundamental research area in computer vision and interactive computer graphics, with many real-world applications. This paper investigates the novel problem of generating a 3D point cloud geometry for a shape from a symbolic part tree representation. In order to learn such a conditional shape generation procedure in an end-to-end fashion, we propose a conditional GAN “part tree”-to-“point cloud” model (PT2PC) that disentangles the structural and geometric factors. The proposed model incorporates the part tree condition into the architecture design by passing messages top-down and bottom-up along the part tree hierarchy. Experimental results and user study demonstrate the strengths of our method in generating perceptually plausible and diverse 3D point clouds, given the part tree condition. We also propose a novel structural measure for evaluating if the generated shape point clouds satisfy the part tree conditions. Code and data can be accessed on the webpage: https://cs.stanford.edu/~kaichun/pt2pc.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_41');
INSERT INTO `paper` VALUES (12581, 'PUGeo-Net: A Geometry-Centric Network for 3D Point Cloud Upsampling', 'Point clouds', 'Deep learning', 'Discrete differential geometry', 'Upsampling', 'Local parameterization', 'In this paper, we propose a novel deep neural network based method, called PUGeo-Net, for upsampling 3D point clouds. PUGeo-Net incorporates discrete differential geometry into deep learning elegantly by learning the first and second fundamental forms that are able to fully represent the local geometry unique up to rigid motion. Specifically, we encode the first fundamental form in a \\(3\\times 3\\) linear transformation matrix \\(\\mathbf{T}\\) for each input point. Such a matrix approximates the augmented Jacobian matrix of a local parameterization that encodes the intrinsic information and builds a one-to-one correspondence between the 2D parametric domain and the 3D tangent plane, so that we can lift the adaptively distributed 2D samples learned from the input to 3D space. After that, we use the learned second fundamental form to compute a normal displacement for each generated sample and project it to the curved surface. As a by-product, PUGeo-Net can compute normals for the original and generated points, which is highly desired for surface reconstruction algorithms. We evaluate PUGeo-Net on a wide range of 3D models with sharp features and rich geometric details and observe that PUGeo-Net consistently outperforms state-of-the-art methods in terms of both accuracy and efficiency for upsampling factor 4\\(\\sim \\)16. We also verify the geometry-centric nature of PUGeo-Net quantitatively. In addition, PUGeo-Net can handle noisy and non-uniformly distributed inputs well, validating its robustness. The code is publicly available at https://github.com/ninaqy/PUGeo.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_44');
INSERT INTO `paper` VALUES (12582, 'PyNET-CA: Enhanced PyNET with Channel Attention for End-to-End Mobile Image Signal Processing', 'RAW to RGB', 'Mobile image signal processing', 'Image reconstruction', 'Deep learning', '', 'Reconstructing RGB image from RAW data obtained with a mobile device is related to a number of image signal processing (ISP) tasks, such as demosaicing, denoising, etc. Deep neural networks have shown promising results over hand-crafted ISP algorithms on solving these tasks separately, or even replacing the whole reconstruction process with one model. Here, we propose PyNET-CA, an end-to-end mobile ISP deep learning algorithm for RAW to RGB reconstruction. The model enhances PyNET, a recently proposed state-of-the-art model for mobile ISP, and improve its performance with channel attention and subpixel reconstruction module. We demonstrate the performance of the proposed method with comparative experiments and results from the AIM 2020 learned smartphone ISP challenge. The source code of our implementation is available at https://github.com/egyptdj/skyb-aim2020-public.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_12');
INSERT INTO `paper` VALUES (12583, 'Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation', 'Multi-view stereo', 'Deep learning', 'Self-adaptive view aggregation', 'Multi-metric pyramid aggregation', '', 'In this paper, we propose an effective and efficient pyramid multi-view stereo (MVS) net with self-adaptive view aggregation for accurate and complete dense point cloud reconstruction. Different from using mean square variance to generate cost volume in previous deep-learning based MVS methods, our VA-MVSNet incorporates the cost variances in different views with small extra memory consumption by introducing two novel self-adaptive view aggregations: pixel-wise view aggregation and voxel-wise view aggregation. To further boost the robustness and completeness of 3D point cloud reconstruction, we extend VA-MVSNet with pyramid multi-scale images input as PVA-MVSNet, where multi-metric constraints are leveraged to aggregate the reliable depth estimation at the coarser scale to fill in the mismatched regions at the finer scale. Experimental results show that our approach establishes a new state-of-the-art on the DTU dataset with significant improvements in the completeness and overall quality, and has strong generalization by achieving a comparable performance as the state-of-the-art methods on the Tanks and Temples benchmark. Our codebase is at https://github.com/yhw-yhw/PVAMVSNet.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_44');
INSERT INTO `paper` VALUES (12584, 'Pyramidal Edge-Maps and Attention Based Guided Thermal Super-Resolution', 'Guided super-resolution', 'Thermal image', 'Hierarchical edge-maps', 'Attention based fusion', 'Convolutional neural network', 'Guided super-resolution (GSR) of thermal images using visible range images is challenging because of the difference in the spectral-range between the images. This in turn means that there is significant texture-mismatch between the images, which manifests as blur and ghosting artifacts in the super-resolved thermal image. To tackle this, we propose a novel algorithm for GSR based on pyramidal edge-maps extracted from the visible image. Our proposed network has two sub-networks. The first sub-network super-resolves the low-resolution thermal image while the second obtains edge-maps from the visible image at a growing perceptual scale and integrates them into the super-resolution sub-network with the help of attention-based fusion. Extraction and integration of multi-level edges allows the super-resolution network to process texture-to-object level information progressively, enabling more straightforward identification of overlapping edges between the input images. Extensive experiments show that our model outperforms the state-of-the-art GSR methods, both quantitatively and qualitatively.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_42');
INSERT INTO `paper` VALUES (12585, 'Quantization Guided JPEG Artifact Correction', 'JPEG', 'Discrete Cosine Transform', 'Artifact correction', 'Quantization', '', 'The JPEG image compression algorithm is the most popular method of image compression because of it’s ability for large compression ratios. However, to achieve such high compression, information is lost. For aggressive quantization settings, this leads to a noticeable reduction in image quality. Artifact correction has been studied in the context of deep neural networks for some time, but the current methods delivering state-of-the-art results require a different model to be trained for each quality setting, greatly limiting their practical application. We solve this problem by creating a novel architecture which is parameterized by the JPEG file’s quantization matrix. This allows our single model to achieve state-of-the-art performance over models trained for specific quality settings. ...', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_18');
INSERT INTO `paper` VALUES (12586, 'Quantized Warping and Residual Temporal Integration for Video Super-Resolution on Fast Motions', 'Super resolution', 'Motion compensation', '', '', '', 'In recent years, numerous deep learning approaches to video super resolution have been proposed, increasing the resolution of one frame using information found in neighboring frames. Such methods either warp frames into alignment using optical flow, or else forgo warping and use optical flow as an additional network input. In this work we point out the disadvantages inherent in these two approaches and propose one that inherits the best features of both, warping with the integer part of the flow and using the fractional part as network input. Moreover, an iterative residual super-resolution approach is proposed to incrementally improve quality as more neighboring frames are provided. Incorporating the above in a recurrent architecture, we train, evaluate and compare the proposed network to the SotA, and note its superior performance in faster motion sequences.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_41');
INSERT INTO `paper` VALUES (12587, 'QuantNet: Learning to Quantize by Learning Within Fully Differentiable Framework', 'Deep neural networks', 'Quantization', 'Compression', '', '', 'Despite the achievements of recent binarization methods on reducing the performance degradation of Binary Neural Networks (BNNs), gradient mismatching caused by the Straight-Through-Estimator (STE) still dominates quantized networks. This paper proposes a meta-based quantizer named QuantNet, which utilizes a differentiable sub-network to directly binarize the full-precision weights without resorting to STE and any learnable gradient estimators. Our method not only solves the problem of gradient mismatching, but also reduces the impact of discretization errors, caused by the binarizing operation in the deployment, on performance. Generally, the proposed algorithm is implemented within a fully differentiable framework, and is easily extended to the general network quantization with any bits. The quantitative experiments on CIFAR-100 and ImageNet demonstrate that QuantNet achieves the significant improvements comparing with previous binarization methods, and even bridges gaps of accuracies between binarized models and full-precision models.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_4');
INSERT INTO `paper` VALUES (12588, 'Quantum-Soft QUBO Suppression for Accurate Object Detection', 'Object detection', 'Quantum computing', 'Pedestrian detection', 'Occlusion', '', 'Non-maximum suppression (NMS) has been adopted by default for removing redundant object detections for decades. It eliminates false positives by only keeping the image \\(\\mathcal {M}\\) with highest detection score and images whose overlap ratio with \\(\\mathcal {M}\\) is less than a predefined threshold. However, this greedy algorithm may not work well for object detection under occlusion scenario where true positives with lower detection scores are possibly suppressed. In this paper, we first map the task of removing redundant detections into Quadratic Unconstrained Binary Optimization (QUBO) framework that consists of detection score from each bounding box and overlap ratio between pair of bounding boxes. Next, we solve the QUBO problem using the proposed Quantum-soft QUBO Suppression (QSQS) algorithm for fast and accurate detection by exploiting quantum computing advantages. Experiments indicate that QSQS improves mean average precision from 74.20% to 75.11% for PASCAL VOC 2007. It consistently outperforms NMS and soft-NMS for Reasonable subset of benchmark pedestrian detection CityPersons.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_10');
INSERT INTO `paper` VALUES (12589, 'Quaternion Equivariant Capsule Networks for 3D Point Clouds', '3D', 'Equivariance', 'Disentanglement', 'Rotation', 'Quaternion', 'We present a 3D capsule module for processing point clouds that is equivariant to 3D rotations and translations, as well as invariant to permutations of the input points. The operator receives a sparse set of local reference frames, computed from an input point cloud and establishes end-to-end transformation equivariance through a novel dynamic routing procedure on quaternions. Further, we theoretically connect dynamic routing between capsules to the well-known Weiszfeld algorithm, a scheme for solving iterative re-weighted least squares (IRLS) problems with provable convergence properties. It is shown that such group dynamic routing can be interpreted as robust IRLS rotation averaging on capsule votes, where information is routed based on the final inlier scores. Based on our operator, we build a capsule network that disentangles geometry from pose, paving the way for more informative descriptors and a structured latent space. Our architecture allows joint object classification and orientation estimation without explicit supervision of rotations. We validate our algorithm empirically on common benchmark datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_1');
INSERT INTO `paper` VALUES (12590, 'QuEST: Quantized Embedding Space for Transferring Knowledge', 'Knowledge distillation', 'Transfer learning', 'Model compression', '', '', 'Knowledge distillation refers to the process of training a student network to achieve better accuracy by learning from a pre-trained teacher network. Most of the existing knowledge distillation methods direct the student to follow the teacher by matching the teacher’s output, feature maps or their distribution. In this work, we propose a novel way to achieve this goal: by distilling the knowledge through a quantized visual words space. According to our method, the teacher’s feature maps are first quantized to represent the main visual concepts (i.e., visual words) encompassed in these maps and then the student is asked to predict those visual word representations. Despite its simplicity, we show that our approach is able to yield results that improve the state of the art on knowledge distillation for model compression and transfer learning scenarios. To that end, we provide an extensive evaluation across several network architectures and most commonly used benchmark datasets.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_11');
INSERT INTO `paper` VALUES (12591, 'RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects', 'Radar', 'Autonomous driving', 'Object detection', '', '', 'We tackle the problem of exploiting Radar for perception in the context of self-driving as Radar provides complementary information to other sensors such as LiDAR or cameras in the form of Doppler velocity. The main challenges of using Radar are the noise and measurement ambiguities which have been a struggle for existing simple input or output fusion methods. To better address this, we propose a new solution that exploits both LiDAR and Radar sensors for perception. Our approach, dubbed RadarNet, features a voxel-based early fusion and an attention-based late fusion, which learn from data to exploit both geometric and dynamic information of Radar data. RadarNet achieves state-of-the-art results on two large-scale real-world datasets in the tasks of object detection and velocity estimation. We further show that exploiting Radar improves the perception capabilities of detecting faraway objects and understanding the motion of dynamic objects.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_29');
INSERT INTO `paper` VALUES (12592, 'RAFT: Recurrent All-Pairs Field Transforms for Optical Flow', '', '', '', '', '', 'We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_24');
INSERT INTO `paper` VALUES (12593, 'Ramifications of Approximate Posterior Inference for Bayesian Deep Learning in Adversarial and Out-of-Distribution Settings', 'OoD detection', 'Bayesian deep learning', 'Uncertainty quantification', 'Generalisation', 'Anomalies', 'Deep neural networks have been successful in diverse discriminative classification tasks, although, they are poorly calibrated often assigning high probability to misclassified predictions. Potential consequences could lead to trustworthiness and accountability of the models when deployed in real applications, where predictions are evaluated based on their confidence scores. Existing solutions suggest the benefits attained by combining deep neural networks and Bayesian inference to quantify uncertainty over the models’ predictions for ambiguous data points. In this work we propose to validate and test the efficacy of likelihood based models in the task of out of distribution detection (OoD). Across different datasets and metrics we show that Bayesian deep learning models indeed outperform conventional neural networks but in the event of minimal overlap between in/out distribution classes, even the best models exhibit a reduction in AUC scores in detecting OoD data. We hypothesise that the sensitivity of neural networks to unseen inputs could be a multi-factor phenomenon arising from the different architectural design choices often amplified by the curse of dimensionality. Preliminary investigations indicate the potential inherent role of bias due to choices of initialisation, architecture or activation functions. Furthermore, we perform an analysis on the effect of adversarial noise resistance methods regarding in and out-of-distribution performance when combined with Bayesian deep learners.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_5');
INSERT INTO `paper` VALUES (12594, 'RANSAC-Flow: Generic Two-Stage Image Alignment', 'Unsupervised dense image alignment', 'Applications to art', '', '', '', 'This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_36');
INSERT INTO `paper` VALUES (12595, 'RBF-Softmax: Learning Deep Representative Prototypes with Radial Basis Function Softmax', '', '', '', '', '', 'Deep neural networks have achieved remarkable successes in learning feature representations for visual classification. However, deep features learned by the softmax cross-entropy loss generally show excessive intra-class variations. We argue that, because the traditional softmax losses aim to optimize only the relative differences between intra-class and inter-class distances (logits), it cannot obtain representative class prototypes (class weights/centers) to regularize intra-class distances, even when the training is converged. Previous efforts mitigate this problem by introducing auxiliary regularization losses. But these modified losses mainly focus on optimizing intra-class compactness, while ignoring keeping reasonable relations between different class prototypes. These lead to weak models and eventually limit their performance. To address this problem, this paper introduces a novel Radial Basis Function (RBF) distances to replace the commonly used inner products in the softmax loss function, such that it can adaptively assign losses to regularize the intra-class and inter-class distances by reshaping the relative differences, and thus creating more representative prototypes of classes to improve optimization. The proposed RBF-Softmax loss function not only effectively reduces intra-class distances, stabilizes the training behavior, and reserves ideal relations between prototypes, but also significantly improves the testing performance. Experiments on visual recognition benchmarks including MNIST, CIFAR-10/100, and ImageNet demonstrate that the proposed RBF-Softmax achieves better results than cross-entropy and other state-of-the-art classification losses. The code is at https://github.com/2han9x1a0release/RBF-Softmax.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_18');
INSERT INTO `paper` VALUES (12596, 'RD-GAN: Few/Zero-Shot Chinese Character Style Transfer via Radical Decomposition and Rendering', 'GAN', 'Style transfer', 'Radical decomposition', 'Few-Shot/Zero-Shot learning', '', 'Style transfer has attracted much interest owing to its various applications. Compared with English character or general artistic style transfer, Chinese character style transfer remains a challenge owing to the large size of the vocabulary (70224 characters in GB18010-2005) and the complexity of the structure. Recently some GAN-based methods were proposed for style transfer; however, they treated Chinese characters as a whole, ignoring the structures and radicals that compose characters. In this paper, a novel radical decomposition-and-rendering-based GAN (RD-GAN) is proposed to utilize the radical-level compositions of Chinese characters and achieves few-shot/zero-shot Chinese character style transfer. The RD-GAN consists of three components: a radical extraction module (REM), radical rendering module (RRM), and multi-level discriminator (MLD). Experiments demonstrate that our method has a powerful few-shot/zero-shot generalization ability by using the radical-level compositions of Chinese characters.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_10');
INSERT INTO `paper` VALUES (12597, 'ReActNet: Towards Precise Binary Neural Network with Generalized Activation Functions', '', '', '', '', '', 'In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-the-arts by a large margin. Specifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0% and 3.6% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0% top-1 accuracy on ImageNet dataset. Code and models are available at: https://github.com/liuzechun/ReActNet.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_9');
INSERT INTO `paper` VALUES (12598, 'READ: Reciprocal Attention Discriminator for Image-to-Video Re-identification', 'Image and video understanding', 'Identity retrieval', 'Re-identification', 'Attention', '', 'Person re-identification (re-ID) is the problem of visually identifying a person given a database of identities. In this work, we focus on image-to-video re-ID which compares a single query image to videos in the gallery. The main challenge is the asymmetry association of an image and a video, and overcoming the difference caused by the additional temporal dimension. To this end, we propose an attention-aware discriminator architecture. The attention occurs across different modalities, and even different identities to aggregate useful spatio-temporal information for comparison. The information is effectively fused into a united feature, followed by the final prediction of a similarity score. The performance of the method is shown with image-to-video person re-identification benchmarks (DukeMTMC-VideoReID, and MARS).', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_20');
INSERT INTO `paper` VALUES (12599, 'Real Image Super Resolution via Heterogeneous Model Ensemble Using GP-NAS', 'Single image super resolution', 'Dense residual network', 'Neural architecture search', '', '', 'With advancement in deep neural network (DNN), recent state-of-the-art (SOTA) image super-resolution (SR) methods have achieved impressive performance using deep residual network with dense skip connections. While these models perform well on benchmark dataset where low-resolution (LR) images are constructed from high-resolution (HR) references with known blur kernel, real image SR is more challenging when both images in the LR-HR pair are collected from real cameras. Based on existing dense residual networks, a Gaussian process based neural architecture search (GP-NAS) scheme is utilized to find candidate network architectures using a large search space by varying the number of dense residual blocks, the block size and the number of features. A suite of heterogeneous models with diverse network structure and hyperparameter are selected for model-ensemble to achieve outstanding performance in real image SR. The proposed method won the first place in all three tracks of the AIM 2020 Real Image Super-Resolution Challenge.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_25');
INSERT INTO `paper` VALUES (12600, 'Real-Time Detection of Multiple Targets from a Moving 360\\(^{\\circ }\\) Panoramic Imager in the Wild', '360\\(^{\\circ }\\) vision', 'Object recognition', 'MobileNets', 'Convolution neural networks', 'Geometrical model', 'Our goal is to develop embedded and mobile vision applications leveraging state-of-the-art visual sensors and efficient neural network architectures deployed on emerging neural computing engines for smart monitoring and inspection purposes. In this paper, we present 360\\(^{\\circ }\\) vision system onboard an automobile or UAV platform for large field-of-view and real-time detection of multiple challenging objects. The targeted objects include flag as a deformable object; UAV as a tiny, flying object which changes its scales and positions rapidly; and grouped objects containing piled sandbags as deformable objects in a group themselves, flag and stop sign to form a scene representing an artificial fake checkpoint. Barrel distortions owing to the 360\\(^{\\circ }\\) optics make the detection task even more challenging. A light-weight neural network model based on MobileNets architecture is transfer learned for detection of the custom objects with very limited training data. In method 1, we generated a dataset of perspective planar images via a virtual camera model which projects a patch on the hemisphere to a 2D plane. In method 2, the panomorph images are directly used without projection. Real-time detection of the objects in 360\\(^{\\circ }\\) video is realized by feeding live streamed frames captured by the full hemispheric (180\\(^{\\circ }\\) \\(\\times \\) 360\\(^{\\circ }\\)) field-of-view ImmerVision Enables panomorph lens to the trained MobileNets model. We found that with only few training data which is far less than 10 times of Vapnik–Chervonenkis dimension of the model, the MobileNets model achieves a detection rate of 80–90% for test data having a similar distribution as the training data. However, the model performance dropped drastically when it was put in action in the wild for unknown data in which both weather and lighting conditions were different. The generalization capability of the model can be improved by training with more data. The contribution of this work is a 360\\(^{\\circ }\\) vision hardware and software system for real-time detection of challenging objects. This system could be configured for very low-power embedded applications by running inferences via a neural computing engine such as Intel Movidius NSC2 or HiSilicon Kirin 970.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_8');
INSERT INTO `paper` VALUES (12601, 'Real-Time Embedded Computer Vision on UAVs:', 'Computer vision', 'Real-time', 'UAVs', 'Embedded hardware', 'Deep learning', 'In this paper we present an overview of the contributed work presented at the UAVision2020 (International workshop on Computer Vision for UAVs) ECCV workshop. Note that during ECCV2020 this workshop was merged with the VisDrone2020 workshop. This paper only summarizes the results of the regular paper track and the ERTI challenge. The workshop focused on real-time image processing on-board of Unmanned Aerial Vehicles (UAVs). For such applications the computational complexity of state-of-the-art computer vision algorithms often conflicts with the need for real-time operation and the extreme resource limitations of the hardware. Apart from a summary of the accepted workshop papers and an overview of the challenge, this work also aims to identify common challenges and concerns which were addressed by multiple authors during the workshop, and their proposed solutions.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_40');
INSERT INTO `paper` VALUES (12602, 'Real-Time Sign Language Detection Using Human Pose Estimation', 'Sign language detection', 'Sign language processing', '', '', '', 'We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the Public DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4 ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_17');
INSERT INTO `paper` VALUES (12603, 'Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms', '', '', '', '', '', 'Numerous learning-based approaches to single image deblurring for camera and object motion blurs have recently been proposed. To generalize such approaches to real-world blurs, large datasets of real blurred images and their ground truth sharp images are essential. However, there are still no such datasets, thus all the existing approaches resort to synthetic ones, which leads to the failure of deblurring real-world images. In this work, we present a large-scale dataset of real-world blurred images and ground truth sharp images for learning and benchmarking single image deblurring methods. To collect our dataset, we build an image acquisition system to simultaneously capture geometrically aligned pairs of blurred and sharp images, and develop a postprocessing method to produce high-quality ground truth images. We analyze the effect of our postprocessing method and the performance of existing deblurring methods. Our analysis shows that our dataset significantly improves deblurring quality for real-world blurred images.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_12');
INSERT INTO `paper` VALUES (12604, 'Recognition of Affective and Grammatical Facial Expressions: A Study for Brazilian Sign Language', 'Facial action unit recognition', 'Non-manual markers', 'Libras', 'Sign language', '', 'Individuals with hearing impairment typically face difficulties in communicating with hearing individuals and during the acquisition of reading and writing skills. Widely adopted by the deaf, Sign Language (SL) has a grammatical structure where facial expressions assume grammatical and affective functions, differentiate lexical items, participate in syntactic construction, and contribute to intensification processes. Automatic Sign Language Recognition (ASLR) technology supports the communication between deaf and hearing individuals, translating sign language gestures into written or spoken sentences of a target language. The recognition of facial expressions can improve ASLR accuracy rates. There are cases where the absence of a facial expression can create wrong translations, making them necessary for the understanding of sign language. This paper presents an approach to facial recognition for sign language. Brazilian Sign Language (Libras) is used as a case study. In our approach, we code Libras’ facial expression using the Facial Action Coding System (FACS). In the paper, we evaluate two convolutional neural networks, a standard CNN and hybrid CNN+LSTM, for AU recognition. We evaluate the models on a challenging real-world video dataset of facial expressions in Libras. The results obtained were 0.87 f1-score average and indicated the potential of the system to recognize Libras’ facial expressions.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_16');
INSERT INTO `paper` VALUES (12605, 'Reconstructing NBA Players', '3D human reconstruction', '', '', '', '', 'Great progress has been made in 3D body pose and shape estimation from a single photo. Yet, state-of-the-art results still suffer from errors due to challenging body poses, modeling clothing, and self occlusions. The domain of basketball games is particularly challenging, as it exhibits all of these challenges. In this paper, we introduce a new approach for reconstruction of basketball players that outperforms the state-of-the-art. Key to our approach is a new method for creating poseable, skinned models of NBA players, and a large database of meshes (derived from the NBA2K19 video game) that we are releasing to the research community. Based on these models, we introduce a new method that takes as input a single photo of a clothed player in any basketball pose and outputs a high resolution mesh and 3D pose for that player. We demonstrate substantial improvement over state-of-the-art, single-image methods for body shape reconstruction. Code and dataset are available at http://grail.cs.washington.edu/projects/nba_players/.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_11');
INSERT INTO `paper` VALUES (12606, 'Reconstructing the Noise Variance Manifold for Image Denoising', '', '', '', '', '', 'Deep Convolutional Neural Networks (CNNs) have been successfully used in many low-level vision problems like image denoising. Although the conditional image generation techniques have led to large improvements in this task, there has been little effort in providing conditional generative adversarial networks (cGANs) with an explicit way of understanding the image noise for object-independent denoising reliable for real-world applications. The task of leveraging structures in the target space is unstable due to the complexity of patterns in natural scenes, so the presence of unnatural artifacts or over-smoothed image areas cannot be avoided. To fill the gap, in this work we introduce the idea of a cGAN which explicitly leverages structure in the image noise variance space. By learning directly a low dimensional manifold of the image noise variance, the generator promotes the removal from the noisy image only that information which spans this manifold. This idea brings many advantages while it can be appended at the end of any denoiser to significantly improve its performance. Based on our experiments, our model substantially outperforms existing state-of-the-art architectures, resulting in denoised images with less over-smoothing and better detail.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_36');
INSERT INTO `paper` VALUES (12607, 'Recurrent Image Annotation with Explicit Inter-label Dependencies', 'Image annotation', 'Multi-label learning', 'CNN-RNN framework', 'Inter-label dependencies', 'Order-free training', 'Inspired by the success of the CNN-RNN framework in the image captioning task, several works have explored this in multi-label image annotation with the hope that the RNN followed by a CNN would encode inter-label dependencies better than using a CNN alone. To do so, for each training sample, the earlier methods converted the ground-truth label-set into a sequence of labels based on their frequencies (e.g., rare-to-frequent) for training the RNN. However, since the ground-truth is an unordered set of labels, imposing a fixed and predefined sequence on them does not naturally align with this task. To address this, some of the recent papers have proposed techniques that are capable to train the RNN without feeding the ground-truth labels in a particular sequence/order. However, most of these techniques leave it to the RNN to implicitly choose one sequence for the ground-truth labels corresponding to each sample at the time of training, thus making it inherently biased. In this paper, we address this limitation and propose a novel approach in which the RNN is explicitly forced to learn multiple relevant inter-label dependencies, without the need of feeding the ground-truth in any particular order. Using thorough empirical comparisons, we demonstrate that our approach outperforms several state-of-the-art techniques on two popular datasets (MS-COCO and NUS-WIDE). Additionally, it provides a new perspecitve of looking at an unordered set of labels as equivalent to a collection of different permutations (sequences) of those labels, thus naturally aligning with the image annotation task. Our code is available at: https://github.com/ayushidutta/multi-order-rnn.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_12');
INSERT INTO `paper` VALUES (12608, 'ReDro: Efficiently Learning Large-Sized SPD Visual Representation', 'Block diagonal matrix', 'Covariance', 'Eigen-decomposition', 'SPD representation', 'Fine-grained image recognition', 'Symmetric positive definite (SPD) matrix has recently been used as an effective visual representation. When learning this representation in deep networks, eigen-decomposition of covariance matrix is usually needed for a key step called matrix normalisation. This could result in significant computational cost, especially when facing the increasing number of channels in recent advanced deep networks.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_1');
INSERT INTO `paper` VALUES (12609, 'Reducing Distributional Uncertainty by Mutual Information Maximisation and Transferable Feature Learning', 'Distributional uncertainty', 'Domain discrepancy', 'Mutual information', 'Object shape', 'Self-supervised learning', 'Distributional uncertainty exists broadly in many real-world applications, one of which in the form of domain discrepancy. Yet in the existing literature, the mathematical definition of it is missing. In this paper, we propose to formulate the distributional uncertainty both between the source(s) and target domain(s) and within each domain using mutual information. Further, to reduce distributional uncertainty (e.g. domain discrepancy), we (1) maximise the mutual information between source and target domains and (2) propose a transferable feature learning scheme, balancing two complementary and discriminative feature learning processes (general texture learning and self-supervised transferable shape learning) according to the uncertainty. We conduct extensive experiments on both domain adaption and domain generalisation using challenging common benchmarks: Office-Home and DomainNet. Results show the great effectiveness of the proposed method and its superiority over the state-of-the-art methods.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_35');
INSERT INTO `paper` VALUES (12610, 'Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder', 'Deep-learning', 'Visual Question Answering', 'Language bias', '', '', 'Recent studies have shown that current VQA models are heavily biased on the language priors in the train set to answer the question, irrespective of the image. E.g., overwhelmingly answer “what sport is” as “tennis” or “what color banana” as “yellow.” This behavior restricts them from real-world application scenarios. In this work, we propose a novel model-agnostic question encoder, Visually-Grounded Question Encoder (VGQE), for VQA that reduces this effect. VGQE utilizes both visual and language modalities equally while encoding the question. Hence the question representation itself gets sufficient visual-grounding, and thus reduces the dependency of the model on the language priors. We demonstrate the effect of VGQE on three recent VQA models and achieve state-of-the-art results on the bias-sensitive split of the VQAv2 dataset; VQA-CPv2. Further, unlike the existing bias-reduction techniques, on the standard VQAv2 benchmark, our approach does not drop the accuracy; instead, it improves the performance.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_2');
INSERT INTO `paper` VALUES (12611, 'Reducing the Sim-to-Real Gap for Event Cameras', '', '', '', '', '', 'Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called ‘events’ with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20–40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_32');
INSERT INTO `paper` VALUES (12612, 'ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes', '', '', '', '', '', 'In this work we study the problem of using referential language to identify common objects in real-world 3D scenes. We focus on a challenging setup where the referred object belongs to a fine-grained object class and the underlying scene contains multiple object instances of that class. Due to the scarcity and unsuitability of existent 3D-oriented linguistic resources for this task, we first develop two large-scale and complementary visio-linguistic datasets: i) Sr3D, which contains 83.5 K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene, and ii) Nr3D which contains 41.5K natural, free-form, utterances collected by deploying a 2-player object reference game in 3D scenes. Using utterances of either datasets, human listeners can recognize the referred object with high (>86%, 92% resp.) accuracy. By tapping on this data, we develop novel neural listeners that can comprehend object-centric natural language and identify the referred object directly in a 3D scene. Our key technical contribution is designing an approach for combining linguistic and geometric information (in the form of 3D point clouds) and creating multi-modal (3D) neural listeners . We also show that architectures which promote object-to-object communication via graph neural networks outperform less context-aware alternatives, and that fine-grained object classification is a bottleneck for language-assisted 3D object identification.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_25');
INSERT INTO `paper` VALUES (12613, 'Reflection Backdoor: A Natural Backdoor Attack on Deep Neural Networks', 'Backdoor attack', 'Natural reflection', 'Deep neural networks', '', '', 'Recent studies have shown that DNNs can be compromised by backdoor attacks crafted at training time. A backdoor attack installs a backdoor into the victim model by injecting a backdoor pattern into a small proportion of the training data. At test time, the victim model behaves normally on clean test data, yet consistently predicts a specific (likely incorrect) target class whenever the backdoor pattern is present in a test example. While existing backdoor attacks are effective, they are not stealthy. The modifications made on training data or labels are often suspicious and can be easily detected by simple data filtering or human inspection. In this paper, we present a new type of backdoor attack inspired by an important natural phenomenon: reflection. Using mathematical modeling of physical reflection models, we propose reflection backdoor (Refool) to plant reflections as backdoor into a victim model. We demonstrate on 3 computer vision tasks and 5 datasets that, Refoolcan attack state-of-the-art DNNs with high success rate, and is resistant to state-of-the-art backdoor defenses.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_11');
INSERT INTO `paper` VALUES (12614, 'Reflection Separation via Multi-bounce Polarization State Tracing', 'Reflection removal', 'Polarization simulation engine', 'Ray-tracing', 'Polarization tracing', '', 'Reflection removal from photographs is an important task in computational photography, but also for computer vision tasks that involve imaging through windows and similar settings. Traditionally, the problem is approached as a single reflection removal problem under very controlled scenarios. In this paper we aim to generalize the reflection removal to real-world scenarios with more complicated light interactions. To this end, we propose a simple yet efficient learning framework for supervised image reflection separation with a polarization-guided ray-tracing model and loss function design. Instead of a conventional image sensor, we use a polarization sensor that instantaneously captures four linearly polarized photos of the scene in the same image. Through a combination of a new polarization-guided image formation model and a novel supervised learning framework for the interpretation of a ray-tracing image formation model, a general method is obtained to tackle general image reflection removal problems. We demonstrate our method with extensive experiments on both real and synthetic data and demonstrate the unprecedented quality of image reconstructions.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_46');
INSERT INTO `paper` VALUES (12615, 'Region Graph Embedding Network for Zero-Shot Learning', 'Zero-shot learning', 'Parts relation reasoning', 'Balance loss', '', '', 'Most of the existing Zero-Shot Learning (ZSL) approaches learn direct embeddings from global features or image parts (regions) to the semantic space, which, however, fail to capture the appearance relationships between different local regions within a single image. In this paper, to model the relations among local image regions, we incorporate the region-based relation reasoning into ZSL. Our method, termed as Region Graph Embedding Network (RGEN), is trained end-to-end from raw image data. Specifically, RGEN consists of two branches: the Constrained Part Attention (CPA) branch and the Parts Relation Reasoning (PRR) branch. CPA branch is built upon attention and produces the image regions. To exploit the progressive interactions among these regions, we represent them as a region graph, on which the parts relation reasoning is performed with graph convolutions, thus leading to our PRR branch. To train our model, we introduce both a transfer loss and a balance loss to contrast class similarities and pursue the maximum response consistency among seen and unseen outputs, respectively. Extensive experiments on four datasets well validate the effectiveness of the proposed method under both ZSL and generalized ZSL settings.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_33');
INSERT INTO `paper` VALUES (12616, 'Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses', 'Transferable adversarial example', 'Universal attack', '', '', '', 'This paper focuses on learning transferable adversarial examples specifically against defense models (models to defense adversarial attacks). In particular, we show that a simple universal perturbation can fool a series of state-of-the-art defenses.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_46');
INSERT INTO `paper` VALUES (12617, 'Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence', 'Image registration', 'Platynereis dumerilii', 'Shape context', '', '', 'Early development of an animal from an egg involves a rapid increase in cell number and several cell fate specification events accompanied by dynamic morphogenetic changes. In order to correlate the morphological changes with the genetic events, one typically needs to monitor the living system with several imaging modalities offering different spatial and temporal resolution. Live imaging allows monitoring the embryo at a high temporal resolution and observing the morphological changes. On the other hand, confocal images of specimens fixed and stained for the expression of certain genes enable observing the transcription states of an embryo at specific time points during development with high spatial resolution. The two imaging modalities cannot, by definition, be applied to the same specimen and thus, separately obtained images of different specimens need to be registered. Biologically, the most meaningful way to register the images is by identifying cellular correspondences between these two imaging modalities. In this way, one can bring the two sources of information into a single domain and combine dynamic information on morphogenesis with static gene expression data. Here we propose a new computational pipeline for identifying cell-to-cell correspondences between images from multiple modalities and for using these correspondences to register 3D images within and across imaging modalities. We demonstrate this pipeline by combining four-dimensional recording of embryogenesis of Spiralian annelid ragworm Platynereis dumerilii with three-dimensional scans of fixed Platynereis dumerilii embryos stained for the expression of a variety of important developmental genes. We compare our approach with methods for aligning point clouds and show that we match the accuracy of these state-of-the-art registration pipelines on synthetic data. We show that our approach outperforms these methods on real biological imaging datasets. Importantly, our approach uniquely provides, in addition to the registration, also the non-redundant matching of corresponding, biologically meaningful entities within the registered specimen which is the prerequisite for generating biological insights from the combined datasets. The complete pipeline is available for public use through a Fiji plugin.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_30');
INSERT INTO `paper` VALUES (12618, 'Regression of Instance Boundary by Aggregated CNN and GCN', 'Regression', 'Semantic segmentation', 'CNN', 'GCN', 'Attention', 'This paper proposes a straightforward, intuitive deep learning approach for (biomedical) image segmentation tasks. Different from the existing dense pixel classification methods, we develop a novel multi-level aggregation network to directly regress the coordinates of the boundary of instances in an end-to-end manner. The network seamlessly combines standard convolution neural network (CNN) with Attention Refinement Module (ARM) and Graph Convolution Network (GCN). By iteratively and hierarchically fusing the features across different layers of the CNN, our approach gains sufficient semantic information from the input image and pays special attention to the local boundaries with the help of ARM and GCN. In particular, thanks to the proposed aggregation GCN, our network benefits from direct feature learning of the instances’ boundary locations and the spatial information propagation across the image. Experiments on several challenging datasets demonstrate that our method achieves comparable results with state-of-the-art approaches but requires less inference time on the segmentation of fetal head in ultrasound images and of optic disc and optic cup in color fundus images.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_12');
INSERT INTO `paper` VALUES (12619, 'Regularization with Latent Space Virtual Adversarial Training', 'Consistency regularization', 'Adversarial training', 'Image classification', 'Semi-supervised learning', 'And unsupervised learning', 'Virtual Adversarial Training (VAT) has shown impressive results among recently developed regularization methods called consistency regularization. VAT utilizes adversarial samples, generated by injecting perturbation in the input space, for training and thereby enhances the generalization ability of a classifier. However, such adversarial samples can be generated only within a very small area around the input data point, which limits the adversarial effectiveness of such samples. To address this problem we propose LVAT (Latent space VAT), which injects perturbation in the latent space instead of the input space. LVAT can generate adversarial samples flexibly, resulting in more adverse effect and thus more effective regularization. The latent space is built by a generative model, and in this paper we examine two different type of models: variational auto-encoder and normalizing flow, specifically Glow.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_33');
INSERT INTO `paper` VALUES (12620, 'Regularized Loss for Weakly Supervised Single Class Semantic Segmentation', '', '', '', '', '', 'Fully supervised semantic segmentation is highly successful, but obtaining dense ground truth is expensive. Thus there is an increasing interest in weakly supervised approaches. We propose a new weakly supervised method for training CNNs to segment an object of a single class of interest. Instead of ground truth, we guide training with a regularized loss function. Regularized loss models prior knowledge about the likely object shape properties and thus guides segmentation towards the more plausible shapes. Training CNNs with regularized loss is difficult. We develop an annealing strategy that is crucial for successful training. The advantage of our method is simplicity: we use standard CNN architectures and intuitive and computationally efficient loss function. Furthermore, we apply the same loss function for any task/dataset, without any tailoring. We first evaluate our approach for salient object segmentation and co-segmentation. These tasks naturally involve one object class of interest. In some cases, our results are only a few points of standard performance measure behind those obtained training the same CNN with full supervision, and state-of-the art results in weakly supervised setting. Then we adapt our approach to weakly supervised multi-class semantic segmentation and obtain state-of-the-art results.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_21');
INSERT INTO `paper` VALUES (12621, 'Reinforced Axial Refinement Network for Monocular 3D Object Detection', '3D Object Detection', 'Refinement', 'Reinforcement learning', '', '', 'Monocular 3D object detection aims to extract the 3D position and properties of objects from a 2D input image. This is an ill-posed problem with a major difficulty lying in the information loss by depth-agnostic cameras. Conventional approaches sample 3D bounding boxes from the space and infer the relationship between the target object and each of them, however, the probability of effective samples is relatively small in the 3D space. To improve the efficiency of sampling, we propose to start with an initial prediction and refine it gradually towards the ground truth, with only one 3d parameter changed in each step. This requires designing a policy which gets a reward after several steps, and thus we adopt reinforcement learning to optimize it. The proposed framework, Reinforced Axial Refinement Network (RAR-Net), serves as a post-processing stage which can be freely integrated into existing monocular 3D detection methods, and improve the performance on the KITTI dataset with small extra computational costs.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_32');
INSERT INTO `paper` VALUES (12622, 'Reinforcement Learning for Improving Object Detection', 'Reinforcement learning', 'Object detection', 'Camera parameters', '', '', 'The performance of a trained object detection neural network depends a lot on the image quality. Generally, images are pre-processed before feeding them into the neural network and domain knowledge about the image dataset is used to choose the pre-processing techniques. In this paper, we introduce an algorithm called ObjectRL to choose the amount of a particular pre-processing to be applied to improve the object detection performances of pre-trained networks. The main motivation for ObjectRL is that an image which looks good to a human eye may not necessarily be the optimal one for a pre-trained object detector to detect objects.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_12');
INSERT INTO `paper` VALUES (12623, 'Relative Pose Estimation of Calibrated Cameras with Known \\(\\mathrm {SE}(3)\\) Invariants', '', '', '', '', '', 'The \\(\\mathrm {SE}(3)\\) invariants of a pose include its rotation angle and screw translation. In this paper, we present a complete comprehensive study of the relative pose estimation problem for a calibrated camera constrained by known \\(\\mathrm {SE}(3)\\) invariant, which involves 5 minimal problems in total. These problems reduces the minimal number of point pairs for relative pose estimation and improves the estimation efficiency and robustness. The \\(\\mathrm {SE}(3)\\) invariant constraints can come from extra sensor measurements or motion assumption. Unlike conventional relative pose estimation with extra constraints, no extrinsic calibration is required to transform the constraints to the camera frame. This advantage comes from the invariance of \\(\\mathrm {SE}(3)\\) invariants cross different coordinate systems on a rigid body and makes the solvers more convenient and flexible in practical applications. In addition to the concept of relative pose estimation constrained by \\(\\mathrm {SE}(3)\\) invariants, we also present a comprehensive study of existing polynomial formulations for relative pose estimation and discover their relationship. Different formulations are carefully chosen for each proposed problems to achieve best efficiency. Experiments on synthetic and real data shows performance improvement compared to conventional relative pose estimation methods. Our source code is available at: http://github.com/prclibo/relative_pose.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_13');
INSERT INTO `paper` VALUES (12624, 'Relative Pose from Deep Learned Depth and a Single Affine Correspondence', 'Pose estimation', 'Minimal solver', 'Depth prediction', 'Affine correspondences', 'Global structure from motion', 'We propose a new approach for combining deep-learned non-metric monocular depth with affine correspondences (ACs) to estimate the relative pose of two calibrated cameras from a single correspondence. Considering the depth information and affine features, two new constraints on the camera pose are derived. The proposed solver is usable within 1-point RANSAC approaches. Thus, the processing time of the robust estimation is linear in the number of correspondences and, therefore, orders of magnitude faster than by using traditional approaches. The proposed 1AC+D (Source code: Open image in new window https://github.com/eivan/one-ac-pose) solver is tested both on synthetic data and on 110395 publicly available real image pairs where we used an off-the-shelf monocular depth network to provide up-to-scale depth per pixel. The proposed 1AC+D leads to similar accuracy as traditional approaches while being significantly faster. When solving large-scale problems, e.g. pose-graph initialization for Structure-from-Motion (SfM) pipelines, the overhead of obtaining ACs and monocular depth is negligible compared to the speed-up gained in the pairwise geometric verification, i.e., relative pose estimation. This is demonstrated on scenes from the 1DSfM dataset using a state-of-the-art global SfM algorithm.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_37');
INSERT INTO `paper` VALUES (12625, 'REMIND Your Neural Network to Prevent Catastrophic Forgetting', 'Online learning', 'Brain-inspired', 'Deep learning', '', '', 'People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND’s robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND’s generality by pioneering online learning for Visual Question Answering (VQA) (https://github.com/tyler-hayes/REMIND).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_28');
INSERT INTO `paper` VALUES (12626, 'Remix: Rebalanced Mixup', 'Imbalanced data', 'Mixup', 'Regularization', 'Image classification', '', 'Deep image classifiers often perform poorly when training data are heavily class-imbalanced. In this work, we propose a new regularization technique, Remix, that relaxes Mixup’s formulation and enables the mixing factors of features and labels to be disentangled. Specifically, when mixing two samples, while features are mixed in the same fashion as Mixup, Remix assigns the label in favor of the minority class by providing a disproportionately higher weight to the minority class. By doing so, the classifier learns to push the decision boundaries towards the majority classes and balance the generalization error between majority and minority classes. We have studied the state-of-the art regularization techniques such as Mixup, Manifold Mixup and CutMix under class-imbalanced regime, and shown that the proposed Remix significantly outperforms these state-of-the-arts and several re-weighting and re-sampling techniques, on the imbalanced datasets constructed by CIFAR-10, CIFAR-100, and CINIC-10. We have also evaluated Remix on a real-world large-scale imbalanced dataset, iNaturalist 2018. The experimental results confirmed that Remix provides consistent and significant improvements over the previous methods.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_9');
INSERT INTO `paper` VALUES (12627, 'Renovating Parsing R-CNN for Accurate Multiple Human Parsing', 'Multiple human parsing', 'Region-based approach', 'Global semantic enhanced FPN', 'Parsing re-scoring network', '', 'Multiple human parsing aims to segment various human parts and associate each part with the corresponding instance simultaneously. This is a very challenging task due to the diverse human appearance, semantic ambiguity of different body parts, and complex background. Through analysis of multiple human parsing task, we observe that human-centric global perception and accurate instance-level parsing scoring are crucial for obtaining high-quality results. But the most state-of-the-art methods have not paid enough attention to these issues. To reverse this phenomenon, we present Renovating Parsing R-CNN (RP R-CNN), which introduces a global semantic enhanced feature pyramid network and a parsing re-scoring network into the existing high-performance pipeline. The proposed RP R-CNN adopts global semantic representation to enhance multi-scale features for generating human parsing maps, and regresses a confidence score to represent its quality. Extensive experiments show that RP R-CNN performs favorably against state-of-the-art methods on CIHP and MHP-v2 datasets. Code and models are available at https://github.com/soeaver/RP-R-CNN.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_25');
INSERT INTO `paper` VALUES (12628, 'Reparameterizing Convolutions for Incremental Multi-Task Learning Without Task Interference', 'Multi-task learning', 'Incremental learning', 'Task interference', '', '', 'Multi-task networks are commonly utilized to alleviate the need for a large number of highly specialized single-task networks. However, two common challenges in developing multi-task models are often overlooked in literature. First, enabling the model to be inherently incremental, continuously incorporating information from new tasks without forgetting the previously learned ones (incremental learning). Second, eliminating adverse interactions amongst tasks, which has been shown to significantly degrade the single-task performance in a multi-task setup (task interference). In this paper, we show that both can be achieved simply by reparameterizing the convolutions of standard neural network architectures into a non-trainable shared part (filter bank) and task-specific parts (modulators), where each modulator has a fraction of the filter bank parameters. Thus, our reparameterization enables the model to learn new tasks without adversely affecting the performance of existing ones. The results of our ablation study attest the efficacy of the proposed reparameterization. Moreover, our method achieves state-of-the-art on two challenging multi-task learning benchmarks, PASCAL-Context and NYUD, and also demonstrates superior incremental learning capability as compared to its close competitors. The code and models are made publicly available (https://github.com/menelaoskanakis/RCM).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_41');
INSERT INTO `paper` VALUES (12629, 'Representation Learning on Visual-Symbolic Graphs for Video Understanding', '', '', '', '', '', 'Events in natural videos typically arise from spatio-temporal interactions between actors and objects and involve multiple co-occurring activities and object classes. To capture this rich visual and semantic context, we propose using two graphs: (1) an attributed spatio-temporal visual graph whose nodes correspond to actors and objects and whose edges encode different types of interactions, and (2) a symbolic graph that models semantic relationships. We further propose a graph neural network for refining the representations of actors, objects and their interactions on the resulting hybrid graph. Our model goes beyond current approaches that assume nodes and edges are of the same type, operate on graphs with fixed edge weights and do not use a symbolic graph. In particular, our framework: a) has specialized attention-based message functions for different node and edge types; b) uses visual edge features; c) integrates visual evidence with label relationships; and d) performs global reasoning in the semantic space. Experiments on challenging video understanding tasks, such as temporal action localization on the Charades dataset, show that the proposed method leads to state-of-the-art performance.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_5');
INSERT INTO `paper` VALUES (12630, 'Representation Sharing for Fast Object Detector Search and Beyond', '', '', '', '', '', 'Region Proposal Network (RPN) provides strong support for handling the scale variation of objects in two-stage object detection. For one-stage detectors which do not have RPN, it is more demanding to have powerful sub-networks capable of directly capturing objects of unknown sizes. To enhance such capability, we propose an extremely efficient neural architecture search method, named Fast And Diverse (FAD), to better explore the optimal configuration of receptive fields and convolution types in the sub-networks for one-stage detectors. FAD consists of a designed search space and an efficient architecture search algorithm. The search space contains a rich set of diverse transformations designed specifically for object detection. To cope with the designed search space, a novel search algorithm termed Representation Sharing (RepShare) is proposed to effectively identify the best combinations of the defined transformations. In our experiments, FAD obtains prominent improvements on two types of one-stage detectors with various backbones. In particular, our FAD detector achieves 46.4 AP on MS-COCO (under single-scale testing), outperforming the state-of-the-art detectors, including the most recent NAS-based detectors, Auto-FPN [42] (searched for 16 GPU-days) and NAS-FCOS [39] (28 GPU-days), while significantly reduces the search cost to 0.6 GPU-days. Beyond object detection, we further demonstrate the generality of FAD on the more challenging instance segmentation, and expect it to benefit more tasks.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_28');
INSERT INTO `paper` VALUES (12631, 'Representative Graph Neural Network', 'Representative graph', 'Dynamic sampling', 'Semantic segmentation', 'Deep learning', '', 'Non-local operation is widely explored to model the long-range dependencies. However, the redundant computation in this operation leads to a prohibitive complexity. In this paper, we present a Representative Graph (RepGraph) layer to dynamically sample a few representative features, which dramatically reduces redundancy. Instead of propagating the messages from all positions, our RepGraph layer computes the response of one node merely with a few representative nodes. The locations of representative nodes come from a learned spatial offset matrix. The RepGraph layer is flexible to integrate into many visual architectures and combine with other operations. With the application of semantic segmentation, without any bells and whistles, our RepGraph network can compete or perform favourably against the state-of-the-art methods on three challenging benchmarks: ADE20K, Cityscapes, and PASCAL-Context datasets. In the task of object detection, our RepGraph layer can also improve the performance on the COCO dataset compared to the non-local operation. Code is available at https://git.io/RepGraph.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_23');
INSERT INTO `paper` VALUES (12632, 'Representative-Discriminative Learning for Open-Set Land Cover Classification of Satellite Imagery', 'Hyperspectral image classification', 'Open-set recognition', '', '', '', 'Land cover classification of satellite imagery is an important step toward analyzing the Earth’s surface. Existing models assume a closed-set setting where both the training and testing classes belong to the same label set. However, due to the unique characteristics of satellite imagery with extremely vast area of versatile cover materials, the training data are bound to be non-representative. In this paper, we study the problem of open-set land cover classification that identifies the samples belonging to unknown classes during testing, while maintaining performance on known classes. Although inherently a classification problem, both representative and discriminative aspects of data need to be exploited in order to better distinguish unknown classes from known. We propose a representative-discriminative open-set recognition (RDOSR) framework, which 1) projects data from the raw image space to the embedding feature space that facilitates differentiating similar classes, and further 2) enhances both the representative and discriminative capacity through transformation to a so-called abundance space. Experiments on multiple satellite benchmarks demonstrate effectiveness of the proposed method. We also show the generality of the proposed approach by achieving promising results on open-set classification tasks using RGB images.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_1');
INSERT INTO `paper` VALUES (12633, 'Residual and Dense UNet for Under-Display Camera Restoration', 'Under-display camera', 'Image restoration', 'Deblurring', 'Enhancement', 'Denoising', 'With the rapid development of electronic products, the increasing demand for full-screen devices has become a new trend, which facilitates the investigation of Under-Display Cameras (UDC). UDC can not only bring larger display-to-body ratio but also improve the interactive experience. However, when imaging sensor is mounted behind a display, existing screen materials will cause severe image degradation due to lower light transmission rate and diffraction effects. In order to promote the research in this field, RLQ-TOD 2020 held the Image Restoration Challenge for Under-Display Camera. The challenge was composed of two tracks – 4k Transparent OLED (T-OLED) and phone Pentile OLED (P-OLED) track. In this paper, we propose a UNet-like structure with two various basic building blocks to tackle this problem. We discover that T-OLED and P-OLED have different preferences with the model structure and the input patch size during training. With the proposed model, our team won the third place in the challenge on both T-OLED and P-OLED tracks.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_30');
INSERT INTO `paper` VALUES (12634, 'Residual Feature Distillation Network for Lightweight Image Super-Resolution', 'Image super-resolution', 'Computational photography', 'Image processing', '', '', 'Recent advances in single image super-resolution (SISR) explored the power of convolutional neural network (CNN) to achieve a better performance. Despite the great success of CNN-based methods, it is not easy to apply these methods to edge devices due to the requirement of heavy computation. To solve this problem, various fast and lightweight CNN models have been proposed. The information distillation network is one of the state-of-the-art methods, which adopts the channel splitting operation to extract distilled features. However, it is not clear enough how this operation helps in the design of efficient SISR models. In this paper, we propose the feature distillation connection (FDC) that is functionally equivalent to the channel splitting operation while being more lightweight and flexible. Thanks to FDC, we can rethink the information multi-distillation network (IMDN) and propose a lightweight and accurate SISR model called residual feature distillation network (RFDN). RFDN uses multiple feature distillation connections to learn more discriminative feature representations. We also propose a shallow residual block (SRB) as the main building block of RFDN so that the network can benefit most from residual learning while still being lightweight enough. Extensive experimental results show that the proposed RFDN achieves a better trade-off against the state-of-the-art methods in terms of performance and model complexity. Moreover, we propose an enhanced RFDN (E-RFDN) and won the first place in the AIM 2020 efficient super-resolution challenge. Code will be available at https://github.com/njulj/RFDN.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_2');
INSERT INTO `paper` VALUES (12635, 'Resolution Switchable Networks for Runtime Efficient Image Recognition', 'Efficient design', 'Multi-resolution', 'Ensemble distillation', '', '', 'We propose a general method to train a single convolutional neural network which is capable of switching image resolutions at inference. Thus the running speed can be selected to meet various computational resource limits. Networks trained with the proposed method are named Resolution Switchable Networks (RS-Nets). The basic training framework shares network parameters for handling images which differ in resolution, yet keeps separate batch normalization layers. Though it is parameter-efficient in design, it leads to inconsistent accuracy variations at different resolutions, for which we provide a detailed analysis from the aspect of the train-test recognition discrepancy. A multi-resolution ensemble distillation is further designed, where a teacher is learnt on the fly as a weighted ensemble over resolutions. Thanks to the ensemble and knowledge distillation, RS-Nets enjoy accuracy improvements at a wide range of resolutions compared with individually trained models. Extensive experiments on the ImageNet dataset are provided, and we additionally consider quantization problems. Code and models are available at https://github.com/yikaiw/RS-Nets.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_32');
INSERT INTO `paper` VALUES (12636, 'Rethinking Bottleneck Structure for Efficient Mobile Network Design', 'Sandglass block', 'Residual block', 'Efficient architecture design', 'Image classification', '', 'The inverted residual block is dominating architecture design for mobile networks recently. It changes the classic residual bottleneck by introducing two design rules: learning inverted residuals and using linear bottlenecks. In this paper, we rethink the necessity of such design changes and find it may bring risks of information loss and gradient confusion. We thus propose to flip the structure and present a novel bottleneck design, called the sandglass block, that performs identity mapping and spatial transformation at higher dimensions and thus alleviates information loss and gradient confusion effectively. Extensive experiments demonstrate that, different from the common belief, such bottleneck structure is more beneficial than the inverted ones for mobile networks. In ImageNet classification, by simply replacing the inverted residual block with our sandglass block without increasing parameters and computation, the classification accuracy can be improved by more than 1.7% over MobileNetV2. On Pascal VOC 2007 test set, we observe that there is also 0.9% mAP improvement in object detection. We further verify the effectiveness of the sandglass block by adding it into the search space of neural architecture search method DARTS. With 25% parameter reduction, the classification accuracy is improved by 0.13% over previous DARTS models. Code can be found at: https://github.com/zhoudaquan/rethinking_bottleneck_design.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_40');
INSERT INTO `paper` VALUES (12637, 'Rethinking Class Activation Mapping for Weakly Supervised Object Localization', 'Weakly Supervised Object Localization (WSOL)', 'Class Activation Mapping (CAM)', '', '', '', 'Weakly supervised object localization (WSOL) is a task of localizing an object in an image only using image-level labels. To tackle the WSOL problem, most previous studies have followed the conventional class activation mapping (CAM) pipeline: (i) training CNNs for a classification objective, (ii) generating a class activation map via global average pooling (GAP) on feature maps, and (iii) extracting bounding boxes by thresholding based on the maximum value of the class activation map. In this work, we reveal the current CAM approach suffers from three fundamental issues: (i) the bias of GAP that assigns a higher weight to a channel with a small activation area, (ii) negatively weighted activations inside the object regions and (iii) instability from the use of the maximum value of a class activation map as a thresholding reference. They collectively cause the problem that the localization to be highly limited to small regions of an object. We propose three simple but robust techniques that alleviate the problems, including thresholded average pooling, negative weight clamping, and percentile as a standard for thresholding. Our solutions are universally applicable to any WSOL methods using CAM and improve their performance drastically. As a result, we achieve the new state-of-the-art performance on three benchmark datasets of CUB-200–2011, ImageNet-1K, and OpenImages30K.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_37');
INSERT INTO `paper` VALUES (12638, 'Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?', '', '', '', '', '', 'The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code: http://github.com/WangYueFt/rfs/.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_16');
INSERT INTO `paper` VALUES (12639, 'Rethinking Image Deraining via Rain Streaks and Vapors', 'Deep image deraining', '', '', '', '', 'Single image deraining regards an input image as a fusion of a background image, a transmission map, rain streaks, and atmosphere light. While advanced models are proposed for image restoration (i.e., background image generation), they regard rain streaks with the same properties as background rather than transmission medium. As vapors (i.e., rain streaks accumulation or fog-like rain) are conveyed in the transmission map to model the veiling effect, the fusion of rain streaks and vapors do not naturally reflect the rain image formation. In this work, we reformulate rain streaks as transmission medium together with vapors to model rain imaging. We propose an encoder-decoder CNN named as SNet to learn the transmission map of rain streaks. As rain streaks appear with various shapes and directions, we use ShuffleNet units within SNet to capture their anisotropic representations. As vapors are brought by rain streaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict the transmission map of vapors in multi-scales based on that of rain streaks. Meanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The SNet, VNet, and ANet are jointly trained to predict transmission maps and atmosphere light for rain image restoration. Extensive experiments on the benchmark datasets demonstrate the effectiveness of the proposed visual model to predict rain streaks and vapors. The proposed deraining method performs favorably against state-of-the-art deraining approaches.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_22');
INSERT INTO `paper` VALUES (12640, 'Rethinking Image Inpainting via a Mutual Encoder-Decoder with Feature Equalizations', 'Deep image inpainting', 'Feature equalizations', '', '', '', 'Deep encoder-decoder based CNNs have advanced image inpainting methods for hole filling. While existing methods recover structures and textures step-by-step in the hole regions, they typically use two encoder-decoders for separate recovery. The CNN features of each encoder are learned to capture either missing structures or textures without considering them as a whole. The insufficient utilization of these encoder features hampers the performance of recovering both structures and textures. In this paper, we propose a mutual encoder-decoder CNN for joint recovery of both. We use CNN features from the deep and shallow layers of the encoder to represent structures and textures of an input image, respectively. The deep layer features are sent to a structure branch, while the shallow layer features are sent to a texture branch. In each branch, we fill holes in multiple scales of the CNN features. The filled CNN features from both branches are concatenated and then equalized. During feature equalization, we reweigh channel attentions first and propose a bilateral propagation activation function to enable spatial equalization. To this end, the filled CNN features of structure and texture mutually benefit each other to represent image content at all feature levels. We then use the equalized feature to supplement decoder features for output image generation through skip connections. Experiments on benchmark datasets show that the proposed method is effective to recover structures and textures and performs favorably against state-of-the-art approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_43');
INSERT INTO `paper` VALUES (12641, 'Rethinking Pseudo-LiDAR Representation', 'Image-based 3D detection', 'Data representation', 'Image', 'pseudo-LiDAR', 'Coordinate transformation', 'The recently proposed pseudo-LiDAR based 3D detectors greatly improve the benchmark of monocular/stereo 3D detection task. However, the underlying mechanism remains obscure to the research community. In this paper, we perform an in-depth investigation and observe that the efficacy of pseudo-LiDAR representation comes from the coordinate transformation, instead of data representation itself. Based on this observation, we design an image based CNN detector named PatchNet, which is more generalized and can be instantiated as pseudo-LiDAR based 3D detectors. Moreover, the pseudo-LiDAR data in our PatchNet is organized as the image representation, which means existing 2D CNN designs can be easily utilized for extracting deep features from input data and boosting 3D detection performance. We conduct extensive experiments on the challenging KITTI dataset, where the proposed PatchNet outperforms all existing pseudo-LiDAR based counterparts. Code has been made available at: https://github.com/xinzhuma/patchnet.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_19');
INSERT INTO `paper` VALUES (12642, 'Rethinking the Defocus Blur Detection Problem and a Real-Time Deep DBD Model', 'Defocus blur detection', 'Self-supervision', 'Hard-mining', '', '', 'Defocus blur detection (DBD) is a classical low level vision task. It has recently attracted attention focusing on designing complex convolutional neural networks (CNN) which make full use of both low level features and high level semantic information. The heavy networks used in these methods lead to low processing speed, resulting difficulty in applying to real-time applications. In this work, we propose novel perspectives on the DBD problem and design convenient approach to build a real-time cost-effective DBD model. First, we observe that the semantic information does not always relate to and sometimes mislead the blur detection. We start from the essential characteristics of the DBD problem and propose a data augmentation method accordingly to inhibit the semantic information and enforce the model to learn image blur related features rather than the semantic features. A novel self-supervision training objective is proposed to enhance the model training consistency and stability. Second, by rethinking the relationship between defocus blur detection and salience detection, we identify two previously ignored but common scenarios, based on which we design a hard mining strategy to enhance the DBD model. By using the proposed techniques, our model that uses a slightly modified U-Net as backbone, improves the processing speed by more than 3 times and performs competitively against state of the art methods. Ablation study is also conducted to verify the effectiveness of each part of our proposed methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_36');
INSERT INTO `paper` VALUES (12643, 'Rethinking the Distribution Gap of Person Re-identification with Camera-Based Batch Normalization', 'Person re-identification', 'Distribution gap', 'Camera-based batch normalization', '', '', 'The fundamental difficulty in person re-identification (ReID) lies in learning the correspondence among individual cameras. It strongly demands costly inter-camera annotations, yet the trained models are not guaranteed to transfer well to previously unseen cameras. These problems significantly limit the application of ReID. This paper rethinks the working mechanism of conventional ReID approaches and puts forward a new solution. With an effective operator named Camera-based Batch Normalization (CBN), we force the image data of all cameras to fall onto the same subspace, so that the distribution gap between any camera pair is largely shrunk. This alignment brings two benefits. First, the trained model enjoys better abilities to generalize across scenarios with unseen cameras as well as transfer across multiple training sets. Second, we can rely on intra-camera annotations, which have been undervalued before due to the lack of cross-camera information, to achieve competitive ReID performance. Experiments on a wide range of ReID tasks demonstrate the effectiveness of our approach. The code is available at https://github.com/automan000/Camera-based-Person-ReID.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_9');
INSERT INTO `paper` VALUES (12644, 'RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval', '', '', '', '', '', 'Image generation from scene description is a cornerstone technique for the controlled generation, which is beneficial to applications such as content creation and image editing. In this work, we aim to synthesize images from scene description with retrieved patches as reference. We propose a differentiable retrieval module. With the differentiable retrieval module, we can (1) make the entire pipeline end-to-end trainable, enabling the learning of better feature embedding for retrieval; (2) encourage the selection of mutually compatible patches with additional objective functions. We conduct extensive quantitative and qualitative experiments to demonstrate that the proposed method can generate realistic and diverse images, where the retrieved patches are reasonable and mutually compatible.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_15');
INSERT INTO `paper` VALUES (12645, 'Reversing the Cycle: Self-supervised Deep Stereo Through Enhanced Monocular Distillation', 'Stereo matching', 'Self-supervised learning', 'Distillation', '', '', 'In many fields, self-supervised learning solutions are rapidly evolving and filling the gap with supervised approaches. This fact occurs for depth estimation based on either monocular or stereo, with the latter often providing a valid source of self-supervision for the former. In contrast, to soften typical stereo artefacts, we propose a novel self-supervised paradigm reversing the link between the two. Purposely, in order to train deep stereo networks, we distill knowledge through a monocular completion network. This architecture exploits single-image clues and few sparse points, sourced by traditional stereo algorithms, to estimate dense yet accurate disparity maps by means of a consensus mechanism over multiple estimations. We thoroughly evaluate with popular stereo datasets the impact of different supervisory signals showing how stereo networks trained with our paradigm outperform existing self-supervised frameworks. Finally, our proposal achieves notable generalization capabilities dealing with domain shift issues. Code available at https://github.com/FilippoAleotti/Reversing.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_36');
INSERT INTO `paper` VALUES (12646, 'REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets', 'Dataset bias', 'Dataset analysis', 'Computer vision fairness', '', '', 'Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. To tackle this issue and to enable the preemptive analysis of large-scale dataset, we present our tool. REVISE (REvealing VIsual biaSEs) is a tool that assists in the investigation of a visual dataset, surfacing potential biases currently along three dimensions: (1) object-based, (2) gender-based, and (3) geography-based. Object-based biases relate to size, context, or diversity of object representation. Gender-based metrics aim to reveal the stereotypical portrayal of people of different genders. Geography-based analyses consider the representation of different geographic locations. REVISE sheds light on the dataset al.ong these dimensions; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool then further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at https://github.com/princetonvisualai/revise-tool.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_43');
INSERT INTO `paper` VALUES (12647, 'Revisiting the Threat Space for Vision-Based Keystroke Inference Attacks', 'Side-channel attack', 'Domain adaptation', 'Synthetic data', '', '', 'A vision-based keystroke inference attack is a side-channel attack in which an attacker uses an optical device to record users on their mobile devices and infer their keystrokes. The threat space for these attacks has been studied in the past, but we argue that the defining characteristics for this threat space, namely the strength of the attacker, are outdated. Previous works do not study adversaries with vision systems that have been trained with deep neural networks because these models require large amounts of training data and curating such a dataset is expensive. To address this, we create a large-scale synthetic dataset to simulate the attack scenario for a keystroke inference attack. We show that first pre-training on synthetic data, followed by adopting transfer learning techniques on real-life data, increases the performance of our deep learning models. This indicates that these models are able to learn rich, meaningful representations from our synthetic data and that training on the synthetic data can help overcome the issue of having small, real-life datasets for vision-based key stroke inference attacks. For this work, we focus on single keypress classification where the input is a frame of a keypress and the output is a predicted key. We are able to get an accuracy of 95.6% after pre-training a CNN on our synthetic data and training on a small set of real-life data in an adversarial domain adaptation framework.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_33');
INSERT INTO `paper` VALUES (12648, 'Rewriting a Deep Generative Model', '', '', '', '', '', 'A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_21');
INSERT INTO `paper` VALUES (12649, 'RGB-D Salient Object Detection with Cross-Modality Modulation and Selection', '', '', '', '', '', 'We present an effective method to progressively integrate and refine the cross-modality complementarities for RGB-D salient object detection (SOD). The proposed network mainly solves two challenging issues: 1) how to effectively integrate the complementary information from RGB image and its corresponding depth map, and 2) how to adaptively select more saliency-related features. First, we propose a cross-modality feature modulation (cmFM) module to enhance feature representations by taking the depth features as prior, which models the complementary relations of RGB-D data. Second, we propose an adaptive feature selection (AFS) module to select saliency-related features and suppress the inferior ones. The AFS module exploits multi-modality spatial feature fusion with the self-modality and cross-modality interdependencies of channel features are considered. Third, we employ a saliency-guided position-edge attention (sg-PEA) module to encourage our network to focus more on saliency-related regions. The above modules as a whole, called cmMS block, facilitates the refinement of saliency features in a coarse-to-fine fashion. Coupled with a bottom-up inference, the refined saliency features enable accurate and edge-preserving SOD. Extensive experiments demonstrate that our network outperforms state-of-the-art saliency detectors on six popular RGB-D SOD benchmarks.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_14');
INSERT INTO `paper` VALUES (12650, 'RhyRNN: Rhythmic RNN for Recognizing Events in Long and Complex Videos', 'Video understanding', 'Complex event recognition', 'RNN', '', '', 'Though many successful approaches have been proposed for recognizing events in short and homogeneous videos, doing so with long and complex videos remains a challenge. One particular reason is that events in long and complex videos can consist of multiple heterogeneous sub-activities (in terms of rhythms, activity variants, composition order, etc.) within quite a long period. This fact brings about two main difficulties: excessive/varying length and complex video dynamic/rhythm. To address this, we propose Rhythmic RNN (RhyRNN) which is capable of handling long video sequences (up to 3,000 frames) as well as capturing rhythms at different scales. We also propose two novel modules: diversity-driven pooling (DivPool) and bilinear reweighting (BR), which consistently and hierarchically abstract higher-level information. We study the behavior of RhyRNN and empirically show that our method works well even when only event-level labels are available in the training stage (compared to algorithms requiring sub-activity labels for recognition), and thus is more practical when the sub-activity labels are missing or difficult to obtain. Extensive experiments on several public datasets demonstrate that, even without fine-tuning the feature backbones, our method can achieve promising performance for long and complex videos that contain multiple sub-activities.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_8');
INSERT INTO `paper` VALUES (12651, 'Robust and On-the-Fly Dataset Denoising for Image Classification', '', '', '', '', '', 'Memorization in over-parameterized neural networks could severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are hard to avoid in extremely large datasets collected with weak supervision. We address this problem by reasoning counterfactually about the loss distribution of examples with uniform random labels had they were trained with the real examples, and use this information to remove noisy examples from the training set. First, we observe that examples with uniform random labels have higher losses when trained with stochastic gradient descent under large learning rates. Then, we propose to model the loss distribution of the counterfactual examples using only the network parameters, which is able to model such examples with remarkable success. Finally, we propose to remove examples whose loss exceeds a certain quantile of the modeled loss distribution. This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead compared to standard training. ODD is able to achieve state-of-the-art results on a wide range of datasets including real-world ones such as WebVision and Clothing1M.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_33');
INSERT INTO `paper` VALUES (12652, 'Robust Long-Term Object Tracking via Improved Discriminative Model Prediction', 'Long-term object tracking', 'Robust object tracking', 'Uncertainty reduction', 'Random erasing', 'Random search', 'We propose an improved discriminative model prediction method for robust long-term tracking based on a pre-trained short-term tracker. The baseline pre-trained short-term tracker is SuperDiMP which combines the bounding-box regressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP improves SuperDiMP in the following three aspects: (1) Uncertainty reduction using random erasing: To make our model robust, we exploit an agreement from multiple images after erasing random small rectangular areas as a certainty. And then, we correct the tracking state of our model accordingly. (2) Random search with spatio-temporal constraints: we propose a robust random search method with a score penalty applied to prevent the problem of sudden detection at a distance. (3) Background augmentation for more discriminative feature learning: We augment various backgrounds that are not included in the search area to train a more robust model in the background clutter. In experiments on the VOT-LT2020 benchmark dataset, the proposed method achieves comparable performance to the state-of-the-art long-term trackers. The source code is available at: https://github.com/bismex/RLT-DIMP.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_40');
INSERT INTO `paper` VALUES (12653, 'Robust Neural Networks Inspired by Strong Stability Preserving Runge-Kutta Methods', '', '', '', '', '', 'Deep neural networks have achieved state-of-the-art performance in a variety of fields. Recent works observe that a class of widely used neural networks can be viewed as the Euler method of numerical discretization. From the numerical discretization perspective, Strong Stability Preserving (SSP) methods are more advanced techniques than the explicit Euler method that produce both accurate and stable solutions. Motivated by the SSP property and a generalized Runge-Kutta method, we proposed Strong Stability Preserving networks (SSP networks) which improve robustness against adversarial attacks. We empirically demonstrate that the proposed networks improve the robustness against adversarial examples without any defensive methods. Further, the SSP networks are complementary with a state-of-the-art adversarial training scheme. Lastly, our experiments show that SSP networks suppress the blow-up of adversarial perturbations. Our results open up a way to study robust architectures of neural networks leveraging rich knowledge from numerical discretization literature.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_24');
INSERT INTO `paper` VALUES (12654, 'Robust Re-Identification by Multiple Views Knowledge Distillation', 'Deep learning', 'Re-Identification', 'Knowledge Distillation', '', '', 'To achieve robustness in Re-Identification, standard methods leverage tracking information in a Video-To-Video fashion. However, these solutions face a large drop in performance for single image queries (e.g., Image-To-Video setting). Recent works address this severe degradation by transferring temporal information from a Video-based network to an Image-based one. In this work, we devise a training strategy that allows the transfer of a superior knowledge, arising from a set of views depicting the target object. Our proposal – Views Knowledge Distillation (VKD) – pins this visual variety as a supervision signal within a teacher-student framework, where the teacher educates a student who observes fewer views. As a result, the student outperforms not only its teacher but also the current state-of-the-art in Image-To-Video by a wide margin (6.3% mAP on MARS, 8.6% on Duke and 5% on VeRi-776). A thorough analysis – on Person, Vehicle and Animal Re-ID – investigates the properties of VKD from a qualitatively and quantitatively perspective. Code is available at https://github.com/aimagelab/VKD.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_6');
INSERT INTO `paper` VALUES (12655, 'Robust Super-Resolution of Real Faces Using Smooth Features', '', '', '', '', '', 'Real low-resolution (LR) face images contain degradations which are too varied and complex to be captured by known downsampling kernels and signal-independent noises. So, in order to successfully super-resolve real faces, a method needs to be robust to a wide range of noise, blur, compression artifacts etc. Some of the recent works attempt to model these degradations from a dataset of real images using a Generative Adversarial Network (GAN). They generate synthetically degraded LR images and use them with corresponding real high-resolution (HR) image to train a super-resolution (SR) network using a combination of a pixel-wise loss and an adversarial loss. In this paper, we propose a two module super-resolution network where the feature extractor module extracts robust features from the LR image, and the SR module generates an HR estimate using only these robust features. We train a degradation GAN to convert bicubically downsampled clean images to real degraded images, and interpolate between the obtained degraded LR image and its clean LR counterpart. This interpolated LR image is then used along with it’s corresponding HR counterpart to train the super-resolution network from end to end. Entropy Regularized Wasserstein Divergence is used to force the encoded features learnt from the clean and degraded images to closely resemble those extracted from the interpolated image to ensure robustness.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_11');
INSERT INTO `paper` VALUES (12656, 'Robust Tracking Against Adversarial Attacks', 'Visual tracking', 'Adversarial attack', '', '', '', 'While deep convolutional neural networks (CNNs) are vulnerable to adversarial attacks, considerably few efforts have been paid to construct robust deep tracking algorithms against adversarial attacks. Current studies on adversarial attack and defense mainly reside in a single image. In this work, we first attempt to generate adversarial examples on top of video sequences to improve the tracking robustness against adversarial attacks. To this end, we take temporal motion into consideration when generating lightweight perturbations over the estimated tracking results frame-by-frame. On one hand, we add the temporal perturbations into the original video sequences as adversarial examples to greatly degrade the tracking performance. On the other hand, we sequentially estimate the perturbations from input sequences and learn to eliminate their effect for performance restoration. We apply the proposed adversarial attack and defense approaches to state-of-the-art deep tracking algorithms. Extensive evaluations on the benchmark datasets demonstrate that our defense method not only eliminates the large performance drops caused by adversarial attacks, but also achieves additional performance gains when deep trackers are not under adversarial attacks. The source code is available at https://github.com/joshuajss/RTAA.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_5');
INSERT INTO `paper` VALUES (12657, 'RobustFusion: Human Volumetric Capture with Data-Driven Visual Cues Using a RGBD Camera', 'Dynamic reconstruction', 'Volumetric capture', 'Robust', 'RGBD camera', '', 'High-quality and complete 4D reconstruction of human activities is critical for immersive VR/AR experience, but it suffers from inherent self-scanning constraint and consequent fragile tracking under the monocular setting. In this paper, inspired by the huge potential of learning-based human modeling, we propose RobustFusion, a robust human performance capture system combined with various data-driven visual cues using a single RGBD camera. To break the orchestrated self-scanning constraint, we propose a data-driven model completion scheme to generate a complete and fine-detailed initial model using only the front-view input. To enable robust tracking, we embrace both the initial model and the various visual cues into a novel performance capture scheme with hybrid motion optimization and semantic volumetric fusion, which can successfully capture challenging human motions under the monocular setting without pre-scanned detailed template and owns the reinitialization ability to recover from tracking failures and the disappear-reoccur scenarios. Extensive experiments demonstrate the robustness of our approach to achieve high-quality 4D reconstruction for challenging human motions, liberating the cumbersome self-scanning constraint.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_15');
INSERT INTO `paper` VALUES (12658, 'RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition', '', '', '', '', '', 'The attention-based encoder-decoder framework has recently achieved impressive results for scene text recognition, and many variants have emerged with improvements in recognition quality. However, it performs poorly on contextless texts (e.g., random character sequences) which is unacceptable in most of real application scenarios. In this paper, we first deeply investigate the decoding process of the decoder. We empirically find that a representative character-level sequence decoder utilizes not only context information but also positional information. Contextual information, which the existing approaches heavily rely on, causes the problem of attention drift. To suppress such side-effect, we propose a novel position enhancement branch, and dynamically fuse its outputs with those of the decoder attention module for scene text recognition. Specifically, it contains a position aware module to enable the encoder to output feature vectors encoding their own spatial positions, and an attention module to estimate glimpses using the positional clue (i.e., the current decoding time step) only. The dynamic fusion is conducted for more robust feature via an element-wise gate mechanism. Theoretically, our proposed method, dubbed RobustScanner, decodes individual characters with dynamic ratio between context and positional clues, and utilizes more positional ones when the decoding sequences with scarce context, and thus is robust and practical. Empirically, it has achieved new state-of-the-art results on popular regular and irregular text recognition benchmarks while without much performance drop on contextless benchmarks, validating its robustness in both contextual and contextless application scenarios.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_9');
INSERT INTO `paper` VALUES (12659, 'Rotation-Robust Intersection over Union for 3D Object Detection', '3D object detection', 'Loss function', 'Rotation-robust', '', '', 'In this paper, we propose a Rotation-robust Intersection over Union (\\(\\textit{RIoU}\\)) for 3D object detection, which aims to learn the overlap of rotated bounding boxes. In most existing 3D object detection methods, the norm-based loss is adopted to individually regress the parameters of bounding boxes, which may suffer from the loss-metric mismatch due to the scaling problem. Motivated by the IoU loss in the axis-aligned 2D object detection which is invariant to the scale, our method jointly optimizes the parameters via the \\(\\textit{RIoU}\\) loss. To tackle the uncertainty of convex caused by rotation, a projection operation is defined to estimate the intersection area. The calculation process of \\(\\textit{RIoU}\\) and its loss function is robust to the rotation condition and feasible for back-propagation, which only comprises basic numerical operations. By incorporating the \\(\\textit{RIoU}\\) loss with the conventional norm-based loss function, we enforce the network to directly optimize the \\(\\textit{RIoU}\\). Experimental results on the KITTI, nuScenes and SUN RGB-D datasets validate the effectiveness of our proposed method. Moreover, we show that our method is suitable for the detection task of 2D rotated objects, such as text boxes and cluttered targets in the aerial images.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_28');
INSERT INTO `paper` VALUES (12660, 'Rotational Outlier Identification in Pose Graphs using Dual Decomposition', 'Pose averaging', 'Outliers', 'Inference in graphical models', '', '', 'In the last few years, there has been an increasing trend to consider Structure from Motion (SfM, in computer vision) and Simultaneous Localization and Mapping (SLAM, in robotics) problems from the point of view of pose averaging (also known as global SfM, in computer vision) or Pose Graph Optimization (PGO, in robotics), where the motion of the camera is reconstructed by considering only relative rigid body transformations instead of including also 3-D points (as done in a full Bundle Adjustment). At a high level, the advantage of this approach is that modern solvers can effectively avoid most of the problems of local minima, and that it is easier to reason about outlier poses (caused by feature mismatches and repetitive structures in the images). In this paper, we contribute to the state of the art of the latter, by proposing a method to detect incorrect orientation measurements prior to pose graph optimization by checking the geometric consistency of rotation measurements. The novel aspects of our method are the use of Expectation-Maximization to fine-tune the covariance of the noise in inlier measurements, and a new approximate graph inference procedure, of independent interest, that is specifically designed to take advantage of evidence on cycles with better performance than standard approaches (Belief Propagation). The paper includes simulation and experimental results that evaluate the performance of our outlier detection and cycle-based inference algorithms on synthetic and real-world data.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_24');
INSERT INTO `paper` VALUES (12661, 'Rotationally-Temporally Consistent Novel View Synthesis of Human Performance Video', 'Novel view video synthesis', 'Synthetic human dataset', '', '', '', 'Novel view video synthesis aims to synthesize novel viewpoints videos given input captures of a human performance taken from multiple reference viewpoints and over consecutive time steps. Despite great advances in model-free novel view synthesis, existing methods present three limitations when applied to complex and time-varying human performance. First, these methods (and related datasets) mainly consider simple and symmetric objects. Second, they do not enforce explicit consistency across generated views. Third, they focus on static and non-moving objects. The fine-grained details of a human subject can therefore suffer from inconsistencies when synthesized across different viewpoints or time steps. To tackle these challenges, we introduce a human-specific framework that employs a learned 3D-aware representation. Specifically, we first introduce a novel siamese network that employs a gating layer for better reconstruction of the latent volumetric representation and, consequently, final visual results. Moreover, features from consecutive time steps are shared inside the network to improve temporal consistency. Second, we introduce a novel loss to explicitly enforce consistency across generated views both in space and in time. Third, we present the Multi-View Human Action (MVHA) dataset, consisting of near 1200 synthetic human performance captured from 54 viewpoints. Experiments on the MVHA, Pose-Varying Human Model and ShapeNet datasets show that our method outperforms the state-of-the-art baselines both in view generation quality and spatio-temporal consistency.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_23');
INSERT INTO `paper` VALUES (12662, 'RPT: Learning Point Set Representation for Siamese Visual Tracking', 'Visual tracking', 'Point set representation', 'Mutil-level aggregation', '', '', 'While remarkable progress has been made in robust visual tracking, accurate target state estimation still remains a highly challenging problem. In this paper, we argue that this issue is closely related to the prevalent bounding box representation, which provides only a coarse spatial extent of object. Thus an efficient visual tracking framework is proposed to accurately estimate the target state with a finer representation as a set of representative points. The point set is trained to indicate the semantically and geometrically significant positions of target region, enabling more fine-grained localization and modeling of object appearance. We further propose a multi-level aggregation strategy to obtain detailed structure information by fusing hierarchical convolution layers. Extensive experiments on several challenging benchmarks including OTB2015, VOT2018, VOT2019 and GOT-10k demonstrate that our method achieves new state-of-the-art performance while running at over 20 FPS.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_43');
INSERT INTO `paper` VALUES (12663, 'RTM3D: Real-Time Monocular 3D Detection from Object Keypoints for Autonomous Driving', 'Real-time monocular 3D detection', 'Autonomous driving', 'Keypoint detection', '', '', 'In this work, we propose an efficient and accurate monocular 3D detection framework in single shot. Most successful 3D detectors take the projection constraint from the 3D bounding box to the 2D box as an important component. Four edges of a 2D box provide only four constraints and the performance deteriorates dramatically with the small error of the 2D detector. Different from these approaches, our method predicts the nine perspective keypoints of a 3D bounding box in image space, and then utilize the geometric relationship of 3D and 2D perspectives to recover the dimension, location, and orientation in 3D space. In this method, the properties of the object can be predicted stably even when the estimation of keypoints is very noisy, which enables us to obtain fast detection speed with a small architecture. Training our method only uses the 3D properties of the object without any extra annotations, category-specific 3D shape priors, or depth maps. Our method is the first real-time system (FPS > 24) for monocular image 3D detection while achieves state-of-the-art performance on the KITTI benchmark.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_38');
INSERT INTO `paper` VALUES (12664, 'RubiksNet: Learnable 3D-Shift for Efficient Video Action Recognition', 'Efficient action recognition', 'Spatiotemporal', 'Learnable shift', 'Budget-constrained', 'Video understanding', 'Video action recognition is a complex task dependent on modeling spatial and temporal context. Standard approaches rely on 2D or 3D convolutions to process such context, resulting in expensive operations with millions of parameters. Recent efficient architectures leverage a channel-wise shift-based primitive as a replacement for temporal convolutions, but remain bottlenecked by spatial convolution operations to maintain strong accuracy and a fixed-shift scheme. Naively extending such developments to a 3D setting is a difficult, intractable goal. To this end, we introduce RubiksNet, a new efficient architecture for video action recognition which is based on a proposed learnable 3D spatiotemporal shift operation instead. We analyze the suitability of our new primitive for video action recognition and explore several novel variations of our approach to enable stronger representational flexibility while maintaining an efficient design. We benchmark our approach on several standard video recognition datasets, and observe that our method achieves comparable or better accuracy than prior work on efficient video action recognition at a fraction of the performance cost, with 2.9–5.9\\(\\times \\) fewer parameters and 2.1–3.7\\(\\times \\) fewer FLOPs. We also perform a series of controlled ablation studies to verify our significant boost in the efficiency-accuracy tradeoff curve is rooted in the core contributions of our RubiksNet architecture.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_30');
INSERT INTO `paper` VALUES (12665, 'S2DNAS: Transforming Static CNN Model for Dynamic Inference via Neural Architecture Search', 'Dynamic inference', 'Neural architecture search', 'CNN', '', '', 'Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural networks (CNNs). In contrast to static methods (e.g., weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on “easy” samples while maintaining the overall model performance.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_11');
INSERT INTO `paper` VALUES (12666, 'S2DNet: Learning Image Features for Accurate Sparse-to-Dense Matching', 'Feature matching', 'Classification', 'Visual localization', '', '', 'Establishing robust and accurate correspondences is a fundamental backbone to many computer vision algorithms. While recent learning-based feature matching methods have shown promising results in providing robust correspondences under challenging conditions, they are often limited in terms of precision. In this paper, we introduce S2DNet, a novel feature matching pipeline, designed and trained to efficiently establish both robust and accurate correspondences. By leveraging a sparse-to-dense matching paradigm, we cast the correspondence learning problem as a supervised classification task to learn to output highly peaked correspondence maps. We show that S2DNet achieves state-of-the-art results on the HPatches benchmark, as well as on several long-term visual localization datasets.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_37');
INSERT INTO `paper` VALUES (12667, 'SA-AE for Any-to-Any Relighting', 'Any-to-any relighting', 'Lighting estimation', 'Deep learning', 'Autoencoder', 'Self-attention mechanism', 'In this paper, we present a novel automatic model Self-Attention AutoEncoder (SA-AE) for generating a relit image from a source image to match the illumination setting of a guide image, which is called any-to-any relighting. In order to reduce the difficulty of learning, we adopt an implicit scene representation learned by the encoder to render the relit image using the decoder. Based on the learned scene representation, a lighting estimation network is designed as a classification task to predict the illumination settings from the guide images. Also, a lighting-to-feature network is well designed to recover the corresponding implicit scene representation from the illumination settings, which is the inverse process of the lighting estimation network. In addition, a self-attention mechanism is introduced in the autoencoder to focus on the re-rendering of the relighting-related regions in the source images. Extensive experiments on the VIDIT dataset show that the proposed approach achieved the 1st place in terms of MPS and the 1st place in terms of SSIM in the AIM 2020 Any-to-any Relighting Challenge.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_32');
INSERT INTO `paper` VALUES (12668, 'SACA Net: Cybersickness Assessment of Individual Viewers for VR Content via Graph-Based Symptom Relation Embedding', 'Cybersickness assessment', 'Individual viewer', 'VR content', 'Physical symptom', 'Symptom relation', 'Recently, cybersickness assessment for VR content is required to deal with viewing safety issues. Assessing physical symptoms of individual viewers is challenging but important to provide detailed and personalized guides for viewing safety. In this paper, we propose a novel symptom-aware cybersickness assessment network (SACA Net) that quantifies physical symptom levels for assessing cybersickness of individual viewers. The SACA Net is designed to utilize the relational characteristics of symptoms for complementary effects among relevant symptoms. The proposed network consists of three main parts: a stimulus symptom context guider, a physiological symptom guider, and a symptom relation embedder. The stimulus symptom context guider and the physiological symptom guider extract symptom features from VR content and human physiology, respectively. The symptom relation embedder refines the stimulus-response symptom features to effectively predict cybersickness by embedding relational characteristics with graph formulation. For validation, we utilize two public 360-degree video datasets that contain cybersickness scores and physiological signals. Experimental results show that the proposed method is effective in predicting human cybersickness with physical symptoms. Further, latent relations among symptoms are interpretable by analyzing relational weights in the proposed network.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_11');
INSERT INTO `paper` VALUES (12669, 'Sat2Graph: Road Graph Extraction Through Graph-Tensor Encoding', '', '', '', '', '', 'Inferring road graphs from satellite imagery is a challenging computer vision task. Prior solutions fall into two categories: (1) pixel-wise segmentation-based approaches, which predict whether each pixel is on a road, and (2) graph-based approaches, which predict the road graph iteratively. We find that these two approaches have complementary strengths while suffering from their own inherent limitations.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_4');
INSERT INTO `paper` VALUES (12670, 'SCAN: Learning to Classify Images Without Labels', 'Unsupervised learning', 'Self-supervised learning', 'Image classification', 'Clustering', '', 'Can we automatically group images into semantically meaningful clusters when ground-truth annotations are absent? The task of unsupervised image classification remains an important, and open challenge in computer vision. Several recent approaches have tried to tackle this problem in an end-to-end fashion. In this paper, we deviate from recent works, and advocate a two-step approach where feature learning and clustering are decoupled. First, a self-supervised task from representation learning is employed to obtain semantically meaningful features. Second, we use the obtained features as a prior in a learnable clustering approach. In doing so, we remove the ability for cluster learning to depend on low-level features, which is present in current end-to-end learning approaches. Experimental evaluation shows that we outperform state-of-the-art methods by large margins, in particular \\(+26.6\\%\\) on CIFAR10, \\(+25.0\\%\\) on CIFAR100-20 and \\(+21.3\\%\\) on STL10 in terms of classification accuracy. Furthermore, our method is the first to perform well on a large-scale dataset for image classification. In particular, we obtain promising results on ImageNet, and outperform several semi-supervised learning methods in the low-data regime without the use of any ground-truth annotations. The code is available at www.github.com/wvangansbeke/Unsupervised-Classification.git.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_16');
INSERT INTO `paper` VALUES (12671, 'ScanRefer: 3D Object Localization in RGB-D Scans Using Natural Language', '', '', '', '', '', 'We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing \\(51,583\\) descriptions of \\(11,046\\) objects from \\(800\\) ScanNet [8] scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D (Code: https://daveredrum.github.io/ScanRefer/).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_13');
INSERT INTO `paper` VALUES (12672, 'Scene Text Image Super-Resolution in the Wild', 'Scene text recognition', 'Super-resolution', 'Dataset', 'Sequence', 'Boundary', 'Low-resolution text images are often seen in natural scenes such as documents captured by mobile phones. Recognizing low-resolution text images is challenging because they lose detailed content information, leading to poor recognition accuracy. An intuitive solution is to introduce super-resolution (SR) techniques as pre-processing. However, previous single image super-resolution (SISR) methods are trained on synthetic low-resolution images (e.g. Bicubic down-sampling), which is simple and not suitable for real low-resolution text recognition. To this end, we propose a real scene text SR dataset, termed TextZoom. It contains paired real low-resolution and high-resolution images which are captured by cameras with different focal length in the wild. It is more authentic and challenging than synthetic data, as shown in Fig. 1. We argue improving the recognition accuracy is the ultimate goal for Scene Text SR. In this purpose, a new Text Super-Resolution Network, termed TSRN, with three novel modules is developed. (1) A sequential residual block is proposed to extract the sequential information of the text images. (2) A boundary-aware loss is designed to sharpen the character boundaries. (3) A central alignment module is proposed to relieve the misalignment problem in TextZoom. Extensive experiments on TextZoom demonstrate that our TSRN largely improves the recognition accuracy by over 13% of CRNN, and by nearly 9.0% of ASTER and MORAN compared to synthetic SR data. Furthermore, our TSRN clearly outperforms 7 state-of-the-art SR methods in boosting the recognition accuracy of LR images in TextZoom. For example, it outperforms LapSRN by over 5% and 8% on the recognition accuracy of ASTER and CRNN. Our results suggest that low-resolution text recognition in the wild is far from being solved, thus more research effort is needed. The codes and models will be released at: github.com/JasonBoy1/TextZoom', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_38');
INSERT INTO `paper` VALUES (12673, 'SceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans', '', '', '', '', '', 'We present a novel approach to reconstructing lightweight, CAD-based representations of scanned 3D environments from commodity RGB-D sensors. Our key idea is to jointly optimize for both CAD model alignments as well as layout estimations of the scanned scene, explicitly modeling inter-relationships between objects-to-objects and objects-to-layout. Since object arrangement and scene layout are intrinsically coupled, we show that treating the problem jointly significantly helps to produce globally-consistent representations of a scene. Object CAD models are aligned to the scene by establishing dense correspondences between geometry, and we introduce a hierarchical layout prediction approach to estimate layout planes from corners and edges of the scene. To this end, we propose a message-passing graph neural network to model the inter-relationships between objects and layout, guiding generation of a globally object alignment in a scene. By considering the global scene layout, we achieve significantly improved CAD alignments compared to state-of-the-art methods, improving from 41.83% to 58.41% alignment accuracy on SUNCG and from 50.05% to 61.24% on ScanNet, respectively. The resulting CAD-based representations makes our method well-suited for applications in content creation such as augmented- or virtual reality.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_36');
INSERT INTO `paper` VALUES (12674, 'SceneSketcher: Fine-Grained Image Retrieval with Scene Sketches', 'Sketch', 'Image retrieval', 'Graph convolutional network', '', '', 'Sketch-based image retrieval (SBIR) has been a popular research topic in recent years. Existing works concentrate on mapping the visual information of sketches and images to a semantic space at the object level. In this paper, for the first time, we study the fine-grained scene-level SBIR problem which aims at retrieving scene images satisfying the user’s specific requirements via a freehand scene sketch. We propose a graph embedding based method to learn the similarity measurement between images and scene sketches, which models the multi-modal information, including the size and appearance of objects as well as their layout information, in an effective manner. To evaluate our approach, we collect a dataset based on SketchyCOCO and extend the dataset using Coco-stuff. Comprehensive experiments demonstrate the significant potential of the proposed approach on the application of fine-grained scene-level image retrieval.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_42');
INSERT INTO `paper` VALUES (12675, 'Score-Level Multi Cue Fusion for Sign Language Recognition', 'Sign language recognition', 'Turkish sign language (TID)', '3D convolutional neural networks', 'Score-level fusion', '', 'Sign Languages are expressed through hand and upper body gestures as well as facial expressions. Therefore, Sign Language Recognition (SLR) needs to focus on all such cues. Previous work uses hand-crafted mechanisms or network aggregation to extract the different cue features, to increase SLR performance. This is slow and involves complicated architectures. We propose a more straightforward approach that focuses on training separate cue models specializing on the dominant hand, hands, face, and upper body regions. We compare the performance of 3D Convolutional Neural Network (CNN) models specializing in these regions, combine them through score-level fusion, and use the weighted alternative. Our experimental results have shown the effectiveness of mixed convolutional models. Their fusion yields up to \\(19\\%\\) accuracy improvement over the baseline using the full upper body. Furthermore, we include a discussion for fusion settings, which can help future work on Sign Language Translation (SLT).', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_21');
INSERT INTO `paper` VALUES (12676, 'ScribbleBox: Interactive Annotation Framework for Video Object Segmentation', '', '', '', '', '', 'Manually labeling video datasets for segmentation tasks is extremely time consuming. We introduce ScribbleBox, an interactive framework for annotating object instances with masks in videos with a significant boost in efficiency. In particular, we split annotation into two steps: annotating objects with tracked boxes, and labeling masks inside these tracks. We introduce automation and interaction in both steps. Box tracks are annotated efficiently by approximating the trajectory using a parametric curve with a small number of control points which the annotator can interactively correct. Our approach tolerates a modest amount of noise in box placements, thus typically requiring only a few clicks to annotate a track to a sufficient accuracy. Segmentation masks are corrected via scribbles which are propagated through time. We show significant performance gains in annotation efficiency over past work. We show that our ScribbleBox approach reaches 88.92% J&F on DAVIS2017 with an average of 9.14 clicks per box track, and only 4 frames requiring scribble annotation in a video of 65.3 frames on average.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_18');
INSERT INTO `paper` VALUES (12677, 'Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization', 'Mixed precision quantization', 'NAS', 'Optimization problem with constraint', 'Soft barrier penalty', '', 'Emergent hardwares can support mixed precision CNN models inference that assign different bitwidths for different layers. Learning to find an optimal mixed precision model that can preserve accuracy and satisfy the specific constraints on model size and computation is extremely challenge due to the difficult in training a mixed precision model and the huge space of all possible bit quantizations.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_1');
INSERT INTO `paper` VALUES (12678, 'Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution', '', '', '', '', '', 'Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1\\(^\\mathbf{st}\\) on the competitive SemanticKITTI leaderboard\\(^\\star \\). It also achieves 8–23\\(\\times \\) computation reduction and 3\\(\\times \\) measured speedup over MinkowskiNet and KPConv with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_41');
INSERT INTO `paper` VALUES (12679, 'Seeing the Un-Scene: Learning Amodal Semantic Maps for Room Navigation', 'Embodied AI', 'Room navigation', '', '', '', 'We introduce a learning-based approach for room navigation using semantic maps. Our proposed architecture learns to predict top-down belief maps of regions that lie beyond the agent’s field of view while modeling architectural and stylistic regularities in houses. First, we train a model to generate amodal semantic top-down maps indicating beliefs of location, size, and shape of rooms by learning the underlying architectural patterns in houses. Next, we use these maps to predict a point that lies in the target room and train a policy to navigate to the point. We empirically demonstrate that by predicting semantic maps, the model learns common correlations found in houses and generalizes to novel environments. We also demonstrate that reducing the task of room navigation to point navigation improves the performance further.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_30');
INSERT INTO `paper` VALUES (12680, 'SegBlocks: Towards Block-Based Adaptive Resolution Networks for Fast Segmentation', '', '', '', '', '', 'We propose a method to reduce the computational cost and memory consumption of existing neural networks, by exploiting spatial redundancies in images. Our method dynamically splits the image into blocks and processes low-complexity regions at a lower resolution. Our novel BlockPad module, implemented in CUDA, replaces zero-padding in order to prevent the discontinuities at patch borders of which existing methods suffer, while keeping memory consumption under control. We demonstrate SegBlocks on Cityscapes semantic segmentation, where the number of floating point operations is reduced by 30% with only 0.2% loss in accuracy (mIoU), and an inference speedup of 50% is achieved with 0.7% decrease in mIoU.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_2');
INSERT INTO `paper` VALUES (12681, 'SegFix: Model-Agnostic Boundary Refinement for Segmentation', 'Semantic segmentation', 'Instance segmentation', 'Boundary refinement', 'Model agnostic', '', 'We present a model-agnostic post-processing scheme to improve the boundary quality for the segmentation result that is generated by any existing segmentation model. Motivated by the empirical observation that the label predictions of interior pixels are more reliable, we propose to replace the originally unreliable predictions of boundary pixels by the predictions of interior pixels. Our approach processes only the input image through two steps: (i) localize the boundary pixels and (ii) identify the corresponding interior pixel for each boundary pixel. We build the correspondence by learning a direction away from the boundary pixel to an interior pixel. Our method requires no prior information of the segmentation models and achieves nearly real-time speed. We empirically verify that our SegFix consistently reduces the boundary errors for segmentation results generated from various state-of-the-art models on Cityscapes, ADE20K and GTA5. Code is available at: https://github.com/openseg-group/openseg.pytorch.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_29');
INSERT INTO `paper` VALUES (12682, 'Segment as Points for Efficient Online Multi-Object Tracking and Segmentation', 'Motion and tracking', 'Tracking', 'Vision for robotics', '', '', 'Current multi-object tracking and segmentation (MOTS) methods follow the tracking-by-detection paradigm and adopt convolutions for feature extraction. However, as affected by the inherent receptive field, convolution based feature extraction inevitably mixes up the foreground features and the background features, resulting in ambiguities in the subsequent instance association. In this paper, we propose a highly effective method for learning instance embeddings based on segments by converting the compact image representation to un-ordered 2D point cloud representation. Our method generates a new tracking-by-points paradigm where discriminative instance embeddings are learned from randomly selected points rather than images. Furthermore, multiple informative data modalities are converted into point-wise representations to enrich point-wise features. The resulting online MOTS framework, named PointTrack, surpasses all the state-of-the-art methods including 3D tracking methods by large margins (5.4% higher MOTSA and 18 times faster over MOTSFusion) with the near real-time speed (22 FPS). Evaluations across three datasets demonstrate both the effectiveness and efficiency of our method. Moreover, based on the observation that current MOTS datasets lack crowded scenes, we build a more challenging MOTS dataset named APOLLO MOTS with higher instance density. Both APOLLO MOTS and our codes are publicly available at https://github.com/detectRecog/PointTrack.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_16');
INSERT INTO `paper` VALUES (12683, 'Segmentations-Leak: Membership Inference Attacks and Defenses in Semantic Image Segmentation', 'Membership inference', 'Data privacy and security', 'Forensics', 'Semantic segmentation', '', 'Today’s success of state of the art methods for semantic segmentation is driven by large datasets. Data is considered an important asset that needs to be protected, as the collection and annotation of such datasets comes at significant efforts and associated costs. In addition, visual data might contain private or sensitive information, that makes it equally unsuited for public release. Unfortunately, recent work on membership inference in the broader area of adversarial machine learning and inference attacks on machine learning models has shown that even black box classifiers leak information on the dataset that they were trained on. We show that such membership inference attacks can be successfully carried out on complex, state of the art models for semantic segmentation. In order to mitigate the associated risks, we also study a series of defenses against such membership inference attacks and find effective counter measures against the existing risks with little effect on the utility of the segmentation method. Finally, we extensively evaluate our attacks and defenses on a range of relevant real-world datasets: Cityscapes, BDD100K, and Mapillary Vistas. Our source code and demos are available at https://github.com/SSAW14/segmentation_membership_inference.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_31');
INSERT INTO `paper` VALUES (12684, 'Segmenting Transparent Objects in the Wild', 'Transparent objects', 'Dataset', 'Benchmark', 'Image segmentation', 'Object boundary', 'Transparent objects such as windows and bottles made by glass widely exist in the real world. Segmenting transparent objects is challenging because these objects have diverse appearance inherited from the image background, making them had similar appearance with their surroundings. Besides the technical difficulty of this task, only a few previous datasets were specially designed and collected to explore this task and most of the existing datasets have major drawbacks. They either possess limited sample size such as merely a thousand of images without manual annotations, or they generate all images by using computer graphics method (i.e. not real image). To address this important problem, this work proposes a large-scale dataset for transparent object segmentation, named Trans10 K, consisting of 10,428 images of real scenarios with carefully manual annotations, which are 10 times larger than the existing datasets. The transparent objects in Trans10 K are extremely challenging due to high diversity in scale, viewpoint and occlusion. To evaluate the effectiveness of Trans10 K, we propose a novel boundary-aware segmentation method, termed TransLab, which exploits boundary as the clue to improve segmentation of transparent objects. Extensive experiments and ablation studies demonstrate the effectiveness of Trans10 K and validate the practicality of learning object boundary in TransLab. For example, TransLab significantly outperforms 20 recent object segmentation methods based on deep learning, showing that this task is largely unsolved. We believe that both Trans10 K and TransLab have important contributions to both the academia and industry, facilitating future researches and applications. The codes and models will be released at: https://github.com/xieenze/Segment_Transparent_Objects.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_41');
INSERT INTO `paper` VALUES (12685, 'Selecting Relevant Features from a Multi-domain Representation for Few-Shot Classification', 'Image recognition', 'Few-shot learning', 'Feature selection', '', '', 'Popular approaches for few-shot classification consist of first learning a generic data representation based on a large annotated dataset, before adapting the representation to new classes given only a few labeled samples. In this work, we propose a new strategy based on feature selection, which is both simpler and more effective than previous feature adaptation approaches. First, we obtain a multi-domain representation by training a set of semantically different feature extractors. Then, given a few-shot learning task, we use our multi-domain feature bank to automatically select the most relevant representations. We show that a simple non-parametric classifier built on top of such features produces high accuracy and generalizes to domains never seen during training, leading to state-of-the-art results on MetaDataset and improved accuracy on mini-ImageNet.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_45');
INSERT INTO `paper` VALUES (12686, 'Self-adapting Confidence Estimation for Stereo', 'Stereo matching', 'Confidence', 'Online adaptation', '', '', 'Estimating the confidence of disparity maps inferred by a stereo algorithm has become a very relevant task in the years, due to the increasing number of applications leveraging such cue. Although self-supervised learning has recently spread across many computer vision tasks, it has been barely considered in the field of confidence estimation. In this paper, we propose a flexible and lightweight solution enabling self-adapting confidence estimation agnostic to the stereo algorithm or network. Our approach relies on the minimum information available in any stereo setup (i.e., the input stereo pair and the output disparity map) to learn an effective confidence measure. This strategy allows us not only a seamless integration with any stereo system, including consumer and industrial devices equipped with undisclosed stereo perception methods, but also, due to its self-adapting capability, for its out-of-the-box deployment in the field. Exhaustive experimental results with different standard datasets support our claims, showing how our solution is the first-ever enabling online learning of accurate confidence estimation for any stereo system and without any requirement for the end-user.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_42');
INSERT INTO `paper` VALUES (12687, 'Self-calibrated Attention Neural Network for Real-World Super Resolution', 'Real-world image super-resolution', 'Receptive field', 'Self-calibrated convolutions', 'Data augmentation', '', 'Single Image Super-Resolution in practical scenarios is quite challenging, because of more complex degradation than bicubic downsampling and diverse degradation differences among devices. To solve this problem, we develop a novel super resolution network with large receptive field called SCA-SR. The contributions mainly contain the following four points. First, we introduce self-calibrated convolutions to low-level vision task for the first time to significantly enlarge the receptive field of SR model. Second, Cutblur methods are used to improve the generalization of model. Third, long skip connection was used in model design to improve the convergence of deep model structure. Fourth, we use both self-ensemble and model-ensemble to improve the robustness of model and reduce the noise introduced by individual model. According to the preliminary results of AIM 2020 Real Image Super-Resolution Challenge, our solution ranks third in both \\(\\times \\)2 and \\(\\times \\)3 tracks.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_27');
INSERT INTO `paper` VALUES (12688, 'Self-challenging Improves Cross-Domain Generalization', 'Cross-domain generalization', 'Robustness', '', '', '', 'Convolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, leading to decent test performance. The performance is nonetheless unmet when tested with different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the out-of-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlate with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of the new domain and without learning extra network parameters. We present the theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective, and architecture-agnostic nature of our RSC method.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_8');
INSERT INTO `paper` VALUES (12689, 'Self-Paced Deep Regression Forests with Consideration on Underrepresented Examples', 'Underrepresented examples', 'Self-paced learning', 'Entropy', 'Deep regression forests', '', 'Deep discriminative models (e.g. deep regression forests, deep neural decision forests) have achieved remarkable success recently to solve problems such as facial age estimation and head pose estimation. Most existing methods pursue robust and unbiased solutions either through learning discriminative features, or reweighting samples. We argue what is more desirable is learning gradually to discriminate like our human beings, and hence we resort to self-paced learning (SPL). Then, a natural question arises: can self-paced regime lead deep discriminative models to achieve more robust and less biased solutions? To this end, this paper proposes a new deep discriminative model—self-paced deep regression forests with consideration on underrepresented examples (SPUDRFs). It tackles the fundamental ranking and selecting problem in SPL from a new perspective: fairness. This paradigm is fundamental and could be easily combined with a variety of deep discriminative models (DDMs). Extensive experiments on two computer vision tasks, i.e., facial age estimation and head pose estimation, demonstrate the efficacy of SPUDRFs, where state-of-the-art performances are achieved.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_17');
INSERT INTO `paper` VALUES (12690, 'Self-Prediction for Joint Instance and Semantic Segmentation of Point Clouds', 'Self-Prediction', 'Instance segmentation', 'Semantic segmentation', 'Point cloud', 'State-of-the-art', 'We develop a novel learning scheme named Self-Prediction for 3D instance and semantic segmentation of point clouds. Distinct from most existing methods that focus on designing convolutional operators, our method designs a new learning scheme to enhance point relation exploring for better segmentation. More specifically, we divide a point cloud sample into two subsets and construct a complete graph based on their representations. Then we use label propagation algorithm to predict labels of one subset when given labels of the other subset. By training with this Self-Prediction task, the backbone network is constrained to fully explore relational context/geometric/shape information and learn more discriminative features for segmentation. Moreover, a general associated framework equipped with our Self-Prediction scheme is designed for enhancing instance and semantic segmentation simultaneously, where instance and semantic representations are combined to perform Self-Prediction. Through this way, instance and semantic segmentation are collaborated and mutually reinforced. Significant performance improvements on instance and semantic segmentation compared with baseline are achieved on S3DIS and ShapeNet. Our method achieves state-of-the-art instance segmentation results on S3DIS and comparable semantic segmentation results compared with state-of-the-arts on S3DIS and ShapeNet when we only take PointNet++ as the backbone network.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_12');
INSERT INTO `paper` VALUES (12691, 'Self-similarity Student for Partial Label Histopathology Image Segmentation', 'Whole slide image', 'Histopathology', 'Noisy label', '', '', 'Delineation of cancerous regions in gigapixel whole slide images (WSIs) is a crucial diagnostic procedure in digital pathology. This process is time-consuming because of the large search space in the gigapixel WSIs, causing chances of omission and misinterpretation at indistinct tumor lesions. To tackle this, the development of an automated cancerous region segmentation method is imperative. We frame this issue as a modeling problem with partial label WSIs, where some cancerous regions may be misclassified as benign and vice versa, producing patches with noisy labels. To learn from these patches, we propose Self-similarity Student, combining teacher-student model paradigm with similarity learning. Specifically, for each patch, we first sample its similar and dissimilar patches according to spatial distance. A teacher-student model is then introduced, featuring the exponential moving average on both student model weights and teacher predictions ensemble. While our student model takes patches, teacher model takes all their corresponding similar and dissimilar patches for learning robust representation against noisy label patches. Following this similarity learning, our similarity ensemble merges similar patches’ ensembled predictions as the pseudo-label of a given patch to counteract its noisy label. On the CAMELYON16 dataset, our method substantially outperforms state-of-the-art noise-aware learning methods by 5% and the supervised-trained baseline by 10% in various degrees of noise. Moreover, our method is superior to the baseline on our TVGH TURP dataset with 2% improvement, demonstrating the generalizability to more clinical histopathology segmentation tasks.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_8');
INSERT INTO `paper` VALUES (12692, 'Self-supervised Attribute-Aware Refinement Network for Low-Quality Text Recognition', 'Scene text recognition', 'Self-supervised learning', 'Mutual information', '', '', 'Scene texts collected from unconstrained environments encompass various types of degradation, including low-resolution, cluttered backgrounds, and irregular shapes. Training a model for text recognition with such types of degradations is notoriously hard. In this work, we analyze this problem in terms of two attributes: semantic and a geometric attribute, which are crucial cues for describing low-quality text. To handle this issue, we propose a new Self-supervised Attribute-Aware Refinement Network (SAAR-Net) that addresses these attributes simultaneously. Specifically, a novel text refining mechanism is combined with self-supervised learning for multiple auxiliary tasks to solve this problem. In addition, it can extract semantic and geometric attributes important to text recognition by introducing mutual information constraint that explicitly preserves invariant and discriminative information across different tasks. Such learned representation encourages our method to evidently generate a clear image, thus leading to better recognition performance. Extensive results demonstrate the effectiveness in refinement and recognition simultaneously.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_17');
INSERT INTO `paper` VALUES (12693, 'Self-supervised Bayesian Deep Learning for Image Recovery with Applications to Compressive Sensing', 'Self-supervised learning', 'Bayesian neural network', 'Compressive sensing', 'Image recovery', '', 'In recent years, deep learning emerges as one promising technique for solving many ill-posed inverse problems in image recovery, and most deep-learning-based solutions are based on supervised learning. Motivated by the practical value of reducing the cost and complexity of constructing labeled training datasets, this paper proposed a self-supervised deep learning approach for image recovery, which is dataset-free. Built upon Bayesian deep network, the proposed method trains a network with random weights that predicts the target image for recovery with uncertainty. Such uncertainty enables the prediction of the target image with small mean squared error by averaging multiple predictions. The proposed method is applied for image reconstruction in compressive sensing (CS), i.e., reconstructing an image from few measurements. The experiments showed that the proposed dataset-free deep learning method not only significantly outperforms traditional non-learning methods, but also is very competitive to the state-of-the-art supervised deep learning methods, especially when the measurements are few and noisy.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_28');
INSERT INTO `paper` VALUES (12694, 'Self-Supervised CycleGAN for Object-Preserving Image-to-Image Domain Adaptation', 'Image-to-image translation', 'Domain adaptation', 'Semantic segmentation', '', '', 'Recent generative adversarial network (GAN) based methods (e.g., CycleGAN) are prone to fail at preserving image-objects in image-to-image translation, which reduces their practicality on tasks such as domain adaptation. Some frameworks have been proposed to adopt a segmentation network as the auxiliary regularization to prevent the content distortion. However, all of them require extra pixel-wise annotations, which is difficult to fulfill in practical applications. In this paper, we propose a novel GAN (namely OP-GAN) to address the problem, which involves a self-supervised module to enforce the image content consistency during image-to-image translations without any extra annotations. We evaluate the proposed OP-GAN on three publicly available datasets. The experimental results demonstrate that our OP-GAN can yield visually plausible translated images and significantly improve the semantic segmentation accuracy in different domain adaptation scenarios with off-the-shelf deep learning networks such as PSPNet and U-Net.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_30');
INSERT INTO `paper` VALUES (12695, 'Self-supervised Keypoint Correspondences for Multi-person Pose Estimation and Tracking in Videos', '', '', '', '', '', 'Video annotation is expensive and time consuming. Consequently, datasets for multi-person pose estimation and tracking are less diverse and have more sparse annotations compared to large scale image datasets for human pose estimation. This makes it challenging to learn deep learning based models for associating keypoints across frames that are robust to nuisance factors such as motion blur and occlusions for the task of multi-person pose tracking. To address this issue, we propose an approach that relies on keypoint correspondences for associating persons in videos. Instead of training the network for estimating keypoint correspondences on video data, it is trained on a large scale image dataset for human pose estimation using self-supervision. Combined with a top-down framework for human pose estimation, we use keypoint correspondences to (i) recover missed pose detections and to (ii) associate pose detections across video frames. Our approach achieves state-of-the-art results for multi-frame pose estimation and multi-person pose tracking on the PoseTrack 2017 and 2018 datasets .', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_3');
INSERT INTO `paper` VALUES (12696, 'Self-supervised Learning of Audio-Visual Objects from Video', '', '', '', '', '', 'Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets. Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_13');
INSERT INTO `paper` VALUES (12697, 'Self-Supervised Monocular 3D Face Reconstruction by Occlusion-Aware Multi-view Geometry Consistency', '3D face reconstruction', 'Multi-view geometry consistency', '', '', '', 'Recent learning-based approaches, in which models are trained by single-view images have shown promising results for monocular 3D face reconstruction, but they suffer from the ill-posed face pose and depth ambiguity issue. In contrast to previous works that only enforce 2D feature constraints, we propose a self-supervised training architecture by leveraging the multi-view geometry consistency, which provides reliable constraints on face pose and depth estimation. We first propose an occlusion-aware view synthesis method to apply multi-view geometry consistency to self-supervised learning. Then we design three novel loss functions for multi-view consistency, including the pixel consistency loss, the depth consistency loss, and the facial landmark-based epipolar loss. Our method is accurate and robust, especially under large variations of expressions, poses, and illumination conditions. Comprehensive experiments on the face alignment and 3D face reconstruction benchmarks have demonstrated superiority over state-of-the-art methods. Our code and model are released in https://github.com/jiaxiangshang/MGCNet.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_4');
INSERT INTO `paper` VALUES (12698, 'Self-supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by Semantic Guidance', '', '', '', '', '', 'Self-supervised monocular depth estimation presents a powerful method to obtain 3D scene information from single camera images, which is trainable on arbitrary image sequences without requiring depth labels, e.g., from a LiDAR sensor. In this work we present a new self-supervised semantically-guided depth estimation (SGDepth) method to deal with moving dynamic-class (DC) objects, such as moving cars and pedestrians, which violate the static-world assumptions typically made during training of such models. Specifically, we propose (i) mutually beneficial cross-domain training of (supervised) semantic segmentation and self-supervised depth estimation with task-specific network heads, (ii) a semantic masking scheme providing guidance to prevent moving DC objects from contaminating the photometric loss, and (iii) a detection method for frames with non-moving DC objects, from which the depth of DC objects can be learned. We demonstrate the performance of our method on several benchmarks, in particular on the Eigen split, where we exceed all baselines without test-time refinement.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_35');
INSERT INTO `paper` VALUES (12699, 'Self-supervised Motion Representation via Scattering Local Motion Cues', 'Motion representation', 'Self-supervised learning', 'Action recognition', '', '', 'Motion representation is key to many computer vision problems but has never been well studied in the literature. Existing works usually rely on the optical flow estimation to assist other tasks such as action recognition, frame prediction, video segmentation, etc. In this paper, we leverage the massive unlabeled video data to learn an accurate explicit motion representation that aligns well with the semantic distribution of the moving objects. Our method subsumes a coarse-to-fine paradigm, which first decodes the low-resolution motion maps from the rich spatial-temporal features of the video, then adaptively upsamples the low-resolution maps to the full-resolution by considering the semantic cues. To achieve this, we propose a novel context guided motion upsampling layer that leverages the spatial context of video objects to learn the upsampling parameters in an efficient way. We prove the effectiveness of our proposed motion representation method on downstream video understanding tasks, e.g., action recognition task. Experimental results show that our method performs favorably against state-of-the-art methods.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_5');
INSERT INTO `paper` VALUES (12700, 'Self-supervised Multi-task Procedure Learning from Instructional Videos', 'Procedure learning', 'Instructional videos', 'Subset selection', 'Self-supervised learning', 'Deep Neural Networks', 'We address the problem of unsupervised procedure learning from instructional videos of multiple tasks using Deep Neural Networks (DNNs). Unlike existing works, we assume that training videos come from multiple tasks without key-step annotations or grammars, and the goals are to classify a test video to the underlying task and to localize its key-steps. Our DNN learns task-dependent attention features from informative regions of each frame without ground-truth bounding boxes and learns to discover and localize key-steps without key-step annotations by using an unsupervised subset selection module as a teacher. It also learns to classify an input video using the discovered key-steps using a learnable key-step feature pooling mechanism that extracts and learns to combine key-step based features for task recognition. By experiments on two instructional video datasets, we show the effectiveness of our method for unsupervised localization of procedure steps and video classification.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_33');
INSERT INTO `paper` VALUES (12701, 'Self-supervised Outdoor Scene Relighting', 'Neural rendering', 'Image relighting', 'Inverse rendering', '', '', 'Outdoor scene relighting is a challenging problem that requires good understanding of the scene geometry, illumination and albedo. Current techniques are completely supervised, requiring high quality synthetic renderings to train a solution. Such renderings are synthesized using priors learned from limited data. In contrast, we propose a self-supervised approach for relighting. Our approach is trained only on corpora of images collected from the internet without any user-supervision. This virtually endless source of training data allows training a general relighting solution. Our approach first decomposes an image into its albedo, geometry and illumination. A novel relighting is then produced by modifying the illumination parameters. Our solution capture shadow using a dedicated shadow prediction map, and does not rely on accurate geometry estimation. We evaluate our technique subjectively and objectively using a new dataset with ground-truth relighting. Results show the ability of our technique to produce photo-realistic and physically plausible results, that generalizes to unseen scenes.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_6');
INSERT INTO `paper` VALUES (12702, 'Self-supervised Single-View 3D Reconstruction via Semantic Consistency', '3D from single images', 'Unsupervised learning', '', '', '', 'We learn a self-supervised, single-view 3D reconstruction model that predicts the 3D mesh shape, texture and camera pose of a target object with a collection of 2D images and silhouettes. The proposed method does not necessitate 3D supervision, manually annotated keypoints, multi-view images of an object or a prior 3D template. The key insight of our work is that objects can be represented as a collection of deformable parts, and each part is semantically coherent across different instances of the same category (e.g., wings on birds and wheels on cars). Therefore, by leveraging part segmentation of a large collection of category-specific images learned via self-supervision, we can effectively enforce semantic consistency between the reconstructed meshes and the original images. This significantly reduces ambiguities during joint prediction of shape and camera pose of an object, along with texture. To the best of our knowledge, we are the first to try and solve the single-view reconstruction problem without a category-specific template mesh or semantic keypoints. Thus our model can easily generalize to various object categories without such labels, e.g., horses, penguins, etc. Through a variety of experiments on several categories of deformable and rigid objects, we demonstrate that our unsupervised method performs comparably if not better than existing category-specific reconstruction methods learned with supervision. More details can be found at the project page https://sites.google.com/nvidia.com/unsup-mesh-2020.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_40');
INSERT INTO `paper` VALUES (12703, 'Self-supervised Video Representation Learning by Pace Prediction', 'Self-supervised learning', 'Video representation', 'Pace', '', '', 'This paper addresses the problem of self-supervised video representation learning from a new perspective – by video pace prediction. It stems from the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making. Specifically, given a video played in natural pace, we randomly sample training clips in different paces and ask a neural network to identify the pace for each video clip. The assumption here is that the network can only succeed in such a pace reasoning task when it understands the underlying video content and learns representative spatio-temporal features. In addition, we further introduce contrastive learning to push the model towards discriminating different paces by maximizing the agreement on similar video content. To validate the effectiveness of the proposed method, we conduct extensive experiments on action recognition and video retrieval tasks with several alternative network architectures. Experimental evaluations show that our approach achieves state-of-the-art performance for self-supervised video representation learning across different network architectures and different benchmarks. The code and pre-trained models are available at https://github.com/laura-wang/video-pace.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_30');
INSERT INTO `paper` VALUES (12704, 'Self-supervising Fine-Grained Region Similarities for Large-Scale Image Localization', '', '', '', '', '', 'The task of large-scale retrieval-based image localization is to estimate the geographical location of a query image by recognizing its nearest reference images from a city-scale dataset. However, the general public benchmarks only provide noisy GPS labels associated with the training images, which act as weak supervisions for learning image-to-image similarities. Such label noise prevents deep neural networks from learning discriminative features for accurate localization. To tackle this challenge, we propose to self-supervise image-to-region similarities in order to fully explore the potential of difficult positive images alongside their sub-regions. The estimated image-to-region similarities can serve as extra training supervision for improving the network in generations, which could in turn gradually refine the fine-grained similarities to achieve optimal performance. Our proposed self-enhanced image-to-region similarity labels effectively deal with the training bottleneck in the state-of-the-art pipelines without any additional parameters or manual annotations in both training and inference. Our method outperforms state-of-the-arts on the standard localization benchmarks by noticeable margins and shows excellent generalization capability on multiple image retrieval datasets (Code of this work is available at https://github.com/yxgeee/SFRS.).', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_22');
INSERT INTO `paper` VALUES (12705, 'Self-Supervision for 3D Real-World Challenges', '', '', '', '', '', 'We consider several possible scenarios involving synthetic and real-world point clouds where supervised learning fails due to data scarcity and large domain gaps. We propose to enrich standard feature representations by leveraging self-supervision through a multi-task model that can solve a 3D puzzle while learning the main task of shape classification or part segmentation.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_48');
INSERT INTO `paper` VALUES (12706, 'Self-supervision with Superpixels: Training Few-Shot Medical Image Segmentation Without Annotation', '', '', '', '', '', 'Few-shot semantic segmentation (FSS) has great potential for medical imaging applications. Most of the existing FSS techniques require abundant annotated semantic classes for training. However, these methods may not be applicable for medical images due to the lack of annotations. To address this problem we make several contributions: (1) A novel self-supervised FSS framework for medical images in order to eliminate the requirement for annotations during training. Additionally, superpixel-based pseudo-labels are generated to provide supervision; (2) An adaptive local prototype pooling module plugged into prototypical networks, to solve the common challenging foreground-background imbalance problem in medical image segmentation; (3) We demonstrate the general applicability of the proposed approach for medical images using three different tasks: abdominal organ segmentation for CT and MRI, as well as cardiac segmentation for MRI. Our results show that, for medical image segmentation, the proposed method outperforms conventional FSS methods which require manual annotations for training.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_45');
INSERT INTO `paper` VALUES (12707, 'Self6D: Self-supervised Monocular 6D Object Pose Estimation', 'Self-supervised learning', '6D pose estimation', '', '', '', '6D object pose estimation is a fundamental problem in computer vision. Convolutional Neural Networks (CNNs) have recently proven to be capable of predicting reliable 6D pose estimates even from monocular images. Nonetheless, CNNs are identified as being extremely data-driven, and acquiring adequate annotations is oftentimes very time-consuming and labor intensive. To overcome this shortcoming, we propose the idea of monocular 6D pose estimation by means of self-supervised learning, removing the need for real annotations. After training our proposed network fully supervised with synthetic RGB data, we leverage recent advances in neural rendering to further self-supervise the model on unannotated real RGB-D data, seeking for a visually and geometrically optimal alignment. Extensive evaluations demonstrate that our proposed self-supervision is able to significantly enhance the model’s original performance, outperforming all other methods relying on synthetic data or employing elaborate techniques from the domain adaptation realm .', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_7');
INSERT INTO `paper` VALUES (12708, 'Semantic Curiosity for Active Visual Learning', 'Embodied learning', 'Active visual learning', 'Semantic curiosity', 'Exploration', '', 'In this paper, we study the task of embodied interactive learning for object detection. Given a set of environments (and some labeling budget), our goal is to learn an object detector by having an agent select what data to obtain labels for. How should an exploration policy decide which trajectory should be labeled? One possibility is to use a trained object detector’s failure cases as an external reward. However, this will require labeling millions of frames required for training RL policies, which is infeasible. Instead, we explore a self-supervised approach for training our exploration policy by introducing a notion of semantic curiosity. Our semantic curiosity policy is based on a simple observation – the detection outputs should be consistent. Therefore, our semantic curiosity rewards trajectories with inconsistent labeling behavior and encourages the exploration policy to explore such areas. The exploration policy trained via semantic curiosity generalizes to novel scenes and helps train an object detector that outperforms baselines trained with other possible alternatives such as random exploration, prediction-error curiosity, and coverage-maximizing exploration.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_19');
INSERT INTO `paper` VALUES (12709, 'Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering', 'VQA', 'Data augmentation', 'Adversarial learning', '', '', 'Visual Question Answering (VQA) has achieved great success thanks to the fast development of deep neural networks (DNN). On the other hand, the data augmentation, as one of the major tricks for DNN, has been widely used in many computer vision tasks. However, there are few works studying the data augmentation problem for VQA and none of the existing image based augmentation schemes (such as rotation and flipping) can be directly applied to VQA due to its semantic structure – an \\(\\langle image, question, answer\\rangle \\) triplet needs to be maintained correctly. For example, a direction related Question-Answer (QA) pair may not be true if the associated image is rotated or flipped. In this paper, instead of directly manipulating images and questions, we use generated adversarial examples for both images and questions as the augmented data. The augmented examples do not change the visual properties presented in the image as well as the semantic meaning of the question, the correctness of the \\(\\langle image, question, answer\\rangle \\) is thus still maintained. We then use adversarial learning to train a classic VQA model (BUTD) with our augmented data. We find that we not only improve the overall performance on VQAv2, but also can withstand adversarial attack effectively, compared to the baseline model. The source code is available at https://github.com/zaynmi/seada-vqa.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_26');
INSERT INTO `paper` VALUES (12710, 'Semantic Flow for Fast and Accurate Scene Parsing', 'Scene parsing', 'Semantic flow', 'Flow alignment module', '', '', 'In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used—atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at https://github.com/donnyyou/torchcv.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_45');
INSERT INTO `paper` VALUES (12711, 'Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching', 'Semantic lines', 'Line detection', 'Attention', 'Ranking', 'Matching', 'A novel algorithm to detect semantic lines is proposed in this paper. We develop three networks: detection network with mirror attention (D-Net) and comparative ranking and matching networks (R-Net and M-Net). D-Net extracts semantic lines by exploiting rich contextual information. To this end, we design the mirror attention module. Then, through pairwise comparisons of extracted semantic lines, we iteratively select the most semantic line and remove redundant ones overlapping with the selected one. For the pairwise comparisons, we develop R-Net and M-Net in the Siamese architecture. Experiments demonstrate that the proposed algorithm outperforms the conventional semantic line detector significantly. Moreover, we apply the proposed algorithm to detect two important kinds of semantic lines successfully: dominant parallel lines and reflection symmetry axes. Our codes are available at https://github.com/dongkwonjin/Semantic-Line-DRM.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_8');
INSERT INTO `paper` VALUES (12712, 'Semantic Object Prediction and Spatial Sound Super-Resolution with Binaural Sounds', '', '', '', '', '', 'Humans can robustly recognize and localize objects by integrating visual and auditory cues. While machines are able to do the same now with images, less work has been done with sounds. This work develops an approach for dense semantic labelling of sound-making objects, purely based on binaural sounds. We propose a novel sensor setup and record a new audio-visual dataset of street scenes with eight professional binaural microphones and a 360\\(^{\\circ }\\) camera. The co-existence of visual and audio cues is leveraged for supervision transfer. In particular, we employ a cross-modal distillation framework that consists of a vision ‘teacher’ method and a sound ‘student’ method – the student method is trained to generate the same results as the teacher method. This way, the auditory system can be trained without using human annotations. We also propose two auxiliary tasks namely, a) a novel task on Spatial Sound Super-resolution to increase the spatial resolution of sounds, and b) dense depth prediction of the scene. We then formulate the three tasks into one end-to-end trainable multi-tasking network aiming to boost the overall performance. Experimental results on the dataset show that 1) our method achieves good results for all the three tasks; and 2) the three tasks are mutually beneficial – training them together achieves the best performance and 3) the number and the orientations of microphones are both important. The data and code will be released on the project page.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_37');
INSERT INTO `paper` VALUES (12713, 'Semantic Relation Preserving Knowledge Distillation for Image-to-Image Translation', 'Knowledge distillation', 'Generative adversarial networks', 'Image-to-image translation', 'Model compression', '', 'Generative adversarial networks (GANs) have shown significant potential in modeling high dimensional distributions of image data, especially on image-to-image translation tasks. However, due to the complexity of these tasks, state-of-the-art models often contain a tremendous amount of parameters, which results in large model size and long inference time. In this work, we propose a novel method to address this problem by applying knowledge distillation together with distillation of a semantic relation preserving matrix. This matrix, derived from the teacher’s feature encoding, helps the student model learn better semantic relations. In contrast to existing compression methods designed for classification tasks, our proposed method adapts well to the image-to-image translation task on GANs. Experiments conducted on 5 different datasets and 3 different pairs of teacher and student models provide strong evidence that our methods achieve impressive results both qualitatively and quantitatively.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_39');
INSERT INTO `paper` VALUES (12714, 'Semantic View Synthesis', '', '', '', '', '', 'We tackle a new problem of semantic view synthesis—generating free-viewpoint rendering of a synthesized scene using a semantic label map as input. We build upon recent advances in semantic image synthesis and view synthesis for handling photographic image content generation and view extrapolation. Direct application of existing image/view synthesis methods, however, results in severe ghosting/blurry artifacts. To address the drawbacks, we propose a two-step approach. First, we focus on synthesizing the color and depth of the visible surface of the 3D scene. We then use the synthesized color and depth to impose explicit constraints on the multiple-plane image (MPI) representation prediction process. Our method produces sharp contents at the original view and geometrically consistent renderings across novel viewpoints. The experiments on numerous indoor and outdoor images show favorable results against several strong baselines and validate the effectiveness of our approach.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_35');
INSERT INTO `paper` VALUES (12715, 'SemanticAdv: Generating Adversarial Examples via Attribute-Conditioned Image Editing', '', '', '', '', '', 'Recent studies have shown that DNNs are vulnerable to adversarial examples which are manipulated instances targeting to mislead DNNs to make incorrect predictions. Currently, most such adversarial examples try to guarantee “subtle perturbation” by limiting the \\(L_p\\) norm of the perturbation. In this paper, we propose SemanticAdv to generate a new type of semantically realistic adversarial examples via attribute-conditioned image editing. Compared to existing methods, our SemanticAdv enables fine-grained analysis and evaluation of DNNs with input variations in the attribute space. We conduct comprehensive experiments to show that our adversarial examples not only exhibit semantically meaningful appearances but also achieve high targeted attack success rates under both whitebox and blackbox settings. Moreover, we show that the existing pixel-based and attribute-based defense methods fail to defend against SemanticAdv. We demonstrate the applicability of SemanticAdv on both face recognition and general street-view images to show its generalization. We believe that our work can shed light on further understanding about vulnerabilities of DNNs as well as novel defense approaches. Our implementation is available at https://github.com/AI-secure/SemanticAdv .', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_2');
INSERT INTO `paper` VALUES (12716, 'Semi-Siamese Training for Shallow Face Learning', 'Face recognition', 'Shallow face learning', '', '', '', 'Most existing public face datasets, such as MS-Celeb-1M and VGGFace2, provide abundant information in both breadth (large number of IDs) and depth (sufficient number of samples) for training. However, in many real-world scenarios of face recognition, the training dataset is limited in depth, i.e. only two face images are available for each ID. We define this situation as Shallow Face Learning, and find it problematic with existing training methods. Unlike deep face data, the shallow face data lacks intra-class diversity. As such, it can lead to collapse of feature dimension and consequently the learned network can easily suffer from degeneration and over-fitting in the collapsed dimension. In this paper, we aim to address the problem by introducing a novel training method named Semi-Siamese Training (SST). A pair of Semi-Siamese networks constitute the forward propagation structure, and the training loss is computed with an updating gallery queue, conducting effective optimization on shallow training data. Our method is developed without extra-dependency, thus can be flexibly integrated with the existing loss functions and network architectures. Extensive experiments on various benchmarks of face recognition show the proposed method significantly improves the training, not only in shallow face learning, but also for conventional deep face data.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_3');
INSERT INTO `paper` VALUES (12717, 'Semi-supervised Crowd Counting via Self-training on Surrogate Tasks', 'Crowd counting', 'Surrogate tasks', 'Self-training', 'Semi-supervised learning', '', 'Most existing crowd counting systems rely on the availability of the object location annotation which can be expensive to obtain. To reduce the annotation cost, one attractive solution is to leverage a large number of unlabeled images to build a crowd counting model in semi-supervised fashion. This paper tackles the semi-supervised crowd counting problem from the perspective of feature learning. Our key idea is to leverage the unlabeled images to train a generic feature extractor rather than the entire network of a crowd counter. The rationale of this design is that learning the feature extractor can be more reliable and robust towards the inevitable noisy supervision generated from the unlabeled data. Also, on top of a good feature extractor, it is possible to build a density map regressor with much fewer density map annotations. Specifically, we proposed a novel semi-supervised crowd counting method which is built upon two innovative components: (1) a set of inter-related binary segmentation tasks are derived from the original density map regression task as the surrogate prediction target; (2) the surrogate target predictors are learned from both labeled and unlabeled data by utilizing a proposed self-training scheme which fully exploits the underlying constraints of these binary segmentation tasks. Through experiments, we show that the proposed method is superior over the existing semi-supervised crowd counting method and other representative baselines.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_15');
INSERT INTO `paper` VALUES (12718, 'Semi-supervised Learning with a Teacher-Student Network for Generalized Attribute Prediction', 'Semi-supervised learning', 'Unlabeled data', 'Visual attributes', '', '', 'This paper presents a study on semi-supervised learning to solve the visual attribute prediction problem. In many applications of vision algorithms, the precise recognition of visual attributes of objects is important but still challenging. This is because defining a class hierarchy of attributes is ambiguous, so training data inevitably suffer from class imbalance and label sparsity, leading to a lack of effective annotations. An intuitive solution is to find a method to effectively learn image representations by utilizing unlabeled images. With that in mind, we propose a multi-teacher-single-student (MTSS) approach inspired by the multi-task learning and the distillation of semi-supervised learning. Our MTSS learns task-specific domain experts called teacher networks using the label embedding technique and learns a unified model called a student network by forcing a model to mimic the distributions learned by domain experts. Our experiments demonstrate that our method not only achieves competitive performance on various benchmarks for fashion attribute prediction, but also improves robustness and cross-domain adaptability for unseen domains.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_30');
INSERT INTO `paper` VALUES (12719, 'Semi-supervised Segmentation Based on Error-Correcting Supervision', '', '', '', '', '', 'Pixel-level classification is an essential part of computer vision. For learning from labeled data, many powerful deep learning models have been developed recently. In this work, we augment such supervised segmentation models by allowing them to learn from unlabeled data. Our semi-supervised approach, termed Error-Correcting Supervision, leverages a collaborative strategy. Apart from the supervised training on the labeled data, the segmentation network is judged by an additional network. The secondary correction network learns on the labeled data to optimally spot correct predictions, as well as to amend incorrect ones. As auxiliary regularization term, the corrector directly influences the supervised training of the segmentation network. On unlabeled data, the output of the correction network is essential to create a proxy for the unknown truth. The corrector’s output is combined with the segmentation network’s prediction to form the new target. We propose a loss function that incorporates both the pseudo-labels as well as the predictive certainty of the correction network. Our approach can easily be added to supervised segmentation models. We show consistent improvements over a supervised baseline on experiments on both the Pascal VOC 2012 and the Cityscapes datasets with varying amounts of labeled data.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_9');
INSERT INTO `paper` VALUES (12720, 'Semi-supervised Semantic Segmentation via Strong-Weak Dual-Branch Network', 'Semi-supervised', 'Strong-weak', 'Semantic segmentation', '', '', 'While existing works have explored a variety of techniques to push the envelop of weakly-supervised semantic segmentation, there is still a significant gap compared to the supervised methods. In real-world application, besides massive amount of weakly-supervised data there are usually a few available pixel-level annotations, based on which semi-supervised track becomes a promising way for semantic segmentation. Current methods simply bundle these two different sets of annotations together to train a segmentation network. However, we discover that such treatment is problematic and achieves even worse results than just using strong labels, which indicates the misuse of the weak ones. To fully explore the potential of the weak labels, we propose to impose separate treatments of strong and weak annotations via a strong-weak dual-branch network, which discriminates the massive inaccurate weak supervisions from those strong ones. We design a shared network component to exploit the joint discrimination of strong and weak annotations; meanwhile, the proposed dual branches separately handle full and weak supervised learning and effectively eliminate their mutual interference. This simple architecture requires only slight additional computational costs during training yet brings significant improvements over the previous methods. Experiments on two standard benchmark datasets show the effectiveness of the proposed method.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_46');
INSERT INTO `paper` VALUES (12721, 'Semi-supervised Viewpoint Estimation with Geometry-Aware Conditional Generation', '3D viewpoint estimation', 'Semi-supervised learning', 'Conditional image generation', '', '', 'There is a growing interest in developing computer vision methods that can learn from limited supervision. In this paper, we consider the problem of learning to predict camera viewpoints, where obtaining ground-truth annotations are expensive and require special equipment, from a limited number of labeled images. We propose a semi-supervised viewpoint estimation method that can learn to infer viewpoint information from unlabeled image pairs, where two images differ by a viewpoint change. In particular our method learns to synthesize the second image by combining the appearance from the first one and viewpoint from the second one. We demonstrate that our method significantly improves the supervised techniques, especially in the low-label regime and outperforms the state-of-the-art semi-supervised methods.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_42');
INSERT INTO `paper` VALUES (12722, 'SemifreddoNets: Partially Frozen Neural Networks for Efficient Computer Vision Systems', 'Efficient machine learning', 'Transfer learning', 'Multi-task learning', 'Neural network hardware', '', 'We propose a system comprised of fixed-topology neural networks having partially frozen weights, named SemifreddoNets. SemifreddoNets work as fully-pipelined hardware blocks that are optimized to have an efficient hardware implementation. Those blocks freeze a certain portion of the parameters at every layer and replace the corresponding multipliers with fixed scalers. Fixing the weights reduces the silicon area, logic delay, and memory requirements, leading to significant savings in cost and power consumption. Unlike traditional layer-wise freezing approaches, SemifreddoNets make a profitable trade between the cost and flexibility by having some of the weights configurable at different scales and levels of abstraction in the model. Although fixing the topology and some of the weights somewhat limits the flexibility, we argue that the efficiency benefits of this strategy outweigh the advantages of a fully configurable model for many use cases. Furthermore, our system uses repeatable blocks, therefore it has the flexibility to adjust model complexity without requiring any hardware change. The hardware implementation of SemifreddoNets provides up to an order of magnitude reduction in silicon area and power consumption as compared to their equivalent implementation on a general-purpose accelerator.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_12');
INSERT INTO `paper` VALUES (12723, 'SEN: A Novel Feature Normalization Dissimilarity Measure for Prototypical Few-Shot Learning Networks', '', '', '', '', '', 'In this paper, we equip Prototypical Networks (PNs) with a novel dissimilarity measure to enable discriminative feature normalization for few-shot learning. The embedding onto the hypersphere requires no direct normalization and is easy to optimize. Our theoretical analysis shows that the proposed dissimilarity measure, denoted the Squared root of the Euclidean distance and the Norm distance (SEN), forces embedding points to be attracted to its correct prototype, while being repelled from all other prototypes, keeping the norm of all points the same. The resulting SEN PN outperforms the regular PN with a considerable margin, with no additional parameters as well as with negligible computational overhead.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_8');
INSERT INTO `paper` VALUES (12724, 'Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation', '', '', '', '', '', 'Stereophonic audio is an indispensable ingredient to enhance human auditory experience. Recent research has explored the usage of visual information as guidance to generate binaural or ambisonic audio from mono ones with stereo supervision. However, this fully supervised paradigm suffers from an inherent drawback: the recording of stereophonic audio usually requires delicate devices that are expensive for wide accessibility. To overcome this challenge, we propose to leverage the vastly available mono data to facilitate the generation of stereophonic audio. Our key observation is that the task of visually indicated audio separation also maps independent audios to their corresponding visual positions, which shares a similar objective with stereophonic audio generation. We integrate both stereo generation and source separation into a unified framework, Sep-Stereo, by considering source separation as a particular type of audio spatialization. Specifically, a novel associative pyramid network architecture is carefully designed for audio-visual feature fusion. Extensive experiments demonstrate that our framework can improve the stereophonic audio generation results while performing accurate sound separation with a shared backbone (Code, models and demo video are available at https://hangz-nju-cuhk.github.io/projects/Sep-Stereo.).', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_4');
INSERT INTO `paper` VALUES (12725, 'SeqHAND: RGB-Sequence-Based 3D Hand Pose and Shape Estimation', '3D hand pose estimations', 'Pose-flow generation', 'Synthetic-to-real domain gap reduction', 'Synthetic hand motion dataset', '', '3D hand pose estimation based on RGB images has been studied for a long time. Most of the studies, however, have performed frame-by-frame estimation based on independent static images. In this paper, we attempt to not only consider the appearance of a hand but incorporate the temporal movement information of a hand in motion into the learning framework, which leads to the necessity of a large-scale dataset with sequential RGB hand images. We propose a novel method that generates a synthetic dataset that mimics natural human hand movements by re-engineering annotations of an extant static hand pose dataset into pose-flows. With the generated dataset, we train a newly proposed recurrent framework, exploiting visuo-temporal features from sequential synthetic hand images and emphasizing smoothness of estimations with temporal consistency constraints. Our novel training strategy of detaching the recurrent layer of the framework during domain finetuning from synthetic to real allows preservation of the visuo-temporal features learned from sequential synthetic hand images. Hand poses that are sequentially estimated consequently produce natural and smooth hand movements which lead to more robust estimations. Utilizing temporal information for 3D hand pose estimation significantly enhances general pose estimations by outperforming state-of-the-art methods in our experiments on hand pose estimation benchmarks.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_8');
INSERT INTO `paper` VALUES (12726, 'Sequential Convolution and Runge-Kutta Residual Architecture for Image Compressed Sensing', 'Compressed sensing', 'Convolutional sensing', 'Runge-Kutta methods', '', '', 'In recent years, Deep Neural Networks (DNN) have empowered Compressed Sensing (CS) substantially and have achieved high reconstruction quality and speed far exceeding traditional CS methods. However, there are still lots of issues to be further explored before it can be practical enough. There are mainly two challenging problems in CS, one is to achieve efficient data sampling, and the other is to reconstruct images with high-quality. To address the two challenges, this paper proposes a novel Runge-Kutta Convolutional Compressed Sensing Network (RK-CCSNet). In the sensing stage, RK-CCSNet applies Sequential Convolutional Module (SCM) to gradually compact measurements through a series of convolution filters. In the reconstruction stage, RK-CCSNet establishes a novel Learned Runge-Kutta Block (LRKB) based on the famous Runge-Kutta methods, reformulating the process of image reconstruction as a discrete dynamical system. Finally, the implementation of RK-CCSNet achieves state-of-the-art performance on influential benchmarks with respect to prestigious baselines, and all the codes are available at https://github.com/rkteddy/RK-CCSNet.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_14');
INSERT INTO `paper` VALUES (12727, 'Sequential Deformation for Accurate Scene Text Detection', 'Scene text detection', 'Deep neural network', 'Sequential deformation', '', '', 'Scene text detection has been significantly advanced over recent years, especially after the emergence of deep neural network. However, due to high diversity of scene texts in scale, orientation, shape and aspect ratio, as well as the inherent limitation of convolutional neural network for geometric transformations, to achieve accurate scene text detection is still an open problem. In this paper, we propose a novel sequential deformation method to effectively model the line-shape of scene text. An auxiliary character counting supervision is further introduced to guide the sequential offset prediction. The whole network can be easily optimized through an end-to-end multi-task manner. Extensive experiments are conducted on public scene text detection datasets including ICDAR 2017 MLT, ICDAR 2015, Total-text and SCUT-CTW1500. The experimental results demonstrate that the proposed method has outperformed previous state-of-the-art methods.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_7');
INSERT INTO `paper` VALUES (12728, 'Sequential Learning for Domain Generalization', 'Sequential learning', 'Meta-learning', 'Domain generalization', '', '', 'In this paper we propose a sequential learning framework for Domain Generalization (DG), the problem of training a model that is robust to domain shift by design. Various DG approaches have been proposed with different motivating intuitions, but they typically optimize for a single step of domain generalization – training on one set of domains and generalizing to one other. Our sequential learning is inspired by the idea lifelong learning, where accumulated experience means that learning the \\(n^{th}\\) thing becomes easier than the \\(1^{st}\\) thing. In DG this means encountering a sequence of domains and at each step training to maximise performance on the next domain. The performance at domain n then depends on the previous \\(n-1\\) learning problems. Thus backpropagating through the sequence means optimizing performance not just for the next domain, but all following domains. Training on all such sequences of domains provides dramatically more ‘practice’ for a base DG learner compared to existing approaches, thus improving performance on a true testing domain. This strategy can be instantiated for different base DG algorithms, but we focus on its application to the recently proposed Meta-Learning Domain generalization (MLDG). We show that for MLDG it leads to a simple to implement and fast algorithm that provides consistent performance improvement on a variety of DG benchmarks.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_39');
INSERT INTO `paper` VALUES (12729, 'SeqXY2SeqZ: Structure Learning for 3D Shapes by Sequentially Predicting 1D Occupancy Segments from 2D Coordinates', '3D reconstruction', 'Voxel grids', 'Implicit function', 'RNN', 'Attention', 'Structure learning for 3D shapes is vital for 3D computer vision. State-of-the-art methods show promising results by representing shapes using implicit functions in 3D that are learned using discriminative neural networks. However, learning implicit functions requires dense and irregular sampling in 3D space, which also makes the sampling methods affect the accuracy of shape reconstruction during test. To avoid dense and irregular sampling in 3D, we propose to represent shapes using 2D functions, where the output of the function at each 2D location is a sequence of line segments inside the shape. Our approach leverages the power of functional representations, but without the disadvantage of 3D sampling. Specifically, we use a voxel tubelization to represent a voxel grid as a set of tubes along any one of the X, Y, or Z axes. Each tube can be indexed by its 2D coordinates on the plane spanned by the other two axes. We further simplify each tube into a sequence of occupancy segments. Each occupancy segment consists of successive voxels occupied by the shape, which leads to a simple representation of its 1D start and end location. Given the 2D coordinates of the tube and a shape feature as condition, this representation enables us to learn 3D shape structures by sequentially predicting the start and end locations of each occupancy segment in the tube. We implement this approach using a Seq2Seq model with attention, called SeqXY2SeqZ, which learns the mapping from a sequence of 2D coordinates along two arbitrary axes to a sequence of 1D locations along the third axis. SeqXY2SeqZ not only benefits from the regularity of voxel grids in training and testing, but also achieves high memory efficiency. Our experiments show that SeqXY2SeqZ outperforms the state-of-the-art methods under the widely used benchmarks.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_36');
INSERT INTO `paper` VALUES (12730, 'SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing Objects', 'Generative adversarial networks', 'Interactive image editing', 'Image synthesis', '', '', 'Recent advances in image generation gave rise to powerful tools for semantic image editing. However, existing approaches can either operate on a single image or require an abundance of additional information. They are not capable of handling the complete set of editing operations, that is addition, manipulation or removal of semantic concepts. To address these limitations, we propose SESAME, a novel generator-discriminator pair for Semantic Editing of Scenes by Adding, Manipulating or Erasing objects. In our setup, the user provides the semantic labels of the areas to be edited and the generator synthesizes the corresponding pixels. In contrast to previous methods that employ a discriminator that trivially concatenates semantics and image as an input, the SESAME discriminator is composed of two input streams that independently process the image and its semantics, using the latter to manipulate the results of the former. We evaluate our model on a diverse set of datasets and report state-of-the-art performance on two tasks: (a) image manipulation and (b) image generation conditioned on semantic labels.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_24');
INSERT INTO `paper` VALUES (12731, 'SF-Net: Single-Frame Supervision for Temporal Action Localization', 'Single-frame annotation', 'Action localization', '', '', '', 'In this paper, we study an intermediate form of supervision, i.e., single-frame supervision, for temporal action localization (TAL). To obtain the single-frame supervision, the annotators are asked to identify only a single frame within the temporal window of an action. This can significantly reduce the labor cost of obtaining full supervision which requires annotating the action boundary. Compared to the weak supervision that only annotates the video-level label, the single-frame supervision introduces extra temporal action signals while maintaining low annotation overhead. To make full use of such single-frame supervision, we propose a unified system called SF-Net. First, we propose to predict an actionness score for each video frame. Along with a typical category score, the actionness score can provide comprehensive information about the occurrence of a potential action and aid the temporal boundary refinement during inference. Second, we mine pseudo action and background frames based on the single-frame annotations. We identify pseudo action frames by adaptively expanding each annotated single frame to its nearby, contextual frames and we mine pseudo background frames from all the unannotated frames across multiple videos. Together with the ground-truth labeled frames, these pseudo-labeled frames are further used for training the classifier. In extensive experiments on THUMOS14, GTEA, and BEOID, SF-Net significantly improves upon state-of-the-art weakly-supervised methods in terms of both segment localization and single-frame localization. Notably, SF-Net achieves comparable results to its fully-supervised counterpart which requires much more resource intensive annotations. The code is available at https://github.com/Flowerfan/SF-Net.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_25');
INSERT INTO `paper` VALUES (12732, 'SG-VAE: Scene Grammar Variational Autoencoder to Generate New Indoor Scenes', 'Scene grammar', 'Indoor scene synthesis', 'VAE', '', '', 'Deep generative models have been used in recent years to learn coherent latent representations in order to synthesize high-quality images. In this work, we propose a neural network to learn a generative model for sampling consistent indoor scene layouts. Our method learns the co-occurrences, and appearance parameters such as shape and pose, for different objects categories through a grammar-based auto-encoder, resulting in a compact and accurate representation for scene layouts. In contrast to existing grammar-based methods with a user-specified grammar, we construct the grammar automatically by extracting a set of production rules on reasoning about object co-occurrences in training data. The extracted grammar is able to represent a scene by an augmented parse tree. The proposed auto-encoder encodes these parse trees to a latent code, and decodes the latent code to a parse tree, thereby ensuring the generated scene is always valid. We experimentally demonstrate that the proposed auto-encoder learns not only to generate valid scenes (i.e. the arrangements and appearances of objects), but it also learns coherent latent representations where nearby latent samples decode to similar scene outputs. The obtained generative model is applicable to several computer vision tasks such as 3D pose and layout estimation from RGB-D data.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_10');
INSERT INTO `paper` VALUES (12733, 'Shape Adaptor: A Learnable Resizing Module', 'Automated Machine Learning', 'Resizing layer', 'Neural architecture search', '', '', 'We present a novel resizing module for neural networks: shape adaptor, a drop-in enhancement built on top of traditional resizing layers, such as pooling, bilinear sampling, and strided convolution. Whilst traditional resizing layers have fixed and deterministic reshaping factors, our module allows for a learnable reshaping factor. Our implementation enables shape adaptors to be trained end-to-end without any additional supervision, through which network architectures can be optimised for each individual task, in a fully automated way. We performed experiments across seven image classification datasets, and results show that by simply using a set of our shape adaptors instead of the original resizing layers, performance increases consistently over human-designed networks, across all datasets. Additionally, we show the effectiveness of shape adaptors on two other applications: network compression and transfer learning.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_39');
INSERT INTO `paper` VALUES (12734, 'Shape and Viewpoint Without Keypoints', '', '', '', '', '', 'We present a learning framework that learns to recover the 3D shape, pose and texture from a single image, trained on an image collection without any ground truth 3D shape, multi-view, camera viewpoints or keypoint supervision. We approach this highly under-constrained problem in a “analysis by synthesis” framework where the goal is to predict the likely shape, texture and camera viewpoint that could produce the image with various learned category-specific priors. Our particular contribution in this paper is a representation of the distribution over cameras, which we call “camera-multiplex”. Instead of picking a point estimate, we maintain a set of camera hypotheses that are optimized during training to best explain the image given the current shape and texture. We call our approach Unsupervised Category-Specific Mesh Reconstruction (U-CMR), and present qualitative and quantitative results on CUB, Pascal 3D and new web-scraped datasets. We obtain state-of-the-art camera prediction results and show that we can learn to predict diverse shapes and textures across objects using an image collection without any keypoint annotations or 3D ground truth. Project page: https://shubham-goel.github.io/ucmr.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_6');
INSERT INTO `paper` VALUES (12735, 'Shape Prior Deformation for Categorical 6D Object Pose and Size Estimation', 'Category-level pose estimation', '3D object detection', 'Shape generation', 'Scene understanding', '', 'We present a novel learning approach to recover the 6D poses and sizes of unseen object instances from an RGB-D image. To handle the intra-class shape variation, we propose a deep network to reconstruct the 3D object model by explicitly modeling the deformation from a pre-learned categorical shape prior. Additionally, our network infers the dense correspondences between the depth observation of the object instance and the reconstructed 3D model to jointly estimate the 6D object pose and size. We design an autoencoder that trains on a collection of object models and compute the mean latent embedding for each category to learn the categorical shape priors. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms the state of the art. Our code is available at https://github.com/mentian/object-deformnet.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_32');
INSERT INTO `paper` VALUES (12736, 'SHARP 2020: The 1st Shape Recovery from Partial Textured 3D Scans Challenge Results', '', '', '', '', '', 'The SHApe Recovery from Partial textured 3D scans challenge, SHARP 2020, is the first edition of a challenge fostering and benchmarking methods for recovering complete textured 3D scans from raw incomplete data. SHARP 2020 is organised as a workshop in conjunction with ECCV 2020. There are two complementary challenges, the first one on 3D human scans, and the second one on generic objects. Challenge 1 is further split into two tracks, focusing, first, on large body and clothing regions, and, second, on fine body details. A novel evaluation metric is proposed to quantify jointly the shape reconstruction, the texture reconstruction and the amount of completed data. Additionally, two unique datasets of 3D scans are proposed, to provide raw ground-truth data for the benchmarks. The datasets are released to the scientific community. Moreover, an accompanying custom library of software routines is also released to the scientific community. It allows for processing 3D scans, generating partial data and performing the evaluation. Results of the competition, analysed in comparison to baselines, show the validity of the proposed evaluation metrics, and highlight the challenging aspects of the task and of the datasets. Details on the SHARP 2020 challenge can be found at https://cvi2.uni.lu/sharp2020/.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_50');
INSERT INTO `paper` VALUES (12737, 'Shift Equivariance in Object Detection', 'Convolutional Neural Networks', 'Object detection', 'Network robustness', 'Shift equivariance', '', 'Robustness to small image translations is a highly desirable property for object detectors. However, recent works have shown that CNN-based classifiers are not shift invariant. It is unclear to what extent this could impact object detection, mainly because of the architectural differences between the two and the dimensionality of the prediction space of modern detectors.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_4');
INSERT INTO `paper` VALUES (12738, 'Shonan Rotation Averaging: Global Optimality by Surfing \\(SO(p)^n\\)', '', '', '', '', '', 'Shonan Rotation Averaging is a fast, simple, and elegant rotation averaging algorithm that is guaranteed to recover globally optimal solutions under mild assumptions on the measurement noise. Our method employs semidefinite relaxation in order to recover provably globally optimal solutions of the rotation averaging problem. In contrast to prior work, we show how to solve large-scale instances of these relaxations using manifold minimization on (only slightly) higher-dimensional rotation manifolds, re-using existing high-performance (but local) structure-from-motion pipelines. Our method thus preserves the speed and scalability of current SFM methods, while recovering globally optimal solutions.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_18');
INSERT INTO `paper` VALUES (12739, 'Short-Term and Long-Term Context Aggregation Network for Video Inpainting', 'Video inpainting', 'Context aggregation', '', '', '', 'Video inpainting aims to restore missing regions of a video and has many applications such as video editing and object removal. However, existing methods either suffer from inaccurate short-term context aggregation or rarely explore long-term frame information. In this work, we present a novel context aggregation network to effectively exploit both short-term and long-term frame information for video inpainting. In the encoding stage, we propose boundary-aware short-term context aggregation, which aligns and aggregates, from neighbor frames, local regions that are closely related to the boundary context of missing regions into the target frame (The target frame refers to the current input frame under inpainting.). Furthermore, we propose dynamic long-term context aggregation to globally refine the feature map generated in the encoding stage using long-term frame features, which are dynamically updated throughout the inpainting process. Experiments show that it outperforms state-of-the-art methods with better inpainting results and fast inpainting speed.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_42');
INSERT INTO `paper` VALUES (12740, 'Shuffle and Attend: Video Domain Adaptation', '', '', '', '', '', 'We address the problem of domain adaptation in videos for the task of human action recognition. Inspired by image-based domain adaptation, we can perform video adaptation by aligning the features of frames or clips of source and target videos. However, equally aligning all clips is sub-optimal as not all clips are informative for the task. As the first novelty, we propose an attention mechanism which focuses on more discriminative clips and directly optimizes for video-level (cf. clip-level) alignment. As the backgrounds are often very different between source and target, the source background-corrupted model adapts poorly to target domain videos. To alleviate this, as a second novelty, we propose to use the clip order prediction as an auxiliary task. The clip order prediction loss, when combined with domain adversarial loss, encourages learning of representations which focus on the humans and objects involved in the actions, rather than the uninformative and widely differing (between source and target) backgrounds. We empirically show that both components contribute positively towards adaptation performance. We report state-of-the-art performances on two out of three challenging public benchmarks, two based on the UCF and HMDB datasets, and one on Kinetics to NEC-Drone datasets. We also support the intuitions and the results with qualitative results.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_40');
INSERT INTO `paper` VALUES (12741, 'Side-Aware Boundary Localization for More Precise Object Detection', '', '', '', '', '', 'Current object detection frameworks mainly rely on bounding box regression to localize objects. Despite the remarkable progress in recent years, the precision of bounding box regression remains unsatisfactory, hence limiting performance in object detection. We observe that precise localization requires careful placement of each side of the bounding box. However, the mainstream approach, which focuses on predicting centers and sizes, is not the most effective way to accomplish this task, especially when there exists displacements with large variance between the anchors and the targets. In this paper, we propose an alternative approach, named as Side-Aware Boundary Localization (SABL), where each side of the bounding box is respectively localized with a dedicated network branch. To tackle the difficulty of precise localization in the presence of displacements with large variance, we further propose a two-step localization scheme, which first predicts a range of movement through bucket prediction and then pinpoints the precise position within the predicted bucket. We test the proposed method on both two-stage and single-stage detection frameworks. Replacing the standard bounding box regression branch with the proposed design leads to significant improvements on Faster R-CNN, RetinaNet, and Cascade R-CNN, by 3.0%, 1.7%, and 0.9%, respectively. Code is available at https://github.com/open-mmlab/mmdetection.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_24');
INSERT INTO `paper` VALUES (12742, 'Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks', 'Sidetuning', 'Finetuning', 'Transfer learning', 'Representation learning', 'Lifelong learning', 'When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than starting from randomly initialized weights. Adaptation can be useful in cases when training data is scarce, when a single learner needs to perform multiple tasks, or when one wishes to encode priors in the network. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others. In this paper, we propose a straightforward alternative: side-tuning. Side-tuning adapts a pre-trained network by training a lightweight “side\" network that is fused with the (unchanged) pre-trained network via summation. This simple method works as well as or better than existing solutions and it resolves some of the basic issues with fine-tuning, fixed features, and other common approaches. In particular, side-tuning is less prone to overfitting, is asymptotically consistent, and does not suffer from catastrophic forgetting in incremental learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including incremental learning (iCIFAR, iTaskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_41');
INSERT INTO `paper` VALUES (12743, 'SideInfNet: A Deep Neural Network for Semi-Automatic Semantic Segmentation with Side Information', 'Semi-automatic semantic segmentation', 'Side information', '', '', '', 'Fully-automatic execution is the ultimate goal for many Computer Vision applications. However, this objective is not always realistic in tasks associated with high failure costs, such as medical applications. For these tasks, semi-automatic methods allowing minimal effort from users to guide computer algorithms are often preferred due to desirable accuracy and performance. Inspired by the practicality and applicability of the semi-automatic approach, this paper proposes a novel deep neural network architecture, namely SideInfNet that effectively integrates features learnt from images with side information extracted from user annotations. To evaluate our method, we applied the proposed network to three semantic segmentation tasks and conducted extensive experiments on benchmark datasets. Experimental results and comparison with prior work have verified the superiority of our model, suggesting the generality and effectiveness of the model in semi-automatic semantic segmentation.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_7');
INSERT INTO `paper` VALUES (12744, 'SignSynth: Data-Driven Sign Language Video Generation', 'Sign language', 'Pose generation', 'Human motion', '', '', 'We present SignSynth, a fully automatic and holistic approach to generating sign language video. Traditionally, Sign Language Production (SLP) relies on animating 3D avatars using expensively annotated data, but so far this approach has not been able to simultaneously provide a realistic, and scalable solution. We introduce a gloss2pose network architecture that is capable of generating human pose sequences conditioned on glosses. (For sign languages a gloss is a written representation that describes a specific sign.) Combined with a generative adversarial pose2video network, we are able to produce natural-looking, high definition sign language video. For sign pose sequence generation, we outperform the SotA by a factor of 18, with a Mean Square Error of 1.0673 in pixels. For video generation we report superior results on three broadcast quality assessment metrics. To evaluate our full gloss-to-video pipeline we introduce two novel error metrics, to assess the perceptual quality and sign representativeness of generated videos. We present promising results, significantly outperforming the SotA in both metrics. Finally we evaluate our approach qualitatively by analysing example sequences.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_21');
INSERT INTO `paper` VALUES (12745, 'SimAug: Learning Robust Representations from Simulation for Trajectory Prediction', 'Trajectory prediction', '3D simulation', 'Robust learning', 'Data augmentation', 'Representation learning', 'This paper studies the problem of predicting future trajectories of people in unseen cameras of novel scenarios and views. We approach this problem through the real-data-free setting in which the model is trained only on 3D simulation data and applied out-of-the-box to a wide variety of real cameras. We propose a novel approach to learn robust representation through augmenting the simulation training data such that the representation can better generalize to unseen real-world test data. The key idea is to mix the feature of the hardest camera view with the adversarial feature of the original view. We refer to our method as SimAug. We show that SimAug achieves promising results on three real-world benchmarks using zero real training data, and state-of-the-art performance in the Stanford Drone and the VIRAT/ActEV dataset when using in-domain training data. Code and models are released at https://next.cs.cmu.edu/simaug.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_17');
INSERT INTO `paper` VALUES (12746, 'Simplicial Complex Based Point Correspondence Between Images Warped onto Manifolds', '', '', '', '', '', 'Recent increase in the availability of warped images projected onto a curved manifold, especially omnidirectional spherical ones, coupled with the success of higher-order assignment methods, has sparked an interest in the search for improved higher-order matching algorithms on warped images due to projection. Although, currently, several existing methods “flatten” such 3D images to use planar graph/hypergraph matching methods, they still suffer from severe distortions and other undesired artifacts, which result in inaccurate matching. Alternatively, current planar methods cannot be trivially extended to effectively match points on images warped on curved manifold. Hence, matching on these warped images persists as a formidable challenge. In this paper, we pose the assignment problem as finding a bijective map between two graph induced simplicial complexes, which are higher-order analogues of graphs. We propose a constrained quadratic assignment problem (QAP) that matches each p-skeleton of the simplicial complexes, iterating from the highest to the lowest dimension. The accuracy and robustness of our approach are illustrated on both synthetic and real-world spherical/warped (projected) images with known ground-truth correspondences. We significantly outperform existing state-of-the-art spherical matching methods on a diverse set of datasets.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_4');
INSERT INTO `paper` VALUES (12747, 'SimPose: Effectively Learning DensePose and Surface Normals of People from Simulated Data', 'Person pose estimation', 'Simulated data', 'Dense pose estimation', '3D surface normal', 'Multi-task objective', 'With a proliferation of generic domain-adaptation approaches, we report a simple yet effective technique for learning difficult per-pixel 2.5D and 3D regression representations of articulated people. We obtained strong sim-to-real domain generalization for the 2.5D DensePose estimation task and the 3D human surface normal estimation task. On the multi-person DensePose MSCOCO benchmark, our approach outperforms the state-of-the-art methods which are trained on real images that are densely labelled. This is an important result since obtaining human manifold’s intrinsic uv coordinates on real images is time consuming and prone to labeling noise. Additionally, we present our model’s 3D surface normal predictions on the MSCOCO dataset that lacks any real 3D surface normal labels. The key to our approach is to mitigate the “Inter-domain Covariate Shift” with a carefully selected training batch from a mixture of domain samples, a deep batch-normalized residual network, and a modified multi-task learning objective. Our approach is complementary to existing domain-adaptation techniques and can be applied to other dense per-pixel pose estimation problems.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_14');
INSERT INTO `paper` VALUES (12748, 'Simulating Content Consistent Vehicle Datasets with Attribute Descent', 'Vehicle retrieval', 'Domain adaptation', 'Synthetic data', '', '', 'This paper uses a graphic engine to simulate a large amount of training data with free annotations. Between synthetic and real data, there is a two-level domain gap, i.e., content level and appearance level. While the latter has been widely studied, we focus on reducing the content gap in attributes like illumination and viewpoint. To reduce the problem complexity, we choose a smaller and more controllable application, vehicle re-identification (re-ID). We introduce a large-scale synthetic dataset VehicleX. Created in Unity, it contains 1,362 vehicles of various 3D models with fully editable attributes. We propose an attribute descent approach to let VehicleX approximate the attributes in real-world datasets. Specifically, we manipulate each attribute in VehicleX, aiming to minimize the discrepancy between VehicleX and real data in terms of the Fréchet Inception Distance (FID). This attribute descent algorithm allows content domain adaptation (DA) orthogonal to existing appearance DA methods. We mix the optimized VehicleX data with real-world vehicle re-ID datasets, and observe consistent improvement. With the augmented datasets, we report competitive accuracy. We make the dataset, engine and our codes available at https://github.com/yorkeyao/VehicleX.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_46');
INSERT INTO `paper` VALUES (12749, 'Simultaneous Detection and Tracking with Motion Modelling for Multiple Object Tracking', 'Multiple object tracking', 'Tracking-by-detection', 'Deep learning', 'Simultaneous detection and tracking.', '', 'Deep learning based Multiple Object Tracking (MOT) currently relies on off-the-shelf detectors for tracking-by-detection. This results in deep models that are detector biased and evaluations that are detector influenced. To resolve this issue, we introduce Deep Motion Modeling Network (DMM-Net) that can estimate multiple objects’ motion parameters to perform joint detection and association in an end-to-end manner. DMM-Net models object features over multiple frames and simultaneously infers object classes, visibility and their motion parameters. These outputs are readily used to update the tracklets for efficient MOT. DMM-Net achieves PR-MOTA score of 12.80 @ 120+ fps for the popular UA-DETRAC challenge - which is better performance and orders of magnitude faster. We also contribute a synthetic large-scale public dataset Omni-MOT for vehicle tracking that provides precise ground-truth annotations to eliminate the detector influence in MOT evaluation. This 14M+ frames dataset is extendable with our public script (Code at Dataset, Dataset Recorder, Omni-MOT Source). We demonstrate the suitability of Omni-MOT for deep learning with DMM-Net, and also make the source code of our network public.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_37');
INSERT INTO `paper` VALUES (12750, 'Single Image Dehazing for a Variety of Haze Scenarios Using Back Projected Pyramid Network', 'Single image dehazing', 'Generative adversarial network', 'Back projection', 'Deep learning', '', 'Learning to dehaze single hazy images, especially using a small training dataset is quite challenging. We propose a novel generative adversarial network architecture for this problem, namely back projected pyramid network (BPPNet), that gives good performance for a variety of challenging haze conditions, including dense haze and inhomogeneous haze. Our architecture incorporates learning of multiple levels of complexities while retaining spatial context through iterative blocks of UNets and structural information of multiple scales through a novel pyramidal convolution block. These blocks together for the generator and are amenable to learning through back projection. We have shown that our network can be trained without over-fitting using as few as 20 image pairs of hazy and non-hazy images. We report the state of the art performances on NTIRE 2018 homogeneous haze datasets for indoor and outdoor images, NTIRE 2019 denseHaze dataset, and NTIRE 2020 non-homogeneous haze dataset.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_10');
INSERT INTO `paper` VALUES (12751, 'Single Image Super-Resolution via a Holistic Attention Network', 'Super-resolution', 'Holistic attention', 'Layer attention', 'Channel-spatial attention', '', 'Informative features play a crucial role in the single image super-resolution task. Channel attention has been demonstrated to be effective for preserving information-rich features in each layer. However, channel attention treats each convolution layer as a separate process that misses the correlation among different layers. To address this problem, we propose a new holistic attention network (HAN), which consists of a layer attention module (LAM) and a channel-spatial attention module (CSAM), to model the holistic interdependencies among layers, channels, and positions. Specifically, the proposed LAM adaptively emphasizes hierarchical features by considering correlations among layers. Meanwhile, CSAM learns the confidence at all the positions of each channel to selectively capture more informative features. Extensive experiments demonstrate that the proposed HAN performs favorably against the state-of-the-art single image super-resolution approaches.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_12');
INSERT INTO `paper` VALUES (12752, 'Single Path One-Shot Neural Architecture Search with Uniform Sampling', '', '', '', '', '', 'We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_32');
INSERT INTO `paper` VALUES (12753, 'Single View Metrology in the Wild', 'Single view metrology', 'Absolute scale estimation', 'Camera calibration', 'Virtual object insertion', '', 'Most 3D reconstruction methods may only recover scene properties up to a global scale ambiguity. We present a novel approach to single view metrology that can recover the absolute scale of a scene represented by 3D heights of objects or camera height above the ground as well as camera parameters of orientation and field of view, using just a monocular image acquired in unconstrained condition. Our method relies on data-driven priors learned by a deep network specifically designed to imbibe weakly supervised constraints from the interplay of the unknown camera with 3D entities such as object heights, through estimation of bounding box projections. We leverage categorical priors for objects such as humans or cars that commonly occur in natural images, as references for scale estimation. We demonstrate state-of-the-art qualitative and quantitative results on several datasets as well as applications including virtual object insertion. Furthermore, the perceptual quality of our outputs is validated by a user study.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_19');
INSERT INTO `paper` VALUES (12754, 'Single-Image Depth Prediction Makes Feature Matching Easier', 'Local feature matching', 'Image matching', '', '', '', 'Good local features improve the robustness of many 3D re-localization and multi-view reconstruction pipelines. The problem is that viewing angle and distance severely impact the recognizability of a local feature. Attempts to improve appearance invariance by choosing better local feature points or by leveraging outside information, have come with pre-requisites that made some of them impractical. In this paper, we propose a surprisingly effective enhancement to local feature extraction, which improves matching. We show that CNN-based depths inferred from single RGB images are quite helpful, despite their flaws. They allow us to pre-warp images and rectify perspective distortions, to significantly enhance SIFT and BRISK features, enabling more good matches, even when cameras are looking at the same scene but in opposite directions.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_28');
INSERT INTO `paper` VALUES (12755, 'Single-Shot Neural Relighting and SVBRDF Estimation', 'Single-image relighting', 'SVBRDF estimation', 'Physically-based networks', '', '', 'We present a novel physically-motivated deep network for joint shape and material estimation, as well as relighting under novel illumination conditions, using a single image captured by a mobile phone camera. Our physically-based modeling leverages a deep cascaded architecture trained on a large-scale synthetic dataset that consists of complex shapes with microfacet SVBRDF. In contrast to prior works that train rendering layers subsequent to inverse rendering, we propose deep feature sharing and joint training that transfer insights across both tasks, to achieve significant improvements in both reconstruction and relighting. We demonstrate in extensive qualitative and quantitative experiments that our network generalizes very well to real images, achieving high-quality shape and material estimation, as well as image-based relighting. Code, models and data will be publicly released.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_6');
INSERT INTO `paper` VALUES (12756, 'SipMask: Spatial Information Preservation for Fast Image and Video Instance Segmentation', 'Instance segmentation', 'Real-time', 'Spatial preservation', '', '', 'Single-stage instance segmentation approaches have recently gained popularity due to their speed and simplicity, but are still lagging behind in accuracy, compared to two-stage methods. We propose a fast single-stage instance segmentation method, called SipMask, that preserves instance-specific spatial information by separating mask prediction of an instance to different sub-regions of a detected bounding-box. Our main contribution is a novel light-weight spatial preservation (SP) module that generates a separate set of spatial coefficients for each sub-region within a bounding-box, leading to improved mask predictions. It also enables accurate delineation of spatially adjacent instances. Further, we introduce a mask alignment weighting loss and a feature alignment scheme to better correlate mask prediction with object detection. On COCO test-dev, our SipMask outperforms the existing single-stage methods. Compared to the state-of-the-art single-stage TensorMask, SipMask obtains an absolute gain of 1.0% (mask AP), while providing a four-fold speedup. In terms of real-time capabilities, SipMask outperforms YOLACT with an absolute gain of 3.0% (mask AP) under similar settings, while operating at comparable speed on a Titan Xp. We also evaluate our SipMask for real-time video instance segmentation, achieving promising results on YouTube-VIS dataset. The source code is available at https://github.com/JialeCao001/SipMask.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_1');
INSERT INTO `paper` VALUES (12757, 'SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing', '', '', '', '', '', 'While models of 3D clothing learned from real data exist, no method can predict clothing deformation as a function of garment size. In this paper, we introduce SizerNet to predict 3D clothing conditioned on human body shape and garment size parameters, and ParserNet to infer garment meshes and shape under clothing with personal details in a single pass from an input mesh. SizerNet allows to estimate and visualize the dressing effect of a garment in various sizes, and ParserNet allows to edit clothing of an input mesh directly, removing the need for scan segmentation, which is a challenging problem in itself. To learn these models, we introduce the SIZER dataset of clothing size variation which includes 100 different subjects wearing casual clothing items in various sizes, totaling to approximately 2000 scans. This dataset includes the scans, registrations to the SMPL model, scans segmented in clothing parts, garment category and size labels. Our experiments show better parsing accuracy and size prediction than baseline methods trained on SIZER. The code, model and dataset will be released for research purposes at: https://virtualhumans.mpi-inf.mpg.de/sizer/.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_1');
INSERT INTO `paper` VALUES (12758, 'Sketch-Guided Object Localization in Natural Images', 'Sketch', 'Cross-modal retrieval', 'Object localization', 'One-shot learning', 'Attention', 'We introduce a novel problem of localizing all the instances of an object (seen or unseen during training) in a natural image via sketch query. We refer to this problem as sketch-guided object localization. This problem is distinctively different from the traditional sketch-based image retrieval task where the gallery set often contains images with only one object. The sketch-guided object localization proves to be more challenging when we consider the following: (i) the sketches used as queries are abstract representations with little information on the shape and salient attributes of the object, (ii) the sketches have significant variability as they are hand-drawn by a diverse set of untrained human subjects, and (iii) there exists a domain gap between sketch queries and target natural images as these are sampled from very different data distributions. To address the problem of sketch-guided object localization, we propose a novel cross-modal attention scheme that guides the region proposal network (RPN) to generate object proposals relevant to the sketch query. These object proposals are later scored against the query to obtain final localization. Our method is effective with as little as a single sketch query. Moreover, it also generalizes well to object categories not seen during training and is effective in localizing multiple object instances present in the image. Furthermore, we extend our framework to a multi-query setting using novel feature fusion and attention fusion strategies introduced in this paper. The localization performance is evaluated on publicly available object detection benchmarks, viz. MS-COCO and PASCAL-VOC, with sketch queries obtained from ‘Quick, Draw!’. The proposed method significantly outperforms related baselines on both single-query and multi-query localization tasks.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_32');
INSERT INTO `paper` VALUES (12759, 'Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation', 'Image gist', 'Key relation', 'Hierarchical Entity Tree', 'Hybrid-LSTM', 'Relation Ranking Module', 'Scene graph aims to faithfully reveal humans’ perception of image content. When humans analyze a scene, they usually prefer to describe image gist first, namely major objects and key relations in a scene graph. This humans’ inherent perceptive habit implies that there exists a hierarchical structure about humans’ preference during the scene parsing procedure. Therefore, we argue that a desirable scene graph should be also hierarchically constructed, and introduce a new scheme for modeling scene graph. Concretely, a scene is represented by a human-mimetic Hierarchical Entity Tree (HET) consisting of a series of image regions. To generate a scene graph based on HET, we parse HET with a Hybrid Long Short-Term Memory (Hybrid-LSTM) which specifically encodes hierarchy and siblings context to capture the structured information embedded in HET. To further prioritize key relations in the scene graph, we devise a Relation Ranking Module (RRM) to dynamically adjust their rankings by learning to capture humans’ subjective perceptive habits from objective entity saliency and size. Experiments indicate that our method not only achieves state-of-the-art performances for scene graph generation, but also is expert in mining image-specific relations which play a great role in serving downstream tasks.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_14');
INSERT INTO `paper` VALUES (12760, 'SLRTP 2020: The Sign Language Recognition, Translation & Production Workshop', '', '', '', '', '', 'The objective of the “Sign Language Recognition, Translation & Production” (SLRTP 2020) Workshop was to bring together researchers who focus on the various aspects of sign language understanding using tools from computer vision and linguistics. The workshop sought to promote a greater linguistic and historical understanding of sign languages within the computer vision community, to foster new collaborations and to identify the most pressing challenges for the field going forwards. The workshop was held in conjunction with the European Conference on Computer Vision (ECCV), 2020.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_13');
INSERT INTO `paper` VALUES (12761, 'SMAP: Single-Shot Multi-person Absolute 3D Pose Estimation', 'Human pose estimation', '3D from a single image', '', '', '', 'Recovering multi-person 3D poses with absolute scales from a single RGB image is a challenging problem due to the inherent depth and scale ambiguity from a single view. Addressing this ambiguity requires to aggregate various cues over the entire image, such as body sizes, scene layouts, and inter-person relationships. However, most previous methods adopt a top-down scheme that first performs 2D pose detection and then regresses the 3D pose and scale for each detected person individually, ignoring global contextual cues. In this paper, we propose a novel system that first regresses a set of 2.5D representations of body parts and then reconstructs the 3D absolute poses based on these 2.5D representations with a depth-aware part association algorithm. Such a single-shot bottom-up scheme allows the system to better learn and reason about the inter-person depth relationship, improving both 3D and 2D pose estimation. The experiments demonstrate that the proposed approach achieves the state-of-the-art performance on the CMU Panoptic and MuPoTS-3D datasets and is applicable to in-the-wild videos.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_33');
INSERT INTO `paper` VALUES (12762, 'SMART: Simultaneous Multi-Agent Recurrent Trajectory Prediction', 'Diverse trajectory prediction', 'Multiple agents', 'Constant time', 'Scene constraints', 'Simulation', 'We propose advances that address two key challenges in future trajectory prediction: (i) multimodality in both training data and predictions and (ii) constant time inference regardless of number of agents. Existing trajectory predictions are fundamentally limited by lack of diversity in training data, which is difficult to acquire with sufficient coverage of possible modes. Our first contribution is an automatic method to simulate diverse trajectories in the top-view. It uses pre-existing datasets and maps as initialization, mines existing trajectories to represent realistic driving behaviors and uses a multi-agent vehicle dynamics simulator to generate diverse new trajectories that cover various modes and are consistent with scene layout constraints. Our second contribution is a novel method that generates diverse predictions while accounting for scene semantics and multi-agent interactions, with constant-time inference independent of the number of agents. We propose a convLSTM with novel state pooling operations and losses to predict scene-consistent states of multiple agents in a single forward pass, along with a CVAE for diversity. We validate our proposed multi-agent trajectory prediction approach by training and testing on the proposed simulated dataset and existing real datasets of traffic scenes. In both cases, our approach outperforms SOTA methods by a large margin, highlighting the benefits of both our diverse dataset simulation and constant-time diverse trajectory prediction methods.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_28');
INSERT INTO `paper` VALUES (12763, 'Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval', '', '', '', '', '', 'Optimising a ranking-based metric, such as Average Precision (AP), is notoriously challenging due to the fact that it is non-differentiable, and hence cannot be optimised directly using gradient-descent methods. To this end, we introduce an objective that optimises instead a smoothed approximation of AP, coined Smooth-AP. Smooth-AP is a plug-and-play objective function that allows for end-to-end training of deep networks with a simple and elegant implementation. We also present an analysis for why directly optimising the ranking based metric of AP offers benefits over other deep metric learning losses.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_39');
INSERT INTO `paper` VALUES (12764, 'SNE-RoadSeg: Incorporating Surface Normal Information into Semantic Segmentation for Accurate Freespace Detection', 'Freespace detection', 'Self-driving cars', 'Data-fusion CNN', 'Semantic driving scene segmentation', 'Surface normal', 'Freespace detection is an essential component of visual perception for self-driving cars. The recent efforts made in data-fusion convolutional neural networks (CNNs) have significantly improved semantic driving scene segmentation. Freespace can be hypothesized as a ground plane, on which the points have similar surface normals. Hence, in this paper, we first introduce a novel module, named surface normal estimator (SNE), which can infer surface normal information from dense depth/disparity images with high accuracy and efficiency. Furthermore, we propose a data-fusion CNN architecture, referred to as RoadSeg, which can extract and fuse features from both RGB images and the inferred surface normal information for accurate freespace detection. For research purposes, we publish a large-scale synthetic freespace detection dataset, named Ready-to-Drive (R2D) road dataset, collected under different illumination and weather conditions. The experimental results demonstrate that our proposed SNE module can benefit all the state-of-the-art CNNs for freespace detection, and our SNE-RoadSeg achieves the best overall performance among different datasets.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_21');
INSERT INTO `paper` VALUES (12765, 'Social Adaptive Module for Weakly-Supervised Group Activity Recognition', 'Group activity recognition', 'Video analysis', 'Scene understanding', '', '', 'This paper presents a new task named weakly-supervised group activity recognition (GAR) which differs from conventional GAR tasks in that only video-level labels are available, yet the important persons within each frame are not provided even in the training data. This eases us to collect and annotate a large-scale NBA dataset and thus raise new challenges to GAR. To mine useful information from weak supervision, we present a key insight that key instances are likely to be related to each other, and thus design a social adaptive module (SAM) to reason about key persons and frames from noisy data. Experiments show significant improvement on the NBA dataset as well as the popular volleyball dataset. In particular, our model trained on video-level annotation achieves comparable accuracy to prior algorithms which required strong labels.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_13');
INSERT INTO `paper` VALUES (12766, 'SODA: Story Oriented Dense Video Captioning Evaluation Framework', 'Automatic evaluation', 'Dense Video Captioning', 'Video story description', '', '', 'Dense Video Captioning (DVC) is a challenging task that localizes all events in a short video and describes them with natural language sentences. The main goal of DVC is video story description, that is, to generate a concise video story that supports human video comprehension without watching it. In recent years, DVC has attracted increasing attention in the vision and language research community, and has been employed as a task of the workshop, ActivityNet Challenge. In the current research community, the official scorer provided by ActivityNet Challenge is the de-facto standard evaluation framework for DVC systems. It computes averaged METEOR scores for matched pairs between generated and reference captions whose Intersection over Union (IoU) exceeds a specific threshold value. However, the current framework does not take into account the story of the video or the ordering of captions. It also tends to give high scores to systems that generate several hundred redundant captions, that humans cannot read. This paper proposes a new evaluation framework, Story Oriented Dense video cAptioning evaluation framework (SODA), for measuring the performance of video story description systems. SODA first tries to find temporally optimal matching between generated and reference captions to capture the story of a video. Then, it computes METEOR scores for the matching and derives F-measure scores from the METEOR scores to penalize redundant captions. To demonstrate that SODA gives low scores for inadequate captions in terms of video story description, we evaluate two state-of-the-art systems with it, varying the number of captions. The results show that SODA gives low scores against too many or too few captions and high scores against captions whose number equals to that of a reference, while the current framework gives good scores for all the cases. Furthermore, we show that SODA tends to give lower scores than the current evaluation framework in evaluating captions in the incorrect order.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_31');
INSERT INTO `paper` VALUES (12767, 'Soft Anchor-Point Object Detection', 'Object detection', 'Anchor-point detector', 'Soft-weighted anchor points', 'Soft-selected pyramid levels', '', 'Recently, anchor-free detection methods have been through great progress. The major two families, anchor-point detection and key-point detection, are at opposite edges of the speed-accuracy trade-off, with anchor-point detectors having the speed advantage. In this work, we boost the performance of the anchor-point detector over the key-point counterparts while maintaining the speed advantage. To achieve this, we formulate the detection problem from the anchor point’s perspective and identify ineffective training as the main problem. Our key insight is that anchor points should be optimized jointly as a group both within and across feature pyramid levels. We propose a simple yet effective training strategy with soft-weighted anchor points and soft-selected pyramid levels to address the false attention issue within each pyramid level and the feature selection issue across all the pyramid levels, respectively. To evaluate the effectiveness, we train a single-stage anchor-free detector called Soft Anchor-Point Detector (SAPD). Experiments show that our concise SAPD pushes the envelope of speed/accuracy trade-off to a new level, outperforming recent state-of-the-art anchor-free and anchor-based detectors. Without bells and whistles, our best model can achieve a single-model single-scale AP of 47.4% on COCO.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_6');
INSERT INTO `paper` VALUES (12768, 'Soft Expert Reward Learning for Vision-and-Language Navigation', 'Soft expert distillation', 'Self perceiving reward', 'Vision-and-language navigation', '', '', 'Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert’s behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_8');
INSERT INTO `paper` VALUES (12769, 'SoftPoolNet: Shape Descriptor for Point Cloud Completion and Classification', '', '', '', '', '', 'Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature – points are stored in an unordered way – makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling. For the decoder stage, we propose regional convolutions, a novel operator aimed at maximizing the global activation entropy. Furthermore, inspired by the local refining procedure in Point Completion Network (PCN), we also propose a patch-deforming operation to simulate deconvolutional operations for point clouds. This paper proves that our regional activation can be incorporated in many point cloud architectures like AtlasNet and PCN, leading to better performance for geometric completion. We evaluate our approach on different 3D tasks such as object completion and classification, achieving state-of-the-art accuracy.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_5');
INSERT INTO `paper` VALUES (12770, 'SOLAR: Second-Order Loss and Attention for Image Retrieval', 'Image retrieval', 'Descriptors', 'Features', '', '', 'Recent works in deep-learning have shown that second-order information is beneficial in many computer-vision tasks. Second-order information can be enforced both in the spatial context and the abstract feature dimensions. In this work, we explore two second-order components. One is focused on second-order spatial information to increase the performance of image descriptors, both local and global. It is used to re-weight feature maps, and thus emphasise salient image locations that are subsequently used for description. The second component is concerned with a second-order similarity (SOS) loss, that we extend to global descriptors for image retrieval, and is used to enhance the triplet loss with hard-negative mining. We validate our approach on two different tasks and datasets for image retrieval and image matching. The results show that our two second-order components complement each other, bringing significant performance improvements in both tasks and lead to state-of-the-art results across the public benchmarks. Code available at: http://github.com/tonyngjichun/SOLAR.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_16');
INSERT INTO `paper` VALUES (12771, 'SOLO: Segmenting Objects by Locations', 'Instance segmentation', 'Location category', '', '', '', 'We present a new, embarrassingly simple approach to instance segmentation. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the “detect-then-segment” strategy (e.g., Mask R-CNN), or predict embedding vectors first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of “instance categories”, which assigns categories to each pixel within an instance according to the instance’s location and size, thus nicely converting instance segmentation into a single-shot classification-solvable problem. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent single-shot instance segmenters in accuracy. We hope that this simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation. Code is available at https://git.io/AdelaiDet.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_38');
INSERT INTO `paper` VALUES (12772, 'Solving Long-Tailed Recognition with Deep Realistic Taxonomic Classifier', 'Realistic predictor', 'Taxonomic classifier', 'Long-tail recognition', '', '', 'Long-tail recognition tackles the natural non-uniformly distributed data in real-world scenarios. While modern classifiers perform well on populated classes, its performance degrades significantly on tail classes. Humans, however, are less affected by this since, when confronted with uncertain examples, they simply opt to provide coarser predictions. Motivated by this, a deep realistic taxonomic classifier (Deep-RTC) is proposed as a new solution to the long-tail problem, combining realism with hierarchical predictions. The model has the option to reject classifying samples at different levels of the taxonomy, once it cannot guarantee the desired performance. Deep-RTC is implemented with a stochastic tree sampling during training to simulate all possible classification conditions at finer or coarser levels and a rejection mechanism at inference time. Experiments on the long-tailed version of four datasets, CIFAR100, AWA2, Imagenet, and iNaturalist, demonstrate that the proposed approach preserves more information on all classes with different popularity levels. Deep-RTC also outperforms the state-of-the-art methods in longtailed recognition, hierarchical classification, and learning with rejection literature using the proposed correctly predicted bits (CPB) metric.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_11');
INSERT INTO `paper` VALUES (12773, 'Solving Phase Retrieval with a Learned Reference', '', '', '', '', '', 'Fourier phase retrieval is a classical problem that deals with the recovery of an image from the amplitude measurements of its Fourier coefficients. Conventional methods solve this problem via iterative (alternating) minimization by leveraging some prior knowledge about the structure of the unknown image. The inherent ambiguities about shift and flip in the Fourier measurements make this problem especially difficult; and most of the existing methods use several random restarts with different permutations. In this paper, we assume that a known (learned) reference is added to the signal before capturing the Fourier amplitude measurements. Our method is inspired by the principle of adding a reference signal in holography. To recover the signal, we implement an iterative phase retrieval method as an unrolled network. Then we use back propagation to learn the reference that provides us the best reconstruction for a fixed number of phase retrieval iterations. We performed a number of simulations on a variety of datasets under different conditions and found that our proposed method for phase retrieval via unrolled network and learned reference provides near-perfect recovery at fixed (small) computational cost. We compared our method with standard Fourier phase retrieval methods and observed significant performance enhancement using the learned reference.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_26');
INSERT INTO `paper` VALUES (12774, 'Solving the Blind Perspective-n-Point Problem End-to-End with Robust Differentiable Geometric Optimization', 'Camera pose estimation', 'PnP', 'Implicit differentiation', '', '', 'Blind Perspective-n-Point (PnP) is the problem of estimating the position and orientation of a camera relative to a scene, given 2D image points and 3D scene points, without prior knowledge of the 2D–3D correspondences. Solving for pose and correspondences simultaneously is extremely challenging since the search space is very large. Fortunately it is a coupled problem: the pose can be found easily given the correspondences and vice versa. Existing approaches assume that noisy correspondences are provided, that a good pose prior is available, or that the problem size is small. We instead propose the first fully end-to-end trainable network for solving the blind PnP problem efficiently and globally, that is, without the need for pose priors. We make use of recent results in differentiating optimization problems to incorporate geometric model fitting into an end-to-end learning framework, including Sinkhorn, RANSAC and PnP algorithms. Our proposed approach significantly outperforms other methods on synthetic and real data.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_15');
INSERT INTO `paper` VALUES (12775, 'Sorghum Segmentation by Skeleton Extraction', '3D plant reconstruction', 'Phenotyping', 'Sorghum', 'Skeleton extraction', 'Segmentation', 'Recently, several high-throughput phenotyping facilities have been established that allow for an automated collection of multiple view images of a large number of plants over time. One of the key problems in phenotyping is identifying individual plant organs such as leaves, stems, or roots. We introduced a novel algorithm that uses a 3D segmented plant on its input by using a voxel carving algorithm, and separates the plant into leaves and stems. Our algorithm first uses voxel thinning that generates a first approximation of the plant 3D skeleton. The skeleton is transformed into a mathematical tree by comparing and assessing paths from each leaf or stem tip to the plant root and pruned by using biologically inspired features, fed into a machine learning classifier, leading to a skeleton that corresponds to the input plant. The final skeleton is then used to identify the plant organs and segment voxels. We validated our system on 20 different plants, each represented in a voxel array of a resolution \\(512^3\\), and the segmentation was executed in under one minute, making our algorithm suitable for the processing of large amounts of plants.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_21');
INSERT INTO `paper` VALUES (12776, 'Sound2Sight: Generating Visual Dynamics from Sound and Context', '', '', '', '', '', 'Learning associations across modalities is critical for robust multimodal reasoning, especially when a modality may be missing during inference. In this paper, we study this problem in the context of audio-conditioned visual synthesis – a task that is important, for example, in occlusion reasoning. Specifically, our goal is to generate future video frames and their motion dynamics conditioned on audio and a few past frames. To tackle this problem, we present Sound2Sight, a deep variational encoder-decoder framework, that is trained to learn a per frame stochastic prior conditioned on a joint embedding of audio and past frames. This embedding is learned via a multi-head attention-based audio-visual transformer encoder. The learned prior is then sampled to further condition a video forecasting module to generate future frames. The stochastic prior allows the model to sample multiple plausible futures that are consistent with the provided audio and the past context. Moreover, to improve the quality and coherence of the generated frames, we propose a multimodal discriminator that differentiates between a synthesized and a real audio-visual clip. We empirically evaluate our approach, vis-á-vis closely-related prior methods, on two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a Surprise Obstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drums dataset. Our extensive experiments demonstrate that Sound2Sight significantly outperforms the state of the art in the generated video quality, while also producing diverse video content.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_42');
INSERT INTO `paper` VALUES (12777, 'SoundSpaces: Audio-Visual Navigation in 3D Environments', '', '', '', '', '', 'Moving around in the world is naturally a multisensory experience, but today’s embodied agents are deaf—restricted to solely their visual perception of the environment. We introduce audio-visual navigation for complex, acoustically and visually realistic 3D environments. By both seeing and hearing, the agent must learn to navigate to a sounding object. We propose a multi-modal deep reinforcement learning approach to train navigation policies end-to-end from a stream of egocentric audio-visual observations, allowing the agent to (1) discover elements of the geometry of the physical space indicated by the reverberating audio and (2) detect and follow sound-emitting targets. We further introduce SoundSpaces: a first-of-its-kind dataset of audio renderings based on geometrical acoustic simulations for two sets of publicly available 3D environments (Matterport3D and Replica), and we instrument Habitat to support the new sensor, making it possible to insert arbitrary sound sources in an array of real-world scanned environments. Our results show that audio greatly benefits embodied visual navigation in 3D spaces, and our work lays groundwork for new research in embodied AI with audio-visual perception. Project: http://vision.cs.utexas.edu/projects/audio_visual_navigation.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_2');
INSERT INTO `paper` VALUES (12778, 'SPAN: Spatial Pyramid Attention Network for Image Manipulation Localization', '', '', '', '', '', 'We present a novel framework, Spatial Pyramid Attention Network (SPAN) for detection and localization of multiple types of image manipulations. The proposed architecture efficiently and effectively models the relationship between image patches at multiple scales by constructing a pyramid of local self-attention blocks. The design includes a novel position projection to encode the spatial positions of the patches. SPAN is trained on a generic, synthetic dataset but can also be fine tuned for specific datasets; The proposed method shows significant gains in performance on standard datasets over previous state-of-the-art methods.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_19');
INSERT INTO `paper` VALUES (12779, 'SPARK: Spatial-Aware Online Incremental Attack Against Visual Tracking', 'Online incremental attack', 'Visual object tracking', 'Adversarial attack', '', '', 'Adversarial attacks of deep neural networks have been intensively studied on image, audio, and natural language classification tasks. Nevertheless, as a typical while important real-world application, the adversarial attacks of online video tracking that traces an object’s moving trajectory instead of its category are rarely explored. In this paper, we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along with an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). To this end, we first propose a spatial-aware basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C&W, and comprehensively analyze the attacking performance. We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) real-time trackers require the attack to satisfy a certain level of efficiency. To address these challenges, we further propose the spatial-aware online inc remental attac k (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible. In addition, as an optimization-based method, SPARK quickly converges to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks. The in-depth evaluation of the state-of-the-art trackers (i.e., SiamRPN++ with AlexNet, MobileNetv2, and ResNet-50, and SiamDW) on OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and transferability of SPARK in misleading the trackers under both UA and TA with minor perturbations.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_13');
INSERT INTO `paper` VALUES (12780, 'Sparse Adversarial Attack via Perturbation Factorization', 'Perturbation factorization', 'Sparse adversarial attack', 'Mixed integer programming', '', '', 'This work studies the sparse adversarial attack, which aims to generate adversarial perturbations onto partial positions of one benign image, such that the perturbed image is incorrectly predicted by one deep neural network (DNN) model. The sparse adversarial attack involves two challenges, i.e., where to perturb, and how to determine the perturbation magnitude. Many existing works determined the perturbed positions manually or heuristically, and then optimized the magnitude using a proper algorithm designed for the dense adversarial attack. In this work, we propose to factorize the perturbation at each pixel to the product of two variables, including the perturbation magnitude and one binary selection factor (i.e., 0 or 1). One pixel is perturbed if its selection factor is 1, otherwise not perturbed. Based on this factorization, we formulate the sparse attack problem as a mixed integer programming (MIP) to jointly optimize the binary selection factors and continuous perturbation magnitudes of all pixels, with a cardinality constraint on selection factors to explicitly control the degree of sparsity. Besides, the perturbation factorization provides the extra flexibility to incorporate other meaningful constraints on selection factors or magnitudes to achieve some desired performance, such as the group-wise sparsity or the enhanced visual imperceptibility. We develop an efficient algorithm by equivalently reformulating the MIP problem as a continuous optimization problem. Extensive experiments demonstrate the superiority of the proposed method over several state-of-the-art sparse attack methods. The implementation of the proposed method is available at https://github.com/wubaoyuan/Sparse-Adversarial-Attack.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_3');
INSERT INTO `paper` VALUES (12781, 'Sparse-to-Dense Depth Completion Revisited: Sampling Strategy and Graph Construction', 'Depth completion', 'Graph neural network', 'Poisson disk sampling', 'Sparse-to-dense', '', 'Depth completion is a widely studied problem of predicting a dense depth map from a sparse set of measurements and a single RGB image. In this work, we approach this problem by addressing two issues that have been under-researched in the open literature: sampling strategy (data term) and graph construction (prior term). First, instead of the popular random sampling strategy, we suggest that Poisson disk sampling is a much more effective solution to create sparse depth map from a dense version. We experimentally compare a class of quasi-random sampling strategies and demonstrate that an optimized sampling strategy can significantly improve the performance of depth completion for the same number of sparse samples. Second, instead of the traditional square kernel, we suggest that dynamic construction of local neighborhood is a better choice for interpolating the missing values. More specifically, we proposed an end-to-end network with a graph convolution module. Since the neighborhood relationship of 3D points is more effectively exploited by our novel graph convolution module, our approach has achieved not only state-of-the-art results for depth completion of indoor scenes but also better generalization ability than other competing methods.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_41');
INSERT INTO `paper` VALUES (12782, 'Spatial Attention Pyramid Network for Unsupervised Domain Adaptation', 'Unsupervised domain adaptation', 'Spatial attention pyramid', 'Object detection', 'Semantic segmentation', 'Instance segmentation', 'Unsupervised domain adaptation is critical in various computer vision tasks, such as object detection, instance segmentation, and semantic segmentation, which aims to alleviate performance degradation caused by domain-shift. Most of previous methods rely on a single-mode distribution of source and target domains to align them with adversarial learning, leading to inferior results in various scenarios. To that end, in this paper, we design a new spatial attention pyramid network for unsupervised domain adaptation. Specifically, we first build the spatial pyramid representation to capture context information of objects at different scales. Guided by the task-specific information, we combine the dense global structure representation and local texture patterns at each spatial location effectively using the spatial attention mechanism. In this way, the network is enforced to focus on the discriminative regions with context information for domain adaptation. We conduct extensive experiments on various challenging datasets for unsupervised domain adaptation on object detection, instance segmentation, and semantic segmentation, which demonstrates that our method performs favorably against the state-of-the-art methods by a large margin. Our source code is available at https://isrc.iscas.ac.cn/gitlab/research/domain-adaption.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_29');
INSERT INTO `paper` VALUES (12783, 'Spatial Geometric Reasoning for Room Layout Estimation via Deep Reinforcement Learning', 'Room layout estimation', 'Reinforcement learning', '', '', '', 'Unlike most existing works that define room layout on a 2D image, we model the layout in 3D as a configuration of the camera and the room. Our spatial geometric representation with only seven variables is more concise but effective, and more importantly enables direct 3D reasoning, e.g. how the camera is positioned relative to the room. This is particularly valuable in applications such as indoor robot navigation. We formulate the problem as a Markov decision process, in which the layout is incrementally adjusted based on the difference between the current layout and the target image, and the policy is learned via deep reinforcement learning. Our framework is end-to-end trainable, requiring no extra optimization, and achieves competitive performance on two challenging room layout datasets.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_33');
INSERT INTO `paper` VALUES (12784, 'Spatial Hierarchy Aware Residual Pyramid Network for Time-of-Flight Depth Denoising', 'Time-of-Flight', 'Multi-Path Interference', 'Spatial hierarchy', 'Residual pyramid', 'Depth denoising', 'Time-of-Flight (ToF) sensors have been increasingly used on mobile devices for depth sensing. However, the existence of noise, such as Multi-Path Interference (MPI) and shot noise, degrades the ToF imaging quality. Previous CNN-based methods remove ToF depth noise without considering the spatial hierarchical structure of the scene, which leads to failures in obtaining high quality depth images from a complex scene. In this paper, we propose a Spatial Hierarchy Aware Residual Pyramid Network, called SHARP-Net, to remove the depth noise by fully exploiting the geometry information of the scene in different scales. SHARP-Net first introduces a Residual Regression Module, which utilizes the depth images and amplitude images as the input, to calculate the depth residual progressively. Then, a Residual Fusion Module, summing over depth residuals from all scales, is imported to refine the depth residual by fusing multi-scale geometry information. Finally, shot noise is further eliminated by a Kernel Prediction Network. Experimental results demonstrate that our method significantly outperforms state-of-the-art ToF depth denoising methods on both synthetic and realistic datasets. The source code is available at https://github.com/ashesknight/tof-mpi-remove.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_3');
INSERT INTO `paper` VALUES (12785, 'Spatial-Adaptive Network for Single Image Denoising', 'Image denoising', 'Image restoration', 'Image processing', '', '', 'Previous works have shown that convolutional neural networks can achieve good performance in image denoising tasks. However, limited by the local rigid convolutional operation, these methods lead to oversmoothing artifacts. A deeper network structure could alleviate these problems, but at the cost of additional computational overhead. In this paper, we propose a novel spatial-adaptive denoising network (SADNet) for efficient single image blind noise removal. To adapt to changes in spatial textures and edges, we design a residual spatial-adaptive block. Deformable convolution is introduced to sample the spatially related features for weighting. An encoder-decoder structure with a context block is introduced to capture multiscale information. By conducting noise removal from coarse to fine, a high-quality noise-free image is obtained. We apply our method to both synthetic and real noisy image datasets. The experimental results demonstrate that our method outperforms the state-of-the-art denoising methods both quantitatively and visually.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_11');
INSERT INTO `paper` VALUES (12786, 'Spatial-Angular Interaction for Light Field Image Super-Resolution', 'Light field imaging', 'Super-resolution', 'Feature decoupling', 'Spatial-angular interaction', '', 'Light field (LF) cameras record both intensity and directions of light rays, and capture scenes from a number of viewpoints. Both information within each perspective (i.e., spatial information) and among different perspectives (i.e., angular information) is beneficial to image super-resolution (SR). In this paper, we propose a spatial-angular interactive network (namely, LF-InterNet) for LF image SR. Specifically, spatial and angular features are first separately extracted from input LFs, and then repetitively interacted to progressively incorporate spatial and angular information. Finally, the interacted features are fused to super-resolve each sub-aperture image. Experimental results demonstrate the superiority of LF-InterNet over the state-of-the-art methods, i.e., our method can achieve high PSNR and SSIM scores with low computational cost, and recover faithful details in the reconstructed images.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_18');
INSERT INTO `paper` VALUES (12787, 'Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation', 'Sparse convolution', 'Sparse sampling', 'Feature interpolation', '', '', 'In the feature maps of CNNs, there commonly exists considerable spatial redundancy that leads to much repetitive processing. Towards reducing this superfluous computation, we propose to compute features only at sparsely sampled locations, which are probabilistically chosen according to activation responses, and then densely reconstruct the feature map with an efficient interpolation procedure. With this sampling-interpolation scheme, our network avoids expending computation on spatial locations that can be effectively interpolated, while being robust to activation prediction errors through broadly distributed sampling. A technical challenge of this sampling-based approach is that the binary decision variables for representing discrete sampling locations are non-differentiable, making them incompatible with backpropagation. To circumvent this issue, we make use of a reparameterization trick based on the Gumbel-Softmax distribution, with which backpropagation can iterate these variables towards binary values. The presented network is experimentally shown to save substantial computation while maintaining accuracy over a variety of computer vision tasks.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_31');
INSERT INTO `paper` VALUES (12788, 'Spatially Aware Multimodal Transformers for TextVQA', 'VQA', 'TextVQA', 'Self-attention', '', '', 'Textual cues are essential for everyday tasks like buying groceries and using public transport. To develop this assistive technology, we study the TextVQA task, i.e., reasoning about text in images to answer a question. Existing approaches are limited in their use of spatial relations and rely on fully-connected transformer-based architectures to implicitly learn the spatial structure of a scene. In contrast, we propose a novel spatially aware self-attention layer such that each visual entity only looks at neighboring entities defined by a spatial graph. Further, each head in our multi-head self-attention layer focuses on a different subset of relations. Our approach has two advantages: (1) each head considers local context instead of dispersing the attention amongst all visual entities; (2) we avoid learning redundant features. We show that our model improves the absolute accuracy of current state-of-the-art methods on TextVQA by 2.2% overall over an improved baseline, and 4.62% on questions that involve spatial reasoning and can be answered correctly using OCR tokens. Similarly on ST-VQA, we improve the absolute accuracy by 4.2%. We further show that spatially aware self-attention improves visual grounding.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_41');
INSERT INTO `paper` VALUES (12789, 'Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory Prediction', 'Trajectory prediction', 'Transformer', 'Graph neural networks', '', '', 'Understanding crowd motion dynamics is critical to real-world applications, e.g., surveillance systems and autonomous driving. This is challenging because it requires effectively modeling the socially aware crowd spatial interaction and complex temporal dependencies. We believe attention is the most important factor for trajectory prediction. In this paper, we present STAR, a Spatio-Temporal grAph tRansformer framework, which tackles trajectory prediction by only attention mechanisms. STAR models intra-graph crowd interaction by TGConv, a novel Transformer-based graph convolution mechanism. The inter-graph temporal dependencies are modeled by separate temporal Transformers. STAR captures complex spatio-temporal interactions by interleaving between spatial and temporal Transformers. To calibrate the temporal prediction for the long-lasting effect of disappeared pedestrians, we introduce a read-writable external memory module, consistently being updated by the temporal Transformer. We show that with only attention mechanism, STAR achieves the state-of-the-art performance on 5 commonly used real-world pedestrian prediction datasets (code available at https://github.com/Majiker/STAR).', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_30');
INSERT INTO `paper` VALUES (12790, 'Spatio-Temporal Handwriting Imitation', 'Offline handwriting generation', 'Style transfer', 'Forgery', 'Handwriting synthesis', '', 'Most people think that their handwriting is unique and cannot be imitated by machines, especially not using completely new content. Current cursive handwriting synthesis is visually limited or needs user interaction. We show that subdividing the process into smaller subtasks makes it possible to imitate someone’s handwriting with a high chance to be visually indistinguishable for humans. Therefore, a given handwritten sample will be used as the target style. This sample is transferred to an online sequence. Then, a method for online handwriting synthesis is used to produce a new realistic-looking text primed with the online input sequence. This new text is rendered and style-adapted to the input pen. We show the effectiveness of the pipeline by generating in- and out-of-vocabulary handwritten samples that are validated in a comprehensive user study. Additionally, we show that also a typical writer identification system can partially be fooled by the created fake handwritings.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_38');
INSERT INTO `paper` VALUES (12791, 'Spatiotemporal Attacks for Embodied Agents', 'Embodied agents', 'Spatiotemporal perturbations', '3D adversarial examples', '', '', 'Adversarial attacks are valuable for providing insights into the blind-spots of deep learning models and help improve their robustness. Existing work on adversarial attacks have mainly focused on static scenes; however, it remains unclear whether such attacks are effective against embodied agents, which could navigate and interact with a dynamic environment. In this work, we take the first step to study adversarial attacks for embodied agents. In particular, we generate spatiotemporal perturbations to form 3D adversarial examples, which exploit the interaction history in both the temporal and spatial dimensions. Regarding the temporal dimension, since agents make predictions based on historical observations, we develop a trajectory attention module to explore scene view contributions, which further help localize 3D objects appeared with highest stimuli. By conciliating with clues from the temporal dimension, along the spatial dimension, we adversarially perturb the physical properties (e.g., texture and 3D shape) of the contextual objects that appeared in the most important scene views. Extensive experiments on the EQA-v1 dataset for several embodied tasks in both the white-box and black-box settings have been conducted, which demonstrate that our perturbations have strong attack and generalization abilities (Our code can be found at https://github.com/liuaishan/SpatiotemporalAttack).', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_8');
INSERT INTO `paper` VALUES (12792, 'Speech-Driven Facial Animation Using Cascaded GANs for Learning of Motion and Texture', 'Realistic facial animation', 'Meta-learning', 'Cascaded GAN', '', '', 'Speech-driven facial animation methods should produce accurate and realistic lip motions with natural expressions and realistic texture portraying target-specific facial characteristics. Moreover, the methods should also be adaptable to any unknown faces and speech quickly during inference. Current state-of-the-art methods fail to generate realistic animation from any speech on unknown faces due to their poor generalization over different facial characteristics, languages, and accents. Some of these failures can be attributed to the end-to-end learning of the complex relationship between the multiple modalities of speech and the video. In this paper, we propose a novel strategy where we partition the problem and learn the motion and texture separately. Firstly, we train a GAN network to learn the lip motion in a canonical landmark using DeepSpeech features and induce eye-blinks before transferring the motion to the person-specific face. Next, we use another GAN based texture generator network to generate high fidelity face corresponding to the motion on person-specific landmark. We use meta-learning to make the texture generator GAN more flexible to adapt to the unknown subject’s traits of the face during inference. Our method gives significantly improved facial animation than the state-of-the-art methods and generalizes well across the different datasets, different languages, and accents, and also works reliably well in presence of noises in the speech.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_25');
INSERT INTO `paper` VALUES (12793, 'Spherical Feature Transform for Deep Metric Learning', '', '', '', '', '', 'Data augmentation in feature space is effective to increase data diversity. Previous methods assume that different classes have the same covariance in their feature distributions. Thus, feature transform between different classes is performed via translation. However, this approach is no longer valid for recent deep metric learning scenarios, where feature normalization is widely adopted and all features lie on a hypersphere.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_25');
INSERT INTO `paper` VALUES (12794, 'Spike-FlowNet: Event-Based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks', 'Event-based vision', 'Optical flow estimation', 'Hybrid network', 'Spiking neural network', 'Self-supervised learning', 'Event-based cameras display great potential for a variety of tasks such as high-speed motion detection and navigation in low-light environments where conventional frame-based cameras suffer critically. This is attributed to their high temporal resolution, high dynamic range, and low-power consumption. However, conventional computer vision methods as well as deep Analog Neural Networks (ANNs) are not suited to work well with the asynchronous and discrete nature of event camera outputs. Spiking Neural Networks (SNNs) serve as ideal paradigms to handle event camera outputs, but deep SNNs suffer in terms of performance due to the spike vanishing phenomenon. To overcome these issues, we present Spike-FlowNet, a deep hybrid neural network architecture integrating SNNs and ANNs for efficiently estimating optical flow from sparse event camera outputs without sacrificing the performance. The network is end-to-end trained with self-supervised learning on Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Spike-FlowNet outperforms its corresponding ANN-based method in terms of the optical flow prediction capability while providing significant computational efficiency.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_22');
INSERT INTO `paper` VALUES (12795, 'Spiral Generative Network for Image Extrapolation', 'Image extrapolation', 'GAN', 'cGAN', 'SpiralNet', '', 'In this paper, motivated by human natural ability to perceive unseen surroundings imaginatively, we propose a novel Spiral Generative Network, SpiralNet, to perform image extrapolation in a spiral manner, which regards extrapolation as an evolution process growing from an input sub-image along a spiral curve to an expanded full image. Our SpiralNet, consisting of ImagineGAN and SliceGAN, disentangles image extrapolation problem into two independent sub-tasks as semantic structure prediction (via ImagineGAN) and contextual detail generation (via SliceGAN), making the whole task more tractable. The design of SliceGAN implicitly harnesses the correlation between generated contents and extrapolating direction, divide-and-conquer while generation-by-parts. Extensive experiments on datasets covering both objects and scenes under different cases show that our method achieves state-of-the-art performance on image extrapolation. We also conduct ablation study to validate efficacy of our design. Our code is available at https://github.com/zhenglab/spiralnet.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_41');
INSERT INTO `paper` VALUES (12796, 'SPL-MLL: Selecting Predictable Landmarks for Multi-label Learning', 'Multi-label learning', 'Predictable landmarks', 'A unified framework', '', '', 'Although significant progress achieved, multi-label classification is still challenging due to the complexity of correlations among different labels. Furthermore, modeling the relationships between input and some (dull) classes further increases the difficulty of accurately predicting all possible labels. In this work, we propose to select a small subset of labels as landmarks which are easy to predict according to input (predictable) and can well recover the other possible labels (representative). Different from existing methods which separate the landmark selection and landmark prediction in the 2-step manner, the proposed algorithm, termed Selecting Predictable Landmarks for Multi-Label Learning (SPL-MLL), jointly conducts landmark selection, landmark prediction, and label recovery in a unified framework, to ensure both the representativeness and predictableness for selected landmarks. We employ the Alternating Direction Method (ADM) to solve our problem. Empirical studies on real-world datasets show that our method achieves superior classification performance over other state-of-the-art methods.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_45');
INSERT INTO `paper` VALUES (12797, 'Splitting Vs. Merging: Mining Object Regions with Discrepancy and Intersection Loss for Weakly Supervised Semantic Segmentation', 'Weakly-supervised learning', 'Deep Convolutional Neural Network (DCNN)', 'Semantic segmentation', '', '', 'In this paper we focus on the task of weakly-supervised semantic segmentation supervised with image-level labels. Since the pixel-level annotation is not available in the training process, we rely on region mining models to estimate the pseudo-masks from the image-level labels. Thus, in order to improve the final segmentation results, we aim to train a region-mining model which could accurately and completely highlight the target object regions for generating high-quality pseudo-masks. However, the region mining models are likely to only highlight the most discriminative regions instead of the entire objects. In this paper, we aim to tackle this problem from a novel perspective of optimization process. We propose a Splitting vs. Merging optimization strategy, which is mainly composed of the Discrepancy loss and the Intersection loss. The proposed Discrepancy loss aims at mining out regions of different spatial patterns instead of only the most discriminative region, which leads to the splitting effect. The Intersection loss aims at mining the common regions of the different maps, which leads to the merging effect. Our Splitting vs. Merging strategy helps to expand the output heatmap of the region mining model to the object scale. Finally, by training the segmentation model with the masks generated by our Splitting vs Merging strategy, we achieve the state-of-the-art weakly-supervised segmentation results on the Pascal VOC 2012 benchmark.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_40');
INSERT INTO `paper` VALUES (12798, 'SPOT: Selective Point Cloud Voting for Better Proposal in Point Cloud Object Detection', '', '', '', '', '', 'The sparsity of point clouds limits deep learning models on capturing long-range dependencies, which makes features extracted by the models ambiguous. In point cloud object detection, ambiguous features make it hard for detectors to locate object centers (Fig. 1) and finally lead to bad detection results. In this work, we propose Selective Point clOud voTing (SPOT) module, a simple effective component that can be easily trained end-to-end in point cloud object detectors to solve this problem. Inspired by probabilistic Hough voting, SPOT incorporates an attention mechanism that helps detectors focus on less ambiguous features and preserves their diversity of mapping to multiple object centers. For evaluating our module, we implement SPOT on advanced baseline detectors and test on two benchmark datasets of clutter indoor scenes, ScanNet and SUN RGB-D. Baselines enhanced by our module can stably improve results in agreement by a large margin and achieve new state-or-the-art detection, especially under more strict evaluation metric that adopts larger IoU threshold, implying our module is the key leading to high-quality object detection in point clouds.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_14');
INSERT INTO `paper` VALUES (12799, 'SpotPatch: Parameter-Efficient Transfer Learning for Mobile Object Detection', '', '', '', '', '', 'As mobile hardware technology advances, on-device computation is becoming more and more affordable.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_41');
INSERT INTO `paper` VALUES (12800, 'Square Attack: A Query-Efficient Black-Box Adversarial Attack via Random Search', '', '', '', '', '', 'We propose the Square Attack, a score-based black-box \\(l_2\\)- and \\(l_\\infty \\)-adversarial attack that does not rely on local gradient information and thus is not affected by gradient masking. Square Attack is based on a randomized search scheme which selects localized square-shaped updates at random positions so that at each iteration the perturbation is situated approximately at the boundary of the feasible set. Our method is significantly more query efficient and achieves a higher success rate compared to the state-of-the-art methods, especially in the untargeted setting. In particular, on ImageNet we improve the average query efficiency in the untargeted setting for various deep networks by a factor of at least 1.8 and up to 3 compared to the recent state-of-the-art \\(l_\\infty \\)-attack of Al-Dujaili & O’Reilly (2020). Moreover, although our attack is black-box, it can also outperform gradient-based white-box attacks on the standard benchmarks achieving a new state-of-the-art in terms of the success rate. The code of our attack is available at https://github.com/max-andr/square-attack.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_29');
INSERT INTO `paper` VALUES (12801, 'SqueezeSegV3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation', 'Point-cloud segmentation', 'Spatially-adaptive convolution', '', '', '', 'LiDAR point-cloud segmentation is an important problem for many applications. For large-scale point cloud segmentation, the de facto method is to project a 3D point cloud to get a 2D LiDAR image and use convolutions to process it. Despite the similarity between regular RGB and LiDAR images, we are the first to discover that the feature distribution of LiDAR images changes drastically at different image locations. Using standard convolutions to process such LiDAR images is problematic, as convolution filters pick up local features that are only active in specific regions in the image. As a result, the capacity of the network is under-utilized and the segmentation performance decreases. To fix this, we propose Spatially-Adaptive Convolution (SAC) to adopt different filters for different locations according to the input image. SAC can be computed efficiently since it can be implemented as a series of element-wise multiplications, im2col, and standard convolution. It is a general framework such that several previous methods can be seen as special cases of SAC. Using SAC, we build SqueezeSegV3 for LiDAR point-cloud segmentation and outperform all previous published methods by at least 2.0% mIoU on the SemanticKITTI benchmark. Code and pretrained model are available at https://github.com/chenfengxu714/SqueezeSegV3.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_1');
INSERT INTO `paper` VALUES (12802, 'SRFlow: Learning the Super-Resolution Space with Normalizing Flow', '', '', '', '', '', 'Super-resolution is an ill-posed problem, since it allows for multiple predictions for a given low-resolution image. This fundamental fact is largely ignored by state-of-the-art deep learning based approaches. These methods instead train a deterministic mapping using combinations of reconstruction and adversarial losses. In this work, we therefore propose SRFlow: a normalizing flow based super-resolution method capable of learning the conditional distribution of the output given the low-resolution input. Our model is trained in a principled manner using a single loss, namely the negative log-likelihood. SRFlow therefore directly accounts for the ill-posed nature of the problem, and learns to predict diverse photo-realistic high-resolution images. Moreover, we utilize the strong image posterior learned by SRFlow to design flexible image manipulation techniques, capable of enhancing super-resolved images by, e.g., transferring content from other images. We perform extensive experiments on faces, as well as on super-resolution in general. SRFlow outperforms state-of-the-art GAN-based approaches in terms of both PSNR and perceptual quality metrics, while allowing for diversity through the exploration of the space of super-resolved solutions. Code: git.io/Jfpyu.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_42');
INSERT INTO `paper` VALUES (12803, 'SRNet: Improving Generalization in 3D Human Pose Estimation with a Split-and-Recombine Approach', 'Human pose estimation', '2D to 3D', 'Long-tailed distribution', '', '', 'Human poses that are rare or unseen in a training set are challenging for a network to predict. Similar to the long-tailed distribution problem in visual recognition, the small number of examples for such poses limits the ability of networks to model them. Interestingly, local pose distributions suffer less from the long-tail problem, i.e., local joint configurations within a rare pose may appear within other poses in the training set, making them less rare. We propose to take advantage of this fact for better generalization to rare and unseen poses. To be specific, our method splits the body into local regions and processes them in separate network branches, utilizing the property that a joint’s position depends mainly on the joints within its local body region. Global coherence is maintained by recombining the global context from the rest of the body into each branch as a low-dimensional vector. With the reduced dimensionality of less relevant body areas, the training set distribution within network branches more closely reflects the statistics of local poses instead of global body poses, without sacrificing information important for joint inference. The proposed split-and-recombine approach, called SRNet, can be easily adapted to both single-image and temporal models, and it leads to appreciable improvements in the prediction of rare and unseen poses.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_30');
INSERT INTO `paper` VALUES (12804, 'SSCGAN: Facial Attribute Editing via Style Skip Connections', 'Facial attribute editing', 'Style feature', 'Skip connection', '', '', 'Existing facial attribute editing methods typically employ an encoder-decoder architecture where the attribute information is expressed as a conditional one-hot vector spatially concatenated with the image or intermediate feature maps. However, such operations only learn the local semantic mapping but ignore global facial statistics. In this work, we focus on solving this issue by editing the channel-wise global information denoted as the style feature. We develop a style skip connection based generative adversarial network, referred to as SSCGAN which enables accurate facial attribute manipulation. Specifically, we inject the target attribute information into multiple style skip connection paths between the encoder and decoder. Each connection extracts the style feature of the latent feature maps in the encoder and then performs a residual learning based mapping function in the global information space guided by the target attributes. In the following, the adjusted style feature will be utilized as the conditional information for instance normalization to transform the corresponding latent feature maps in the decoder. In addition, to avoid the vanishing of spatial details (e.g. hairstyle or pupil locations), we further introduce the skip connection based spatial information transfer module. Through the global-wise style and local-wise spatial information manipulation, the proposed method can produce better results in terms of attribute generation accuracy and image quality. Experimental results demonstrate the proposed algorithm performs favorably against the state-of-the-art methods.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_25');
INSERT INTO `paper` VALUES (12805, 'SSN: Shape Signature Networks for Multi-class Object Detection from Point Clouds', '', '', '', '', '', 'Multi-class 3D object detection aims to localize and classify objects of multiple categories from point clouds. Due to the nature of point clouds, i.e. unstructured, sparse and noisy, some features benefitting multi-class discrimination are underexploited, such as shape information. In this paper, we propose a novel 3D shape signature to explore the shape information from point clouds. By incorporating operations of symmetry, convex hull and Chebyshev fitting, the proposed shape signature is not only compact and effective but also robust to the noise, which serves as a soft constraint to improve the feature capability of multi-class discrimination. Based on the proposed shape signature, we develop the shape signature networks (SSN) for 3D object detection, which consist of pyramid feature encoding part, shape-aware grouping heads and explicit shape encoding objective. Experiments show that the proposed method performs remarkably better than existing methods on two large-scale datasets. Furthermore, our shape signature can act as a plug-and-play component and ablation study shows its effectiveness and good scalability (Source code at SSN and also available at mmdetection3d soon.).', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_35');
INSERT INTO `paper` VALUES (12806, 'Stable Low-Rank Tensor Decomposition for Compression of Convolutional Neural Network', 'Convolutional neural network acceleration', 'Low-rank tensor decomposition', 'Sensitivity', 'Degeneracy correction', '', 'Most state-of-the-art deep neural networks are overparameterized and exhibit a high computational cost. A straightforward approach to this problem is to replace convolutional kernels with its low-rank tensor approximations, whereas the Canonical Polyadic tensor Decomposition is one of the most suited models. However, fitting the convolutional tensors by numerical optimization algorithms often encounters diverging components, i.e., extremely large rank-one tensors but canceling each other. Such degeneracy often causes the non-interpretable result and numerical instability for the neural network ne-tuning. This paper is the first study on degeneracy in the tensor decomposition of convolutional kernels. We present a novel method, which can stabilize the low-rank approximation of convolutional kernels and ensure efficient compression while preserving the high-quality performance of the neural networks. We evaluate our approach on popular CNN architectures for image classification and show that our method results in much lower accuracy degradation and provides consistent performance.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_31');
INSERT INTO `paper` VALUES (12807, 'Stacking Networks Dynamically for Image Restoration Based on the Plug-and-Play Framework', 'Low-level vision', 'Image restoration', 'Plug-and-play', '', '', 'Recently, stacked networks show powerful performance in Image Restoration, such as challenging motion deblurring problems. However, the number of stacking levels is a hyper-parameter fine-tuned manually, making the stacking levels static during training without theoretical explanations for optimal settings. To address this challenge, we leverage the iterative process of the traditional plug-and-play method to provide a dynamic stacked network for Image Restoration. Specifically, a new degradation model with a novel update scheme is designed to integrate the deep neural network as the prior within the plug-and-play model. Compared with static stacked networks, our models are stacked dynamically during training via iterations, guided by a solid mathematical explanation. Theoretical proof on the convergence of the dynamic stacking process is provided. Experiments on the noise dataset BSD68, Set12, and motion blur dataset GoPro demonstrate that our framework outperforms the state-of-the-art in terms of PSNR and SSIM score without extra training process.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_27');
INSERT INTO `paper` VALUES (12808, 'STAR: Sparse Trained Articulated Human Body Regressor', '', '', '', '', '', 'The SMPL body model is widely used for the estimation, synthesis, and analysis of 3D human pose and shape. While popular, we show that SMPL has several limitations and introduce STAR, which is quantitatively and qualitatively superior to SMPL. First, SMPL has a huge number of parameters resulting from its use of global blend shapes. These dense pose-corrective offsets relate every vertex on the mesh to all the joints in the kinematic tree, capturing spurious long-range correlations. To address this, we define per-joint pose correctives and learn the subset of mesh vertices that are influenced by each joint movement. This sparse formulation results in more realistic deformations and significantly reduces the number of model parameters to 20% of SMPL. When trained on the same data as SMPL, STAR generalizes better despite having many fewer parameters. Second, SMPL factors pose-dependent deformations from body shape while, in reality, people with different shapes deform differently. Consequently, we learn shape-dependent pose-corrective blend shapes that depend on both body pose and BMI. Third, we show that the shape space of SMPL is not rich enough to capture the variation in the human population. We address this by training STAR with an additional 10,000 scans of male and female subjects, and show that this results in better model generalization. STAR is compact, generalizes better to new bodies and is a drop-in replacement for SMPL. STAR is publicly available for research purposes at http://star.is.tue.mpg.de.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_36');
INSERT INTO `paper` VALUES (12809, 'STEm-Seg: Spatio-Temporal Embeddings for Instance Segmentation in Videos', '', '', '', '', '', 'Existing methods for instance segmentation in videos typically involve multi-stage pipelines that follow the tracking-by-detection paradigm and model a video clip as a sequence of images. Multiple networks are used to detect objects in individual frames, and then associate these detections over time. Hence, these methods are often non-end-to-end trainable and highly tailored to specific tasks. In this paper, we propose a different approach that is well-suited to a variety of tasks involving instance segmentation in videos. In particular, we model a video clip as a single 3D spatio-temporal volume, and propose a novel approach that segments and tracks instances across space and time in a single stage. Our problem formulation is centered around the idea of spatio-temporal embeddings which are trained to cluster pixels belonging to a specific object instance over an entire video clip. To this end, we introduce (i) novel mixing functions that enhance the feature representation of spatio-temporal embeddings, and (ii) a single-stage, proposal-free network that can reason about temporal context. Our network is trained end-to-end to learn spatio-temporal embeddings as well as parameters required to cluster these embeddings, thus simplifying inference. Our method achieves state-of-the-art results across multiple datasets and tasks. Code and models are available at https://github.com/sabarim/STEm-Seg.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_10');
INSERT INTO `paper` VALUES (12810, 'Stereo Event-Based Particle Tracking Velocimetry for 3D Fluid Flow Reconstruction', 'Fluid imaging', 'Event-based camera', 'Particle Imaging Velocimetry', 'Stereo-PTV', 'Optimization', 'Existing Particle Imaging Velocimetry techniques require the use of high-speed cameras to reconstruct time-resolved fluid flows. These cameras provide high-resolution images at high frame rates, which generates bandwidth and memory issues. By capturing only changes in the brightness with a very low latency and at low data rate, event-based cameras have the ability to tackle such issues. In this paper, we present a new framework that retrieves dense 3D measurements of the fluid velocity field using a pair of event-based cameras. First, we track particles inside the two event sequences in order to estimate their 2D velocity in the two sequences of images. A stereo-matching step is then performed to retrieve their 3D positions. These intermediate outputs are incorporated into an optimization framework that also includes physically plausible regularizers, in order to retrieve the 3D velocity field. Extensive experiments on both simulated and real data demonstrate the efficacy of our approach.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_3');
INSERT INTO `paper` VALUES (12811, 'Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction', 'Stochastic bundle adjustment', 'Clustering', '3D reconstruction', '', '', 'Current bundle adjustment solvers such as the Levenberg-Marquardt (LM) algorithm are limited by the bottleneck in solving the Reduced Camera System (RCS) whose dimension is proportional to the camera number. When the problem is scaled up, this step is neither efficient in computation nor manageable for a single compute node. In this work, we propose a stochastic bundle adjustment algorithm which seeks to decompose the RCS approximately inside the LM iterations to improve the efficiency and scalability. It first reformulates the quadratic programming problem of an LM iteration based on the clustering of the visibility graph by introducing the equality constraints across clusters. Then, we propose to relax it into a chance constrained problem and solve it through sampled convex program. The relaxation is intended to eliminate the interdependence between clusters embodied by the constraints, so that a large RCS can be decomposed into independent linear sub-problems. Numerical experiments on unordered Internet image sets and sequential SLAM image sets, as well as distributed experiments on large-scale datasets, have demonstrated the high efficiency and scalability of the proposed approach. Codes are released at https://github.com/zlthinker/STBA.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_22');
INSERT INTO `paper` VALUES (12812, 'Stochastic Fine-Grained Labeling of Multi-state Sign Glosses for Continuous Sign Language Recognition', '', '', '', '', '', 'In this paper, we propose novel stochastic modeling of various components of a continuous sign language recognition (CSLR) system that is based on the transformer encoder and connectionist temporal classification (CTC). Most importantly, We model each sign gloss with multiple states, and the number of states is a categorical random variable that follows a learned probability distribution, providing stochastic fine-grained labels for training the CTC decoder. We further propose a stochastic frame dropping mechanism and a gradient stopping method to deal with the severe overfitting problem in training the transformer model with CTC loss. These two methods also help reduce the training computation, both in terms of time and space, significantly. We evaluated our model on popular CSLR datasets, and show its effectiveness compared to the state-of-the-art methods.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_11');
INSERT INTO `paper` VALUES (12813, 'Stochastic Frequency Masking to Improve Super-Resolution and Denoising Networks', 'Image restoration', 'Super-resolution', 'Denoising', 'Kernel overfitting', '', 'Super-resolution and denoising are ill-posed yet fundamental image restoration tasks. In blind settings, the degradation kernel or the noise level are unknown. This makes restoration even more challenging, notably for learning-based methods, as they tend to overfit to the degradation seen during training. We present an analysis, in the frequency domain, of degradation-kernel overfitting in super-resolution and introduce a conditional learning perspective that extends to both super-resolution and denoising. Building on our formulation, we propose a stochastic frequency masking of images used in training to regularize the networks and address the overfitting problem. Our technique improves state-of-the-art methods on blind super-resolution with different synthetic kernels, real super-resolution, blind Gaussian denoising, and real-image denoising.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_44');
INSERT INTO `paper` VALUES (12814, 'Streaming Object Detection for 3-D Point Clouds', '', '', '', '', '', 'Autonomous vehicles operate in a dynamic environment, where the speed with which a vehicle can perceive and react impacts the safety and efficacy of the system. LiDAR provides a prominent sensory modality that informs many existing perceptual systems including object detection, segmentation, motion estimation, and action recognition. The latency for perceptual systems based on point cloud data can be dominated by the amount of time for a complete rotational scan (e.g. 100 ms). This built-in data capture latency is artificial, and based on treating the point cloud as a camera image in order to leverage camera-inspired architectures. However, unlike camera sensors, most LiDAR point cloud data is natively a streaming data source in which laser reflections are sequentially recorded based on the precession of the laser beam. In this work, we explore how to build an object detector that removes this artificial latency constraint, and instead operates on native streaming data in order to significantly reduce latency. This approach has the added benefit of reducing the peak computational burden on inference hardware by spreading the computation over the acquisition time for a scan. We demonstrate a family of streaming detection systems based on sequential modeling through a series of modifications to the traditional detection meta-architecture. We highlight how this model may achieve competitive if not superior predictive performance with state-of-the-art, traditional non-streaming detection systems while achieving significant latency gains (e.g. \\(1/15^\\text {th}\\)–\\(1/3^\\text {rd}\\) of peak latency). Our results show that operating on LiDAR data in its native streaming formulation offers several advantages for self driving object detection – advantages that we hope will be useful for any LiDAR perception system where minimizing latency is critical for safe and efficient operation.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_25');
INSERT INTO `paper` VALUES (12815, 'Structural Deep Metric Learning for Room Layout Estimation', 'Deep metric learning', 'Room layout estimation', 'Structured prediction', '', '', 'In this paper, we propose a structural deep metric learning (SDML) method for room layout estimation, which aims to recover the 3D spatial layout of a cluttered indoor scene from a monocular RGB image. Different from existing room layout estimation methods that solve a regression or per-pixel classification problem, we formulate the room layout estimation problem from a metric learning perspective where we explicitly model the structural relations across different images. We propose to learn a latent embedding space where the Euclidean distance can characterize the actual structural difference between the layouts of two rooms. We then minimize the discrepancy between an image and its ground-truth layout in the learned embedding space. We employ a metric model and a layout encoder to map the RGB images and the ground-truth layouts to the embedding space, respectively, and a layout decoder to map the embeddings to the corresponding layouts, where the whole framework is trained in an end-to-end manner. We perform experiments on the widely used Hedau and LSUN datasets and achieve state-of-the-art performance.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_43');
INSERT INTO `paper` VALUES (12816, 'Structural Plan of Indoor Scenes with Personalized Preferences', 'Interior layout', 'Personalised preferences', 'Conditional graph generation', 'Conditional scene instantiation', '', 'In this paper, we propose an assistive model that supports professional interior designers to produce industrial interior decoration solutions and to meet the personalized preferences of the property owners. The proposed model is able to automatically produce the layout of objects of a particular indoor scene according to property owners’ preferences. In particular, the model consists of the extraction of abstract graph, conditional graph generation, and conditional scene instantiation. We provide an interior layout dataset that contains real-world 11000 designs from professional designers. Our numerical results on the dataset demonstrate the effectiveness of the proposed model compared with the state-of-art methods.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_27');
INSERT INTO `paper` VALUES (12817, 'Structure-Aware Generation Network for Recipe Generation from Images', 'Structure learning', 'Text generation', 'Image-to-text', '', '', 'Sharing food has become very popular with the development of social media. For many real-world applications, people are keen to know the underlying recipes of a food item. In this paper, we are interested in automatically generating cooking instructions for food. We investigate an open research task of generating cooking instructions based on only food images and ingredients, which is similar to the image captioning task. However, compared with image captioning datasets, the target recipes are long-length paragraphs and do not have annotations on structure information. To address the above limitations, we propose a novel framework of Structure-aware Generation Network (SGN) to tackle the food recipe generation task. Our approach brings together several novel ideas in a systematic framework: (1) exploiting an unsupervised learning approach to obtain the sentence-level tree structure labels before training; (2) generating trees of target recipes from images with the supervision of tree structure labels learned from (1); and (3) integrating the inferred tree structures with the recipe generation procedure. Our proposed model can produce high-quality and coherent recipes, and achieve the state-of-the-art performance on the benchmark Recipe1M dataset.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_22');
INSERT INTO `paper` VALUES (12818, 'Structure-Aware Human-Action Generation', 'Action generation', 'Graph convolutional network', 'Self-attention', 'Generative adversarial networks (GAN)', '', 'Generating long-range skeleton-based human actions has been a challenging problem since small deviations of one frame can cause a malformed action sequence. Most existing methods borrow ideas from video generation, which naively treat skeleton nodes/joints as pixels of images without considering the rich inter-frame and intra-frame structure information, leading to potential distorted actions. Graph convolutional networks (GCNs) is a promising way to leverage structure information to learn structure representations. However, directly adopting GCNs to tackle such continuous action sequences both in spatial and temporal spaces is challenging as the action graph could be huge. To overcome this issue, we propose a variant of GCNs (SA-GCNs) to leverage the powerful self-attention mechanism to adaptively sparsify a complete action graph in the temporal space. Our method could dynamically attend to important past frames and construct a sparse graph to apply in the GCN framework, well-capturing the structure information in action sequences. Extensive experimental results demonstrate the superiority of our method on two standard human action datasets compared with existing methods.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_2');
INSERT INTO `paper` VALUES (12819, 'Structured Landmark Detection via Topology-Adapting Deep Graph Learning', 'Landmark detection', 'GCN', 'Adaptive topology', '', '', 'Image landmark detection aims to automatically identify the locations of predefined fiducial points. Despite recent success in this field, higher-ordered structural modeling to capture implicit or explicit relationships among anatomical landmarks has not been adequately exploited. In this work, we present a new topology-adapting deep graph learning approach for accurate anatomical facial and medical (e.g., hand, pelvis) landmark detection. The proposed method constructs graph signals leveraging both local image features and global shape features. The adaptive graph topology naturally explores and lands on task-specific structures which are learned end-to-end with two Graph Convolutional Networks (GCNs). Extensive experiments are conducted on three public facial image datasets (WFLW, 300W, and COFW-68) as well as three real-world X-ray medical datasets (Cephalometric (public), Hand and Pelvis). Quantitative results comparing with the previous state-of-the-art approaches across all studied datasets indicating the superior performance in both robustness and accuracy. Qualitative visualizations of the learned graph topologies demonstrate a physically plausible connectivity laying behind the landmarks.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_16');
INSERT INTO `paper` VALUES (12820, 'Structured3D: A Large Photo-Realistic Dataset for Structured 3D Modeling', 'Dataset', '3D structure', 'Photo-realistic rendering', '', '', 'Recently, there has been growing interest in developing learning-based methods to detect and utilize salient semi-global or global structures, such as junctions, lines, planes, cuboids, smooth surfaces, and all types of symmetries, for 3D scene modeling and understanding. However, the ground truth annotations are often obtained via human labor, which is particularly challenging and inefficient for such tasks due to the large number of 3D structure instances (e.g., line segments) and other factors such as viewpoints and occlusions. In this paper, we present a new synthetic dataset, Structured3D, with the aim of providing large-scale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks. We take advantage of the availability of professional interior designs and automatically extract 3D structures from them. We generate high-quality images with an industry-leading rendering engine. We use our synthetic dataset in combination with real images to train deep networks for room layout estimation and demonstrate improved performance on benchmark datasets.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_30');
INSERT INTO `paper` VALUES (12821, 'StructureFromGAN: Single Image 3D Model Reconstruction and Photorealistic Texturing', '3D object reconstruction', 'Neural renderer', 'Conditional GAN', '', '', 'We present a generative adversarial model for single photo 3D reconstruction and high resolution texturing. Our framework leverages a neural renderer and a 3D Morphable model of an object. We train our generator on the semantic labelling-to-image translation task. This allows our model to learn rich priors about object appearance and perform all-around texture and shape reconstruction from a single image. Our new generator architecture leverages a power of StyleGAN2 model for image-to-image translation with fine texture detail at the \\(1024 \\times 1024\\) resolution. We evaluate our framework quantitatively and qualitatively on Florence Face and Appolo Cars datasets on the tasks of car 3D reconstruction and texturing. Extensive experiments demonstrate that our framework achieves and surpasses the state-of-the-art in single photo 3D object reconstruction and texturing using 3D morphable models. We made our code publicly available (http://www.zefirus.org/StructureFromGAN).', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_40');
INSERT INTO `paper` VALUES (12822, 'Style Transfer for Co-speech Gesture Animation: A Multi-speaker Conditional-Mixture Approach', 'Gesture animation', 'Style transfer', 'Co-speech gestures', '', '', 'How can we teach robots or virtual assistants to gesture naturally? Can we go further and adapt the gesturing style to follow a specific speaker? Gestures that are naturally timed with corresponding speech during human communication are called co-speech gestures. A key challenge, called gesture style transfer, is to learn a model that generates these gestures for a speaking agent ‘A’ in the gesturing style of a target speaker ‘B’. A secondary goal is to simultaneously learn to generate co-speech gestures for multiple speakers while remembering what is unique about each speaker. We call this challenge style preservation. In this paper, we propose a new model, named Mix-StAGE, which trains a single model for multiple speakers while learning unique style embeddings for each speaker’s gestures in an end-to-end manner. A novelty of Mix-StAGE is to learn a mixture of generative models which allows for conditioning on the unique gesture style of each speaker. As Mix-StAGE disentangles style and content of gestures, gesturing styles for the same input speech can be altered by simply switching the style embeddings. Mix-StAGE also allows for style preservation when learning simultaneously from multiple speakers. We also introduce a new dataset, Pose-Audio-Transcript-Style (PATS), designed to study gesture generation and style transfer. Our proposed Mix-StAGE model significantly outperforms the previous state-of-the-art approach for gesture generation and provides a path towards performing gesture style transfer across multiple speakers. Link to code, data and videos: http://chahuja.com/mix-stage.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_15');
INSERT INTO `paper` VALUES (12823, 'StyleGAN2 Distillation for Feed-Forward Image Manipulation', 'Computer vision', 'StyleGAN2', 'Distillation', 'Synthetic data', '', 'StyleGAN2 is a state-of-the-art network in generating realistic images. Besides, it was explicitly trained to have disentangled directions in latent space, which allows efficient image manipulation by varying latent factors. Editing existing images requires embedding a given image into the latent space of StyleGAN2. Latent code optimization via backpropagation is commonly used for qualitative embedding of real world images, although it is prohibitively slow for many applications. We propose a way to distill a particular image manipulation of StyleGAN2 into image-to-image network trained in paired way. The resulting pipeline is an alternative to existing GANs, trained on unpaired data. We provide results of human faces’ transformation: gender swap, aging/rejuvenation, style transfer and image morphing. We show that the quality of generation using our method is comparable to StyleGAN2 backpropagation and current state-of-the-art methods in these particular tasks.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_11');
INSERT INTO `paper` VALUES (12824, 'Sub-center ArcFace: Boosting Face Recognition by Large-Scale Noisy Web Faces', 'Face recognition', 'Sub-class', 'Large-scale', 'Noisy data', '', 'Margin-based deep face recognition methods (e.g. SphereFace, CosFace, and ArcFace) have achieved remarkable success in unconstrained face recognition. However, these methods are susceptible to the massive label noise in the training data and thus require laborious human effort to clean the datasets. In this paper, we relax the intra-class constraint of ArcFace to improve the robustness to label noise. More specifically, we design K sub-centers for each class and the training sample only needs to be close to any of the K positive sub-centers instead of the only one positive center. The proposed sub-center ArcFace encourages one dominant sub-class that contains the majority of clean faces and non-dominant sub-classes that include hard or noisy faces. Extensive experiments confirm the robustness of sub-center ArcFace under massive real-world noise. After the model achieves enough discriminative power, we directly drop non-dominant sub-centers and high-confident noisy samples, which helps recapture intra-compactness, decrease the influence from noise, and achieve comparable performance compared to ArcFace trained on the manually cleaned dataset. By taking advantage of the large-scale raw web faces (Celeb500K), sub-center Arcface achieves state-of-the-art performance on IJB-B, IJB-C, MegaFace, and FRVT.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_43');
INSERT INTO `paper` VALUES (12825, 'Subtensor Quantization for Mobilenets', '', '', '', '', '', 'Quantization for deep neural networks (DNN) have enabled developers to deploy models with less memory and more efficient low-power inference. However, not all DNN designs are friendly to quantization. For example, the popular Mobilenet architecture has been tuned to reduce parameter size and computational latency with separable depthwise convolutions, but not all quantization algorithms work well and the accuracy can suffer against its float point versions. In this paper, we analyzed several root causes of quantization loss and proposed alternatives that do not rely on per-channel or training-aware approaches. We evaluate the image classification task on ImageNet dataset, and our post-training quantized 8-bit inference top-1 accuracy in within 0.7% of the floating point version.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_10');
INSERT INTO `paper` VALUES (12826, 'SumGraph: Video Summarization via Recursive Graph Modeling', 'Video summarization', 'Graph convolutional networks', 'Recursive graph refinement', '', '', 'The goal of video summarization is to select keyframes that are visually diverse and can represent a whole story of an input video. State-of-the-art approaches for video summarization have mostly regarded the task as a frame-wise keyframe selection problem by aggregating all frames with equal weight. However, to find informative parts of the video, it is necessary to consider how all the frames of the video are related to each other. To this end, we cast video summarization as a graph modeling problem. We propose recursive graph modeling networks for video summarization, termed SumGraph, to represent a relation graph, where frames are regarded as nodes and nodes are connected by semantic relationships among frames. Our networks accomplish this through a recursive approach to refine an initially estimated graph to correctly classify each node as a keyframe by reasoning the graph representation via graph convolutional networks. To leverage SumGraph in a more practical environment, we also present a way to adapt our graph modeling in an unsupervised fashion. With SumGraph, we achieved state-of-the-art performance on several benchmarks for video summarization in both supervised and unsupervised manners.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_39');
INSERT INTO `paper` VALUES (12827, 'Supervised Edge Attention Network for Accurate Image Instance Segmentation', 'Fully convolutional box head', 'Supervised edge attention module', 'IoU prediction branch', 'Instance segmentation', '', 'Effectively keeping boundary of the mask complete is important in instance segmentation. In this task, many works segment instance based on a bounding box from the box head, which means the quality of the detection also affects the completeness of the mask. To circumvent this issue, we propose a fully convolutional box head and a supervised edge attention module in mask head. The box head contains one new IoU prediction branch. It learns association between object features and detected bounding boxes to provide more accurate bounding boxes for segmentation. The edge attention module utilizes attention mechanism to highlight object and suppress background noise, and a supervised branch is devised to guide the network to focus on the edge of instances precisely. To evaluate the effectiveness, we conduct experiments on COCO dataset. Without bells and whistles, our approach achieves impressive and robust improvement compared to baseline models. Code is at https://github.com//IPIU-detection/SEANet.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_37');
INSERT INTO `paper` VALUES (12828, 'Suppress and Balance: A Simple Gated Network for Salient Object Detection', 'Salient object detection', 'Gated network', 'Dual branch', 'Fold-ASPP', '', 'Most salient object detection approaches use U-Net or feature pyramid networks (FPN) as their basic structures. These methods ignore two key problems when the encoder exchanges information with the decoder: one is the lack of interference control between them, the other is without considering the disparity of the contributions of different encoder blocks. In this work, we propose a simple gated network (GateNet) to solve both issues at once. With the help of multilevel gate units, the valuable context information from the encoder can be optimally transmitted to the decoder. We design a novel gated dual branch structure to build the cooperation among different levels of features and improve the discriminability of the whole network. Through the dual branch design, more details of the saliency map can be further restored. In addition, we adopt the atrous spatial pyramid pooling based on the proposed “Fold” operation (Fold-ASPP) to accurately localize salient objects of various scales. Extensive experiments on five challenging datasets demonstrate that the proposed model performs favorably against most state-of-the-art methods under different evaluation metrics.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_3');
INSERT INTO `paper` VALUES (12829, 'Suppressing Mislabeled Data via Grouping and Self-attention', 'Noisy-labeled data', 'Mixup', 'Noisy-robust learning', '', '', 'Deep networks achieve excellent results on large-scale clean data but degrade significantly when learning from noisy labels. To suppressing the impact of mislabeled data, this paper proposes a conceptually simple yet efficient training block, termed as Attentive Feature Mixup (AFM), which allows paying more attention to clean samples and less to mislabeled ones via sample interactions in small groups. Specifically, this plug-and-play AFM first leverages a group-to-attend module to construct groups and assign attention weights for group-wise samples, and then uses a mixup module with the attention weights to interpolate massive noisy-suppressed samples. The AFM has several appealing benefits for noise-robust deep learning. (i) It does not rely on any assumptions and extra clean subset. (ii) With massive interpolations, the ratio of useless samples is reduced dramatically compared to the original noisy ratio. (iii) It jointly optimizes the interpolation weights with classifiers, suppressing the influence of mislabeled data via low attention weights. (iv) It partially inherits the vicinal risk minimization of mixup to alleviate over-fitting while improves it by sampling fewer feature-target vectors around mislabeled data from the mixup vicinal distribution. Extensive experiments demonstrate that AFM yields state-of-the-art results on two challenging real-world noisy datasets: Food101N and Clothing1M.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_46');
INSERT INTO `paper` VALUES (12830, 'Surface Normal Estimation of Tilted Images via Spatial Rectifier', 'Surface normal estimation', 'Spatial rectifier', 'Tilted images', '', '', 'In this paper, we present a spatial rectifier to estimate surface normals of tilted images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. Our two main hypotheses are: (1) visual scene layout is indicative of the gravity direction; and (2) not all surfaces are equally represented by a learned estimator due to the structured distribution of the training data, thus, there exists a transformation for each tilted image that is more responsive to the learned estimator than others. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. Along with the spatial rectifier, we propose a novel truncated angular loss that offers a stronger gradient at smaller angular errors and robustness to outliers. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_16');
INSERT INTO `paper` VALUES (12831, 'Symbiotic Adversarial Learning for Attribute-Based Person Search', 'Person search', 'Cross-modal retrieval', 'Adversarial learning', '', '', 'Attribute-based person search is in significant demand for applications where no detected query images are available, such as identifying a criminal from witness. However, the task itself is quite challenging because there is a huge modality gap between images and physical descriptions of attributes. Often, there may also be a large number of unseen categories (attribute combinations). The current state-of-the-art methods either focus on learning better cross-modal embeddings by mining only seen data, or they explicitly use generative adversarial networks (GANs) to synthesize unseen features. The former tends to produce poor embeddings due to insufficient data, while the latter does not preserve intra-class compactness during generation. In this paper, we present a symbiotic adversarial learning framework, called SAL. Two GANs sit at the base of the framework in a symbiotic learning scheme: one synthesizes features of unseen classes/categories, while the other optimizes the embedding and performs the cross-modal alignment on the common embedding space. Specifically, two different types of generative adversarial networks learn collaboratively throughout the training process and the interactions between the two mutually benefit each other. Extensive evaluations show SAL’s superiority over nine state-of-the-art methods with two challenging pedestrian benchmarks, PETA and Market-1501. The code is publicly available at: https://github.com/ycao5602/SAL.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_14');
INSERT INTO `paper` VALUES (12832, 'Synthesis and Completion of Facades from Satellite Imagery', 'Image synthesis and completion', 'Inverse procedural modeling', 'Satellite imagery', '', '', 'Automatic satellite-based reconstruction enables large and widespread creation of urban areas. However, satellite imagery is often noisy and incomplete, and is not suitable for reconstructing detailed building facades. We present a machine learning-based inverse procedural modeling method to automatically create synthetic facades from satellite imagery. Our key observation is that building facades exhibit regular, grid-like structures. Hence, we can overcome the low-resolution, noisy, and partial building data obtained from satellite imagery by synthesizing the underlying facade layout. Our method infers regular facade details from satellite-based image-fragments of a building, and applies them to occluded or under-sampled parts of the building, resulting in plausible, crisp facades. Using urban areas from six cities, we compare our approach to several state-of-the-art image completion/in-filling methods and our approach consistently creates better facade images.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_34');
INSERT INTO `paper` VALUES (12833, 'Synthesize Then Compare: Detecting Failures and Anomalies for Semantic Segmentation', 'Failure detection', 'Anomaly segmentation', 'Semantic segmentation', '', '', 'The ability to detect failures and anomalies are fundamental requirements for building reliable systems for computer vision applications, especially safety-critical applications of semantic segmentation, such as autonomous driving and medical image analysis. In this paper, we systematically study failure and anomaly detection for semantic segmentation and propose a unified framework, consisting of two modules, to address these two related problems. The first module is an image synthesis module, which generates a synthesized image from a segmentation layout map, and the second is a comparison module, which computes the difference between the synthesized image and the input image. We validate our framework on three challenging datasets and improve the state-of-the-arts by large margins, i.e., 6% AUPR-Error on Cityscapes, 7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on StreetHazards anomaly segmentation.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_9');
INSERT INTO `paper` VALUES (12834, 'Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative Adversarial Networks', 'Synthetic 3D Face', 'Face generation', 'Generative Adversarial Networks', '3D morphable models', 'Facial expression generation', 'Generating realistic 3D faces is of high importance for computer graphics and computer vision applications. Generally, research on 3D face generation revolves around linear statistical models of the facial surface. Nevertheless, these models cannot represent faithfully either the facial texture or the normals of the face, which are very crucial for photo-realistic face synthesis. Recently, it was demonstrated that Generative Adversarial Networks (GANs) can be used for generating high-quality textures of faces. Nevertheless, the generation process either omits the geometry and normals, or independent processes are used to produce 3D shape information. In this paper, we present the first methodology that generates high-quality texture, shape, and normals jointly, which can be used for photo-realistic synthesis. To do so, we propose a novel GAN that can generate data from different modalities while exploiting their correlations. Furthermore, we demonstrate how we can condition the generation on the expression and create faces with various facial expressions. The qualitative results shown in this paper are compressed due to size limitations, full-resolution results and the accompanying video can be found in the supplementary documents. The code and models are available at the project page: https://github.com/barisgecer/TBGAN.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_25');
INSERT INTO `paper` VALUES (12835, 'Synthetic Convolutional Features for Improved Semantic Segmentation', '', '', '', '', '', 'Recently, learning-based image synthesis has enabled to generate high resolution images, either applying popular adversarial training or a powerful perceptual loss. However, it remains challenging to successfully leverage synthetic data for improving semantic segmentation with additional synthetic images. Therefore, we suggest to generate intermediate convolutional features and propose the first synthesis approach that is catered to such intermediate convolutional features. This allows us to generate new features from label masks and include them successfully into the training procedure in order to improve the performance of semantic segmentation. Experimental results and analysis on two challenging datasets Cityscapes and ADE20K show that our generated feature improves performance on segmentation tasks.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_19');
INSERT INTO `paper` VALUES (12836, 'Table Structure Recognition Using Top-Down and Bottom-Up Cues', 'Document image', 'Table detection', 'Table cell detection', 'Row and column association', 'Table structure recognition', 'Tables are information-rich structured objects in document images. While significant work has been done in localizing tables as graphic objects in document images, only limited attempts exist on table structure recognition. Most existing literature on structure recognition depends on extraction of meta-features from the pdf document or on the optical character recognition (ocr) models to extract low-level layout features from the image. However, these methods fail to generalize well because of the absence of meta-features or errors made by the ocr when there is a significant variance in table layouts and text organization. In our work, we focus on tables that have complex structures, dense content, and varying layouts with no dependency on meta-features and/or ocr.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_5');
INSERT INTO `paper` VALUES (12837, 'TAFSSL: Task-Adaptive Feature Sub-Space Learning for Few-Shot Classification', 'Transductive', 'Semi-supervised', 'Few-Shot Learning', '', '', 'Recently, Few-Shot Learning (FSL), or learning from very few (typically 1 or 5) examples per novel class (unseen during training), has received a lot of attention and significant performance advances. While number of techniques have been proposed for FSL, several factors have emerged as most important for FSL performance, awarding SOTA even to the simplest of techniques. These are: the backbone architecture (bigger is better), type of pre-training (meta-training vs multi-class), quantity and diversity of the base classes (the more the merrier), and using auxiliary self-supervised tasks (a proxy for increasing the diversity). In this paper we propose TAFSSL, a simple technique for improving the few shot performance in cases when some additional unlabeled data accompanies the few-shot task. TAFSSL is built upon the intuition of reducing the feature and sampling noise inherent to few-shot tasks comprised of novel classes unseen during pre-training. Specifically, we show that on the challenging miniImageNet and tieredImageNet benchmarks, TAFSSL can improve the current state-of-the-art in both transductive and semi-supervised FSL settings by more than \\(5\\%\\), while increasing the benefit of using unlabeled data in FSL to above \\(10\\%\\) performance gain.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_31');
INSERT INTO `paper` VALUES (12838, 'Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping', '', '', '', '', '', 'We present an autoencoder-based semi-supervised approach to classify perceived human emotions from walking styles obtained from videos or motion-captured data and represented as sequences of 3D poses. Given the motion on each joint in the pose at each time step extracted from 3D pose sequences, we hierarchically pool these joint motions in a bottom-up manner in the encoder, following the kinematic chains in the human body. We also constrain the latent embeddings of the encoder to contain the space of psychologically-motivated affective features underlying the gaits. We train the decoder to reconstruct the motions per joint per time step in a top-down manner from the latent embeddings. For the annotated data, we also train a classifier to map the latent embeddings to emotion labels. Our semi-supervised approach achieves a mean average precision of 0.84 on the Emotion-Gait benchmark dataset, which contains both labeled and unlabeled gaits collected from multiple sources. We outperform current state-of-art algorithms for both emotion recognition and action recognition from 3D gaits by 7%–23% on the absolute. More importantly, we improve the average precision by 10%–50% on the absolute on classes that each makes up less than 25% of the labeled part of the Emotion-Gait benchmark dataset.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_9');
INSERT INTO `paper` VALUES (12839, 'Talking-Head Generation with Rhythmic Head Motion', '', '', '', '', '', 'When people deliver a speech, they naturally move heads, and this rhythmic head motion conveys prosodic information. However, generating a lip-synced video while moving head naturally is challenging. While remarkably successful, existing works either generate still talking-face videos or rely on landmark/video frames as sparse/dense mapping guidance to generate head movements, which leads to unrealistic or uncontrollable video synthesis. To overcome the limitations, we propose a 3D-aware generative network along with a hybrid embedding module and a non-linear composition module. Through modeling the head motion and facial expressions (In our setting, facial expression means facial movement (e.g., blinks, and lip & chin movements).) explicitly, manipulating 3D animation carefully, and embedding reference images dynamically, our approach achieves controllable, photo-realistic, and temporally coherent talking-head videos with natural head movements. Thoughtful experiments on several standard benchmarks demonstrate that our method achieves significantly better results than the state-of-the-art methods in both quantitative and qualitative comparisons. The code is available on https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_3');
INSERT INTO `paper` VALUES (12840, 'TANet: Towards Fully Automatic Tooth Arrangement', 'Deep learning', 'Orthodontics', 'Tooth arrangement', '6D pose prediction', 'Structure', 'Determining optimal target tooth arrangements is a key step of treatment planning in digital orthodontics. Existing practice for specifying the target tooth arrangement involves tedious manual operations with the outcome quality depending heavily on the experience of individual specialists, leading to inefficiency and undesirable variations in treatment results. In this work, we proposed a learning-based method for fast and automatic tooth arrangement. To achieve this, we formulate the tooth arrangement task as a novel structured 6-DOF pose prediction problem and solve it by proposing a new neural network architecture to learn from a large set of clinical data that encode successful orthodontic treatment cases. Our method has been validated with extensive experiments and shows promising results both qualitatively and quantitatively.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_29');
INSERT INTO `paper` VALUES (12841, 'TAO: A Large-Scale Benchmark for Tracking Any Object', 'Datasets', 'Video object detection', 'Tracking', '', '', 'For many years, multi-object tracking benchmarks have focused on a handful of categories. Motivated primarily by surveillance and self-driving applications, these datasets provide tracks for people, vehicles, and animals, ignoring the vast majority of objects in the world. By contrast, in the related field of object detection, the introduction of large-scale, diverse datasets (e.g., COCO) have fostered significant progress in developing highly robust solutions. To bridge this gap, we introduce a similarly diverse dataset for Tracking Any Object (TAO) (http://taodataset.org/). It consists of 2,907 high resolution videos, captured in diverse environments, which are half a minute long on average. Importantly, we adopt a bottom-up approach for discovering a large vocabulary of 833 categories, an order of magnitude more than prior tracking benchmarks. To this end, we ask annotators to label objects that move at any point in the video, and give names to them post factum. Our vocabulary is both significantly larger and qualitatively different from existing tracking datasets. To ensure scalability of annotation, we employ a federated approach that focuses manual effort on labeling tracks for those relevant objects in a video (e.g., those that move). We perform an extensive evaluation of state-of-the-art trackers and make a number of important discoveries regarding large-vocabulary tracking in an open-world. In particular, we show that existing single- and multi-object trackers struggle when applied to this scenario in the wild, and that detection-based, multi-object trackers are in fact competitive with user-initialized ones. We hope that our dataset and analysis will boost further progress in the tracking community.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_26');
INSERT INTO `paper` VALUES (12842, 'Targeted Attack for Deep Hashing Based Retrieval', 'Targeted attack', 'Deep hashing', 'Adversarial attack', 'Similarity retrieval', '', 'The deep hashing based retrieval method is widely adopted in large-scale image and video retrieval. However, there is little investigation on its security. In this paper, we propose a novel method, dubbed deep hashing targeted attack (DHTA), to study the targeted attack on such retrieval. Specifically, we first formulate the targeted attack as a point-to-set optimization, which minimizes the average distance between the hash code of an adversarial example and those of a set of objects with the target label. Then we design a novel component-voting scheme to obtain an anchor code as the representative of the set of hash codes of objects with the target label, whose optimality guarantee is also theoretically derived. To balance the performance and perceptibility, we propose to minimize the Hamming distance between the hash code of the adversarial example and the anchor code under the \\(\\ell ^\\infty \\) restriction on the perturbation. Extensive experiments verify that DHTA is effective in attacking both deep hashing based image retrieval and video retrieval.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_36');
INSERT INTO `paper` VALUES (12843, 'Task-Aware Quantization Network for JPEG Image Compression', 'JPEG image compression', 'Adaptive quantization', 'Bitrate approximation', '', '', 'We propose to learn a deep neural network for JPEG image compression, which predicts image-specific optimized quantization tables fully compatible with the standard JPEG encoder and decoder. Moreover, our approach provides the capability to learn task-specific quantization tables in a principled way by adjusting the objective function of the network. The main challenge to realize this idea is that there exist non-differentiable components in the encoder such as run-length encoding and Huffman coding and it is not straightforward to predict the probability distribution of the quantized image representations. We address these issues by learning a differentiable loss function that approximates bitrates using simple network blocks—two MLPs and an LSTM. We evaluate the proposed algorithm using multiple task-specific losses—two for semantic image understanding and another two for conventional image compression—and demonstrate the effectiveness of our approach to the individual tasks.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_19');
INSERT INTO `paper` VALUES (12844, 'Task-Conditioned Domain Adaptation for Pedestrian Detection in Thermal Imagery', 'Object detection', 'Pedestrian detection', 'Thermal imagery', 'Task-conditioned', 'Domain adaptation', 'Pedestrian detection is a core problem in computer vision that sees broad application in video surveillance and, more recently, in advanced driving assistance systems. Despite its broad application and interest, it remains a challenging problem in part due to the vast range of conditions under which it must be robust. Pedestrian detection at nighttime and during adverse weather conditions is particularly challenging, which is one of the reasons why thermal and multispectral approaches have been become popular in recent years. In this paper, we propose a novel approach to domain adaptation that significantly improves pedestrian detection performance in the thermal domain. The key idea behind our technique is to adapt an RGB-trained detection network to simultaneously solve two related tasks. An auxiliary classification task that distinguishes between daytime and nighttime thermal images is added to the main detection task during domain adaptation. The internal representation learned to perform this classification task is used to condition a YOLOv3 detector at multiple points in order to improve its adaptation to the thermal domain. We validate the effectiveness of task-conditioned domain adaptation by comparing with the state-of-the-art on the KAIST Multispectral Pedestrian Detection Benchmark. To the best of our knowledge, our proposed task-conditioned approach achieves the best single-modality detection results.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_33');
INSERT INTO `paper` VALUES (12845, 'TCGM: An Information-Theoretic Framework for Semi-supervised Multi-modality Learning', 'Total Correlation', 'Semi-supervised', 'Multi-modality', 'Conditional independence', 'Information intersection', 'Fusing data from multiple modalities provides more information to train machine learning systems. However, it is prohibitively expensive and time-consuming to label each modality with a large amount of data, which leads to a crucial problem of semi-supervised multi-modal learning. Existing methods suffer from either ineffective fusion across modalities or lack of theoretical guarantees under proper assumptions. In this paper, we propose a novel information-theoretic approach - namely, Total Correlation Gain Maximization (TCGM) – for semi-supervised multi-modal learning, which is endowed with promising properties: (i) it can utilize effectively the information across different modalities of unlabeled data points to facilitate training classifiers of each modality (ii) it has theoretical guarantee to identify Bayesian classifiers, i.e., the ground truth posteriors of all modalities. Specifically, by maximizing TC-induced loss (namely TC gain) over classifiers of all modalities, these classifiers can cooperatively discover the equivalent class of ground-truth classifiers; and identify the unique ones by leveraging limited percentage of labeled data. We apply our method to various tasks and achieve state-of-the-art results, including the news classification (Newsgroup dataset), emotion recognition (IEMOCAP and MOSI datasets), and disease prediction (Alzheimer’s Disease Neuroimaging Initiative dataset).', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_11');
INSERT INTO `paper` VALUES (12846, 'TDMPNet: Prototype Network with Recurrent Top-Down Modulation for Robust Object Classification Under Partial Occlusion', '', '', '', '', '', 'Despite deep convolutional neural networks’ great success in object classification, recent work has shown that they suffer from a severe generalization performance drop under occlusion conditions that do not appear in the training data. Due to the large variability of occluders in terms of shape and appearance, training data can hardly cover all possible occlusion conditions. However, in practice we expect models to reliably generalize to various novel occlusion conditions, rather than being limited to the training conditions. In this work, we integrate inductive priors including prototypes, partial matching and top-down modulation into deep neural networks to realize robust object classification under novel occlusion conditions, with limited occlusion in training data. We first introduce prototype learning as its regularization encourages compact data clusters for better generalization ability. Then, a visibility map at the intermediate layer based on feature dictionary and activation scale is estimated for partial matching, whose prior sifts irrelevant information out when comparing features with prototypes. Further, inspired by the important role of feedback connection in neuroscience for object recognition under occlusion, a structural prior, i.e. top-down modulation, is introduced into convolution layers, purposefully reducing the contamination by occlusion during feature extraction. Experiment results on partially occluded MNIST, vehicles from the PASCAL3D+ dataset, and vehicles from the cropped COCO dataset demonstrate the improvement under both simulated and real-world novel occlusion conditions, as well as under the transfer of datasets.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_31');
INSERT INTO `paper` VALUES (12847, 'Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces from Images', 'Cross-modal', 'Visuo-tactile', 'Viewpoint selection', 'Physical property estimation', 'Neural architecture search', 'The connection between visual input and tactile sensing is critical for object manipulation tasks such as grasping and pushing. In this work, we introduce the challenging task of estimating a set of tactile physical properties from visual information. We aim to build a model that learns the complex mapping between visual information and tactile physical properties. We construct a first of its kind image-tactile dataset with over 400 multiview image sequences and the corresponding tactile properties. A total of fifteen tactile physical properties across categories including friction, compliance, adhesion, texture, and thermal conductance are measured and then estimated by our models. We develop a cross-modal framework comprised of an adversarial objective and a novel visuo-tactile joint classification loss. Additionally, we introduce a neural architecture search framework capable of selecting optimal combinations of viewing angles for estimating a given physical property.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_1');
INSERT INTO `paper` VALUES (12848, 'Temporal Aggregate Representations for Long-Range Video Understanding', 'Action anticipation', 'Temporal aggregation', '', '', '', 'Future prediction, especially in long-range videos, requires reasoning from current and past observations. In this work, we address questions of temporal extent, scaling, and level of semantic abstraction with a flexible multi-granular temporal aggregation framework. We show that it is possible to achieve state of the art in both next action and dense anticipation with simple techniques such as max-pooling and attention. To demonstrate the anticipation capabilities of our model, we conduct experiments on Breakfast, 50Salads, and EPIC-Kitchens datasets, where we achieve state-of-the-art results. With minimal modifications, our model can also be extended for video segmentation and action recognition.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_10');
INSERT INTO `paper` VALUES (12849, 'Temporal Coherence or Temporal Motion: Which Is More Critical for Video-Based Person Re-identification?', 'Video-based person re-identification', 'Temporal coherence', 'Feature augmentation', 'Adversarial learning', '', 'Video-based person re-identification aims to match pedestrians with the consecutive video sequences. While a rich line of work focuses solely on extracting the motion features from pedestrian videos, we show in this paper that the temporal coherence plays a more critical role. To distill the temporal coherence part of video representation from frame representations, we propose a simple yet effective Adversarial Feature Augmentation (AFA) method, which highlights the temporal coherence features by introducing adversarial augmented temporal motion noise. Specifically, we disentangle the video representation into the temporal coherence and motion parts and randomly change the scale of the temporal motion features as the adversarial noise. The proposed AFA method is a general lightweight component that can be readily incorporated into various methods with negligible cost. We conduct extensive experiments on three challenging datasets including MARS, iLIDS-VID, and DukeMTMC-VideoReID, and the experimental results verify our argument and demonstrate the effectiveness of the proposed method.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_39');
INSERT INTO `paper` VALUES (12850, 'Temporal Complementary Learning for Video Person Re-identification', 'Video person re-identification', 'Complementary learning', 'Feature Enhancing', '', '', 'This paper proposes a Temporal Complementary Learning Network that extracts complementary features of consecutive video frames for video person re-identification. Firstly, we introduce a Temporal Saliency Erasing (TSE) module including a saliency erasing operation and a series of ordered learners. Specifically, for a specific frame of a video, the saliency erasing operation drives the specific learner to mine new and complementary parts by erasing the parts activated by previous frames. Such that the diverse visual features can be discovered for consecutive frames and finally form an integral characteristic of the target identity. Furthermore, a Temporal Saliency Boosting (TSB) module is designed to propagate the salient information among video frames to enhance the salient feature. It is complementary to TSE by effectively alleviating the information loss caused by the erasing operation of TSE. Extensive experiments show our method performs favorably against state-of-the-arts. The source code is available at https://github.com/blue-blue272/VideoReID-TCLNet.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_24');
INSERT INTO `paper` VALUES (12851, 'Temporal Distinct Representation Learning for Action Recognition', 'Video representation learning', 'Action recognition', 'Progressive Enhancement Module', 'Temporal Diversity Loss', '', 'Motivated by the previous success of Two-Dimensional Convolutional Neural Network (2D CNN) on image recognition, researchers endeavor to leverage it to characterize videos. However, one limitation of applying 2D CNN to analyze videos is that different frames of a video share the same 2D CNN kernels, which may result in repeated and redundant information utilization, especially in the spatial semantics extraction process, hence neglecting the critical variations among frames. In this paper, we attempt to tackle this issue through two ways. 1) Design a sequential channel filtering mechanism, i.e., Progressive Enhancement Module (PEM), to excite the discriminative channels of features from different frames step by step, and thus avoid repeated information extraction. 2) Create a Temporal Diversity Loss (TD Loss) to force the kernels to concentrate on and capture the variations among frames rather than the image regions with similar appearance. Our method is evaluated on benchmark temporal reasoning datasets Something-Something V1 and V2, and it achieves visible improvements over the best competitor by \\(2.4\\%\\) and \\(1.3\\%\\), respectively. Besides, performance improvements over the 2D-CNN-based state-of-the-arts on the large-scale dataset Kinetics are also witnessed.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_22');
INSERT INTO `paper` VALUES (12852, 'Temporal Keypoint Matching and Refinement Network for Pose Estimation and Tracking', 'Pose estimation and tracking', 'Temporal keypoint matching', 'Temporal keypoint refinement', '', '', 'Multi-person pose estimation and tracking in realistic videos is very challenging due to factors such as occlusions, fast motion and pose variations. Top-down approaches are commonly used for this task, which involves three stages: person detection, single-person pose estimation, and pose association across time. Recently, significant progress has been made in person detection and single-person pose estimation. In this paper, we mainly focus on improving pose association and estimation in a video to build a strong pose estimator and tracker. To this end, we propose a novel temporal keypoint matching and refinement network. Specifically, we propose two network modules, temporal keypoint matching and temporal keypoint refinement, which are incorporated into a single-person pose estimatin network. The temporal keypoint matching module learns a simialrity metric for matching keypoints across frames. Pose matching is performed by aggregating keypoint similarities between poses in adjacent frames. The temporal keypoint refinement module serves to correct individual poses by utilizing their associated poses in neighboring frames as temporal context. We validate the effectiveness of our proposed network on two benchmark datasets: PoseTrack 2017 and PoseTrack 2018. Exprimental results show that our approach achieves state-of-the-art performance on both datasets.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_41');
INSERT INTO `paper` VALUES (12853, 'TENet: Triple Excitation Network for Video Salient Object Detection', '', '', '', '', '', 'In this paper, we propose a simple yet effective approach, named Triple Excitation Network, to reinforce the training of video salient object detection (VSOD) from three aspects, spatial, temporal, and online excitations. These excitation mechanisms are designed following the spirit of curriculum learning and aim to reduce learning ambiguities at the beginning of training by selectively exciting feature activations using ground truth. Then we gradually reduce the weight of ground truth excitations by a curriculum rate and replace it by a curriculum complementary map for better and faster convergence. In particular, the spatial excitation strengthens feature activations for clear object boundaries, while the temporal excitation imposes motions to emphasize spatio-temporal salient regions. Spatial and temporal excitations can combat the saliency shifting problem and conflict between spatial and temporal features of VSOD. Furthermore, our semi-curriculum learning design enables the first online refinement strategy for VSOD, which allows exciting and boosting saliency responses during testing without re-training. The proposed triple excitations can easily plug in different VSOD methods. Extensive experiments show the effectiveness of all three excitation methods and the proposed method outperforms state-of-the-art image and video salient object detection methods.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_13');
INSERT INTO `paper` VALUES (12854, 'Tensor Low-Rank Reconstruction for Semantic Segmentation', 'Semantic segmentation', 'Low-rank reconstruction', 'Tensor decomposition', '', '', 'Context information plays an indispensable role in the success of semantic segmentation. Recently, non-local self-attention based methods are proved to be effective for context information collection. Since the desired context consists of spatial-wise and channel-wise attentions, 3D representation is an appropriate formulation. However, these non-local methods describe 3D context information based on a 2D similarity matrix, where space compression may lead to channel-wise attention missing. An alternative is to model the contextual information directly without compression. However, this effort confronts a fundamental difficulty, namely the high-rank property of context information. In this paper, we propose a new approach to model the 3D context representations, which not only avoids the space compression but also tackles the high-rank difficulty. Here, inspired by tensor canonical-polyadic decomposition theory (i.e, a high-rank tensor can be expressed as a combination of rank-1 tensors.), we design a low-rank-to-high-rank context reconstruction framework (i.e, RecoNet). Specifically, we first introduce the tensor generation module (TGM), which generates a number of rank-1 tensors to capture fragments of context feature. Then we use these rank-1 tensors to recover the high-rank context features through our proposed tensor reconstruction module (TRM). Extensive experiments show that our method achieves state-of-the-art on various public datasets. Additionally, our proposed method has more than 100 times less computational cost compared with conventional non-local-based methods.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_4');
INSERT INTO `paper` VALUES (12855, 'Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction', 'Simulation', 'Perception and prediction', 'Self-driving vehicles', '', '', 'We present a novel method for testing the safety of self-driving vehicles in simulation. We propose an alternative to sensor simulation, as sensor simulation is expensive and has large domain gaps. Instead, we directly simulate the outputs of the self-driving vehicle’s perception and prediction system, enabling realistic motion planning testing. Specifically, we use paired data in the form of ground truth labels and real perception and prediction outputs to train a model that predicts what the online system will produce. Importantly, the inputs to our system consists of high definition maps, bounding boxes, and trajectories, which can be easily sketched by a test engineer in a matter of minutes. This makes our approach a much more scalable solution. Quantitative results on two large-scale datasets demonstrate that we can realistically test motion planning using our simulations.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_19');
INSERT INTO `paper` VALUES (12856, 'TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D Video', 'Human shape reconstruction', 'Human texture generation', '', '', '', 'We present TexMesh, a novel approach to reconstruct detailed human meshes with high-resolution full-body texture from RGB-D video. TexMesh enables high quality free-viewpoint rendering of humans. Given the RGB frames, the captured environment map, and the coarse per-frame human mesh from RGB-D tracking, our method reconstructs spatiotemporally consistent and detailed per-frame meshes along with a high-resolution albedo texture. By using the incident illumination we are able to accurately estimate local surface geometry and albedo, which allows us to further use photometric constraints to adapt a synthetically trained model to real-world sequences in a self-supervised manner for detailed surface geometry and high-resolution texture estimation. In practice, we train our models on a short example sequence for self-adaptation and the model runs at interactive framerate afterwards. We validate TexMesh on synthetic and real-world data, and show it outperforms the state of art quantitatively and qualitatively.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_29');
INSERT INTO `paper` VALUES (12857, 'TextCaps: A Dataset for Image Captioning with Reading Comprehension', '', '', '', '', '', 'Image descriptions can help visually impaired people to quickly understand the image content. While we made significant progress in automatically describing images and optical character recognition, current approaches are unable to include written text in their descriptions, although text is omnipresent in human environments and frequently critical to understand our surroundings. To study how to comprehend text in the context of an image we collect a novel dataset, TextCaps, with 145k captions for 28k images. Our dataset challenges a model to recognize text, relate it to its visual context, and decide what part of the text to copy or paraphrase, requiring spatial, semantic, and visual reasoning between multiple text tokens and visual entities, such as objects. We study baselines and adapt existing approaches to this new task, which we refer to as image captioning with reading comprehension. Our analysis with automatic and human studies shows that our new TextCaps dataset provides many new technical challenges over previous datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_44');
INSERT INTO `paper` VALUES (12858, 'Texture Hallucination for Large-Factor Painting Super-Resolution', 'Texture hallucination', 'Large-factor', 'Painting super-resolution', 'Wavelet texture loss', 'Degradation loss', 'We aim to super-resolve digital paintings, synthesizing realistic details from high-resolution reference painting materials for very large scaling factors (e.g., 8\\(\\times \\), 16\\(\\times \\)). However, previous single image super-resolution (SISR) methods would either lose textural details or introduce unpleasing artifacts. On the other hand, reference-based SR (Ref-SR) methods can transfer textures to some extent, but is still impractical to handle very large factors and keep fidelity with original input. To solve these problems, we propose an efficient high-resolution hallucination network for very large scaling factors with efficient network structure and feature transferring. To transfer more detailed textures, we design a wavelet texture loss, which helps to enhance more high-frequency components. At the same time, to reduce the smoothing effect brought by the image reconstruction loss, we further relax the reconstruction constraint with a degradation loss which ensures the consistency between downscaled super-resolution results and low-resolution inputs. We also collected a high-resolution (e.g., 4K resolution) painting dataset PaintHD by considering both physical size and image resolution. We demonstrate the effectiveness of our method with extensive experiments on PaintHD by comparing with SISR and Ref-SR state-of-the-art methods.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_13');
INSERT INTO `paper` VALUES (12859, 'TF-NAS: Rethinking Three Search Freedoms of Latency-Constrained Differentiable Neural Architecture Search', 'Differentiable NAS', 'Latency-constrained', 'Three Freedoms', '', '', 'With the flourish of differentiable neural architecture search (NAS), automatically searching latency-constrained architectures gives a new perspective to reduce human labor and expertise. However, the searched architectures are usually suboptimal in accuracy and may have large jitters around the target latency. In this paper, we rethink three freedoms of differentiable NAS, i.e. operation-level, depth-level and width-level, and propose a novel method, named Three-Freedom NAS (TF-NAS), to achieve both good classification accuracy and precise latency constraint. For the operation-level, we present a bi-sampling search algorithm to moderate the operation collapse. For the depth-level, we introduce a sink-connecting search space to ensure the mutual exclusion between skip and other candidate operations, as well as eliminate the architecture redundancy. For the width-level, we propose an elasticity-scaling strategy that achieves precise latency constraint in a progressively fine-grained manner. Experiments on ImageNet demonstrate the effectiveness of TF-NAS. Particularly, our searched TF-NAS-A obtains 76.9% top-1 accuracy, achieving state-of-the-art results with less latency. Code is available at https://github.com/AberHu/TF-NAS.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_8');
INSERT INTO `paper` VALUES (12860, 'Thanks for Nothing: Predicting Zero-Valued Activations with Lightweight Convolutional Neural Networks', 'Convolutional neural networks', 'Dynamic pruning', '', '', '', 'Convolutional neural networks (CNNs) introduce state-of-the-art results for various tasks with the price of high computational demands. Inspired by the observation that spatial correlation exists in CNN output feature maps (ofms), we propose a method to dynamically predict whether ofm activations are zero-valued or not according to their neighboring activation values, thereby avoiding zero-valued activations and reducing the number of convolution operations. We implement the zero activation predictor (ZAP) with a lightweight CNN, which imposes negligible overheads and is easy to deploy on existing models. ZAPs are trained by mimicking hidden layer ouputs; thereby, enabling a parallel and label-free training. Furthermore, without retraining, each ZAP can be tuned to a different operating point trading accuracy for MAC reduction.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_14');
INSERT INTO `paper` VALUES (12861, 'The 1st Tiny Object Detection Challenge: Methods and Results', 'Tiny Object Detection', 'Visual recognition', '', '', '', 'The 1st Tiny Object Detection (TOD) Challenge aims to encourage research in developing novel and accurate methods for tiny object detection in images which have wide views, with a current focus on tiny person detection. The TinyPerson dataset was used for the TOD Challenge and is publicly released. It has 1610 images and 72651 box-level annotations. Around 36 participating teams from the globe competed in the 1st TOD Challenge. In this paper, we provide a brief summary of the 1st TOD Challenge including brief introductions to the top three methods.The submission leaderboard will be reopened for researchers that are interested in the TOD challenge. The benchmark dataset and other information can be found at: https://github.com/ucas-vg/TinyBenchmark.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_23');
INSERT INTO `paper` VALUES (12862, 'The Average Mixing Kernel Signature', 'Shape representation', 'Shape analysis', 'Quantum walks', '', '', 'We introduce the Average Mixing Kernel Signature (AMKS), a novel signature for points on non-rigid three-dimensional shapes based on the average mixing kernel and continuous-time quantum walks. The average mixing kernel holds information on the average transition probabilities of a quantum walk between each pair of vertices of the mesh until a time T. We define the AMKS by decomposing the spectral contributions of the kernel into several bands, allowing us to limit the influence of noise-dominated high-frequency components and obtain a more descriptive signature. We also show through a perturbation theory analysis of the kernel that choosing a finite stopping time T leads to noise and deformation robustness for the AMKS. We perform an extensive experimental evaluation on two widely used shape matching datasets under varying level of noise, showing that the AMKS outperforms two state-of-the-art descriptors, namely the Heat Kernel Signature (HKS) and the similarly quantum-walk based Wave Kernel Signature (WKS) .', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_1');
INSERT INTO `paper` VALUES (12863, 'The Devil Is in Classification: A Simple Framework for Long-Tail Instance Segmentation', 'Long-tail distribution', 'Instance segmentation', 'Object detection', 'Long-tail classification', '', 'Most existing object instance detection and segmentation models only work well on fairly balanced benchmarks where per-category training sample numbers are comparable, such as COCO. They tend to suffer performance drop on realistic datasets that are usually long-tailed. This work aims to study and address such open challenges. Specifically, we systematically investigate performance drop of the state-of-the-art two-stage instance segmentation model Mask R-CNN on the recent long-tail LVIS dataset, and unveil that a major cause is the inaccurate classification of object proposals. Based on such an observation, we first consider various techniques for improving long-tail classification performance which indeed enhance instance segmentation results. We then propose a simple calibration framework to more effectively alleviate classification head bias with a bi-level class balanced sampling approach. Without bells and whistles, it significantly boosts the performance of instance segmentation for tail classes on the recent LVIS dataset and our sampled COCO-LT dataset. Our analysis provides useful insights for solving long-tail instance detection and segmentation problems, and the straightforward SimCal method can serve as a simple but strong baseline. With the method we have won the 2019 LVIS challenge. Codes and models are available at https://github.com/twangnh/SimCal.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_43');
INSERT INTO `paper` VALUES (12864, 'The Devil Is in the Details: Self-supervised Attention for Vehicle Re-identification', 'Vehicle re-identification', 'Self-supervised learning', 'Variational auto-encoder', 'Deep representation learning', '', 'In recent years, the research community has approached the problem of vehicle re-identification (re-id) with attention-based models, specifically focusing on regions of a vehicle containing discriminative information. These re-id methods rely on expensive key-point labels, part annotations, and additional attributes including vehicle make, model, and color. Given the large number of vehicle re-id datasets with various levels of annotations, strongly-supervised methods are unable to scale across different domains. In this paper, we present Self-supervised Attention for Vehicle Re-identification (SAVER), a novel approach to effectively learn vehicle-specific discriminative features. Through extensive experimentation, we show that SAVER improves upon the state-of-the-art on challenging VeRi, VehicleID, Vehicle-1M and VERI-Wild datasets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_22');
INSERT INTO `paper` VALUES (12865, 'The Eighth Visual Object Tracking VOT2020 Challenge Results', 'Visual object tracking', 'Performance evaluation protocol', 'State-of-the-art benchmark', 'RGB', 'RGBD', 'The Visual Object Tracking challenge VOT2020 is the eighth annual tracker benchmarking activity organized by the VOT initiative. Results of 58 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The VOT2020 challenge was composed of five sub-challenges focusing on different tracking domains: (i) VOT-ST2020 challenge focused on short-term tracking in RGB, (ii) VOT-RT2020 challenge focused on “real-time” short-term tracking in RGB, (iii) VOT-LT2020 focused on long-term tracking namely coping with target disappearance and reappearance, (iv) VOT-RGBT2020 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2020 challenge focused on long-term tracking in RGB and depth imagery. Only the VOT-ST2020 datasets were refreshed. A significant novelty is introduction of a new VOT short-term tracking evaluation methodology, and introduction of segmentation ground truth in the VOT-ST2020 challenge – bounding boxes will no longer be used in the VOT-ST challenges. A new VOT Python toolkit that implements all these novelites was introduced. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_39');
INSERT INTO `paper` VALUES (12866, 'The Group Loss for Deep Metric Learning', 'Deep metric learning', 'Image retrieval', 'Image clustering', '', '', 'Deep metric learning has yielded impressive results in tasks such as clustering and image retrieval by leveraging neural networks to obtain highly discriminative feature embeddings, which can be used to group samples into different classes. Much research has been devoted to the design of smart loss functions or data mining strategies for training such networks. Most methods consider only pairs or triplets of samples within a mini-batch to compute the loss function, which is commonly based on the distance between embeddings. We propose Group Loss, a loss function based on a differentiable label-propagation method that enforces embedding similarity across all samples of a group while promoting, at the same time, low-density regions amongst data points belonging to different groups. Guided by the smoothness assumption that “similar objects should belong to the same group”, the proposed loss trains the neural network for a classification task, enforcing a consistent labelling amongst samples within a class. We show state-of-the-art results on clustering and image retrieval on several datasets, and show the potential of our method when combined with other techniques such as ensembles. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/group_loss.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_17');
INSERT INTO `paper` VALUES (12867, 'The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement', '', '', '', '', '', 'Existing disentanglement methods for deep generative models rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization term that encourages the Hessian of a generative model with respect to its input to be diagonal. We introduce a model-agnostic, unbiased stochastic approximation of this term based on Hutchinson’s estimator to compute it efficiently during training. Our method can be applied to a wide range of deep generators with just a few lines of code. We show that training with the Hessian Penalty often causes axis-aligned disentanglement to emerge in latent space when applied to ProGAN on several datasets. Additionally, we use our regularization term to identify interpretable directions in BigGAN’s latent space in an unsupervised fashion. Finally, we provide empirical evidence that the Hessian Penalty encourages substantial shrinkage when applied to over-parameterized latent spaces. We encourage readers to view videos of our disentanglement results at www.wpeebles.com/hessian-penalty, and code at https://github.com/wpeebles/hessian_penalty.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_35');
INSERT INTO `paper` VALUES (12868, 'The Impact of Real Rain in a Vision Task', 'Rain', 'Object detection', 'Transfer learning', 'Deraining', '', 'Single image deraining has made impressive progress in recent years. However, the proposed methods are heavily based on high-quality synthetic data for supervised learning which are not representative of practical applications with low-quality real-world images. In a real setting, the rainy images portray a scene with a complex degradation caused by the rain weather and the low-quality factors. The goal of this paper is to investigate the impact of two visual factors that affect vision tasks: image quality and rain effect. To evaluate this, an image dataset with images varying these factors has been created. Aiming to evaluate them, different object detection algorithms are applied and evaluated on the dataset. Our findings indicate that the fine-tuned models can efficiently cope with this problem regardless of the rain intensity of the scene, however it is greatly affected by the image quality gap.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_21');
INSERT INTO `paper` VALUES (12869, 'The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale', '', '', '', '', '', 'Traffic signs are essential map features for smart cities and navigation. To develop accurate and robust algorithms for traffic sign detection and classification, a large-scale and diverse benchmark dataset is required. In this paper, we introduce a new traffic sign dataset of 105K street-level images around the world covering 400 manually annotated traffic sign classes in diverse scenes, wide range of geographical locations, and varying weather and lighting conditions. The dataset includes 52K fully annotated images. Additionally, we show how to augment the dataset with 53K semi-supervised, partially annotated images. This is the largest and the most diverse traffic sign dataset consisting of images from all over the world with fine-grained annotations of traffic sign classes. We run extensive experiments to establish strong baselines for both detection and classification tasks. In addition, we verify that the diversity of this dataset enables effective transfer learning for existing large-scale benchmark datasets on traffic sign detection and classification. The dataset is freely available for academic research (www.mapillary.com/dataset/trafficsign) .', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_5');
INSERT INTO `paper` VALUES (12870, 'The Phong Surface: Efficient 3D Model Fitting Using Lifted Optimization', 'Model-fitting', 'Optimization', 'Hand tracking', 'Pose estimation', '', 'Realtime perceptual and interaction capabilities in mixed reality require a range of 3D tracking problems to be solved at low latency on resource-constrained hardware such as head-mounted devices. Indeed, for devices such as HoloLens 2 where the CPU and GPU are left available for applications, multiple tracking subsystems are required to run on a continuous, real-time basis while sharing a single Digital Signal Processor. To solve model-fitting problems for HoloLens 2 hand tracking, where the computational budget is approximately 100 times smaller than an iPhone 7, we introduce a new surface model: the ‘Phong surface’. Using ideas from computer graphics, the Phong surface describes the same 3D shape as a triangulated mesh model, but with continuous surface normals which enable the use of lifting-based optimization, providing significant efficiency gains over ICP-based methods. We show that Phong surfaces retain the convergence benefits of smoother surface models, while triangle meshes do not.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_40');
INSERT INTO `paper` VALUES (12871, 'The Semantic Mutex Watershed for Efficient Bottom-Up Semantic Instance Segmentation', '', '', '', '', '', 'Semantic instance segmentation is the task of simultaneously partitioning an image into distinct segments while associating each pixel with a class label. In commonly used pipelines, segmentation and label assignment are solved separately since joint optimization is computationally expensive. We propose a greedy algorithm for joint graph partitioning and labeling derived from the efficient Mutex Watershed partitioning algorithm. It optimizes an objective function closely related to the Asymmetric Multiway Cut objective and empirically shows efficient scaling behavior. Due to the algorithm’s efficiency it can operate directly on pixels without prior over-segmentation of the image into superpixels. We evaluate the performance on the Cityscapes dataset (2D urban scenes) and on a 3D microscopy volume. In urban scenes, the proposed algorithm combined with current deep neural networks outperforms the strong baseline of ‘Panoptic Feature Pyramid Networks’ by Kirillov et al. (2019). In the 3D electron microscopy images, we show explicitly that our joint formulation outperforms a separate optimization of the partitioning and labeling problems.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_13');
INSERT INTO `paper` VALUES (12872, 'Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues', 'Face forgery detection', 'Frequency', 'Collaborative learning', '', '', 'As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We find that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F\\(^3\\)-Net), taking advantages of two different but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F\\(^3\\)-Net significantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_6');
INSERT INTO `paper` VALUES (12873, 'TIDE: A General Toolbox for Identifying Object Detection Errors', 'Error diagnosis', 'Object detection', 'Instance segmentation', '', '', 'We introduce TIDE, a framework and associated toolbox (https://dbolya.github.io/tide/) for analyzing the sources of error in object detection and instance segmentation algorithms. Importantly, our framework is applicable across datasets and can be applied directly to output prediction files without required knowledge of the underlying prediction system. Thus, our framework can be used as a drop-in replacement for the standard mAP computation while providing a comprehensive analysis of each model’s strengths and weaknesses. We segment errors into six types and, crucially, are the first to introduce a technique for measuring the contribution of each error in a way that isolates its effect on overall performance. We show that such a representation is critical for drawing accurate, comprehensive conclusions through in-depth analysis across 4 datasets and 7 recognition models.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_33');
INSERT INTO `paper` VALUES (12874, 'Time Series Modeling for Phenotypic Prediction and Phenotype-Genotype Mapping Using Neural Networks', 'Time series modeling', 'Neural networks', 'Phenotypic prediction', 'Phenotype-genotype mapping', 'Long short-term memory', 'Image-based high throughput plant phenotyping refers to the process of computing phenotypes non-destructively by analyzing images of plants captured at regular time intervals. The non-invasive measurements of phenotypes at multiple timestamps during a plant’s life cycle provides the motivation to extend the application of time series modeling in the field of phenomic research to (1) predict phenotypes for missing imaging days or for a time in the future based on analyzing past measurements; (2) predict a derived or composite phenotype from its one or more constituents and (3) bridge the phenotype-genotype gap to contribute in the study of improved crop breeding and understanding the genetic regulation of temporal variation of phenotypes. The paper uses long short-term memory, a variant of recurrent neural networks, for phenotype-genotype mapping, while autoregressive neural networks, autoregressive neural network with exogenous input and non-linear input output neural networks are used for phenotypic prediction. The experimental analyses on the benchmark dataset called Phenoseries dataset show the efficacy and future prospects of this foundational study.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_17');
INSERT INTO `paper` VALUES (12875, 'TopoAL: An Adversarial Learning Approach for Topology-Aware Road Segmentation', 'Road networks', 'Adversarial Learning', 'Generative Adversarial Network', 'Topology learning', '', 'Most state-of-the-art approaches to road extraction from aerial images rely on a CNN trained to label road pixels as foreground and remainder of the image as background. The CNN is usually trained by minimizing pixel-wise losses, which is less than ideal to produce binary masks that preserve the road network’s global connectivity.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_14');
INSERT INTO `paper` VALUES (12876, 'TopoGAN: A Topology-Aware Generative Adversarial Network', 'Topology', 'Persistent homology', 'Generative Adversarial Network', '', '', 'Existing generative adversarial networks (GANs) focus on generating realistic images based on CNN-derived image features, but fail to preserve the structural properties of real images. This can be fatal in applications where the underlying structure (e.g.., neurons, vessels, membranes, and road networks) of the image carries crucial semantic meaning. In this paper, we propose a novel GAN model that learns the topology of real images, i.e., connectedness and loopy-ness. In particular, we introduce a new loss that bridges the gap between synthetic image distribution and real image distribution in the topological feature space. By optimizing this loss, the generator produces images with the same structural topology as real images. We also propose new GAN evaluation metrics that measure the topological realism of the synthetic images. We show in experiments that our method generates synthetic images with realistic topology. We also highlight the increased performance that our method brings to downstream tasks such as segmentation.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_8');
INSERT INTO `paper` VALUES (12877, 'Topology-Change-Aware Volumetric Fusion for Dynamic Scene Reconstruction', 'Reconstruction', 'Topology change', 'Fusion', 'Dynamic scene', '', 'Topology change is a challenging problem for 4D reconstruction of dynamic scenes. In the classic volumetric fusion-based framework, a mesh is usually extracted from the TSDF volume as the canonical surface representation to help estimating deformation field. However, the surface and Embedded Deformation Graph (EDG) representations bring conflicts under topology changes since the surface mesh has fixed-connectivity but the deformation field can be discontinuous. In this paper, the classic framework is re-designed to enable 4D reconstruction of dynamic scene under topology changes, by introducing a novel structure of Non-manifold Volumetric Grid to the re-design of both TSDF and EDG, which allows connectivity updates by cell splitting and replication. Experiments show convincing reconstruction results for dynamic scenes of topology changes, as compared to the state-of-the-art methods.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_16');
INSERT INTO `paper` VALUES (12878, 'Topology-Preserving Class-Incremental Learning', 'Topology-Preserving Class-Incremental Learning (TPCIL)', 'Class-Incremental Learning (CIL)', 'Elastic Hebbian Graph (EHG)', 'Topology-Preserving Loss (TPL)', '', 'A well-known issue for class-incremental learning is the catastrophic forgetting phenomenon, where the network’s recognition performance on old classes degrades severely when incrementally learning new classes. To alleviate forgetting, we put forward to preserve the old class knowledge by maintaining the topology of the network’s feature space. On this basis, we propose a novel topology-preserving class-incremental learning (TPCIL) framework. TPCIL uses an elastic Hebbian graph (EHG) to model the feature space topology, which is constructed with the competitive Hebbian learning rule. To maintain the topology, we develop the topology-preserving loss (TPL) that penalizes the changes of EHG’s neighboring relationships during incremental learning phases. Comprehensive experiments on CIFAR100, ImageNet, and subImageNet datasets demonstrate the power of the TPCIL for continuously learning new classes with less forgetting. The code will be released.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_16');
INSERT INTO `paper` VALUES (12879, 'Toward Continuous-Time Representations of Human Motion', 'Motion generation', '3D Motion', 'Sequence representation', '', '', 'For human motion understanding and generation, it is common to represent the motion sequence via a hidden state of a recurrent neural network, learned in an end-to-end fashion. While powerful, this representation is inflexible as these recurrent models are trained with a specific frame rate, and the hidden state is further hard to interpret. In this paper, we show that we can instead represent the continuous motion via latent parametric curves, leveraging techniques from computer graphics and signal processing. Our parametric representation is powerful enough to faithfully represent continuous motion with few parameters, easy to obtain, and is effective when used for downstream tasks. We validate the proposed method on AMASS and Human3.6M datasets through reconstruction and on a downstream task of point-to-point prediction, and show that our method is able to generate realistic motion. See our demo at www.github.com/WeiyuDu/motion-encode.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_37');
INSERT INTO `paper` VALUES (12880, 'Toward Faster and Simpler Matrix Normalization via Rank-1 Update', '', '', '', '', '', 'Bilinear pooling has been used in many computer vision tasks and recent studies discover that matrix normalization is a vital step for achieving impressive performance of bilinear pooling. The standard matrix normalization, however, needs singular value decomposition (SVD), which is not well suited in the GPU platform, limiting its efficiency in training and inference. To resolve this issue, the Newton-Schulz (NS) iteration method has been proposed to approximate the matrix square-root. Although it is GPU-friendly, the NS iteration still takes several (expensive) iterations of matrix-matrix multiplications. Furthermore, the NS iteration is incompatible with the compact bilinear features obtained from Tensor Sketch (TS) or Random Maclaurin (RM). To overcome those known limitations, in this paper we propose a “rank-1 update normalization” (RUN), which only needs matrix-vector multiplications and is hence substantially more efficient than the NS iteration using matrix-matrix multiplications. Moreover, RUN readily supports the normalization on compact bilinear features from TS or RM. Besides, RUN is simpler than the NS iteration and easier for implementation in practice. As RUN is a differentiable procedure, we can plug it in a CNN-based an end-to-end training setting. Extensive experiments on four public benchmarks demonstrates that, for the full bilinear pooling, RUN achieves comparable accuracy with a substantial speedup over the NS iteration. For the compact bilinear pooling, RUN achieves comparable accuracy with a significant speedup over SVD-based normalization.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_13');
INSERT INTO `paper` VALUES (12881, 'Toward Fine-Grained Facial Expression Manipulation', 'GANs', 'Expression editing', 'Image-to-image translation', '', '', 'Facial expression manipulation aims at editing facial expression with a given condition. Previous methods edit an input image under the guidance of a discrete emotion label or absolute condition (e.g., facial action units) to possess the desired expression. However, these methods either suffer from changing condition-irrelevant regions or are inefficient for fine-grained editing. In this study, we take these two objectives into consideration and propose a novel method. First, we replace continuous absolute condition with relative condition, specifically, relative action units. With relative action units, the generator learns to only transform regions of interest which are specified by non-zero-valued relative AUs. Second, our generator is built on U-Net but strengthened by multi-scale feature fusion (MSF) mechanism for high-quality expression editing purposes. Extensive experiments on both quantitative and qualitative evaluation demonstrate the improvements of our proposed approach compared to the state-of-the-art expression editing methods. Code is available at https://github.com/junleen/Expression-manipulator.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_3');
INSERT INTO `paper` VALUES (12882, 'Toward Unsupervised, Multi-object Discovery in Large-Scale Image Collections', 'Object discovery', 'Large-scale', 'Optimization', 'Region proposals', 'Unsupervised learning', 'This paper addresses the problem of discovering the objects present in a collection of images without any supervision. We build on the optimization approach of Vo et al. [34] with several key novelties: (1) We propose a novel saliency-based region proposal algorithm that achieves significantly higher overlap with ground-truth objects than other competitive methods. This procedure leverages off-the-shelf CNN features trained on classification tasks without any bounding box information, but is otherwise unsupervised. (2) We exploit the inherent hierarchical structure of proposals as an effective regularizer for the approach to object discovery of [34], boosting its performance to significantly improve over the state of the art on several standard benchmarks. (3) We adopt a two-stage strategy to select promising proposals using small random sets of images before using the whole image collection to discover the objects it depicts, allowing us to tackle, for the first time (to the best of our knowledge), the discovery of multiple objects in each one of the pictures making up datasets with up to 20,000 images, an over five-fold increase compared to existing methods, and a first step toward true large-scale unsupervised image interpretation.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_46');
INSERT INTO `paper` VALUES (12883, 'Towards Analyzing Semantic Robustness of Deep Neural Networks', '', '', '', '', '', 'Despite the impressive performance of Deep Neural Networks (DNNs) on various vision tasks, they still exhibit erroneous high sensitivity toward semantic primitives (e.g. object pose). We propose a theoretically grounded analysis for DNN robustness in the semantic space. We qualitatively analyze different DNNs’ semantic robustness by visualizing the DNN global behavior as semantic maps and observe interesting behavior of some DNNs. Since generating these semantic maps does not scale well with the dimensionality of the semantic space, we develop a bottom-up approach to detect robust regions of DNNs. To achieve this, we formalize the problem of finding robust semantic regions of the network as optimizing integral bounds and we develop expressions for update directions of the region bounds. We use our developed formulations to quantitatively evaluate the semantic robustness of different popular network architectures. We show through extensive experimentation that several networks, while trained on the same dataset and enjoying comparable accuracy, do not necessarily perform similarly in semantic robustness. For example, InceptionV3 is more accurate despite being less semantically robust than ResNet50. We hope that this tool will serve as a milestone towards understanding the semantic robustness of DNNs.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_2');
INSERT INTO `paper` VALUES (12884, 'Towards Automated Testing and Robustification by Semantic Adversarial Data Generation', '', '', '', '', '', 'Widespread application of computer vision systems in real world tasks is currently hindered by their unexpected behavior on unseen examples. This occurs due to limitations of empirical testing on finite test sets and lack of systematic methods to identify the breaking points of a trained model. In this work we propose semantic adversarial editing, a method to synthesize plausible but difficult data points on which our target model breaks down. We achieve this with a differentiable object synthesizer which can change an object’s appearance while retaining its pose. Constrained adversarial optimization of object appearance through this synthesizer produces rare/difficult versions of an object which fool the target object detector. Experiments show that our approach effectively synthesizes difficult test data, dropping the performance of YoloV3 detector by more than 20 mAP points by changing the appearance of a single object and discovering failure modes of the model. The generated semantic adversarial data can also be used to robustify the detector through data augmentation, consistently improving its performance in both standard and out-of-dataset-distribution test sets, across three different datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_29');
INSERT INTO `paper` VALUES (12885, 'Towards Causal Benchmarking of Bias in Face Analysis Algorithms', 'Faces', 'Fairness', 'Bias', 'Causality', 'Counterfactuals', 'Measuring algorithmic bias is crucial both to assess algorithmic fairness, and to guide the improvement of algorithms. Current bias measurement methods in computer vision are based on observational datasets, and so conflate algorithmic bias with dataset bias. To address this problem we develop an experimental method for measuring algorithmic bias of face analysis algorithms, which directly manipulates the attributes of interest, e.g., gender and skin tone, in order to reveal causal links between attribute variation and performance change. Our method is based on generating synthetic image grids that differ along specific attributes while leaving other attributes constant. Crucially, we rely on the perception of human observers to control for synthesis inaccuracies when measuring algorithmic bias. We validate our method by comparing it to a traditional observational bias analysis study in gender classification algorithms. The two methods reach different conclusions. While the observational method reports gender and skin color biases, the experimental method reveals biases due to gender, hair length, age, and facial hair. We also show that our synthetic transects allow for more straightforward bias analysis on minority and intersectional groups.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_32');
INSERT INTO `paper` VALUES (12886, 'Towards Confirmable Automated Plant Cover Determination', 'Deep learning', 'Machine learning', 'Computer vision', 'Weakly supervised segmentation', 'Plants', 'Changes in plant community composition reflect environmental changes like in land-use and climate. While we have the means to record the changes in composition automatically nowadays, we still lack methods to analyze the generated data masses automatically.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_22');
INSERT INTO `paper` VALUES (12887, 'Towards Content-Independent Multi-Reference Super-Resolution: Adaptive Pattern Matching and Feature Aggregation', 'Super-resolution', 'Content-independent multi-reference', 'Universal reference pool', 'Local feature enhancement', '', 'Recovering realistic textures from a largely down-sampled low resolution (LR) image with complicated patterns is a challenging problem in image super-resolution. This work investigates a novel multi-reference based super-resolution problem by proposing a Content Independent Multi-Reference Super-Resolution (CIMR-SR) model, which is able to adaptively match the visual pattern between references and target image in the low resolution and enhance the feature representation of the target image in the higher resolution. CIMR-SR significantly improves the flexibility of the recently proposed reference-based super-resolution (RefSR), which needs to select the specific high-resolution reference (e.g., content similarity, camera view and relative scale) for each target image. In practice, a universal reference pool (RP) is built up for recovering all LR targets by searching the local matched patterns. By exploiting feature-based patch searching and attentive reference feature aggregation, the proposed CIMR-SR generates realistic images with much better perceptual quality and richer fine-details. Extensive experiments demonstrate the proposed CIMR-SR outperforms state-of-the-art methods in both qualitative and quantitative reconstructions.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_4');
INSERT INTO `paper` VALUES (12888, 'Towards Efficient Coarse-to-Fine Networks for Action and Gesture Recognition', 'Action and gesture recognition', 'Spatiotemporal coarse to fine decomposition', 'Weight reparameterization', 'Budgeted computation', '', 'State-of-the-art approaches to video-based action and gesture recognition often employ two key concepts: First, they employ multistream processing; second, they use an ensemble of convolutional networks. We improve and extend both aspects. First, we systematically yield enhanced receptive fields for complementary feature extraction via coarse-to-fine decomposition of input imagery along the spatial and temporal dimensions, and adaptively focus on training important feature pathways using a reparameterized fully connected layer. Second, we develop a ‘use when needed’ scheme with a ‘coarse-exit’ strategy that allows selective use of expensive high-resolution processing in a data-dependent fashion to retain accuracy while reducing computation cost. Our C2F learning approach builds ensemble networks that outperform most competing methods in terms of both reduced computation cost and improved accuracy on the Something-Something V1, V2, and Jester datasets, while also remaining competitive on the Kinetics-400 dataset. Uniquely, our C2F ensemble networks can operate at varying computation budget constraints.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_3');
INSERT INTO `paper` VALUES (12889, 'Towards End-to-End Video-Based Eye-Tracking', 'Eye tracking', 'Gaze estimation', 'Computer vision dataset', '', '', 'Estimating eye-gaze from images alone is a challenging task, in large parts due to un-observable person-specific factors. Achieving high accuracy typically requires labeled data from test users which may not be attainable in real applications. We observe that there exists a strong relationship between what users are looking at and the appearance of the user’s eyes. In response to this understanding, we propose a novel dataset and accompanying method which aims to explicitly learn these semantic and temporal relationships. Our video dataset consists of time-synchronized screen recordings, user-facing camera views, and eye gaze data, which allows for new benchmarks in temporal gaze tracking as well as label-free refinement of gaze. Importantly, we demonstrate that the fusion of information from visual stimuli as well as eye images can lead towards achieving performance similar to literature-reported figures acquired through supervised personalization. Our final method yields significant performance improvements on our proposed EVE dataset, with up to \\(28\\%\\) improvement in Point-of-Gaze estimates (resulting in \\(2.49^\\circ \\) in angular error), paving the path towards high-accuracy screen-based eye tracking purely from webcam sensors. The dataset and reference source code are available at https://ait.ethz.ch/projects/2020/EVE.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_44');
INSERT INTO `paper` VALUES (12890, 'Towards Fast, Accurate and Stable 3D Dense Face Alignment', '3D dense face alignment', '3D face reconstruction', '', '', '', 'Existing methods of 3D dthus limiting the scope of their practical applications. In this paper, we propose a novel regression framework which makes a balance among speed, accuracy and stability. Firstly, on the basis of a lightweight backbone, we propose a meta-joint optimization strategy to dynamically regress a small set of 3DMM parameters, which greatly enhances speed and accuracy simultaneously. To further improve the stability on videos, we present a virtual synthesis method to transform one still image to a short-video which incorporates in-plane and out-of-plane face moving. On the premise of high accuracy and stability, our model runs at 50 fps on a single CPU core and outperforms other state-of-the-art heavy models simultaneously. Experiments on several challenging datasets validate the efficiency of our method. The code and models will be available at https://github.com/cleardusk/3DDFA_V2.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_10');
INSERT INTO `paper` VALUES (12891, 'Towards Generalization Across Depth for Monocular 3D Object Detection', '', '', '', '', '', 'While expensive LiDAR and stereo camera rigs have enabled the development of successful 3D object detection methods, monocular RGB-only approaches lag much behind. This work advances the state of the art by introducing MoVi-3D, a novel, single-stage deep architecture for monocular 3D object detection. MoVi-3D builds upon a novel approach which leverages geometrical information to generate, both at training and test time, virtual views where the object appearance is normalized with respect to distance. These virtually generated views facilitate the detection task as they significantly reduce the visual appearance variability associated to objects placed at different distances from the camera. As a consequence, the deep model is relieved from learning depth-specific representations and its complexity can be significantly reduced. In particular, in this work we show that, thanks to our virtual views generation process, a lightweight, single-stage architecture suffices to set new state-of-the-art results on the popular KITTI3D benchmark.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_46');
INSERT INTO `paper` VALUES (12892, 'Towards Part-Aware Monocular 3D Human Pose Estimation: An Architecture Search Approach', '3D pose estimation', 'Body parts', 'Neural architecture search', '', '', 'Even though most existing monocular 3D pose estimation approaches achieve very competitive results, they ignore the heterogeneity among human body parts by estimating them with the same network architecture. To accurately estimate 3D poses of different body parts, we attempt to build a part-aware 3D pose estimator by searching a set of network architectures. Consequently, our model automatically learns to select a suitable architecture to estimate each body part. Compared to models built on the commonly used ResNet-50 backbone, it reduces 62% parameters and achieves better performance. With roughly the same computational complexity as previous models, our approach achieves state-of-the-art results on both the single-person and multi-person 3D pose estimation benchmarks.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_42');
INSERT INTO `paper` VALUES (12893, 'Towards Practical and Efficient High-Resolution HDR Deghosting with CNN', '', '', '', '', '', 'Generating High Dynamic Range (HDR) image in the presence of camera and object motion is a tedious task. If uncorrected, these motions will manifest as ghosting artifacts in the fused HDR image. On one end of the spectrum, there exist methods that generate high-quality results that are computationally demanding and too slow. On the other end, there are few faster methods that produce unsatisfactory results. With ever increasing sensor/display resolution, currently we are very much in need of faster methods that produce high-quality images. In this paper, we present a deep neural network based approach to generate high-quality ghost-free HDR for high-resolution images. Our proposed method is fast and fuses a sequence of three high-resolution images (16-megapixel resolution) in about 10 s. Through experiments and ablations, on different publicly available datasets, we show that the proposed method achieves state-of-the-art performance in terms of accuracy and speed.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_30');
INSERT INTO `paper` VALUES (12894, 'Towards Precise Completion of Deformable Shapes', 'Shape completion', '3D deep learning', 'Shape analysis', '', '', 'According to Aristotle, “the whole is greater than the sum of its parts”. This statement was adopted to explain human perception by the Gestalt psychology school of thought in the twentieth century. Here, we claim that when observing a part of an object which was previously acquired as a whole, one could deal with both partial correspondence and shape completion in a holistic manner. More specifically, given the geometry of a full, articulated object in a given pose, as well as a partial scan of the same object in a different pose, we address the new problem of matching the part to the whole while simultaneously reconstructing the new pose from its partial observation. Our approach is data-driven and takes the form of a Siamese autoencoder without the requirement of a consistent vertex labeling at inference time; as such, it can be used on unorganized point clouds as well as on triangle meshes. We demonstrate the practical effectiveness of our model in the applications of single-view deformable shape completion and dense shape correspondence, both on synthetic and real-world geometric data, where we outperform prior work by a large margin.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_22');
INSERT INTO `paper` VALUES (12895, 'Towards Real-Time Multi-Object Tracking', 'Multi-Object Tracking', '', '', '', '', 'Modern multiple object tracking (MOT) systems usually follow the tracking-by-detection paradigm. It has 1) a detection model for target localization and 2) an appearance embedding model for data association. Having the two models separately executed might lead to efficiency problems, as the running time is simply a sum of the two steps without investigating potential structures that can be shared between them. Existing research efforts on real-time MOT usually focus on the association step, so they are essentially real-time association methods but not real-time MOT system. In this paper, we propose an MOT system that allows target detection and appearance embedding to be learned in a shared model. Specifically, we incorporate the appearance embedding model into a single-shot detector, such that the model can simultaneously output detections and the corresponding embeddings. We further propose a simple and fast association method that works in conjunction with the joint model. In both components the computation cost is significantly reduced compared with former MOT systems, resulting in a neat and fast baseline for future follow-ups on real-time MOT algorithm design. To our knowledge, this work reports the first (near) real-time MOT system, with a running speed of 22 to 40 FPS depending on the input resolution. Meanwhile, its tracking accuracy is comparable to the state-of-the-art trackers embodying separate detection and embedding (SDE) learning (\\(64.4\\%\\) MOTA v.s. \\(66.1\\%\\) MOTA on MOT-16 challenge). Code and models are available at https://github.com/Zhongdao/Towards-Realtime-MOT.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_7');
INSERT INTO `paper` VALUES (12896, 'Towards Recognizing Unseen Categories in Unseen Domains', 'Zero-Shot Learning', 'Domain Generalization', '', '', '', 'Current deep visual recognition systems suffer from severe performance degradation when they encounter new images from classes and scenarios unseen during training. Hence, the core challenge of Zero-Shot Learning (ZSL) is to cope with the semantic-shift whereas the main challenge of Domain Adaptation and Domain Generalization (DG) is the domain-shift. While historically ZSL and DG tasks are tackled in isolation, this work develops with the ambitious goal of solving them jointly, i.e. by recognizing unseen visual concepts in unseen domains. We present CuMix (Curriculum Mixup for recognizing unseen categories in unseen domains), a holistic algorithm to tackle ZSL, DG and ZSL+DG. The key idea of CuMix is to simulate the test-time domain and semantic shift using images and features from unseen domains and categories generated by mixing up the multiple source domains and categories available during training. Moreover, a curriculum-based mixing policy is devised to generate increasingly complex training samples. Results on standard ZSL and DG datasets and on ZSL+DG using the DomainNet benchmark demonstrate the effectiveness of our approach.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_28');
INSERT INTO `paper` VALUES (12897, 'Towards Reliable Evaluation of Algorithms for Road Network Reconstruction from Aerial Images', '', '', '', '', '', 'Existing connectivity-oriented performance measures rank road delineation algorithms inconsistently, which makes it difficult to decide which one is best for a given application. We show that these inconsistencies stem from design flaws that make the metrics insensitive to whole classes of errors. This insensitivity is undesirable in metrics intended for capturing overall general quality of road reconstructions. In particular, the scores do not reflect the time needed for a human to fix the errors, because each one has to be fixed individually. To provide more reliable evaluation, we design three new metrics that are sensitive to all classes of errors. This sensitivity makes them more consistent even though they use very different approaches to comparing ground-truth and reconstructed road networks. We use both synthetic and real data to demonstrate this and advocate the use of these corrected metrics as a tool to gauge future progress.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_42');
INSERT INTO `paper` VALUES (12898, 'Towards Streaming Perception', '', '', '', '', '', 'Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular image frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as “streaming accuracy”. The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any image understanding task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal “sweet spot” that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming image understanding, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and “doing nothing”.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_28');
INSERT INTO `paper` VALUES (12899, 'Towards Unique and Informative Captioning of Images', '', '', '', '', '', 'Despite considerable progress, state of the art image captioning models produce generic captions, leaving out important image details. Furthermore, these systems may even misrepresent the image in order to produce a simpler caption consisting of common concepts. In this paper, we first analyze both modern captioning systems and evaluation metrics through empirical experiments to quantify these phenomena. We find that modern captioning systems return higher likelihoods for incorrect distractor sentences compared to ground truth captions, and that evaluation metrics like SPICE can be ‘topped’ using simple captioning systems relying on object detectors. Inspired by these observations, we design a new metric (SPICE-U) by introducing a notion of uniqueness over the concepts generated in a caption. We show that SPICE-U is better correlated with human judgements compared to SPICE, and effectively captures notions of diversity and descriptiveness. Finally, we also demonstrate a general technique to improve any existing captioning model – by using mutual information as a re-ranking objective during decoding. Empirically, this results in more unique and informative captions, and improves three different state-of-the-art models on SPICE-U as well as average score over existing metrics (Code is available at https://github.com/princetonvisualai/SPICE-U).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_37');
INSERT INTO `paper` VALUES (12900, 'TP-LSD: Tri-Points Based Line Segment Detector', 'Line segment detection', 'Low-level vision', 'Deep learning', '', '', 'This paper proposes a novel deep convolutional model, Tri-Points Based Line Segment Detector (TP-LSD), to detect line segments in an image at real-time speed. The previous related methods typically use the two-step strategy, relying on either heuristic post-process or extra classifier. To realize one-step detection with a faster and more compact model, we introduce the tri-points representation, converting the line segment detection to the end-to-end prediction of a root-point and two endpoints for each line segment. TP-LSD has two branches: tri-points extraction branch and line segmentation branch. The former predicts the heat map of root-points and the two displacement maps of endpoints. The latter segments the pixels on straight lines out from background. Moreover, the line segmentation map is reused in the first branch as structural prior. We propose an additional novel evaluation metric and evaluate our method on Wireframe and YorkUrban datasets, demonstrating not only the competitive accuracy compared to the most recent methods, but also the real-time run speed up to 78 FPS with the \\(320\\times 320\\) input.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_46');
INSERT INTO `paper` VALUES (12901, 'TPFN: Applying Outer Product Along Time to Multimodal Sentiment Analysis Fusion on Incomplete Data', 'Multimodal sentiment analysis', 'Multimodal learning', 'Matrix/tensor decomposition', 'Incomplete data', '', 'Multimodal sentiment analysis (MSA) has been widely investigated in both computer vision and natural language processing. However, studies on the imperfect data especially with missing values are still far from success and challenging, even though such an issue is ubiquitous in the real world. Although previous works show the promising performance by exploiting the low-rank structures of the fused features, only the first-order statistics of the temporal dynamics are concerned. To this end, we propose a novel network architecture termed Time Product Fusion Network (TPFN), which takes the high-order statistics over both modalities and temporal dynamics into account. We construct the fused features by the outer product along adjacent time-steps, such that richer modal and temporal interactions are utilized. In addition, we claim that the low-rank structures can be obtained by regularizing the Frobenius norm of latent factors instead of the fused features. Experiments on CMU-MOSI and CMU-MOSEI datasets show that TPFN can compete with state-of-the art approaches in multimodal sentiment analysis in cases of both random and structured missing values.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_26');
INSERT INTO `paper` VALUES (12902, 'Tracking Emerges by Looking Around Static Scenes, with Neural 3D Mapping', '', '', '', '', '', 'We hypothesize that an agent that can look around in static scenes can learn rich visual representations applicable to 3D object tracking in complex dynamic scenes. We are motivated in this pursuit by the fact that the physical world itself is mostly static, and multiview correspondence labels are relatively cheap to collect in static scenes, e.g., by triangulation. We propose to leverage multiview data of static points in arbitrary scenes (static or dynamic), to learn a neural 3D mapping module which produces features that are correspondable across time. The neural 3D mapper consumes RGB-D data as input, and produces a 3D voxel grid of deep features as output. We train the voxel features to be correspondable across viewpoints, using a contrastive loss, and correspondability across time emerges automatically. At test time, given an RGB-D video with approximate camera poses, and given the 3D box of an object to track, we track the target object by generating a map of each timestep and locating the object’s features within each map. In contrast to models that represent video streams in 2D or 2.5D, our model’s 3D scene representation is disentangled from projection artifacts, is stable under camera motion, and is robust to partial occlusions. We test the proposed architectures in challenging simulated and real data, and show that our unsupervised 3D object trackers outperform prior unsupervised 2D and 2.5D trackers, and approach the accuracy of supervised trackers. This work demonstrates that 3D object trackers can emerge without tracking labels, through multiview self-supervision on static data.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_36');
INSERT INTO `paper` VALUES (12903, 'Tracking Objects as Points', 'Multi-object tracking', 'Conditioned detection', '3D object tracking', '', '', 'Tracking has traditionally been the art of following interest points through space and time. This changed with the rise of powerful deep networks. Nowadays, tracking is dominated by pipelines that perform object detection followed by temporal association, also known as tracking-by-detection. We present a simultaneous detection and tracking algorithm that is simpler, faster, and more accurate than the state of the art. Our tracker, CenterTrack, applies a detection model to a pair of images and detections from the prior frame. Given this minimal input, CenterTrack localizes objects and predicts their associations with the previous frame. That’s it. CenterTrack is simple, online (no peeking into the future), and real-time. It achieves \\(67.8\\%\\) MOTA on the MOT17 challenge at 22 FPS and \\(89.4\\%\\) MOTA on the KITTI tracking benchmark at 15 FPS, setting a new state of the art on both datasets. CenterTrack is easily extended to monocular 3D tracking by regressing additional 3D attributes. Using monocular video input, it achieves \\(28.3\\%\\) AMOTA@0.2 on the newly released nuScenes 3D tracking benchmark, substantially outperforming the monocular baseline on this benchmark while running at 28 FPS.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_28');
INSERT INTO `paper` VALUES (12904, 'TRADI: Tracking Deep Neural Network Weight Distributions', 'Deep Neural Networks', 'Weight distribution', 'Uncertainty', 'Ensembles', 'Out-of-distribution detection', 'During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by aggregating predictions from an ensemble of networks sampled from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does neither require any change in the architecture, nor in the training procedure. We evaluate our method, TRADI, on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to ensemble approaches.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_7');
INSERT INTO `paper` VALUES (12905, 'Traffic Accident Benchmark for Causality Recognition', '', '', '', '', '', 'We propose a brand new benchmark for analyzing causality in traffic accident videos by decomposing an accident into a pair of events, cause and effect. We collect videos containing traffic accident scenes and annotate cause and effect events for each accident with their temporal intervals and semantic labels; such annotations are not available in existing datasets for accident anticipation task. Our dataset has the following two advantages over the existing ones, which would facilitate practical research for causality analysis. First, the decomposition of an accident into cause and effect events provides atomic cues for reasoning on a complex environment and planning future actions. Second, the prediction of cause and effect in an accident makes a system more interpretable to humans, which mitigates the ambiguity of legal liabilities among agents engaged in the accident. Using the proposed dataset, we analyze accidents by localizing the temporal intervals of their causes and effects and classifying the semantic labels of the accidents. The dataset as well as the implementations of baseline models are available in the code repository (https://github.com/tackgeun/CausalityInTrafficAccident).', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_32');
INSERT INTO `paper` VALUES (12906, 'Training Interpretable Convolutional Neural Networks by Differentiating Class-Specific Filters', 'Class-specific filters', 'Interpretability', 'Disentangled representation', 'Filter-class entanglement', 'Gate', 'Convolutional neural networks (CNNs) have been successfully used in a range of tasks. However, CNNs are often viewed as “black-box” and lack of interpretability. One main reason is due to the filter-class entanglement – an intricate many-to-many correspondence between filters and classes. Most existing works attempt post-hoc interpretation on a pre-trained model, while neglecting to reduce the entanglement underlying the model. In contrast, we focus on alleviating filter-class entanglement during training. Inspired by cellular differentiation, we propose a novel strategy to train interpretable CNNs by encouraging class-specific filters, among which each filter responds to only one (or few) class. Concretely, we design a learnable sparse Class-Specific Gate (CSG) structure to assign each filter with one (or few) class in a flexible way. The gate allows a filter’s activation to pass only when the input samples come from the specific class. Extensive experiments demonstrate the fabulous performance of our method in generating a sparse and highly class-related representation of the input, which leads to stronger interpretability. Moreover, comparing with the standard training strategy, our model displays benefits in applications like object localization and adversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_37');
INSERT INTO `paper` VALUES (12907, 'Trajectron++: Dynamically-Feasible Trajectory Forecasting with Heterogeneous Data', 'Trajectory forecasting', 'Spatiotemporal graph modeling', 'Human-robot interaction', 'Autonomous driving', '', 'Reasoning about human motion is an important prerequisite to safe and socially-aware robotic navigation. As a result, multi-agent behavior prediction has become a core component of modern human-robot interactive systems, such as self-driving cars. While there exist many methods for trajectory forecasting, most do not enforce dynamic constraints and do not account for environmental information (e.g., maps). Towards this end, we present Trajectron++, a modular, graph-structured recurrent model that forecasts the trajectories of a general number of diverse agents while incorporating agent dynamics and heterogeneous data (e.g., semantic maps). Trajectron++ is designed to be tightly integrated with robotic planning and control frameworks; for example, it can produce predictions that are optionally conditioned on ego-agent motion plans. We demonstrate its performance on several challenging real-world trajectory forecasting datasets, outperforming a wide array of state-of-the-art deterministic and generative methods.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_40');
INSERT INTO `paper` VALUES (12908, 'Transform Domain Pyramidal Dilated Convolution Networks for Restoration of Under Display Camera Images', 'Under-display camera', 'Image restoration', 'Wavelet', 'T-OLED', 'P-OLED', 'Under-display camera (UDC) is a novel technology that can make digital imaging experience in handheld devices seamless by providing large screen-to-body ratio. UDC images are severely degraded owing to their positioning under a display screen. This work addresses the restoration of images degraded as a result of UDC imaging. Two different networks are proposed for the restoration of images taken with two types of UDC technologies. The first method uses a pyramidal dilated convolution within a wavelet decomposed convolutional neural network for pentile-organic LED (P-OLED) based display system. The second method employs pyramidal dilated convolution within a discrete cosine transform based dual domain network to restore images taken using a transparent-organic LED (T-OLED) based UDC system. The first method produced very good quality restored images and was the winning entry in European Conference on Computer Vision (ECCV) 2020 challenge on image restoration for Under-display Camera - Track 2 - P-OLED evaluated based on PSNR and SSIM. The second method scored \\(4^{th}\\) position in Track-1 (T-OLED) of the challenge evaluated based on the same metrics.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_28');
INSERT INTO `paper` VALUES (12909, 'Transformation Consistency Regularization – A Semi-supervised Paradigm for Image-to-Image Translation', '', '', '', '', '', 'Scarcity of labeled data has motivated the development of semi-supervised learning methods, which learn from large portions of unlabeled data alongside a few labeled samples. Consistency Regularization between model’s predictions under different input perturbations, particularly has shown to provide state-of-the art results in a semi-supervised framework. However, most of these method have been limited to classification and segmentation applications. We propose Transformation Consistency Regularization, which delves into a more challenging setting of image-to-image translation, which remains unexplored by semi-supervised algorithms. The method introduces a diverse set of geometric transformations and enforces the model’s predictions for unlabeled data to be invariant to those transformations. We evaluate the efficacy of our algorithm on three different applications: image colorization, denoising and super-resolution. Our method is significantly data efficient, requiring only around 10–20% of labeled samples to achieve similar image reconstructions to its fully-supervised counterpart. Furthermore, we show the effectiveness of our method in video processing applications, where knowledge from a few frames can be leveraged to enhance the quality of the rest of the movie.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_35');
INSERT INTO `paper` VALUES (12910, 'Transforming and Projecting Images into Class-Conditional Generative Networks', '', '', '', '', '', 'We present a method for projecting an input image into the space of a class-conditional generative neural network. We propose a method that optimizes for transformation to counteract the model biases in generative neural networks. Specifically, we demonstrate that one can solve for image translation, scale, and global color transformation, during the projection optimization to address the object-center bias and color bias of a Generative Adversarial Network. This projection process poses a difficult optimization problem, and purely gradient-based optimizations fail to find good solutions. We describe a hybrid optimization strategy that finds good projections by estimating transformations and class parameters. We show the effectiveness of our method on real images and further demonstrate how the corresponding projections lead to better editability of these images. The project page and the code is available at https://minyoungg.github.io/GAN-Transform-and-Project/.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_2');
INSERT INTO `paper` VALUES (12911, 'Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning', 'Semi-Supervised Learning', 'Hierarchical optimal transport', '', '', '', 'Semi-Supervised Learning (SSL) based on Convolutional Neural Networks (CNNs) have recently been proven as powerful tools for standard tasks such as image classification when there is not a sufficient amount of labeled data available during the training. In this work, we consider the general setting of the SSL problem for image classification, where the labeled and unlabeled data come from the same underlying distribution. We propose a new SSL method that adopts a hierarchical Optimal Transport (OT) technique to find a mapping from empirical unlabeled measures to corresponding labeled measures by leveraging the minimum amount of transportation cost in the label space. Based on this mapping, pseudo-labels for the unlabeled data are inferred, which are then used along with the labeled data for training the CNN. We evaluated and compared our method with state-of-the-art SSL approaches on standard datasets to demonstrate the superiority of our SSL method.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_30');
INSERT INTO `paper` VALUES (12912, 'TRRNet: Tiered Relation Reasoning for Compositional Visual Question Answering', 'Visual question answering', 'Visual reasoning', '', '', '', 'Compositional visual question answering requires reasoning over both semantic and geometry object relations. We propose a novel tiered reasoning method that dynamically selects object level candidates based on language representations and generates robust pairwise relations within the selected candidate objects. The proposed tiered relation reasoning method can be compatible with the majority of the existing visual reasoning frameworks, leading to significant performance improvement with very little extra computational cost. Moreover, we propose a policy network that decides the appropriate reasoning steps based on question complexity and current reasoning status. In experiments, our model achieves state-of-the-art performance on two VQA datasets.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_25');
INSERT INTO `paper` VALUES (12913, 'Truncated Inference for Latent Variable Optimization Problems: Application to Robust Estimation and Learning', 'Majorization-minimization', 'Latent variable models', 'Stochastic gradient methods', '', '', 'Optimization problems with an auxiliary latent variable structure in addition to the main model parameters occur frequently in computer vision and machine learning. The additional latent variables make the underlying optimization task expensive, either in terms of memory (by maintaining the latent variables), or in terms of runtime (repeated exact inference of latent variables). We aim to remove the need to maintain the latent variables and propose two formally justified methods, that dynamically adapt the required accuracy of latent variable inference. These methods have applications in large scale robust estimation and in learning energy-based models from labeled data.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_28');
INSERT INTO `paper` VALUES (12914, 'TSIT: A Simple and Versatile Framework for Image-to-Image Translation', '', '', '', '', '', 'We introduce a simple and versatile framework for image-to-image translation. We unearth the importance of normalization layers, and provide a carefully designed two-stream generative model with newly proposed feature transformations in a coarse-to-fine fashion. This allows multi-scale semantic structure information and style representation to be effectively captured and fused by the network, permitting our method to scale to various tasks in both unsupervised and supervised settings. No additional constraints (e.g., cycle consistency) are needed, contributing to a very clean and simple method. Multi-modal image synthesis with arbitrary style control is made possible. A systematic study compares the proposed method with several state-of-the-art task-specific baselines, verifying its effectiveness in both perceptual quality and quantitative evaluations. GitHub: https://github.com/EndlessSora/TSIT.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_13');
INSERT INTO `paper` VALUES (12915, 'TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images', 'Image-to-Image Translation', 'Generative adversarial network', 'One-shot unsupervised learning', '', '', 'An unsupervised image-to-image translation (UI2I) task deals with learning a mapping between two domains without paired images. While existing UI2I methods usually require numerous unpaired images from different domains for training, there are many scenarios where training data is quite limited. In this paper, we argue that even if each domain contains a single image, UI2I can still be achieved. To this end, we propose TuiGAN, a generative model that is trained on only two unpaired images and amounts to one-shot unsupervised learning. With TuiGAN, an image is translated in a coarse-to-fine manner where the generated image is gradually refined from global structures to local details. We conduct extensive experiments to verify that our versatile method can outperform strong baselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of achieving comparable performance with the state-of-the-art UI2I models trained with sufficient data.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_2');
INSERT INTO `paper` VALUES (12916, 'TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval', '', '', '', '', '', 'We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic. The dataset contains 109K queries collected on 21.8K videos from 6 TV shows of diverse genres, where each query is associated with a tight temporal window. The queries are also labeled with query types that indicate whether each of them is more related to video or subtitle or both, allowing for in-depth analysis of the dataset and the methods that built on top of it. Strict qualification and post-annotation verification tests are applied to ensure the quality of the collected data. Additionally, we present several baselines and a novel Cross-modal Moment Localization (XML) network for multimodal moment retrieval tasks. The proposed XML model uses a late fusion design with a novel Convolutional Start-End detector (ConvSE), surpassing baselines by a large margin and with better efficiency, providing a strong starting point for future work. (TVR dataset and code are publicly available: https://tvr.cs.unc.edu/. We also introduce TVC for multimodal captioning at https://tvr.cs.unc.edu/tvc.html).', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_27');
INSERT INTO `paper` VALUES (12917, 'Two Stream Active Query Suggestion for Active Learning in Connectomics', 'Active learning', 'Connectomics', 'Object detection', 'Semantic segmentation', 'Image classification', 'For large-scale vision tasks in biomedical images, the labeled data is often limited to train effective deep models. Active learning is a common solution, where a query suggestion method selects representative unlabeled samples for annotation, and the new labels are used to improve the base model. However, most query suggestion models optimize their learnable parameters only on the limited labeled data and consequently become less effective for the more challenging unlabeled data. To tackle this, we propose a two-stream active query suggestion approach. In addition to the supervised feature extractor, we introduce an unsupervised one optimized on all raw images to capture diverse image features, which can later be improved by fine-tuning on new labels. As a use case, we build an end-to-end active learning framework with our query suggestion method for 3D synapse detection and mitochondria segmentation in connectomics. With the framework, we curate, to our best knowledge, the largest connectomics dataset with dense synapses and mitochondria annotation. On this new dataset, our method outperforms previous state-of-the-art methods by 3.1% for synapse and 3.8% for mitochondria in terms of region-of-interest proposal accuracy. We also apply our method to image classification, where it outperforms previous approaches on CIFAR-10 under the same limited annotation budget. The project page is https://zudi-lin.github.io/projects/#two_stream_active.', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_7');
INSERT INTO `paper` VALUES (12918, 'Two-Branch Recurrent Network for Isolating Deepfakes in Videos', 'Deepfake detection', 'Two-branch recurrent net', 'Loss function', '', '', 'The current spike of hyper-realistic faces artificially generated using deepfakes calls for media forensics solutions that are tailored to video streams and work reliably with a low false alarm rate at the video level. We present a method for deepfake detection based on a two-branch network structure that isolates digitally manipulated faces by learning to amplify artifacts while suppressing the high-level face content. Unlike current methods that extract spatial frequencies as a preprocessing step, we propose a two-branch structure: one branch propagates the original information, while the other branch suppresses the face content yet amplifies multi-band frequencies using a Laplacian of Gaussian (LoG) as a bottleneck layer. To better isolate manipulated faces, we derive a novel cost function that, unlike regular classification, compresses the variability of natural faces and pushes away the unrealistic facial samples in the feature space. Our two novel components show promising results on the FaceForensics+ +, Celeb-DF, and Facebook’s DFDC preview benchmarks, when compared to prior work. We then offer a full, detailed ablation study of our network architecture and cost function. Finally, although the bar is still high to get very remarkable figures at a very low false alarm rate, our study shows that we can achieve good video-level performance when cross-testing in terms of video-level AUC.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_39');
INSERT INTO `paper` VALUES (12919, 'Two-Phase Pseudo Label Densification for Self-training Based Domain Adaptation', 'Unsupervised domain adaptataion', 'Self-training', '', '', '', 'Recently, deep self-training approaches emerged as a powerful solution to the unsupervised domain adaptation. The self-training scheme involves iterative processing of target data; it generates target pseudo labels and retrains the network. However, since only the confident predictions are taken as pseudo labels, existing self-training approaches inevitably produce sparse pseudo labels in practice. We see this is critical because the resulting insufficient training-signals lead to a sub-optimal, error-prone model. In order to tackle this problem, we propose a novel Two-phase Pseudo Label Densification framework, referred to as TPLD. In the first phase, we use sliding window voting to propagate the confident predictions, utilizing intrinsic spatial-correlations in the images. In the second phase, we perform a confidence-based easy-hard classification. For the easy samples, we now employ their full pseudo-labels. For the hard ones, we instead adopt adversarial learning to enforce hard-to-easy feature alignment. To ease the training process and avoid noisy predictions, we introduce the bootstrapping mechanism to the original self-training loss. We show the proposed TPLD can be easily integrated into existing self-training based approaches and improves the performance significantly. Combined with the recently proposed CRST self-training framework, we achieve new state-of-the-art results on two standard UDA benchmarks.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_32');
INSERT INTO `paper` VALUES (12920, 'Two-Stage Training for Improved Classification of Poorly Localized Object Images', 'Poorly-localized objects', 'Low-quality bounding boxes', 'Limited quality', 'Noisy', 'Occluded bounding boxes', 'State-of-the-art object classifiers finetuned from a pretrained (e.g. from ImageNet) model on a domain-specific dataset can accurately classify well-localized object images. However, such classifiers often fail on poorly localized images (images with lots of context, heavily occluded/partially visible, and off-centered objects). In this paper, we propose a two-stage training scheme to improve the classification of such noisy detections, often produced by low-compute algorithms such as motion based background removal techniques that run on the edge. The proposed two-stage training pipeline first trains a classifier from scratch with extreme image augmentation, followed by finetuning in the second stage. The first stage incorporates a lot of contextual information around the objects, given access to the corresponding full images. This stage works very well for classification of poorly localized input images, but generates a lot of false positives by classifying non-object images as objects. To reduce the false positives, a second training is done on the tight ground-truth bounding boxes (as done traditionally) by using the trained model in the first stage as the initial model and very slowly adjusting its weights during the training. To demonstrate the efficacy of our approach, we curated a new classification dataset for poorly localized images - noisy PASCAL VOC 2007 test dataset. Using this dataset, we show that the proposed two-stage training scheme can significantly improve the accuracy of the trained classifier on both well-localized and poorly-localized object images.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_18');
INSERT INTO `paper` VALUES (12921, 'Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization', 'Temporal action localization', 'Weakly-supervised learning', '', '', '', 'Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and localize all action instances in an untrimmed video under only video-level supervision. However, without frame-level annotations, it is challenging for W-TAL methods to identify false positive action proposals and generate action proposals with precise temporal boundaries. In this paper, we present a Two-Stream Consensus Network (TSCN) to simultaneously address these challenges. The proposed TSCN features an iterative refinement training method, where a frame-level pseudo ground truth is iteratively updated, and used to provide frame-level supervision for improved model training and false positive action proposal elimination. Furthermore, we propose a new attention normalization loss to encourage the predicted attention to act like a binary selection, and promote the precise localization of action instance boundaries. Experiments conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN outperforms current state-of-the-art methods, and even achieves comparable results with some recent fully-supervised methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_3');
INSERT INTO `paper` VALUES (12922, 'UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods and Results', 'Under-display camera', 'Image restoration', 'Denoising', 'Debluring', '', 'This paper is the report of the first Under-Display Camera (UDC) image restoration challenge in conjunction with the RLQ workshop at ECCV 2020. The challenge is based on a newly-collected database of Under-Display Camera. The challenge tracks correspond to two types of display: a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams registered the challenge, eight and nine teams submitted the results during the testing phase for each track. The results in the paper are state-of-the-art restoration performance of Under-Display Camera Restoration. Datasets and paper are available at https://yzhouas.github.io/projects/UDC/udc.html.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_26');
INSERT INTO `paper` VALUES (12923, 'UFO\\(^2\\): A Unified Framework Towards Omni-supervised Object Detection', 'Omni-supervised', 'Weakly-supervised', 'Object detection', '', '', 'Existing work on object detection often relies on a single form of annotation: the model is trained using either accurate yet costly bounding boxes or cheaper but less expressive image-level tags. However, real-world annotations are often diverse in form, which challenges these existing works. In this paper, we present UFO\\(^2\\), a unified object detection framework that can handle different forms of supervision simultaneously. Specifically, UFO\\(^2\\) incorporates strong supervision (e.g., boxes), various forms of partial supervision (e.g., class tags, points, and scribbles), and unlabeled data. Through rigorous evaluations, we demonstrate that each form of label can be utilized to either train a model from scratch or to further improve a pre-trained model. We also use UFO\\(^2\\) to investigate budget-aware omni-supervised learning, i.e., various annotation policies are studied under a fixed annotation budget: we show that competitive performance needs no strong labels for all data. Finally, we demonstrate the generalization of UFO\\(^2\\), detecting more than 1,000 different objects without bounding box annotations.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_18');
INSERT INTO `paper` VALUES (12924, 'Ultra Fast Structure-Aware Deep Lane Detection', 'Lane detection', 'Fast formulation', 'Structural loss', 'Anchor', '', 'Modern methods mainly regard lane detection as a problem of pixel-wise segmentation, which is struggling to address the problem of challenging scenarios and speed. Inspired by human perception, the recognition of lanes under severe occlusion and extreme lighting conditions is mainly based on contextual and global information. Motivated by this observation, we propose a novel, simple, yet effective formulation aiming at extremely fast speed and challenging scenarios. Specifically, we treat the process of lane detection as a row-based selecting problem using global features. With the help of row-based selecting, our formulation could significantly reduce the computational cost. Using a large receptive field on global features, we could also handle the challenging scenarios. Moreover, based on the formulation, we also propose a structural loss to explicitly model the structure of lanes. Extensive experiments on two lane detection benchmark datasets show that our method could achieve the state-of-the-art performance in terms of both speed and accuracy. A light weight version could even achieve 300+ frames per second with the same resolution, which is at least 4x faster than previous state-of-the-art methods. Our code is available at https://github.com/cfzd/Ultra-Fast-Lane-Detection.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_17');
INSERT INTO `paper` VALUES (12925, 'Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos', 'Spatio-temporal action recognition', 'Weak supervision', 'Video understanding', 'Mulitple Instance Learning', '', 'Despite the recent advances in video classification, progress in spatio-temporal action recognition has lagged behind. A major contributing factor has been the prohibitive cost of annotating videos frame-by-frame. In this paper, we present a spatio-temporal action recognition model that is trained with only video-level labels, which are significantly easier to annotate. Our method leverages per-frame person detectors which have been trained on large image datasets within a Multiple Instance Learning framework. We show how we can apply our method in cases where the standard Multiple Instance Learning assumption, that each bag contains at least one instance with the specified label, is invalid using a novel probabilistic variant of MIL where we estimate the uncertainty of each prediction. Furthermore, we report the first weakly-supervised results on the AVA dataset and state-of-the-art results among weakly-supervised methods on UCF101-24.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_44');
INSERT INTO `paper` VALUES (12926, 'Unconstrained Text Detection in Manga: A New Dataset and Baseline', 'Text-binarization', 'Text-datasets', 'Binarization-evaluation', 'Neural-networks', 'Japanese-text-detection', 'The detection and recognition of unconstrained text is an open problem in research. Text in comic books has unusual styles that raise many challenges for text detection. This work aims to binarize text in a comic genre with highly sophisticated text styles: Japanese manga. To overcome the lack of a manga dataset with text annotations at a pixel level, we create our own. To improve the evaluation and search of an optimal model, in addition to standard metrics in binarization, we implement other special metrics. Using these resources, we designed and evaluated a deep network model, outperforming current methods for text binarization in manga in most metrics.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_38');
INSERT INTO `paper` VALUES (12927, 'Understanding Compositional Structures in Art Historical Images Using Pose and Gaze Priors', 'Compositional structures', 'Art history', 'Computer vision', '', '', 'Image compositions as a tool for analysis of artworks is of extreme significance for art historians. These compositions are useful in analyzing the interactions in an image to study artists and their artworks. Max Imdahl in his work called Ikonik, along with other prominent art historians of the 20\\(^\\mathrm{th}\\) century, underlined the aesthetic and semantic importance of the structural composition of an image. Understanding underlying compositional structures within images is challenging and a time consuming task. Generating these structures automatically using computer vision techniques (1) can help art historians towards their sophisticated analysis by saving lot of time; providing an overview and access to huge image repositories and (2) also provide an important step towards an understanding of man made imagery by machines. In this work, we attempt to automate this process using the existing state of the art machine learning techniques, without involving any form of training. Our approach, inspired by Max Imdahl’s pioneering work, focuses on two central themes of image composition: (a) detection of action regions and action lines of the artwork; and (b) pose-based segmentation of foreground and background. Currently, our approach works for artworks comprising of protagonists (persons) in an image. In order to validate our approach qualitatively and quantitatively, we conduct a user study involving experts and non-experts. The outcome of the study highly correlates with our approach and also demonstrates its domain-agnostic capability. We have open-sourced the code: https://github.com/image-compostion-canvas-group/image-compostion-canvas', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_9');
INSERT INTO `paper` VALUES (12928, 'Understanding Political Communication Styles in Televised Debates via Body Movements', '', '', '', '', '', 'Televised political debates have received much attention by scholars in political communication and social psychology who study nonverbal cues in interpersonal communication and their impact on candidate evaluations. An abundance of political multimedia and new platforms have required leaders to develop an effective and unique communication “style” which may rely on nonverbal devices such as face and body. Emotions conveyed by expressive gestures of candidates during debates have been shown to elicit stronger reactions from the public than rhetorical statements alone. Candidates, for example, may exploit assertive and aggressive gestures to communicate their confidence and attract supporters. Existing studies, however, are based largely on manual coding of human gestures, which may not be scalable or reproducible. The main objectives of our paper are to investigate the role of body movements of candidates using a systematic and automated approach as well as understand the context and effects of gestures. For this analysis, we collected a dataset of political debate videos from the 2020 Democratic presidential primaries and analyzed facial expressions and gestures of candidates. Our preliminary analysis demonstrates that candidates employ gestures to varying extents, and the amount of body movement is correlated with emotions conveyed in the candidates’ facial expressions. We discuss our dataset, preliminary results, and future directions in the following sections.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_55');
INSERT INTO `paper` VALUES (12929, 'Unified Image and Video Saliency Modeling', 'Visual saliency', 'Video saliency', 'Domain adaptation', '', '', 'Visual saliency modeling for images and videos is treated as two independent tasks in recent computer vision literature. While image saliency modeling is a well-studied problem and progress on benchmarks like SALICON and MIT300 is slowing, video saliency models have shown rapid gains on the recent DHF1K benchmark. Here, we take a step back and ask: Can image and video saliency modeling be approached via a unified model, with mutual benefit? We identify different sources of domain shift between image and video saliency data and between different video saliency datasets as a key challenge for effective joint modelling. To address this we propose four novel domain adaptation techniques—Domain-Adaptive Priors, Domain-Adaptive Fusion, Domain-Adaptive Smoothing and Bypass-RNN—in addition to an improved formulation of learned Gaussian priors. We integrate these techniques into a simple and lightweight encoder-RNN-decoder-style network, UNISAL, and train it jointly with image and video saliency data. We evaluate our method on the video saliency datasets DHF1K, Hollywood-2 and UCF-Sports, and the image saliency datasets SALICON and MIT300. With one set of parameters, UNISAL achieves state-of-the-art performance on all video saliency datasets and is on par with the state-of-the-art for image saliency datasets, despite faster runtime and a 5 to 20-fold smaller model size compared to all competing deep methods. We provide retrospective analyses and ablation studies which confirm the importance of the domain shift modeling. The code is available at https://github.com/rdroste/unisal.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_25');
INSERT INTO `paper` VALUES (12930, 'Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing', 'Audio-visual video parsing', 'Weakly-supervised', 'LLP dataset', '', '', 'In this paper, we introduce a new problem, named audio-visual video parsing, which aims to parse a video into temporal event segments and label them as either audible, visible, or both. Such a problem is essential for a complete understanding of the scene depicted inside a video. To facilitate exploration, we collect a Look, Listen, and Parse (LLP) dataset to investigate audio-visual video parsing in a weakly-supervised manner. This task can be naturally formulated as a Multimodal Multiple Instance Learning (MMIL) problem. Concretely, we propose a novel hybrid attention network to explore unimodal and cross-modal temporal contexts simultaneously. We develop an attentive MMIL pooling method to adaptively explore useful audio and visual content from different temporal extent and modalities. Furthermore, we discover and mitigate modality bias and noisy label issues with an individual-guided learning mechanism and label smoothing technique, respectively. Experimental results show that the challenging audio-visual video parsing can be achieved even with only video-level weak labels. Our proposed framework can effectively leverage unimodal and cross-modal temporal contexts and alleviate modality bias and noisy labels problems.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_26');
INSERT INTO `paper` VALUES (12931, 'Unifying Deep Local and Global Features for Image Search', 'Deep features', 'Image retrieval', 'Unified model', '', '', 'Image retrieval is the problem of searching an image database for items that are similar to a query image. To address this task, two main types of image representations have been studied: global and local image features. In this work, our key contribution is to unify global and local features into a single deep model, enabling accurate retrieval with efficient feature extraction. We refer to the new model as DELG, standing for DEep Local and Global features. We leverage lessons from recent feature learning work and propose a model that combines generalized mean pooling for global features and attentive selection for local features. The entire network can be learned end-to-end by carefully balancing the gradient flow between two heads – requiring only image-level labels. We also introduce an autoencoder-based dimensionality reduction technique for local features, which is integrated into the model, improving training efficiency and matching performance. Comprehensive experiments show that our model achieves state-of-the-art image retrieval on the Revisited Oxford and Paris datasets, and state-of-the-art single-model instance-level recognition on the Google Landmarks dataset v2. Code and models are available at https://github.com/tensorflow/models/tree/master/research/delf.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58565-5_43');
INSERT INTO `paper` VALUES (12932, 'UnionDet: Union-Level Detector Towards Real-Time Human-Object Interaction Detection', 'Visual relationships', 'Real-time detection', 'Human-object interaction detection', 'Object detection', '', 'Recent advances in deep neural networks have achieved significant progress in detecting individual objects from an image. However, object detection is not sufficient to fully understand a visual scene. Towards a deeper visual understanding, the interactions between objects, especially humans and objects are essential. Most prior works have obtained this information with a bottom-up approach, where the objects are first detected and the interactions are predicted sequentially by pairing the objects. This is a major bottleneck in HOI detection inference time. To tackle this problem, we propose UnionDet, a one-stage meta-architecture for HOI detection powered by a novel union-level detector that eliminates this additional inference stage by directly capturing the region of interaction. Our one-stage detector for human-object interaction shows a significant reduction in interaction prediction time (\\(4{\\times }{\\sim }14{\\times }\\)) while outperforming state-of-the-art methods on two public datasets: V-COCO and HICO-DET.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_30');
INSERT INTO `paper` VALUES (12933, 'UNITER: UNiversal Image-TExt Representation Learning', '', '', '', '', '', 'Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR\\(^2\\) (Code is available at https://github.com/ChenRocks/UNITER.).', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_7');
INSERT INTO `paper` VALUES (12934, 'Unpaired Image-to-Image Translation Using Adversarial Consistency Loss', 'Generative adversarial networks', 'Dual learning', 'Image synthesis', '', '', 'Unpaired image-to-image translation is a class of vision problems whose goal is to find the mapping between different image domains using unpaired training data. Cycle-consistency loss is a widely used constraint for such problems. However, due to the strict pixel-level constraint, it cannot perform shape changes, remove large objects, or ignore irrelevant texture. In this paper, we propose a novel adversarial-consistency loss for image-to-image translation. This loss does not require the translated image to be translated back to be a specific source image but can encourage the translated images to retain important features of the source images and overcome the drawbacks of cycle-consistency loss noted above. Our method achieves state-of-the-art results on three challenging tasks: glasses removal, male-to-female translation, and selfie-to-anime translation.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_46');
INSERT INTO `paper` VALUES (12935, 'Unpaired Learning of Deep Image Denoising', 'Image denoising', 'Unpaired learning', 'Convolutional networks', 'Self-supervised learning', '', 'We investigate the task of learning blind image denoising networks from an unpaired set of clean and noisy images. Such problem setting generally is practical and valuable considering that it is feasible to collect unpaired noisy and clean images in most real-world applications. And we further assume that the noise can be signal dependent but is spatially uncorrelated. In order to facilitate unpaired learning of denoising network, this paper presents a two-stage scheme by incorporating self-supervised learning and knowledge distillation. For self-supervised learning, we suggest a dilated blind-spot network (D-BSN) to learn denoising solely from real noisy images. Due to the spatial independence of noise, we adopt a network by stacking \\(1\\times 1\\) convolution layers to estimate the noise level map for each image. Both the D-BSN and image-specific noise model (\\(\\text {CNN}_{\\text {est}}\\)) can be jointly trained via maximizing the constrained log-likelihood. Given the output of D-BSN and estimated noise level map, improved denoising performance can be further obtained based on the Bayes’ rule. As for knowledge distillation, we first apply the learned noise models to clean images to synthesize a paired set of training images, and use the real noisy images and the corresponding denoising results in the first stage to form another paired set. Then, the ultimate denoising model can be distilled by training an existing denoising network using these two paired sets. Experiments show that our unpaired learning method performs favorably on both synthetic noisy images and real-world noisy photographs in terms of quantitative and qualitative evaluation. Code is available at https://github.com/XHWXD/DBSN.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58548-8_21');
INSERT INTO `paper` VALUES (12936, 'Unselfie: Translating Selfies to Neutral-Pose Portraits in the Wild', 'Image editing', 'Selfie', 'Human pose transfer', '', '', 'Due to the ubiquity of smartphones, it is popular to take photos of one’s self, or “selfies.” Such photos are convenient to take, because they do not require specialized equipment or a third-party photographer. However, in selfies, constraints such as human arm length often make the body pose look unnatural. To address this issue, we introduce unselfie, a novel photographic transformation that automatically translates a selfie into a neutral-pose portrait. To achieve this, we first collect an unpaired dataset, and introduce a way to synthesize paired training data for self-supervised learning. Then, to unselfie a photo, we propose a new three-stage pipeline, where we first find a target neutral pose, inpaint the body texture, and finally refine and composite the person on the background. To obtain a suitable target neutral pose, we propose a novel nearest pose search module that makes the reposing task easier and enables the generation of multiple neutral-pose results among which users can choose the best one they like. Qualitative and quantitative evaluations show the superiority of our pipeline over alternatives.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_10');
INSERT INTO `paper` VALUES (12937, 'Unsupervised 3D Human Pose Representation with Viewpoint and Pose Disentanglement', 'Representation learning', '3D human pose', 'Pose denoising', 'Unsupervised action recognition', '', 'Learning a good 3D human pose representation is important for human pose related tasks, e.g. human 3D pose estimation and action recognition. Within all these problems, preserving the intrinsic pose information and adapting to view variations are two critical issues. In this work, we propose a novel Siamese denoising autoencoder to learn a 3D pose representation by disentangling the pose-dependent and view-dependent feature from the human skeleton data, in a fully unsupervised manner. These two disentangled features are utilized together as the representation of the 3D pose. To consider both the kinematic and geometric dependencies, a sequential bidirectional recursive network (SeBiReNet) is further proposed to model the human skeleton data. Extensive experiments demonstrate that the learned representation 1) preserves the intrinsic information of human pose, 2) shows good transferability across datasets and tasks. Notably, our approach achieves state-of-the-art performance on two inherently different tasks: pose denoising and unsupervised action recognition. Code and models are available at: https://github.com/NIEQiang001/unsupervised-human-pose.git.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_7');
INSERT INTO `paper` VALUES (12938, 'Unsupervised Cross-Modal Alignment for Multi-person 3D Pose Estimation', '', '', '', '', '', 'We present a deployment friendly, fast bottom-up framework for multi-person 3D human pose estimation. We adopt a novel neural representation of multi-person 3D pose which unifies the position of person instances with their corresponding 3D pose representation. This is realized by learning a generative pose embedding which not only ensures plausible 3D pose predictions, but also eliminates the usual keypoint grouping operation as employed in prior bottom-up approaches. Further, we propose a practical deployment paradigm where paired 2D or 3D pose annotations are unavailable. In the absence of any paired supervision, we leverage a frozen network, as a teacher model, which is trained on an auxiliary task of multi-person 2D pose estimation. We cast the learning as a cross-modal alignment problem and propose training objectives to realize a shared latent space between two diverse modalities. We aim to enhance the model’s ability to perform beyond the limiting teacher network by enriching the latent-to-3D pose mapping using artificially synthesized multi-person 3D scene samples. Our approach not only generalizes to in-the-wild images, but also yields a superior trade-off between speed and performance, compared to prior top-down approaches. Our approach also yields state-of-the-art multi-person 3D pose estimation performance among the bottom-up approaches under consistent supervision levels.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_3');
INSERT INTO `paper` VALUES (12939, 'Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss', 'Unsupervised metric learning', 'Attention map', 'Consistency loss', 'Contrastive loss', '', 'Existing approaches for unsupervised metric learning focus on exploring self-supervision information within the input image itself. We observe that, when analyzing images, human eyes often compare images against each other instead of examining images individually. In addition, they often pay attention to certain keypoints, image regions, or objects which are discriminative between image classes but highly consistent within classes. Even if the image is being transformed, the attention pattern will be consistent. Motivated by this observation, we develop a new approach to unsupervised deep metric learning where the network is learned based on self-supervision information across images instead of within one single image. To characterize the consistent pattern of human attention during image comparisons, we introduce the idea of transformed attention consistency. It assumes that visually similar images, even undergoing different image transforms, should share the same consistent visual attention map. This consistency leads to a pairwise self-supervision loss, allowing us to learn a Siamese deep neural network to encode and compare images against their transformed or matched pairs. To further enhance the inter-class discriminative power of the feature generated by this network, we adapt the concept of triplet loss from supervised metric learning to our unsupervised case and introduce the contrastive clustering loss. Our extensive experimental results on benchmark datasets demonstrate that our proposed method outperforms current state-of-the-art methods for unsupervised metric learning by a large margin.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_9');
INSERT INTO `paper` VALUES (12940, 'Unsupervised Discovery of Sign Terms by K-Nearest Neighbours Approach', 'Unsupervised learning', 'Sign language', 'Term discovery', '', '', 'In order to utilize the large amount of unlabeled sign language resources, unsupervised learning methods are needed. Motivated by the successful results of unsupervised term discovery (UTD) in spoken languages, here we explore how to apply similar methods for sign terms discovery. Our goal is to find the repeating terms from continuous sign videos without any supervision. Using visual features extracted from RGB videos, we show that a k-nearest neighbours based discovery algorithm designed for speech can also discover sign terms. We also run experiments using a baseline UTD algorithm and comment on their differences.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_22');
INSERT INTO `paper` VALUES (12941, 'Unsupervised Domain Adaptation for Plant Organ Counting', '', '', '', '', '', 'Supervised learning is often used to count objects in images, but for counting small, densely located objects, the required image annotations are burdensome to collect. Counting plant organs for image-based plant phenotyping falls within this category. Object counting in plant images is further challenged by having plant image datasets with significant domain shift due to different experimental conditions, e.g. applying an annotated dataset of indoor plant images for use on outdoor images, or on a different plant species. In this paper, we propose a domain-adversarial learning approach for domain adaptation of density map estimation for the purposes of object counting. The approach does not assume perfectly aligned distributions between the source and target datasets, which makes it more broadly applicable within general object counting and plant organ counting tasks. Evaluation on two diverse object counting tasks (wheat spikelets, leaves) demonstrates consistent performance on the target datasets across different classes of domain shift: from indoor-to-outdoor images and from species-to-species adaptation.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_23');
INSERT INTO `paper` VALUES (12942, 'Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images Through Generative Latent Search', 'Unsupervised domain adaptation', 'Semantic segmentation', 'Near IR dataset', 'VAE', '', 'Segmentation of the pixels corresponding to human skin is an essential first step in multiple applications ranging from surveillance to heart-rate estimation from remote-photoplethysmography. However, the existing literature considers the problem only in the visible-range of the EM-spectrum which limits their utility in low or no light settings where the criticality of the application is higher. To alleviate this problem, we consider the problem of skin segmentation from the Near-infrared images. However, Deep learning based state-of-the-art segmentation techniques demands large amounts of labelled data that is unavailable for the current problem. Therefore we cast the skin segmentation problem as that of target-independent Unsupervised Domain Adaptation (UDA) where we use the data from the Red-channel of the visible-range to develop skin segmentation algorithm on NIR images. We propose a method for target-independent segmentation where the ‘nearest-clone’ of a target image in the source domain is searched and used as a proxy in the segmentation network trained only on the source domain. We prove the existence of ‘nearest-clone’ and propose a method to find it through an optimization algorithm over the latent space of a Deep generative model based on variational inference. We demonstrate the efficacy of the proposed method for NIR skin segmentation over the state-of-the-art UDA segmentation methods on the two newly created skin segmentation datasets in NIR domain despite not having access to the target NIR data. Additionally, we report state-of-the-art results for adaption from Synthia to Cityscapes which is a popular setting in Unsupervised Domain Adaptation for semantic segmentation. The code and datasets are available at https://github.com/ambekarsameer96/GLSS.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_25');
INSERT INTO `paper` VALUES (12943, 'Unsupervised Domain Adaptation in the Dissimilarity Space for Person Re-identification', 'Deep learning', 'Domain adaptation', 'Maximum mean discrepancy', 'Dissimilarity space', 'Person re-identification', 'Person re-identification (ReID) remains a challenging task in many real-word video analytics and surveillance applications, even though state-of-the-art accuracy has improved considerably with the advent of deep learning (DL) models trained on large image datasets. Given the shift in distributions that typically occurs between video data captured from the source and target domains, and absence of labeled data from the target domain, it is difficult to adapt a DL model for accurate recognition of target data. DL models for unsupervised domain adaptation (UDA) are commonly designed in the feature representation space. We argue that for pair-wise matchers that rely on metric learning, e.g., Siamese networks for person ReID, the UDA objective should consist in aligning pair-wise dissimilarity between domains, rather than aligning feature representations. Moreover, dissimilarity representations are more suitable for designing open-set ReID systems, where identities differ in the source and target domains. In this paper, we propose a novel Dissimilarity-based Maximum Mean Discrepancy (D-MMD) loss for aligning pair-wise distances that can be optimized via gradient descent using relatively small batch sizes. From a person ReID perspective, the evaluation of D-MMD loss is straightforward since the tracklet information (provided by a person tracker) allows to label a distance vector as being either within-class (within-tracklet) or between-class (between-tracklet). This allows approximating the underlying distribution of target pair-wise distances for D-MMD loss optimization, and accordingly align source and target distance distributions. Empirical results with three challenging benchmark datasets show that the proposed D-MMD loss decreases as source and domain distributions become more similar. Extensive experimental evaluation also indicates that UDA methods that rely on the D-MMD loss can significantly outperform baseline and state-of-the-art UDA methods for person ReID. The dissimilarity space transformation allows to design reliable pair-wise matchers, without the common requirement for data augmentation and/or complex networks. Code is available on GitHub link: https://github.com/djidje/D-MMD.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58583-9_10');
INSERT INTO `paper` VALUES (12944, 'Unsupervised Domain Adaptation with Noise Resistible Mutual-Training for Person Re-identification', 'Unsupervised domain adaptation', 'Person re-identification', 'Collaborative clustering', 'Mutual instance selection', '', 'Unsupervised domain adaptation (UDA) in the task of person re-identification (re-ID) is highly challenging due to large domain divergence and no class overlap between domains. Pseudo-label based self-training is one of the representative techniques to address UDA. However, label noise caused by unsupervised clustering is always a trouble to self-training methods. To depress noises in pseudo-labels, this paper proposes a Noise Resistible Mutual-Training (NRMT) method, which maintains two networks during training to perform collaborative clustering and mutual instance selection. On one hand, collaborative clustering eases the fitting to noisy instances by allowing the two networks to use pseudo-labels provided by each other as an additional supervision. On the other hand, mutual instance selection further selects reliable and informative instances for training according to the peer-confidence and relationship disagreement of the networks. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art UDA methods for person re-ID.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_31');
INSERT INTO `paper` VALUES (12945, 'Unsupervised Domain Attention Adaptation Network for Caricature Attribute Recognition', 'Unsupervised domain adaptation', 'Caricature', 'Attribute recognition', 'Attention', '', 'Caricature attributes provide distinctive facial features to help research in Psychology and Neuroscience. However, unlike the facial photo attribute datasets that have a quantity of annotated images, the annotations of caricature attributes are rare. To facility the research in attribute learning of caricatures, we propose a caricature attribute dataset, namely WebCariA. Moreover, to utilize models that trained by face attributes, we propose a novel unsupervised domain adaptation framework for cross-modality (i.e., photos to caricatures) attribute recognition, with an integrated inter- and intra-domain consistency learning scheme. Specifically, the inter-domain consistency learning scheme consisting an image-to-image translator to first fill the domain gap between photos and caricatures by generating intermediate image samples, and a label consistency learning module to align their semantic information. The intra-domain consistency learning scheme integrates the common feature consistency learning module with a novel attribute-aware attention-consistency learning module for a more efficient alignment. We did an extensive ablation study to show the effectiveness of the proposed method. And the proposed method also outperforms the state-of-the-art methods by a margin. The implementation of the proposed method is available at https://github.com/KeleiHe/DAAN.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_2');
INSERT INTO `paper` VALUES (12946, 'Unsupervised Image Classification for Deep Representation Learning', 'Unsupervised learning', 'Representation learning', '', '', '', 'Deep clustering against self-supervised learning (SSL) is a very important and promising direction for unsupervised visual representation learning since it requires little domain knowledge to design pretext tasks. However, the key component, embedding clustering, limits its extension to the extremely large-scale dataset due to its prerequisite to save the global latent embedding of the entire dataset. In this work, we aim to make this framework more simple and elegant without performance decline. We propose an unsupervised image classification framework without using embedding clustering, which is very similar to standard supervised training manner. For detailed interpretation, we further analyze its relation with deep clustering and contrastive learning. Extensive experiments on ImageNet dataset have been conducted to prove the effectiveness of our method. Furthermore, the experiments on transfer learning benchmarks have verified its generalization to other downstream tasks, including multi-label image classification, object detection, semantic segmentation and few-shot image classification.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_30');
INSERT INTO `paper` VALUES (12947, 'Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from Point Sets', '', '', '', '', '', 'Automatic discovery of category-specific 3D keypoints from a collection of objects of a category is a challenging problem. The difficulty is added when objects are represented by 3D point clouds, with variations in shape and semantic parts and unknown coordinate frames. We define keypoints to be category-specific, if they meaningfully represent objects’ shape and their correspondences can be simply established order-wise across all objects. This paper aims at learning such 3D keypoints, in an unsupervised manner, using a collection of misaligned 3D point clouds of objects from an unknown category. In order to do so, we model shapes defined by the keypoints, within a category, using the symmetric linear basis shapes without assuming the plane of symmetry to be known. The usage of symmetry prior leads us to learn stable keypoints suitable for higher misalignments. To the best of our knowledge, this is the first work on learning such keypoints directly from 3D point clouds for a general category. Using objects from four benchmark datasets, we demonstrate the quality of our learned keypoints by quantitative and qualitative evaluations. Our experiments also show that the keypoints discovered by our method are geometrically and semantically consistent.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_33');
INSERT INTO `paper` VALUES (12948, 'Unsupervised Learning of Optical Flow with Deep Feature Similarity', 'Unsupervised', 'Self-supervised', 'Optical flow', 'Deep feature', 'Similarity', 'Deep unsupervised learning for optical flow has been proposed, where the loss measures image similarity with the warping function parameterized by estimated flow. The census transform, instead of image pixel values, is often used for the image similarity. In this work, rather than the handcrafted features i.e. census or pixel values, we propose to use deep self-supervised features with a novel similarity measure, which fuses multi-layer similarities. With the fused similarity, our network better learns flow by minimizing our proposed feature separation loss. The proposed method is a polarizing scheme, resulting in a more discriminative similarity map. In the process, the features are also updated to get high similarity for matching pairs and low for uncertain pairs, given estimated flow. We evaluate our method on FlyingChairs, MPI Sintel, and KITTI benchmarks. In quantitative and qualitative comparisons, our method effectively improves the state-of-the-art techniques.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_11');
INSERT INTO `paper` VALUES (12949, 'Unsupervised Learning of Video Representations via Dense Trajectory Clustering', 'Unsupervised representation learning', 'Action recognition', '', '', '', 'This paper addresses the task of unsupervised learning of representations for action recognition in videos. Previous works proposed to utilize future prediction, or other domain-specific objectives to train a network, but achieved only limited success. In contrast, in the relevant field of image representation learning, simpler, discrimination-based methods have recently bridged the gap to fully-supervised performance. We first propose to adapt two top performing objectives in this class - instance recognition and local aggregation, to the video domain. In particular, the latter approach iterates between clustering the videos in the feature space of a network and updating it to respect the cluster with a non-parametric classification loss. We observe promising performance, but qualitative analysis shows that the learned representations fail to capture motion patterns, grouping the videos based on appearance. To mitigate this issue, we turn to the heuristic-based IDT descriptors, that were manually designed to encode motion patterns in videos. We form the clusters in the IDT space, using these descriptors as a an unsupervised prior in the iterative local aggregation algorithm. Our experiments demonstrates that this approach outperform prior work on UCF101 and HMDB51 action recognition benchmarks. We also qualitatively analyze the learned representations and show that they successfully capture video dynamics.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_28');
INSERT INTO `paper` VALUES (12950, 'Unsupervised Monocular Depth Estimation for Night-Time Images Using Adversarial Domain Feature Adaptation', '', '', '', '', '', 'In this paper, we look into the problem of estimating per-pixel depth maps from unconstrained RGB monocular night-time images which is a difficult task that has not been addressed adequately in the literature. The state-of-the-art day-time depth estimation methods fail miserably when tested with night-time images due to a large domain shift between them. The usual photometric losses used for training these networks may not work for night-time images due to the absence of uniform lighting which is commonly present in day-time images, making it a difficult problem to solve. We propose to solve this problem by posing it as a domain adaptation problem where a network trained with day-time images is adapted to work for night-time images. Specifically, an encoder is trained to generate features from night-time images that are indistinguishable from those obtained from day-time images by using a PatchGAN-based adversarial discriminative learning method. Unlike the existing methods that directly adapt depth prediction (network output), we propose to adapt feature maps obtained from the encoder network so that a pre-trained day-time depth decoder can be directly used for predicting depth from these adapted features. Hence, the resulting method is termed as “Adversarial Domain Feature Adaptation (ADFA)” and its efficacy is demonstrated through experimentation on the challenging Oxford night driving dataset. To the best of our knowledge, this work is a first of its kind to estimate depth from unconstrained night-time monocular RGB images that uses a completely unsupervised learning process. The modular encoder-decoder architecture for the proposed ADFA method allows us to use the encoder module as a feature extractor which can be used in many other applications. One such application is demonstrated where the features obtained from our adapted encoder network are shown to outperform other state-of-the-art methods in a visual place recognition problem, thereby, further establishing the usefulness and effectiveness of the proposed approach.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_27');
INSERT INTO `paper` VALUES (12951, 'Unsupervised Multi-view CNN for Salient View Selection of 3D Objects and Scenes', 'Unsupervised 3D deep learning', 'Multi-view CNN', 'View-object consistency', 'View selection', '', 'We present an unsupervised 3D deep learning framework based on a ubiquitously true proposition named view-object consistency as it states that a 3D object and its projected 2D views always belong to the same object class. To validate its effectiveness, we design a multi-view CNN for the salient view selection of 3D objects, which quintessentially cannot be handled by supervised learning due to the difficulty of data collection. Our unsupervised multi-view CNN branches off two channels which encode the knowledge within each 2D view and the 3D object respectively and also exploits both intra-view and inter-view knowledge of the object. It ends with a new loss layer which formulates the view-object consistency by impelling the two channels to generate consistent classification outcomes. We experimentally demonstrate the superiority of our method over state-of-the-art methods and showcase that it can be used to select salient views of 3D scenes containing multiple objects.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58529-7_27');
INSERT INTO `paper` VALUES (12952, 'Unsupervised Shape and Pose Disentanglement for 3D Meshes', '3D deep learning', 'Disentanglement', 'Body shape', 'Mesh auto-encoder', 'Representation learning', 'Parametric models of humans, faces, hands and animals have been widely used for a range of tasks such as image-based reconstruction, shape correspondence estimation, and animation. Their key strength is the ability to factor surface variations into shape and pose dependent components. Learning such models requires lots of expert knowledge and hand-defined object-specific constraints, making the learning approach unscalable to novel objects. In this paper, we present a simple yet effective approach to learn disentangled shape and pose representations in an unsupervised setting. We use a combination of self-consistency and cross-consistency constraints to learn pose and shape space from registered meshes. We additionally incorporate as-rigid-as-possible deformation(ARAP) into the training loop to avoid degenerate solutions. We demonstrate the usefulness of learned representations through a number of tasks including pose transfer and shape retrieval. The experiments on datasets of 3D humans, faces, hands and animals demonstrate the generality of our approach. Code is made available at https://virtualhumans.mpi-inf.mpg.de/unsup_shape_pose/.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_21');
INSERT INTO `paper` VALUES (12953, 'Unsupervised Sketch to Photo Synthesis', '', '', '', '', '', 'Humans can envision a realistic photo given a free-hand sketch that is not only spatially imprecise and geometrically distorted but also without colors and visual details. We study unsupervised sketch to photo synthesis for the first time, learning from unpaired sketch and photo data where the target photo for a sketch is unknown during training. Existing works only deal with either style difference or spatial deformation alone, synthesizing photos from edge-aligned line drawings or transforming shapes within the same modality, e.g., color images.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_3');
INSERT INTO `paper` VALUES (12954, 'Unsupervised Video Object Segmentation with Joint Hotspot Tracking', 'Unsupervised video object segmentation', 'Hotspot tracking', 'Weighted correlation siamese network', '', '', 'Object tracking is a well-studied problem in computer vision while identifying salient spots of objects in a video is a less explored direction in the literature. Video eye gaze estimation methods aim to tackle a related task but salient spots in those methods are not bounded by objects and tend to produce very scattered, unstable predictions due to the noisy ground truth data. We reformulate the problem of detecting and tracking of salient object spots as a new task called object hotspot tracking. In this paper, we propose to tackle this task jointly with unsupervised video object segmentation, in real-time, with a unified framework to exploit the synergy between the two. Specifically, we propose a Weighted Correlation Siamese Network (WCS-Net) which employs a Weighted Correlation Block (WCB) for encoding the pixel-wise correspondence between a template frame and the search frame. In addition, WCB takes the initial mask/hotspot as guidance to enhance the influence of salient regions for robust tracking. Our system can operate online during inference and jointly produce the object mask and hotspot track-lets at 33 FPS. Experimental results validate the effectiveness of our network design, and show the benefits of jointly solving the hotspot tracking and object segmentation problems. In particular, our method performs favorably against state-of-the-art video eye gaze models in object hotspot tracking, and outperforms existing methods on three benchmark datasets for unsupervised video object segmentation.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_29');
INSERT INTO `paper` VALUES (12955, 'URIE: Universal Image Enhancement for Visual Recognition in the Wild', 'Visual recognition', 'Image enhancement', '', '', '', 'Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks where input images are degraded.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_43');
INSERT INTO `paper` VALUES (12956, 'URVOS: Unified Referring Video Object Segmentation Network with a Large-Scale Benchmark', 'Video object segmentation', 'Referring object segmentation', '', '', '', 'We propose a unified referring video object segmentation network (URVOS). URVOS takes a video and a referring expression as inputs, and estimates the object masks referred by the given language expression in the whole video frames. Our algorithm addresses the challenging problem by performing language-based object segmentation and mask propagation jointly using a single deep neural network with a proper combination of two attention models. In addition, we construct the first large-scale referring video object segmentation dataset called Refer-Youtube-VOS. We evaluate our model on two benchmark datasets including ours and demonstrate the effectiveness of the proposed approach. The dataset is released at https://github.com/skynbe/Refer-Youtube-VOS.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_13');
INSERT INTO `paper` VALUES (12957, 'Using Sentences as Semantic Representations in Large Scale Zero-Shot Learning', '', '', '', '', '', 'Zero-shot learning (ZSL) aims to recognize instances of unseen classes, for which no visual instance is available during training, by learning multimodal relations between samples from seen classes and corresponding class semantic representations. These class representations usually consist of either attributes, which do not scale well to large datasets, or word embeddings, which lead to poorer performance. A good trade-off could be to employ short sentences in natural language as class descriptions. We explore different solutions to use such short descriptions in a ZSL setting and show that while simple methods cannot achieve very good results with sentences alone, a combination of usual word embeddings and sentences can significantly outperform current state-of-the-art.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_42');
INSERT INTO `paper` VALUES (12958, 'Utilizing Patch-Level Category Activation Patterns for Multiple Class Novelty Detection', 'Multiple class novelty detection', 'Class activation patterns', '', '', '', 'For any recognition system, the ability to identify novel class samples during inference is an important aspect of the system’s robustness. This problem of detecting novel class samples during inference is commonly referred to as Multiple Class Novelty Detection. In this paper, we propose a novel method that makes deep convolutional neural networks robust to novel classes. Specifically, during training one branch performs traditional classification (referred to as global inference), and the other branch provides patch-level information to keep track of the class-specific activation patterns (referred to as local inference). Both global and local branch information are combined to train a novelty detection network, which is used during inference to identify novel classes. We evaluate the proposed method on four datasets (Caltech256, CUB-200, Stanford Dogs and FounderType-200) and show that the proposed method is able to identify novel class samples better compared to the other deep convolutional neural network-based methods.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_25');
INSERT INTO `paper` VALUES (12959, 'V2VNet: Vehicle-to-Vehicle Communication for Joint Perception and Prediction', 'Autonomous driving', 'Object detection', 'Motion forecast', '', '', 'In this paper, we explore the use of vehicle-to-vehicle (V2V) communication to improve the perception and motion forecasting performance of self-driving vehicles. By intelligently aggregating the information received from multiple nearby vehicles, we can observe the same scene from different viewpoints. This allows us to see through occlusions and detect actors at long range, where the observations are very sparse or non-existent. We also show that our approach of sending compressed deep feature map activations achieves high accuracy while satisfying communication bandwidth requirements.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_36');
INSERT INTO `paper` VALUES (12960, 'Variational Connectionist Temporal Classification', 'Connectionist Temporal Classification', 'Scene text recognition', 'Handwritten text recognition', '', '', 'Connectionist Temporal Classification (CTC) is a training criterion designed for sequence labelling problems where the alignment between the inputs and the target labels is unknown. One of the key steps is to add a blank symbol to the target vocabulary. However, CTC tends to output spiky distributions since it prefers to output blank symbol most of the time. These spiky distributions show inferior alignments and the non-blank symbols are not learned sufficiently. To remedy this, we propose variational CTC (Var-CTC) to enhance the learning of non-blank symbols. The proposed Var-CTC converts the output distribution of vanilla CTC with hierarchy distribution. It first learns the approximated posterior distribution of blank to determine whether to output a specific non-blank symbol or not. Then it learns the alignment between non-blank symbols and input sequence. Experiments on scene text recognition and offline handwritten text recognition show Var-CTC achieves better alignments. Besides, with the enhanced learning of non-blank symbols, the confidence scores of model outputs are more discriminative. Compared with the vanilla CTC, the proposed Var-CTC can improve the recall performance by a large margin when the models maintain the same level of precision.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_28');
INSERT INTO `paper` VALUES (12961, 'Variational Diffusion Autoencoders with Random Walk Sampling', 'Deep learning', 'Variational inference', 'Manifold learning', 'Image and video synthesis', 'Generative models', 'Variational autoencoders (VAEs) and generative adversarial networks (GANs) enjoy an intuitive connection to manifold learning: in training the decoder/generator is optimized to approximate a homeomorphism between the data distribution and the sampling space. This is a construction that strives to define the data manifold. A major obstacle to VAEs and GANs, however, is choosing a suitable prior that matches the data topology. Well-known consequences of poorly picked priors are posterior and mode collapse. To our knowledge, no existing method sidesteps this user choice. Conversely, diffusion maps automatically infer the data topology and enjoy a rigorous connection to manifold learning, but do not scale easily or provide the inverse homeomorphism (i.e. decoder/generator). We propose a method (https://github.com/lihenryhfl/vdae) that combines these approaches into a generative model that inherits the asymptotic guarantees of diffusion maps while preserving the scalability of deep models. We prove approximation theoretic results for the dimension dependence of our proposed method. Finally, we demonstrate the effectiveness of our method with various real and synthetic datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_22');
INSERT INTO `paper` VALUES (12962, 'VarSR: Variational Super-Resolution Network for Very Low Resolution Images', 'Single image super resolution', 'Variational super resolution', 'Very low resolution image', '', '', 'As is well known, single image super-resolution (SR) is an ill-posed problem where multiple high resolution (HR) images can be matched to one low resolution (LR) image due to the difference in their representation capabilities. Such many-to-one nature is particularly magnified when super-resolving with large upscaling factors from very low dimensional domains such as 8 \\(\\times \\) 8 resolution where detailed information of HR is hardly discovered. Most existing methods are optimized for deterministic generation of SR images under pre-defined objectives such as pixel-level reconstruction and thus limited to the one-to-one correspondence between LR and SR images against the nature. In this paper, we propose VarSR, Variational Super Resolution Network, that matches latent distributions of LR and HR images to recover the missing details. Specifically, we draw samples from the learned common latent distribution of LR and HR to generate diverse SR images as the many-to-one relationship. Experimental results validate that our method can produce more accurate and perceptually plausible SR images from very low resolutions compared to the deterministic techniques.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_26');
INSERT INTO `paper` VALUES (12963, 'VCNet: A Robust Approach to Blind Image Inpainting', 'Blind image inpainting', 'Visual consistency', 'Spatial normalization', 'Generative adversarial networks', '', 'Blind inpainting is a task to automatically complete visual contents without specifying masks for missing areas in an image. Previous work assumes known missing-region-pattern, limiting the application scope. We instead relax the assumption by defining a new blind inpainting setting, making training a neural system robust against various unknown missing region patterns. Specifically, we propose a two-stage visual consistency network (VCN) to estimate where to fill (via masks) and generate what to fill. In this procedure, the unavoidable potential mask prediction errors lead to severe artifacts in the subsequent repairing. To address it, our VCN predicts semantically inconsistent regions first, making mask prediction more tractable. Then it repairs these estimated missing regions using a new spatial normalization, making VCN robust to mask prediction errors. Semantically convincing and visually compelling content can be generated. Extensive experiments show that our method is effective and robust in blind image inpainting. And our VCN allows for a wide spectrum of applications.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_45');
INSERT INTO `paper` VALUES (12964, 'Vectorizing World Buildings: Planar Graph Reconstruction by Primitive Detection and Relationship Inference', 'Vectorization', 'Remote sensing', 'Deep learning', 'Planar graph', '', 'This paper tackles a 2D architecture vectorization problem, whose task is to infer an outdoor building architecture as a 2D planar graph from a single RGB image. We provide a new benchmark with ground-truth annotations for 2,001 complex buildings across the cities of Atlanta, Paris, and Las Vegas. We also propose a novel algorithm utilizing 1) convolutional neural networks (CNNs) that detects geometric primitives and infers their relationships and 2) an integer programming (IP) that assembles the information into a 2D planar graph. While being a trivial task for human vision, the inference of a graph structure with an arbitrary topology is still an open problem for computer vision. Qualitative and quantitative evaluations demonstrate that our algorithm makes significant improvements over the current state-of-the-art, towards an intelligent system at the level of human perception. We will share code and data.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_42');
INSERT INTO `paper` VALUES (12965, 'Video Object Detection via Object-Level Temporal Aggregation', 'Video object detection', 'Object tracking', 'Temporal aggregation', 'Keyframe scheduling', '', 'While single-image object detectors can be naively applied to videos in a frame-by-frame fashion, the prediction is often temporally inconsistent. Moreover, the computation can be redundant since neighboring frames are inherently similar to each other. In this work we propose to improve video object detection via temporal aggregation. Specifically, a detection model is applied on sparse keyframes to handle new objects, occlusions, and rapid motions. We then use real-time trackers to exploit temporal cues and track the detected objects in the remaining frames, which enhances efficiency and temporal coherence. Object status at the bounding-box level is propagated across frames and updated by our aggregation modules. For keyframe scheduling, we propose adaptive policies using reinforcement learning and simple heuristics. The proposed framework achieves the state-of-the-art performance on the Imagenet VID 2015 dataset while running real-time on CPU. Extensive experiments are done to show the effectiveness of our training strategies and justify the model designs.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_10');
INSERT INTO `paper` VALUES (12966, 'Video Object Segmentation with Episodic Graph Memory Networks', 'Video segmentation', 'Episodic graph memory', 'Learn to update', '', '', 'How to make a segmentation model efficiently adapt to a specific video as well as online target appearance variations is a fundamental issue in the field of video object segmentation. In this work, a graph memory network is developed to address the novel idea of “learning to update the segmentation model”. Specifically, we exploit an episodic memory network, organized as a fully connected graph, to store frames as nodes and capture cross-frame correlations by edges. Further, learnable controllers are embedded to ease memory reading and writing, as well as maintain a fixed memory scale. The structured, external memory design enables our model to comprehensively mine and quickly store new knowledge, even with limited visual information, and the differentiable memory controllers slowly learn an abstract method for storing useful representations in the memory and how to later use these representations for prediction, via gradient descent. In addition, the proposed graph memory network yields a neat yet principled framework, which can generalize well to both one-shot and zero-shot video object segmentation tasks. Extensive experiments on four challenging benchmark datasets verify that our graph memory network is able to facilitate the adaptation of the segmentation network for case-by-case video object segmentation.', 'ECCV', '2020', '03 December 2020', 'https://doi.org/10.1007/978-3-030-58580-8_39');
INSERT INTO `paper` VALUES (12967, 'Video Representation Learning by Recognizing Temporal Transformations', 'Representation learning', 'Video analysis', 'Self-supervised learning', 'Unsupervised learning', 'Time dynamics', 'We introduce a novel self-supervised learning approach to learn representations of videos that are responsive to changes in the motion dynamics. Our representations can be learned from data without human annotation and provide a substantial boost to the training of neural networks on small labeled data sets for tasks such as action recognition, which require to accurately distinguish the motion of objects. We promote an accurate learning of motion without human annotation by training a neural network to discriminate a video sequence from its temporally transformed versions. To learn to distinguish non-trivial motions, the design of the transformations is based on two principles: 1) To define clusters of motions based on time warps of different magnitude; 2) To ensure that the discrimination is feasible only by observing and analyzing as many image frames as possible. Thus, we introduce the following transformations: forward-backward playback, random frame skipping, and uniform frame skipping. Our experiments show that networks trained with the proposed method yield representations with improved transfer performance for action recognition on UCF101 and HMDB51.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_26');
INSERT INTO `paper` VALUES (12968, 'Video Super-Resolution with Recurrent Structure-Detail Network', 'Video super-resolution', 'Recurrent neural network', 'Two-stream block', '', '', 'Most video super-resolution methods super-resolve a single reference frame with the help of neighboring frames in a temporal sliding window. They are less efficient compared to the recurrent-based methods. In this work, we propose a novel recurrent video super-resolution method which is both effective and efficient in exploiting previous frames to super-resolve the current frame. It divides the input into structure and detail components which are fed to a recurrent unit composed of several proposed two-stream structure-detail blocks. In addition, a hidden state adaptation module that allows the current frame to selectively use information from hidden state is introduced to enhance its robustness to appearance change and error accumulation. Extensive ablation study validate the effectiveness of the proposed modules. Experiments on several benchmark datasets demonstrate superior performance of the proposed method compared to state-of-the-art methods on video super-resolution. Code is available at https://github.com/junpan19/RSDN.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_38');
INSERT INTO `paper` VALUES (12969, 'Video-Based Remote Physiological Measurement via Cross-Verified Feature Disentangling', '', '', '', '', '', 'Remote physiological measurements, e.g., remote photoplethysmography (rPPG) based heart rate (HR), heart rate variability (HRV) and respiration frequency (RF) measuring, are playing more and more important roles under the application scenarios where contact measurement is inconvenient or impossible. Since the amplitude of the physiological signals is very small, they can be easily affected by head movements, lighting conditions, and sensor diversities. To address these challenges, we propose a cross-verified feature disentangling strategy to disentangle the physiological features with non-physiological representations, and then use the distilled physiological features for robust multi-task physiological measurements. We first transform the input face videos into a multi-scale spatial-temporal map (MSTmap), which can suppress the irrelevant background and noise features while retaining most of the temporal characteristics of the periodic physiological signals. Then we take pairwise MSTmaps as inputs to an autoencoder architecture with two encoders (one for physiological signals and the other for non-physiological information) and use a cross-verified scheme to obtain physiological features disentangled with the non-physiological features. The disentangled features are finally used for the joint prediction of multiple physiological signals like average HR values and rPPG signals. Comprehensive experiments on different large-scale public datasets of multiple physiological measurement tasks as well as the cross-database testing demonstrate the robustness of our approach.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_18');
INSERT INTO `paper` VALUES (12970, 'View-Invariant Probabilistic Embedding for Human Pose', 'Human pose embedding', 'Probabilistic embedding', 'View-invariant pose retrieval', '', '', 'Depictions of similar human body configurations can vary with changing viewpoints. Using only 2D information, we would like to enable vision algorithms to recognize similarity in human body poses across multiple views. This ability is useful for analyzing body movements and human behaviors in images and videos. In this paper, we propose an approach for learning a compact view-invariant embedding space from 2D joint keypoints alone, without explicitly predicting 3D poses. Since 2D poses are projected from 3D space, they have an inherent ambiguity, which is difficult to represent through a deterministic mapping. Hence, we use probabilistic embeddings to model this input uncertainty. Experimental results show that our embedding model achieves higher accuracy when retrieving similar poses across different camera views, in comparison with 2D-to-3D pose lifting models. We also demonstrate the effectiveness of applying our embeddings to view-invariant action recognition and video alignment. Our code is available at https://github.com/google-research/google-research/tree/master/poem.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_4');
INSERT INTO `paper` VALUES (12971, 'Virtual Multi-view Fusion for 3D Semantic Segmentation', '3D semantic segmentation', 'Scene understanding', '', '', '', 'Semantic segmentation of 3D meshes is an important problem for 3D scene understanding. In this paper we revisit the classic multiview representation of 3D meshes and study several techniques that make them effective for 3D semantic segmentation of meshes. Given a 3D mesh reconstructed from RGBD sensors, our method effectively chooses different virtual views of the 3D mesh and renders multiple 2D channels for training an effective 2D semantic segmentation model. Features from multiple per view predictions are finally fused on 3D mesh vertices to predict mesh semantic segmentation labels. Using the large scale indoor 3D semantic segmentation benchmark of ScanNet, we show that our virtual views enable more effective training of 2D semantic segmentation networks than previous multiview approaches. When the 2D per pixel predictions are aggregated on 3D surfaces, our virtual multiview fusion method is able to achieve significantly better 3D semantic segmentation results compared to all prior multiview approaches and recent 3D convolution approaches.', 'ECCV', '2020', '30 November 2020', 'https://doi.org/10.1007/978-3-030-58586-0_31');
INSERT INTO `paper` VALUES (12972, 'VisDrone-CC2020: The Vision Meets Drone Crowd Counting Challenge Results', 'VisDrone', 'Crowd counting', 'Challenge', 'Benchmark', '', 'Crowd counting on the drone platform is an interesting topic in computer vision, which brings new challenges such as small object inference, background clutter and wide viewpoint. However, there are few algorithms focusing on crowd counting on the drone-captured data due to the lack of comprehensive datasets. To this end, we collect a large-scale dataset and organize the Vision Meets Drone Crowd Counting Challenge (VisDrone-CC2020) in conjunction with the 16th European Conference on Computer Vision (ECCV 2020) to promote the developments in the related fields. The collected dataset is formed by 3, 360 images, including 2, 460 images for training, and 900 images for testing. Specifically, we manually annotate persons with points in each video frame. There are 14 algorithms from 15 institutes submitted to the VisDrone-CC2020 Challenge. We provide a detailed analysis of the evaluation results and conclude the challenge. More information can be found at the website: http://www.aiskyeye.com/.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_41');
INSERT INTO `paper` VALUES (12973, 'VisDrone-DET2020: The Vision Meets Drone Object Detection in Image Challenge Results', 'Drone', 'Object detection', 'Evaluation', '', '', 'The Vision Meets Drone Object Detection in Image Challenge (VisDrone-DET 2020) is the third annual object detector benchmarking activity. Compared with the previous VisDrone-DET 2018 and VisDrone-DET 2019 challenges, many submitted object detectors exceed the recent state-of-the-art detectors. Based on the selected 29 robust detection methods, we discuss the experimental results comprehensively, which shows the effectiveness of ensemble learning and data augmentation in drone captured object detection. The full challenge results are publicly available at the website http://aiskyeye.com/leaderboard/.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_42');
INSERT INTO `paper` VALUES (12974, 'VisDrone-MOT2020: The Vision Meets Drone Multiple Object Tracking Challenge Results', 'Drone-based multiple object tracking', 'Drone', 'Performance evaluation', '', '', 'The Vision Meets Drone (VisDrone2020) Multiple Object Tracking (MOT) is the third annual UAV MOT tracking evaluation activity organized by the VisDrone team, in conjunction with European Conference on Computer Vision (ECCV 2020). The VisDrone-MOT2020 consists of 79 challenging video sequences, including 56 videos (\\(\\sim \\)24K frames) for training, 7 videos (\\(\\sim \\)3K frames) for validation and 17 videos (\\(\\sim \\)6K frames) for evaluation. All frames in these sequences are manually annotated with high-quality bounding boxes. Results of 12 participating MOT algorithms are presented and analyzed in detail. The challenging results, video sequences as well as the evaluation toolkit are made available at http://aiskyeye.com/. By holding VisDrone-MOT2020 challenge, we hope to facilitate future research and applications of MOT algorithms on drone videos.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_43');
INSERT INTO `paper` VALUES (12975, 'VisDrone-SOT2020: The Vision Meets Drone Single Object Tracking Challenge Results', 'Drone-based single object tracking', 'Drone', 'Performance evaluation', '', '', 'The Vision Meets Drone (VisDrone2020) Single Object Tracking is the third annual UAV tracking evaluation activity organized by the VisDrone team, in conjunction with European Conference on Computer Vision (ECCV 2020). The VisDrone-SOT2020 Challenge presents and discusses the results of 13 participating algorithms in detail. By using ensemble of different trackers trained on several large-scale datasets, the top performer in VisDrone-SOT2020 achieves better results than the counterparts in VisDrone-SOT2018 and VisDrone-SOT2019. The challenging results, collected videos as well as the valuation toolkit are made available at http://aiskyeye.com/. By holding VisDrone-SOT2020 challenge, we hope to provide the community a dedicated platform for developing and evaluating drone-based tracking approaches.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_44');
INSERT INTO `paper` VALUES (12976, 'Visible Feature Guidance for Crowd Pedestrian Detection', '', '', '', '', '', 'Heavy occlusion and dense gathering in crowd scene make pedestrian detection become a challenging problem, because it’s difficult to guess a precise full bounding box according to the invisible human part. To crack this nut, we propose a mechanism called Visible Feature Guidance (VFG) for both training and inference. During training, we adopt visible feature to regress the simultaneous outputs of visible bounding box and full bounding box. Then we perform NMS only on visible bounding boxes to achieve the best fitting full box in inference. This manner can alleviate the incapable influence brought by NMS in crowd scene and make full bounding box more precisely. Furthermore, in order to ease feature association in the post application process, such as pedestrian tracking, we apply Hungarian algorithm to associate parts for a human instance. Our proposed method can stably bring about 2–3% improvements in mAP and \\(\\text {AP}_{50}\\) for both two-stage and one-stage detector. It’s also more effective for \\(\\text {MR}^{-2}\\) especially with the stricter IoU. Experiments on Crowdhuman, Cityperson, Caltech and KITTI datasets show that visible feature guidance can help detector achieve promisingly better performances. Moreover, parts association produces a strong benchmark on Crowdhuman for the vision community.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_20');
INSERT INTO `paper` VALUES (12977, 'Visual Compositional Learning for Human-Object Interaction Detection', 'Human-object interaction', 'Compositional learning', '', '', '', 'Human-Object interaction (HOI) detection aims to localize and infer relationships between human and objects in an image. It is challenging because an enormous number of possible combinations of objects and verbs types forms a long-tail distribution. We devise a deep Visual Compositional Learning (VCL) framework, which is a simple yet efficient framework to effectively address this problem. VCL first decomposes an HOI representation into object and verb specific features, and then composes new interaction samples in the feature space via stitching the decomposed features. The integration of decomposition and composition enables VCL to share object and verb features among different HOI samples and images, and to generate new interaction samples and new types of HOI, and thus largely alleviates the long-tail distribution problem and benefits low-shot or zero-shot HOI detection. Extensive experiments demonstrate that the proposed VCL can effectively improve the generalization of HOI detection on HICO-DET and V-COCO and outperforms the recent state-of-the-art methods on HICO-DET. Code is available at https://github.com/zhihou7/VCL.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_35');
INSERT INTO `paper` VALUES (12978, 'Visual Memorability for Robotic Interestingness via Unsupervised Online Learning', 'Unsupervised', 'Online', 'Memorability', 'Interestingness', '', 'In this paper, we explore the problem of interesting scene prediction for mobile robots. This area is currently underexplored but is crucial for many practical applications such as autonomous exploration and decision making. Inspired by industrial demands, we first propose a novel translation-invariant visual memory for recalling and identifying interesting scenes, then design a three-stage architecture of long-term, short-term, and online learning. This enables our system to learn human-like experience, environmental knowledge, and online adaption, respectively. Our approach achieves much higher accuracy than the state-of-the-art algorithms on challenging robotic interestingness datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_4');
INSERT INTO `paper` VALUES (12979, 'Visual Question Answering on Image Sets', '', '', '', '', '', 'We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets – indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human-annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_4');
INSERT INTO `paper` VALUES (12980, 'Visual Relation Grounding in Videos', '', '', '', '', '', 'In this paper, we explore a novel task named visual Relation Grounding in Videos (vRGV). The task aims at spatio-temporally localizing the given relations in the form of subject-predicate-object in the videos, so as to provide supportive visual facts for other high-level video-language tasks (e.g., video-language grounding and video question answering). The challenges in this task include but not limited to: (1) both the subject and object are required to be spatio-temporally localized to ground a query relation; (2) the temporal dynamic nature of visual relations in videos is difficult to capture; and (3) the grounding should be achieved without any direct supervision in space and time. To ground the relations, we tackle the challenges by collaboratively optimizing two sequences of regions over a constructed hierarchical spatio-temporal region graph through relation attending and reconstruction, in which we further propose a message passing mechanism by spatial attention shifting between visual entities. Experimental results demonstrate that our model can not only outperform baseline approaches significantly, but also produces visually meaningful facts to support visual grounding. (Code is available at https://github.com/doc-doc/vRGV).', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_27');
INSERT INTO `paper` VALUES (12981, 'Visual-Relation Conscious Image Generation from Structured-Text', '', '', '', '', '', 'We propose an end-to-end network for image generation from given structured-text that consists of the visual-relation layout module and stacking-GANs. Our visual-relation layout module uses relations among entities in the structured-text in two ways: comprehensive usage and individual usage. We comprehensively use all relations together to localize initial bounding-boxes (BBs) of all the entities. We use individual relation separately to predict from the initial BBs relation-units for all the relations. We then unify all the relation-units to produce the visual-relation layout, i.e., BBs for all the entities so that each of them uniquely corresponds to each entity while keeping its involved relations. Our visual-relation layout reflects the scene structure given in the input text. The stacking-GANs is the stack of three GANs conditioned on the visual-relation layout and the output of previous GAN, consistently capturing the scene structure. Our network realistically renders entities’ details while keeping the scene structure. Experimental results on two public datasets show the effectiveness of our method.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_18');
INSERT INTO `paper` VALUES (12982, 'VisualCOMET: Reasoning About the Dynamic Context of a Still Image', '', '', '', '', '', 'Even from a single frame of a still image, people can reason about the dynamic story of the image before, after, and beyond the frame. For example, given an image of a man struggling to stay afloat in water, we can reason that the man fell into the water sometime in the past, the intent of that man at the moment is to stay alive, and he will need help in the near future or else he will get washed away. We propose VisualCOMET, (Visual Commonsense Reasoning in Time.) the novel framework of visual commonsense reasoning tasks to predict events that might have happened before, events that might happen next, and the intents of the people at present. To support research toward visual commonsense reasoning, we introduce the first large-scale repository of Visual Commonsense Graphs that consists of over 1.4 million textual descriptions of visual commonsense inferences carefully annotated over a diverse set of 59,000 images, each paired with short video summaries of before and after. In addition, we provide person-grounding (i.e., co-reference links) between people appearing in the image and people mentioned in the textual commonsense descriptions, allowing for tighter integration between images and text. We establish strong baseline performances on this task and demonstrate that integration between visual and textual commonsense reasoning is the key and wins over non-integrative alternatives.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_30');
INSERT INTO `paper` VALUES (12983, 'VisualEchoes: Spatial Image Representation Learning Through Echolocation', '', '', '', '', '', 'Several animal species (e.g., bats, dolphins, and whales) and even visually impaired humans have the remarkable ability to perform echolocation: a biological sonar used to perceive spatial layout and locate objects in the world. We explore the spatial cues contained in echoes and how they can benefit vision tasks that require spatial reasoning. First we capture echo responses in photo-realistic 3D indoor scene environments. Then we propose a novel interaction-based representation learning framework that learns useful visual features via echolocation. We show that the learned image features are useful for multiple downstream vision tasks requiring spatial reasoning—monocular depth estimation, surface normal estimation, and visual navigation—with results comparable or even better than heavily supervised pre-training. Our work opens a new path for representation learning for embodied agents, where supervision comes from interacting with the physical world.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_38');
INSERT INTO `paper` VALUES (12984, 'ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural Language', 'Person search by natural language', 'Person re-identification', 'Vision and language', 'Metric learning', '', 'Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as a performance boost by a robust feature learning that the referred identity can be accurately bundled by multiple attribute cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into sub-spaces corresponding to attributes using a light auxiliary attribute segmentation layer. It then aligns these visual features with the textual attributes parsed from the sentences via a novel contrastive learning loss. We validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Codes and models are available at https://github.com/Jarr0d/ViTAA.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_24');
INSERT INTO `paper` VALUES (12985, 'VLANet: Video-Language Alignment Network for Weakly-Supervised Video Moment Retrieval', 'Multi-modal learning', 'Weakly-supervised learning', 'Video moment retrieval', '', '', 'Video Moment Retrieval (VMR) is a task to localize the temporal moment in untrimmed video specified by natural language query. For VMR, several methods that require full supervision for training have been proposed. Unfortunately, acquiring a large number of training videos with labeled temporal boundaries for each query is a labor-intensive process. This paper explores a method for performing VMR in a weakly-supervised manner (wVMR): training is performed without temporal moment labels but only with the text query that describes a segment of the video. Existing methods on wVMR generate multi-scale proposals and apply query-guided attention mechanism to highlight the most relevant proposal. To leverage the weak supervision, contrastive learning is used which predicts higher scores for the correct video-query pairs than for the incorrect pairs. It has been observed that a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention map which limits the localization performance. To address this issue, Video-Language Alignment Network (VLANet) is proposed that learns a sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, and thus substantially reduces candidate proposals which leads to lower computation load and sharper attention. Next, the Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flows to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss which enforces semantically similar videos and queries to cluster. The experiments show that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_10');
INSERT INTO `paper` VALUES (12986, 'Volumetric Transformer Networks', 'Spatial invariance', 'Attention', 'Feature channels', 'Fine-grained image recognition', 'Instance-level image retrieval', 'Existing techniques to encode spatial invariance within deep convolutional neural networks (CNNs) apply the same warping field to all the feature channels. This does not account for the fact that the individual feature channels can represent different semantic parts, which can undergo different spatial transformations w.r.t. a canonical configuration. To overcome this limitation, we introduce a learnable module, the volumetric transformer network (VTN), that predicts channel-wise warping fields so as to reconfigure intermediate CNN features spatially and channel-wisely. We design our VTN as an encoder-decoder network, with modules dedicated to letting the information flow across the feature channels, to account for the dependencies between the semantic parts. We further propose a loss function defined between the warped features of pairs of instances, which improves the localization ability of VTN. Our experiments show that VTN consistently boosts the features’ representation power and consequently the networks’ accuracy on fine-grained image recognition and instance-level image retrieval.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_34');
INSERT INTO `paper` VALUES (12987, 'VoxelPose: Towards Multi-camera 3D Human Pose Estimation in Wild Environment', '3D human pose estimation', '', '', '', '', 'We present VoxelPose to estimate 3D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 2D pose estimates, VoxelPose directly operates in the 3D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the 3D voxel space and fed into Cuboid Proposal Network (CPN) to localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58452-8_12');
INSERT INTO `paper` VALUES (12988, 'VPN: Learning Video-Pose Embedding for Activities of Daily Living', 'Action recognition', 'Video', 'Pose', 'Embedding', 'Attention', 'In this paper, we focus on the spatio-temporal aspect of recognizing Activities of Daily Living (ADL). ADL have two specific properties (i) subtle spatio-temporal patterns and (ii) similar visual patterns varying with time. Therefore, ADL may look very similar and often necessitate to look at their fine-grained details to distinguish them. Because the recent spatio-temporal 3D ConvNets are too rigid to capture the subtle visual patterns across an action, we propose a novel Video-Pose Network: VPN. The 2 key components of this VPN are a spatial embedding and an attention network. The spatial embedding projects the 3D poses and RGB cues in a common semantic space. This enables the action recognition framework to learn better spatio-temporal features exploiting both modalities. In order to discriminate similar actions, the attention network provides two functionalities - (i) an end-to-end learnable pose backbone exploiting the topology of human body, and (ii) a coupler to provide joint spatio-temporal attention weights across a video. Experiments (Code/models: https://github.com/srijandas07/VPN) show that VPN outperforms the state-of-the-art results for action classification on a large scale human activity dataset: NTU-RGB+D 120, its subset NTU-RGB+D 60, a real-world challenging human activity dataset: Toyota Smarthome and a small scale human-object interaction dataset Northwestern UCLA.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_5');
INSERT INTO `paper` VALUES (12989, 'VQA-LOL: Visual Question Answering Under the Lens of Logic', 'Visual question answering', 'Logical robustness', '', '', '', 'Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this Lens of Logic, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our Lens of Logic (LOL) model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fréchet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_23');
INSERT INTO `paper` VALUES (12990, 'W2S: Microscopy Data with Joint Denoising and Super-Resolution for Widefield to SIM Mapping', 'Image restoration dataset', 'Denoising', 'Super-resolution', 'Microscopy imaging', 'Joint optimization', 'In fluorescence microscopy live-cell imaging, there is a critical trade-off between the signal-to-noise ratio and spatial resolution on one side, and the integrity of the biological sample on the other side. To obtain clean high-resolution (HR) images, one can either use microscopy techniques, such as structured-illumination microscopy (SIM), or apply denoising and super-resolution (SR) algorithms. However, the former option requires multiple shots that can damage the samples, and although efficient deep learning based algorithms exist for the latter option, no benchmark exists to evaluate these algorithms on the joint denoising and SR (JDSR) tasks.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_31');
INSERT INTO `paper` VALUES (12991, 'Watch Hours in Minutes: Summarizing Videos with User Intent', 'Query-focused', 'Video summarization', 'Attention', 'User-intent', '', 'With the ever increasing growth of videos, automatic video summarization has become an important task which has attracted lot of interest in the research community. One of the challenges which makes it a hard problem to solve is presence of multiple ‘correct answers’. Because of the highly subjective nature of the task, there can be different “ideal” summaries of a video. Modelling user intent in the form of queries has been posed in literature as a way to alleviate this problem. The query-focused summary is expected to contain shots which are relevant to the query in conjunction with other important shots. For practical deployments in which very long videos need to be summarized, this need to capture user’s intent becomes all the more pronounced. In this work, we propose a simple two stage method which takes user query and video as input and generates a query-focused summary. Specifically, in the first stage, we employ attention within a segment and across all segments, combined with the query to learn the feature representation of each shot. In the second stage, such learned features are again fused with the query to learn the score of each shot by regressing through fully connected layers. We then assemble the summary by arranging the top scoring shots in chronological order. Extensive experiments on a benchmark query-focused video summarization dataset for long videos give better results as compared to the current state of the art, thereby demonstrating the effectiveness of our method even without employing computationally expensive architectures like LSTMs, variational autoencoders, GANs or reinforcement learning, as done by most past works.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_47');
INSERT INTO `paper` VALUES (12992, 'Wavelet-Based Dual-Branch Network for Image Demoiréing', 'Deep learning', 'Image demoiréing', 'Wavelet', '', '', 'When smartphone cameras are used to take photos of digital screens, usually moiré patterns result, severely degrading photo quality. In this paper, we design a wavelet-based dual-branch network (WDNet) with a spatial attention mechanism for image demoiréing. Existing image restoration methods working in the RGB domain have difficulty in distinguishing moiré patterns from true scene texture. Unlike these methods, our network removes moiré patterns in the wavelet domain to separate the frequencies of moiré patterns from the image content. The network combines dense convolution modules and dilated convolution modules supporting large receptive fields. Extensive experiments demonstrate the effectiveness of our method, and we further show that WDNet generalizes to removing moiré artifacts on non-screen images. Although designed for image demoiréing, WDNet has been applied to two other low-level vision tasks, outperforming state-of-the-art image deraining and deraindrop methods on the Rain100h and Raindrop800 data sets, respectively.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_6');
INSERT INTO `paper` VALUES (12993, 'WaveTransform: Crafting Adversarial Examples via Input Decomposition', 'Transformed domain attacks', 'Resiliency', 'Transferability', 'Wavelet', 'CNN', 'Frequency spectrum has played a significant role in learning unique and discriminating features for object recognition. Both low and high frequency information present in images have been extracted and learnt by a host of representation learning techniques, including deep learning. Inspired by this observation, we introduce a novel class of adversarial attacks, namely ‘WaveTransform’, that creates adversarial noise corresponding to low-frequency and high-frequency subbands, separately (or in combination). The frequency subbands are analyzed using wavelet decomposition; the subbands are corrupted and then used to construct an adversarial example. Experiments are performed using multiple databases and CNN models to establish the effectiveness of the proposed WaveTransform attack and analyze the importance of a particular frequency component. The robustness of the proposed attack is also evaluated through its transferability and resiliency against a recent adversarial defense algorithm. Experiments show that the proposed attack is effective against the defense algorithm and is also transferable across CNNs.', 'ECCV', '2020', '10 January 2021', 'https://doi.org/10.1007/978-3-030-66415-2_10');
INSERT INTO `paper` VALUES (12994, 'WDRN: A Wavelet Decomposed RelightNet for Image Relighting', 'Gray loss', 'Illumination', 'Relighting', 'Wavelet', '', 'The task of recalibrating the illumination settings in an image to a target configuration is known as relighting. Relighting techniques have potential applications in digital photography, gaming industry and in augmented reality. In this paper, we address the one-to-one relighting problem where an image at a target illumination settings is predicted given an input image with specific illumination conditions. To this end, we propose a wavelet decomposed RelightNet called WDRN which is a novel encoder-decoder network employing wavelet based decomposition followed by convolution layers under a muti-resolution framework. We also propose a novel loss function called gray loss that ensures efficient learning of gradient in illumination along different directions of the ground truth image giving rise to visually superior relit images. The proposed solution won the first position in the relighting challenge event in advances in image manipulation (AIM) 2020 workshop which proves its effectiveness measured in terms of a Mean Perceptual Score which in turn is measured using SSIM and a Learned Perceptual Image Patch Similarity score.', 'ECCV', '2020', '30 January 2021', 'https://doi.org/10.1007/978-3-030-67070-2_31');
INSERT INTO `paper` VALUES (12995, 'We Have So Much in Common: Modeling Semantic Relational Set Abstractions in Videos', 'Set abstraction', 'Video understanding', 'Relational learning', '', '', 'Identifying common patterns among events is a key capability for human and machine perception, as it underlies intelligent decision making. Here, we propose an approach for learning semantic relational set abstractions on videos, inspired by human learning. Our model combines visual features as input with natural language supervision to generate high-level representations of similarities across a set of videos. This allows our model to perform cognitive tasks such as set abstraction (which general concept is in common among a set of videos?), set completion (which new video goes well with the set?), and odd one out detection (which video does not belong to the set?). Experiments on two video benchmarks, Kinetics and Multi-Moments in Time, show that robust and versatile representations emerge when learning to recognize commonalities among sets. We compare our model to several baseline algorithms and show that significant improvements result from explicitly learning relational abstractions with semantic supervision. Code and models are available online (Project website: abstraction.csail.mit.edu).', 'ECCV', '2020', '04 December 2020', 'https://doi.org/10.1007/978-3-030-58523-5_2');
INSERT INTO `paper` VALUES (12996, 'We Learn Better Road Pothole Detection: From Attention Aggregation to Adversarial Domain Adaptation', '', '', '', '', '', 'Manual visual inspection performed by certified inspectors is still the main form of road pothole detection. This process is, however, not only tedious, time-consuming and costly, but also dangerous for the inspectors. Furthermore, the road pothole detection results are always subjective, because they depend entirely on the individual experience. Our recently introduced disparity (or inverse depth) transformation algorithm allows better discrimination between damaged and undamaged road areas, and it can be easily deployed to any semantic segmentation network for better road pothole detection results. To boost the performance, we propose a novel attention aggregation (AA) framework, which takes the advantages of different types of attention modules. In addition, we develop an effective training set augmentation technique based on adversarial domain adaptation, where the synthetic road RGB images and transformed road disparity (or inverse depth) images are generated to enhance the training of semantic segmentation networks. The experimental results demonstrate that, firstly, the transformed disparity (or inverse depth) images become more informative; secondly, AA-UNet and AA-RTFNet, our best performing implementations, respectively outperform all other state-of-the-art single-modal and data-fusion networks for road pothole detection; and finally, the training set augmentation technique based on adversarial domain adaptation not only improves the accuracy of the state-of-the-art semantic segmentation networks, but also accelerates their convergence.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66823-5_17');
INSERT INTO `paper` VALUES (12997, 'Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints', '3D hand pose', 'Weakly-supervised', 'Biomechanical constraints', '', '', 'Estimating 3D hand pose from 2D images is a difficult, inverse problem due to the inherent scale and depth ambiguities. Current state-of-the-art methods train fully supervised deep neural networks with 3D ground-truth data. However, acquiring 3D annotations is expensive, typically requiring calibrated multi-view setups or labour intensive manual annotations. While annotations of 2D keypoints are much easier to obtain, how to efficiently leverage such weakly-supervised data to improve the task of 3D hand pose prediction remains an important open question. The key difficulty stems from the fact that direct application of additional 2D supervision mostly benefits the 2D proxy objective but does little to alleviate the depth and scale ambiguities. Embracing this challenge we propose a set of novel losses that constrain the prediction of a neural network to lie within the range of biomechanically feasible 3D hand configurations. We show by extensive experiments that our proposed constraints significantly reduce the depth ambiguity and allow the network to more effectively leverage additional 2D annotated images. For example, on the challenging freiHAND dataset, using additional 2D annotation without our proposed biomechanical constraints reduces the depth error by only \\(15\\%\\), whereas the error is reduced significantly by \\(50\\%\\) when the proposed biomechanical constraints are used.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_13');
INSERT INTO `paper` VALUES (12998, 'Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows', '3D human sensing', 'Normalizing flows', 'Semantic alignment', '', '', 'Monocular 3D human pose and shape estimation is challenging due to the many degrees of freedom of the human body and the difficulty to acquire training data for large-scale supervised learning in complex visual scenes. In this paper we present practical semi-supervised and self-supervised models that support training and good generalization in real-world images and video. Our formulation is based on kinematic latent normalizing flow representations and dynamics, as well as differentiable, semantic body part alignment loss functions that support self-supervised learning. In extensive experiments using 3D motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, as well as image repositories like COCO, we show that the proposed methods outperform the state of the art, supporting the practical construction of an accurate family of models based on large-scale training with diverse and incompletely labeled image and video data.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58539-6_28');
INSERT INTO `paper` VALUES (12999, 'Weakly Supervised 3D Object Detection from Lidar Point Cloud', '3d object detection', 'Weakly supervised learning', '', '', '', 'It is laborious to manually label point cloud data for training high-quality 3D object detectors. This work proposes a weakly supervised approach for 3D object detection, only requiring a small set of weakly annotated scenes, associated with a few precisely labeled object instances. This is achieved by a two-stage architecture design. Stage-1 learns to generate cylindrical object proposals under weak supervision, i.e., only the horizontal centers of objects are click-annotated in bird’s view scenes. Stage-2 learns to refine the cylindrical proposals to get cuboids and confidence scores, using a few well-labeled instances. Using only 500 weakly annotated scenes and 534 precisely labeled vehicle instances, our method achieves \\(85\\) \\(-\\) \\(95\\)% the performance of current top-leading, fully supervised detectors (requiring 3, 712 exhaustively and precisely annotated scenes with 15, 654 instances). Moreover, with our elaborately designed network architecture, our trained model can be applied as a 3D object annotator, supporting both automatic and active (human-in-the-loop) working modes. The annotations generated by our model can be used to train 3D object detectors, achieving over 94% of their original performance (with manually labeled training data). Our experiments also show our model’s potential in boosting performance when given more training data. Above designs make our approach highly practical and introduce new opportunities for learning 3D object detection at reduced annotation cost.', 'ECCV', '2020', '28 November 2020', 'https://doi.org/10.1007/978-3-030-58601-0_31');
INSERT INTO `paper` VALUES (13000, 'Weakly Supervised Instance Segmentation by Learning Annotation Consistent Instances', '', '', '', '', '', 'Recent approaches for weakly supervised instance segmentations depend on two components: (i) a pseudo label generation model which provides instances that are consistent with a given annotation; and (ii) an instance segmentation model, which is trained in a supervised manner using the pseudo labels as ground-truth. Unlike previous approaches, we explicitly model the uncertainty in the pseudo label generation process using a conditional distribution. The samples drawn from our conditional distribution provide accurate pseudo labels due to the use of semantic class aware unary terms, boundary aware pairwise smoothness terms, and annotation aware higher order terms. Furthermore, we represent the instance segmentation model as an annotation agnostic prediction distribution. In contrast to previous methods, our representation allows us to define a joint probabilistic learning objective that minimizes the dissimilarity between the two distributions. Our approach achieves state of the art results on the PASCAL VOC 2012 data set, outperforming the best baseline by \\(4.2\\%\\ \\text {mAP}^r_{0.5}\\) and \\(4.8\\%\\ \\text {mAP}^r_{0.75}\\).', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58604-1_16');
INSERT INTO `paper` VALUES (13001, 'Weakly Supervised Learning with Side Information for Noisy Labeled Images', 'Weakly supervised learning', 'Noisy labels', 'Side information', 'Large scale web images', '', 'In many real-world datasets, like WebVision, the performance of DNN based classifier is often limited by the noisy labeled data. To tackle this problem, some image related side information, such as captions and tags, often reveal underlying relationships across images. In this paper, we present an efficient weakly-supervised learning by using a Side Information Network (SINet), which aims to effectively carry out a large scale classification with severely noisy labels. The proposed SINet consists of a visual prototype module and a noise weighting module. The visual prototype module is designed to generate a compact representation for each category by introducing the side information. The noise weighting module aims to estimate the correctness of each noisy image and produce a confidence score for image ranking during the training procedure. The propsed SINet can largely alleviate the negative impact of noisy image labels, and is beneficial to train a high performance CNN based classifier. Besides, we released a fine-grained product dataset called AliProducts, which contains more than 2.5 million noisy web images crawled from the internet by using queries generated from 50,000 fine-grained semantic classes. Extensive experiments on several popular benchmarks (i.e. Webvision, ImageNet and Clothing-1M) and our proposed AliProducts achieve state-of-the-art performance. The SINet has won the first place in the 5000 category classification task on WebVision Challenge 2019, and outperforms other competitors by a large margin.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_19');
INSERT INTO `paper` VALUES (13002, 'Weakly Supervised Minirhizotron Image Segmentation with MIL-CAM', '', '', '', '', '', 'We present a multiple instance learning class activation map (MIL-CAM) approach for pixel-level minirhizotron image segmentation given weak image-level labels. Minirhizotrons are used to image plant roots in situ. Minirhizotron imagery is often composed of soil containing a few long and thin root objects of small diameter. The roots prove to be challenging for existing semantic image segmentation methods to discriminate. In addition to learning from weak labels, our proposed MIL-CAM approach re-weights the root versus soil pixels during analysis for improved performance due to the heavy imbalance between soil and root pixels. The proposed approach outperforms other attention map and multiple instance learning methods for localization of root objects in minirhizotron imagery.', 'ECCV', '2020', '05 January 2021', 'https://doi.org/10.1007/978-3-030-65414-6_30');
INSERT INTO `paper` VALUES (13003, 'Weakly Supervised Semantic Segmentation with Boundary Exploration', 'Weak supervision', 'Semantic segmentation', 'Deep learning', '', '', 'Weakly supervised semantic segmentation with image-level labels has attracted a lot of attention recently because these labels are already available in most datasets. To obtain semantic segmentation under weak supervision, this paper presents a simple yet effective approach based on the idea of explicitly exploring object boundaries from training images to keep coincidence of segmentation and boundaries. Specifically, we synthesize boundary annotations by exploiting coarse localization maps obtained from CNN classifier, and use annotations to train the proposed network called BENet which further excavates more object boundaries to provide constraints for segmentation. Finally generated pseudo annotations of training images are used to supervise an off-the-shelf segmentation network. We evaluate the proposed method on PASCAL VOC 2012 benchmark and the final results achieve 65.7% and 66.6% mIoU scores on val and test sets respectively, which outperforms previous methods trained under image-level supervision.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_21');
INSERT INTO `paper` VALUES (13004, 'Weakly-Supervised 3D Shape Completion in the Wild', '', '', '', '', '', '3D shape completion for real data is important but challenging, since partial point clouds acquired by real-world sensors are usually sparse, noisy and unaligned. Different from previous methods, we address the problem of learning 3D complete shape from unaligned and real-world partial point clouds. To this end, we propose a weakly-supervised method to estimate both 3D canonical shape and 6-DoF pose for alignment, given multiple partial observations associated with the same instance. The network jointly optimizes canonical shapes and poses with multi-view geometry constraints during training, and can infer the complete shape given a single partial point cloud. Moreover, learned pose estimation can facilitate partial point cloud registration. Experiments on both synthetic and real data show that it is feasible and promising to learn 3D shape completion through large-scale data without shape and pose supervision.', 'ECCV', '2020', '29 October 2020', 'https://doi.org/10.1007/978-3-030-58558-7_17');
INSERT INTO `paper` VALUES (13005, 'Weakly-Supervised Action Localization with Expectation-Maximization Multi-Instance Learning', 'Weakly-supervised learning', 'Action localization', 'Multiple instance learning', '', '', 'Weakly-supervised action localization requires training a model to localize the action segments in the video given only video level action label. It can be solved under the Multiple Instance Learning (MIL) framework, where a bag (video) contains multiple instances (action segments). Since only the bag’s label is known, the main challenge is assigning which key instances within the bag to trigger the bag’s label. Most previous models use attention-based approaches applying attentions to generate the bag’s representation from instances, and then train it via the bag’s classification. These models, however, implicitly violate the MIL assumption that instances in negative bags should be uniformly negative. In this work, we explicitly model the key instances assignment as a hidden variable and adopt an Expectation-Maximization (EM) framework. We derive two pseudo-label generation schemes to model the E and M process and iteratively optimize the likelihood lower bound. We show that our EM-MIL approach more accurately models both the learning objective and the MIL assumptions. It achieves state-of-the-art performance on two standard benchmarks, THUMOS14 and ActivityNet1.2.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_43');
INSERT INTO `paper` VALUES (13006, 'Weakly-Supervised Cell Tracking via Backward-and-Forward Propagation', 'Cell tracking', 'Weakly-supervised learning', 'Multi-object tracking', 'Cell detection', 'Tracking', 'We propose a weakly-supervised cell tracking method that can train a convolutional neural network (CNN) by using only the annotation of “cell detection” (i.e., the coordinates of cell positions) without association information, in which cell positions can be easily obtained by nuclear staining. First, we train co-detection CNN that detects cells in successive frames by using weak-labels. Our key assumption is that co-detection CNN implicitly learns association in addition to detection. To obtain the association, we propose a backward-and-forward propagation method that analyzes the correspondence of cell positions in the outputs of co-detection CNN. Experiments demonstrated that the proposed method can associate cells by analyzing co-detection CNN. Even though the method uses only weak supervision, the performance of our method was almost the same as the state-of-the-art supervised method. Code is publicly available in https://github.com/naivete5656/WSCTBFP.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58610-2_7');
INSERT INTO `paper` VALUES (13007, 'Weakly-Supervised Crowd Counting Learns from Sorting Rather Than Locations', 'Weakly-supervised', 'Sorting', 'Multi-frames', 'Crowd counting', '', 'In crowd counting datasets, the location labels are costly, yet, they are not taken into the evaluation metrics. Besides, existing multi-task approaches employ high-level tasks to improve counting accuracy. This research tendency increases the demand for more annotations. In this paper, we propose a weakly-supervised counting network, which directly regresses the crowd numbers without the location supervision. Moreover, we train the network to count by exploiting the relationship among the images. We propose a soft-label sorting network along with the counting network, which sorts the given images by their crowd numbers. The sorting network drives the shared backbone CNN model to obtain density-sensitive ability explicitly. Therefore, the proposed method improves the counting accuracy by utilizing the information hidden in crowd numbers, rather than learning from extra labels, such as locations and perspectives. We evaluate our proposed method on three crowd counting datasets, and the performance of our method plays favorably against the fully supervised state-of-the-art approaches.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_1');
INSERT INTO `paper` VALUES (13008, 'Weakly-Supervised Learning of Human Dynamics', 'Artificial neural networks', 'Human motion', 'Forward dynamics', 'Inverse dynamics', 'Weakly-supervised learning', 'This paper proposes a weakly-supervised learning framework for dynamics estimation from human motion. Although there are many solutions to capture pure human motion readily available, their data is not sufficient to analyze quality and efficiency of movements. Instead, the forces and moments driving human motion (the dynamics) need to be considered. Since recording dynamics is a laborious task that requires expensive sensors and complex, time-consuming optimization, dynamics data sets are small compared to human motion data sets and are rarely made public. The proposed approach takes advantage of easily obtainable motion data which enables weakly-supervised learning on small dynamics sets and weakly-supervised domain transfer. Our method includes novel neural network (NN) layers for forward and inverse dynamics during end-to-end training. On this basis, a cyclic loss between pure motion data can be minimized, i.e. no ground truth forces and moments are required during training. The proposed method achieves state-of-the-art results in terms of ground reaction force, ground reaction moment and joint torque regression and is able to maintain good performance on substantially reduced sets.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_5');
INSERT INTO `paper` VALUES (13009, 'Webly Supervised Image Classification with Self-contained Confidence', 'Webly supervised learning', 'Noisy labels', 'Model uncertainty', '', '', 'This paper focuses on webly supervised learning (WSL), where datasets are built by crawling samples from the Internet and directly using search queries as web labels. Although WSL benefits from fast and low-cost data collection, noises in web labels hinder better performance of the image classification model. To alleviate this problem, in recent works, self-label supervised loss \\(\\mathcal {L}_s\\) is utilized together with webly supervised loss \\(\\mathcal {L}_w\\). \\(\\mathcal {L}_s\\) relies on pseudo labels predicted by the model itself. Since the correctness of the web label or pseudo label is usually on a case-by-case basis for each web sample, it is desirable to adjust the balance between \\(\\mathcal {L}_s\\) and \\(\\mathcal {L}_w\\) on sample level. Inspired by the ability of Deep Neural Networks (DNNs) in confidence prediction, we introduce Self-Contained Confidence (SCC) by adapting model uncertainty for WSL setting, and use it to sample-wisely balance \\(\\mathcal {L}_s\\) and \\(\\mathcal {L}_w\\). Therefore, a simple yet effective WSL framework is proposed. A series of SCC-friendly regularization approaches are investigated, among which the proposed graph-enhanced mixup is the most effective method to provide high-quality confidence to enhance our framework. The proposed WSL framework has achieved the state-of-the-art results on two large-scale WSL datasets, WebVision-1000 and Food101-N. Code is available at https://github.com/bigvideoresearch/SCC.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_46');
INSERT INTO `paper` VALUES (13010, 'Weighing Counts: Sequential Crowd Counting by Reinforcement Learning', 'Crowd counting', 'Reinforcement learning', '', '', '', 'We formulate counting as a sequential decision problem and present a novel crowd counting model solvable by deep reinforcement learning. In contrast to existing counting models that directly output count values, we divide one-step estimation into a sequence of much easier and more tractable sub-decision problems. Such sequential decision nature corresponds exactly to a physical process in reality—scale weighing. Inspired by scale weighing, we propose a novel ‘counting scale’ termed LibraNet where the count value is analogized by weight. By virtually placing a crowd image on one side of a scale, LibraNet (agent) sequentially learns to place appropriate weights on the other side to match the crowd count. At each step, LibraNet chooses one weight (action) from the weight box (the pre-defined action pool) according to the current crowd image features and weights placed on the scale pan (state). LibraNet is required to learn to balance the scale according to the feedback of the needle (Q values). We show that LibraNet exactly implements scale weighing by visualizing the decision process how LibraNet chooses actions. Extensive experiments demonstrate the effectiveness of our design choices and report state-of-the-art results on a few crowd counting benchmarks, including ShanghaiTech, UCF_CC_50 and UCF-QNRF. We also demonstrate good cross-dataset generalization of LibraNet. Code and models are made available at https://git.io/libranet.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58607-2_10');
INSERT INTO `paper` VALUES (13011, 'Weight Decay Scheduling and Knowledge Distillation for Active Learning', 'Active learning', 'Weight decay', 'Knowledge distillation', '', '', 'Although convolutional neural networks perform extremely well for numerous computer vision tasks, a considerably large amount of labeled data is required to ensure a good outcome. Data labeling is labor-intensive, and in some cases, the labeling budget may be limited. Active learning is a technique that can reduce the labeling required. With this technique, the neural network selects on its own the unlabeled data most helpful for learning, and then requests the human annotator for the labels. Most existing active learning methods have focused on acquisition functions for an effective selection of the informative samples. However, in this paper, we focus on the data-incremental nature of active learning, and propose a method for properly tuning the weight decay as the amount of data increases. We also demonstrate that the performance can be improved by knowledge distillation using a low-performance teacher model trained from the previous acquisition step. In addition, we present a novel perspective of the weight decay, which provides a regularization effect by limiting the number of effective parameters and channels in the convolutional filter. We validate our methods on the MNIST, CIFAR-10, and CIFAR-100 datasets using convolutional neural networks of various sizes.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_26');
INSERT INTO `paper` VALUES (13012, 'Weight Excitation: Built-in Attention Mechanisms in Convolutional Neural Networks', 'Convolutional neural network', 'Convolution filter weights', 'Weight reparameterization', 'Attention mechanism', '', 'We propose novel approaches for simultaneously identifying important weights of a convolutional neural network (ConvNet) and providing more attention to the important weights during training. More formally, we identify two characteristics of a weight, its magnitude and its location, which can be linked with the importance of the weight. By targeting these characteristics of a weight during training, we develop two separate weight excitation (WE) mechanisms via weight reparameterization-based backpropagation modifications. We demonstrate significant improvements over popular baseline ConvNets on multiple computer vision applications using WE (e.g. 1.3% accuracy improvement over ResNet50 baseline on ImageNet image classification, etc.). These improvements come at no extra computational cost or ConvNet structural change during inference. Additionally, including WE methods in a convolution block is straightforward, requiring few lines of extra code. Lastly, WE mechanisms can provide complementary benefits when used with external attention mechanisms such as the popular Squeeze-and-Excitation attention block.', 'ECCV', '2020', '24 September 2020', 'https://doi.org/10.1007/978-3-030-58577-8_6');
INSERT INTO `paper` VALUES (13013, 'Weight-Dependent Gates for Differentiable Neural Network Pruning', 'Weight-dependent gates', 'Latency predict net', 'Accuracy-latency trade-off', 'Network pruning', '', 'In this paper, we propose a simple and effective network pruning framework, which introduces novel weight-dependent gates to prune filter adaptively. We argue that the pruning decision should depend on the convolutional weights, in other words, it should be a learnable function of filter weights. We thus construct the weight-dependent gates (W-Gates) to learn the information from filter weights and obtain binary filter gates to prune or keep the filters automatically. To prune the network under hardware constraint, we train a Latency Predict Net (LPNet) to estimate the hardware latency of candidate pruned networks. Based on the proposed LPNet, we can optimize W-Gates and the pruning ratio of each layer under latency constraint. The whole framework is differentiable and can be optimized by gradient-based method to achieve a compact network with better trade-off between accuracy and efficiency. We have demonstrated the effectiveness of our method on Resnet34 and Resnet50, achieving up to 1.33/1.28 higher Top-1 accuracy with lower hardware latency on ImageNet. Compared with state-of-the-art pruning methods, our method achieves superior performance(This work is done when Yun Li, Weiqun Wu and Zechun Liu are interns at Megvii Inc (Face++)).', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_3');
INSERT INTO `paper` VALUES (13014, 'WeightNet: Revisiting the Design Space of Weight Networks', 'CNN', 'Weight network', 'Conditional kernel', '', '', 'We present a conceptually simple, flexible and effective framework for weight generating networks. Our approach is general that unifies two current distinct and extremely effective SENet and CondConv into the same framework on weight space. The method, called WeightNet, generalizes the two methods by simply adding one more grouped fully-connected layer to the attention activation layer. We use the WeightNet, composed entirely of (grouped) fully-connected layers, to directly output the convolutional weight. WeightNet is easy and memory-conserving to train, on the kernel space instead of the feature space. Because of the flexibility, our method outperforms existing approaches on both ImageNet and COCO detection tasks, achieving better Accuracy-FLOPs and Accuracy-Parameter trade-offs. The framework on the flexible weight space has the potential to further improve the performance. Code is available at https://github.com/megvii-model/WeightNet.', 'ECCV', '2020', '16 November 2020', 'https://doi.org/10.1007/978-3-030-58555-6_46');
INSERT INTO `paper` VALUES (13015, 'What Does CNN Shift Invariance Look Like? A Visualization Study', 'Feature extraction', 'Shift invariance', 'Robust recognition', '', '', 'Feature extraction with convolutional neural networks (CNNs) is a popular method to represent images for machine learning tasks. These representations seek to capture global image content, and ideally should be independent of geometric transformations. We focus on measuring and visualizing the shift invariance of extracted features from popular off-the-shelf CNN models. We present the results of three experiments comparing representations of millions of images with exhaustively shifted objects, examining both local invariance (within a few pixels) and global invariance (across the image frame). We conclude that features extracted from popular networks are not globally invariant, and that biases and artifacts exist within this variance. Additionally, we determine that anti-aliased models significantly improve local invariance but do not impact global invariance. Finally, we provide a code repository for experiment reproduction, as well as a website to interact with our results at https://jakehlee.github.io/visualize-invariance.', 'ECCV', '2020', '31 January 2021', 'https://doi.org/10.1007/978-3-030-68238-5_15');
INSERT INTO `paper` VALUES (13016, 'What Is Learned in Deep Uncalibrated Photometric Stereo?', 'Uncalibrated photometric stereo', 'Generalized bas-relief ambiguity', 'Deep neural network', '', '', 'This paper targets at discovering what a deep uncalibrated photometric stereo network learns to resolve the problem’s inherent ambiguity, and designing an effective network architecture based on the new insight to improve the performance. The recently proposed deep uncalibrated photometric stereo method achieved promising results in estimating directional lightings. However, what specifically inside the network contributes to its success remains a mystery. In this paper, we analyze the features learned by this method and find that they strikingly resemble attached shadows, shadings, and specular highlights, which are known to provide useful clues in resolving the generalized bas-relief (GBR) ambiguity. Based on this insight, we propose a guided calibration network, named GCNet, that explicitly leverages object shape and shading information for improved lighting estimation. Experiments on synthetic and real datasets show that GCNet achieves improved results in lighting estimation for photometric stereo, which echoes the findings of our analysis. We further demonstrate that GCNet can be directly integrated with existing calibrated methods to achieve improved results on surface normal estimation. Our code and model can be found at https://guanyingc.github.io/UPS-GCNet.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58568-6_44');
INSERT INTO `paper` VALUES (13017, 'What Leads to Generalization of Object Proposals?', 'Object proposals', 'Object detection', 'Generalization', '', '', 'Object proposal generation is often the first step in many detection models. It is lucrative to train a good proposal model, that generalizes to unseen classes. Motivated by this, we study how a detection model trained on a small set of source classes can provide proposals that generalize to unseen classes. We systematically study the properties of the dataset – visual diversity and label space granularity – required for good generalization. We show the trade-off between using fine-grained labels and coarse labels. We introduce the idea of prototypical classes: a set of sufficient and necessary classes required to train a detection model to obtain generalized proposals in a more data-efficient way. On the Open Images V4 dataset, we show that only \\(25\\%\\) of the classes can be selected to form such a prototypical set. The resulting proposals from a model trained with these classes is only \\(4.3\\%\\) worse than using all the classes, in terms of average recall (AR). We also demonstrate that Faster R-CNN model leads to better generalization of proposals compared to a single-stage network like RetinaNet.', 'ECCV', '2020', '03 January 2021', 'https://doi.org/10.1007/978-3-030-66096-3_32');
INSERT INTO `paper` VALUES (13018, 'What Makes Fake Images Detectable? Understanding Properties that Generalize', 'Image forensics', 'Generative models', 'Image manipulation', 'Visualization', 'Generalization', 'The quality of image generation and manipulation is reaching impressive levels, making it increasingly difficult for a human to distinguish between what is real and what is fake. However, deep networks can still pick up on the subtle artifacts in these doctored images. We seek to understand what properties of fake images make them detectable and identify what generalizes across different model architectures, datasets, and variations in training. We use a patch-based classifier with limited receptive fields to visualize which regions of fake images are more easily detectable. We further show a technique to exaggerate these detectable properties and demonstrate that, even when the image generator is adversarially finetuned against a fake image classifier, it is still imperfect and leaves detectable artifacts in certain image patches. Code is available at https://github.com/chail/patch-forensics.', 'ECCV', '2020', '13 November 2020', 'https://doi.org/10.1007/978-3-030-58574-7_7');
INSERT INTO `paper` VALUES (13019, 'What Matters in Unsupervised Optical Flow', '', '', '', '', '', 'We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58536-5_33');
INSERT INTO `paper` VALUES (13020, 'When Does Self-supervision Improve Few-Shot Learning?', '', '', '', '', '', 'We investigate the role of self-supervised learning (SSL) in the context of few-shot learning. Although recent research has shown the benefits of SSL on large unlabeled datasets, its utility on small datasets is relatively unexplored. We find that SSL reduces the relative error rate of few-shot meta-learners by 4%–27%, even when the datasets are small and only utilizing images within the datasets. The improvements are greater when the training set is smaller or the task is more challenging. Although the benefits of SSL may increase with larger training sets, we observe that SSL can hurt the performance when the distributions of images used for meta-learning and SSL are different. We conduct a systematic study by varying the degree of domain shift and analyzing the performance of several meta-learners on a multitude of domains. Based on this analysis we present a technique that automatically selects images for SSL from a large, generic pool of unlabeled images for a given dataset that provides further improvements.', 'ECCV', '2020', '09 November 2020', 'https://doi.org/10.1007/978-3-030-58571-6_38');
INSERT INTO `paper` VALUES (13021, 'Where to Explore Next? ExHistCNN for History-Aware Autonomous 3D Exploration', 'Next best view', 'CNN', '3D exploration', '3D reconstruction', '', 'In this work we address the problem of autonomous 3D exploration of an unknown indoor environment using a depth camera. We cast the problem as the estimation of the Next Best View (NBV) that maximises the coverage of the unknown area. We do this by re-formulating NBV estimation as a classification problem and we propose a novel learning-based metric that encodes both, the current 3D observation (a depth frame) and the history of the ongoing reconstruction. One of the major contributions of this work is about introducing a new representation for the 3D reconstruction history as an auxiliary utility map which is efficiently coupled with the current depth observation. With both pieces of information, we train a light-weight CNN, named ExHistCNN, that estimates the NBV as a set of directions towards which the depth sensor finds most unexplored areas. We perform extensive evaluation on both synthetic and real room scans demonstrating that the proposed ExHistCNN is able to approach the exploration performance of an oracle using the complete knowledge of the 3D environment.', 'ECCV', '2020', '07 October 2020', 'https://doi.org/10.1007/978-3-030-58526-6_8');
INSERT INTO `paper` VALUES (13022, 'Who Left the Dogs Out? 3D Animal Reconstruction with Expectation Maximization in the Loop', '', '', '', '', '', 'We introduce an automatic, end-to-end method for recovering the 3D pose and shape of dogs from monocular internet images. The large variation in shape between dog breeds, significant occlusion and low quality of internet images makes this a challenging problem. We learn a richer prior over shapes than previous work, which helps regularize parameter estimation. We demonstrate results on the Stanford Dog Dataset, an ‘in the wild’ dataset of 20,580 dog images for which we have collected 2D joint and silhouette annotations to split for training and evaluation. In order to capture the large shape variety of dogs, we show that the natural variation in the 2D dataset is enough to learn a detailed 3D prior through expectation maximization (EM). As a by-product of training, we generate a new parameterized model (including limb scaling) SMBLD which we release alongside our new annotation dataset StanfordExtra to the research community.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_12');
INSERT INTO `paper` VALUES (13023, 'Whole-Body Human Pose Estimation in the Wild', 'Whole-body human pose estimation', 'Facial landmark detection', 'Hand keypoint estimation', '', '', 'This paper investigates the task of 2D human whole-body pose estimation, which aims to localize dense landmarks on the entire human body including face, hands, body, and feet. As existing datasets do not have whole-body annotations, previous methods have to assemble different deep models trained independently on different datasets of the human face, hand, and body, struggling with dataset biases and large model complexity. To fill in this blank, we introduce COCO-WholeBody which extends COCO dataset with whole-body annotations. To our best knowledge, it is the first benchmark that has manual annotations on the entire human body, including 133 dense landmarks with 68 on the face, 42 on hands and 23 on the body and feet. A single-network model, named ZoomNet, is devised to take into account the hierarchical structure of the full human body to solve the scale variation of different body parts of the same person. ZoomNet is able to significantly outperform existing methods on the proposed COCO-WholeBody dataset. Extensive experiments show that COCO-WholeBody not only can be used to train deep models from scratch for whole-body pose estimation but also can serve as a powerful pre-training dataset for many different tasks such as facial landmark detection and hand keypoint estimation. The dataset is publicly available at https://github.com/jin-s13/COCO-WholeBody.', 'ECCV', '2020', '05 November 2020', 'https://doi.org/10.1007/978-3-030-58545-7_12');
INSERT INTO `paper` VALUES (13024, 'Why Are Deep Representations Good Perceptual Quality Features?', '', '', '', '', '', 'Recently, intermediate feature maps of pre-trained convolutional neural networks have shown significant perceptual quality improvements, when they are used in the loss function for training new networks. It is believed that these features are better at encoding the perceptual quality and provide more efficient representations of input images compared to other perceptual metrics such as SSIM and PSNR. However, there have been no systematic studies to determine the underlying reason. Due to the lack of such an analysis, it is not possible to evaluate the performance of a particular set of features or to improve the perceptual quality even more by carefully selecting a subset of features from a pre-trained CNN. This work shows that the capabilities of pre-trained deep CNN features in optimizing the perceptual quality are correlated with their success in capturing basic human visual perception characteristics. In particular, we focus our analysis on fundamental aspects of human perception, such as the contrast sensitivity and orientation selectivity. We introduce two new formulations to measure the frequency and orientation selectivity of the features learned by convolutional layers for evaluating deep features learned by widely-used deep CNNs such as VGG-16. We demonstrate that the pre-trained CNN features which receive higher scores are better at predicting human quality judgment. Furthermore, we show the possibility of using our method to select deep features to form a new loss function, which improves the image reconstruction quality for the well-known single-image super-resolution problem.', 'ECCV', '2020', '17 November 2020', 'https://doi.org/10.1007/978-3-030-58542-6_27');
INSERT INTO `paper` VALUES (13025, 'Why Do These Match? Explaining the Behavior of Image Similarity Models', 'Explainable AI', 'Image similarity models', 'Fashion compatibility', 'Image retrieval', '', 'Explaining a deep learning model can help users understand its behavior and allow researchers to discern its shortcomings. Recent work has primarily focused on explaining models for tasks like image classification or visual question answering. In this paper, we introduce Salient Attributes for Network Explanation (SANE) to explain image similarity models, where a model’s output is a score measuring the similarity of two inputs rather than a classification score. In this task, an explanation depends on both of the input images, so standard methods do not apply. Our SANE explanations pairs a saliency map identifying important image regions with an attribute that best explains the match. We find that our explanations provide additional information not typically captured by saliency maps alone, and can also improve performance on the classic task of attribute recognition. Our approach’s ability to generalize is demonstrated on two datasets from diverse domains, Polyvore Outfits and Animals with Attributes 2. Code available at: https://github.com/VisionLearningGroup/SANE.', 'ECCV', '2020', '27 November 2020', 'https://doi.org/10.1007/978-3-030-58621-8_38');
INSERT INTO `paper` VALUES (13026, 'World-Consistent Video-to-Video Synthesis', 'Neural rendering', 'Video synthesis', 'GAN', '', '', 'Video-to-video synthesis (vid2vid) aims for converting high-level semantic inputs to photorealistic videos. While existing vid2vid methods can achieve short-term temporal consistency, they fail to ensure the long-term one. This is because they lack knowledge of the 3D world being rendered and generate each frame only based on the past few frames. To address the limitation, we introduce a novel vid2vid framework that efficiently and effectively utilizes all past generated frames during rendering. This is achieved by condensing the 3D world rendered so far into a physically-grounded estimate of the current frame, which we call the guidance image. We further propose a novel neural network architecture to take advantage of the information stored in the guidance images. Extensive experimental results on several challenging datasets verify the effectiveness of our approach in achieving world consistency—the output video is consistent within the entire rendered 3D world.', 'ECCV', '2020', '07 November 2020', 'https://doi.org/10.1007/978-3-030-58598-3_22');
INSERT INTO `paper` VALUES (13027, 'XingGAN for Person Image Generation', 'Generative Adversarial Networks (GANs)', 'Person image generation', 'Appearance cues', 'Shape cues', '', 'We propose a novel Generative Adversarial Network (XingGAN or CrossingGAN) for person image generation tasks, i.e., translating the pose of a given person to a desired one. The proposed Xing generator consists of two generation branches that model the person’s appearance and shape information, respectively. Moreover, we propose two novel blocks to effectively transfer and update the person’s shape and appearance embeddings in a crossing way to mutually improve each other, which has not been considered by any other existing GAN-based image generation work. Extensive experiments on two challenging datasets, i.e., Market-1501 and DeepFashion, demonstrate that the proposed XingGAN advances the state-of-the-art performance both in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at https://github.com/Ha0Tang/XingGAN.', 'ECCV', '2020', '20 November 2020', 'https://doi.org/10.1007/978-3-030-58595-2_43');
INSERT INTO `paper` VALUES (13028, 'Yet Another Intermediate-Level Attack', 'Adversarial examples', 'Transferability', 'Feature maps', '', '', 'The transferability of adversarial examples across deep neural network (DNN) models is the crux of a spectrum of black-box attacks. In this paper, we propose a novel method to enhance the black-box transferability of baseline adversarial examples. By establishing a linear mapping of the intermediate-level discrepancies (between a set of adversarial inputs and their benign counterparts) for predicting the evoked adversarial loss, we aim to take full advantage of the optimization procedure of mulch-step baseline attacks. We conducted extensive experiments to verify the effectiveness of our method on CIFAR-100 and ImageNet. Experimental results demonstrate that it outperforms previous state-of-the-arts considerably. Our code is at https://github.com/qizhangli/ila-plus-plus.', 'ECCV', '2020', '10 October 2020', 'https://doi.org/10.1007/978-3-030-58517-4_15');
INSERT INTO `paper` VALUES (13029, 'YOLO in the Dark - Domain Adaptation Method for Merging Multiple Models', 'Knowledge distillation', 'Domain adaptation', 'Object detection', '', '', 'Generating models to handle new visual tasks requires additional datasets, which take considerable effort to create. We propose a method of domain adaptation for merging multiple models with less effort than creating an additional dataset. This method merges pre-trained models in different domains using glue layers and a generative model, which feeds latent features to the glue layers to train them without an additional dataset. We also propose a generative model that is created by distilling knowledge from pre-trained models. This enables the dataset to be reused to create latent features for training the glue layers. We apply this method to object detection in a low-light situation. The YOLO-in-the-Dark model comprises two models, Learning-to-See-in-the-Dark model and YOLO. We present the proposed method and report the result of domain adaptation to detect objects from RAW short-exposure low-light images. The YOLO-in-the-Dark model uses fewer computing resources than the naive approach.', 'ECCV', '2020', '12 November 2020', 'https://doi.org/10.1007/978-3-030-58589-1_21');
INSERT INTO `paper` VALUES (13030, 'You Are Here: Geolocation by Embedding Maps and Images', 'Geolocalisation', 'Image-map embeddings', 'Cross domain localisation', 'Representation learning', '', 'We present a novel approach to geolocalising panoramic images on a 2-D cartographic map based on learning a low dimensional embedded space, which allows a comparison between an image captured at a location and local neighbourhoods of the map. The representation is not sufficiently discriminatory to allow localisation from a single image, but when concatenated along a route, localisation converges quickly, with over 90% accuracy being achieved for routes of around 200 m in length when using Google Street View and Open Street Map data. The method generalises a previous fixed semantic feature based approach and achieves significantly higher localisation accuracy and faster convergence.', 'ECCV', '2020', '03 November 2020', 'https://doi.org/10.1007/978-3-030-58592-1_30');
INSERT INTO `paper` VALUES (13031, 'Zero-Shot Image Super-Resolution with Depth Guided Internal Degradation Learning', 'Image super-resolution', 'Zero-shot', 'Depth guidance', '', '', 'In the past few years, we have witnessed the great progress of image super-resolution (SR) thanks to the power of deep learning. However, a major limitation of the current image SR approaches is that they assume a pre-determined degradation model or kernel, e.g. bicubic, controls the image degradation process. This makes them easily fail to generalize in a real-world or non-ideal environment since the degradation model of an unseen image may not obey the pre-determined kernel used when training the SR model. In this work, we introduce a simple yet effective zero-shot image super-resolution model. Our zero-shot SR model learns an image-specific super-resolution network (SRN) from a low-resolution input image alone, without relying on external training sets. To circumvent the difficulty caused by the unknown internal degradation model of an image, we propose to learn an image-specific degradation simulation network (DSN) together with our image-specific SRN. Specifically, we exploit the depth information, naturally indicating the scales of local image patches, of an image to extract the unpaired high/low-resolution patch collection to train our networks. According to the benchmark test on four datasets with depth labels or estimated depth maps, our proposed depth guided degradation model learning-based image super-resolution (DGDML-SR) achieves visually pleasing results and can outperform the state-of-the-arts in perceptual metrics.', 'ECCV', '2020', '19 November 2020', 'https://doi.org/10.1007/978-3-030-58520-4_16');

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user`  (
  `userid` int(6) NOT NULL AUTO_INCREMENT,
  `username` varchar(12) CHARACTER SET latin1 COLLATE latin1_swedish_ci NOT NULL,
  `password` varchar(18) CHARACTER SET latin1 COLLATE latin1_swedish_ci NOT NULL,
  PRIMARY KEY (`userid`) USING BTREE,
  UNIQUE INDEX `name`(`username`) USING BTREE,
  UNIQUE INDEX `id`(`userid`) USING BTREE
) ENGINE = MyISAM AUTO_INCREMENT = 4 CHARACTER SET = latin1 COLLATE = latin1_swedish_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES (1, 'Tiantian', '123456');
INSERT INTO `user` VALUES (2, 'taylor', '7777777');

-- ----------------------------
-- Table structure for userpaper
-- ----------------------------
DROP TABLE IF EXISTS `userpaper`;
CREATE TABLE `userpaper`  (
  `pid` int(6) NOT NULL,
  `userid` int(6) NOT NULL,
  `remarks` varchar(500) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  `collect` int(11) NOT NULL,
  `folder` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL,
  PRIMARY KEY (`pid`, `userid`) USING BTREE
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;

-- ----------------------------
-- Records of userpaper
-- ----------------------------
INSERT INTO `userpaper` VALUES (10004, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10011, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10012, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10019, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10026, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10069, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10094, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10101, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10110, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10127, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10130, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10141, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10148, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10158, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10171, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10175, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10203, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10214, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10227, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10240, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10241, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10258, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10280, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10282, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10296, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10305, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10313, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10337, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10340, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10351, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10380, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10390, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10394, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10405, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10409, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10426, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10435, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10455, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10458, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10476, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10480, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10509, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10513, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10524, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10525, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10542, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10567, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10580, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10580, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10600, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10610, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10619, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10622, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10637, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10668, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10669, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10670, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10680, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10700, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10704, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10709, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10725, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10734, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10748, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10800, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10816, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10847, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10852, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10857, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10865, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10887, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10912, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10915, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10935, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10936, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10965, 1, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10970, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (10971, 2, '', 1, '默认收藏夹');
INSERT INTO `userpaper` VALUES (10974, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (10996, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11013, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11047, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11072, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11092, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11102, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11105, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11112, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11118, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11137, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11147, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11155, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11157, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11164, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11172, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11186, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11190, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11195, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11204, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11205, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11216, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11224, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11235, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11236, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11241, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11243, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11251, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11276, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11306, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11330, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11332, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11339, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11355, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11355, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11362, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11381, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11386, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11388, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11392, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11398, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11431, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11447, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11454, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11459, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11482, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11488, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11490, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11510, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11510, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11511, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11522, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11529, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11547, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11563, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11572, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11585, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11586, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11587, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11609, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11621, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11644, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11658, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11660, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11676, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11684, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11693, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11698, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11734, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11737, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11741, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11758, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11759, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11768, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11790, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11798, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11802, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11817, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11845, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11857, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11861, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11864, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11868, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11883, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11910, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11914, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11919, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11938, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (11944, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (11991, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12009, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12016, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12053, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12070, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12075, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12081, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12087, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12093, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12100, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12110, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12110, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12118, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12151, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12153, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12159, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12170, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12208, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12232, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12238, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12242, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12266, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12280, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12286, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12293, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12298, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12307, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12322, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12330, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12332, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12343, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12363, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12374, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12391, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12394, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12411, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12422, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12429, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12447, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12470, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12471, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12474, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12514, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12539, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12547, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12567, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12579, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12595, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12607, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12626, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12627, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12662, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12685, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12694, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12710, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12713, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12717, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12723, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12733, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12745, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12751, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12759, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12771, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12775, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12778, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12800, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12829, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12851, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12854, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12857, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12860, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12871, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12882, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12887, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12892, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12892, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12915, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12945, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12949, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12967, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12990, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (12995, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (12998, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (13002, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (13004, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (13010, 1, '', 0, '');
INSERT INTO `userpaper` VALUES (13028, 2, '', 0, '');
INSERT INTO `userpaper` VALUES (13030, 2, '', 0, '');

SET FOREIGN_KEY_CHECKS = 1;
